Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf:
this can include variances in scanner quality and reso-
lution, in addition to the fov selected during patient scans.
https://doi.org/10.1007/978-3-031-43907-0_29
geometry-invariant abnormality detection
301
dimensions, e.g. by registering data to a population atlas.
even though these methods are state-of-the-art, they have stringent data
requirements, such as having a consistent geometry of the input data, e.g., in a
whole-body imaging scenario, it is not possible to crop a region of interest and
feed it to the algorithm, as this cropped region will be wrongly detected as an
anomaly.
in that case, we can
contract this range to represent the area displayed in the image (fig. 2).
as such, the transformer must be informed of
the location of a given token in relationship to the whole-body.
to do this, we use the same coordconv principle applied to the input fed to
the vq-vae.
in addition, we calculate the area under the precision-recall curve
(auprc) as a suitable measure for segmentation performance under class imbal-
ance.
308
a. patel et al.
5
conclusion
detection and segmentation of anomalous regions, particularly for cancer
patients, is essential for staging, treatment and intervention planning.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf:
for each foreground pixel in the annotation a at location (x, y), we label
(x, y, zfw) and (x, y, zbw) as foreground pixels in d.
3.
the cohort consists of 141 patients with pancreatic ductal adeno-
carcinoma, of an equal ratio of male to female patients.
given a 3d arterial
ct of the abdominal area, we automatically extract the vertebrae

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf:
although some recent methods have attempted to
extend cam to cover more areas, the fundamental problem still needs
to be solved.
tumor epithelial ssue 
necrosis ssue 
tumor-associated 
stroma
necrosis may appear as 
areas of pink, 
amorphous material 
under the microscope, 
and may be surrounded 
by viable tumor cells 
and stroma.
however, the
cam generated based on the image-level labels can only highlight the most dis-
criminative region, but fail to locate the complete object, leading to defective
pseudo labels, as shown in fig.
[7] proposed an erasure-based method that con-
tinuously expands the scope of attention areas to obtain rich content of pseudo
labels.
as a result, the image-level label supervision may
not be suﬃcient to pinpoint the complete target area.
a higher similarity represents a higher possibility of
this location belonging to the corresponding semantic category.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf:
contrastive
methods can also be used to learn dense, i.e., patch-level or even pixel- or voxel-
level representations: pixels of augmented image views from the same region
of the original image should have similar representations, while diﬀerent pixels
should have dissimilar ones
this means that matching regions
describing the same location of the scene on diﬀerent views should be positive
pairs, while non-matching regions should be negative pairs.
next, we sample m diﬀerent positions from the patches’ overlapping
region.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf:
with the abundance of medical image data,
many research institutions release models trained on various datasets
that can form a huge pool of candidate source models to choose from.
[10,21,30].
compared with the distributed training across multiple centers, there are no
speciﬁc ethical issues or computational design of distributed/federated learning
frameworks with the “pre-train-then-ﬁne-tune” workﬂow.
with the
increasing number of pre-trained networks provided by the community, model
repositories like hugging face
if the features are generalizable, foreground region features will
likely follow a similar distribution even without ﬁne-tuning.
[2] dataset is composed of ten dif-
ferent datasets with various challenging characteristics, which are widely used in
the medical image analysis ﬁeld.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf:
moreover,
data protection policies in diﬀerent clinical centers and hospitals limit
the training of data-dependent deep models.
[13] extend the above work to semantic segmentation and proposed
a method named model-invariant feature learning, which takes full advantage of
the diverse characteristics of the source-domain models.
such privacy breaches
may detrimental to the privacy protection policies of hospitals.
moreover, the
target domain uses the same neural network as the source domain, which is
not desirable for low-resource target users like hospitals [15].
we assume that k-square-neighbors
of a pixel as a cell region, i.e., for a logits map with height h and width w, we
deﬁne the region as follow:
nk{(i, j) | (i, j) ∈ (h, w)} = {(u, v) | |u − i| ≤ k, |v − j| ≤ k}
(1)
where (i, j) denotes centre of region, and k denotes the size of k-square-neighbors.
due to the unique characteristics of cell morphology, merely relying on uncer-
tainty information is insuﬃcient to produce high-quality ensemble logits map
that accurately capture the relevance between the source and target domains.
then accord-
ing to c-classes classiﬁcation tasks, we divide the cell region into c subsets,
n c
k(i, j) = {(u, v) ∈ nk(i, j) | yt = c}.
after that, we determine the degree of
black-box domain adaptative cell segmentation
753
impurity in an area of interest by analyzing the statistics of the boundary region,
which represents the level of semantic information ambiguity.
speciﬁcally, the
number of diﬀerent objects within the area is considered a proxy for its impu-
rity level, with higher counts indicating higher impurity.the boundary impurity
p(i,j) can be calculated as:
p(i,j)
n
= −
c

c=1
|n c
k(i, j)|
|nk(i, j)| log |n c
k(i, j)|
|nk(i, j)|
(3)
where | ·
| denotes the number of pixels in the area.
firstly, we acquire 50 images from a cohort of patients with
triple negative breast cancer (tnbc), which is released by naylor et al

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf:
statistical shape modeling is the computational process of
discovering signiﬁcant shape parameters from segmented anatomies cap-
tured by medical images (such as mri and ct scans), which can fully
describe subject-speciﬁc anatomy in the context of a population.
deep learning
techniques can learn complex non-linear representations of shapes and
generate statistical shape models that are more faithful to the under-
lying population-level variability.
mesh2ssm can also learn a
population-speciﬁc template, reducing any bias due to template selec-
tion.
[26], while group-wise non-parametric approaches
ﬁnd correspondences by considering the variability of the entire cohort during
the optimization process.
[7].
traditional ssm methods assume that population variability follows a gaus-
sian distribution, which implies that a linear combination of training shapes
can express unseen shapes.
mesh2ssm leverages unsupervised, permutation-invariant representation learn-
ing to learn the low dimensional nonlinear shape descriptor directly from mesh
data and uses the learned features to generate a correspondence model of the
population.
3
experiments and discussion
dataset: we use the publicly available decath-pancreas dataset of 273 seg-
mentations from patients who underwent pancreatic mass resection
[13,21] analysis module helps in mitigating bias and
capturing non-linear characteristics of the data.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf:
patient selection for such
l. fillioux and j. boyd—these authors contributed equally to this work.
increasingly, clinical centers are investing
in the digitisation of such tissue slides to enable both automatic processing as
well as research studies to elucidate the underlying biological processes of cancer.
[16] is a dataset that consists of resections of lymph nodes,
where each wsi is annotated with a binary label indicating the presence of
tumour tissue in the slide, and all slides containing tumors have a pixel-level
annotation indicating the metastatic region.
tcga-luad is a tcga lung adenocarcinoma dataset that contains 541
wsis along with genetic information about each patient.
as a mil task, we chose the
task of predicting the patient mutation status of tp53, a tumor suppressor gene
that is highly relevant in oncology studies.
we evaluate our method on each
dataset by accuracy and area under receiver operating characteristic curve
(auroc).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf:
in contrast to previous data-driven meth-
ods, our recursive network fully exploits the hierarchical organization of the ves-
sel and learns a low-dimensional manifold encoding branch connectivity along
with geometry features describing the target surface.
this subset consisted of 1694
healthy vessel segments reconstructed from 2d mra images of patients.
in those cases, we selected the
sample with the lowest radius, which ensures proper alignment with the center-
line principal direction.
the chosen metrics have been widely used in
the ﬁeld of blood vessel 3d modeling, and have shown to provide reliable and
accurate quantiﬁcation of blood vessels main characteristics

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf:
we, therefore, propose a novel approach towards pathology detection that
uses anatomical region bounding boxes, solely deﬁned on anatomical structures,
as proxies for pathology bounding boxes.
these region boxes are easier to anno-
tate – the physiological shape of a healthy subject’s thorax can be learned rela-
tively easily by medical students – and generalize better than those of patholo-
gies, such that huge labeled datasets are available [21].
in summary:
– we propose anatomy-driven pathology detection (adpd), a pathology detec-
tion approach for chest x-rays, trained with pathology classiﬁcation labels
together with anatomical region bounding boxes as proxies for pathologies.
anatomy-driven pathology detection on chest x-rays
59
backbone
(densenet121)
region detector
(detr decoder)
region tokens
pathology
classifier
(on regions)
box prediction
(with region fusion)
pneumonia
infiltration
cardiomegaly
pneumonia
infiltration
cardiomegaly
pneumonia
infiltration
cardiomegaly
target
predicted
training + inference
inference only
pneumonia: 0.72
pneumonia
fig.
for each region, observed pathologies are predicted
using a shared classiﬁer.
[16] to extract anatomical region features before predicting
observed pathologies for each region using either a linear model or a gcn model
based on pathology co-occurrences.
this approach has been further extended to
use gcns on anatomical region relationships [1].
unlike our and the other
described methods, it does however not use anatomical region bounding boxes.
as no anatomical region can occur more than once in each chest
x-ray, each query token is assigned to exactly one pre-deﬁned anatomical region,
such that the number of tokens equals the number of anatomical regions.
as described next, the resulting per-region features from
the output of the decoder layer will be used for predictions on each region.
for predicting whether the associated region is present, we use a binary clas-
siﬁer with a single linear layer, for bounding box prediction we use a three-layer
mlp followed by sigmoid.
each of these predictors
is applied independently to each region with their weights shared across regions.
we also did not observe
improvements when using several decoder layers and observed degrading perfor-
mance when using roi pooling to compute region features.
3.2
inference
during inference, the trained model predicts anatomical region bounding boxes
and per-region pathology probabilities, which are then used to predict pathology
bounding boxes in two steps, as shown in fig.
2. in step (i), pathology probabili-
ties are ﬁrst thresholded and for each positive pathology (with probability larger
than the threshold) the bounding box of the corresponding anatomical region
is predicted as its pathology box, using the pathology probability as box score.
this means, if a region contains several predicted pathologies, then all of its
predicted pathologies share the same bounding box during step (i).
as many anatomical regions are at least
partially overlapping, and we use a small iou-overlap threshold, this allows the
model to either pull the predicted boxes to relevant subparts of an anatomical
region or to predict that pathologies stretch over several regions.
3.3
training
the anatomical region detector is trained using the detr loss
mil-adpd: region predictions are ﬁrst aggregated
using lse pooling and then trained using image-level supervision.
here, the target set of observed pathologies is available for each
anatomical region individually such that the pathology observation prediction
can directly be trained for each anatomical region.
[17] loss
function independently on each region-pathology pair and average the results
over all regions and pathologies.
to train using mil, we ﬁrst aggregate the predicted pathology prob-
abilities of each region over all detected regions in the image using lse pooling
it is derived
from the mimic-cxr dataset [9,10], which is based on imaging studies from
65 079 patients performed at beth israel deaconess medical center in boston,
us.
we con-
sider the image-level label for a pathology to be positive if any region is positively
labeled with that pathology.
we use the provided jpg-images [11]2 and follow the oﬃcial mimic-cxr
training split but only keep samples containing a scene graph with at least ﬁve
valid region bounding boxes, resulting in a total of 234 307 training samples.
[20]3 from the national insti-
tutes of health clinical center in the us.
note that for evaluation only pathology
bounding boxes are required (to compute the metrics), while during training
only anatomical region bounding boxes (without considering pathologies) are
required.
all images are center-cropped and resized to 224 × 224.
cardiomegaly (red) is detected almost perfectly, as it is always exactly localized
at one anatomical region.
without wbf, results degrade for both of our models, highlighting the impor-
tance of merging region boxes.
4 loc-adpd detects cardiomegaly almost
perfectly, as it is always exactly localized at one anatomical region.
first, due to the dependence on region
proxies, for pathologies covering only a small part of a region, our models pre-
dict the whole region, as highlighted by their incapability to detect nodules.
addi-
tionally, while not requiring pathology bounding boxes, our models still require
supervision in the form of anatomical region bounding boxes, and loc-adpd
requires anatomy-level labels.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf:
https://doi.org/10.1007/978-3-031-43907-0_56
584
o. l. barbed et al.
for example, for improved monitoring of existing patients or augmented reality
during training or real explorations.
[14] of high interest for the community.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf:
(1), we can update the
mixing ratio between the two probability maps pspa and pspe with the weighted
entropy guidance at each pixel location by
ps2 =
hspe
hspa + hspe
⊗ pspa +
hspa
hspa + hspe
⊗ pspe,
(4)
where “⊗” denotes pixel-wise multiplication.
these datasets are collected
from diversiﬁed patients in multiple medical centers with various data acquisi-
tion systems.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf:
based
on longitudinal imaging for a given patient it requires establishing which lesions
are corresponding (i.e., same lesion, observed at diﬀerent timepoints), which
lesions have disappeared and which are new compared to prior scanning.
to increase the system robustness and emulate the clini-
cian’s reading strategies, we propose to use multi-scale embeddings to enable the
system to progressively reﬁne the ﬁne-grained location.
hence, we ensure the spatial coherence of the tracked
lesion location using well-deﬁned anatomical landmarks.
during inference, the extracted
embeddings are used to generate a cascade of cosine similarity maps that initially locate
the corresponding location in a follow-up image within a larger area and subsequently
improve the matching accuracy through gradual reﬁnement.
time points.
3
method
3.1
problem deﬁnition
let i1 (i.e., template or baseline image) and i2 (i.e., query or follow-up image)
be two 3d-ct scans acquired at time t1 and t2, respectively, additionally, let p1
576
a. vizitiu et al.
and p2 denote the point of interest (i.e., the lesion center) in both images.
the
problem of lesion tracking can be formulated as ﬁnding the optimal transforma-
tion that maps p1 to its corresponding location, p2, in i2.
3.2
training stage
let d = {x1, x2, ..., xn} be a set of n unpaired and unlabeled 3d-ct volumes.
we arbitrary sam-
ple npos positive pixel pairs from the overlapping area of xa and xq, denoted
by a+ = {a+
1 , ..., a+
j ..., a+
npos}, q+ = {q+
1 , ..., q+
j , ..., q+
npos}, 1 ≤ j ≤ npos.
we denote the positive embed-
dings at ith scale at pixel location a+
j , q+
j as fai
j, fqi
j ∈ rl.
similarly, we denote
the negative embeddings at pixel location h−
k associated to a positive positive
pixel pair (a+
j , q+
j ) as f i
jk ∈ rl.
[8] and the national lung screening trial (nlst)
dataset [12].
[11] medical imag-
ing dataset, containing 3891 pairs of lesions with information on their location
and size.
for nlst, we randomly selected
a subset of 1045 test images coming from 420 patients with up to 3 studies.
a
certiﬁed radiologist annotated the testing data by identifying the location and
size of the pulmonary nodules, resulting in a total of 825 paired annotations.
evaluation metrics: we use mean euclidean distance (med) to measure
the distance between predicted lesion center and ground truth, and the center
point matching accuracy (i.e., percentage of accurately matched lesions given the
annotated lesion radius), denoted with cpm@radius.
(color ﬁgure online)
580
a. vizitiu et al.
location reﬁnement.
2.
on the nlst dataset, our proposed method obtains a center point matching
accuracy of 92.12% (table 2).
the green and red markers denote the ground-truth
and predicted lesion location.
we found that adopting a multi-scale approach (instead of the global/local
approach as proposed in [5]) can lead to embeddings that better capture the
anatomical location and are able to handle lesions that vary in size or appear-
ance at diﬀerent scales.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf:
even though displacement estimation in use and computer
vision share some challenges, use displacement estimation has two dis-
tinct characteristics that set it apart from the computer vision coun-
terpart: high-frequency nature of rf data, and the physical rules that
govern the motion pattern.
the center fre-
quency was 8 mhz and the sampling frequency was 40 mhz.
in vivo data was collected at johns hopkins hospital from patients with liver
cancer during open-surgical rf thermal ablation by a research antares siemens
system using a vf 10-5 linear array with the sampling frequency of 40 mhz and
the center frequency of 6.67 mhz.
the institutional review board approved the
study with the consent of the patients.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf:
however, most methods often fail to separate adjacent
nuclei and are particularly sensitive to point annotations that deviate
from the center of nuclei, resulting in lower accuracy.
we detect and segment nuclei by combining a binary segmentation mod-
ule, an oﬀset regression module, and a center detection module to deter-
mine foreground pixels, delineate boundaries and identify instances.
next, segmentation pre-
dictions are used to repeatedly generate pseudo oﬀset maps that indi-
cate the most likely nuclei center.
[5,10,15,20,23,28] have emerged as an attractive
alternative using cheap and inexact labels e.g., center point annotations.
moreover, prior methods [17,21,22] typi-
cally assume that point labels are located precisely at the center of the nuclei.
in real-world scenarios, point annotation locations may shift from nuclei centers
as a result of the expert labeling process, leading to a lower performance after
model training.
to make
the model robust to center point shifts, we introduce an expectation maximiza-
tion (em)
note that previous
approaches [17,21,22] optimize model parameters only using a ﬁxed set of point
labels, while we instead alternatively update model parameters and the center
point locations.
this reﬁnement process ensures that the model maintains high
performance even when the point annotation is not exactly located at the center
of the nuclei.
the contributions of this paper are as follows: (1) we propose an end-to-
end weakly supervised segmentation model that simultaneously predicts binary
mask, oﬀset map, and center map to accurately identify and segment nuclei.
(3) we
introduce an em algorithm-based reﬁnement process to encourage model robust-
ness on center-shifted point labels.
it consists of an encoder and three modules
for binary segmentation, oﬀset map and center map prediction.
to train oﬀset map
and center map modules(blue lines), pseudo labels are generated using point label and
predicted binary segmentation mask(green lines).
during inference, the instance map,
obtained by predicted oﬀset map and center map, is multiplied with predicted binary
mask to produce instance segmentation prediction(orange lines).
(color ﬁgure online)
model consists of three modules: 1) binary segmentation module, 2) oﬀset map
module, and 3) center map module (fig. 1).
the fea-
ture maps are further processed through a series of residual units (rus) and
attention units (aus) to predict a binary segmentation mask ˆb, an oﬀset map
ˆo, and a center map ˆc.
then, we gener-
ate the pseudo oﬀset map o by using ˆb and p. next, following [29], we generate
the center map c by expanding the point label p with gaussian kernel within
a radius r. herein, our model is trained wih a segmentation loss lb(v,k, ˆb), an
oﬀset map loss lo(o, ˆo), and a center map loss lc(c, ˆc).
then, we generate an instance map i, which shares the same values
among the same instances as follows:
i(x, y) = arg min
i
||(x ˆci, y ˆci) − ((x, y) + ˆo(x, y))||2,
(1)
where (x, y) represents a coordinate and (x ˆci, y ˆci) means the location of ith
point obtained from ˆc.
[3,24] by computing
distances di between all center points pi ∈ p and pixels.
the boundaries of the
diagram in v are deﬁned as 0, while center points and the other regions are
deﬁned as 1 and 2, respectively.
for k-means clustering, we concatenate the rgb values and the geodesic dis-
tance value di truncated by d∗ to generate the feature vectors fi = (di, ri, gi, bi).
lv + lk.
532
s. nam et al.
center map loss.
to achieve instance-level predictions, we introduce a center
map module.
[0, 1]w ×h where
ˆc = 1 identiﬁes nuclei centers and ˆc = 0 for other pixels.
by placing the center map module
at the end of the model, the model is able to retain center point information along
the rus, so that each module can inherently reﬂect the information into their
predictions.
fig.
3. (a) input image (top) and ground truth (bottom), (b) instance map (top) and
center map (bottom) generated by the optimal nuclei center points, (c) those by shifted
points (6–8), and (d) those by reﬁned points.
inspired by [2], we deﬁne an oﬀset
vector o(x, y) that indicates the displacement of a point (x, y) to the center of
its corresponding nucleus.
2.2
reﬁnement via expectation maximization algorithm
training with nuclei (center) shifted point labels can lead to blurry center map
predictions (see fig.
to address
this, we propose an em based center point reﬁnement process.
in the e-step, we update the center of each nucleus according to ˆo.
we use
ˆo to generate reﬁned point labels p ′, since ˆo is reliable regardless of the point
location i.e., center of the nuclei or shifted.
p′
i = arg min
x,y
|

¯x,¯y∈vi
ˆb(x, y) × ˆo(x + ¯x, y + ¯y)|,
(5)
where vi is ith voronoi region and p′
i is the reﬁned center point.
in the m-step of iteration n, we generate c′ by adapting
the gaussian mask to p ′, and then use it to train oﬀset and center map modules.
(6)
since reliable ˆo is necessary to reﬁne nuclei centers, reﬁnement starts after 30
epochs.
e and m steps are alternately repeated to correct imprecise annotations
bringing them closer to the real nuclei center points.
for a fair
comparison, images were pre-processed before training/testing i.e., normalized
and cropped to 250×250 patches following the setting used in [17].
534
s. nam et al.
to make point labels, we use the center point of full mask annotations.
shift
indicates the number of pixels point annotations deviate from the nuclei center.
on cpm17, our method outperformed the prior approach by a large mar-
gin of +3.4% in dice and +7.2% in aji when the point label is located at the
nuclei center.
regarding reﬁnement, we observed that our strategy is more bene-
ﬁcial when points exhibit signiﬁcant shifts i.e., on both cpm and monuseg.
figure 3 showcases the eﬀectiveness of the reﬁnement process wherein the model
generates precise instance and center maps.
this demonstrates that our method separates adjacent nuclei accurately, and
maintains its robustness, achieving consistent performance even when the point
annotations are not located at the center of the nuclei.
additionally, in fig. 4, we
qualitatively show the results to highlight how our method precisely separates
adjacent nuclei.
pronet for nuclei instance segmentation
535
table 2. evaluation on the eﬀect of oﬀset and center maps.
on the other hand, since there was no reﬁnement pro-
cess due to the absence of the oﬀset map, inaccurate points extracted from the
center map are obtained in the real-world scenario.
these ﬁndings validate the utility of the center map
and oﬀset map modules i.e., they synergistically facilitate precise instance delin-
eation and nuclei boundary prediction.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_1.pdf:
however, previ-
ous pet enhancement methods rely heavily on the paired lpet and
spet data which are rare in clinic.
however, acquiring high-quality pet images requires
injecting a suﬃcient dose (standard dose) of radionuclides into the human body,
which poses unacceptable radiation hazards for pregnant women and infants
even following the as low as reasonably achievable (alara) principle
but these
supervised methods relied heavily on the paired lpet and spet data that are
rare in actual clinic due to radiation exposure and involuntary motions (e.g., res-
piratory and muscle relaxation).
however, these methods still require
lpet to train models, which contradicts with the fact that only spet scans
are conducted in clinic.
red boxes and arrows
show areas for detailed comparison.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf:
yet, acquiring large training datasets and
their corresponding labels, especially from a cohort of patients, can be costly
or even infeasible, which poses a signiﬁcant challenge in developing a dl model
with high performance [7].
this is due to sensitive privacy issues in patients’ data,
particularly in collaborative research, which restricts access to labels from diﬀer-
ent domains.
• our framework is eﬀective at preserving privacy, since it carries out da using
only pre-trained network parameters, without transferring any patient data.
hence, we achieve the condition for
the parameter ﬂuctuation that the centers of parameters of θs and θt should
be the same in the vector space, and the length of the ﬂuctuation vector should
544
k. lee et al.
be less than a certain small threshold (0 < r ≪ 1).
while the database also provides
labels for the detection task, we processed these labels as segmentation masks
using a region growing method [15].
as the
evaluation metrics, dice coeﬃcient (d. coef), prauc, which is an area under a
precision-recall curve, and cohen kappa (κ) were employed [30].
area
under the precision-recall curve (pr-auc) values were reported.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf:
such annotation is usually available based on the
diagnostic report of a patient, and do not require additional annotation
eﬀort from the physicians.
this is due to the low inter-class variance (a malignant region usually
occupies only a small portion of a us image), high intra-class variance
(due to the us sensor capturing a 2d slice of a 3d object leading to large
viewpoint variations), and low training data availability.
we posit that
even when we have only the image level label, still formulating the prob-
lem as object detection (with bounding box output) helps a deep neural
network (dnn) model focus on the relevant region of interest.
206–215, 2023.
https://doi.org/10.1007/978-3-031-43907-0_20
gall bladder cancer detection from us images
207
low cost, and accessibility make us a popular non-invasive diagnostic modality for
patients with suspected gall bladder (gb) aﬄictions.
all three images
have been scanned from the same patient, but due to the sensor’s scanning plane, the
appearances change drastically.
on the other hand, the image-level
malignancy label is usually available at a low cost, as it can be obtained readily
from the diagnostic report of a patient without additional eﬀort from clinicians.
the motivation is that, running a classiﬁer on a focused attention/ proposal
region in an object detection pipeline would help tackle the low inter-class and
high intra-class variations.
as transformers are increasingly outshining cnns due to their ability
to aggregate focused cues from a large area
inspired by the success of the multiple instance learning (mil) paradigm for
weakly supervised training on medical imaging tasks [20,22], we train a detec-
tion transformer, detr, using the mil paradigm for weakly supervised malignant
region detection.
in this, one generates region proposals for images, and then con-
siders the images as bags and region proposals as instances to solve the instance
classiﬁcation (object detection) under the mil constraints
[3] consisting of 1255 image samples from 218 patients.
the dataset contains 990 non-malignant (171 patients) and 265 malignant (47
patients) gb images (see fig.
we did the cross-validation splits at the patient level, and all
images of any patient appeared either in the train or validation split.
fig.
the location
information in the object queries learned by the class-agnostic detr ensures generation
of high-quality proposals.
since the patient information is not available with the data, we use random
stratiﬁed splitting for 5-fold cross-validation.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf:
first, dradio contains 2,799 ct-scans of patients in portal venous
phase with a radiological (weak) annotation, i.e. realized by a radiologist, indi-
cating four diﬀerent stages of cirrhosis: no cirrhosis, mild cirrhosis, moderate
cirrhosis and severe cirrhosis (yradio).
it contains 106 ct-scans from diﬀerent patients in portal venous
phase, with an identiﬁed histopathological status (metavir score) obtained
by a histological analysis, designated as y1
histo.
similarly
to the metavir score in d1
histo, we also binarize the ishak score, as proposed
in [16,20], which results in two cohorts of 34 healthy and 15 pathological patients.
in all datasets, we select the slices based on the liver segmentation of the
patients.
we ﬁrst sample n patients, where n
is the batch size, in a balanced way with respect to the radiological/histological
classes; namely, we roughly have the same number of subjects per class.
for d2
histo, which has fewer patients than the batch size, we
use a balanced sampling strategy with respect to the radiological/histological
classes with no obligation of one slice per patient in the batch.
as we work
with 2d slices rather than 3d volumes, we compute the average probability per
patient of having the pathology.
the evaluation results presented later are based
on the patient-level aggregated prediction.
first, we
can notice that our method outperforms all other pretraining methods in d1
histo
and d1+2
histo, which are the two datasets with more patients.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf:
speciﬁcally, we iteratively perform intra-slide
clustering for the regions (4096 × 4096 patches) within each wsi to yield
the prototypes and encourage the region representations to be closer
to the assigned prototypes.
our community has witnessed
the progress of the de facto representation learning paradigm from the super-
vised imagenet pre-training to self-supervised learning (ssl)
nevertheless, these works only consider learning the
representations at the patch level, i.e, the cellular organization, but neglecting
macro-scale morphological features, e.g., tissue phenotypes and intra-tumoral
heterogeneity.
[8], a milestone work,
introduces hierarchical pre-training (dino [6]) for the patch-level (256 × 256)
and region-level (4096 × 4096) in a two-stage manner, achieving superior per-
formance on slide-level tasks.
the aforementioned meth-
ods share the same two-stage pre-training paradigm, i.e., patch-to-region/slide.
however, they are essentially instance discrimination
where only the self-invariance of region/slide is considered, leaving the intra-
and inter-slide semantic structures unexplored.
in this paper, we propose to encode the intra- and inter-slide semantic struc-
tures by modeling the mutual-region/slide relations, which is called slpd: slide-
level prototypical distillation for wsis.
in order to learn this intra-slide semantic structure, we
encourage the region representations to be closer to the assigned prototypes.
slpd
261
(e) intra-slide disllaon 
(f) inter-slide disllaon 
(d) global (le) vs. slide-level clustering (right)
(a) hierarchical structure of wsi
(b) two-stage pretraining
probability 
distribuon 
prototype 
within slide 
prototypes 
across slides 
(c) slide-level prototypical disllaon 
wsi
region
patch
image
fig.
1. (a) a wsi possesses the hierarchical structure of wsi-region-patch-image, from
coarse to ﬁne.
(b) two-stage pre-training paradigm successively performs the image-to-
patch and patch-to-region aggregations.
besides self-distillation, region representa-
tions are associated with the prototypes within and across slides to comprehensively
understand wsis.
then, we learn the inter-slide
semantic structure by building correspondences between region representations
and cross-slide prototypes.
1(a), a wsi exhibits hierarchical structure at varying resolu-
tions under 20× magniﬁcation: 1) the 4096×4096 regions describing macro-scale
organizations of cells, 2) the 256 × 256 patches capturing local clusters of cells,
3) and the 16 × 16 images characterizing the ﬁne-grained features at the cell-
level.
then a region-level vision transformer vit4096-256 aggre-
gates these tokens to obtain region-level representations.
with this hierarchical
aggregation strategy, a wsi can be represented as a bag of region-level repre-
sentations, which are then aggregated with another vision transformer, vitwsi-
4096, to perform slide-level prediction tasks.
taking stage two as an
example, dino distills the knowledge from teacher to student by minimizing the
cross-entropy between the probability distributions of two views at region-level:
lself = ex∼pdh(gt(ˆz), gs(z)),
(1)
where h(a, b) = −a log b, and pd is the data distribution that all regions are
drawn from.
the teacher and the student share the same architecture consisting
of an encoder (e.g., vit) and a projection head gt/gs. ˆz and z are the embeddings
of two views at region-level yielded by the encoder.
2.3
slide-level clustering
many histopathologic features have been established based on the morpho-
logic phenotypes of the tumor, such as tumor invasion, anaplasia, necrosis and
mitoses, which are then used for cancer diagnosis, prognosis and the estimation
of response-to-treatment in patients [3,9]. to obtain meaningful representations
of slides, we aim to explore and maintain such histopathologic features in the
latent space.
clustering can reveal the representative patterns in the data and has
achieved success in the area of unsupervised representation learning [4,5,24,26].
to characterize the histopathologic features underlying the slides, a straight-
forward practice is the global clustering, i.e., clustering the region embeddings
from all the wsis, as shown in the left of fig.
however, the obtained
clustering centers, i.e., the prototypes, are inclined to represent the visual bias
slpd
263
related to staining or scanning procedure rather than medically relevant fea-
tures [33].
meanwhile, this clustering strategy ignores the hierarchical structure
“region→wsi→whole dataset” underlying the data, where the id of the wsi
can be served as an extra learning signal.
speciﬁcally, we conduct k-means algorithm before the
start of each epoch over ln region embeddings {zl
n}ln
l=1 of wn to obtain m pro-
totypes {cm
n ∈ rd}m
m=1.
each group of proto-
types is expected to encode the semantic structure (e.g., the combination of
histopathologic features) of the wsi.
2.4
intra-slide distillation
the self-distillation utilized by hipt in stage two encourages the correspondence
between two views of a region at the macro-scale because the organizations of
cells share mutual information spatially.
however, the self-distillation, which
solely mines the spatial correspondences inside the 4096 × 4096 region, cannot
comprehensively understand the histopathologic consistency at the slide-level.
2.3, a
slide can be abstracted by a group of prototypes, which capture the semantic
structure of the wsi.
thus this distillation objective is encoding such informa-
tion into the corresponding region embedding, which makes the learning process
semantic structure-aware at the slide-level.
2.5
inter-slide distillation
tumors of diﬀerent patients can exhibit morphological similarities in some
respects
speciﬁcally, for a region embedding z belonging to the slide w and
assigned to the prototype c, we ﬁrst search the top-k nearest neighbors of w in
the dataset based on the semantic similarity, denoted as { ˆwk}k
k=1.
(4)
the inter-slide distillation can encode the sldie-level information complementary
to that of intra-slide distillation into the region embeddings.
we
leverage the pre-trained vit256-16 in stage one provided by hipt to tokenize
the patches within each region.
we use the pre-trained vit256-16 and vit4096-256 to extract
embeddings at the patch-level (256 × 256) and the region-level (4096 × 4096)
for downstream tasks.
we adopt the 10-fold cross validated accuracy (acc.)
and area under the curve (auc) to evaluate the weakly-supervised classiﬁcation
performance.
[28]
region-level
slpd
0.856±0.025
0.926±0.017
0.879±0.035
0.863±0.076
3
patch-level
dino
0.825±0.054
0.905±0.059
0.847±0.032
0.848±0.075
4
ds-mil
[22]
region-level
dino
0.841±0.036
0.917±0.035
0.854±0.032
0.848±0.075
5
region-level
slpd
0.858±0.040
0.938±0.026
0.854±0.039
0.876±0.050
6
region-level
dino
0.843±0.044
0.926±0.032
0.849±0.037
0.854±0.069
7
region-level
dino+lintra
0.850±0.042
0.931±0.041
0.866±0.030
0.881±0.069
8
vitwsi-4096 [8]
region-level
dino+linter
0.850±0.043
0.938±0.028
0.860±0.030
0.874±0.059
9
region-level
slpd
0.864±0.042
0.939±0.022
0.869±0.039
0.886±0.057
k-nearest neighbors (knn) evaluation
10
mean
region-level
dino
0.770±0.031
0.840±0.038
0.837±0.014
0.724±0.055
11
region-level
dino+lintra
0.776±0.039
0.850±0.023
0.841±0.012
0.731±0.064
12
region-level
dino+linter
0.782±0.027
0.854±0.025
0.845±0.014
0.738±0.080
13
region-level
slpd
0.792±0.035
0.863±0.024
0.849±0.014
0.751±0.079
3.1
weakly-supervised classiﬁcation
we conduct experiments on two slide-level classiﬁcation tasks, nsclc subtyp-
ing and brca subtyping, and report the results in table 1.
the region-level
embeddings generated by slpd outperform the patch-level embeddings across
two aggregators3 and two tasks (#1∼ 5).
vitwsi-4096 is the aggregator with region-level
embeddings.
#
ablation
method
nsclc
brca
acc.
auc
acc.
auc
1
diﬀerent cluster-
ing methods
global
0.848±0.045
0.925±0.033
0.842±0.048
0.863±0.060
2
slide-level
0.850±0.042
0.931±0.041
0.866±0.030
0.881±0.069
3
diﬀerent inter-
slide distillations
region
0.828±0.040
0.915±0.025
0.843±0.024
0.849±0.067
4
prototype
0.850±0.043
0.938±0.028
0.860±0.030
0.874±0.059
5
number of
prototypes
m = 2
0.859±0.036
0.936±0.021
0.869±0.039
0.886±0.057
6
m = 3
0.864±0.035
0.938±0.022
0.861±0.056
0.878±0.069
7
m = 4
0.864±0.042
0.939±0.022
0.860±0.031
0.872±0.060
8
number of
slide neighbors
k
the proposed inter-slide distillation is
semantic structure-aware at the slide-level, since we build the correspondence
between the region embedding and the matched prototype (#4 in table 2).
to
verify the necessity of this distillation method, we turn to another design where
the inter-slide correspondence is explored through two nearest region embeddings
across slides (#3 in table 2).
as can be seen, the region-level correspondences
lead to inferior performances, even worse than the baseline (#5 in table 1),
because the learning process is not guided by the slide-level information.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf:
on the one hand, annotat-
ing wsis requires strong medical expertise, is expensive, time-consuming, and
labels are usually provided at the slide or patient level.
through knowledge distillation, we encour-
250
g. bontempo et al.
age agreement across the predictions delivered at diﬀerent resolutions, while indi-
vidual scale features are learned in isolation to preserve the diversity in terms
of information content.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf:
medical education is essential for providing the best patient
care in medicine, but creating educational materials using real-world
data poses many challenges.
therefore, medical image editing, which allows users to
create their intended disease characteristics, can be useful for educa-
tion.
evaluation by ﬁve expert physicians demon-
strated that the edited images appeared natural as medical images and
that the disease characteristics were accurately reproduced.
(a) users can edit the segmentation map
obtained from an input image to express intended ﬁne-grained disease characteristics.
for example, although small but signif-
icant disease characteristics (e.g., depth of cancer invasion) can sometimes alter
diagnosis and treatment, collecting pairs with and without these characteristics
is cumbersome.
therefore, medical image editing that allows users to generate their
intended disease characteristics is useful for precise medical education
our goal
is to develop high-precision medical image editing according to the ﬁne-grained
characteristics of individual diseases, rather than at the level of disease cate-
gories.
for example, even if two diseases belong to the same disease category
of “lung tumor,” the impression of benign or malignant will diﬀer depending on
ﬁne-grained characteristics, such as whether the margins are “smooth” or “spic-
ulated.”
these ﬁne-grained characteristics consist of low- to mid-level image
medical image editing
405
features to distinguish the substructures of organs and diseases, which we call
anatomical elements.
additionally, accurately modeling
certain ﬁne-grained characteristics, such as the textual variations of disease, can
be a daunting task.
both were
in-house datasets collected from a single hospital.
subsequently,
segmentation maps from the testing images were edited to generate images with
the intended characteristics (see fig.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf:
our classiﬁer is pretrained on
2,668 scans from a public dataset and 1,149 subjects with longitudinal
chest cts, billing codes, medications, and laboratory tests from ehrs
of our home institution.
in this work, we jointly learn from longitudinal medical imag-
ing, demographics, billing codes, medications, and lab values to classify spns.
modalities
counts (cases/controls)
demo img code med lab subjects scans
ehr-pulmonary
–
288,428
–
nlst
–
–
–
533/801
1066/1602
image-ehr
257/665
641/1624
image-ehr-spn
58/169
76/405
demo: demographics, img: chest cts, code:
this study used an imaging-only cohort from the nlst [28] and
three multimodal cohorts from our home institution with irb approval (table 1).
we randomly
sampled from the control group to obtain a 4:6 case control ratio.
we searched all records in our ehr archives for patients
who had billing codes from a broad set of pulmonary conditions, intending to
capture pulmonary conditions beyond just malignancy.
we searched our
institution’s imaging archive for patients with three chest cts within ﬁve years.
we did not compare against
these contextual embeddings because none have been publically released, but
integrating these with longitudinal imaging is an area of future investigation.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_9.pdf:
region-based active learn-
ing (al) involves training the model on a limited number of annotated
image regions instead of requesting annotations of the entire images.
these annotation regions are iteratively selected, with the goal of opti-
mizing model performance while minimizing the annotated area.
the
standard method for region selection evaluates the informativeness of
all square regions of a speciﬁed size and then selects a speciﬁc quan-
tity of the most informative regions.
we ﬁnd that the eﬃciency of this
method highly depends on the choice of al step size (i.e., the combina-
tion of region size and the number of selected regions per wsi), and a
suboptimal al step size can result in redundant annotation requests or
inﬂated computation costs.
speciﬁcally, we dynamically determine each region
by ﬁrst identifying an informative area and then detecting its optimal
bounding box, as opposed to selecting regions of a uniform predeﬁned
shape and size as in the standard method.
with only 2.6% of tissue area annotated, we achieve full annota-
tion performance and thereby substantially reduce the costs of annotat-
ing a wsi dataset.
keywords: active learning · region selection · whole slide images
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 9.
90–100, 2023.
https://doi.org/10.1007/978-3-031-43895-0_9
adaptive region selection for al in wsi semantic segmentation
91
1
introduction
semantic segmentation on histological whole slide images (wsis) allows precise
detection of tumor boundaries, thereby facilitating the assessment of metas-
tases
[18]. identifying potentially informative image regions (i.e., providing
useful information for model training) allows requesting the minimum amount
of annotations for model optimization, and a decrease in annotated area reduces
both localization and delineation workloads.
the challenge is to eﬀectively select
annotation regions in order to achieve full annotation performance with the least
annotated area, resulting in high sampling eﬃciency.
we use region-based active learning (al) [13] to progressively identify anno-
tation regions, based on iteratively updated segmentation models.
each region
selection process consists of two steps.
the enhancement of priority maps, such as highlighting easy-to-label pixels [13],
edge pixels [6] or pixels with a low estimated segmentation quality [2], is also
a popular area of research.
second, on the priority map, regions are selected
according to a region selection method.
prior works have rarely looked into
region selection methods; the majority followed the standard approach [13] where
a sliding window divides the priority map into ﬁxed-sized square regions, the
selection priority of each region is calculated as the cumulative informativeness
of its constituent pixels, and a number of regions with the highest priorities are
then selected.
all of these works selected square regions of a manually predeﬁned
size, disregarding the actual shape and size of informative areas.
this work focuses on region selection methods, a topic that has been largely
neglected in literature until now, but which we show to have a great impact on al
sampling eﬃciency (i.e., the annotated area required to reach the full annotation
performance).
we discover that the sampling eﬃciency of the aforementioned
standard method decreases as the al step size (i.e., the annotated area at each
al cycle, determined by the multiplication of the region size and the number of
selected regions per wsi) increases.
to avoid extensive al step size tuning, we
propose an adaptive region selection method with reduced reliance on this al
hyperparameter.
1. region-based al workﬂow for selecting annotation regions.
(image resolution: 0.25 µm
px )
region by ﬁrst identifying an informative area with connected component detec-
tion and then detecting its bounding box.
2
method
2.1
region-based active learning for wsi annotation
we are given an unlabeled pool u = {x1 . . .
we denote the
jth annotated rectangular region in xi as rij = (cij
x , cij
y , wij, hij), where (cij
x , cij
y )
are the center coordinates of the region and wij, hij are the width and height of
that region, respectively.
in the standard region selection method, where ﬁxed-
size square regions are selected, wij = hij = l, ∀i, j, where l is predeﬁned.
figure 1 illustrates the workﬂow of region-based al for wsi annotation.
second, k regions are selected based on mi
using a region selection method.
the informativeness measure is not the focus
adaptive region selection for al in wsi semantic segmentation
93
of this study, we therefore adopt the most commonly used one that quantiﬁes
model uncertainty (details in sect.
next we describe the four region selection
methods evaluated in this work.
2.2
region selection methods
random.
each region contains at least 10% of tissue and does not overlap with
other regions.
the selection priority of
each region is calculated as the summed priority of the constituent pixels, and
k regions with the highest priorities are then selected.
standard (non-square) we
implement a generalized version of the standard method that allows non-square
region selections by including multiple region candidates centered at each pixel
with various aspect ratios.
speciﬁcally, we deﬁne a variable region width
w as spanning from 1
2l to l with a stride of 256 pixels and determine the corre-
sponding region height as h = l2
w .
the k regions are selected sequentially; when selecting the
jth region rij in xi, we ﬁrst set the priorities of all pixels in previously selected
regions (if any) to zero.
note that standard (non-square)
can be understood as an ablation study of the proposed method adaptive to
examine the eﬀect of variable region shape by maintaining constant region size.
2.3
wsi semantic segmentation framework
this section describes the breast cancer metastases segmentation task we use
for evaluating the al region selection methods.
following [11], a
patch is labeled as positive if the center 128 × 128 pixels area contains at least
94
j. qiu et al.
fig.
2.
standard
(non-
square):
region
candi-
dates for l = 8192 pixels.
fig.
in each training epoch, 20 patches
per wsi are extracted at random positions within the annotated area; for wsis
containing annotated metastases, positive and negative patches are extracted
with equal probability.
adaptive region selection for al in wsi semantic segmentation
95
fully-connected layers with sizes of 512 and 2, followed by a softmax activation
layer.
since the camelyon16 dataset is fully annotated,
we perform al by assuming all wsis are unannotated and revealing the anno-
tation of a region only after it is selected during the al procedure.
we use the camelyon16 challenge metric free response oper-
ating characteristic (froc) score
4. miou (tumor) as a function of annotated tissue area (%) for four region selec-
tion methods across various al step sizes.
the ﬁnal annotated
tissue area of random can be less than standard as it stops sampling a wsi if no region
contains more than 10% of tissue.
curves of adaptive are interpolated as the annotated
area diﬀers between repetitions.
comparison of region selection methods.
figure 4 compares the sampling
eﬃciency of the four region selection methods across various al step sizes (i.e.,
the combinations of region size l ∈ {4096, 8192, 12288} pixels and the number
of selected regions per wsi k ∈ {1, 3, 5}).
when using region selection method standard, the sampling eﬃciency advan-
tage of uncertainty sampling over random sampling decreases as al step size
increases.
a small al step size minimizes the annotated tissue area for a certain
high level of model performance, such as an miou (tumor) of 0.7, yet requires a
large number of al cycles to achieve full annotation performance (fig. 4 (a–d)),
adaptive region selection for al in wsi semantic segmentation
97
table 1.
annotated tissue area (%) required to achieve full annotation performance.
5. visualization of ﬁve regions selected with three region selection methods,
applied to an exemplary priority map produced in a second al cycle (regions were
randomly selected in the ﬁrst al cycle, k = 5, l = 4096 pixels).
region sizes increase
from top to bottom: l ∈ {4096, 8192, 12288} pixels.
a large al step size allows for full anno-
tation performance to be achieved in a small number of al cycles, but at the
expense of rapidly expanding the annotated tissue area (fig.
table 1 shows that adaptive achieves full annotation performance with fewer
al cycles than standard for small al step sizes and less annotated tissue area
for large al step sizes.
as a result, when region selection method adaptive is
used, uncertainty sampling consistently outperforms random sampling.
4(e–i)) shows that adaptive eﬀectively prevents the rapid expansion of
annotated tissue area as al step size increases, demonstrating greater robustness
to al step size choices than standard.
additionally, we visualize examples of selected regions
in fig. 5 and show that adaptive avoids two region selection issues of standard:
small, isolated informative areas are missed, and irrelevant pixels are selected
due to the region shape and size restrictions.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf:
[1]. another group
of studies employ conditional generative adversarial networks (cgans)
this large-scale pre-training enables the model to learn
756
j. ye et al.
common yet diverse image characteristics and generate realistic medical images.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf:
using two medical benchmark datasets for melanoma detec-
tion and bone age estimation, we apply our r2r framework to vgg,
resnet and eﬃcientnet architectures and thereby reveal and correct
real dataset-intrinsic artifacts, as well as synthetic variants in a con-
trolled setting.
we demonstrate the applicability and high
automation of r2r on two medical tasks, including melanoma detection and bone
age estimation, using the vgg-16, resnet-18 and eﬃcientnet-b0 dnn architec-
tures.
1
(bottom center).
shown are band-aid, ruler,
skin marker, and synthetic artifacts for the isic dataset, as well as “l”-marker and
synthetic artifacts for the bone age dataset.
4.1
experimental setup
we train vgg-16
[26], resnet-18 [11] and eﬃcientnet-b0 [28] models on the
isic 2019 dataset [7,8,30] for skin lesion classiﬁcation and pediatric bone age
dataset
[10] for bone age estimation based on hand radiographs.
besides the synthetic clever hans for bone age classiﬁcation, we
encountered the use of “l” markings, resulting from physical lead markers placed
by radiologist to specify the anatomical side.
interestingly, the “l” markings are
larger for hands of younger children, as all hands are scaled to similar size [10],
oﬀering the model to learn a shortcut by estimating the bone age based on the
relative size of the “l” markings, instead of valid features.
as shown in tab. 1 (isic
2019) and appendix a.2 (bone age), we are generally able to improve model
behavior with all methods.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf:
compared to the explainable baseline model,
our method achieves more than 6 % higher accuracy in predicting both
malignancy (93.0 %) and mean characteristic features of lung nodules.
in
addition to the malignancy of the lung nodules, which is the main goal of the
prediction task, the radiologists also marked certain nodule characteristics such
as sphericity, margin or spiculation.
this idea can be used for region-wise
prototypical samples
the latent vectors of all capsules are being accumulated for a dense layer
to predict a target score and for a decoder network to reconstruct the region of interest.
for this, a cluster
cost reduces the euclidean distance of a sample’s capsule vector oa to the nearest
prototype vector pj of group pas which is dedicated to its correct attribute score.
the proposed approach is evaluated using the publicly available lidc-
idri dataset consisting of 1018 clinical thoracic ct scans from patients with
non-small cell lung cancer (nsclc)
nodules were also scored according to their characteristics with respect to pre-
deﬁned attributes, namely subtlety (diﬃculty of detection, 1-extremely subtle,
5-obvious), internal structure (1-soft tissue, 4-air), pattern of calciﬁcation (1-
popcorn, 6-absent), sphericity (1-linear, 5-round), margin (1-poorly deﬁned, 5-
sharp), lobulation (1-no lobulation, 5-marked lobulation), spiculation (1-no spic-
ulation, 5-marked spiculation), and texture (1-non-solid, 5-solid).
another area of research would be to explore other types of
privileged information that require less extra annotation eﬀort, such as medi-
cal reports, to train the attribute capsules.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf:

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_46.pdf:
[22] aggregates the features of a
group of key sampling points near it.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf:
we apply the k-center method from
coreset
although there are publicly available
liver tumor datasets [1,24], they only contain major tumor types and diﬀer in
image characteristics and label distribution from our hospital’s data.
deploying
a model trained from public data to our hospital directly will be problematic.
collecting large-scale data from our hospital and training a new model will be
expensive.
therefore, we can use the model trained from them as a starting
point and use slpt to adapt it to our hospital with minimum cost.
we col-
lected a dataset from our in-house hospital comprising 941 ct scans with eight
categories: hepatocellular carcinoma, cholangioma, metastasis, hepatoblastoma,
hemangioma, focal nodular hyperplasia, cyst, and others.
evaluation of diﬀerent tunings on the lesion segmentation with limited data
(40 class-balanced patients).
the
labeling budget for each step is 20 patients.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf:
however, the current
pathology workﬂow is sub-optimal and low-throughput since it is, by and large, manu-
ally conducted, and the large volume of workloads can result in dysfunction or errors in
cancer grading, which have an adversarial effect on patient care and safety [2].

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf:
we adopt ﬁve metrics for evaluation: accuracy (acc), average precision (pre),
average sensitivity (sen), macro f1-score (f1) and macro area under curve
(auc).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf:
in hospitals, collected multi-phase cts are normally grouped by
patients rather than lesions, which makes single-phase lesion annotation insuﬃ-
cient for feature fusion learning.
however, the number of lesions inside a single
patient can vary from one to dozens and they can be of diﬀerent types in realis-
tic cases.
we design a pre-processing unit to reduce the
annotation cost, where we obtain lesion area on multi-phase cts from annota-
tions marked on a single phase.
the overall architecture of the proposed transliver model.
2.1
pre-processing unit
the single-phase annotated lesion has the position and class labels in all phases
but they are not aligned, so we could have diﬃculty ﬁnding out which lesions in
diﬀerent phases are the same with 2 or more lesions in one patient.
to reduce
errors caused by unregistered data and address the situation that one patient
has multiple lesions of diﬀerent types, we pre-process the multi-phase liver cts
registered and grouped by lesions.
we
choose an atlas phase art as suggested by clinicians and other phases of cts
are registered to the art phase of every patient.
[4], but they are also prone to overﬁt on small datasets such as
private hospital datasets.
the employed single-phase annotated dataset is collected from sir run
run shaw hospital (srrsh), aﬃliated with the zhejiang university school of
medicine, and has received the ethics approval of irb.
after the pre-processing unit with window
dice threshold of 0.3, we screen 761 lesions from 444 patients with four phases
hybrid transformer model for multi-phase liver lesion classiﬁcation
335
of cts, seven types of lesions (13.2% of hcc, 5.3% of hm, 11.3% of icc, 22.6%
of hh, 31.1% of hc, 8.7% of fnh, and 7.8% of ha), and totally 4820 slices.
lesions from the same patient are either
assigned to the training and validation set or the test set, but not both.
implementations.
we
measured performance by precision (pre.), sensitivity (sen.), speciﬁcity (spe.),
f1-score (f1), area under the curve (auc), and accuracy (acc.).
results.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf:
(2) large-scale colonoscopy generation: the proposed
approach can be used to generate large-scale datasets with no/arbitrary anno-
tations, which signiﬁcantly beneﬁts the medical image society, laying the foun-
dation for large-scale pre-training models in automatic colonoscopy analysis.
this would lead to the model
generating more background-like polyps since the large background region will
easily overwhelm the small foreground polyp regions during training.
h × w ,
(7)
where p = 1 means the pixel p at (h, w) belongs to the polyp region and p = 0
means it belongs to the background region.
methods endoscene clinicdb
kvasir
colondb
etis
overall
ap
f1
ap
f1
ap
f1
ap
f1
ap
f1
ap
f1
center
86.9
91.4 84.7
89.2
75.6
81.4
62.2
72.3
62.7
70.1
56.6
76.0
+ldm
84.1
84.4
90.4 89.9
81.3
81.8
73.4
74.5
65.2
71.7
62.0
76.9
+sdm
87.8 86.9
88.7
91.0 77.0
82.8
71.8
78.1
68.2
72.6
61.8
79.1
+ours
85.0
89.1
86.1
90.8
77.3 84.7 74.2 80.2 68.7 75.6 65.7 81.3
sparse
89.9
87.8
81.4
86.4
75.6
80.2
78.2
73.2
63.8
62.4
63.7
73.2
+ldm
87.4
76.3
95.0 93.5 81.5
58.8
80.0
71.0
64.4
54.3
65.3
66.3
+sdm
94.5 90.5 88.7
86.5
79.0
80.5
81.4 76.8
67.8
67.1
65.2
76.7
+ours
92.8
86.2
92.2
90.6
81.6 82.3 80.1
79.8 72.4 70.4 66.4 79.0
deform
98.1 94.4 89.7
89.9
80.2 74.4
82.2 75.5
65.3
54.7
64.5
71.8
+ldm
94.6
90.5
91.6
89.5
79.3
73.4
78.0
73.2
69.0
64.0
63.4
73.3
+sdm
96.0
90.6
90.3
91.2
82.2
78.9
80.1
75.1
67.5
66.7
65.1
75.8

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf:
we evaluate
the eﬀectiveness of all methods in terms of four evaluation metrics, i.e., classiﬁ-
cation accuracy (acc), speciﬁcity (spe), sensitivity (sen) and area under the
roc curve (auc).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf:
here, the cross-transformer
is based on multi-headed cross-attention mechanism that densely aggregates rel-
evant input image features, based on pairwise attention scores between each posi-
tion in the base tissue image with every region of the reference tissue image.
4.3
human evaluation study
we conducted a study with a group of ten pathologists having an average subject
experience of 8.5 years.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf:

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf:
keywords: failure detection · distribution shifts · benchmark
1
introduction
although machine learning-based classiﬁcation systems have achieved signiﬁcant
breakthroughs in various research and practical areas, their clinical application is
still lacking.
4.2.
abbreviations: b: benign, m: malignant, pred.: prediction, gt: ground truth, conﬁd.:
conﬁdence, source: source domain, target: target domain.
extensively studied in the computer vision community with a variety of recent
benchmarks covering nuanced realistic distribution shifts [13,15,19,27], and is
also studied in isolated cases in the biomedical community [2,3,33].
[18,25].
402
t. j. bungert et al.
we argue, that silent failures, which occur when test cases break both the
classiﬁer and the csf, are a signiﬁcant bottleneck in the clinical translation of
ml systems and require further attention in the medical community.
our study provides valuable insights and the
underlying framework is made openly available to catalyze future research in the
community.
we emulate two acquisition shifts by deﬁning either images
from the memorial sloan kettering cancer center (mskcc) or hospital clinic
barcelona (hcb) as the target domain and the remaining images as the source
silent failures in medical image classiﬁcation
403
domain.
nine clusters are
identiﬁed per concept and the resulting plots show the closest-to-center image
per cluster as a visual representation of the concept.
the area
404
t. j. bungert et al.
under the risk-coverage curve aurc reﬂects this task, since it considers both the
classiﬁer’s accuracy as well as the csf’s ability to detect failures by assigning
low conﬁdence scores.
similarly,
“acq”/“man” denote averages over all acquisition/manifestation shifts per dataset.
results with further metrics are
reported in appendix table 2
dataset
chest x-ray
dermoscopy
fc-microscopy
lung nodule ct
study
iid
cor
acq
iid
cor
acq
man
iid
cor
acq
iid
cor
man
msr
15.3
when looking beyond the averages displayed in table 1 and ana-
lyzing the results of individual clinical centers, corruptions and manifestation
shifts, one remarkable pattern can be observed: in various cases, the same csf
showed opposing behavior between two variants of the same shift on the same
dataset.
figure 1c provides a concept
cluster plot that visually conﬁrms how some of these lesions (purple dot) share
characteristics of the benign cluster of the source domain (turquoise dot), such
silent failures in medical image classiﬁcation
407
as being smaller, brighter, and rounder compared to malignant source-lesions
(blue dot).
figure 1b (right) further provides insights about the general behavior
of the csf: silent failures occur for both classes and are either located at the
cluster border (i.e. decision boundary), deeper inside the opposing cluster center
(severe class confusions), or represent outliers.
most silent failures occur at the
boundary, where the csf should reﬂect class ambiguities by low scores, hint-
ing at general misbehavior or overconﬁdence in this area.
on the lung nodule
ct data (fig. 2a), we see how the classiﬁer and csf break down when a malig-
nant sample (typically: small bright, round) exhibits characteristics typical to
benign lesions (larger, less cohesive contour, darker) and vice versa.
this pat-
tern of contrary class characteristics is also observed on the dermoscopy dataset
(2c).
figure 2h shows a classi-
ﬁcation failure from a target clinical center together with the model’s conﬁdence
as measured by msr and dg.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf:
the medical imaging community generates a wealth of data-
sets, many of which are openly accessible and annotated for speciﬁc
diseases and tasks such as multi-organ or lesion segmentation.
our ﬁndings oﬀer
a new direction for the medical imaging community to eﬀectively uti-
lize the wealth of available data for improved segmentation performance.
this results in a situation where a zoo
of partially labeled datasets is available to the community.
as liu et al. pointed out, one common caveat is that
many methods force the resulting model to average between distinct annota-
tion protocol characteristics [22] by combining labels from diﬀerent datasets for
the same target structure (visualized in fig.
2) it retains diﬀerent annotation protocol characteristics for
the same target structure and 3) allows for overlapping target structures with
diﬀerent level of detail such as liver, liver vessel and liver tumor.
furthermore, we introduce a training
schedule and dataset preprocessing which balances varying dataset size and class
characteristics.
2.1
problem deﬁnition
we begin with a dataset collection of k datasets d(k), k ∈
even if classes from diﬀerent datasets
refer to the same target structure we consider them as unique, since the exact
annotation protocols and labeling characteristics of the annotations are unknown
and can vary between datasets: c(k) ∩ c(j) = ∅, ∀k ̸= j. this implies that the
network must be capable of predicting multiple classes for one voxel to account
for the inconsistent class deﬁnitions.
2.2
multitalent
network modiﬁcations.
this regularizes the
loss if only a few voxels of one class are present in one image and a larger area
is present in another image of the same batch.
the ﬁrst group
includes all “diﬃcult” classes for which the default nnu-net has a dice smaller
than 75 (labeled by a “d” in table 4 in the appendix).
the second group includes
all cancer classes because of their clinical relevance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf:
annotators
can make additional clicks in missegmented areas to iteratively reﬁne the seg-
mentation mask, which often signiﬁcantly improves the prediction compared to
non-interactive models [4,13].
this leads to a
more precise guidance with a smaller radius σi near edges and a larger radius
in homogeneous areas such as clicks in the center of the object of interest.
we discard the 513 tumor-free patients, leaving us with 501 volumes.
using gdt as a proxy signal to adapt
σi for each click ci mitigates this weakness by imposing large σi in homogeneous
areas and small, precise σi near edges (fig. 1a)).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf:
however,
the lack of a well-established and large-scale ultrasound video dataset
with high-quality annotations has posed a persistent challenge for the
research community.
furthermore, we propose a novel frequency and localization fea-
ture aggregation network (fla-net) that learns temporal features from
the frequency domain and predicts additional lesion location positions to
assist with breast lesion segmentation.
we also devise a localization-based
contrastive loss to reduce the lesion location distance between neighbor-
ing video frames within the same video and enlarge the location distances
between frames from diﬀerent ultrasound videos.
our fla-net learns
frequency-based temporal features and then uses them to predict auxiliary breast
lesion location maps to assist the segmentation of breast lesions in video frames.
additionally, we devise a contrastive loss to enhance the breast lesion location
shifting more attention to breast lesion segmentation in ultrasound videos
499
fig.
moreover, we devise
a location-aware contrastive loss (see lcontrastive) to reduce location distance of frames
from the same video and enlarge the location distance of diﬀerent video frames.
moreover, we devise a location-based contrastive loss to regularize the breast
lesion locations of inter-video frames and intra-video frames.
speciﬁcally, we ﬁrst
group three features into two groups ({xt, xt−1} and {xt, xt−2}) and develop
a channel attention function ca(·) to obtain two attention maps.
then, we element-wise multiply the obtained atten-
tion map from each group with the input features, and the multiplication results
(see y1 and y2) are then transformed into complex numbers by splitting them
into real and imaginary parts along the channel dimension.
math-
ematically, ot is computed by ot = bconv(z1 + z2), where “bconv” contains a
3 × 3 convolution layer, a group normalization, and a relu activation function.
3.2
two-branch decoder
after obtaining the frequency features, we introduce a two-branch decoder con-
sisting of a segmentation branch and a localization branch to incorporate tem-
poral features from nearby frames into the current frame.
location ground truth.
instead of formulating it as a regression problem,
we adopt a likelihood heatmap-based approach to encode the location of breast
lesions, since it is more robust to occlusion and motion blur.
to do so, we compute
a bounding box of the annotated breast lesion segmentation result, and then take
the center coordinates of the bounding box.
after that, we apply a gaussian
kernel with a standard deviation of 5 on the center coordinates to generate a
heatmap, which is taken as the ground truth of the breast lesion localization.
3.3
location-based contrastive loss
note that the breast lesion locations of neighboring ultrasound video frames are
close, while the breast lesion location distance is large for diﬀerent ultrasound
502
j. lin et al.
table 2.
[16]
video
0.762
0.659
0.799
0.037
our fla-net
video
0.789
0.687
0.815
0.033
videos, which are often obtained from diﬀerent patients.
motivated by this, we
further devise a location-based contrastive loss to make the breast lesion loca-
tions at the same video to be close, while pushing the lesion locations of frames
from diﬀerent videos away.
hence, we devise a location-based
contrastive loss based on a triplet loss [15], and the deﬁnition is given by:
lcontrastive = max(mse(ht, ht−1) − mse(ht, nt)
moreover, our
method has larger dice, jaccard, f1-score results and a smaller mae result than
“basic+fla+lb”, which shows that our location-based contrastive loss has its
contribution to the success of our video breast lesion segmentation method.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf:
automated segmen-
tation is crucial, but it is also challenging due to three factors: the infected regions
often vary in shape, size, and location, appear similar to surrounding tissues, and
can disperse within the lung cavity.
we advocate using chest x-ray datasets such as chestx-
ray and chestxr may beneﬁt covid-19 segmentation using ct scans because
of three reasons: (1) supplement limited ct data and contribute to training
a more accurate segmentation model; (2) provide large-scale chest x-rays with
labeled features, including pneumonia, thus can help the segmentation model
to recognize patterns and features speciﬁc to covid-19 infections; and (3) help
improve the generalization of the segmentation model by enabling it to learn from
diﬀerent populations and imaging facilities.
it is diﬃcult to directly learn cross-modal depen-
dencies using the features obtained by the encoder because ct and x-ray data
were collected from diﬀerent patients.
all ct images were
acquired without intravenous contrast enhancement from patients with posi-
tive reverse transcription polymerase chain reaction (rt-pcr) for sars-
cov-2.
the chestx-
ray14 dataset comprises 112,120 x-ray images showing positive cases from 30,805
patients, encompassing 14 disease image labels pertaining to thoracic and lung
ailments.
we follow the extension of
[20] for weight initialization and use the adamw optimizer [11] and empirically
set the initial learning rate to 0.0001, batch size to 2 and 32 for segmentation
and classiﬁcation, maximum iterations to 25w, momentum factor λ to 0.99, and
the number of prototypes k to 256.
to evaluate the covid-19 segmentation performance, we utilized six met-
rics, including the dice similarity coeﬃcient (dsc), intersection over union
(iou), sensitivity (sen), speciﬁcity (spe), hausdorﬀ distance (hd), and aver-
age surface distance (asd).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf:
our
laplacian-former shows ﬁner boundaries (high-frequency details) for the region of the
stomach and less false positive prediction for the pancreas.
speciﬁcally, our frequency attention
emphasizes the ﬁne details and texture characteristics that are indicative of skin
lesion structures and ampliﬁes regions with signiﬁcant intensity variations, thus
accentuating the texture patterns present in the image and resulting in better
performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf:
the main
reason is that the lesion scale in the two public datasets are relatively small,
which matches the fact few patients have very large nodule or mass.
in this paper, we propose a scale-aware test-time click adaptation (sattca)
method, which simply utilizes easily obtainable lesion click (i.e., the center
detected nodule) to adjust the parameters of the network normalization lay-
ers
typically, a neural network with weighted parameters θ
is trained to predict the lesion area ˆs = θ(i), with the goal of minimizing the
loss function l(s, ˆs).
for each roi input, the center point c of the lesion, which is represented
as pc = ( d
2 , h
2 , w
2 ) in cartesian coordinate system, can be used as a reference
point to assist the network in improving segmentation performance.
then we generate an ellipsoid mask mi
(around the center ci of the detected nodule) whose size is proportional to the size
of b to supervise the parameter updating during test-time adaptation.
to achieve this, we employ a clipping strategy to adjust the proportion of fore-
ground and background in the input image, producing a group of input images
with dimensions of 64×96×96, 32×48×48, and 16×24×24.
due to diﬀerences in the statistical distribution of pulmonary
nodule scale in image data from diﬀerent medical centers, the segmentation
results of some images, especially for large nodules, are worse than expected.
then we make a projection on the main connected region of ˆsi along three
test-time click adaptation for pulmonary lesion segmentation
685
coordinate axes to obtain the size of the bounding box bi = (d, w, h) of the
pre-segmentation result, and generate an ellipsoid mi with three axes length
proportional to the corresponding side length of the bounding box bi.
firstly, we present the
quantitative comparison in table 2, where we group the nodules and masses in
each dataset at 10 mm intervals and calculate the average segmentation perfor-
mance diﬀerences of the nodules in each scale group.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_72.pdf:

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf:
focal region size sl
r is the number of sub-windows horizontally and vertically in
attended regions at level l. the focal sa module proceeds in two main steps, sub-
window pooling and attention computation.
subsequently, each
ct scan is cropped to a 128 × 128 × 64 voxel patch around the prostate area,
which is used as input for 3d models.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf:
this is a concern as label noise has been shown to decrease the accuracy of
supervised models [20,22,35], making it a key area of focus for both research
and practical applications.
existing research in this area
has focused on the development of noise-resistant network architectures [15], the
incorporation of domain-speciﬁc prior knowledge [29], or more recent strategies
that update the noisy masks before memorization
the images are annotated with lesion
type, diagnosis, and anatomical location metadata.
the morphological transfor-
mations included erosion, dilation, and aﬃne transformations, which respectively
reduced, enlarged, and displaced the annotated area.
3.2
setup
we train a nnu–net [12] as a segmentation network from scratch.
center: sensitivity of ˜ν to initialization for α = 0.7,
β

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf:
in clinics, couinaud segments obtained from
manual annotation are tedious and time-consuming, based on the vasculature
used as rough guide (fig. 1).
speciﬁcally, given a 3d ct
image containing only the area covered by the liver mask (l), the 3d unet
[6] output a binary vessel mask (m).
a morphological dilation is then used to
enclose more vessel pixels in the m-covered area, generating a vessel attention
map (m ′).
to solve this issue, we propose a strategy of continuous spatial sampling
point data based on the m ′. speciﬁcally, the model randomly samples t points
in each training epoch, of which t/2 points fall in the smaller space covered by
the m ′, which enables the model to increase access to important data in the
region during training.
in addition, we apply a random perturbation oﬀset =
(δx, δy, δz) in the range of [−1, 1] to each point pt = (xt, yt, zt) ∈ m ′ in this
region to obtain a new point pt = (xt + δx, yt + δy, zt + δz), and the intensity
in this coordinate obtained by trilinear interpolation.
besides, compared with the 3d view, it is obvious that the voxel-based cnn
methods are easy to pay attention to the local area and produce a large area
of error segmentation, so the reconstructed surface is uneven.
segmentation blur in boundary areas with high uncertainty.
besides, perturbations are
applied to the points in the coverage area of the vessel attention map, so that our
full method performs arbitrary point sampling in the continuous space near the
vessel, and is encouraged to implicitly learn the couinaud boundary in countless
points.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf:
this representation is more aware of the structure
for convex objects, as the energy value for centers is signiﬁcantly diﬀerent from
pixels close to the boundary.
the vision community has been working on direct
object skeletonization from images [7,9,16,19].
for the classiﬁcation mode, the
model output has (k+1) channels with one channel representing the background
region.
therefore we calculate the local skeleton for sdt on-the-ﬂy after
all spatial transformations instead of pre-computing to prevent the model from
hallucinating the structure of parts outside of the currently visible region.
besides, under
the hausdorﬀ distance for evaluating shape-similarity between ground-truth and
predicted masks, our sdt reports an average score of 44.82 across two test splits,
which improves the previous state-of-the-art approach (i.e., fullnet with an aver-
age score of 50.15) by 10.6%.
however, for regression mode, if the background value is 0, we need to
use a threshold τ > 0 to decide the foreground region, which results in shrank
masks.
to separate the background region from the foreground objects, we assign
an energy value of −b to the background pixels (b ≥ 0).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf:
however, ct imaging has relatively high radiation doses that can pose a risk of
radiation exposure to patients.
this
makes mr imaging safer for patients, particularly for those who require frequent
or repeated scans.
how-
ever, their applicability to medical images has not been fully explored due to the
absence of publicly available pre-trained diﬀusion models tailored for the med-
ical imaging community.
to mimic the real-world setting, the diﬀusion models were
trained on a diverse dataset, including images from diﬀerent centers and scan-
ners.
the testing set (e.g., mr images) is from a new medical center that has
not appeared in the training set.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_53.pdf:
although contrast-enhanced ct images have better contrast
for pulmonary vessels compared to non-contrast ct images, the acquisition of
contrast-enhanced ct images needs to inject a certain amount of contrast agent
to the patients.
some patients have concerns about the possible risk of contrast
media [2].
however, all these
methods ignored the signiﬁcant variability in hu values of pulmonary vessels at
diﬀerent regions.
to summarize, there exist several challenges for pulmonary vessel segmen-
tation in non-contrast ct images: (1) the contrast between pulmonary vessels
and background voxels is extremely low (fig. 1(c)); (2) pulmonary vessels have
a complex structure and signiﬁcant variability in vessel appearance, with diﬀer-
ent scales in diﬀerent areas.
for
the input ct images, we propose an auto contrast enhancement (ace) module
to automatically adjust the range of hu values in diﬀerent areas of ct images.
this study is approved by the ethical committee of west
china hospital of sichuan university, china.
(d–e) compared to baseline, csnb can
enhance the ability to capture vascular features with widely variable size, shape, and
location.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf:
to this
end, we propose a novel deep learning network for personalized diagno-
sis in an adaptive manner, utilizing personal characteristics in diagnos-
ing dermatitis in a mobile- and fl-based environment.
apd-net incorporates a novel architectural design that lever-
ages personalized and centralized parameters, along with a ﬁne-tuning
method based on a modiﬁed ga to identify personal characteristics.
[18], and inter-
national skin imaging collaboration (isic)
2. pipeline for apd-net and representation of ga
this work aims to develop a mobile-based fl system that can provide a
personalized and customized diagnosis to patients across diﬀerent clients.
before diagnosis, a set
of newly ﬁne-tuned parameters (pf) is generated by ga. by jointly using both
pg and p ∗
pk, the initial population of a set of personalized parameters (
here, the ﬁtness scores are compared for all
fine-tuning network in federated learning for personalized skin diagnosis
381
individual chromosomes, and the n number of chromosomes that achieve a high
ﬁtness score is selected to form a new population.
finally, the optimally
ﬁne-tuned parameters are utilized to diagnose patients in each client.
here, the novel
architecture of apd-net is diﬀerentiated by the use of the dp architecture
that allows (1) to diagnose patients adaptively, and (2) to evaluate the ﬁtness
function.
as illustrated in algorithm i, the newly generated chromosomes,
which yield a large value of the ﬁtness score, are contained in a new population.
in apd-net, the ﬁtness score is evaluated by the ﬁtness function that is jointly
utilized in the architecture of apd-net as illustrated in fig.
therefore, the apd-net with our ga oﬀers
high accuracy in both the conventional diagnosis for overall patients and the
personalized diagnosis for each patient at a speciﬁc client.
fine-tuning network in federated learning for personalized skin diagnosis
383
2.3
training and fine-tuning apd-net
to summarize, (1) apd-net is initially trained in the fl server using gradi-
ents from many clients.
by
using the proposed ga, the new population of ﬁne-tuned parameters (p (i)
datasets
nevus
melanoma
others
total
7pt
575
268
168
1011
isic
5193
284
27349
32826
ham
6705
1113
2197
10015
testset (25%)
nevus
melanoma
others
total
7pt
144
67
42
253
isic
1299
71
6838
8207
ham
1677
279
550
2504
# images
(# for testset)
group
nevus
melanoma
others
7pt
g-00
575 (144)
268 (67)
168 (42)
isic
g-01
180 (45)
80 (20)
17070 (4268)
g-02
685 (172)
363 (91)
7915 (1979)
g-03
4328 (1082)
furthermore, in this work, we collected
skin images through the tertiary referral hospital under the approval of the
institutional review board (irb no. 1908-161-1059) and obtained images with
the consent of the subjects according to the principles of the declaration of
helsinki from 51 patients and subjects.
the salient characteristics of these models are summarized
in table 2(right).
table 3(right)
shows the performances of the dl models in every client (group).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf:
through theoretical analysis and extensive empirical evalua-
tion (code is available at https://github.com/biomedia-mira/subgroup-
separability), we ﬁnd a relationship between subgroup separability, sub-
group disparities, and performance degradation when models are trained
on data with systematic bias such as underdiagnosis.
recently, it was shown that these models might rely on
sensitive information when making their predictions [7,8] and that they exhibit
performance disparities across protected population subgroups [20].
however, this property is unlikely to hold for all
modalities and protected characteristics.
a more realistic premise is that sub-
group separability varies across characteristics and modalities.
https://doi.org/10.1007/978-3-031-43898-1_18
180
c. jones et al.
image classiﬁers (e.g. biological sex from chest x-ray can be predicted with
> 0.98 auc).
this is especially relevant in
medical imaging, where attributes such as age, biological sex, self-reported race,
socioeconomic status, and geographic location are often considered sensitive for
various clinical, ethical, and societal reasons.
we show that the ability of
models to detect which group an individual belongs to varies across modalities
and groups in medical imaging and that this property has profound consequences
for the performance and fairness of deep classiﬁers.
to the best of our knowledge,
ours is the ﬁrst work which analyses group-fair image classiﬁcation through the
lens of subgroup separability.
our contributions are threefold:
– we demonstrate empirically that subgroup separability varies across real-
world modalities and protected characteristics.
– we show theoretically that such diﬀerences in subgroup separability aﬀect
model bias in learned classiﬁers and that group fairness metrics may be inap-
propriate for datasets with low subgroup separability.
2
related work
group-fair image analysis seeks to mitigate performance disparities caused by
models exploiting sensitive information.
[20] highlighted that classiﬁcation models trained through erm underdiag-
nose historically underserved population subgroups.
suppose we have access to a
(biased) training dataset, where ptr is the conditional distribution between train-
ing images and training labels; we say that such a dataset is biased if ptr ̸= p.
we focus on group fairness, where each individual belongs to a subgroup a ∈ a
and aim to learn a fair model that maximises performance for all groups when
deployed on an unbiased test dataset drawn from p. we assume that the groups
are consistent across both datasets.
formally, group a = a∗ is said to
be underdiagnosed if it satisﬁes eq.
(2), notice that the prediction for any individual is a linear
combination of the mappings for each subgroup, weighted by the probability the
individual belongs to each group.
this model underdiagnoses group
a = a∗ whilst recovering the unbiased mapping for other groups.
the group-
wise tpr of an unbiased model, tpr(u)
a , is expressed in eq.
tpr(u)
a
= |ˆp(y|x+, a) > 0.5|
n+,a
≈ |p(y|x+, a) > 0.5|
n+,a
(6)
here, n+,a denotes the number of positive samples belonging to group a in
the test set.
(8) demonstrate that
tpr of the underdiagnosed group is directly aﬀected by bias from the training
set while other groups are mainly unaﬀected.
given this diﬀerence across groups,
an appropriately selected group fairness metric may be able to identify the bias,
in some cases even without access to an unbiased test set [23].
in such sit-
uations, we expect performance degradation to be uniform across groups and
thus not be detected by group fairness metrics.
4, we empirically investigate (i) how subgroup separa-
bility varies in the wild, (ii) how separability impacts performance for each group
when underdiagnosis bias is added to the datasets, (iii) how models encode sen-
sitive information in their representations.
we use test-
set area under receiver operating characteristic curve (auc) as a proxy for
separability, reporting results over ten random seeds in table 1.
table 1. separability of protected subgroups in real-world datasets, measured by test-
set auc of classiﬁers trained to predict the groups.
mean and standard deviation are
reported over ten random seeds, with results sorted by ascending mean auc.
dataset-attribute
modality
subgroups
auc
group 0 group 1
μ
σ
papila-sex
fundus image
male
female
0.642 0.057
ham10000-sex
skin dermatology male
female
0.723 0.015
ham10000-age
skin dermatology <60
≥60
0.803 0.020
papila-age
fundus image
<60
≥60
0.812 0.046
fitzpatrick17k-skin skin dermatology i–iii
iv–vi
0.891 0.010
chexpert-age
chest x-ray
<60
≥60
0.920 0.003
mimic-age
chest x-ray
<60
≥60
0.930 0.002
chexpert-race
chest x-ray
white
non-white 0.936 0.005
mimic-race
chest x-ray
white
non-white 0.951 0.004
chexpert-sex
chest x-ray
male
female
0.980 0.020
mimic-sex
chest x-ray
male
female
0.986 0.008
all attributes can
be predicted from chest x-ray scans with > 0.9 auc, implying that the modal-
ity encodes substantial information about patient identity.
age is consistently
well predicted across all modalities, whereas separability of biological sex varies,
184
c. jones et al.
with prediction of sex from fundus images being especially weak.
we inject underdiagnosis bias into
each training dataset by randomly mislabelling 25% of positive individuals in
group 1 (see table 1) as negative.
we illustrate the mean percentage point
accuracy degradation for each group in fig.
in these experiments, the
proportion of mislabelled images is small relative to the total population; thus,
the underdiagnosed subgroups mostly recover from label bias by sharing the
subgroup separability in medical image classiﬁcation
185
correct mapping with the uncorrupted group.
as subgroup separabil-
ity increases, performance degrades more for the underdiagnosed group (group
1), whilst performance for the uncorrupted group (group 0) remains somewhat
unharmed.
we see a statistically signiﬁcant performance drop for group 0 in the
mimic-sex experiment – we believe this is because the model learns separate
group-wise mappings, shrinking the eﬀective size of the dataset for group 0.
use of sensitive information in biased models
finally, we investigate how biased models use sensitive information.
this indicates that group
fairness metrics may be insuﬃcient for detecting bias when separability is low.
we renew the call for future datasets to be
released with patient metadata and multiple annotations to enable analysis of
diﬀerent sources and causes of bias.
reproducibility and impact.
https://doi.org/10.1038/s42256-020-00257-z
7. gichoya, j.w., et al.: ai recognition of patient race in medical imaging: a mod-
elling study.
https://doi.org/10.1016/
s2589-7500(22)00063-2
8. glocker, b., jones, c., bernhardt, m., winzeck, s.: algorithmic encoding of pro-
tected characteristics in chest x-ray disease detection models.
kovalyk, o., morales-s´anchez, j., verd´u-monedero, r., sell´es-navarro, i., palaz´on-
cabanes, a., sancho-g´omez, j.l.: papila: dataset with fundus images and clini-
cal data of both eyes of the same patient for glaucoma assessment.
https://doi.org/10.1145/3368555.3384468
19. rajpurkar, p., et al.: chexnet: radiologist-level pneumonia detection on chest x-
rays with deep learning, november 2017
20. seyyed-kalantari, l., zhang, h., mcdermott, m.b., chen, i.y., ghassemi, m.:
underdiagnosis bias of artiﬁcial intelligence algorithms applied to chest radiographs
in under-served patient populations.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf:
1(b), the intensity values vs.
spectral bands for the local positive (cancer) area and negative (normal) area
are highly correlated.
figure 1(b) plots the spatial similarity among all bands, and shows large cosine
similarity scores among nearby bands (error band of line chart in the light color
area) and small scores between bands in a long distance.
for
the spatial feature extraction stream, inspired from spatial redundancy between
adjacent bands, we group adjacent bands into a spectral agent.
following [23,27], we partition the datasets
into training, validation, and test sets using a patient-centric hard split approach
with a ratio of 3:1:1.
speciﬁcally, each patient’s data is allocated entirely to one
of the three sets, ensuring that the same patient’s data do not appear in multiple
sets.
left ﬁgures plot each
feature embedding in ascending order of the number of times they are dominant in the
population (y-axis) and feature dimension (x-axis) on the statistical results of the test
set.
inserting spectral information at all spatial layers (i.e., l1 to l4) and only
at l2 and l4 produce similar results, indicating that spectral features do not pos-
sess complex multilevel characteristics relative to spatial features.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf:
recent researches on cancer segmentation in dynamic con-
trast enhanced magnetic resonance imaging (dce-mri) usually resort to
the combination of temporal kinetic characteristics and deep learning to
improve segmentation performance.
[5,6]. how-
ever, the aforementioned methods require the complete dce-mri sequences and
overlook the diﬃculty in assessing complete temporal sequences and the missing
time point problem, especially post-contrast phase, due to the privacy protection
and patient conditions.
specif-
ically, given the pre-contrast and post-contrast images, the latent kinetic code is
learned using a score function of ddpm, which contains suﬃcient hemodynamic
characteristics to facilitate segmentation performance.
in this manner, our latent kinetic code can
be interpreted to provide tic information and hemodynamic characteristics for
accurate cancer segmentation.
(7)
where lssim is used to evaluate tumor structural characteristics, s and g rep-
resents segmentation map and ground truth, respectively; μs is the mean of s
and μg is the mean of g; ϕs represents the variance of s and ϕg represents
the variance of g; c1 and c2 denote the constant to hold training stable
[13], which contains a total of 64 patients with the contrast-
enhanced mri protocol: a pre-contrast scan, followed by 2 consecutive post-
contrast time points (as shown in fig. 3).
in contrast, the deep fea-
tures capture essential characteristics to reveal the structural information and
hemodynamic changes of tumors.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf:
therefore, magnetic resonance (mr)
imaging has been recommended to enhance the segmentation of soft tis-
sue oars in the han region.
based on our two empirical observations
that deformable registration of ct and mr images of the same patient
is inherently imperfect and that concatenating such images at the input
layer of a deep learning network cannot optimally exploit the information
provided by the mr modality, we propose a novel modality fusion mod-
ule (mfm) that learns to spatially align mr-based feature maps before
fusing them with ct-based feature maps.
our
implementation within the nnu-net framework shows promising results
on a dataset of ct and mr image pairs from the same patients.
therefore, the integration of complementary imaging modalities, such as
magnetic resonance (mr), has been strongly recommended in clinical practice to
enhance the segmentation of several soft tissue oars in the han region
our study therefore aims to evaluate the impact of
mr integration on the quality and robustness of automatic oar segmentation
in the han region, therefore contributing to the growing body of research on
multimodal methods for medical image analysis.
the third
group, hybrid fusion, aims to combine the strengths of early and late fusion [24]
by employing two or more separate encoders (i.e. one for each modality) and a
single decoder, where features from diﬀerent resolution levels of the encoder are
fused and fed into the decoder that produces the ﬁnal full-resolution segmenta-
tion.
similar conclusions were reached in a review of multimodal seg-
mentation methods in the medical imaging community by zhou et al.
relevant to the ﬁeld of multimodal segmentation are
also developments on unpaired multimodal segmentation, where cross-modality
learning is employed to take advantage of diﬀerent image modalities covering
the same anatomy, but without the constraint to collect images from the same
patients [5,10,19].
although the methodologies comprising cyclegans and/or
multiple segmentation networks [10,19] seem promising, they can be excessively
complex for the task of han oar segmentation where both ct and mr image
modalities from the same patient are often available.
when segmenting oars in the han region for the purpose of rt
planning, a multimodal segmentation model that can leverage the information
from ct and mr images of the same patient might be beneﬁcial compared to
separate single-modal models.
this can be mitigated with image registration,
but not completely, mainly due to diﬀerent patient positioning that especially
aﬀects the deformation of soft tissues, and various modality-speciﬁc artifacts
(e.g. motion, implants, partial volume eﬀect, etc.).
an important repercus-
sion is that image registration errors propagate into oar delineations, which
is particularly salient in the han region.
the fundamental idea is that stn
can learn meaningful features that are spatially invariant to characteristics of
the input data, without the need for extra supervision, thereby enhancing task
multimodal ct and mr segmentation of han oars
749
fig.
the han-seg dataset comprises ct and t1-weighted mr images of
56 patients, which were deformably registered with the simpleelastix registra-
tion tool, and corresponding curated manual delineations of 30 oars (for details,
please refer to [14]).
4
discussion
in this study, we evaluated the impact on the quality and robustness of auto-
matic oar segmentation in the han region caused by the incorporation of the
mr modality into the segmentation framework.
the choice of using nnu-net as the backbone was based on the rationale that
nnu-net already incorporates numerous state-of-the-art dl innovations pro-
posed in recent years, and therefore validation of the proposed mfm is more
challenging in comparison to simply improving a vanilla u-net architecture, and
consequently also more valuable to the research community.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf:
pi-cai22 provides multimodal mr images of 220 patients
with prostate cancer, including t2-weighted imaging (t2w), high b-value
diﬀusion-weighted imaging (dwi), and apparent diﬀusion coeﬃcient (adc)
maps.
after standard resampling and center cropping, all images have a size of
(24,384,384).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf:

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf:
it provides 361 training scans with man-
ual labels from 11 medical centers.
for msd datasets, we perform 5-fold cross-validation and ran the base-
line experiments with our codebase using exactly the same hyperparameters as men-
tioned.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_54.pdf:
the proposed method is evaluated on our
collected dce-mri dataset containing 206 patients with biopsy-proven
breast cancers.
keywords: breast cancer · weakly-supervised learning · medical
image segmentation · contrastive learning · dce-mri
1
introduction
breast cancer is the most common cause of cancer-related deaths among women
all around the world [8].
early diagnosis and treatment is beneﬁcial to improve
the survival rate and prognosis of breast cancer patients.
the method obtained a dice value of 83% using
the interval-slice annotation, on a testing dataset containing only 28 patients.
the extreme points are deﬁned as the left-, right-, anterior-,
posterior-, inferior-, and superior-most points of the cancerous region in 3d.
to further increase the area of foreground, the voxel at location k is
considered as new foreground seed if y (k) is greater than 0.8 and new back-
ground seed if y (k) is less than 0.1.
z(k) denotes the feature
vector of the voxel at location k. sim(·, ·) is the cosine similarity function.
if s(k) is greater than αn, the voxel at location k
is considered as positive.
we evaluated our method on an in-house breast dce-mri dataset
collected from the cancer center of sun yat-sen university.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf:
lesion location.
so we introduce uncertainty navigator for segmen-
tation(un) as a feature decoder, which incorporates the pixel-wise uncertainty
in u sand lesion location information in m with the segmentation feature maps
to generate the segmentation result and reliable features.
it ﬁrstly generates
reliable classiﬁcation features rc fusing the initial classiﬁcation feature maps f c
4
and the rich information (e.g., lesion location and boundary characteristic) in
rs, which can be expressed by:
rc = f c
4 ⊕ (conv(d3(rs))
a total of 157 patients
who suﬀer the breast cancer are considered - 43 achieve pcr and 114 non-pcr.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf:
we
then perform a simple yet eﬀective addition of the mapped image feature and
the encoded prior evolution feature, similar to the fusion of time embeddings, to
avoid redundant computation.
2.3
evolution uncertainty
similar to typical evolutionary algorithms, the ﬁnal results of boundary evolu-
tion are heavily inﬂuenced by the initialized population.
3
experiment
3.1
datasets and evaluation metrics
datasets: we use two publicly available skin lesion segmentation datasets from
diﬀerent institutions in our experiments: the isic-2016 dataset and the ph2
dataset.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf:
https://doi.org/10.1007/978-3-031-43901-8_51
deep probability contour framework
535
ing, monitoring, and follow-up radiotherapy (rt) planning [2,19]. delineation of
region of interest (roi) is a crucial step in rt planning.
it enables the extrac-
tion of semi-quantitative metrics such as standardized uptake values (suvs),
which normalize pixel intensities based on patient weight and radiotracer dose
[20].
however, the performance
of kspc depends heavily on the tuning parameters of bandwidth and threshold
in the model, and it lacks information from other patients.
mathematically, a
100 ω% region of a density f is deﬁned as the level set l(fω) = {f(x) ≥ fω}
with its corresponding contour level fω such that p(x∈ l(fω) = 1 − ω, where x
is a random variable and l(fω) has a minimal hypervolume [11].
in other words,
for any ω ∈ (0, 1), the 100 ω% contour refers to the region with the smallest area
which encompasses 100 ω% of the probability mass of the density function [11].
in practice, fω can be estimated using the following result.
the hecktor training dataset consists of
224 patients diagnosed with oropharyngeal cancer
for each patient, fdg-
pet input images and corresponding labels in binary description (0 s and 1 s)
for the primary gross tumour volume are provided and co-registered to a size
of 144 × 144 × 144 using bounding box information encompassing the tumour.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf:
however, if the local center has limited image collection capability,
there may also not be enough unlabeled data for semi-supervised learn-
ing to be eﬀective.
to overcome this issue, other partner centers can be
consulted to help enrich the pool of unlabeled images, but this can result
in data heterogeneity, which could hinder ssl that functions under the
assumption of consistent data distribution.
speciﬁcally, cu2l is built upon the teacher-student architecture with
the following tailored learning processes: (i) local pseudo-label learning
for reinforcing conﬁrmation of the data distribution of the local center;
(ii) category-level regularized non-parametric unlabeled-to-labeled learn-
ing for robustly mining shared information by using the limited expert
labels to regularize the intra-class features across centers to be discrim-
inative and generalized; (iii) stability learning under perturbations to
further enhance robustness to heterogeneity.
our method is evaluated
on prostate mri data from six diﬀerent clinical centers and shows supe-
rior performance compared to other semi-supervised methods.
https://doi.org/10.1007/978-3-031-43901-8_1
4
z. xu et al.
40
45
50
55
60
65
70
75
80
suponly
mt
ua-mt
ict
cpcl
cct
cps
ssnet
local labeled 
local unlabeled
external multi-site unlabeled support data 
…
,
,
,
typical semi-supervised learning (ssl)
effectively work under the assumption that the 
centralized local i.i.d. unlabeled data is abundant

limited performance gain or even fail when the local
unlabeled data is also limited (e.g., due to restricted 
image collection capabilities or a scarcity of patients)

multi-site semi-supervised learning (ms-ssl)
the unlabeled image pool can be quickly enriched
via the support from partner clinical centers with low 
barriers of entry (only unlabeled images are required)

data heterogeneity due to different scanners, 
scanning protocols and subject groups, which violate 
the typical ssl assumption of i.i.d. data
local: c1.
details of the acquisition protocols and number of scans for the six diﬀerent
centers.
each center supplied t2-weighted mr images of the prostate.
center source
#scans field strength (t) resolution (in-plane/through-plane in mm) coil
scanner
c1
runmc [1]
30
3
0.6–0.625/3.6–4
surface
siemens
c2
bmc
unfor-
tunately, such “abundance” may be unobtainable in practice, i.e., the local unla-
beled pool is also limited due to restricted image collection capabilities or scarce
patient samples.
as a speciﬁc case shown in table 1, there are only limited
prostate scans available per center.
to eﬃciently enrich
the unlabeled pool, seeking support from other centers is a viable solution, as
illustrated in fig.
yet, due to diﬀerences in imaging protocols and variations in
patient demographics, this solution usually introduces data heterogeneity, lead-
category-level regularized unlabeled-to-labeled learning
5
ing to a quality problem.
, the local unlabeled data is involved into pseudo-
label supervised-like learning to reinforce ﬁtting of the local data distribution;
(ii) considering that intra-class variance hinders eﬀective ms-ssl, we introduce
a non-parametric unlabeled-to-labeled learning scheme, which takes advantage of
the scarce expert labels to explicitly constrain the prototype-propagated predic-
tions, to help the model exploit discriminative and domain-insensitive features
from heterogeneous multi-site data to support the local center.
our method is evaluated on prostate mri data
from six diﬀerent clinical centers and shows promising performance on tackling
ms-ssl compared to other semi-supervised methods.
2
methods
2.1
problem formulation and basic architecture
in our scenario of ms-ssl, we have access to a local target dataset dlocal
(consisted of a labeled sub-set dl
local and an unlabeled sub-set du
local) and
the external unlabeled support datasets du
e
= m
j=1 du,j
e , where m is the
number of support centers.
considering the
large variance on slice thickness among diﬀerent centers [7,8], our experiments
are performed in 2d.
the key
challenge of ms-ssl is the proper design of lu for robustly exploiting multi-site
unlabeled data {du
local, du
e } to support the local center.
inherently, the challenge of ms-ssl stems
from intra-class variation, which results from diﬀerent imaging protocols, disease
progress and patient demographics. inspired by prototypical networks [13,19,25]
that compare class prototypes with pixel features to perform segmentation,
here, we introduce a non-parametric unlabeled-to-labeled (u2l) learning scheme
that utilizes expert labels to explicitly constrain the prototype-propagated pre-
dictions.
we utilize prostate t2-weighted mr images from six diﬀerent clini-
cal centers (c1–6)
cu2l (ours)
8
10
86
86.46 (6.72)
76.74 (9.97)
82.30 (9.93)
70.71 (12.94)
supervised (upper bound) 18
0
0
89.19 (4.33)
80.76 (6.71)
85.01 (4.35)
74.15 (6.44)
rizes the characteristics of the six data sources, following [7,8], where [7,8] also
reveal the severity of inter-center heterogeneity here through extensive experi-
ments.
compared to c1 and
c2, scans from c3 to c6 are taken from patients with prostate cancer, either for
detection or staging purposes, which can cause inherent semantic diﬀerences in
the prostate region to further aggravate heterogeneity.
following [7,8], we crop
each scan to preserve the slices with the prostate region only and then resize and
normalize it to 384 × 384 px in the axial plane with zero mean and unit variance.
we take c1 or c2 as the local target center and randomly divide their 30 scans
into 18, 3, and 9 samples as training, validation, and test sets, respectively.
implementation and evaluation metrics.
considering the large
variance in slice thickness among diﬀerent centers, we adopt the 2d architecture.
table 2 presents the quantitative results with either c1
or c2 as the local target center, wherein only 6 or 8 local scans are anno-
tated.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf:
to address this issue, we propose a more
eﬃcient approach, the eﬃcient group enhanced unet (ege-unet).
we incorporate a group multi-axis hadamard product attention module
(ghpa) and a group aggregation bridge module (gab) in a lightweight
manner.
as
estimated by the american cancer society, there were approximately 100,350
new cases and over 6,500 deaths in 2020
thus, an automated skin lesion
segmentation system is imperative, as it can assist medical professionals in
swiftly identifying lesion areas and facilitating subsequent treatment processes.
this work was partially supported by the national natural science foundation of
china (grant no. 61977045).
to be speciﬁc, ege-unet leverages two key modules: the group multi-axis
hadamard product attention module (ghpa) and group aggregation bridge
ege-unet: an eﬃcient group enhanced unet
483
module (gab).
subsequently, inspired by the multi-head
mode in mhsa, we propose ghpa, which divides the input into diﬀerent groups
and performs hpa in each group.
therefore, gab integrates high-level
and low-level features with diﬀerent sizes based on group aggregation, and addi-
tionally introduce mask information to assist feature fusion.
group multi-axis hadamard product attention module.
however,
ege-unet: an eﬃcient group enhanced unet
485
fig.
the architecture of group aggregation bridge module (gab).
utilizing simple hpa alone is insuﬃcient to extract information from multiple
perspectives, resulting in unsatisfactory results.
for the last group, we only use dw
on the feature map.
note that all kernel size employed in dw are 3.
group aggregation bridge module.
secondly, we
partition both feature maps into four groups along the channel dimension, and
concatenate one group from the low-level features with one from the high-level
features to obtain four groups of fused features.
for each group of fused features,
the mask is concatenated.
besides, it is noteworthy that ege-unet is the ﬁrst lightweight model
ege-unet: an eﬃcient group enhanced unet
487
table 1.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf:
alternatively, feature perturbation methods augment data by
perturbing data in feature space [7,22] and logit space [9].
although these augmentation approaches have been successful for natural
images, their usage for medical image semantic segmentation is quite restricted
as objects in medical images contain non-rigid morphological characteristics that
should be sensitively preserved.
3) our method preserves underlying mor-
phological characteristics of medical images by augmenting data with quasi-
imperceptible perturbation.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf:
the objective of this process is to attend to speciﬁc areas, which
mimics the shifted window attention mechanism in swin transformer
for both datasets, we randomly
split 80% of the samples on the patient level as the training set and the remaining
20% as the test set.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf:
the key insight is to deploy a linear combination of lesion edge,
either detected or computed, and the source image highlights the aﬀected
lesion area so that a learning model focuses more on the preserved lesion
structure instead of skin tone, thus iteratively improving segmentation
performance.
[1] yet some
improvements still remain to be addressed, importantly in areas that allow both
algorithmic performance and fairness [2], and in certain medical applications that
promise to signiﬁcantly lessen morbidity and mortality.
while the
em pattern may appear simple to recognize, its diagnosis can be challenging for
those with or without a medical background alike, as only 20% of united states
patients have the stereotypical bull’s eye lesion
such dl-assisted segmentation not only helps
clinicians in pre-screening patients but also improves downstream tasks such as
lesion classiﬁcation.
our lyme disease dataset contains two parts: (i)
a classiﬁcation dataset, composed of more than 3,000 diseased skin images that
are either obtained from public resources or clinicians with patient-informed con-
sent, and (ii) a segmentation dataset containing 185 samples that are manually
annotated for three regions—i.e., background, skin (light vs. dark), and lesion—
conducted under clinician supervision and institutional review boards (irb)
approval.
edgemixup helps the model to
focus more on the lesion area comparing fig.
annotated segmentation and classiﬁcation dataset characteristics, broken
down by ita-based skin tones (light skin/ dark skin) and disease types.
split
skin
sd-sub
no
em
hz
tc
total
df
ka
pg
tc
tf
total
seg
–
62
62
61
185
30
30
30
30
30
150
–
47/15
46/16
40/21
133/52
23/7
27/3
27/3
24/6
29/1
130/20
class
885
740
698
704
3027
40
40
40
40
40
200
822/63
682/58
608/90
609/95
2721/306
36/4
36/4
29/1
33/7
30/0
164/16
aﬀected area, further detection will reﬁne and constrain the detected boundary.
edgemixup removes more skin areas after each iteration and gradually gets
close to the real lesion at the third iteration.
all skin images are either collected from publicly available sources
or from clinicians with patient informed consent.
table 1 show the characteristics of these two datasets for both classiﬁcation
and segmentation tasks broken down by the disease type and skin tone, as cal-
culated by the individual typology angle (ita)
one prominent observation is
that ls images are more abundant than ds images due to a disparity in the avail-
ability of ds imagery found from either public sources or from clinicians with
patient consent.
our evaluation metrics include accuracy gap, the (rawlsian) minimum accuracy
across subpopulations, area under the receiver operating characteristic curve
(auc), and joint metrics (caiα and cauciα).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf:
keywords: brain tumor segmentation · edge-oriented module ·
transformer
1
introduction
accurate segmentation of brain tumors from mri images is of great signiﬁcance
as it enables more accurate assessment of tumor morphology, size, location, and
distribution range, thereby providing clinicians with a reliable basis for diagnosis
and treatment [16].
[20] considers
global context to coarsely locate lesion area and paying special attention to the
ambiguous area to specify the exact edges of the skin cancers.
the brats 2020 dataset [14] consists of mri image data from 369 patients,
with each patient having four modalities (t1, t1ce, t2 and t2-flair) of
skull-striped mri, which are aligned to a standard brain template.
the medseg dataset includes mri images of t1, t1ce, t2, and t2 flair
modalities from 255 patients with medulloblastoma.
the
red region represents wt, the yellow means tc and the white denotes et (color
ﬁgure online).
speciﬁcally, eoformer accurately segments both tc and et region boundaries.
3.4
ablation
we evaluate the eﬀectiveness of our proposed eoformer framework by con-
ducting ablation experiments on the brats 2020.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf:
multitask learning ·
hybrid cnn-transformer
1
introduction
breast cancer is the leading cause of cancer-related fatalities among women.
currently, it holds the highest incidence rate of cancer among women in the u.s.,
and in 2022 it accounted for 31% of all newly diagnosed cancer cases [1].
hmss dataset does not
provide the segmentation ground-truth masks, and for this study we arranged
with a group of experienced radiologists to prepare the masks for hmss.
bus dataset no. of images distribution
source
hmss
1,948
b:812, m:1136
netherlands
busi
647
b:437, m:210
egypt
busis
562
b:306, m:256
china
dataset b
163
b:109, m:54
spain
total
3,320
b: 1,664, m: 1,656
3.2
evaluation metrics
for performance evaluation of the classiﬁcation task, we used the following met-
rics: accuracy (acc), sensitivity (sens), speciﬁcity (spec), f1 score, area under
the curve of receiver operating characteristic (auc), false positive rate (fpr),
and false negative rate (fnr).
to avoid data leakage and bias, we selected the train, test, and vali-
dation sets based on the cases, i.e., the images from one case (patient) were
350
b. shareef et al.
table 2. performance metrics of the compared methods for bus image classiﬁcation
and segmentation.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf:
in this paper, we make a ﬁrst attempt to explore a deep
learning method for unsupervised gland segmentation, where no man-
ual annotations are required.
then, a morphology-aware semantic grouping module is
employed to summarize the overall information about glands by explic-
itly grouping the semantics of their sub-region proposals.
the cue can be described as: each gland is comprised
of a border region with high gray levels that surrounds the interior epithelial tis-
sues.
then, considering that our segmentation target is the gland,
we employ a morphology-aware semantic grouping module to summarize the
semantic information about glands by explicitly grouping the semantics of the
sub-region proposals.
(2) we propose to leverage an empirical cue to select gland sub-regions
and explicitly group their semantics into a complete gland region, thus avoid-
ing over-segmentation and under-segmentation in the segmentation results.
we deploy a msg for variation module to group the two gland sub-regions in the
embedding space with lmsgv , and a msg for omission module to dynamically reﬁne
the proposal map generated by the proposal mining frame (see gland boundary in p
and rp).
meantime, a morphology-aware semantic grouping
(msg) module is used to summarize the overall information about glands from
their sub-region proposals.
2.1
selective proposal mining
instead of generating pseudo-labels for the gland region directly from all the
pixels of the gland images as previous works typically do, which could lead to
over-segmentation and under-segmentation results, we propose using the empir-
ical cue as extra hints to guide the proposal generation process.
we train the
encoder in a self-supervised manner, and the loss function l consists of a typical
self-supervised loss lss, which is the cross-entropy loss between the feature map
fi and the one-hot cluster label ci = arg max (fi), and a spatial continuity loss
lsc, which regularizes the vertical and horizontal variance among pixels within
a certain area s to assure the continuity and completeness of the gland border
regions (see fig.
sub-region proposal selection via the empirical cue.
particularly, we select the region
with the highest average gray level as the proposal for the gland border.
then,
we ﬁll the areas surrounded by the gland border proposal and consider them as
the proposal for the interior epithelial tissues, while the rest areas of the gland
image are regarded as the background (i.e., non-glandular region).
finally, we
obtain the proposal map pi ∈ r3×h×w , which contains the two proposals for
two gland sub-regions and one background proposal.
2.2
morphology-aware semantic grouping
a direct merge of the two sub-region proposals to train a fully-supervised seg-
mentation network may not be optimal for our case.
without msg, the performance is not good
enough, due to signiﬁcant sub-region variation and gland omission.
it can be
observed that the segmentation performance without the msg modules is not
satisfactory due to the signiﬁcant sub-region variation and

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_15.pdf:
in
our experiments, the proposed method achieves a sensitivity of 85.0%
and speciﬁcity of 92.6% for detecting gastric tumors on a hold-out test
set consisting of 100 patients with cancer and
keywords: gastric cancer · large-scale cancer screening · mask
transformers · non-contrast ct
m.yuan—work was done during an internship at damo academy, alibaba group.
[16], which is
mainly attributed to patients being diagnosed with advanced-stage disease har-
boring unresectable tumors.
however, patients with early-stage disease
have a substantially higher ﬁve-year survival rate of around 72% [16].
[15], they are challenging to apply to the general population due to their
invasiveness, moderate sensitivity/speciﬁcity, high cost, or side eﬀects.
it is a non-invasive, relatively low-cost, and safe procedure that
exposes patients to less radiation dose and does not require the use of contrast
injection that may cause serious side eﬀects (compared to multi-phase contrast-
enhanced ct).
these results demonstrate
the potential of our approach for opportunistic screening of gastric cancer in
asymptomatic patients using non-contrast ct scans.
however, our framework is speciﬁcally designed for non-
contrast ct scans, which is beneﬁcial for asymptomatic patients.
[28] have recently proposed interpreting
the queries as clustering centers and adding regulatory constraints for learning
the cluster representations of the queries.
(b) learnable object queries interact with the
multi-level u-net features through a transformer decoder and produce learned clus-
ter centers.
(c) all the pixels are assigned to cluster centers by matrix multiplication.
recent studies [27,28] suggest interpreting
object queries as cluster centers, which naturally exhibit intra-cluster similarity
and inter-class discrepancy.
speciﬁcally, given image x ∈ rh×w ×d, annotation y ∈ rk×hw d, and
patient class p ∈ l, our model consists of three components: 1) a cnn back-
bone to extract its pixel-wise features f ∈ rc×hw d (fig. 1a), 2) a transformer
module (fig. 1b), and 3) a multi-task cluster inference module (fig. 1c).
[28] to substitute spatial-wise softmax in the original settings.
150
m. yuan et al.
we further interpret the object queries as cluster centers from a cluster anal-
ysis perspective.
all the pixels in the convolutional feature map are assigned to
diﬀerent clusters based on these centers.
∈ rn×hw d is computed as the cluster-wise softmax function
over the matrix product between the cluster centers c and pixel-wise feature
matrix f, i.e.,
m = softmaxn(r) = softmaxn(cf).
the aggregation of pixels is achieved by
z = ckm, where the cluster-wise classiﬁcation ck is represented by an mlp
that projects the cluster centers c to k channels (the number of segmentation
classes).
the learned cluster centers possess high-level semantics with both inter-
cluster discrepancy and intra-cluster similarity for eﬀective classiﬁcation.
rather
than directly classifying the ﬁnal feature map, we ﬁrst generate the cluster-
path feature vector by taking the channel-wise average of cluster centers ¯c =
1
n

i=1 ci ∈ rc.
our study analyzed a dataset of ct scans col-
lected from guangdong province people’s hospital between years 2018 and 2020,
with 2,139 patients consisting of 787 gastric cancer and 1,352 normal cases.
we
used the latest patients in the second half of 2020 as a hold-out test set, result-
ing in a training set of 687 gastric cancer and 1,204 normal cases, and a test
set of 100 gastric cancer and 148 normal cases.
to further evaluate speciﬁcity
in a larger population, we collected an external test set of 903 normal cases
from shengjing hospital.
all patients underwent multi-phase cts with a median spac-
ing of 0.75 × 0.75 × 5.0 mm and an average size of (512, 512, 108) voxel.
2. (a) roc curve for our model versus two experts on the hold-out test set of
n = 248 patients for binary classiﬁcation.
[8,18] to localize the stomach
region in the entire image in the testing phase.
evaluation metrics and reader study.
for the binary classiﬁcation, model
performance is evaluated using area under roc curve (auc), sensitivity (sens.),
and speciﬁcity (spec.).
(78.1, 91.1) (88.0, 96.5) (96.7, 98.7)
table 2. patient-level detection and tumor-level localization results (%) over gas-
tric cancer across diﬀerent t-stages.
method
criteria t1
t2
t3
t4
miss-t
nnunet-joint [8] patient
30.0(3/10) 66.7(6/9) 94.1(32/34) 100.0(9/9) 86.1(31/36)
tumor
20.0(2/10) 55.6(5/9) 94.1(32/34) 100.0(9/9) 80.6(29/36)
ours
patient
60.0(6/10) 77.8(7/9) 94.1(32/34) 100.0(9/9) 86.1(31/36)
tumor
30.0(3/10) 66.7(6/9) 94.1(32/34) 100.0(9/9) 80.6(30/36)
radiologist 1
patient
50.0(5/10) 55.6(5/9)
76.5(26/34) 88.9(8/9)
77.8(28/36)
radiologist 2
patient
30.0(3/10) 55.6(5/9) 85.3(29/34) 100.0(9/9) 80.6(29/36)
the ground truth is greater than 0.01, measured by the dice score.
a reader
study was conducted with two experienced radiologists, one from guangdong
province people’s hospital with 20 years of experience and the other from the
first aﬃliated hospital of zhejiang university with 9 years of experience in
gastric imaging.
no patient information or records were provided
to the readers.
[10],
ugis and endoscopy screening performance in large population [4], and early stage
gastric cancer detection rate of senior radiologists on narrow-band imaging with mag-
nifying endoscopy (me-nbi)
in table 2, we report the performance of patient-level
detection and tumor-level localization stratiﬁed by tumor (t) stage.
our method surpasses or
performs on par with established screening tools [4,7,10] in terms of sensitivity
for gastric cancer detection at a similar speciﬁcity level with a relatively large
testing patient size (n = 1151 by integrating the internal and external test
sets), as shown in table 3.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf:
the whole process can be formulated as follows:
o = enci(i), p = enct(p), sground = op ⊤, lcls = loss(sground; t),
(1)
where o ∈ rn×d, p ∈ rm×d denote the image and text features respectively for
n candidate region proposals and m target objects, sground ∈ rn×m represents
the cross-modal alignment scores, and t ∈ {0, 1}n×m is the target matrix.
3.2
language syntax based prompt fusion
as mentioned above, it is diﬃcult for a single prompt input structure such as
glip to cover all necessary descriptions even through careful designation of
the prompt.
more speciﬁcally, the vlm outputs a set of candidate
region proposals ci for each prompt pi, and these candidates carry more multi-
dimensional information than prompts.
in addition, the can-
didate, e.g., cij ∈ ci, carries richer information that can be further utilized, such
multiple prompt fusion for zero-shot lesion detection
287
as central coordinate xj and yj, region size wj and hj, category label, and pre-
diction conﬁdence score.
as
such, we consider clustering the center coordinate (xj, yj) and region size (wj, hj)
respectively to ﬁlter out those candidates with the wrong location and size.
there are
four sub-modules in our approach, where the location cluster floc and size clus-
ter fsize discard the candidates with large deviations and abnormal sizes.
the ﬁrst three
rows in table 1 represent the results of single prompt by only providing shape,
color, and location information, respectively.
2, with the same group
of multiple prompts, the accuracy of ﬁne-tuned model has increased almost twice
as much as that of zero-shot, further demonstrating the eﬀectiveness of our
290
m. guo et al.
table 4. results with diﬀerent fusion strategies.
strategy
dataset
isic 2016
cvc-300
bccd
ap
ap50
ap
ap50
ap
ap50
equally
16.8
25.2
30.8
40.4
12.5
21.6
category
13.2
20.4
30.8
40.4
15.3
24.9
attribute
19.8
30.9
36.1
47.9
15.8
32.6
method in both settings.
our approach has three key components, i.e., location cluster, size clus-
ter and prediction corrector.
the location cluster ﬁlters out the candidates with
severe deviation from the target.
components
isic 2016
cvc-300
bccd
location
cluster
size
cluster
prediction
corrector
ap (%)

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf:
the algorithm was
evaluated on data from a hospital in a diﬀerent country and various sub-
sets of this data that correspond to diﬀerent levels of domain shift.
supported by swedish e-science research center, vinnova, the ceniit career
development program at link¨oping university, and wallenberg ai, wasp funded by
the knut and alice wallenberg foundation.
in this work, we evaluate an attention-based mil model on unseen data
from a new hospital and propose a way to quantify the domain shift severity.
the model is trained to perform binary classiﬁcation of wsis from lymph nodes
of breast cancer patients.
we split the data from the new hospital into several
subsets to investigate clinically realistic scenarios triggering diﬀerent levels of
domain shift.
the novel contributions of our work can be summarised as:
1. proposing an unsupervised metric named fr´echet domain distance (fdd)
for quantifying the eﬀects of domain shift in attention-based mil;
2. showing how fdd can help to identify subsets of patient cases for which mil
performance is worse than reported on the in-domain test data;
3.
comparing the eﬀectiveness of using uncertainty estimation versus learnt rep-
resentations for domain shift detection in mil.
2
methods
our experiments center on an mil algorithm with attention developed for clas-
siﬁcation in digital pathology.
141 wsis from axillary nodes dissection cases (57 wsis with metastases):
potentially large shift as some patients have already started neoadjuvant
treatment as well as the tissue may be aﬀected from the procedure of sentinel
lymph node removal.
2a.
we deem that the information needed to do this
type of subset divisions would be available without labelling since the patient
cases in a clinical setting would already contain such information.
the classiﬁcation perfor-
mance is evaluated using the area under receiver operating characteristic curve
(roc-auc) and matthews correlation coeﬃcient (mcc) [2].
our results show that domain shift is
present between the wsis from the same hospital (camelyon data) and another
medical centre (brln data).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf:
keywords: breast cancer · mammogram · risk prediction
1
introduction
breast cancer impacts women globally [15] and mammographic screening for
women over a certain age has been shown to reduce mortality
however, these
imaging techniques are expensive and add additional burdens for the patient.
recently, several studies [8,32,33] revealed the potential of artiﬁcial intelligence
(ai) to develop a better risk assessment model to identify women who may ben-
eﬁt from supplemental screening or a personalized screening interval and these
may lead to improved screening outcomes.
in clinical practice, breast density and traditional statistical methods for pre-
dicting breast cancer risks such as the gail
recently, deep neural network based models that predict a patient’s risk score
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp.
these
models do not require additional patient information and have been shown to
outperform traditional statistical models.
integrating prior mammograms into deep learning models for breast
cancer risk prediction can provide a more comprehensive evaluation of a patient’s
breast health.
in this paper, we introduce a deep neural network that makes use of prior
mammograms, to assess a patient’s risk of developing breast cancer, dubbed
prime+
we hypothesize that
mammographic parenchymal pattern changes between current and prior allow
the model to better assess a patient’s risk.
the method is trained and evaluated on a large and diverse dataset of over
9,000 patients and shown to outperform a model based on state-of-the art risk
prediction techniques for mammography [33].
for medical applications, x typically
represents patient information like age, family history, genetic makeup, and diag-
nostic test results (e.g., a mammogram).
we typically want to estimate the hazard function h(t), which measures the
rate at which patients experience the event of interest at time t, given that they
have survived up to that point.
speciﬁcally, h(t) is 1 if the patient is diag-
nosed with cancer within t years and 0 otherwise.
= xcp c ⊕ xcurr, which is
then used by the base hazard network and time-dependent hazard network to
predict the cumulative hazard function ˆh.
enhanced risk prediction with prior images
393
3
experiments
3.1
dataset
we compiled an in-house mammography dataset comprising 16,113 exams
(64,452 images) from 9,113 patients across institutions from the united states,
gathered between 2010 and 2021.
we partitioned the dataset
by patient to create training, validation, and test sets.
the validation set con-
tains 800 exams (198 cancer, 210 benign, 392 normal) from 400 patients, and
the test set contains 1,200 exams (302 cancer, 290 benign, 608 normal) from
600 patients.
time-dependent roc
analysis generates an roc curve and the area under the curve (auc) for each
speciﬁc time point in the follow-up period, enabling evaluation of the model’s
performance over time.
our results suggest that incorporating changes
in patients using prior mammograms and a transformer decoder improves the
performance of breast cancer risk prediction models.
analysis based on density.
women with dense breasts have a four-to six-fold
higher risk of breast cancer
lastly, we divided the exams based on the level of breast density, with a fatty
group consisting of density a and b, and a dense group consisting of density c
and d. both the baseline and prime+ performs better in fatty group than dense
group.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf:
radiological follow-up of oncological patients requires the analysis
and comparison of multiple unregistered scans acquired every few months.
experimental results on
lung (83 cts from 19 patients) and liver (77 cects from 18 patients) datasets
with more than two scans per patient yielded an individual lesion change class
accuracy of 98% and 85%, and identiﬁcation of patterns of lesion change with
an accuracy of 96% and 76%, respectively.
keywords: longitudinal follow-up · lesion matching · lesion change analysis
1
introduction
the periodic acquisition and analysis of volumetric ct and mri scans of oncology
patients is essential for the evaluation of the disease status, the selection of the treat-
ment, and the response to treatment.
currently, scans are acquired every 2–12 months
according to the patient’s characteristics, disease stage, and treatment regime.
as
treatments improve and patients live longer, the number of scans in longitudinal studies
increases and their interpretation is more challenging and time-consuming.
experimental results on lung (83 cts, 19 patients) and liver (77 cects, 18 patients)
datasets show that our method yields high classiﬁcation accuracy.
similarity-based
methods pair two lesions with similar features, e.g., intensity, shape, location
1. longitudinal study of a patient with liver metastases (color overlays): three consecutive (a–
c) illustrative slices of unregistered cect scans acquired at times ti; matching colors correspond
to matching lesions; (d) lesion changes graph: nodes correspond to lesions vi
j where j is the lesion
number at time ti (dotted rectangle); edges correspond to lesion matches (consecutive straight,
non-consecutive curved).
, sn
be a series of n ≥ 2 consecutive patient scans acquired at times
ti,1 ≤ i ≤ n. let g = (v, e ) be a directed acyclic graph where v =

v i
, 1 ≤
i < k ≤ n
	
be a set of
forward-directed edges connecting vertices in v i to vertices in v k. edge ei,k
j,l indicates
that the lesions corresponding to vertices vi
j, vk
l are the same lesion, i.e., that the lesion
appears in scans si, sk in the same location.
graph-theoretic automatic lesion tracking and detection
111
2.2
lesion matching computation
lesion matchings are determined by the location and relative proximity of the lesions
in two or more registered scans.
let τm =

tﬁrst
m , tlast
m

be the time interval between the ﬁrst and last
scans of ccm, and let centroid(ccm) be the center of mass of all lesions in ccm at all times.
3
experimental results
we evaluated our method with two studies on retrospectively collected patient datasets
that were manually annotated by an expert radiologist.
dataset: lung and liver ct studies were retrospectively obtained from two medical
centers (hadassah univ hosp jerusalem israel) during the routine clinical examination
of patients with metastatic disease.
each patient study consists of at least 3 scans.
dlung consists of 83 chest ct scans from 19 patients with a mean 4.4 ± 2.0
scans/patient, a mean time interval between consecutive scans of 125.9 ± 81.3 days, and
voxel sizes of 0.6–1.0 × 0.6–1.0
dliver consists of 77 abdominal
cect scans from 18 patients with a mean 4.3 ± 2.0 scans/patient, a mean time interval
between consecutive scans of 109.7 ± 93.5 days, and voxel sizes of 0.6–1.0 × 0.6–1.0
×
for each non-consecutive
edge connecting lesions vi
j, vk
l , he analyzed the corresponding region in the skipped
scans sj at tj ∈ ]ti, tk[ for possible missed lesions.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf:
early diagnosis of renal cancer can greatly improve the sur-
vival rate of patients.
compared with ct and mri, ceus is
radiation-free, cost-eﬀective, and safe in patients with renal dysfunction.
(2) in the video-based diagnosis stage, the network
automatically chooses high-conﬁdence region features of each frame according to
the single-frame detection results and performs temporal aggregation to output
a more accurate diagnosis.
3
experimental results
3.1
materials and implementations
we collect a renal tumor us dataset of 179 cases from two medical centers,
which is split into the training and validation sets.
we further collect 36 cases
from the two medical centers mentioned above (14 benign cases) and another
center (fujian provincial hospital, 22 malignant cases) to form the test set.
2. there is an obvious visual diﬀerence
between the images from the fujian provincial hospital (last column in fig.
2)
and the other two centers, which raises the complexity of the task but can better
verify our method’s generalization ability.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf:
more importantly, this research
highlights the usage of fpe can eﬀectively replace skip connections by providing
more comprehensive multi-scale characteristics from full stages in encoder.
[8], and
etc. by aiding in forming a coarse location of the polyp and contributing to
improved accuracy and performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf:
(a) few
works report detailed performance in the clinically relevant region of less than 1
fp/image.
m&m surpasses previous works by a large margin in this region.
1a, most works focus on reporting
recalls outside the clinically relevant region of less than 1 fp/image.
to tackle the high rate of false positives in mammography, we identify three
challenges: (1) a malignant mammogram typically contains only one malignant
ﬁnding.
we split the data into train/val/test with an 80:10:10
ratio at the patient level; (2) inhouse-a: an evaluation dataset collected from
m&m: a multi-view and mil sparse detector
783
table 1.
[24]
53.2
36.2
-17.0 64.3
77.0
85.5
m&m (ours)
57.1
53.6 -3.5
87.7
90.9
92.5
a u.s. multi-site mammography operator; (3) inhouse-b: an evaluation dataset
collected from a u.s. academic hospital (see [18], sec.
we
report free response operating characteristic (froc) curves and recalls at var-
ious fp/image (r@t).
following [3,5,16,29], a proposal is considered true pos-
itive if its center lies within the ground truth box.
for classiﬁcation, we report
the area under the receiver operating characteristic curve (auc).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf:
a novel framework named pixel-lesion-patient network
(plan) is proposed.
it also has an image-wise classiﬁer to eﬀectively
aggregate global information and predict patient-level diagnosis.
a large-
scale multi-phase dataset is collected containing 939 tumor patients and
810 normal subjects.
on the non-contrast tumor screening task, plan achieves
95% and 96% in patient-level sensitivity and speciﬁcity.
early detection and accurate diagnosis of liver tumors may improve overall
partially supported by the national natural science foundation of china (grant
82071885), basic research projects of liaoning provincial department of education
(ljkmz20221160), the national youth talent support program of china, and science
and technology innovation talent project in shenyang (rc210265).
https://doi.org/10.1007/978-3-031-43904-9_8
liver tumor screening and diagnosis in ct
73
patient outcomes, in which imaging plays a key role [11]. computed tomog-
raphy (ct) is one of the most important imaging modalities for liver tumors.
dynamic contrast-enhanced (dce) ct is widely used for diagnostics, but it
requires iodine contrast injection which can cause reaction and potential risks
in patients.
after an incidental
tumor is found, the patient may undergo further imaging examination such as
a multi-phase dce ct for diﬀerential diagnosis [11], which can provide useful
discriminative information such as the vascularity of lesions and the pattern of
contrast agent enhancement [19].
(1) tumor screening involves ﬁnding tumor patients
in a large pool of healthy subjects and patients.
most existing works in tumor
segmentation and detection did not explicitly consider it since their training and
testing images are all tumor patients.
we col-
lect a large-scale dataset with both tumor and non-tumor subjects, where the
non-tumor subjects includes not only healthy ones, but also patients with various
diﬀuse liver diseases such as steatosis and hepatitis to improve the robustness of
the algorithm.
such metrics cannot reﬂect the lesion-level accuracy (how many lesion
instances are correctly detected and classiﬁed) and may bias to large lesions when
a patient has multiple tumors.
patient-level metrics (e.g. classifying whether a
subject has malignant tumors) are also useful for treatment recommendation
in clinical practice [18,20].
therefore, we assess our algorithm thoroughly with
pixel, lesion, and patient-level metrics.
the pixel-
wise segmentation architectures may not be optimal for lesion and patient-level
evaluation metrics since they cannot consider a lesion or an image holistically.
[3,4,17] have emerged in the
computer vision community and achieved the state-of-the-art performance in
instance segmentation tasks.
inspired by them, we propose a novel end-to-end framework
named pixel-lesion-patient network (plan) for lesion segmentation and classi-
ﬁcation, as well as patient classiﬁcation.
it contains three branches with bottom-
up cooperation: the segmentation map from the pixel branch helps to initialize
the lesion branch, which is an improved mask transformer aiming to segment and
classify each lesion; the patient branch aggregates information from the whole
image and predicts image-level labels of each lesion type, with regularization
terms to encourage consistency with the lesion branch.
we collected a large-scale multi-phase dataset containing 810 non-tumor sub-
jects and 939 tumor patients.
on the non-contrast tumor screening
and diagnosis task, plan achieves 95.0%, 96.4%, and 0.965 in patient-level sen-
sitivity, speciﬁcity, and average auc for malignant and benign patients, in con-
trast to 94.4%, 93.7%, and 0.889 for the widely-used nnu-net [8].
1. framework of the pixel-lesion-patient network (plan).
a group of q learnable feature vectors {qi ∈ rm}q
i=1 are
randomly initialized as object queries.
76
k. yan et al.
2.2
pixel-lesion-patient network (plan)
our goal is to segment the mask and classify the type of each tumor in a liver ct.
we also hope to make patient-level diagnoses for each ct scan.
(3) a patient branch is attached to make
dedicated image-level predictions with a proposed lesion-patient consistency loss.
patient branch.
a patient-level diagnosis is useful for triage.
intuitively, we can also infer patient-level labels from
segmentation results by checking if there is any lesion in the predicted mask.
we equip plan with a dedicated patient branch to aggregate such global
information to make better patient-level prediction.
since one patient can have
multiple liver tumors of diﬀerent types, in our problem, we give each image
several hierarchical binary labels.
liver tumor screening and diagnosis in ct
77
a lesion-patient consistency loss is further proposed to encourage coher-
ence of the lesion and patient-level predictions.
[6], we compute a pseudo patient-level prediction ˜c ∈ rc from the lesion-level
predictions by max-pooling the class probability of each class across all lesion
queries (discarding the no-object class).
we also have the probability vector from
the patient branch ˜p ∈ rc corresponding to the c ﬁne-grained classes.
the overall loss of plan is listed in eq. 1, where lpixel is the combined cross-
entropy (ce) and dice loss for the pixel branch as in nnu-net [8]; llesion-class
is the ce loss [3] for lesion classiﬁcation in the lesion branch; llesion-mask is
the combined ce and dice loss [3] for binary lesion segmentation in the lesion
branch with the foreground-enhanced sampling strategy; lpatient is the binary
ce loss for the multi-label classiﬁcation task in the patient branch.
our dataset contains 810 normal subjects and 939 patients with liver
tumors.
each normal subject has a non-contrast (nc) ct, while each patient
has a dynamic contrast-enhanced (dce) ct scan with nc, arterial, and venous
phases.
in the former setting,
both normal and patient data are used and randomly split into 1149 training,
100 validation, and 500 testing.
in the latter one, only patient data are used
with 641 training, 100 validation, and 200 testing.
another hold-out set of 150
patients and 100 normal cts are used for reader study to compare our accuracy
with two radiologists.
we ﬁrst train an nnu-net on public datasets to segment liver and surround-
ing organs (gallbladder, hepatic vein, spleen, stomach, and pancreas), and then
crop the liver region to train plan.
patient-level performance on the test set of 500 cases.
patient-level results.
for the baselines, patient-level labels are
inferred from their predicted masks by counting lesion pixels.
as displayed in
table 1, plan achieves the best accuracy on all tasks, especially in nc pre-
liminary diagnosis tasks, which demonstrates the eﬀectiveness of its dedicated
patient branch that can explicitly aggregate features from the whole image.
lesion and pixel-level results.
in this work, we focus more on
patient and lesion-level metrics.
we consider patients with only one
tumor type in this study.
it can be seen that
our proposed anchor queries produced by the pixel branch, fes loss, and lesion-
patient consistency loss are useful for the ﬁnal performance.
the eﬃcacy of the
lesion and patient branches has been analyzed above based on the lesion and
patient-level results.
in the patient level,
[5] achieved auc=0.75 in nc ct tumor screening, while our auc is 0.985.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf:
2) the characteristic of gastroscopic images
exhibits distinct diﬀerences from the natural images [18,19,21] and is often of
high similarity in global but high diversity in local.
some appearances of lesions are quite rare and can only be observed in a
few patients.
current pseudo-
label generation methods rely on the objectiveness score threshold to generate
pseudo-labels, which makes them perform below expectations on gld, because
the characteristic of gastroscopic lesions makes it diﬃcult to set a suitable thresh-
old to discover potential lesions meanwhile avoiding introducing much noise.
the main challenge for this goal is the characteristic
of gastroscopic lesions.
then, ppg obtains the
prototype feature vector pc by calculating the center of each class memory mc,
and the prototype feature vector set can be expressed as pt = {p1, p2, ..., pc}.
lgmdd collects about 1m+ gastroscopic images from 2 hospi-
tals of about 500 patients and their diagnosis reports.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf:
we use the question of predicting response to radiotherapy
in colorectal cancer patients as an exemplar for developing prediction
models that do provide such contextual information and therefore can
eﬀectively support clinical decision making.
there is a growing body of
evidence that about 30% of colorectal cancer patients do not respond to
radiotherapy and will need alternative treatment.
the consensus molec-
ular subtypes for colorectal cancer (cms) provide one such approach to
categorising patients based on their disease biology.
by jointly predict-
ing a patient’s response to radiotherapy, the presence of cms4, and the
epithelial tissue map from morphological features extracted from stan-
dard h&e slides we provide a comprehensive clinically relevant assess-
ment of a biopsy.
https://doi.org/10.1007/978-3-031-43904-9_73
joint prediction in colorectal cancer biopsies
759
1
introduction
in the uk, approximately 11,500 patients are diagnosed with rectal cancer each
year
a common form of treatment for such patients is neoadjuvant therapy,
including chemotherapy and radiotherapy, which can be given to patients with
locally advanced rectal cancer to shrink the tumour prior to surgery.
recent
evidence suggests that 10–20% of patients will have a complete pathological
response to neoadjuvant therapy and can therefore avoid surgery altogether [2,5].
however, one third of patients do not beneﬁt from radiotherapy treatment prior
to surgery [8], hence it is important to determine how a patient will respond to
radiotherapy with a personalized approach in order to avoid overtreatment.
histology-based digital biomarkers enable the possibility to predict a
patient’s response to therapy.
various studies
have investigated the link between cms and patient outcomes, suggesting that
patients with tumour classiﬁed as cms4, which features stromal invasion [9] and
shows signiﬁcantly higher stroma content
[15]
found that the features they developed representing spatial organisation reﬂected
characteristics of the four cms classes.
other work
has looked at predicting chemoradiotherapy response in rectal cancer patients
from h&e images using diﬀerent approaches, but without providing contextual
interpretations [19,22].
as local cell communities form the nodes of
such a graph it can eﬀectively model the micro-anatomy of the tissue.
pathologists and
oncologists can use this information to inspect the validity of the prediction result
and interrogate key aspects of the spatial biology that is critical for patient man-
agement.
we achieve 0.82 auc predicting complete response to radio-
therapy using deep learning on wsis for crc patients, whilst providing novel
interpretability of the results.
we calculate the mean patch features
for these superpixel regions, and use the superpixel features and centers as our graph
nodes, applying delaunay triangulation to generate the edges of the graph.
the superpixels centers are used as the nodes of the graph, and the node
features are the weighted mean of the corresponding patch features which overlap
with the superpixel region.
these epithelial segmentation masks were generated at 10x mag-
niﬁcation (1 µm2/pixel) with a u-net [17] which was trained and validated on
666 full tissue sections belonging to 362 patients from the focus cohort [18].
grampian and aristotle are used in both training and
validation, with a 70/30% training-validation split, keeping any wsis from a
single patient in the same dataset.
there are 365 slides total in our dataset, from
249 patients.
for example, the model demonstrates that
cms4 patients are less likely to respond to radiotherapy.
in the top slide, we observe high cms4 activation in stromal rich regions,
764
r. wood et al.
and interestingly also high cms4 activation in the bottom center, dissociating
from the response to rt activation map.
this could be explained by the lym-
phocyte content, supported by the higher epithelial map activations in the same
location.
both slides are classiﬁed
as cms4 and the patients did not have a complete response to radiotherapy.
importantly, this level of visualisation
is not only accessible to pathologists, this joint prediction model also enhances
the communication between pathologists and oncologists which is critical for
patient management.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf:
https://doi.org/10.1007/978-3-031-43904-9_72
748
c. li et al.
quantify data diﬃculty based on data characteristics such as complexity [19].

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf:
our ultimate goal is to group those detections
into sets corresponding to distinct polyps.
these recordings were captured from standard colonoscopy pro-
cedures conducted at six medical centers during the period of 2019 to 2022.
in addition, we evaluate the eﬀectiveness of reid by measuring the aver-
age polyp fragmentation rate (fr), deﬁned as the average number of tracklets
polyps are split into.
here, we investigate if the proposed reid model, used to group disjoint
tracklets of the same polyp, can increase the accuracy of cadx.
data.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf:
the coherence loss uses the feature centers generated
by the static images to guide the frame attention in the video model.
keywords: breast ultrasound classiﬁcation · ultrasound video ·
coherence loss
1
introduction
breast cancer is a life-threatening disease that has surpassed lung cancer as lead-
ing cancer in some countries and regions [20].
1, the feature points of static images are more
concentrated, while the feature of video frames sometimes are away from the
class centers.
frames far from the centers are harder to classify.
therefore, it is a
promising approach to guide the video model to pay more attention to important
frames close to the class center with the assistance of static keyframe images.
during training, we construct category fea-
ture centers for malignant and benign examples respectively using center loss [26]
on static image inputs and use the centers to guide the training of video frame
attention.
speciﬁcally, we propose coherence loss, which promotes the frames
close to the centers to have high attention weights and decreases the weights for
frames far from the centers.
due to the feature centers being generated by the
larger scale image dataset, it provides more accurate and discriminative feature
centers which can guide the video frame attention to focus on important frames,
and ﬁnally leads to better video classiﬁcation.
the frames with clear diagnostic characteristics
are given higher attention values.
we analyze the relationship between ultrasound video data and image data,
and propose the coherence loss to use image feature centers to guide the
training of frame attention.
the coherence loss is proposed to guide the
frame attention by using the feature centers generated by the images.
to promote the formation of feature centers, we apply the cen-
ter loss
3.2
training with coherence loss
in this section, we introduce the coherence loss to guide the frame attention with
the assistance of the category feature centers.
we use the same method as center
loss [26] to obtain the feature centers for the malignancy and benign lesions,
which are denoted as cmal and cbenign, respectively.
the distances of frame features and the feature centers can measure the qual-
ity of the frames.
the frame features close to the centers are more discriminative
for the classiﬁcation task.
speciﬁcally, we push the frames close to the centers
to have higher attention weights and decrease the weights far from the centers.
to do this, for each video frame with feature fi, we ﬁrst calculate the feature
distance from its corresponding class center.
lcenter means the center loss.
the feature centers
kga-net for breast ultrasound video classiﬁcation
447
table 1. comparison with other video models.
without the image dataset, we generate the feature centers
from the video frames.
it also shows that the feature centers generated by the
image dataset are more discriminative than that of the video dataset.
it is not
only because the lesion number of busi is larger than busv, but also because
the images in busi are all the keyframes that contain typical characteristics of
lesions.
furthermore, we plot the
relationships between the predicted attention values and the feature distances
to the centers.
we propose the coherence loss to guide the training of the
video model by the guidance of feature centers of the images.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf:
however, in clinical
medicine, doctors use slide-level labels to summarize patient-level labels
as a diagnostic result, indicating the involvement of three levels of patch,
slide, and patient in actual pathology image analysis, which we refer to
as the multi-level multi-instance learning (ml-mil) problem.
to address
this issue, we propose a novel and general framework called patients and
slides are equal (p&sre), inspired by the doctor’s diagnostic process of
repeatedly conﬁrming labels at the patient and slide level.
in this frame-
work, we treat patients and slides as instances at the same level and use
transformers and attention mechanisms to build connections between
them.
this allows for interaction between patient-level and slide-level
information and the correction of their respective features to achieve bet-
ter classiﬁcation performance.
the results show
that our method improves the performance of the baselines on both slide
and patient levels.
https://doi.org/10.1007/978-3-031-43904-9_7
64
f. li et al.
1
introduction
pathological image analysis is a vital area of research within medical image
analysis, focused on utilizing computer technology to aid doctors in diagnosing
and treating diseases by analyzing pathological tissue slide images [5]. advance-
ments in pathological image analysis have been made in early cancer diagnosis,
tumor localization, and grading, and treatment planning [3,10].
despite this, the clinical pathological analysis
presents certain challenges and complexities, with the ultimate diagnosis relying
on patients rather than slides.
speciﬁcally, in clinical problems of pathological image analysis, doctors usu-
ally summarize patient-level labels based on slide labels as the diagnostic results
[1,6].
for example, for the pathological discrimination diagnosis task of intesti-
nal tuberculosis(itb) and crohn’s desease(cd), the categories of postoperative
slides are divided into three types (normal, cd, itb), and doctors will summa-
rize the binary results of patients (itb or cd) based on slide-level labels [6].
similar situations exist in other tasks, such as the classiﬁcation of breast cancer
metastases in lymph nodes, where slide categories may have diﬀerent classiﬁca-
tions, and the corresponding diagnosis of the same patient is whether the cancer
has spread to the regional lymph nodes (n-stage)
1, actual pathological image analysis involves the relationships of patches,
slides, and patients, which is called a multi-level multi-instance learning (ml-
mil) problem.
among them, for patients and slides, patients are bags while
slides are instances, and for slides and patches, slides are bags while patches are
instances.
the second method is to treat slide-patient as a new mil problem
according to the traditional mil thinking, where slides are regarded as instances
and patient labels as bags.
although this method seems reasonable, the number
of patients is usually relatively small, and deep learning models usually require a
large amount of data for training.
therefore, the insuﬃcient number of samples
at the slide-patient level may make it diﬃcult for the model to learn enough
information.
to address the multi-level multi-instance learning (ml-mil) problem in med-
ical ﬁeld, we propose a novel framework called patients and slides are equal
(p&sre).
inspired by the iterative labeling process in medical diagnosis, this
framework treats patients and slides as instances at the same level and uses
transformers and attention mechanisms to build connections between them.
this
simple yet eﬀective method allows for interaction between patient-level and slide-
level information to correct their respective features and improve classiﬁcation
performance.
slide-level feature vectors; then, at the slide-patient level, we use self-attention
mechanisms to combine the slides of the same patient into patient-level feature
vectors, and treat these patient-level feature vectors together with all slide-level
feature vectors of the same patient as instances at the same level, which are
inputted into transformers for feature interaction and prediction of patient- and
slide-level labels.
our contributions include:
1) proposing a novel general framework to address the unique “patch-slide-
patient” ml-mil problem in the medical ﬁeld.
before this, no other frame-
work had directly tackled this speciﬁc problem, making our proposal a
ground-breaking step in the application of ml-mil in healthcare;
2) proposing a simple yet highly eﬀective method that leverages self-attention
mechanisms and transformer models to enhance the interaction between slide
and patient information.
this innovative approach not only improves the clas-
siﬁcation performance at the patient level but also at the slide level, show-
casing its eﬀectiveness and versatility;
3) conducting extensive experiments on two separate datasets.
the second part is the patient-slide level mil, which
generates patient-level features using attention mechanism and interacts the fea-
tures with transformer.
to enhance readability, we ﬁrst provide the following
symbolization for ml-mil: for a patient xi, it has a patient-level classiﬁcation
label yi.
for patient xi, there may exist ni slides si={sj|j=1 to ni}, where the
classiﬁcation label for each slide sj is denoted as zj.
here, i,j, and k are indices for
patient, slide, and patch levels, respectively.
fig.
this framework consists of two-
level mil parts: slide-patch level mil and patient-slide level mil
2.2
slide-patch level mil
our proposed framework has strong scalability as it can be based on any
attention-based mil method.
2.3
patient-slide level mil
after performing patch-slide level mil, we move on to patient-slide level mil.
in general mil algorithms, the patient is regarded as the bag and the slide as
the instance.
however, considering the diagnostic process in clinical practice,
we propose to treat both patients and slides as instances at the same level.
speciﬁcally, our p&sre framework for patient-slide level consists of two parts:
patient-level feature generation based on self-attention and patient-slide feature
interaction based on transformer [11].
patient-level feature generation based on self-attention.
therefore, we
directly use a fully connected (fc) layer to integrate the feature-level features
into patient-level features vi through attention mechanism, serving as patient
instances.
then, we perform a
weighted average of the vectors based on this weight to obtain the patient fea-
ture vi:
αj = fc({hj|j
= 1 to ni})
(2)
vj =
ni

j=1
αj ∗ hj/
ni

j=1
αj
(3)
patient-slide feature interaction based on transformer.
after doctors summarize the patient-level
results, they typically review the slides to double-check the diagnosis results.
this
patient-slide feature interaction (psfi) naturally lends itself to the construction
of a transformer, and information exchange and integration between slides and
patient level are bidirectional.
by
using the self-attention-based transformer structure, each input token is treated
equally (i.e., viewed as the same instance level), and tokens can interact exten-
sively with each other, enabling mutual correction between patients and slides
and even between slides.
speciﬁcally, we merge the slide feature set {hj} and
the patient feature vi into the input tokens t in
i
= {h1, h2, ..., hni, vi} = {t},
68
f. li et al.
and then input them into a multi-layer transformer through self-attention and
feed-forward neural network layers to obtain the interaction information between
slides and output tokens t out
i
:
βk,l = softmax(w qtk
t (w ktl)/
√
d)
(4)
tk =
ni+1

l=1
βk,lw v tl
(5)
t′
k = relu(tkw r + b1)w o + b2
(6)
where d is the dimension of the token, and tk and tl come from t in
i .
then, all output tokens are input into a shared
fc layer, and the patient’s predicted logits y ′
i and the predicted classiﬁcation
logits {z′
j|j
during training, we sampled one
patient at a time and pre-extracted their batch-level features for all slides, in
order to save gpu memory.
due to the issue of class imbalance in both slide
level and patient level, we use the lade
cd-itb is a private dataset consisting of 853 slides from 163
patients, with binary patient-level labels of cd or itb in a ratio of 103:60 and
tri-class slide-level labels of cd, itb, and normal slides in a ratio of 436:121:296,
respectively.
on average, there were 5 slides per patient.
we adopted a patient-level stratiﬁcation approach for 5-fold
cross-validation, with 20% of the training set randomly assigned as the valida-
tion set for each fold.
[1] is a publicly dataset, and its train-
ing set comprises 500 slides from 100 breast cancer patients with lymph node
metastases.
there
were 5 slides per patient on average.
the patients are divided into two groups
p&sre: a ml-mil framework for pathological image analysis
69
based on their pn stage, namely lymph node positive and lymph node negative,
in proportions of 24:76, respectively.
following the reference [4], we employed a transformer with 8
heads and 8 layers in the patient-slide feature interactions.
at the patient level,
we used two approaches for prediction: maxs, where the feature of the instance
that achieves the maximum positive probability from the slide-level mil model is
selected to patient-level model, and maxmins, where the mean value of features
of the maximum and minimum positive probability from the slide-level mil
model is selected to patient-level model.
the results of 5-fold cv at the slide and patient levels are reported in table 1
and table 2, respectively.
abmil with p&sre improves the f1 score
from 0.565 to 0.579 for the cd-itb dataset and from 0.529 to 0.571 for the
camelyon17 dataset at the slide-level, and improves the f1 score from 0.522
to 0.599 for the cd-itb dataset and from 0.842 to 0.861 for the camelyon17
dataset at the patient-level.
therefore, the ablation experiments demonstrate
the eﬀectiveness of p&sre in enhancing the classiﬁcation performance at both
the slide and patient levels.
[9]
57.0
57.3
57.3
56.9
55.4
63.8
63.8
50.5
(ours) dsmil + p&sre (w/o psfi)
56.7
57.4
57.4
56.6
55.5
64.6
64.6
51.7
(ours) dsmil + p&sre
58.5 59.1
59.1 57.3 55.8 66.4
66.4 52.3
table 2. patient-level 5-fold cv results (%)
method
cd-itb dataset
camelyon17 dataset
pre
recall acc
f1
pre
recall acc
f1
abmil (maxs)
36.9
63.3
46.6
46.6
78.0
94.4
75.0
85.0
abmil+ (maxmins)
38.2
65.0
48.5
48.2
78.3
94.7
76.0
85.7
abmil(baseline+mean) (on probabilities of slides) 38.2
43.3
53.4
40.6
98.3 75.0
80.0 85.1
(ours) abmil + p&sre (w/o psfi)
48.3
60.0
59.5
52.2
81.5
87.1
75.2
84.2
(ours) abmil + p&sre
56.8 71.0
66.3 59.9 78.0
96.1
76.4
86.1
dsmil + (maxs)
50.8
56.7
63.8
53.5
78.7
97.4
78
87.1
dsmil + (maxmins)
50.0
60.0
63.2
54.6
79.3
85.5
72
82.3
dsmil(baseline)+mean (on probabilities of slides)
43.8
57.7
53.3
48.1
100
73.7
80.0 84.9
(ours) dsmil + p&sre (w/o psfi)
49.7
61.7
62.8
54.9
78.0
97.9
77.4
86.8
(ours) dsmil + p&sre
60.0 56.7
69.7 57.7 80.2
96.8
79.4
87.7
4
limitations
our study has some limitations that should be addressed.
for instance, we did
not explore the possibility of treating patches as an equivalent level to slides and
patients.
the primary reason is that the vast number of patches required for
analysis is signiﬁcantly larger than that of slides and patients, which presents a
computational challenge for training.
we ﬁrst classify the process from patch to slide to the patient
in medical pathology diagnosis as a multi-level mil problem.
based on existing
state-of-the-art mil methods, we then extend the framework to p&sre, which
p&sre: a ml-mil framework for pathological image analysis
71
conducts feature extraction and interaction at the slide-patient level.
by intro-
ducing a transformer, the framework enables iterative interaction and correction
of information between patients and slides, resulting in better performance at
both the patient level and slide level compared to existing state-of-the-art algo-
rithms on two validation datasets.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf:
over the past few years,
the research community has devoted great eﬀort to understanding colonoscopy
videos using either optical ﬂow
then the enhanced anchor feature is fed into three detection heads composed of
a 3 × 3 conv and a 1 × 1 conv to produce center, size, and oﬀset features for
detection loss:
ldetection = lcenter
focal + λsizelsize
l1 + λoffloﬀset
l1
(6)
where lfocal is focal loss and ll1 is l1 loss.
[18], we select the foreground and background
region on both two frames guided by ground truth boxes to conduct contrastive
learning.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf:
apart
from measuring quality and monitoring adverse event, this task also serves in
facilitating education, statistical analysis, and evaluating surgical performance.
self-supervised learning for endoscopy
571
furthermore, the ability to recognize phases allows real-time monitoring and
decision-making assistance during surgery, thus improving patient safety and
outcomes.
3.1
masked siamese networks
ssl has become an active research area, giving rise to eﬃcient learning methods
such as simclr [7], swav
we compiled a dataset of laparoscopic procedures videos exclu-
sively performed on patients aged 18 years or older.
the dataset consists of 7,877
videos recorded at eight diﬀerent medical centers in israel.
we have curated a dataset comprising 13,979 colonoscopy videos
of patients aged 18 years or older.
these videos were recorded during standard
colonoscopy procedures performed at six diﬀerent medical centers between the
years 2019 and 2022.
each experiment is repeated three times with a random sample of train
videos, and we report the mean and standard deviation (shaded area).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf:
an automated bleeding risk rating system of gastric varices
(gv) aims to predict the bleeding risk and severity of gv, in order
to assist endoscopists in diagnosis and decrease the mortality rate of
patients with liver cirrhosis and portal hypertension.
to solve
this issue, we constructively introduce the segmentation of gv into the
classiﬁcation framework and propose the region-constraint module and
cross-region attention module for better feature localization and to learn
the correlation of context information.
we also collect a gv bleeding
risks rating dataset (gvbleed) with 1678 gastroscopy images from 411
patients that are jointly annotated in three levels of risks by senior clin-
ical endoscopists.
https://doi.org/10.1007/978-3-031-43904-9_1
4
y. jiang et al.
keywords: gastric varices · bleeding risk rating · cross-region
attention
1
introduction
esophagogastric varices are one of the common manifestations in patients with
liver cirrhosis and portal hypertension and occur in about 50 percent of patients
with liver cirrhosis [3,6].
the occurrence of esophagogastric variceal bleeding is
the most serious adverse event in patients with cirrhosis, with a 6-week acute
bleeding mortality rate as high as 15%–20% percent [14].
it is crucial to iden-
tify high-risk patients and oﬀer prophylactic treatment at the appropriate time.
several rating systems have
been proposed to describe gv based on the anatomical area.
[10] published a more detailed examination describing the
form, location, and color.
this may cause incon-
sistency or even misdiagnosis due to the variant experience of endoscopists in
diﬀerent hospitals.
first, the gv area may look like reg-
ular stomach rugae as it is caused by the blood vessels bulging and crumpling
up the stomach (see fig. 1).
also, since the gv images are taken from diﬀerent
distances and angles, the number of pixels of the gv area may not reﬂect its
actual size.
consequently, the model may fail to focus on the important gv
areas for prediction as shown in fig.
with the segmentation information, we further propose a
region-constraint module (rcm) and a cross-region attention module (cram)
for better feature localization and utilization.
the segmentation results to constrain the cam heatmaps of the feature maps
extracted by the classiﬁcation backbone, avoiding the model making predictions
based on incorrect areas.
in cram, the varices features are extracted using the
segmentation results and combined with an attention mechanism to learn the
intra-class correlation and cross-region correlation between the target area and
the context.
in this work, we collect a gv bleeding risks rating
dataset (gvbleed) that contains 1678 gastroscopy images from 411 patients
with diﬀerent levels of gv bleeding risks.
three senior clinical endoscopists are
invited to grade the bleeding risk of the retrospective data in three levels and
annotated the corresponding segmentation masks of gv areas.
in sum, the contributions of this paper are: 1) a novel gv bleeding risk rating
framework that constructively introduces segmentation to enhance the robust-
ness of representation learning; 2) a region-constraint module for better feature
localization and a cross-region attention module to learn the correlation of tar-
get gv with its context; 3) a gv bleeding risk rating dataset (gvbleed) with
high-quality annotation from multiple experienced endoscopists.
the framework consists of a segmentation module, a cross-region attention module,
and a region constraint module.
2, which consists
of a segmentation module (sm), a region constraint module (rcm), and a cross-
region attention module (cram).
then, the image together
with the mask are fed into the cram to extract the cross-region attentive
feature map, and a class activation map (cam) is calculated to represent the
concentrated regions through rcm.
finally, a simple classiﬁer is used to predict
the bleeding risk using the extracted feature map.
2.1
segmentation module
due to the large intra-class variation between gv with the same bleeding risk
and small inter-class variation between gv and normal tissue or gv with diﬀer-
ent bleeding risks, existing classiﬁcation models exhibit poor perform and tend
to lose focus on the gv areas.
[11] as the segmentation network, considering its great per-
formance, and calculate the diceloss between the segmentaion result mp and
ground truth mask of vaices region mgt for optimizing the network:
lse = 1 −
2σmp ∗ mgt
σm 2p
although such strategy can improve the classiﬁcation
performance, it may still lose focus in some hard cases where the gv area can
hardly be distinguished.
to further regularize the attention and fully utilize the
context information around the gv area, on top of the segmentation framework
we proposed the cross-region attention module and the region-constraint module.
2.2
cross-region attention module
inspired by the self-attention mechanism [17,18], we propose a cross-region atten-
tion module (cram) to learn the correlation of context information.
given the image i and the predicted varices mask mp, a feature
extraction step is ﬁrst performed to generate the image feature vm, the local
varices feature vvl and global varices feature vvg:
vm = fim(i),
vvl = fvl(i ∗ mp),
vvg = fvg(concat[i, mp]),
(2)
then, through similarity measuring, we can compute the attention with
a = (vvl)t vvg,
wij =
exp(aij)
σp(exp(apj)),
(3)
which composes of two correlations: self-attention over varices regions and cross-
region attention between varices and background regions.
then the cross-region attentive feature v is
fed into a classiﬁer to predict the bleeding risk.
2.3
region constraint module
to improve the focus ability of the model, we propose the region constraint
module (rcm) to add a constraint on the class activation map (cam) of the
classiﬁcation model.
after getting the cam, we regularize cam by calculating the dice loss between
the cam and ground truth mask of varices region lco.
2.4
network training
in our framework, we use the cross entropy loss as the classiﬁcation loss:
lcl = −
c

c=1
log
exp(pc)
σc
i=1exp(pi)yc
(5)
8
y. jiang et al.
table 1.
all of these cases are collected
from 411 patients in a grade-iii class-a hospital during the period from 2017
to 2022.
in the current version, images from patients with ages elder than 18 are
retained1.
4
experiments
4.1
implementation details
in experiments, the weights ωs, ωco, and ωcl of the segmentation loss, region
constraint loss, and classiﬁcation loss are set to 0.2, 1, and 1, respectively.
the
details of the three-step training are as follows: 1) segmentation module: we
trained the segmentation network for 600 epochs, using adam as the optimizer,
and the learning rate is initialized as 1e−3 and drops to 1e−4 after 300 epochs.
2) cross-region attention module and region constraint module: we
used the ground-truth varices masks and images as the inputs of the cram,
and jointly trained the cram and rcm for 100 epochs.
the blue areas represent the primary areas of concern.
with the proposed segmask
and rcm module, the network tends to focus on the important gastric varices areas
as endoscopists do.
besides, we further design a region-constraint module for better
feature localization and a cross-region attention module to learn the correlation
of target gv with its context.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf:
the development of a holistic approach to reducing bias
aggregationinmultimodalmedicaldataandpromotingequityinhealthcare
ishighlydemanded.racialdisparitiesexistinthepresentationanddevelop-
mentofalgorithmsforpulmonaryembolism(pe),anddeepsurvivalpredic-
tionmodelcanbede-biasedwithmultimodaldata.inthispaper,wepresent
a novel survival prediction (sp) framework with demographic bias disen-
tanglement for pe.
implicit biases can negatively aﬀect patient care, particu-
larly for marginalized populations with lower socioeconomic status [30]. evidence
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_50.
in an algorithm used to
predict healthcare costs, black patients who received the same health risk scores
as white patients were consistently sicker [21].
ai model quality relies on input
data and addressing bias is a crucial research area.
black patients exhibit a 50% higher age-standardized pe fatality rate and a
twofold risk for pe hospitalization than white patients [18,24].
hospitalized black
patients with pe were younger than whites.
racial disparities exist in pe and demonstrate the inequities
that aﬀect black patients.
(2) we proposed a de-biased
survival prediction framework with demographic bias disentanglement.
the pulmonary embolism dataset used in this study from 918 patients
(163 deceased, median age 64 years, range 13–99 years, 52% female), including
3978 ctpa images and 918 clinical reports, which were identiﬁed via retro-
spective review across three institutions.
for
each patient, the race labels, survival time-to-event labels and pesi variables
are collected from clinical data, and the 11 pesi variables are used to calcu-
late the pesi scores, which include age, sex, comorbid illnesses (cancer, heart
failure, chronic lung disease), pulse, systolic blood pressure, respiratory rate,
temperature, altered mental status, and arterial oxygen saturation at the time
of diagnosis
1, which compares the impact of diﬀerent population distributions.
the feature with the
highest pe probability from a patient’s multiple ctpas is considered as the most
pe-related visual representation.
next, the gatortron [29] model is employed
to recognize clinical concepts and identify medical relations for getting accurate
patient information from pe clinical reports.
[7]
that is trained to predict patient ranking using a multimodal combination of risk
predictions from the above three sp modules.
these coxph models calculate
the corresponding time-to-event evaluation and predict the fusion of patients’
risk as the survival outcome.
besides, clinical data in the form of text reports
and pesi variables objectively reﬂect the patient’s physiological information and
the physician’s diagnosis, exhibiting smaller race biases in correlation with sur-
vival across diﬀerent races.
the data from 3 institutions are randomly split
520
z. zhong et al.
table 1.
0.816
0.124 0.759
0.756
0.768
0.012
into training, validation, and testing sets, with a ratio of 7:1:2, the same ratio of
survival events is maintained in each institution.
the lung region of cpta images is extracted with a slice thickness of 1.25 mm
and scaled to n × 512 × 512 pixels [10].
the out-
puts from each patient’s medical history, clinical diagnosis, observations, and
radiologist impression are separately generated and concatenated to form the
1024 × 4 features.
the kaplan-meier (k-m) survival curve [14], as shown
in fig. 3, is used to compare the survival prediction between high-risk and low-
risk patient groups.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf:
it allows a model to learn the dependency
between various characteristics of text and image.
furthermore, we utilize our
module as a region of interest (roi) generator to classify the inﬂamma-
tion of the sacroiliac joints.
keywords: image segmentation · multi modal learning · cross
position attention · text-guided attention · medical image
1
introduction
advances in deep learning have been witnessed in many research areas over
the past decade.
in particular, segmentation which identify region of inter-
est (roi) in an automatic way is an essential medical imaging process.
among the popular architec-
tures, variants of u-net have been widely adopted due to their eﬀective encoder-
decoder structure, proﬁcient at capturing the characteristics of cells in images.
[10] generated the
positional characteristics of lesions or target objects as text labels.
here, the text semantics (t) can be a sentence indicating
the location or characteristics of an interested region in an image such as a lesion
shown in fig.
this module utilizes not
only the image feature map from the image encoder but also the global text
representation from the text encoder to learn the dependency between various
characteristics of text and image.
[8] contains
30 digital microscopic tissue images of several patients and qata-cov19 are
covid-19 chest x-ray images.
sij is the dataset privately prepared for this study which con-
sists of 804 mri slices of nineteen healthy subjects and sixty patients diagnosed
with axial spondyloarthritis.
figure 3 also shows that even on the qata-cov19 and
monuseg datasets, cpam t g predicted the most accurate segmentation masks
(see the red box areas).
as active sacroiliitis is a disease that occurs between the pelvic bone
and sacral bone, when a mr slice is input, the diagnostic system ﬁrst separates
the area around the pelvic bone into an roi patch and uses it as an input for the
active sacroiliitis classiﬁcation network [7].

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_3.pdf:
the rapid identiﬁcation and accurate diagnosis of breast can-
cer, known as the killer of women, have become greatly signiﬁcant for
those patients.
keywords: breast cancer · histopathological image · super-resolution ·
classiﬁcation · joint training
1
introduction
breast cancer is one of the high-mortality cancers among women in the 21st
century.
every year, 1.2 million women around the world suﬀer from breast
cancer and about 0.5 million die of it
https://doi.org/10.1007/978-3-031-43904-9_3
24
l. xie et al.
will make a correct assessment of the patient’s risk and improve the chances
of survival.
considering the improvement of histopathological images’ acquisition equipment
will cost lots of money while signiﬁcantly increasing patients’ expense of detec-
tion.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf:
contextual information pro-
vides comprehensive information about nodules such as location, shape,
and peripheral vessels, and experienced radiologists can search for clues
from previous cases as a reference to enrich the basis of decision-making.
[10] trained
a 3d region proposal network to detect suspicious nodules and then selected the
top ﬁve to predict the probability of lung cancer for the whole ct scan, instead
of each nodule.
for the ldct, we annotate more than 12,852 nodules from 8,271 patients
from the nlst dataset
for the ncct, we annotate over 4,029 nodules from
over 2,565 patients from our collaborating hospital.
besides, positional encoding is added in a learnable manner to retain location
information.
as for a group of nodules, we cluster them into n groups {c1, ..., cn} by
towards accurate lung nodule malignancy prediction like radiologists
203
minimizing the objective function n
i=1

p∈ci d(p, pi) where d is the euclidean
distance function and p represents the nodule embedding, and refer the cen-
ter of each cluster, pi =
1
|ci|

p∈ci p, as its prototype.
updating prototype online: the prototypes are updated in an online man-
ner, thereby allowing them to adjust quickly to changes in the nodule represen-
tations.
there are 8,271 patients
enrolled in this study.
3
i=1 cls loss(y, pi)
▷ update loss
18: end for
patient, and localized and labeled the nodules in the scan as benign or malignant
based on the rough candidate nodule location and whether the patient develops
lung cancer provided by nlst metadata.
the in-house cohort was retrospectively collected
from 2,565 patients at our collaborating hospital between 2019 and 2022.
[21].
train-val-test: the training set contains 9,910 (9,413 benign and 497 malig-
nant) nodules from 6,366 patients at nlst, and 2,592 (843 benign and 1,749
malignant) nodules from 2,113 patients at the in-house cohort.
the validation
set contains 1,499 (1,426 benign and 73 malignant) nodules from 964 patients
at nlst.
the nlst test set has 1,443 (1,370 benign and 73 malignant) nod-
ules from 941 patients.
the in-house test set has 1,437 (1,298 benign and 139
malignant) nodules from 452 patients.
evaluation metrics: the area under the receiver operat-
ing characteristic curve (auc) is used to evaluate the malignancy prediction
performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf:
accurately classifying the histological subtype of non-small cell lung
cancer (nsclc) using computed tomography (ct) images is critical for clinicians
in determining the best treatment options for patients.
we formulate the unique encoders as follows:
hp
υ = ep
υ(iυ), υ ∈ {aυ, cυ, sυ}
(4)
where ep
v(·) is the encoder function dedicated to capture single-view characteristics.
exclusion criteria involves patients diag-
nosed with large cell carcinoma or not otherwise speciﬁed, along with cases that have
contouring inaccuracies or lacked tumor delineation
we evaluate the
performance of nsclc classiﬁcation in ﬁve-fold cross validation on the nsclc-tcia
dataset, and measure accuracy (acc), sensitivity (sen), speciﬁcity (spe), and the area
under the receiver operating characteristic (roc) curve (auc) as evaluation metrics.
then one
128 × 128 pixel slice is cropped from each view as input based on the center of the
tumor.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf:
given
the complex tissue characteristics of pathological whole slide images (wsi), it
is crucial to develop adaptation strategies allowing (1) training data eﬃciency,
and (2) data fusion ﬂexibility for pathological image analysis.
language models prove
to be eﬀective in capturing semantic characteristics with a lower data acquisition
and annotation cost in medical areas [12].
prompt tuning proves to be an
eﬃcient adaptation method for both vision and language models [22,23]. orig-
inating from natural language processing, “prompting” refers to adding (man-
ual) text instructions to model inputs, whose goal is to help the pre-trained
model better understand the current task.
the evaluation metric is patient-wise accuracy, where the prediction of a wsi
is obtained by a soft vote over the patches, and accuracy is averaged class-wise.
implementation.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf:
2.2
contrastive learning
the characteristics of self-supervised learning are deﬁning the proxy objective
or addressing pretext tasks using pseudo labels for the unlabeled instances.
panda-mil collects the eosin-stained biopsies
with region-based masks indicating the benign (normal) and cancerous (abnor-
mal) tissue, combined by stroma and epithelium.
we follow the previous methods [4,13,14] to employ the instant-level
area under curve (auc) and the average precision (ap) for a fair comparison.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf:
pancreatic ductal adenocarcinoma (pdac) is a highly lethal
cancer in which the tumor-vascular involvement greatly aﬀects the
resectability and, thus, overall survival of patients.
this paper proposes a novel learnable neural distance that describes the
precise relationship between the tumor and vessels in ct images of dif-
ferent patients, adopting it as a major feature for prognosis prediction.
we extensively evaluated
and compared the proposed method with existing methods in the multi-
center (n = 4) dataset with 1,070 patients with pdac, and statistical
h. dong—work was done during an internship at alibaba damo academy.
https://doi.org/10.1007/978-3-031-43904-9_24
242
h. dong et al.
analysis conﬁrmed its clinical eﬀectiveness in the external test set con-
sisting of three centers.
the developed risk marker was the strongest pre-
dictor of overall survival among preoperative factors and it has the poten-
tial to be combined with established clinical factors to select patients at
higher risk who might beneﬁt from neoadjuvant therapy.
neoadjuvant
chemotherapy can increase the likelihood of achieving a margin-negative resec-
tion and avoid unnecessary surgery in patients with aggressive tumor types [23].
providing accurate and objective preoperative biomarkers is crucial for triaging
patients who are most likely to beneﬁt from neoadjuvant chemotherapy.
how-
ever, current clinical markers such as larger tumor size and high carbohydrate
antigen (ca) 19-9 level may not be suﬃcient to accurately tailor neoadjuvant
treatment for patients [19].
therefore, multi-phase contrast-enhanced ct has a
great potential to enable personalized prognostic prediction for pdac, lever-
aging its ability to provide a wealth of texture information that can aid in the
development of accurate and eﬀective prognostic models [2,10].
previous studies have utilized image texture analysis with hand-crafted fea-
tures to predict the survival of patients with pdacs [1], but the representational
fig.
1. two examples of spatial information between vessel (orange region) and tumor
(green region).
the minimum distance, which refers to the closest distance between the
superior mesenteric artery (sma) and the pdac tumor region, is almost identical in
these two cases.
our proposed model has the
potential to be used in combination with clinical factors for risk stratiﬁcation
and treatment decisions for patients with pdac.
2.2
neural distance: positional and structural information
between pdac and vessels
the vascular involvement in patients with pdac aﬀects the resectability and
treatment planning [5].
in this study, we used data from shengjing hospital to train our
method with 892 patients, and data from three other centers, including guang-
dong provincial people’s hospital, tianjin medical university and sun yat-
sen university cancer center for independent testing with 178 patients.
pdac masks for 340 patients were manually labeled by a radiol-
ogist from shengjing hospital with 18 years of experience in pancreatic cancer,
while the rest were predicted using self-learning models [11,24] and checked by
the same annotator.
we also reported the survival auc, which esti-
mates the cumulative area under the roc curve for the ﬁrst 36 months.
combining the texture-aware
transformer and regular structure information improved the results from 0.630
to 0.648, as tumor invasion strongly aﬀects the survival of pdac patients.
nested 5-fold cv (n = 892)
independent test (n = 178)
c-index
auc
c-index auc
3dcnn-p [12]
0.630 ± 0.009
0.668 ± 0.019
0.674
0.740
early fusion [17]
0.635 ± 0.011
0.670 ± 0.024
0.696
0.779
deepct-pdac [21] 0.640 ± 0.018
0.680 ± 0.036
0.697
0.773
ours
0.656 ± 0.017 0.695 ± 0.023 0.710
0.792
patterns and tumor-vascular involvement, demonstrated its eﬀectiveness with
better performance in both nested 5-fold cross-validation and the multi-center
independent test set.
in table 3, we used univariate and multivariate cox proportional-hazards
models to evaluate our signature and other clinicopathologic factors in the inde-
pendent test set.
to demonstrate the added value of our
signature as a tool to select patients for neoadjuvant treatment before surgery,
we plotted kaplan-meier survival curves in fig.
3. we further stratify patients by
our signature after grouping them by tumor size and ca19-9, two clinically used
preoperative criteria for selection, and also age.
our signature could signiﬁcantly
stratify patients in all cases and those in the high-risk group had worse outcomes
and might be considered as potential neoadjuvant treatment candidates (e.g. 33
high-risk patients with larger tumor size and high ca19-9).
independent test set (n = 178)
univariate analysis
multivariate analysis
hr (95% ci)
p-value
hr (95% ci)
p-value
proposed (high vs low risk)
2.42(1.64-3.58)
<0.0001
1.85(1.08-3.17)
0.027
age (> 60 vs = 60)
1.49(1.01-2.20)
0.043
1.01(0.65-1.58)
0.888
sex (male vs female)
1.28(0.86-1.90)
0.221
-
-
pt (pt3-pt4 vs pt1-pt2)
3.17(2.10-4.77)
<0.0001
2.44(1.54-3.86)
0.00015
pn (positive ve negative)
1.47(0.98-2.20)
0.008
1.34(0.85-2.12)
0.210
resection margin (r1 vs r0)
2.84(1.64-4.93)
<0.0001
1.68(0.92-3.07)
0.091
ca19-9 (> 210 vs ≤ 210 u/ml)
0.94(0.64-1.39)
0.759
-
-
tumor size (> 25 vs ≤ 25 mm)
2.36(1.59-3.52)
<0.0001
0.99(0.52-1.85)
0.963
tumor location (head vs tail)
1.06(0.63-1.79)
0.819
-
-
fig.
3. kaplan-meier analyses of overall survival according to the proposed signature in
all patients in the independent test set (n = 178) and subgroups deﬁned by preoperative
factors.
high risk group indicated by the proposed method is the potential patient group
that could beneﬁt from neoadjuvant treatment before surgery.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf:
to address this
issue, we establish a framework to adjust cnns to “think like sonog-
raphers” for gout diagnosis, which consists of three novel components:
(1) where to adjust: modeling sonographers’ gaze map to emphasize
the region that needs adjust; (2) what to adjust: classifying instances
to systematically detect predictions made based on unreasonable/biased
reasoning and adjust; (3) how to adjust: developing a training mecha-
nism to balance gout prediction accuracy and attention reasonability for
improved cnns.
keywords: musculoskeletal ultrasound · gout diagnosis · gaze
tracking · reasonability
1
introduction
gout is the most common inﬂammatory arthritis and musculoskeletal ultrasound
(mskus) scanning is recommended to diagnose gout due to the non-ionizing
radiation, fast imaging speed, and non-invasive characteristics of mskus
[7].
however, misdiagnosis of gout can occur frequently when a patient’s clinical
characteristics are atypical.
yellow boxes denote the gaze areas of the sonographers and
red arrows denote the surrounding fascial tissues; (b) grad-cam visual of resnet18;
(c) grad-cam visual of densenet121; (d) grad-cam visual of our method.
firstly, the
gout-characteristics contain various types including double contour sign, syn-
ovial hypertrophy, synovial eﬀusion, synovial dislocation and bone erosion, and
these gout-characteristics are small and diﬃcult to localize in mskus.
secondly,
the surrounding fascial tissues such as the muscle, sarcolemma and articular
capsule have similar visual traits with gout-characteristics, and we found the
existing cnn models can’t accurately pay attention to the gout-characteristics
that radiologist doctors pay attention to during the diagnosis process (as shown
in fig.
in medical image analysis, recent works have attempted to inject the recorded
gaze information of clinicians into deep cnn models for helping the models to
predict correctly based on lesion area.
(1)
where to adjust: modeling sonographers’ gaze map to emphasize the region that
needs adjust; (2) what to adjust: classify the instances to systemically detect
predictions made based on unreasonable/biased reasoning and adjust; (3) how
to adjust: developing a training mechanism to strike the balance between gout
prediction accuracy and attention reasonability.
2
method
fig.
1) where to adjust: we model
the sonographers’ gaze map to emphasize the region that needs control.
3)
how to adjust: a training mechanism is developed to strike the balance between
gout diagnosis and attention accuracy for improving cnn.
2.1
where to adjust
it is essential to obtain the gaze map corresponding to each mskus to emphasize
the region where gouty features are obvious.
inspired by cam technique, it is needed to decide whether the
attention region given to an cnn model is reasonable for diagnosis of gout.
we ﬁrstly use the grad-cam technique [12] to acquire the salient attention
region scam that cnn model perceives for diﬀerential diagnosis of gout.
to
ensure the scale of the attention region scam is the same as the sonographers’
gaze map ssono which is modeled by saliency model, we normalize scam to
the values between 0 and 1, get scam.
3. four categories: (a) rp (b)
rip (c) up (d) uip. yellow boxes
denote the gaze areas of the sono-
graphers.
rp: reasonable precise: the attention
region focusses on the gouty features which
are important for sonographers’ decision, and
the diagnosis is precise.
rip:
reasonable
imprecise: although
attention region focusses on the gouty fea-
tures, while the diagnosis result is imprecise.
uip: unreasonable imprecise: the atten-
tion region focusses on irrelevant features, and
the diagnosis is imprecise.
in
this way, cnns not only ﬁnish correct gout
diagnosis, but also acquire the attention
region that agreements with the sonographers’
gaze map.
2.3
how to adjust
we proposed a training mechanism (algorithm 1) which can strike the balance
between the gout diagnosis error and the reasonability error of attention region
to promote the cnns to “think like sonographers”.
in addition to reducing the
diagnosis error, we also want to minimize the diﬀerence between sonographers’
gaze map ssono and normalized salient attention region scam, which directly
leads to our target:
the mskus data were collected for patients
suspected of metatarsal gout in nanjing drum tower hosptial.
dataset totally contains 1127
us images from diﬀerent patients including 509 gout images and 618 healthy
images.
five metrics were used to evaluate model performance:
accuracy (acc), area under curve (auc), correlation coeﬃcient (cc), sim-
ilarity (sim) and kullback-leibler divergence (kld)
acc and auc were
implemented to assess the gout classiﬁcation performance of each model, while
cc, sim, and kld were used to evaluate the similarity of the areas that the
model and sonographers focus on during diagnoses.
this
indicated that the models with tls focused on the areas shown to be similar to
the actual sonographers.
with tls, however, models could focus on the
crucial areas in lesions, allowing them to think like sonographers.
yellow
boxes denote sonographers’ gaze areas and red arrows denote the fascial tissues.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf:
in addition, we
employ a group of conﬁdence maps with a series of ﬂexible sigmoid
alpha functions (saf) to aware and describe the inﬁltrative area of
microvessel for enhancing diagnosis.
https://doi.org/10.1007/978-3-031-43987-2_17
170
h. han et al.
keywords: contrast-enhanced ultrasound · thyroid nodule ·
dynamic perfusion · inﬁltration awareness
1
introduction
contrast-enhanced ultrasound (ceus) as a modality of functional imaging has
the ability to assess the intensity of vascular perfusion and haemodynamics in
the thyroid nodule, thus considered a valuable new approach in the determi-
nation of benign vs. malignant nodules [1].
according to clinical experience, for thyroid nodules diagnosis,
there are two characteristic that are important when analyzing ceus video.
radiologists identify the area
of lesions by comparing various frames, but each individual frame does not eﬀectively
describe the lesions area with precision.
the red border represents the area of the lesion.
the
yellow line indicates the lesion area labeled by radiologists on gray us, while the red
arrow corresponds to the inﬁltration area or continuous lesion expansion on ceus.
the overall architecture which contains four parts: cross-task shared feature
extraction, temporal-based lesions area recognition, microvessel inﬁltration awareness
and thyroid nodules diagnosis.
even the sota segmentation methods cannot accurately identify the lesion
area for blurred lesion boundaries, thus, the existing automatic diagnosis network
using ceus still requires manual labeling of pixel-level labels which will lose key
information around the tissues
whether the awareness of inﬁltrative area information can
be helpful in the improvement of diagnostic accuracy is still unexplored.
here, we propose an explanatory framework for the diagnosis of thyroid nod-
ules based on dynamic ceus video, which considers the dynamic perfusion
characteristics and the ampliﬁcation of the lesion region caused by microves-
sel inﬁltration.
second, we adopt
a group of conﬁdence maps instead of binary masks to perceive the inﬁltrative
expansion area from gray us to ceus of microvessels for improving diagnosis.
the tasks of
lesion area recognition and diﬀerential diagnosis are pixel-level and image-level
classiﬁcations, and some low-level features of these two tasks can be shared inter-
actively [8].
we ﬁrst fed the ceus video i ∈ rc×t ×h×w into the cross-task
feature extraction (cfa) module to jointly generate the features fiden and fcls
for lesion area recognition and diﬀerential diagnosis, respectively.
after that,
in the temporal-based lesions area recognition (tlar) module, an enhanced
v-net with the tpa is implemented to identify the relatively clear lesion area
which are visible on both gray us and ceus video.
because microvessel inva-
sion expansion causes the tumor size and margin depicted by ceus video to be
larger than that of gray us, we further adopt a group of conﬁdence maps based
on sigmoid alpha functions (saf) to aware the inﬁltrative area of microvessels
for improving diagnosis.
multiple receptive ﬁelds are obtained through
diﬀerent branches, and then group normalization and relu activation are per-
formed to obtain multi-scale features fmuti.
then, we use the cross-task feature
adaptive unit to generate the features fiden and fcls required for lesions area
recognition and thyroid nodules diagnosis via the following formula [10]:
2.1
temporal-based lesions area recognition (tlar)
the great challenge of automatic recognition of lesion area from ceus video is
that the semantic information of the lesion area is diﬀerent in the ceus video of
the diﬀerent microvessel perfusion periods.
thus, the interactive fusion of semantic information
of the whole microvessel perfusion period will promote the identiﬁcation of the
lesion area, and we design the temporal projection attention (tpa) to realize
this idea.
after the temporal projection,
a group convolution with a group size of 4 is employed on k to extract the
local temporal attention l ∈ rc× h
16 × w
16 .
(3)
where gonv(·) is the group convolution, σ denotes the normalization, “⊕” is the
concatenation operation.
2.2
microvessel inﬁltration awareness (mia)
we design a mia module to learn the inﬁltrative areas of microvessel.
it is generally believed that the diﬀeren-
tiation between benign and malignant thyroid nodules is related to the pixels
around the boundaries of the lesion [14], especially in the inﬁltrative areas of
microvessel [3].
therefore, we ﬁrstly build the initial probability distribution pd
based on the distance between the pixels and the annotation boundaries by using
saf in order to aware the inﬁltrative areas.
i ∈ (1, n]
(7)
where “⊕” represents the concatenation operation;convblock consists of a
group of asymmetric convolutions (e.g., conv1 × 5, conv5 × 1 and conv1 × 1); n
denotes the number of the layers of ipo unit.
in this way, we can get
a group of probability map ˆp to aware the microvascular inﬁltration.
our dataset contained 282 consecutive patients who underwent thy-
roid nodule examination at nanjing drum tower hospital.
all patients per-
formed dynamic ceus examination by an experienced sonographer using an
iu22 scanner (philips healthcare, bothell, wa) equipped with a linear trans-
ducer l9-3 probe.
all data were approved by the institu-
tional review board of nanjing drum tower hospital, and all patients signed
the informed consent before enrollment into the study.
for the lesion area recognition task, our method achieved the highest dice of
85.54% and recall of 90.40%, and the visualized results were shown in fig.
a1, although sota method fails to focus on
lesion areas, our method can pinpoint discriminating lesion areas.
3. (a)comparison of the visualised results with the sota method, green and red
contours is the automatically recognized area and ground-truth.
therefore, for balancing the eﬃciency and performance, the
number of ipo was set as n = 3 and α was set as α = {1, 5, 9} to generate a
group of conﬁdence maps that can simulate the process of microvessel inﬁltration.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf:
survival prediction is crucial for cancer patients as it provides early
prognostic information for treatment planning.
however, existing deep survival models are not well devel-
oped in utilizing multi-modality images (e.g., pet-ct) and in extracting region-
speciﬁc information (e.g., the prognostic information in primary tumor (pt) and
metastatic lymph node (mln) regions).
this framework has a merging encoder to fuse multi-modality information and a
diverging decoder to extract region-speciﬁc information.
in the diverging decoder, we propose a region-speciﬁc attention gate
(rag) block to screen out the features related to lesion regions.
our xsurv combines the complementary information
in pet and ct images and extracts the region-speciﬁc prognostic information
in pt and mln regions.
https://doi.org/10.1007/978-3-031-43987-2_39
merging-diverging hybrid transformer networks
401
1
introduction
head and neck (h&n) cancer refers to malignant tumors in h&n regions, which is
among the most common cancers worldwide [1]. survival prediction, a regression task
that models the survival outcomes of patients, is crucial for h&n cancer patients: it pro-
vides early prognostic information to guide treatment planning and potentially improves
the overall survival outcomes of patients [2].
therefore, survival prediction from pet-ct images in h&n cancer has
attracted wide attention and serves as a key research area.
[22] attempted to address this limitation by proposing a multi-scale
non-local attention fusion (mnaf) block for survival prediction of glioma patients,
in which multi-modality features were fused via non-local attention mechanism [23] at
multiple scales.
secondly, although deep survival models have advantages in performing end-to-end
survival prediction without requiring tumor masks, this also incurs difﬁculties in extract-
ing region-speciﬁc information, such as the prognostic information in primary tumor
402
m. meng et al.
(pt) and metastatic lymph node (mln) regions.
our xsurv has a merg-
ing encoder to fuse complementary anatomical and metabolic information in pet and
ct images and has a diverging decoder to extract region-speciﬁc prognostic informa-
tion in pt and mln regions.
this
framework is specialized in leveraging multi-modality images and extracting region-
speciﬁc information, which potentially could be applied to many survival prediction
tasks with multi-modality imaging.
(iii) we propose a region-speciﬁc attention gate (rag) block
for region-speciﬁc feature extraction, which screens out the features related to lesion
regions.
merging-diverging hybrid transformer networks
403
2
method
figure 1 illustrates the overall architecture of our xsurv, which presents an x-shape
architecture consisting of a merging encoder for multi-modality feature learning and a
diverging decoder for region-speciﬁc feature extraction.
in this study, we extend this idea to multi-
modality feature learning, which parallelly aggregates global inter-modality and local
intra-modality information via hpca blocks, to discover inter-modality interactions
while preserving intra-modality characteristics.
the detailed architecture of the proposed (a) hybrid parallel cross-attention (hpca)
block and (b) region-speciﬁc attention gate (rag) block.
2.2
pt-mln diverging decoder
as shown in fig.
in addition, clinical indicators (e.g., age, gender) also can be integrated by
the coxph model.
3
experimental setup
3.1
dataset and preprocessing
we adopted the training dataset of hecktor 2022 (refer to https://hecktor.grand-cha
llenge.org/), including 488 h&n cancer patients acquired from seven medical centers
each patient underwent pretreatment pet/ct and has clinical indicators.
the patients from two centers (chum and chuv) were
used for testing and other patients for training, which split the data into 386/102 patients
in training/testing sets.
each image was cropped to 160 × 160 × 160 voxels with the tumor located
in the center.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf:
https://doi.org/10.1007/978-3-031-43987-2_76
nasdm: nuclei-aware histopathology image generation
787
be used to generate histopathology images with speciﬁc characteristics, such as
visual patterns identifying rare cancer subtypes [4].
as there are substantial visual variations
across images, we construct a representative test set by randomly sampling a
7.5% area from each image and its corresponding mask to be held-out for test-
ing.
further, future works can
explore generation of patches conditioned on neighboring patches, as this enables
generation of larger tissue areas by composing patches together.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf:
https://doi.org/10.1007/978-3-031-43987-2_62
deep learning for prostate tumor-associated stroma identiﬁcation
643
biopsies and the patient has organ-conﬁned cancer with no contraindications,
radical prostatectomy (rp) is the standard of care
radical
prostatectomy histopathology samples are essential for validating the biopsy-
determined grade group [5,6].
machine learning algorithms have been used to quantify
the percentage of tumor to stroma in bladder cancer patients, but required
dichotomizing patients based on a threshold
software has been used to seg-
ment tumor and stroma tissue in breast cancer patient samples, but the method
required constant supervision by a pathologist [15].
given the spatial nature of cancer
ﬁeld eﬀect and tumor microenvironment, our graph-based method oﬀers valu-
able insights into stroma region analysis.
(1) dataset a comprises 513 tiles extracted from the whole mount slides of 40
patients, sourced from the archives of the pathology department at cedars-
sinai medical center (irb# pro00029960).
it combines two sets of tiles: 224
images from 20 patients featuring stroma, normal glands, low-grade and high-
grade cancer
[22], along with 289 images from 20 patients with dense high-grade
cancer (gleason grades 4 and 5) and cribriform/non-cribriform glands [23].
the prostate tissue within these
slides had an average tumor area proportion of 9%, with an average tumor area of
77 square mm.
an expert pathologist annotated the tumor region boundaries at
the region-level, providing exhaustive annotations for all tumor foci.
(3) dataset
c comprised 6134 negative biopsy slides obtained from 262 patients’ biopsy pro-
cedures, where all samples were diagnosed as negative.
this model was then
applied to generate stroma masks for all slides in datasets b and c. to precisely
isolate stroma tissues and avoid data bleeding from epithelial tissues, we only
extracted patches where over 99.5% of the regions were identiﬁed as stroma at
40x magniﬁcation to construct the stroma classiﬁcation dataset.
for positive tumor-associated stroma patches, we sampled patches near
tumor glands within annotated tumor region boundaries, as we presumed that
tumor regions represent zones in which the greatest amount of damage has pro-
gressed.
for negative stroma patches, we calculated the tumor distance for each
patch by measuring the euclidean distance from the patch center to the nearest
edge of the labeled tumor regions.
negative stroma patches were then sampled
from whole mount slides with a gleason group smaller than 3 and a tumor dis-
tance larger than 5 mm.
we measure the prediction performance using the area under the receiver
operating characteristic (auroc), f1 score, precision, and recall.
4
results and discussions
table 1.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf:
firstly, we use a vanilla
retinanet to detect top-k suspicious cells and extract region-of-interest
(roi) features.
keywords: cervical abnormal cell detection · consistency learning ·
cervical cytologic images
1
introduction
cervical cancer is the second most common cancer among adult women.
nevertheless, delayed
diagnosis of cervical cancer until an advanced stage will have a negative impact
on patient prognosis and consume medical resources.
although tct has been widely used in clinical applications and
has signiﬁcantly reduced the mortality rates caused by cervical cancer, it is still
unavailable for population-wide screening
2) conventional object
detection methods intend to directly extract the feature from the object area
to locate and classify the object simultaneously.
the proposed pcn fc(·) takes
the top-k patches as inputs, which are cropped from original images according
to the proposal location, denoted as ip = cr(i, p), where cr(·) denotes the crop
function, i and p denote input image and proposal boxes predicted by fd(·),
respectively.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_77.pdf:
in this work we present, triangular anal-
ysis of geographical interplay of lymphocytes (triangil), a novel app-
roach involving building of heterogeneous subgraphs to precisely cap-
ture the spatial interplay between multiple cell families.
the triaangil’s eﬃcacy for
microenvironment characterization from qif images is demonstrated in
problems of predicting (1) response to immunotherapy (n = 122) and
(2) overall survival (n = 135) in patients with lung cancer in comparison
with four hand-crafted approaches namely dentil, gg, ccg, spatil,
and deep learning with gnn.
immunotherapy
(io) is the standard treatment for patients with advanced non-small cell lung
cancer (nsclc)
[19] but only 27–45% of patients respond to this treatment [21].
therefore, better algorithms and improved biomarkers are essential for identify-
ing which cancer patients are most likely to respond to io in advance of treat-
ment.
in this study, we introduce a novel approach called
triangular analysis of geographical interplay of lymphocytes (triangil), rep-
resenting a unique and interpretable way to characterize the distribution, and
higher-order interaction of various cell families (e.g., cancerous cells, stromal
cells, lymphocyte subtypes) across digital histopathology slides.
we demonstrate
the eﬃcacy of triaangil for characterizing tme in the context of predicting
1) response to io with immune checkpoint inhibitors (ici), 2) overall survival
(os), in patients with nsclc, and 3) providing novel insights into the spa-
tial interplay between diﬀerent immune cell subtype.
2
previous related work and novel contributions
many studies have only looked at the density of a single biomarker (e.g. tils),
to show that a high density of tils is associated with improved patient sur-
vival and treatment response in nsclc
[16] to create multiple
nuclear subgraphs based on cell-to-cell proximity to predict tumor aggressive-
ness and patient outcome [16].
all of these approaches point to overwhelming
evidence that spatial architecture of cells in tme is critical in predicting cancer
triangular analysis of geographical interplay of lymphocytes (triangil)
799
outcome.
this in turn allows for development of
machine classiﬁers to predict outcome and response in lung cancer patients
treated with io.
(2) triangil includes a set of quantitative metrics that capture the interplay
within and between nuclei corresponding to diﬀerent types/families.
next, we call gettrianglefeatures() function to quantify
triangular relationships by extracting features from the resulting subgraphs
(e.g. perimeter and area of triangles; complete list of features in supplemental
table 1).
4
experimental results and discussion
4.1
dataset
the cohort employed in this study was composed of pre-treatment tumor biopsy
specimens from patients with nsclc from ﬁve centers (two centers for training
triangular analysis of geographical interplay of lymphocytes (triangil)
801
algorithm 1: finding triangles
input: a jagged array del : delaunay graph with three vertices of every
triangle in each row, a hashmap φ : maps nodes to their type
output: triangle features trifeatset
let triindex ← ∅ be the list for triangle indices
for i = 1 to i = length(del) do
let marker ← ∅ be a auxiliary list to keep the viewed markers
for j = 1 to 3 do
if φ(del(i, j))
== 3 then
triindex ← triindex  i
end
end
trifeatset ←gettrianglefeatures(del(triindex, :))
return trifeatset
(st) and three centers for independent validation (sv)).
the entire analysis was
carried out using 122 patients in experiment 1 (73 in st, and 49 in sv) and
135 patients in experiment 2 (81 in st, and 54 in sv).
til density (dentil): for every patient, multiple density measures includ-
ing the number of diﬀerent cells types and their ratios are calculated
architectural
features (e.g., perimeter, triangle area, edge length) were then calculated on
these global graphs for each patient.
ccg: for every patient, subgraphs are built on nuclei regardless of their type
and only based on their euclidean distance.
(c1) a zoomed-in region showing the edges based on
euclidean distances.
(d1)
shows a zoomed-in region.
(d2) shows the same region in the pruned delaunay sub-
graph.
(e1) a zoomed-in region showing the cd4+-tumor-stroma triangles.
spatil: for each patient, ﬁrst, subgraphs are built on individual cell types
based on a distance parameter.
after selecting every two cell types, features are extracted from their
convex hulls (e.g. the number of clusters of each cell type, area intersected
between clusters [9]; complete list of combinations in supplemental table 3).
here, for each tile in the slide, a delaunay graph was constructed regardless of cell
subtypes, and tile-level feature representations (e.g.side length minimum, maxi-
mum, mean, and standard deviation, triangle area minimum, maximum, mean,
and standard deviation) were aggregated by a transformer according to their spa-
tial arrangement [31].
4.3
experiment 1: immunotherapy response prediction in lung
cancer
design: triangil was also trained to diﬀerentiate between patients who
responded to io and those who did not.
for our study, the responders to io
were identiﬁed as those patients with complete response, partial response, and
triangular analysis of geographical interplay of lymphocytes (triangil)
803
stable disease, and non-responders were patients with progressive disease.
a
linear discriminant analysis (lda) classiﬁer was trained on st to predict which
patients would respond to io.
the ability to identify responders post-io was assessed
by the area under the receiver operating characteristic curve (auc) in sv.
results: the two top predictive triangil features were found to be the num-
ber of edges between stroma and cd4+ cells, and the number of edges between
stroma and tumor cells with more interactions between stromal cells and both
cd4+ and tumor cells being associated with response to io.
2. visual illustration of the qualitative diﬀerence in feature representations
between nsclc patients with short-term (top) and long-term survival (lower row).
the third
column shows a zoomed-in region.
(color ﬁgure online)
804
s. arabyarmohammadi et al.
4.4
experiment 2: predicting survival in lung cancer patients
treated with immunotherapy
design: st was used to construct a least absolute shrinkage and selection oper-
ator (lasso)
[6] using the
triangil features, to obtain risk score for each patient.
the median risk score in st was used as a thresh-
old in both st and sv to dichotomize patients into low-risk/high-risk categories.
the c-index evaluates the correlation between risk predictions
and survival times, aiming to maximize the discrimination between high-risk and
low-risk patients [11].
os is the time between the initiation of io to the death
of the patient.
the patients were censored if the date of death was unknown.
result: figure 2 presents some triangil features in a ﬁeld of view for a patient
with long-term survival and another with short-term survival.
(color ﬁgure online)
triangular analysis of geographical interplay of lymphocytes (triangil)
805
were 0.64, and 0.63 respectively.
5
concluding remarks
we presented a new approach, triangular analysis of geographical interplay of
lymphocytes (triangil), to quantitatively chartacterize the spatial arrange-
ment and relative geographical interplay of multiple cell families across patho-
logical images.
triangil was predic-
tive of response after io (n = 122) and also demonstrated a strong correlation
with os in nsclc patients treated with io (n = 135).
research reported in this publication was supported by the
national cancer institute under award numbers r01ca268287a1, u01 ca269181,
r01 ca26820701a1, r01ca249992- 01a1, r01ca202752- 01a1, r01ca208236- 01a1,
r01ca216579- 01a1, r01ca220581-01a1, r01ca257612- 01a1, 1u01ca239055- 01,
1u01ca248226- 01, 1u54ca254566- 01, national heart, lung and blood institute
1r01hl15127701a1, r01hl15807101a1, national institute of biomedical imaging
and bioengineering 1r43eb028736- 01, va merit review award ibx004121a from
the united states department of veterans aﬀairs biomedical laboratory research and
development service the oﬃce of the assistant secretary of defense for health aﬀairs,
through the breast cancer research program (w81xwh- 19- 1-0668), the prostate
cancer research program (w81xwh- 20-1- 0851), the lung cancer research program
(w81xwh-18-1-0440, w81xwh-20-1-0595), the peer reviewed cancer research pro-
gram (w81xwh- 18-1-0404, w81xwh- 21-1-0345, w81xwh- 21-1-0160), the kidney
precision medicine project (kpmp) glue grant and sponsored research agreements
from bristol myers-squibb, boehringer-ingelheim, eli-lilly and astrazeneca.
the con-
tent is solely the responsibility of the authors and does not necessarily represent the
oﬃcial views of the national institutes of health, the u.s. department of veterans
aﬀairs, the department of defense, or the united states government.
64, 7–12 (2017)
14. lee, g., veltri, r.w., zhu, g., ali, s., epstein, j.i., madabhushi, a.: nuclear shape
and architecture in benign ﬁelds predict biochemical recurrence in prostate cancer
patients following radical prostatectomy: preliminary ﬁndings.
prognostic implications of residual disease tumor-inﬁltrating lym-
phocytes and residual cancer burden in triple-negative breast cancer patients after
neoadjuvant chemotherapy.
375, 1823–1833 (2016)
triangular analysis of geographical interplay of lymphocytes (triangil)
807
22.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf:
for example, if the levels of expression in a region of
the her2 slide are high, the corresponding region in the h&e slide is highly
likely to contain a high density of cancerous cells.
furthermore, based on the observation that
any dissimilarity between the patch embeddings at corresponding locations in the
generated and groundtruth ihc images is indicative to the level of inconsistency
of the gt at that location, we employ an adaptive weighting scheme in asp.
to measure the consistency at a given patch location, we use
the cosine similarity between the embeddings of the generated ihc patch and
the corresponding gt patch.
cs = zs
ˆ
y · zs
y , where s is index of the
spatial location.
fig.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf:
using the publicly available herohe challenge data
set, the method achieved a state-of-the-art performance of 90% area under the
receiver operating characteristic curve.
current deep learning approaches
to wsi analysis typically operate at three different histopathological scales: whole slide-
level, region-level, and cell-level [4].
extracted
tiles that contained artifacts were discarded (e.g., tiles that had an overlap of >10%
with background artifacts such blurred areas or pen markers).
[15] that was trained on a
subset of data from one of the medical centers in the camelyon17 data set to ensure
homogeneity of staining [16].
to create the tile-level embeddings, we used the method proposed by [17] to sum-
marize the convolutional neural network (cnn) features with nonnegative matrix fac-
torization (nmf) for k = 2 factors.
for cell of origin (coo) prediction of activated b-cell like (abc) or germinal center
b-cell like (gcb) tumors in diffuse large b-cell lymphoma (dlbcl), we used data from
the phase 3 goya (nct01287741) and phase 2 cavalli (nct02055820) clinical
trials, hereafter referred to as ct1 and ct2, respectively.
in fact, for the her2 classiﬁcation task, combined embeddings obtained using the
xformer architecture achieved, to our knowledge, the best performance yet reported on
the herohe challenge data set (area under the receiver operating characteristic curve
[auc], 90%; f1 score, 82%).
when reviewed by a trained pathologist, cells with positive gradients
had characteristics previously associated with breast cancer tumors (e.g., larger nuclei,
more visible nucleoli, differences in size and shape).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf:
(1) the patient-level feature extraction diﬃculties from
gigapixel wsis and tens of thousands of genes, and (2) eﬀective fusion
considering high-order relevance modeling.
concretely, we ﬁrst propose
a group multi-head self-attention gene encoder to capture global struc-
tured features in gene expression cohorts.
finally,
we combine the classiﬁcation tokens of paired modalities and propose
a triplet learning module to learn high-order relevance and discrimina-
tive patient-level information.
(3) image-omic
feature fusion [2,3] may fail to model high-order relevance and the inherent struc-
tural characteristics of each modality, making the fusion less eﬀective.
besides, vision-language models in the computer
vision community stand out for their remarkable versatility [13,14]. neverthe-
less, constrained by computing resources, the most commonly used multimodal
representation learning strategy, contrastive learning, which relies on a large
number of negative samples to avoid model collapse
a big domain gap also hampers their usage in leverag-
ing the structural characteristic of tumor micro-environment and genomic assay.
concretely, we
ﬁrst propose a transformer-based gene encoder, group multi-head self attention
(groupmsa), to capture global structured features in gene expression cohorts.
we assume that one patch-level feature embedding can be
reconstructed by its adjacent patches, and this process enhances the learning
ability for pathological characteristics of diﬀerent tissues.
furthermore, to model the high-order
relevance of the two modalities, we combine cls tokens of paired image and
genomic data to form uniﬁed representations and propose a triplet learning mod-
ule to diﬀerentiate patient-level positive and negative samples in a mini-batch.
two pre-
training objectives are considered: 1) building triplets by concatenated cls tokens of
each modality and enhancing the discriminability according to category relations, and
2) reconstructing the missing patch embeddings by its adjacent patches.
can still learn high-order relevance and discriminative patient-level information
between these two modalities in pre-training thanks to the triplet learning mod-
ule.
1, the overall
framework consists of three parts: 1) group-based genetic encoder groupmsa
(sect. 2.1), 2) eﬃcient patch aggregator (sect. 2.2) and 3) gene-induced multi-
modal fusion (sect. 2.3).
-training for image-omic classiﬁcation
511
2.1
group multi-head self attention
in this section, we propose group multi-head self attention (groupmsa), a
specialized gene encoder to capture structured features in genomic data cohorts.
speciﬁcally, inspired by tokenisation techniques in natural language process-
ing
firstly, the fragment features are divided into groups and there
are ngr learnable group tokens linked to each group resulting in (nf/ngr
+ 1)
tokens per group.
then the prepared tokens are fed to a vanilla multi-head
self-attention (msa) block to extract intra-group information.
after that, we
model cross-group interactions by another msa layer on the global scale with
the locally learned group tokens and a ﬁnal classiﬁcation token clsge ∈ rd.
finally, groupmsa could learn dense semantics from the genomic data cohort.
we follow the preprocessing strategy of clam
[11] to acquire patch-level embedding sequence, i.e., each foreground patch with
256×256 pixels is fed into an imagenet-pretrained resnet50 and the background
region is discarded.
note that
we reconstruct the missing feature embeddings rather than the raw pixels of the
masked areas, which is diﬀerent from traditional mim methods like simmim
in addition, in order to construct the mini-batch, the sub-
sequences we intercept in the mpm pre-training phase may not be suﬃciently
representative of the image-level characteristics.
firstly, we pre-train the groupmsa module by patient-
level annotations in advance and froze it in the following iterations.
after extracting the input patch embeddings and gene sequence separately, we
concatenate clsimg and clsge as clspat ∈ r2d to represent patient-level
characteristics.
suppose we obtain a triplet list {x, x+, x−} during current iteration, where
x, x+, x− are concatenated tokens of anchor clspat, positive clspat, and neg-
ative clspat, respectively.
to enhance the global modeling capability, i.e.,
extracting more precise patient-level features, we expect that the distance
between the anchor and the positive sample gets closer, while the negative sam-
ple is farther away.
applying the pre-trained backbone to image-omic
classiﬁcation task is straightforward, since gimp pre-training allows it to learn
representative patient-level features.
we collect correspond-
ing rna-seq fpkm data for each patient and the length of the input genomic
sequence is 60,480.
2 (c) and (d), clspat with gimp pre-trained are well sepa-
rated between luad and lusc, i.e., gimp pays more attention to the category-
related feature distribution and could extract more discriminative patient-level
features during triplet learning.
since the ﬁxed sub-
sequence length l = 6000 is used in our setting, it is sometimes smaller than the
original patch number, e.g., the maximum size 148,569, the pre-trained model
without genetic guidance may be not aware of suﬃciently accurate patient-
level characteristics, i.e., ineﬀectively focused on normal tissues.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf:
how-
ever, eﬀorts to stratify patient risk are relatively under-explored.
while
most current techniques utilize small ﬁelds of view (so-called local fea-
tures) to link histopathology images to patient outcome, in this work we
investigate the combination of global (i.e., contextual) and local features
in a graph-based neural network for patient risk stratiﬁcation.
our results suggest that the proposed model is capable
of stratifying patients into statistically signiﬁcant risk groups (p < 0.01
across the two datasets) with clinical utility while competing models fail
to achieve a statistical signiﬁcance endpoint (p = 0.148 − 0.494).
as such, there is a paucity of eﬀorts that embark
on utilizing machine learning models for patient prognostication and survival
analysis (for example, predicting risk of cancer recurrence or expected patient
survival).
while prognostication and survival analysis oﬀer invaluable insights
for patient management, biological studies and drug development eﬀorts, they
require careful tracking of patients for a lengthy period of time; rendering this
as a task that requires a signiﬁcant amount of eﬀort and funding.
in the machine learning domain, patient prognostication can be treated as a
weakly supervised problem, which a model would predict the outcome (e.g., time
to cancer recurrence) based on the histopathology images.
this paper aims to investigate the potential of extracting ﬁne and coarse
features from histopathology slides and integrating them for risk stratiﬁcation
in cancer patients.
therefore, the contributions of this work can be summarized
as: 1) a novel graph-based model for predicting survival that extracts both local
and global properties by identifying morphological super-nodes; 2) introducing
a ﬁne-coarse feature distillation module with 3 various strategies to aggregate
interactions at diﬀerent scales; 3) outperforming sota approaches in both risk
prediction and patient stratiﬁcation scenarios on two datasets; 4) publishing
two large and rare prostate cancer datasets containing more than 220 graphs for
active surveillance and 240 graphs for brachytherapy cases.
[18], identify novel
subtypes [12], or later descendants are even able to pinpoint sub-areas with a
high diagnostic value [19].
2.2
survival analysis and gnns in histopathology
mil-based models have been utilized for outcome prediction [29,32] which can
also be integrated with attention-based variants [14]. gnns due to their struc-
tural preserving capacity [28] have drawn attention in various histology domains
by constructing the graph on cells or patches.
below, we have provided
details of each module.
3.1
problem formulation
for pn, which is the n-th patient, a set of patches {patchj}m
j=1 is extracted
from the related whole slide images.
finally, a speciﬁc graph (gn) for
the n-th patient (pn) can be constructed by assuming patches as nodes.
therefore, for each patient such
as pn, we have a graph deﬁned by adjacency matrix an with size m × m
the ﬁnal model (ϵθ) with parameters θ utilizes gn and
sn to predict the risk associated with this patient:
riskn = ϵθ(gn, sn) = ϵθ(graph(xn, an), sn)
(1)
3.2
self-supervised encoder
due to computational limits and large number of patches available for each
patient, we utilize a self-supervised approach to train an encoder to reduce
the inputs’ feature space size.
this term is motivated by the original mincut problem
and intends to solve it for the the patients’ graph.
[5], while the main survival loss still controls
the global extraction process.
3.5
fine-coarse distillation
we propose our ﬁne-coarse morphological feature distillation module to leverage
all-scale interactions in the ﬁnal prediction by ﬁnding a local and a global patient-
level representations (ˆhl,n, ˆhg,n).
the ﬁrst set (pca-as) includes 179 pca patients who
were managed with active surveillance (as).
radical therapy is considered
overtreatment in these patients, so they are instead monitored with regular
serum prostate-speciﬁc antigen (psa) measurements, physical examinations,
sequential biopsies, and magnetic resonance imaging [23].
although
majority of patients in our cohort are classiﬁed as low-risk based on nccn guide-
lines [21], a signiﬁcant subset of them experienced disease upgrade that triggered
deﬁnitive therapy (range: 6.2 to 224 months after diagnosis).
the second dataset (pca-bt) includes 105 pca patients with low to high
risk disease who went through brachytherapy.
we utilize concordance-index (c-index) that mea-
sures the relative ordering of patients with observed events and un-censored
cases relative to censored instances [2].
patient stratiﬁcation.
the capacity of stratifying patients into risk groups
(e.g., low and high risk) is another criterion that we employ to assess the util-
ity of models in clinical practice.
we evaluate model performances via kaplan-
meier curve [15] (cut-oﬀ set as the ratio of patients with recurrence within 3
772
p. azadi et al.
fig.
our model stratiﬁed pca-as
patients into high- and low-risk groups with median time to progression of 36.5
and 131.7 months, respectively.
while
none of the baselines are capable of assigning patients into risk groups with
statistical signiﬁcance, our distillation policies achieve signiﬁcant separation in
both pca-as and pca-bt datasets; suggesting that global histo-morphological
properties improve patient stratiﬁcation performance.
furthermore, our ﬁndings
have signiﬁcant clinical implications as they identify, for the ﬁrst time, high-
risk prostate cancer patients who are otherwise known to be low-risk based on
clinico-pathological parameters.
this group should be managed diﬀerently from
the rest of the low-risk prostate cancer patients in the clinic.
while a prognostic biomarker provides information
about a patient’s outcome (without speciﬁc recommendation on the next course
of action), a predictive biomarker gives insights about the eﬀect of a therapeutic
intervention and potential actions that can be taken.
ablation study.
achieving higher
c-indices in our all model versions indicates the important role of coarse features
and global context in patient risk estimation in addition to local patterns.
all-in
773
table 2. ablation study on diﬀerent modules.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf:
enabling multimodal analytics promises to reveal
novel predictive patterns of patient outcomes.
keywords: histopathological image analysis · multimodal learning ·
cancer diagnosis · survival prediction
1
introduction
cancers are a group of heterogeneous diseases reﬂecting deep interactions
between pathological and genomics variants in tumor tissue environments
therefore, synergizing multimodal data could deepen a cross-
scale understanding towards improved patient prognostication.
overall, we formulate the objective of multimodal feature
learning by converting image patches and tabular genomics data into group-
wise embeddings, and then extracting multimodal patient-wise embeddings.
more speciﬁcally, we construct group-wise representations for both image and
genomics modalities.
group-wise image and genomics embedding.
we deﬁne the group-wise
genomics representation by referring to n = 8 major functional groups obtained
from [22].
each group contains a list of well-deﬁned molecular features related to
cancer biology, including transcription factors, tumor suppression, cytokines and
pathology-and-genomics multimodal transformer for survival prediction
625
growth factors, cell diﬀerentiation markers, homeodomain proteins, translocated
cancer genes, and protein kinases.
the group-wise genomics representation is
deﬁned as gn ∈ r1×dg, where n ∈ n, dg is the attribute dimension in each group
which could be various.
to better extract high-dimensional group-wise genomics
representation, we use a self-normalizing network (snn) together with scaled
exponential linear units (selu) and alpha dropout for feature extraction to
generate the group-wise embedding gn ∈ r1×256 for each group.
for group-wise wsis representation, we ﬁrst cropped all tissue-region image
tiles from the entire wsi and extracted cnn-based (e.g., resnet50)
we construct the group-wise wsis
representation by randomly splitting image tile features into n groups (i.e., the
same number as genomics categories).
therefore, group-wise image representa-
tion could be deﬁned as in ∈ rkn×1024, where n ∈ n and kn represents tile
k in group
[17], which is
able to weight the feature embeddings in the group, together with a dimension
deduction (e.g., fully-connected layers) to achieve the group-wise embedding.
the abr and the group-wise embedding in ∈ r1×256 are deﬁned as:
ak =
epx{wt (tanh(v1hk) ⊙ (sigm(v2hk))}
k
j=1 epx{wt (tanh(v1hj) ⊙ (sigm(v2hj))}
(1)
where w,v1 and v2 are the learnable parameters.
in =
k

k=1
akhk
(2)
patient-wise multimodal feature embedding.
to aggregate patient-wise
multimodal feature embedding from the group-wise representations, as shown
in fig.
in the pathological image stream,
the patient-wise image representation is aggregated by n group representations
as ip ∈ rn×256, where p ∈ p and p is the number of patients.
similarly, the
patient-wise genomics representation is aggregated as gp ∈ rn×256.
after gener-
ating patient-wise representation, we utilize two transformer layers
[17] as eq. 1 to adaptively compute a
weighted sum of each modality feature embeddings to ﬁnally construct patient-
wise embedding as ip
embedding ∈ r1×256 and gp
embedding ∈ r1×256 in each modal-
ity.
626
k. ding et al.
multimodal fusion in pretraining and finetuning.
ideally, the image and genomics embeddings belonging to the
same patient should have a higher relevance between each other.
[8,20], which contain 440 and 153 patients.
finally, we included 426
patients of tcga-coad and 145 patients of tcga-read.
we followed the previous studies [5–7] to partition the overall sur-
vival (os) months into four non-overlapping intervals by using the quartiles of
event times of uncensored patients for discretized-survival c-index calculation
(see appendix 2).
in addition, our model reﬂects its eﬃciency on the limited ﬁne-
tuning data (e.g., 75 patients are used for ﬁnetuning on tcga-read, which
are only 22% of tcga-coad ﬁnetuning data).
for the tcga-read dataset, as the number
of uncensored patients is limited, we use 75%, 50%, and 25% of the ﬁnetuning
data to allow at least one uncensored patient to be included for ﬁnetuning.
4
conclusion
developing data-eﬃcient multimodal learning is crucial to advance the survival
assessment of cancer patients in a variety of clinical data scenarios.
we demon-
strated that the proposed pathomics framework is useful for improving the
survival prediction of colon and rectum cancer patients.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf:
some works adopt the k-means clustering on all instances in
a bag to obtain k cluster centers i.e., instance prototypes, and then use these prototypes
to represent the bags
these clustering-based mil algorithms can signiﬁcantly
reduce the redundant instances, and thereby improving the training efﬁciency for wsi
classiﬁcation.however,itisdifferentfork-meanstospecifytheclusternumberaswellas
the initial cluster centers, and different initial values may lead to different cluster results,
thus affecting the performance of mil.
xi =

ij
i
j=n
j=1,
(1)
where xi denotes a patient, yi the label of xi, ij
i is the j-th instance of xi, n is the number
of patients and n is the number of instances.
∈ rn×dk, the k-means clustering algorithm is applied on all
instances to get k centers (prototypes).
however, the k-means clustering algorithm is
sensitive to the initial selection of cluster centers, i.e. different initializations can lead to
different results, and the ﬁnal result may not be the global optimal solution.
token-mixing mlp is a cross-location operation to mix all
prototypes, whilechannel-mixingmlpis apre-locationoperationtomixfeatures of each
prototype.
we collected
a total of 854 diagnostic slides from the national cancer institute data portal (https://
portal.gdc.cancer.gov).
we selected accuracy (acc) and area under curve
(auc) as evaluation metrics.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf:
to overcome
these limitations, we aim to perform gland instance segmentation to accurately
identify the target location and prevent misclassiﬁcation of background tissue.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf:
based on the character
of these methods, we can group them into two categories: the traditional ul
methods and the deep learning ul methods.
+ 1] = b[k] + p[k]
2 · a
(4)
where k represents the kth epoch erosion of the connected component, b[k] is
the conﬁdence score of pixels eroded in the kth epoch and b[0] = 0.5 as the
initial condition, p[k] means number of pixels eroded in the kth epoch, and a
is the area of the connected component.
2 by setting the center of connected
component as 1, constructing voronoi diagram, setting voronoi edge as 0, and
ignoring other pixels.
right: illustration of average con-
vex hull area and the average of connected component area based on usmi.
51.0± 0.9(52.4)
3.2
experimental results
to evaluate the eﬀectiveness of ssimnet, we compare it with several deep learn-
ing based and conventional unsupervised segmentation methods on the men-
tioned datasets, including minibatch k-means (termed as mkmeans), gaus-
sian mixture model [9] (termed as gmm), invariant information clustering
[12] (termed as iic), double dip
the reason lies in that our method considers mining as
strong prior knowledge from tissue slice itself, which renders a tighter constraint
on our model, leading the model to predict a lower conﬁdence in the easily-
confused region.
it
also conforms the eﬀectiveness of our method on eliminating the model confusion
in the region between adjacent nuclei and the ability in capturing nuclei shape.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf:
however, in cancer prognosis applications, the cost of
acquiring patients’ survival information is high and can be extremely
diﬃcult in practice.
by considering that there exists a certain common
mechanism for tumor progression among diﬀerent subtypes of breast
invasive carcinoma(brca), it becomes critical to utilize data from a
related subtype of brca to help predict the patients’ survival in the
target domain.
to address this issue, we proposed a tils-tumor inter-
actions guided unsupervised domain adaptation (t2uda) algorithm to
predict the patients’ survival on the target bc subtype.
we evaluated the performance of
our method on the brca cohort derived from the cancer genome atlas
(tcga), and the experimental results indicated that t2uda outper-
formed other domain adaption methods for predicting patients’ clinical
outcomes.
https://doi.org/10.1007/978-3-031-43987-2_59
t2uda
613
keywords: tumor-inﬁltrating lymphocytes · unsupervised domain
adaption · prognosis prediction · graph attention network · breast
cancer
1
introduction
breast cancer (bc) is the most common cancer diagnosed among females and
the second leading cause of cancer death among women after lung cancer [1].
thus, eﬀective and accurate prognosis of bc
as well as stratifying cancer patients into diﬀerent subgroups for personalized
cancer management has attracted more attention than ever before.
[5] presented a novel
approach for predicting the prognosis of er-positive bc patients by quantifying
nuclear shape and orientation from histopathological images.
however, due to the high-cost of collecting survival information
from the patients, it is still a challenge to build eﬀective machine learning models
for speciﬁc bc subtypes with limited annotation data.
to deal with the above challenges, several researchers began to design domain
adaption algorithms, which utilize the labeled data from a related cancer sub-
type to help predict the patients’ survival in the target domain.
[7] presented a new representation learning-based unsuper-
vised domain adaption method to predict the clinical outcome of cancer patients
on the target domain.
based on the above considerations, in this paper, we proposed a tils-tumor
interactions guided unsupervised domain adaptation (t2uda) algorithm to pre-
dict the patients’ survival on the target bc subtype.
we evalu-
ated the performance of our method on the breast invasive carcinoma (brca)
cohort derived from the cancer genome atlas (tcga), and the experimental
results indicated that t2uda outperforms other domain adaption methods for
predicting patients’ clinical outcomes.
t2uda aligns tils-tumor
edge interaction weights in the ttia module and feature vectors in the fa module
to reduce the discrepancy between diﬀerent domains and achieving prognosis task on
target bc patients.
then we calculated the tumor and tils area ratios in each
patch and selected 300 patches with the largest ratios of each tissue type.
the gat layer generate a new
group of node features h′
the
cox proportional hazard model was applied to predict the patients’ clinical out-
come [16], and its negative log partial likelihood function can be formulated
as:
lprognosis =
n

i=1
δi
⎛
⎝θt xi − log

j∈r(ti)
exp

θt xj

⎞
⎠
(6)
where xi represents the output of the last layer for the prognosis task and r (ti)
is the risk set at time ti, which represents the set of patients that are still under
risk before time t. in addition, δi is an indicator variable.
sample i refers to
censored patient if δi = 0, otherwise δi = 1.
overall objective.
speciﬁcally, the
brca dataset includes 661 patients with hematoxylin and eosin (he)-stained
pathological imaging and corresponding survival information.
among the col-
lected brca patients in tcga, the number of er positive(er+) and er
negative(er−) patients are 515 and 146, respectively.
we evaluated the performance of our model using
the concordance index (ci) and area under the curve (auc) as performance
metrics.
5) source only: it was trained on
t2uda
619
proportion of edges connected tils 
and tumor patches
source domain
target domain
patient a, 84 months
patient b, 13 months
patient d, 19 months
patient c, 73 months
 tumor patch
 tils patch
（a）
(b)
0
10
20
30
40
50
60
low-risk group
high-risk group
source
target
fig.
4. (a): compare proportion of edges connected tils and tumor patches of source
and target domain (b): compare interactions between tils patches and tumor patches
for long survival patients and short survival patients of source and target domain.
in addition, we also evaluated the patient stratiﬁcation performance of dif-
ferent methods.
we also examined the consistency of important edges in each group of strat-
iﬁed patients based on the tils-tumor interaction weights calculated by the
620
y. wu et al.
gat-based framework in the source and target domains.
4(a),
for both the source and target domains, the proportion of edges that connect
tils and tumor regions in the low-risk group was higher than that in the high-
risk group, showing that the interaction between tils and tumors played a
critical role in prognostic prediction in diﬀerent bc subtypes.
, the weights of the edges connecting tumor and tils regions
were higher for patients in the low survival risk group in both source and target
domains.
4
conclusion
in this paper, we presented an unsupervised domain adaptation algorithm that
leverages tils-tumor interactions to predict patients’ survival in a target bc
subtype(t2uda).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf:
the feature embeddings of
the slide-level t (thumbnail), region-level r (×5), and the patch-level p (×10)
can be represented as,
t = {t},
r = {r1, r2, · · · , rn},
p = {p 1, p 2, · · · , p n}, pi = {pi,1, pi,2, · · · , pi,m},
(1)
where t, ri, pi,j ∈ r1×c correspond to the feature embeddings of each patch in
thumbnail, region, and patch levels, respectively.
n is the total number of the
region nodes and m is the number of patch nodes belonging to a certain region
node, and c denotes the dimension of feature embedding (1,024 in our experi-
ments).
there are two kinds of edges in the
graph: spatial edges to denote the 8-adjacent spatial relationships among dif-
ferent patches in the same levels, and scaling edges to denote the relationship
between patches across diﬀerent levels at the same location.
at the end of
the hierarchical gnn part, we use the ihpool [6] progressively aggregate the
hierarchical graph.
2.3
hierarchical interaction vit
we further propose a hierarchical interaction vit (hivit) to learn long-range
correlation within the wsi pyramids, which includes three key components:
patch-level (pl) blocks, bidirectional interaction (bi) blocks, and region-level
(rl) blocks.
patch-level block.
the bi block performs bidirectional interaction, and the interaction
progress from region nodes to patch nodes is:
rl′
i ∈ rl′,
rl′ = se(rl) · rl,
p l+1
i
= {pl+1
i,1 , pl+1
i,2 , · · · , pl+1
i,k },
pl+1
i,k = ˆpl+1
i,k
[8] and the rl′
i means the
i-th region node in rl′, and ˆpl+1
i,k is the k-th patch node linked to the i-th region
760
z. guo et al.
node after the interaction.
ˆr
l+1 = se( ¯p
l+1) · ¯p
l+1 + rl,
(6)
where the mean(·) is the operation to get the mean value of patch nodes set
ˆp
l+1
i
associated with the i-th region node and ¯p
l+1
1
∈ r1×c and the c is the
feature channel of nodes, and ˆr
l+1 is the region nodes set after interaction.
region-level block.
the ﬁnal part of this module is to learn the long-range
correlations of the interacted region-level nodes:
rl+1 = rl( ˆr
l+1)
the accuracy (acc) and area under the curve (auc)
are used as the evaluation metric.
the third row changes the bidirectional interaction
mechanism into just one direction from region-level to patch-level.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf:
the input wsis are pre-processed to extract 256 × 256 × 3
dimensional patches from the tumor area at a 10× magniﬁcation level.
the dataset for the former task was collected from 168 patients with 332
wsis from seoul national university hospital.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf:
https://doi.org/10.1007/978-3-031-43987-2_72
746
w. hou et al.
1
introduction
the ability to predict the future risk of patients with cancer can signiﬁcantly
assist clinical management decisions, such as treatment and monitoring [21].
generally, pathologists need to manually assess the pathological images obtained
by whole-slide scanning systems for clinical decision-making, e.g., cancer diagno-
sis and prognosis
[26] extracted the phenotype
patterns of the patient via a clustering algorithm, which provides meaningful
medical prior to guide the aggregation of patch features.
⎞
⎠ ,
(6)
where δi denote the censorship of i-th patient, o(i) and o(j) denote the survival
output of i-th and j-th patient in a batch, respectively.
3
experiments
3.1
experimental settings
dataset.
in this study, we used a colorectal cancer (crc)(385 cases)
cohort collected from co-operated hospital to evaluate the proposed method.
the concordance index (ci) [23] is used to measure the
fraction of all pairs of patients whose survival risks are correctly ordered.
moreover, to
evaluate the ability of patients stratiﬁcation, the kaplan-meier (km) analysis
is used [23].
w/o transformer
0.592 ± 0.010
0.647 ± 0.002
0.616 ± 0.005
ours
hgt
0.607 ± 0.004 0.657 ± 0.003 0.646 ± 0.003
752
w. hou et al.
3.3
interpretability of the proposed framework
we selected the crc dataset for further interpretable analysis, as it is one of the
leading causes of mortality in industrialized countries, and its prognosis-related
factors have been widely studied [3,8].
all the patients across the ﬁve test folds are combined and analysis
here.
for each cohort, patients were stratiﬁed into high-risk (red curves) and low-risk
(green curves) groups by the median score output by predictive models.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_57.pdf:
the area of each nucleus instance is obtained via subtraction between
the segmentation and contour prediction maps [1].
the images are extracted
from 16 colorectal adenocarcinoma wsis, each of which belongs to an individual
patient, and scanned with an omnyx vl120 scanner within the department of
pathology at university hospitals coventry and warwickshire, uk. cpm17
[28] contains 32 training and 32 validation images, whose sizes are 500 ×

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf:
for example, small and
scattered thyroid cells with a light hue and relatively low cell density are usually
low-grade and indicative of early-stage cancer; whereas large and dark cells with
extreme-dense agglomeration are usually middle- or late-grade [3]. correspond-
ingly, accurate location of cell boundaries is essential for both pathologists and
computer-aided diagnosis (cad) systems to assist decision [7].
and nb denote the nuclei area and boundary, respec-
tively.
to address this issue, we
generate a novel intensity-based noise, which can adaptively behave stronger in
the dark nuclei areas and weaker in bright cytoplasm or background regions.
patient-level
images were partitioned ﬁrst for training and test images, and patch-level cura-
tion was performed.
quantitative comparisons in both fully-supervised and semi-supervised man-
ners.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf:
(1) luad-gm dataset: the objec-
tive is to predict the epidermal growth factor receptor (egfr) gene mutations
in patients with lung adenocarcinoma (luad) using 723 whole slide image
(wsi) slices, where 47% of cases have egfr mutations.
the numbers displayed within
each group represent the average likelihood of the egfr mutation predicted
by the patches.
with the help of the label-disambiguation-based instance-level
supervision, iib-mil can identify highly positive and negative related patches
to the wsi-label, i.e., the cyan-blue group and yellow group.
double-checked
by pathologists, we ﬁnd that the cyan-blue group consists of patches from lung
adenocarcinoma and the yellow group consists of patches from the squamous
cells.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf:
we introduce a new ai-ready computational pathology
dataset containing restained and co-registered digitized images from
eight head-and-neck squamous cell carcinoma patients.
https://doi.org/10.1007/978-3-031-43987-2_68
an ai-ready multiplex staining dataset
705
1
introduction
accurate spatial characterization of tumor immune microenvironment is critical
for precise therapeutic stratiﬁcation of cancer patients (e.g. via immunother-
apy).
demographics and other relevant details of the eight anonymized head-and-
neck squamous cell carcinoma patients, including ecog performance score, pack-year,
and surgical pathology stage (ajcc8).
id
age gender race
ecog smoking py
pstage cancer site
cancer subsite
case1 49
male
white 3
current
21
1
oral cavity ventral tongue
case2 64
male
white 3
former
20
4
larynx
vocal cord
case3 60
male
black
2
current
45
4
larynx
false vocal cord
case4
53
male
white 1
current
68
4
larynx
supraglottic
case5 38
male
white 0
never
0
4
oral cavity lateral tongue
case6 76
female
white 1
former
30
2
oral cavity lateral tongue
case7 73
male
white 1
former
100
3
larynx
glottis
case8 56
male
white 0
never
0
2
oral cavity tongue
2
dataset
the complete staining protocols for this dataset are given in the accompany-
ing supplementary material.
images were acquired at 20× magniﬁcation at
moﬃtt cancer center.
the demographics and other relevant information for all
eight head-and-neck squamous cell carcinoma patients is given in table 1.
2.1
region-of-interest selection and image registration
after scanning the full images at low resolution, nine regions of interest (rois)
from each slide were chosen by an experienced pathologist on both mif and
mihc images: three in the tumor core (tc), three at the tumor margin (tm),
and three outside in the adjacent stroma (s) area.
the size of the rois was
standardized at 1356×1012 pixels with a resolution of 0.5 µm/pixel for a total
surface area of 0.343 mm2.
after that, hematoxylin- and dapi-stained rois were used as references
to align mihc and mif rois again using fiji and subdivided into 512×512
patches, resulting in total of 268 co-registered mihc and mif patches (∼33
co-registered mif/mihc images per patient).
the dapi images were segmented using cellpose [13] and man-
ually corrected by a trained technician and approved by a pathologist.
lyon19 ihc cd3/cd8 images
are taken from breast, colon, and prostate cancer patients.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf:
however, existing spatial embedding strategies in transformer can only
represent ﬁxed structural information, which are hard to tackle the scale-
varying and isotropic characteristics of wsis.
however,
these methods still rely on at least patient-level annotations.
[19],
we extracted multiple anchors by clustering the location coordinates of patches
for the auxiliary description of the wsi structure.
the embedding of relative distance and polar angle information helps the model
maintain the semantic and structural integrity of the wsi and meanwhile pre-
vents the wsi representation from collapsing to the local area throughout the
training process.
accuracy (acc) and area
under the roc curve (auc) are employed as evaluation metrics.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf:
the wsis were collected from four medi-
cal institutions of ×20 magniﬁcation (0.475 μm/pixel) with an average size of
5000 × 5000.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf:
the second dataset is a private hepatocellular carcinoma (hcc) dataset col-
lected from sir run run shaw hospital, hangzhou, china.
namely, area under curve (auc), f1 score, and
slide-level accuracy (acc).
mean pool-
ing performs better on this dataset due to the large area of tumor in the wsis
(about 60% patches are tumor patches), which mitigates the impact of average
pooling on instances.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf:
if tc of a lesion-like region
exhibit negative symptoms, denoted as negative temporal contexts (ntc), radi-
ologists are less likely to report it as a lesion [15].
for each region of interest (roi) r proposed by a basic detector, we extract
temporal contexts from previous frames.
2.
6
h. yu et al.
3.1
basic real-time detector
the basic real-time detector comprises three main components: a lightweight
backbone (e.g. resnet34 [6]), a region proposal network (rpn)

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_50.pdf:
speciﬁcally, artifusion formulates the arti-
fact region restoration as a gradual denoising process, and its training
relies solely on artifact-free images to simplify the training complexity.
discarding the local region with artifacts for deep
learning models is another solution, but it may result in the loss of critical con-
textual information.
innovatively, our framework formulates the artifact restoration as a regional
denoising process, which thus can to the most extent preserve the stain style
and avoid the loss of contextual information in the non-artifact region.
during the inference stage, we
ﬁrst use a threshold method to detect the artifact region in the input image
x0.
then, unlike the conventional diﬀusion models [5] that aim to generate the
entire image, artifusion selectively performs denoising resampling only in the
artifact region to maximally preserve the original morphology and stain style in
the artifact-free region, as shown in fig.
speciﬁcally, we represent the artifact-
free region and the artifact region in the input image as x0⊙(1−m) and x0⊙m,
respectively [10], where m is a boolean mask indicating the artifact region and ⊙
is the pixel-wise multiplication operator.
to perform the denoising resampling,
we write the input image xin
t
at each reverse step from t to t − 1 as the sum of
the diﬀused artifact-free region and the denoised artifact region, i.e.,
xin
t
= xsample
t
⊙ (1 − m)
+ xout
t+1 ⊙ m,
(2)
where xsample
t
o⊙(1−m) is artifact-free region diﬀused for t times using the gaus-
sian transition kernel i.e. xsample
t
∼ n(√¯αtx0, (1− ¯αti)) with ¯αt = t
i=1(1−βi);
and xout
t+1 is the output from the denoising network in the previous reverse
step i.e., pθ(xout
t+1|xin
t+1).
we also illustrate the gradual denoising process in the artifact region by
artifusion, at time step t = 0, 50, 100, 150.
we use the following metrics: l2
distance (l2) with respect to the artifact region, the mean-squared error (mse)
over the whole image, structural similarity index (ssim)

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf:
breast cancer (bc) is one of the most common cancers iden-
tiﬁed globally among women, which has become the leading cause of
death.
keywords: breast cancer · hematoxylin and eosin staining ·
immunohistochemical staining · multi-modal pre-training
1
introduction
breast cancer (bc) is one of the most common malignant tumors in women
worldwide and it causes nearly 0.7 million deaths in 2020
in addition to obtaining the histological characteristics of tumors from hema-
toxylin and eosin (h&e) staining images, immunohistochemical (ihc) staining
images are also widely used for pathological diagnoses, such as the human epi-
dermal growth factor receptor 2 (her2), the estrogen receptor (er), and the
progesterone receptor (pr) [22].
the wsis, genetic
and clinical data from a patient could be used for isomorphic data pre-training.
next, the otsu threshold is applied to
extract the foreground area, which is cropped into non-overlapping 256 × 256
images.
four standard
metrics are used to measure the her2 status prediction results, including the
area under the receiver operator characteristic curve (auc), precision, recall,
and f1-score.
the
region in the red box shows our mmp-mae could learn the semantic information from
the adjacent area.
team irisai ﬁrst segment the tumor
area and then predict the her2 status.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf:
in clinical diagnosis, pathologists typ-
ically grade diseases by manually estimating the proportion of stained membrane area
in contrast, as annotating the centers of nuclei requires much fewer
efforts, weakly supervised learning has been studied for nuclei segmentation
this is because
gω is utilized to predict the category of semantic points, which are the center points of
cells and related to the membrane.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf:
all images were acquired during clinical routine at the kardinal schwarzenberg
hospital.
the mean and median age of patients at the date of dissection was
47 and 50 years, respectively.
the data set comprised 13 male and 27 female patients,
corresponding to a slight gender imbalance.
overviews at a 2x magniﬁcation were generated to manually deﬁne
scan areas, focus points were automatically deﬁned and adapted if needed.
for each patch, we checked that at least 75 % of the area was covered with tissue (green
color channel) in order to exclude empty areas [5]. to obtain a representation indepen-
dent of the wsi size, we extracted 1024 patches with a size of 256×256 pixel per wsi,
resulting in 1024 patch-descriptors per wsi [5].

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf:
however,
these models can only recognize samples from predeﬁned categories, when
they are deployed in the clinic, data from new unknown categories are con-
stantly emerging.
in
addition, they only consider the global alignment of samples to the category
center, ignoring the local inter-sample alignment thus leading to poor clustering
performance.
after the cross-pseudo-supervision training
described above, we are able to assign the instances to their corresponding clus-
tering centers.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf:
after scanning whole-slide images (wsis) from tct samples, automatic
tct screening is highly desired due to the large population versus the lim-
ited number of pathologists.
we lever-
age fastgan [16] as the backbone for the sake of training stability and compu-
tational eﬃciency.
the numbers in the center and
the bottom right corner of each square indicate the feature map size and the channel
number, respectively.
in this study, we collect 14,477 images with 256 × 256 pixels from
three collaborative clinical centers.
in contrast, model iii and cellgan can accurately cap-
ture the morphological characteristics of diﬀerent cell types.
in each fold, one group is selected as the testing data
while the other four are used for training.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_53.pdf:
many stain style transfer methods have been proposed to elimi-
nate the variance of stain styles across diﬀerent medical institutions or
even diﬀerent batches.
however, stain styles
can vary signiﬁcantly across diﬀerent pathology labs or institutions.
meanwhile, in some clinical settings, multiple institu-
tions or hospitals are involved, where stain normalization is usually employed for
multiple stain styles to one style alignment.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf:
vulvovaginal candidiasis (vvc) is the most prevalent human
candidal infection, estimated to aﬄict approximately 75% of all women
at least once in their lifetime.
keywords: whole slide image · vulvovaginal candidiasis ·
attention-guided
1
introduction
vulvovaginal candidiasis (vvc) is a type of fungal infection caused by candida,
which results in discomforting symptoms, including itching and burning in the
genital area [4,18].
it is the most prevalent human candidal infection, estimated
to aﬄict approximately 75% of all women at least once in their lifetime [1,20],
resulting in huge consumption of medical resources.
in experimental exploration, we ﬁnd that, if we train the detection net-
work directly, the bounding-box annotation indicates the location of candida
and can rapidly establish a rough understanding of the morphology of candida.
our approach has two key goals: (1) to ensure that the features from the origi-
nal image remain consistent after undergoing various image augmentations, and
(2) to construct an image without the region of candida, resulting in highly
dissimilar features compared to the original.
at the same time, we hope
that fmasked should not contain the characteristics of candida, which is repelled
from faug.
additionally, we need the attention to cover only the
partial area around candida, without false positive regions.
we report the performance using ﬁve common metrics: area under
the receiver operating characteristic curve (auc), accuracy (acc), sensitivity
(sen), speciﬁcity (spe), and f1-score.
comparisons for image-level classiﬁcation.
after pt, the model can focus on
the candida area, edges of cells, and folds that resemble candida, as shown in
fig.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf:
colorectal cancer is a prevalent form of cancer, and many
patients develop colorectal cancer liver metastasis (crlm) as a result.
our
experimental results show that a multi-plane architecture based on 3d
bi-directional lstm, which we call mpbd-lstm, works best, achieving
an area under curve (auc) of 0.79.
https://doi.org/10.1007/978-3-031-43987-2_37
380
x. li et al.
1
introduction
colorectal cancer is the third most common malignant tumor, and nearly half
of all patients with colorectal cancer develop liver metastasis during the course
of the disease
patients
with colorectal cancer typically undergo contrast-enhanced computed tomogra-
phy (cect) scans multiple times during follow-up visits after surgery for early
detection of crlm, generating a 5d dataset.
1. representative slices from 3d ct images of diﬀerent patients in our dataset,
at a/v phases and timestamps t0, t1, t2 (cropped to 256 × 256 for better view).
characreristics of our dataset
cohort # of positive cases # of negative cases total cases positive rate
1st
60
141
201
0.299
2nd
9
59
68
0.132
total
69
200
269
0.257
when patients undergo cect scans to detect crlm, typically three phases
are captured: the unenhanced plain scan phase (p), the portal venous phase
(v), and the arterial phase (a).
that means patients have not been
diagnosed as crlm when they took the scans.
382
x. li et al.
– patients were previously diagnosed with colorectal cancer tnm stage i to
stage iii, and recovered from colorectal radical surgery.
– patients have two or more times of cect scans.
– we already determined whether or not the patients had liver metastases
within 2 years after the surgery, and manually labeled the dataset based
on this.
– no potential focal infection in the liver before the colorectal radical surgery.
– no metastases in other organs before the liver metastases.
– no other malignant tumors.
our retrospective dataset includes two cohorts from two hospitals.
the ﬁrst
cohort consists of 201 patients and the second cohort includes 68 patients.
patients may have diﬀerent numbers of ct scans, ranging from 2 to 6, depending
on the number of follow-up visits.
3
experiments
3.1
data augmentation and selection
we selected 170 patients who underwent three or more cect scans from our
original dataset, and cropped the images to only include the liver area, as shown
in fig.
we applied the same augmentation
technique consistently to all phases and timestamps of each patient’s data.
1, patients b and c are diagnosed with positive crlm
later.
mpbd-lstm correctly yields a positive prediction for patient b with a
conﬁdence of 0.82, but incorrectly yields a negative prediction for patient c with
a conﬁdence of 0.77.
with similar conﬁdence in the two cases, the error is likely
due to the relatively smaller liver size of patient c. beyond this case, we ﬁnd
that small liver size is also present in most of the false negative cases.
how to eﬀectively address inter-patient variability in the
dataset, perhaps by better fusing the 5d features, requires further research from
the community in the future.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf:
under the same flops budget (4.1g), our
searched resnet50 model achieves 96.2% accuracy and 96.5% area under
the roc curve (auc), which are 4.8% and 4.7% higher than those with
the baseline settings, respectively.
(a) region of interest (roi) extrac-
tion and patch generation, and (b) patch detection and wsi classiﬁcation.
mod-
ule (a) extracts the region of interest (roi) from wsi and generates patches,
detection of basal cell carcinoma in whole slide images
265
while module (b) uses optimal model architecture from nas to analyze features
from patches and generate classiﬁcations.
fig.
the
patient data were separated between training and testing to prevent overlap.
(5) auc: roc curve area, reﬂecting the false/true positive rate trade-oﬀ.
as shown in table 2, the s resnet50 model outperformed in all metrics,
showing 4.8%, 4.5%, 5.4%, 4.7% and 4.7% improvements in accuracy, sensitivity,
speciﬁcity, f1 score, and auc, respectively, over ori resnet50, and surpassing
270
h. xu et al.
table 3. performance of searched models with diﬀerent searching methods.
a
comparison of the labeled areas in column (a) and the red areas in column (b)
indicates that the predicted areas are generally similar in scope to the corre-
sponding labeled areas.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf:
to alleviate this
limitation, we innovatively introduce a diffusion-based dose prediction (diffdp)
model for predicting the radiotherapy dose distribution of cancer patients.
to ensure the accuracy of the
prediction, we further design a structure encoder to extract anatomical information
from patient anatomy images and enable the noise predictor to be aware of the
dose constraints within several essential organs, i.e., the planning target volume
and organs at risk.
extensive experiments on an in-house dataset with 130 rectum
cancer patients demonstrate the superiority of our method.
deep
learning
1
introduction
radiotherapy, one of the mainstream treatments for cancer patients, has gained notable
advancements in past decades.
consequently, it is essential to develop a robust method-
ology to automatically predict the dose distribution for cancer patients, relieving the
burden on dosimetrists and accelerating the radiotherapy procedure.
[15] to predict the dose of prostate
cancer patients.
1. instances from a rectum cancer patient.
unlike other dl models, the
diffusion model is trained without any extra assumption about target data distribution,
thus evading the average effect and alleviating the over-smoothing problem [24]. figure 1
(4) provides an example in which the diffusion-based model predicts a dose map with
shaper and clearer boundaries of ray-penetrated areas.
diffdp: radiotherapy dose prediction via a diffusion model
193
in this paper, we investigate the feasibility of applying a diffusion model to the dose
prediction task and propose a diffusion-based model, called diffdp, to automatically
predict the clinically acceptable dose distribution for rectum cancer patients.
(2) we introduce a structure encoder to extract the anatomical information
available in the ct images and organ segmentation masks, and exploit the anatomical
information to guide the noise predictor in the diffusion model towards generating more
precisepredictions.(3)theproposeddiffdpisextensivelyevaluatedonaclinicaldataset
consisting of 130 rectum cancer patients, and the results demonstrate that our approach
outperforms other state-of-the-art methods.
an image set of cancer patient
is deﬁned as {x, y}, where x ∈ rh×w×(2+o) represents the structure images, “2” signiﬁes
the ct image and the segmentation mask of the ptv, and o denotes the total number
of segmentation mask of oars.
we measure the performance of our model on an in-house
rectum cancer dataset which contains 130 patients who underwent volumetric modulated
arc therapy (vmat) treatment at west china hospital.
concretely, for every patient, the
ct images, ptv segmentation, oars segmentations, and the clinically planned dose
distribution are included.
we randomly select 98
patients for model training, 10 patients for validation, and the remaining 22 patients for
test.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf:
keywords: detection-free · contrastive learning · pathology image
classiﬁcation · cervical cancer
1
introduction
cervical cancer is a common and severe disease that aﬀects millions of women
globally, particularly in developing countries [9].
in this study, we have collected 5384
cervical cytopathological wsi by 20x lens, each with 20000 × 20000 pixels, from
our collaborating hospitals.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf:

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_15.pdf:
recent works
have shown that automatic classiﬁcation of af sub-types can be done using ct
volumes of the left atrium and surrounding eat, which can be used to screen
for patients with high risk of peaf.
– we propose a novel radiomics-informed deep learning (ridl) method for
af sub-type classiﬁcation from ct volumes, which achieves state-of-the-art
results and can be used to screen for patients with high risk of peaf.
xi has two channels, one
consisting of the 3d ct volume centered around the left atrium and the other
the binary region-of-interest (roi) mask indicating eat.
(1)
our method applies feature calculations locally to cubic patches centered around
each voxel, such that features are obtained on a voxel basis and reﬂect the
statistics of the neighbouring region.
for a cubic patch with radius p and input
xi, the local feature at location (h, w, d), denoted by rp
i,(h,w,d), is obtained by
performing r on the cubic patch in xi centered around (h, w, d):
rp
i,(h,w,d) = fr(xi,[h−p:h+p,w−p:w+p,d−p:d+p]) ,
(2)
where the input of fr is the cubic sub-volume.
we use a dataset of 172 patients containing 94 paaf and 78 peaf
cases collected from the sun yat-sen memorial hospital in china.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf:
in practice,
this also aligns with the fact that the annotated text information represents the
direct justiﬁcation for identifying lesion regions in the clinic.
(3)
hence, lia indicates which attribute the f t,: is closest to since each vector f t,:
is mapped from the t-th group of feature maps through the context network h(·).
[1] is a dataset for pulmonary nodule classiﬁcation or detec-
tion based on low-dose ct, which involves 1,010 patients.
we cropped all the nodules with a
square shape of a doubled equivalent diameter at the annotated center, then resized
them to the volume of 32 × 32 × 32.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf:
in clinical practice, patients may undergo a second round of rt to
achieve complete tumor control when the ﬁrst course of treatment fails
to eradicate cancer completely.
a region-preserving attention
module (ram) is designed to understand the long-range prior knowl-
edge of the esophageal structure, while preserving the regional patterns.
sparsely labeled medical images for various isolated tasks necessitate
eﬃcient utilization of knowledge from relevant datasets and tasks.
to ensure optimal treatment
outcomes, both the cancerous region and the adjacent organ-at-risk (oar) must
be accurately delineated, to focus the high-energy radiation solely on the cancer-
ous area while protecting the oars from any harm.
gross tumor volume (gtv)
represents the area of the tumor that can be identiﬁed with a high degree of cer-
tainty and is of paramount importance in clinical practice.
in the clinical setting, patients may undergo a second round of rt treatment
to achieve complete tumor control when initial treatment fails to completely
eradicate cancer
however, the precise delineation of the gtv is labor-
intensive, and is restricted to specialized hospitals with highly skilled rt experts.
a region-preserving attention module (ram) is designed to eﬀec-
tively capture the long-range prior knowledge in the esophageal structure, while
preserving regional tumor patterns.
our training approach leverages multi-center datasets containing relevant anno-
tations, that challenges the network to retrieve information from e1 using the features
from e2.
to achieve this, we eﬃciently exploit
knowledge from multi-center datasets that are not tailored for second-course
gtv segmentation.
during the second course of rt, a ct image
i2 of the same patient is acquired.
therefore, the input to encoder e1 consists of the concatenation of i1 and g1 to
encode the prior information (features f d
1 ) from the ﬁrst course, while encoder
e2 embeds both low- and high-level features f d
2 of the local pattern of i2 (fig. 1),
f d
1 = e1(i1, g1), f d
2 = e2(i2), d = 0, 1, 2, 3, 4
(1)
where the spatial shape of f d
1/2 is h
2d × w
2d × d
2d , with 2d+4 channels.
region-preserving attention module.
to eﬀectively learn the prior knowl-
edge in the elongated esophagus, we design a region-preserving attention module
514
y. sun et al.
(ram), as shown in fig.
here, we denote g1/g2 as prior/target
annotations respectively, which are not limited only to the gtv areas.
in summary, our training strategy is not dataset-speciﬁc or target-speciﬁc,
thus allowing the integration of prior knowledge from multi-center esophageal
gtv-related datasets, which eﬀectively improves the network’s ability to retrieve
information for the second course from the three key aspects stated in sect.
the paired ﬁrst-second course dataset, sp, is collected from sun yat-
sen university cancer center (ethics approval number: b2023-107-01), com-
prising paired ct scans of 69 distinct patients from south china.
we collected
the gtv dataset sv from medmind technology co., ltd., which has ct scans
from 179 patients.
additionally, we collect se from segthor [12],
consisting of ct scans and esophagus annotations from 40 patients who did not
516
y. sun et al.
table 1.
we randomly split sp into training and test datasets
at the patient-level.
the training dataset includes sv, se, and 41 patients from
sp (denoted as strain
p
), while the test dataset comprises 28 patients from sp
(denoted as stest
p
).
the ct volumes from the ﬁrst and second course
in sp are aligned based on the center of the lung mask [8].
notably, the paired ﬁrst-second course dataset stest
p
pertains to the same group
of patients, thereby ensuring that any performance drop can be attributed solely
to diﬀerences in courses of rt, rather than variations across diﬀerent patients.
figure 2 illustrates the reduction in the gtv area after the initial course of
rt, where the transverse plane is taken from the same location relative to the
vertebrae (yellow lines).
this suggests that deep learning-based approaches may not rely solely on the
identiﬁcation of malignant tissue patterns, as doctors do, but rather predict high-
risk areas statistically.
table 2 presents the information gain
derived from multi-center datasets using quantiﬁed metrics for segmentation
performance.
however, in fig. 3, it can be
observed that the model failed to accurately track the gtv area along the
esophagus (orange arrows) due to the soft and elongated nature of the esophageal
tissue, which deforms easily during ct scans performed at diﬀerent times.
3 that the network has a better
understanding of the tumor proliferation with sv, while it still fails to track the
gtv area along the esophagus as pointed by the orange arrow.
net-
works with inadequate knowledge of the esophagus may fail in identifying the tumor
within the esophagus (orange arrows), whereas a limited understanding of tumor mor-
phology can deteriorate the ability to detect the tumor in the adjacent area (blue
arrows).
region-preserving
attention
module.
we attribute the drawback is due to the
location-agnostic nature of the operations in mha, where the local regional
correlations are perturbed.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_8.pdf:
moreover, and likely more important, the use of gadolinium-based
contrast agents requires the cannulation of a vein, and the injection of
the contrast media which is cumbersome and places a burden on the
patient.
the results show that the multi-b-value dwi-based
fusion model can potentially be used to synthesize ce-mri, thus the-
oretically reducing or avoiding the use of gbca, thereby minimizing
the burden to patients.
keywords: contrast-enhanced mri · diﬀusion-weighted imaging ·
deep learning · multi-sequence fusion · breast cancer
1
introduction
breast cancer is the most common cancer and the leading cause of cancer death
in women
early detection of breast cancer allows patients to receive timely
treatment, which may have less burden and a higher probability of survival [6].
among current clinical imaging modalities, magnetic resonance imaging (mri)
has the highest sensitivity for breast cancer detection
however, the use of
gadolinium-based contrast agents (gbca) requires iv-cannulation, which is a
burden to patients, time consuming and cumbersome in a screening situation.
among them, synthe-
sis of ce-mri is very important as mentioned above, but few studies have been
done by researchers in this area due to its challenging nature.
studies have
shown that dwi could be used to detect lesions, distinguish malignant from
benign breast lesions, predict patient prognosis, etc
in particular,
dwi can capture the dynamic diﬀusion state of water molecules to estimate the
vascular distribution in tissues, which is closely related to the contrast-enhanced
regions in ce-mri. dwi may be a valuable alternative in breast cancer detection
in patients with contraindications to gbca [3]. inspired by this, we develop a
multi-sequence fusion network based on t1-weighted mri and multi-b-value
dwi to synthesize ce-mri.
2
methods
2.1
patient collection and pre-processing
this study was approved by institutional review board of our cancer institute
with a waiver of informed consent.
we retrospectively collected 765 patients with
breast cancer presenting at our cancer institute from january 2015 to novem-
ber 2020, all patients had biopsy-proven breast cancers (all cancers included
in this study were invasive breast cancers, and ductal carcinoma in situ had
been excluded).
in each convolutional layer group of the reconstruction module, we use two
3 × 3 ﬁlters (same padding) with strides 1 and 2, respectively.
if the ce-mri was successfully synthesized, the enhanced region would be high-
lighted in the diﬀerence mri, otherwise it would not.
fig.
the trade-oﬀ parameter λ1 was set
to 100 during training, and the trade-oﬀ parameter of the reconstruction loss in
the reconstruction module is set to 5. masks for all breasts were used (weighted
by a factor of 100 during the calculation of the loss between generated and real
ce-mri) to reduce the inﬂuence of signals in the thoracic area.
3. it can be
seen from the visualization results that after the diﬀerence between the generated
ce-mri and the original t1-weighted mri, the lesion position of the breast is
highlighted, the red circle represents the highlighted area, which proves that our
method can eﬀectively synthesize contrast-enhanced images, highlighting the
same parts as the real enhanced position.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf:
keywords: large-scale clinical dataset · deep-supervision · multi-scale
segmentation · breast ultrasound images
registration number: 4319
1
introduction
breast cancer is a serious health problem with high incidence and wide prevalence for
women throughout the world
clinical researches have shown that ultrasound imaging is an effective tool for screen-
ing breast cancer, due to its critical characteristics of non-invasiveness, non-radiation and
inexpensiveness [5–7].
92
m. li et al.
3
experiments
3.1
dataset and implementation details
we collected 10927 cases for this research from yunnan cancer hospital.
for external validation,
we further test our model on two independent publicly-available datasets collected by
stu-hospital (dataset 1)

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf:
https://doi.org/10.1007/978-3-031-43990-2_7
mammo-net for multi-view mammogram classiﬁcation
69
1
introduction
breast cancer is the most prevalent form of cancer among women and can have
serious physical and mental health consequences if left unchecked [5].
by
identifying breast cancer early, patients can receive targeted treatment before
the disease progresses.
radiologists typically focus
on areas with breast lesions during mammogram reading [11,22], which provides
valuable guidance.
furthermore, there are diﬀerences
70
c. ji et al.
between multi-view mammograms of the same patient, arising from variations
in breast shape and density.
after that, we
employ pyramid loss to make the network attention being consistent with the
supervision of radiologists’ gaze heat maps, guiding the network to focus on
the same lesion areas as the radiologists.
to enhance the learning of important attention areas, we pro-
pose a pyramid loss constraint that requires consistency between the network
and gaze attention maps.
||n(i)||2
,
(6)
mammo-net for multi-view mammogram classiﬁcation
73
where n(i) is a reconstruction of pv generated by a fully connected network n
from i and the euclidean norm || · ||2 is applied to obtain unit-length vectors.
in contrastive learning, we consider the same patient mammograms as positive
samples and those from diﬀerent patient mammograms in the same batch ˜
p
minimizing the similarity between the same
patient mammograms enables the model to learn shared features.
maximizing
the dissimilarity between diﬀerent patient mammograms enhances the model’s
robustness.
we used accuracy
(acc) and the area under the roc curve (auc)
this model requires gaze input during both the training
and inference stages, which limits its practical use in hospitals without eye-
trackers.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf:
asymmetry is a crucial characteristic of bilateral mammo-
grams (bi-mg) when abnormalities are developing.
keywords: bilateral mammogram · asymmetric transformer ·
disentanglement · self-adversarial learning · synthesis
1
introduction
breast cancer (bc) is the most common cancer in women and incidence is
increasing [14].
with the wide adoption of population-based mammography
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
however, it is
disentanglement of asymmetrical abnormality on bilateral mammograms
61
diﬃcult to train the generator in a supervised manner due to the lack of annota-
tions of the location for asymmetrical pairs.
for each tumor
insertion, we randomly select a position within the breast region.
the in-house dataset comprises 43,258 mammography exams from
10,670 women between 2004–2020, collected from a hospital with irb approvals.
in this study, we randomly select 20% women of the full dataset, comprising 6,000
normal (bi-rads = 1) and 28,732 abnormal (bi-rads ̸= 1) images.
each dataset is randomly split into training, validation, and testing sets at the
patient level in an 8:1:1 ratio, respectively (except for that inbreast which is
split with a ratio of 6:2:2, to keep enough normal samples for the test).
the training
takes 3–24 h (related to the size of the dataset) on each dataset.
to assess the performance of diﬀerent models in classiﬁcation tasks, we
calculate the area under the receiver operating characteristic curve (auc) met-
ric.
without using pixel-level asymmetry ground
disentanglement of asymmetrical abnormality on bilateral mammograms
65
truth from the “synthesis” method, our generator tends to excessively remove
asymmetric abnormalities at the cost of leading to the formation of black holes or
areas that are visibly darker than the surrounding tissue because of the limitation
of our discriminator and lack of pixel-level supervision.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf:
despite recent developments in ct planning that enabled
automation in patient positioning, time-consuming scout scans are still
needed to compute dose proﬁle and ensure the patient is properly posi-
tioned.
in this paper, we present a novel method which eliminates the
need for scout scans in ct lung cancer screening by estimating patient
scan range, isocenter, and water equivalent diameter (wed) from 3d
camera images.
while recent advances in human body modeling [4,5,12,13,15] have allowed for
automation of patient positioning, scout scans are still required as they are used
by automatic exposure control system in the ct scanners to compute the dose
to be delivered in order to maintain constant image quality [3].
any patient movement during the time between the two scans may cause mis-
alignment and incorrect dose proﬁle, which could ultimately result in a repeat
of the entire process.
finally, while minimal, the radiation dose administered to
the patient is further increased by a scout scan.
we introduce a novel method for estimating patient scanning parameters
from non-ionizing 3d camera images to eliminate the need for scout scans dur-
ing pre-scanning.
for ldct lung cancer screening, our framework automatically
estimates the patient’s lung position (which serves as a reference point to start
the scan), the patient’s isocenter (which is used to determine the table height
for scanning), and an estimate of patient’s water equivalent diameter (wed)
proﬁles along the craniocaudal direction which is a well established method for
deﬁning size speciﬁc dose estimate (ssde) in ct imaging
we trained our models on a large collection of ct scans acquired
from over 60, 000 patients from over 15 sites across north america, europe and
asia.
the contributions of this work can be summarized as follows:
– a novel workﬂow for automated ct lung cancer screening without the need
for scout scan
– a clinically relevant method meeting iec 62985:2019 requirements on wed
estimation.
– a generative model of patient wed trained on over 60, 000 patients.
– a novel method for real-time reﬁnement of wed, which can be used for dose
modulation
2
method
water equivalent diameter (wed) is a robust patient-size descriptor [17] used
for ct dose planning.
the wed of a patient is thus a function taking
as input a craniocaudal coordinate and outputting the wed of the patient at
that given position.
as our focus here is on lung cancer
screening, we deﬁne ‘wed proﬁle’ to be the 1d curve obtained by uniformly
sampling the wed function along the craniocaudal axis within the lung region.
we then train an encoder network to map the patient depth image to
the wed manifold.
in this approach, our latent vector represents the
encoding of a patient in the latent space.
this way, a single autodecoder can
learn patient-speciﬁc continuous wed functions.
2.3
real-time wed reﬁnement
while the depth image provides critical information on the patient anatomy,
it may not always be suﬃcient to accurately predict the wed proﬁles.
for
example, some patients may have implants or other medical devices that cannot
be guessed solely from the depth image.
first, we use our encoder network to initialize
the latent vector to a point in the manifold that is close to the current patient.
as the table
moves and the patient gets scanned, ct data is being acquired and ground truth
wed can be computed for portion of the body that has been scanned, along
with the corresponding craniocaudal coordinate.
1.
3
results
3.1
data
our ct scan dataset consists of 62, 420 patients from 16 diﬀerent sites across
north america, asia and europe.
(color ﬁgure online)
of depth image and ct scan from 2, 742 patients from 6 diﬀerent sites across
north america and europe acquired using a ceiling-mounted kinect 2 camera.
our evaluation set consists of 110 pairs of depth image and ct scan from 110
patients from a separate site in europe.
3.2
patient preparation
patient positioning is the ﬁrst step in lung cancer screening workﬂow.
we propose
to estimate the table position by regressing the patient isocenter and the starting
point of the scan by estimating the location of the patient’s lung top.
we deﬁne the starting position of the scan as the location
of the patient’s lung top.
[7] taking the camera depth
image as input and outputting a gaussian heatmap centered at the patient’s lung
top location.
we trained our model on 2, 742 patients using adaloss
we report an accuracy of 100% on our evaluation
set of 110 patients.
428
b. teixeira et al.
fig.
while the original prediction was oﬀ towards the center of the lung,
the real-time reﬁnement was able to correct the error.
isocenter.
the patient isocenter is deﬁned as the centerline of the patient’s
body.
[1] taking the camera depth image as input and
outputting the patient isocenter.
we trained our model
on 2, 742 patients using adadelta
2.
3.3
water equivalent diameter
we trained our autodecoder model on our unpaired ct scan dataset of 62, 420
patients with a latent vector of size 32.
the encoder was trained on our paired
ct scan and depth image dataset of 2, 742 patients.
we trained this baseline model on 2, 742 patients using the
adadelta
overall, our
approach shows best results with an update frequency of 20 mm, with a mean
lateral error of 15.93 mm and a mean ap error of 10.40 mm. figure 4 presents
a qualitative evaluation on patients with diﬀerent body morphology.
the δrel metric is deﬁned as:
δrel(z) =

ˆ
wed(z) − wed(z)
wed(z)

(1)
where:
–
ˆ
wed(z) is the predicted water equivalent diameter
– wed(z) is the ground truth water equivalent diameter
– z is the position along the craniocaudal axis of the patient.
430
b. teixeira et al.
fig.
4. qualitative analysis of the proposed method with 2 cm reﬁnement on patient
with diﬀerent morphology.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_54.pdf:
(1) the encoder group employs a vision
transformer backbone
(2) the gaussian-
probabilistic modeling group consists of a gaussian probabilistic guided
unet-like decoder branch(gudb) and gaussian probabilistic-induced transi-
tion(git) modules.
(3) the ensemble binary decoders group includes a
unet-like structure branch(udb)
2.2
encoder group
to balance the trade-oﬀ between computational speed and feature representation
capability, we utilize the pre-trained pvtv2-b2 model
moreover, the encoder
output features are presented as {xe
i }4
i=1 with channels of [2c, 4c, 8c, 16c].
2.3
gaussian-probabilistic modeling group
to incorporate both polyp location probability and surface pattern information
in a progressive manner, we propose the gaussian probabilistic-induced tran-
sition (git) method.
we further introduce residual learn-
ing in a parallel manner at diﬀerent group-aware scales.
2.4
ensemble binary decoders group
during colonoscopy, endoscopists often use the two-physician observation app-
roach to improve the detection rate of polyps.
false positives (fps) occur when a wrong detection
output is provided for a negative region, and false negatives (fns) occur
when a polyp is missed in a positive image.
we selected images from two unseen datasets with 0∼2% polyp labeled area
to perform the test.
small polyps are deﬁned as the polyp area accounts for 0∼2% of the
entire image.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf:
age-related macular degeneration (amd) is the leading
cause of blindness in the elderly.
current grading systems based on imag-
ing biomarkers only coarsely group disease stages into broad categories
that lack prognostic value for future disease progression.
our method represents patient
time series as trajectories in a latent feature space built with contrastive
learning.
https://doi.org/10.1007/978-3-031-43990-2_68
clustering disease trajectories for temporal biomarker proposal in amd
725
keywords: contrastive learning · biomarker discovery · clustering ·
disease trajectories · age-related macular degeneration
1
introduction
age-related macular degeneration (amd) is the leading cause of blindness in the
elderly, aﬀecting nearly 200 million people worldwide [24].
patients with early
stages of the disease exhibit few symptoms until suddenly converting to the late
stage, at which point their central vision rapidly deteriorates
[12]. clinicians
currently diagnose amd, and stratify patients, using biomarkers derived from
optical coherence tomography (oct), which provides high-resolution images of
fig.
however, the widely adopted amd grading system [7,13], which
coarsely groups patients into broad categories for early and intermediate amd,
only has limited prognostic value for late amd.
in their search for new biomarkers, clinicians have annotated known biomark-
ers in longitudinal datasets that monitor patients over time and mapped them
against disease progression [2,16,19].
at the core of our method is the novel strategy to
represent patient time series as trajectories in a latent feature space.
then, we identify population-level patterns of
amd progression by clustering these sub-trajectories using a newly introduced
distance metric that encodes three distinct temporal criteria.
late amd is classiﬁed into either
choroidal neovascularisation (cnv), identiﬁed by subretinal ﬂuid, or geographic
atrophy, signalled by progressive loss of photoreceptors and retinal thinning.
contrastive methods
[3,8,26] encode invariance to a set of image transformations, which are uncorre-
lated with disease features, resulting in a more expressive feature space.
however, all aforementioned methods group single images acquired at one point
in time, and in doing so neglect temporal dynamics.
afterwards, we test our method on
a second independent unseen dataset, which was obtained from moorﬁelds eye
hospital.
after strict quality control, the development dataset
consists of 46,496 scans of 6,236 eyes from 3,456 patients.
the unseen dataset
is larger, containing 114,062 scans of 7,253 eyes from 3,819 patients.
visual acuity scores, which measured the patient’s functional
quality of vision using a logmar chart, are available at 83,964 time points.
we illustrate clusters assignments, denoted by colour,
resulting from three combinations of φ and λ.
3.3
extracting sub-trajectories via partitioning
naively clustering whole time series of patients ignores two characteristics of
longitudinal data.
firstly, individual time series are not directly comparable as
patients enter and leave the study at diﬀerent stages of their overall progression.
inspired by traclus [11], the state of the art in trajectory clustering,
we adapt their partition-and-group framework by assuming that trajectories can
clustering disease trajectories for temporal biomarker proposal in amd
729
be partitioned into a common set of sub-trajectories that capture singular tran-
sitions between progressive states of the disease.
the
ﬁrst, formulated in eq. 2, matches two sub-trajectories, u and v , of patients
who progress between the same start and end states:
dtransition(u, v ) = ∥ustart − vstart∥2
however, by ignoring intermediary images, this metric does not respect the
disease pathway along which patients progress.
(4)
spectral clustering: as the non-linearity of dsubtraj prohibits the use of
k-means for clustering, we instead use spectral clustering [22] to group similar
sub-trajectories.
using a, we group
sub-trajectories into k clusters.
clusters show two representative sub-trajectories originating from
diﬀerent patients, each containing ﬁve longitudinal images with the time and location
of greatest progression marked by arrows.
3.5
qualitative and quantitative evaluation of clusters
initially, we tune the hyperparameters, λ, φ and k, on the development dataset
by heuristically selecting values that result in higher uniformity between sub-
trajectories within each cluster.
two teams of two ophthalmologists then review
20 sub-trajectories from distinct patients in each cluster, interpreting and sum-
marising any consistently observed temporal dynamics.
we also
include a demographic baseline using age and sex.
each experiment uses 10-fold
cross validation on random 80/20 partitions, while ensuring a patient-wise split.
development dataset
time to late amd ↓ time to cnv ↓ time to crora ↓ current visual acuity ↓
demographic
0.756±0.01
0.822±0.012
0.703±0.028
0.381±0.007
current grading system
0.757±0.01
0.819±0.012
0.685±0.035
0.367±0.008
single timepoint clusters
0.747±0.013
0.776±0.015
0.630±0.05
0.230±0.005
sub-trajectory clusters
0.739±0.01
0.748±0.011
0.636±0.031
0.375±0.007
fully supervised
0.709±0.015
0.726±0.012
0.609±0.033
0.199±0.004
unseen dataset
demographic
1.343±0.027
1.241±0.017
1.216±0.062
0.188±0.007
current grading system
1.308±0.018
1.244±0.022
1.286±0.053
0.177±0.008
single timepoint clusters
1.325±0.049
1.341±0.080
1.297±0.096
0.136±0.005
sub-trajectory clusters
1.322±0.029
1.235±0.027
1.257±0.056
0.188±0.006
fully supervised
1.301±0.044
1.298±0.08
1.255±0.097
0.135±0.006
4
experiments and results
sub-trajectory clusters are candidate temporal biomarkers: by ﬁrst
applying our method to the development dataset we found that using λ = 0.75,
φ = 0.75 and k = 30 resulted in the most uniform and homogeneous clusters
while still limiting the total number of clusters to a reasonable amount.
in all tasks the standard biomarkers are only marginally more indicative of risk
than the patient’s age and sex.
as late stage patients were overrepresented in our
datasets, we also intend to apply our method to datasets with greater numbers
of patients progressing from earlier disease stages.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf:
compared with the other weak annotations, scribbles can
provide more location information about the segmentation targets, especially
for objects with irregular shapes [1].
3
experiments and results
3.1
dataset and implementation details
we used the publicly available abdomen ct dataset word [17] for experiments,
which consists of 150 abdominal ct volumes from patients with rectal cancer,
prostate cancer or cervical cancer before radiotherapy.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf:
ai generated vir-
tual contrast-enhanced mri (vce-mri) in primary gross-tumor-volume (gtv)
delineation for patients with nasopharyngeal carcinoma (npc).
we retrospec-
tively retrieved 303 biopsy-proven npc patients from three oncology centers.
288 patients were used for model training and 15 patients were used to synthesize
vce-mri for clinical evaluation.
results showed the
mean accuracy to distinguish vce-mri from ce-mri was 53.33%; no signiﬁ-
cant difference was observed in clarity of tumor-to-normal tissue interface between
vce-mri and ce-mri; for the veracity of contrast enhancement in tumor inva-
sion risk areas and efﬁcacy in primary tumor staging, a jaccard index of 76.04%
and accuracy of 86.67% were obtained, respectively.
npc is characterized by a distinct
geographical distribution in southeast asia, north africa, and arctic
however, accurately delineating the npc tumor is challenging
due to the highly inﬁltrative nature of npc and its complex location, which is surrounded
by critical organs such as brainstem, spinal cord, temporal lobes, etc. to improve the
visibility of npc tumor for precise gross-tumor-volume (gtv) delineation, contrast-
enhanced mri (ce-mri) is administrated through injection of gadolinium-based con-
trast agents (gbcas) during mri scanning.
despite the superior tumor-to-normal tis-
sue contrast of ce-mri, the use of gbcas during mri scanning can result in a fatal
systemic disease known as nephrogenic systemic ﬁbrosis (nsf) in patients with renal
insufﬁciency [4].
it was reported that
the incidence rate of nsf is around 4% after gbca administration in patients with
severe renal insufﬁciency, and the mortality rate can reach 31%
currently, there
is no effective treatment for nsf, making it crucial to ﬁnd a ce-mri alternative for
patients at risk of nsf.
in recent years, artiﬁcial intelligence (ai), especially deep learning, plays a game-
changingroleinmedicalimaging[7,8],whichshowedgreatpotentialtoeliminatetheuse
of the toxic gbcas through synthesizing virtual contrast-enhanced mri (vce-mri)
from gadolinium-free sequences, such as t1-weighted (t1w) and t2-weighted (t2w)
mri
in the area of rt, li et al.
in addition to the advantage
of eliminating the use of gbca, vce-mri synthesis can also speed up the clinical
workﬂow by eliminating the need for acquiring ce-mri scan, which saves time for
both clinical staff and patients.
furthermore, the clinical evalu-
ation of ai-based techniques can help identify areas for improvement and optimization,
leading to development of more effective algorithms.
clinical evaluation of ai-assisted virtual contrast enhanced mri
543
to bridge this bench-to-bedside research gap, in this study, we conducted a series of
clinicalevaluationstoassesstheeffectivenessofsyntheticvce-mriinnpcdelineation,
with a particular focus on assessment in vce-mri image quality and primary gtv
delineation.
the
success of this study would ﬁll the current knowledge gap and provide the medical
community with a clinical reference prior to clinical application of the novel vce-mri
technique in npc rt.
2
materials and methods
2.1
data description
patient data was retrospectively collected from three oncology centers in hong kong.
this dataset included 303 biopsy-proven (stage i-ivb) npc patients who received radi-
ation treatment during 2012–2016.
the three hospitals were labelled as institution-1
(110 patients), institution-2 (58 patients), and institution-3 (135 patients), respectively.
for each patient, t1w mri, t2w mri, gadolinium-based ce-mri, and planning ct
were retrieved.
mri images were automatically registered as mri images for each
patient were scanned in the same position.
the use of this dataset was approved by the
institutional review board of the university of hong kong/hospital authority hong
kong west cluster (hku/ha hkw irb) with reference number uw21-412, and the
research ethics committee (kowloon central/kowloon east) with reference number
kc/ke-18-0085/er-1.
due to the retrospective nature of this study, patient consent was
waived.
for model development, 288 patients were used for model development and
15 patients were used to synthesize vce-mri for clinical evaluation.
the details of
patient characteristics and the number split for training and testing of each dataset were
illustrated in table 1.
the effectiveness of this network in vce-mri synthesis for npc patients
has been demonstrated by li et al.
different from the original study, which used
single institutional data for model development and utilized min-max value of the whole
dataset for data normalization, in this work, we used mean and standard deviation of
each individual patient to normalize mri intensities due to the heterogeneity of the mri
intensities across institutions [15].
544
w. li et al.
table 1. details of the multi-institutional patient characteristics.
institution
(vendor-fs)
patient no.
(train/test)
avg. age
modality
tr (ms)
te (ms)
institution-1
(siemens-1.5t)
110 (105/5)
56 ± 11
t1w
562–739
13–17
t2w
7640
97
ce-mri
562–739
13–17
institution-2
(philips-3t)
58 (53/5)
49 ± 15
t1w
4.8–9.4
2.4–8.0
t2w
3500–4900
50–80
ce-mri
4.8–9.4
2.4–8.0
institution-3
(siemens-3t)
135 (130/5)
57 ± 12
t1w
620
9.8
t2w
2500
74
ce-mri
3.42
1.11
2.3
clinical evaluations
the evaluation methods used in this study included image quality assessment of vce-
mri and primary gtv delineation.
considering the clinical burden of oncologists, 15 patients were included for
clinical evaluations.
to evaluate the image quality of synthetic
vce-mri against the real ce-mri, we conducted four rt-related evaluations: (i) dis-
tinguishability between ce-mri and vce-mri; (ii) clarity of tumor-to-normal tissue
interface; (iii) veracity of contrast enhancement in tumor invasion risk areas; and (iv)
efﬁcacy in primary tumor staging.
the vce-mri and ce-mri volumes were imported
as individual patients to eclipse system and randomly and blindly shown to oncologists
for evaluation.
(i) distinguishability between ce-mri and vce-mri. to evaluate the reality of vce-
mri, oncologists were invited to differentiate the synthetic patients (i.e., image
volumes that generated from synthetic vce-mri) from real patients (i.e., image
volumes that generated from real ce-mri).
the judgement results were recorded, and the
accuracy of each institution and the overall accuracy were calculated.
paired two-tailed t-test (with a signiﬁcance level of p = 0.05) was applied to analyses
if the scores obtained from real patients and synthetic patients are signiﬁcantly
different.
(iii) veracity of contrast enhancement in tumor invasion risk areas.
in addition to the
critical tumor-normal tissue interface, the areas surrounding the npc tumor will
also be considered during delineation.
to better evaluate the veracity of contrast
enhancement in vce-mri, we selected 25 tumor invasion risk areas according to
[16], including 13 high-risk areas and 12 medium-risk areas, and asked oncologists
to determine whether these areas were at risk of being invaded according to the
contrast-enhanced tumor regions.
the 13 high-risk areas include: retropharyngeal
space, parapharyngeal space, levator veli palatine muscle, prestyloid compartment,
tensor veli palatine muscle, poststyloid compartment, nasal cavity, pterygoid pro-
cess, basis of sphenoid bone, petrous apex, prevertebral muscle, clivus, and foramen
lacerum.
the 12 medium-risk areas include foramen ovale, great wing of sphenoid
bone, medial pterygoid muscle, oropharynx, cavernous sinus, sphenoidal sinus,
pterygopalatine fossa, lateral pterygoid muscle, hypoglossal canal, foramen rotun-
dum, ethmoid sinus, and jugular foramen.
the areas considered at risk of tumor
invasion were recorded.
[17] was utilized to quantitatively evaluate the results of
recorded risk areas from ce-mri and vce-mri.
the ji could be calculated by:
ji = |rce ∩ rvce|/|rce ∪ rvce|
(1)
where rce and rvce represents the set of risk areas that recorded from ce-mri and
corresponding vce-mri, respectively.
an accurate
tumor delineation improves local control and reduce toxicity to surrounding normal
tissues, thus potentially improving patient survival [20]. to evaluate the feasibility of
eliminating the use of gbca by replacing ce-mri with vce-mri in tumor delineation,
oncologists were asked to contour the primary gtv under assistance of vce-mri.
for
comparison, ce-mri was also imported to eclipse for tumor delineation but assigned
as a different patient, which were shown to oncologists in a random and blind manner.
to mimic the real clinical setting, contrast-free t1w, t2w mri and corresponding ct
546
w. li et al.
of each patient were imported into the eclipse system since sometimes t1w and t2w
mri will also be referenced during tumor delineation.
due to both real patients and syn-
thetic patients were involved in delineation, to erase the delineation memory of the same
patient, we separated the patients to two datasets, each with the same number of patients,
both two datasets with mixed real patients and synthetic patients without overlaps (i.e.,
the ce-mri and vce-mri from the same patient are not in the same dataset).when ﬁn-
ished the ﬁrst dataset delineation, there was a one-month interval before the delineation
of the second dataset.
after the delineation of all patients, the dice similarity coefﬁ-
cient (dsc)
[22] of the gtvs delineated from real
patients and corresponding synthetic patients were calculated to evaluate the accuracy
of delineated contours.
the
dsc can be expressed as:
dsc = 2 ∗ |cce ∩ cvce|/(|cce| + |cvce|)
(2)
where cce and cvce represent the contours delineated from real patients and synthetic
patients, respectively.
(3)
where d(x, cvce) and d(y, cce) represent the distance from point x in contour cce to
contour cvce and the distance from point y in contour cvce to contour cce.
3
results and discussion
3.1
image quality of vce-mri
table 2 summarizes the results of the four vce-mri quality evaluation metrics, includ-
ing: (i) distinguishability between ce-mri and vce-mri; (ii) clarity of tumor-to-
normal tissue interface; (iii) veracity of contrast enhancement in tumor invasion risk
areas; and (iv) efﬁcacy in primary tumor staging.
for institution-1, 2 real patients were judged as synthetic
and 1 synthetic patient was considered as real.
for institution-2, 2 real patients
clinical evaluation of ai-assisted virtual contrast enhanced mri
547
were determined as synthetic and 4 synthetic patients were determined as real.
for
institution-3, 2 real patients were judged as synthetic and 3 synthetic patients were
considered as real.
in total, 6 real patients were judged as synthetic and 8 synthetic
patients were judged as real.
the overall clarity scores of tumor-
to-normal tissue interface for real and synthetic patients were 3.67 with a median of
4 and 3.47 with a median of 4, respectively.
the average scores for real and synthetic
patients were 3.6 and 3, 3.6 and 3.8, 3.8 and 3.6 for institution-1, institution-2, and
institution-3, respectively.
5 real patients got a higher score than synthetic patients
and 3 synthetic patients obtained a higher score than real patients.
the scores of the
other 7 patient pairs were the same.
(iii) veracity of contrast enhancement in tumor invasion risk areas.
the overall
ji score between the recorded tumor invasion risk areas from ce-mri and vce-
mri was 74.06%.
the average ji obtained from institution-1, institution-2, and
institution-3 dataset were similar with a result of 71.54%, 74.78% and 75.85%,
respectively.
in total, 126 risk areas were recorded from the ce-mri for all of the
evaluation patients, while 10 (7.94%) false positive high risk invasion areas and 9
(7.14%) false negative high risk invasion areas were recorded from vce-mri.
(iv) efﬁcacy in primary tumor staging.
a t-staging accuracy of 86.67% was obtained
using vce-mri. 13 patient pairs obtained the same staging results.
for the
institution-2 data, all synthetic patients observed the same stages as real patients.
for the two t-stage disagreement patients, one synthetic patient was staged as phase
iv while the corresponding real patient was staged as phase iii, the other synthetic
patient was staged as i while corresponding real patient was staged as phase iii.
table 2. image quality evaluation results of vce-mri: (a) distinguishability between ce-mri
and vce-mri; (b) clarity of tumor-to-normal tissue interface; (c) veracity of contrast enhance-
ment in risk areas; and (d) t-staging.
abbreviations: inst: institution; c.a.: center-based average;
o.a.: overall average; syn: synthetic.
(a)
(b)
inst-1
inst-2
inst-3
inst-1
inst-2
inst-3
/
/
/
real
syn
real
syn
real
syn
c.a.
70%
40%
50%
3.6
3
3.6
3.8
3.8
3.6
o.a.
53.33%
real: 3.67
syn: 3.47
(c)
(d)
inst-1
inst-2
inst-3
inst-1
inst-2
inst-3
c.a.
71.54%
74.78%
75.85%
80%
100%
80%
o.a.
74.06%
86.67%
548
w. li et al.
figure 1 illustrates an example of the synthetic vce-mri.
for institution-1, institution-2, and institution-3, the average dsc were
0.741, 0.794 and 0.751 respectively, while the average hd were 2.303 mm, 1.456 mm,
and 2.037 mm respectively.
figure 2 illustrated the delineated primary gtv contours
from an average patient with the dsc of 0.765 and hd of 1.938 mm.
the green contour
shows the primary gtv that delineated form the synthetic patient, while the red contour
was delineated from corresponding real gbca-based patient.
fig.
2. illustration of the primary gtvs from a typical patient with an average dsc and hd.
clinical evaluation of ai-assisted virtual contrast enhanced mri
549
4

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf:
however,
the da scheme received less attention, despite its potential to leverage the data
characteristic and address overﬁtting as the root of generalization problems.
recent motion models rely on dnns using either a fem
model [15] or complex training with population-based models [18].
inducing this kind of soft tissue deforma-
tion ultimately led to improved model performance in patient- and lesion-level
pca detection on an independent test set.
fig.
finally, we quantify the eﬀect of our proposed transformation on the clinical
task of patient-level pca diagnosis and lesion-level pca detection.
to consider clinically informative results,
we use the partial area under the receiver operating characteristic (pauroc)
for patient-level evaluation with the sensitivity threshold of 78.75%, which is
90% of the sensitivity of radiologists for pi-rads ≥ 4.
afterward, we evaluate
model performances on object-level using the free-response receiver operating
characteristic (froc) and the number of detections at the radiologists’ lesion
level performance for pi-rads ≥ 4, at 0.32 average number of false positives
per scan.
every patient
underwent extended systematic and targeted mri trans-rectal ultrasound-fusion
transperineal biopsy.
the samples were evaluated according to the inter-
national society of urological pathology (isup) standards under the supervision
of a dedicated uropathologist.
we summarize the patient-level pauroc and f1-scores; and
lesion-level froc results on the independent test set showing the advantage
of using anatomy-informed da.
both variants of the proposed anatomy-informed da (3.a and 3.b) increased
the sensitivity value around the clinical pi-rads ≥ 4 performance point compared to
the simple (1) and random elastic (3) da schemes, approaching it closely.
extending the basic da scheme with the proposed anatomy-informed defor-
mation not only increased the sensitivity closely matching the radiologists’
patient-level diagnostic performance but also improved the detection of pca
538
b. kovacs et al.
on a lesion level.
interestingly, while the use of random deformable transforma-
tion also improved lesion-level performance, it did not approach the diagnostic
performance of the radiologists, unlike the anatomy-informed da.
at the selected patient- and object-level working points, the model with the
proposed rectum- and bladder-informed da scheme reached the best results with
signiﬁcant improvements (p < 0.05) compared to the model with the basic da
setting by increasing the f1-score with 5.11% and identifying 4 more lesions
(5.3%) from the 76 lesions in our test set.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf:
particularly, we design a dynamical conﬁdence constraint (dcc) loss that con-
strains the model to focus on similar areas of wsis for both tasks.
based on ordered conﬁdence weights, we constrain the prediction networks of
histology and molecular markers to focus on the wsi areas important for both
predictions, thus modeling inter-omic interactions.
this publication presents independent research funded by the national institute for
health and care research (nihr).
sjp is funded by national institute for health and care research (nihr) clinician
scientist fellowship (nihr/cs/009/011).
jiang, s., zanazzi, g.j., hassanpour, s.: predicting prognosis and idh mutation
status for patients with lower-grade gliomas using whole slide images.
trpkov, k., et al.: new developments in existing who entities and evolving molecu-
lar concepts: the genitourinary pathology society (gups) update on renal neoplasia.
zhang, y., luo, l., dou, q., heng, p.a.: triplet attention and dual-pool contrastive
learning for clinic-driven multi-label medical image classiﬁcation.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf:
the
ﬁrst network is a conventional semantic segmentation network which
extracts a kidney region mask and an initial tumor region mask.
the
second network, which we name protuberance detection network, identi-
ﬁes the protruded regions from the kidney region mask.
given the initial
tumor region mask and the protruded region mask, the last network fuses
them and predicts the ﬁnal kidney tumor mask accurately.
thus, early detection of kidney tumors
can help to improve patient’s prognosis.
the ﬁrst is a base network, which extracts kidneys
and an initial tumor region masks.
the second protuberance detection network
receives the kidney region mask as its input and predicts a protruded region
mask.
the last fusion network receives the initial tumor mask and the pro-
truded region mask to predict a ﬁnal tumor mask.
while this method demonstrated high
sensitivity (95%), its false positives per patient remained high (15 false positives
per patient).
3
proposed method
to capture the protuberances in kidneys, we speciﬁcally train a protuberance
detection network, which receives a kidney region mask as an input and separates
protruded regions from it.
the ﬁrst base network is responsible for predicting kidney and tumor region
masks.
in detail,
we perform a summation of the initial tumor mask and the protruded region
mask, and then concatenate the result with the input image.
to enable a segmentation of protruded regions only, a sep-
arate annotation of each region is usually required.
however, annotating such
areas is time-consuming and preparing a large number of data is challenging.
the ﬁrst channel is the input image, and the second
channel is the result of summation of the initial tumor mask and the protruded
region mask.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf:
we propose the object-wise intensity-
sum loss, a simple yet surprisingly eﬀective metric invariant to muscle
deformation and projection direction, utilizing information in ct and
x-ray images collected from the same patient.
through the evaluation using a 539-patient dataset, we showed that
the proposed method signiﬁcantly outperformed conventional methods.
however, dxa and ct
require special equipment that is much less accessible in a small clinic.
the contribution of this paper
is three-fold: 1) proposal of the object-wise intensity-sum (owis) loss, a simple
yet eﬀective metric invariant to muscle deformation and projection direction, for
quantitative learning of the absolute volume and lean mass of the muscles, 2)
proposal of partially aligned training utilizing the aligned (paired) dataset for
the rigid object for the pixel-wise supervision in an unpaired image translation
task, 3) extensive evaluation of the performance using a 539-patient dataset.
1. variations in the muscle volume and lean muscle density among the 552 patients
in our dataset.
patient #1 (young, male) and patient #2 (old, female) had similar
bmi and almost the same gluteus maximus volume, while the lean muscle mass was
signiﬁcantly diﬀerent, likely due to the fatty degeneration in patient #2, which was
clearly observable in the projections of the lean muscle mass volume.
fig.
we collected a dataset
of 552 patients subject to the total hip arthroplasty surgery (455 females and
500
y. gu et al.
97 males, height 156.9 ± 8.3 cm, weight 57.5 ± 11.9 kg, bmi 23.294 ± 3.951
ethical approval was obtained from the institutional review
boards of the institutions participating in this study (irb approval numbers:
15056-3 for osaka university and 2019-m-6 for nara institute of science and
technology).
we acquired a pair of pre-operative x-ray and ct images from
each patient, assuming consistency in bone shape, lean muscle mass, and muscle
volume.
then, object-wise drrs
for the three conversions were generated for each segmented individual object
(bone/muscle) region.
a 2d-3d registration [21] of each
bone between ct and x-ray image of the same patient was performed to obtain
its drr aligned with the x-ray image, which is used in the proposed partially
aligned training.
furthermore, the conventional method did not utilize the paired
information of an x-ray image and drr (obtained from the same patient).
we
took advantage of the paired information, proposing the object-wise intensity-
sum loss, a simple yet eﬀective metric invariant to patient pose and projection
direction, for quantitative learning.
chen, l.-k., lee, w.-j., peng, l.-n., liu, l.-k., et al.: recent advances in sarcope-
nia research in asia: 2016 update from the asian working group for sarcopenia.
https://doi.org/10.1002/jcsm.12783
5. shu, x., lin, t., wang, h., zhao, y., et al.: diagnosis, prevalence, and mortality of
sarcopenia in dialysis patients: a systematic review and meta-analysis.
https://doi.org/10.1002/jcsm.12890
6. edwards, m.h., dennision, e.m., sayer, a.a., et al.: osteoporosis and sarcopenia
in older age.
https://doi.org/10.1123/ijsnem.2013-0228
9. feliciano, e.m.c., et al.: evaluation of automated computed tomography segmen-
tation to assess body composition and mortality associations in cancer patients.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf:
the current standard of care for evaluating surgical suc-
cess is to investigate the resection margins, which refers to the area surrounding
the excised tumor.
2.1
data curation
ex-vivo: data is collected from fresh breast tissue samples from the patients
referred to bcs at kingston health sciences center over two years.
the study
is approved by the institutional research ethics board and patients consent to be
included.
in total 51 cancer
and 149 normal spectra are collected and stratiﬁed into ﬁve folds (4 for cross
validation and 1 prospectively) with each patient restricted to one fold only.
intra-operative: a stream of iknife data is collected during a bcs case
(27 min) at kingston health sciences center.
clinical relevance: hormone receptor status plays an important role in deter-
mining breast cancer prognosis and tailoring treatment plans for patients [6].
as can be seen, the proposed egt model with aver-
age accuracy of 94.1% outperformed all the baselines statistically signiﬁcantly
(maximum p-values of 0.02 in one-tail paired wilcoxon signed-rank test).
average(standard deviation) of accuracy (acc), balanced accuracy (bac)
sensitivity (sen), speciﬁcity (spc), and the area under the curve (auc) for the
proposed evidential graph transformer in comparison with graph transformer (gtn),
graph convolution (gcn), and non-graph convolution (cnn) baselines.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf:
3c; it shows that our model assigns the highest weight
to the “dark corner” group, as the domain distance between “dark corner” and
derm7pt-clin is the closest, as shown in fig.
further, the “clean” group is assigned the
smallest weight as the domain distance between them is the largest, indicating
that their domains are signiﬁcantly diﬀerent and contain less useful information
for target domain prediction.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_48.pdf:
inspired by dab-detr, we propose to use
an anchor circle (x, y, r) as the query for circular object detection, where (x, y)
is the center of the circle and r is the radius.
we propose a novel circle cross
attention module which enables us to apply circle center (x, y) to extract image
features around a circle and make use of circle radius to modulate the cross
attention map.
ˆci = (ˆx, ˆy, ˆr) consists of the circle center and
circle radius.
s
and m are used to measure the performance of small scale with area less than
322 and median scale with area between 322 and 962.
3.2
implementation details
two variants of our proposed method for nuclei detection, circleformer and
circleformer-d are built with a circle cross attention module and a deformable
circle cross attention module, respectively.
we extend transformer-based box detection method to provide additional seg-
mentation output inside the detection region, denoted as deformable-detr-
joint.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf:
reported metrics
(in %age) are the average across 3 runs.
3.3
results
we chose overall accuracy and area under receiver operating characteristic
curve (auroc) as the evaluation metrics.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf:
due to model complexity and limited training data, ml performance often varies
across data subgroups or domains, such as diﬀerent patient subpopulations or
varied data acquisition scenarios.
the main objective of
unsupervised clustering is to group data points into distinct classes of similar
traits.
however, due to the complexity and high dimensionality of the medical
imaging data and the resulting diﬃculty in establishing a concrete notion of
similarity, extracting low-dimensional characteristics becomes the key to estab-
lishing the best criteria for grouping.
deep unsupervised clustering algorithms could map the medical imaging
data back to their causal factors or underlying domains, such as image acqui-
sition equipment, patient subpopulations, or other meaningful data subgroups.
to that end, we propose a mechanism that is intended
to constrain the model towards identifying clusters in the data that are not
associated with given variables of choice (already known class labels or sub-
group structures).
notably,
both vade and dec end up clustering the data by color, as it is the most
striking distinguishing characteristic of these images.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf:
further,
we introduce a semantic foreground-background adversarial loss during
training that aids in delineating the region of mitochondria instances
from the background clutter.
– to accurately delineate the region of mitochondria instances from the clut-
tered background, we further introduce a semantic foreground-background
(fg-bg) adversarial loss during the training that aids in learning improved
instance-level features.
(c) the lfg−bg
loss improves the instance-level features, thereby aiding in the better separability of
the region of mitochondria instances from the cluttered background.
semantic fg-bg adversarial loss: as discussed earlier, a common chal-
lenge in mitochondria instance segmentation is to accurately delineate the region
of mitochondria instances from the cluttered background.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf:
the synthetic histopathology images
paired with synthetic instance maps will be added to the real dataset for
this work is supported by chinese key-area research and development program of
guangdong province (2020b0101350001), and the guangdong basic and applied basic
research foundation (2023a1515011464, 2020b1515020048), and the national natural
science foundation of china (no. 62102267, no. 61976250), and the shenzhen science
and technology program (jcyj20220818103001002, jcyj20220530141211024), and
the guangdong provincial key laboratory of big data computing, the chinese uni-
versity of hong kong, shenzhen.
intuitively, since the labeled images are
samples from the population of histopathology images, if the underlying distri-
bution of histopathology images is learned, one can generate inﬁnite images and
their pixel-level labels to augment the original dataset.
the distance transform consists
of the horizontal and the vertical distance transform, which are obtained by cal-
culating the normalized distance of each pixel in a nucleus to the horizontal and
the vertical line passing through the nucleus center [5].
patches close to the cluster centers are selected.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf:
most existing methods classify
nuclei independently or do not make full use of the semantic similarity
betweennucleiandtheirgroupingfeatures.inthispaper,weproposeanovel
end-to-end nuclei detection and classiﬁcation framework based on a group-
ingtransformer-basedclassiﬁer.thenucleiclassiﬁerlearnsandupdatesthe
representations of nuclei groups and categories via hierarchically grouping
the nucleus embeddings.
for
the eﬃciency of the fully transformer-based framework, we take the nucleus
group embeddings as the input prompts of backbone, which helps harvest
grouping guided features by tuning only the prompts instead of the whole
backbone.
for example, the
this work was supported in part by the chinese key-area research and development
program of guangdong province (2020b0101350001), in part by the national natural
science foundation of china (no. 62102267, no. 61976250), in part by the guangdong
basic and applied basic research foundation (2023a1515011464, 2020b1515020048),
in part by the shenzhen science and technology program (jcyj20220818103001002,
jcyj20220530141211024), and the guangdong provincial key laboratory of big data
computing, the chinese university of hong kong, shenzhen.
in this paper, we aim to obtain the location and category of cells, which
only needs aﬀordable labels of centroids or bounding boxes.
diﬀerently, in
pathological images, experts often identify nuclear communities via their rela-
tionships and spatial distribution.
[11] build a location-based
graph for nuclei classiﬁcation.
on the other hand, there exist domain gaps in the patho-
logical images of diﬀerent organs, staining, and institutions, which makes it nec-
essary to ﬁne-tune models to new applications.
we
prepend the embeddings of nucleus clusters to the input space and freeze the
entire pre-trained transformer backbone so that these group embeddings act
as prompt information to help the backbone extract grouping-aware features.
the channel number of each feature map is
aligned via a 1 × 1 convolution layer and a group normalization operator.
encoder and decoder.
then we merge the embeddings belonging to the same
group into a primary group via eq.
to learn group-aware representations, we
further propose to share the embeddings of prompts with those of initial groups
in the proposed gtc.
[1] is a breast cancer dataset with three types and consists of
120 image tiles from 113 patients.
‘detached gtc & pt’ means that group features and
prompts are independent.
when the group number is large than 64, the groups may contain too
few nuclei to capture their common patterns.
it is suggested to set the group
number to a moderate value such as 64.
prompt-based grouping transformer
577
table 3.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_50.pdf:

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf:
to that end, we
randomly sample from each sequence i pairs of smaller patches x1, x2 ∈ rh×w
from the same spatial location but consecutive time points x1 ⊂ it, x2 ⊂ it+1.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf:
it has several special characteristics including 1) avoidance of
image augmentation, 2) huge number of channels, 3) simple end-to-end architec-
ture, 4) free from the interference of multi-texture-pattern arrangements.
brachial plexus syndrome occurs not
infrequently in patients with malignant disease.
however, how to most effectively combine texture features with deep
learning, called deep texture, is still an open area of research.
2
materials and method
2.1
dataset preparation and preprocessing
following irb approval for this study, we search for patients with metastatic breast
cancer who had a breast cancer mri performed between 2010 and 2020 and had mor-
phologically positive bp on the mri report from our electronic medical records (emr)
in * hospital.
totally, 189 patients including 141 normal patients and 41 abnormal ones
are obtained.
the range of the age are varying from 15 to 85 years old.
the female patient
number and male patient number are almost even.
all patient experiences three kinds of mri sequences including t2,
t1 and post-gadolinium.
approximately 25% sequences are conducted with ge signa
hdi 1.5t and the rest are produced by ge signa™ hero 3.0t. patients are required
to maintain a decubitus position while scanning.
only patients that had all three sequences segmented (t2, t1
and post-gadolinium) were included in the dataset.
according above requirements, we developed a method to produce a serial of novel
texture patterns by introducing a directed triangle idea with an adjacent triple pixel as a
ternary group, called triple point pattern (tpp), to extract the local texture information.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf:
keywords: liver tumor ablation · needle-based procedures ·
patient-speciﬁc interventions · ct-guidance · medical image
augmentation
1
introduction
needle-based liver tumor ablation techniques (e.g., radiofrequency, microwave,
laser, cryoablation) have a great potential for local curative tumor control
ct-guidance is a widely used imaging modality for placing
the needles, monitoring the treatment, and following up patients.
however, such unsupervised methods
fail at solving our problem due to lack of similar image features between the
contrasted (cct) and non-contrasted (ncct) image in the vascular tree region
(see sect. 3.3).
yet, seg-
menting vessels from non-contrasted images remains a challenge for the medical
imaging community
294
s. el hadramy et al.
2.1
vessel map extraction
we call vessel map (vm) the region of interest deﬁning the vascular tree in
the ncct.
mathematical morphology operators, in
particular a dilation operation [23], are performed on the segmented region of
interest to slightly increase its dimensions.
all the deformations are created using the same number of
control points and characteristics of the normal distributions.
2.3
neural network
predicting the vascular tree location in the deformed intraoperative ncct is
done using a u-net [5] architecture.
the output is the intraoperative vessel
map.
2.4
augmented ct
once the network has been trained on the patient-speciﬁc preoperative data, the
next step is to augment and visualize the intraoperative ncct.
aiming at a patient-
speciﬁc prediction, we only train on a “subject” at a time.
while the voxelmorph
network accurately registers the liver shape, the displacement ﬁeld is almost null
in the region of vessels inside the parenchyma.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf:
to develop the foundation model,
we construct a large-scale endoscopy video dataset by combining 9 pub-
licly available datasets and a privately collected dataset from baoshan
branch of renji hospital in shanghai, china.
combining 9 public and a new private collected dataset from baoshan branch of
renji hospital in shanghai, china, with over 33k video clips with up to 5 million
frames.
2.2
self-supervised pre-train via spatial-temporal matching
considering the diﬃculties of tackling the context information related with
lesions, tissues, and dynamic scenes in endoscopic data, we pre-train endo-fm
to be robust to such spatial-temporal characteristics.
the global
views {vi
g∈rt i
g×3×hg×wg}g
i=1 are generated by uniformly sampling x with dif-
ferent frame rates, and the local ones {vj
l ∈rt j
l ×3×hl×wl}l
j=1 are generated by
uniformly sampling video frames with diﬀerent frame rates from a randomly
cropped region of x (tl ≤ tg).
speciﬁcally, the context information presented
in diﬀerent frames of the same endoscope video can vary under two key factors:
1) the proportion of tissue and lesions within the frame, and 2) the presence
or absence of lesion areas.
[23]
bidmc
580
90444
laparoscope cholecystectomy
ours
baoshan branch
16494
2491952 colonoscope polyp, erosion, etc.
of renji hospital 7653
1170753 gastroscope
summary
6 providers
32896
5024101 3 protocols
10+ diseases
downstream polypdiag
the
pre-training is ﬁnished with 30 epochs with a cosine schedule [16].
3
experiment
3.1
datasets and downstream setup
we collect all possible public endoscope video datasets and a new one from
baoshan branch of renji hospital for pre-training.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_58.pdf:
furthermore, the prostate
is known to display large bulk displacement caused by patient movement and
needle insertions
we use a spectral domain oct
imaging system (telesto i, thorlabs gmbh, ger) with a center wavelength λ0
of 1325 nm to acquire axial scans (a-scans) at a sampling rate of 91.3 khz.
(color ﬁgure online)
e = σ
ϵ =
f/a
δl/l0
,
(1)
with the force f, the area a, initial sample length l0 and assuming incompress-
ibility, quasi-static loading and neglecting viscoelasticity.
we report the area under the receiver operating characteristic
(auroc) and area under the precision recall curve (auprc) for both external
and oce sensing.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf:
https://doi.org/10.1007/978-3-031-43996-4_64
towards multi-modal anatomical landmark detection
669
eloquent brain regions can improve the patient’s survival rate and functional
outcomes of the procedure [2].
the arrows point to the brain tumor region.
each cnn network contains six successive blocks, and each
block consists of one convolution layer and one group norm, with leaky relu
as the activation function.
with the assumption that
the brain shift moves the anatomy within a limited range, during the inference
time, we searched within a range of [-5,5] mm in each direction in the us around
the reference mri landmark location to ﬁnd the best match.
first, we calculated the sift features at the
reference landmark’s location in mri.
where xi and x′
i, and n are the ground truth landmark location, model predic-
tion, and the total number of landmarks per subject, respectively.
5
results
table 1 lists the mean and standard deviation of landmark identiﬁcation errors
(in mm) between the predicted position and the ground truth in intra-operative
us for each patient of the resect dataset.
in the table, we also provide the
severity of brain shift for each patient.
the level of brain shift is listed beside the patient id.
patient id
proposed cl sift algorithm patient id
proposed cl sift algorithm
1 (small)
1.80 ± 0.78
12.16 ± 4.75
15 (medium) 3.07 ± 1.39
19.33 ± 4.49
2 (medium)
6.16 ± 1.40
17.84 ± 8.50
16 (medium) 6.42 ± 0.92
12.48 ± 4.84
3 (large)
7.79 ± 0.55
13.37 ± 6.08
17 (large)
8.13 ± 0.72
18.57 ± 6.27
4 (small)
3.59 ± 0.73
13.97 ± 5.50
18 (medium) 4.19 ± 0.87
13.71 ± 5.815
5 (large)
10.65 ± 1.13
20.71 ± 6.12
19 (medium) 3.97 ± 0.93
27.70 ± 16.49
6 (medium)
2.20 ± 0.93
28.11 ± 11.90
21 (medium) 6.01 ± 0.77
24.40 ± 13.73
7 (small)
1.96 ± 0.96
24.07 ± 8.02
23 (large)
6.97 ± 1.03
22.50 ± 6.72
8 (small)
2.56 ± 0.79
19.30 ± 6.09
24 (small)
1.33 ± 0.49
14.91 ± 6.09
12 (large)
23.77 ± 0.96
22.01 ± 6.64
25 (large)
9.94 ± 2.43
15.37 ± 5.42
13 (medium) 6.18 ± 1.43
13.86 ± 6.99
26 (small)
2.95 ± 0.88
17.93 ± 10.15
14 (medium) 3.39 ± 0.69
21.67 ± 6.46
27 (medium) 6.42 ± 0.76
19.20 ± 8.65
towards more accurate inter-modal landmark localization, there are still aspects
to be improved.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf:
keywords: registration · inter-modal · error estimation · deep
learning
1
introduction
resection of early-stage brain tumors can greatly reduce the mortality rate of
patients.
an example of an mri-ius
pair from a patient is shown in fig.
2.3
uncertainty quantiﬁcation
for registration error regression in surgical applications, knowledge regarding
the reliability of the automated results is instrumental for the safety and well-
being of the patients.
to
prevent information leakage, we ensured that each patient was included in only
one of the split sets.
in this test, patches can contain large areas of zeros (image con-
tent out of the scanning fov of the ius).
one limitation of our work lies in the limited patient data, as public ius datasets
are scarce, while the settings and properties of us scanners can vary, potentially
aﬀecting the dl model designs.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf:
mesoscopic ﬂuorescence lifetime imaging (flim) of tissue
ﬂuorophores (i.e., collagen and metabolic co-factors nadh and fad) emission
has demonstrated the potential to demarcate the extent of head and neck cancer
in patients undergoing surgical procedures of the oral cavity and the orophar-
ynx.
flim data from n = 22
patients undergoing upper aerodigestive oncologic surgery were used to train and
validate the classiﬁcation model using leave-one-patient-out cross-validation.
our
approach identiﬁed all patients with positive surgical margins (n = 3) conﬁrmed
by pathology.
furthermore, the proposed method reported a point-level sensitivity
of 0.75 and a speciﬁcity of 0.78 across optically interrogated tissue surface for
all n = 22 patients.
https://doi.org/10.1007/978-3-031-43996-4_56
588
m. a. hassan et al.
1
introduction
residual tumor in the cavity after head and neck cancer (hnc) surgery is a signiﬁcant
concern as it increases the risk of cancer recurrence and can negatively impact the
patient’s prognosis
due to the sensitivity of these
ﬂuorophores to their microenvironment, the presence of tumor changes their emission
properties (i.e., intensity and lifetime characteristics) relative to healthy tissue, thereby
enabling the optical detection of cancer
our proposed approach identiﬁed all patients
with psm.
the flim device includes a 440 nm continuous wave laser that serves as an aiming
beam; this aiming beam enables real-time visualization of the locations where ﬂuores-
cence (point measurements) is collected by generating visible blue illumination at the
location where data is acquired.
anatomy, surgical outcome, and tissue label breakdown for the 22 patients
surgical outcome
no.
patients
tissue label
no. flim points
clear margin
19
healthy
170,535
psm
3
cancer
2,451
flim data was acquired using the da vinci sp robotic surgical platform.
finally,
the patient’s surgical cavity was scanned to check for residual tumor.
2.2
patient cohort and flim data labeling
the research was performed under the approval of the uc davis institutional review
board (irb) and with the patient’s informed consent.
all patients were anesthetized,
intubated, and prepared for surgery as part of the standard of care.
n = 22 patients are
represented in this study, comprising hnc in the palatine tonsil (n = 15) and the base of
the tongue (n = 7).
for each patient, the operating surgeon conducted an en bloc surgi-
cal tumor resection procedure (achieved by tors-electrocautery instruments), and the
resulting excised specimen was sent to a surgical pathology room for grossing.
after the surgical excision of the tumor, an in vivo flim scan of approximately 90
s was conducted within the patient’s surgical cavity, where the tumor was excised.
2.3
flim preprocessing
the raw flim waveform contains background noise, instrument artifacts, and other
types of interference, which need to be carefully processed and analyzed to extract
meaningful information (i.e., the ﬂuorescence signal decay characteristics).
the evaluation followed a leave-one-patient-out cross-validation approach.
the inter-
polation consists of ﬁtting a disk to the segmented aiming beam pixel location for each
point measurement and applying a color map (e.g., green: healthy and red: cancer) for
each point prediction.
three
novelty detection models were evaluated, and all three models could identify the presence
of residual tumors in the cavity for the three patients.
the oc-svm
and robust covariance reported a high standard deviation, indicating that the performance
of the classiﬁcation model is inconsistent across different patients.
moreover, the current approach correctly identiﬁed all patients with psms (see
fig.
the flim-
based classiﬁcation model could help guide the surgical team in real-time, providing
information on the location and extent of cancerous tissue.
in context to the standard of care, the proposed residual tumor detection model
exhibits high patient-level sensitivity (sensitivity = 1) in detecting patients with psms.
in
contrast, defect-driven ifsa reports a patient-level sensitivity of 0.5 [6, 7].
our approach
exhibits a low patient-level speciﬁcity compared to ifsa.
therefore, completely
resecting cancerous tissue and improving patient outcomes.
the false positive predictions from the classiﬁcation model presented two trends:
false positives in an isolated region and false positives spreading across a larger region.
2. gods classiﬁcation overlay of in vivo cavity scans of three patients presenting with
residual tumor.
the columns represent each patient, and the rows depict the ground truth labels,
thepoint-predictionoverlay,andtheaugmentedsurgicalview.fpr-falsepositiverate,fnr-false
negative rate.
accounted for by the interpolation approach used for the classiﬁer augmentation (refer
to supplementary section fig.
on the other hand, false positives spreading across
a larger region are much more complex to interpret.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf:
cur-
rent deep learning-driven methods still get trapped in inaccurate colono-
scopic lesion segmentation due to diverse sizes and irregular shapes of
diﬀerent types of polyps and adenomas, noise and artifacts, and illumi-
nation variations in colonoscopic video images.
[10] introduced task-relevant feature replenishment networks for cross-
center polyp segmentation, while tian et al.
on the other hand, various polyps and adenomas with diﬀerent
pathological features have similar visual characteristics to intestinal folds.
first, the cascade-transformer encoder can extract local
and global semantic features of colorectal lesions with diﬀerent pathological
characteristics due to its pyramid representation and linear spatial reduction
attention.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_50.pdf:
keywords: segmentation · radiotherapy · dose guidance · deep
learning
1
introduction
radiotherapy (rt) has proven eﬀective and eﬃcient in treating cancer patients.
et al.
be performed accurately and, more importantly, must be patient-safe.
in the ﬁeld of rt planning for brain tumor patients, the recent study
of [17] shows that current dl-based segmentation algorithms for target struc-
tures carry a signiﬁcant chance of producing false positive outliers, which can
have a considerable negative eﬀect on applied radiation dose, and ultimately,
they may impact treatment eﬀectiveness.
we present results on a clin-
ical dataset comprising ﬁfty post-operative glioblastoma (gbm) patients.
[20]) is trained to output target segmentation predictions for the
gross tumor volume (gtv) based on patient mri sequences.
[20]) is trained to output target segmentation predic-
tions (st ) for the gross tumor volume (gtv) based on patient mri sequences imr.
[12]. originally proposed for head and neck cancer [12], this approach has
been recently extended for brain tumor patients [9] with levels of prediction error
below 2.5 gy, which is less than 5% of the prescribed dose.
we refer the reader to [9] for further
details.
segmentation models:
to develop and test the proposed approach, we
employed a separate in-house dataset (i.e., diﬀerent cases than those used to
train the dose predictor model) of 50 cases from post-operative gmb patients
receiving standard rt treatment.
indeed, we remark that both models failed to yield acceptable
segmentation quality in this area.
in case no. 4, both models failed to segment the
diﬀuse tumor area alongside the skull; however, as shown in fig.
2-case no. 4, the
standard bce+softdice model would yield a centrally located radiation dose,
with strong negative clinical impact to the patient.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf:
we evaluated the proposed approach on a dataset assembled from 22
videos of esd surgery cases, which are collected from the endoscopy centre of
the prince of wales hospital in hong kong.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf:
to demonstrate application, we perform
registration on a breast cancer patient case with a segmented tumor and compare
performance to other image-to-physical and image-to-image registration methods.
https://doi.org/10.1007/978-3-031-43996-4_33
regularized kelvinlet functions to model linear elasticity
345
1
introduction
image-to-physical registration is a necessary process for computer-assisted surgery to
align preoperative imaging to the intraoperative physical space of the patient to in-form
surgical decision making.
(+) denotes nipple location, (·) denotes x0 location.
equation (2) represents the forcing function for a point
source fδ(x), where f is the point source forcing vector and x0 is the load location.
the second
validates the registration method in a breast cancer patient and compares registration
accuracy and computation time to previously proposed methods.
3.1
hyperparameters sensitivity analysis
this dataset consists of supine breast mr images simulating surgical deformations of
11 breasts from 7 healthy volunteers.
outliers
are noted as (x) and are 1.5•iqr.
3.2
registration methods comparison
this dataset consists of supine breast mr images simulating surgical deformations from
one breast cancer patient.
a 71-year-old patient with invasive mammary carcinoma in
the left breast was enrolled in a study approved by the institutional review board at
vanderbilt university.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf:
through intensive experimentation, we demonstrated that
our method can successfully and eﬀectively detect the sensing area, estab-
lishing a new performance benchmark.
code and data are available at
https://github.com/br0202/sensing area detection.git.
keywords: laparoscopic image-guided intervention · minimally
invasive surgery · detection of sensing area
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43996-4 25.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43996-4_25
detecting the sensing area of a laparoscopic probe in cancer surgery
261
1
introduction
cancer remains a signiﬁcant public health challenge worldwide, with a new diag-
nosis occurring every two minutes in the uk (cancer research uk1).
in practice, imprecise intraoperative cancer
tissue detection and visualization results in missed cancer or the unnecessary
removal of healthy tissues, which leads to increased costs and potential harm to
the patient.
there is a pressing need for more reliable and accurate intraopera-
tive visualization tools for minimally invasive surgery (mis) to improve surgical
outcomes and enhance patient care.
fig.
however, the use of
this probe presents a visualization challenge as the probe is non-imaging and is
air-gapped from the tissue, making it challenging for the surgeon to locate the
probe-sensing area on the tissue surface.
it is crucial to accurately determine the sensing area, with positive signal
potentially indicating cancer or aﬀected lymph nodes.
geometrically, the sensing
area is deﬁned as the intersection point between the gamma probe axis and
the tissue surface in 3d space, but projected onto the 2d laparoscopic image.
in this study, in order to provide sensing area visual-
ization ground truth, we modiﬁed a non-functional ‘sensei’ probe by adding
a miniaturized laser module to clearly optically indicate the sensing area on
the laparoscopic images - i.e. the ‘probe axis-surface intersection’.
with this setup, we aim to transform the sensing area localization
problem from a geometrical issue to a high-level content inference problem in
2d.
detecting the sensing area of a laparoscopic probe in cancer surgery
263
although the intermediate depth information was not our ﬁnal aim and can
be bypassed, the 3d surface information was necessary in the intersection point
inference.
at each position, stereo rgb
images were captured i) under normal laparoscopic illumination with the laser
oﬀ; ii) with the laparoscopic light blocked and the laser on; and iii) with the
laparoscopic light blocked and the laser oﬀ. subtraction of the images with laser
on and oﬀ readily allowed segmentation of the laser area and calculation of its
central point, i.e. the ground truth probe axis-surface intersection.
to obtain the intersection point,
the authors used the structure from motion (sfm) method to compute the
detecting the sensing area of a laparoscopic probe in cancer surgery
265
3d tissue surface, combining it with the estimated pose of the probe, all within
the laparoscope coordinate system.
furthermore, this sim-
ple methodology facilitated an average inference time of 50 frames per second,
enabling real-time sensing area map generation for intraoperative surgery.
3. sensing area detection.
detecting the sensing area of a laparoscopic probe in cancer surgery
267
4.4
implementation
evaluation metrics.
to evaluate sensing area location errors, euclidean dis-
tance was adopted to measure the error between the predicted intersection points
and the ground truth laser points.
hence, we used the median depth value of a square area of 5 pixels
around the points where depth value was not available.
figure 5 shows visualization results of our method using resnet and mlp.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_29.pdf:
this information is then used to register the 3d ultrasound image with
the patient’s anatomy.
frames were cropped to
remove the patient and probe characteristics, then down-sampled to a size of
128 × 128 with an image spacing of 0.22 mm per pixel.
we have evaluated our
reconstruction with a commonly used in state-of-the-art metric called ﬁnal drift
error, which measures the distance between the center point of the ﬁnal frame
1 https://pytorch.org/docs/stable/index.html.
310
s. el hadramy et al.
according to the real relative position and the estimated one in the sequence.
this remains a challenge for the community even in the case of
linear probe motions.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf:
imaging patients with inﬂammatory bowel disease (ibd) can
be especially problematic, owing to involuntary bowel movements and
diﬃculties with long breath-holds during acquisition.
we investigate this hypothesis by evalu-
ating a downstream task, automatically scoring ibd in the area of the
terminal ileum on the reconstructed images and show evidence that our
method does not suﬀer a synthetic domain bias.
mri is preferred
over computed tomography (ct) imaging as it does not use radiation, which is
an important consideration in younger patients.
unfortunately, mr acquisition
for patients with crohn’s disease can easily become compromised by respiratory
motion.
as a result, many patients’ images are degraded by respiration, involuntary
movements and peristalsis.
(4) experiments with existing models
for predicting the degree of small bowel inﬂammation in crohn’s disease patients
show that mocosr can retain diagnostically relevant features and maintain the
original hr feature distribution for downstream image analysis tasks.
how-
ever, this technique requires radiologists to label and evaluate diseased bowel
segments, and patients’ scan times are long.
(2) truncation at the k-space, retain-
ing only the central region of the data.
the data contains 908 mri series from 97 patients.
the lower
shows the number of patients in each severity level of inﬂammation.
[ms]/te [ms] matrix
slice [mm] fov
time [s]
axial
t1 ffe (e-thrive) 5.9/3.4
512 × 512 × 96 3.00
375
20.7 × 2
axial postcon single-shot t2 tse
587/120
528 × 528 × 72 3.50
375
22.3 × 2
coronal
single-shot t2 tse
554/120
512 × 512 × 34 3.00
375
21.1
inﬂammation class
healthy
mild
moderate
severe
number of patients
100
49
42
9
table 2.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf:
the dgfm guides the network to concentrate the feature repre-
sentation of the 3d inter-slice information in the region of interest (roi)
by introducing the average ct image and segmentation mask as comple-
ments of the original ldct input.
(2) most of
them extracted the features with a ﬁxed resolution, failing to eﬀectively lever-
age multi-scale features which are essential to image restoration task [27,32].
avg ct is the average image among adjacent ct slices
of each patient.
munication.
we ﬁrst calculate the
average ct image of adjacent ct slices of each patient to provide the 3d spatial
structure information of ct volume.
meanwhile, the roi mask is obtained by
a pre-trained segmentation network to guide the network to concentrate on the
focus area or tissue area.
the 3d-ircadb dataset
is used for liver and its lesion detection which consists of 2823 512×512 ct ﬁles
from 20 patients.
we choose 1663 ct images from 16 patients for training, 226
ct images from 2 patients for validation and 185 ct images from 2 patients for
testing.
similarly, the pancreas dataset is used for pancreas segmentation
which consists of 19328 512 × 512 ct ﬁles from 82 patients.
we choose 5638
ct images from 65 patients for training, 668 ct images from 8 patients for
validation and 753 ct images from 9 patients for testing.
(a)
is the hr image and its red rectangle region displays the liver and its lateral issues.
(a) is the
hr image and its red rectangle region shows the pancreas and kidney.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_48.pdf:
however, holes in
the reconstruction appear due to failures in triangulation and inaccurate depth
estimation or in areas not observed in any image.
the volume density σ(x) can be
interpreted as the diﬀerential probability of a ray terminating at an inﬁnitesimal
particle at location x.
an
adaptive gain factor g is applied by the endoscope’s internal logic and gamma
correction is also used to adapt to non-linear human vision, achieving better
contrast perception in mid tones and dark areas.
(d) number of frames seeing each surface
point, with gt unobserved areas shown in gray.
note that the watertight prior inherent to an sdf allows the
network to hallucinate unseen areas.
remarkably, these unsurveyed areas con-
tinue the tubular shape of the colon and we found them to be mostly accurate
510
v. m. batlle et al.
when compared to the ground truth.
for example, the curved areas of the colon
where a wall is occluded behind the corner of the curve is reconstructed, as shown
in fig.
3. this ability to “ﬁll in” observation gaps may be useful in providing
the endoscopist with an estimate of the percentage of unsurveyed area during a
procedure.
in the last rows of table 1, we
compute accuracy metrics for this extended region.
it includes not only surveyed
areas, but also neighboring areas that were not observed.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_73.pdf:
second, we solve the
initialization challenge for 2d-3d registration by leveraging tissue struc-
ture through cascaded rigid areas guidance and distance ﬁeld regulariza-
tion.
https://doi.org/10.1007/978-3-031-43999-5_73
772
a. leroy et al.
attention in medical imaging due to the various contexts where it applies, like image
fusion between 2d real-time acquisitions and either pre-operative 3d images for
guided interventions or reference planning volumes for patient positioning in radi-
ation therapy (rt).
however, the registration process is substantially diﬃcult due
to the visual characteristics, resolution scale, and dimensional diﬀerences between
thetwomodalities.inaddition,histologicalpreparationinvolvestissueﬁxationand
slicing,leadingtoseverecollapseandout-of-planedeformations.(semi-)automated
methodshavebeendevelopedtoavoidtime-consumingandbiasedmanualmapping,
including protocols with 3d mold or landmarks [10,22], volume reconstruction to
perform 3d registration [2,18,19,23], or optimization algorithms for direct multi-
modalcomparison[3,15].morerecently,deeplearning(dl)hasbeenintroducedbut
is limited to 2d/2d and requires prior plane selection
onepromisingsolutionistorelyonrigidstructuresthataresupposedlymorerobust
duringthepreparation.structuralinformationtoguideimageregistrationhasbeen
studiedwiththehelpofsegmentationsintothetrainingloop[11],orbylearningnew
image representations for reﬁned mapping [12].
in this paper, we propose to leverage the structural features of tissue and
more particularly the rigid areas to guide the registration process with two dis-
tinct contributions: (1) a cascaded rigid alignment driven by stiﬀ regions and
coupled with recursive plane selection, and (2) an improved 2d/3d deformable
motion model with distance ﬁeld regularization to handle out-of-plane deforma-
tion.
a diﬀerentiable spatial transform r
ﬁnally warps mct for similarity optimization with mh. similarly to [14], we
adopt a loss lrigid masked on empty slices to avoid the introduction of noise at
slices within the gradient where no data is provided, and directly train on the
dice similarity coeﬃcient (dsc) between rigid areas:
lrigid(mh, r(mh, mct))
it maps each voxel v
of ct to its distance with the closest point m to the rigid area mct .
we can then
control the displacement ﬁeld, with close tissue being more highly constrained
than isolated areas: φ′ = φ⊙(δ+ϵ), where ⊙ is the hadamard product and ϵ is
a hyperparameter matrix allowing small displacement even for cartilage areas for
which distance transform is null.
our clinical dataset consists of 108 patients for
whom were acquired both a pre-operative h&n ct scan and 4 to 11 wsis
after laryngectomy (with a total amount of 849 wsis).
we split the dataset patient-wise
into three groups for training (64), validation (20), and testing (24).
we implemented our model with pytorch1.13 frame-
work and trained for 600 (800 for mr/ct) epochs with a batch size of 8 (4 for
mr/ct) patients parallelized over 4 nvidia gtx 1080 tis.
evaluation.
for quantitative assessment, we computed
the dsc as well as the hausdorﬀ distance between cartilages, and the average
distance between characteristic landmarks disposed before registration(table 1).
we
also compared against msv-regsynnet on its own validation dataset for gener-
alization assessment: we yielded comparable results for the ﬁrst cohort and sig-
niﬁcantly better ones for the second, which proves that structuregnet behaves
well on other modalities and that the structure awareness is an essential asset for
better registration, as pelvis is a location where organs are moving.
the
typical error cases are the inclusion of cartilage or edema, which highlights the
structuregnet: 2d-3d multimodal registration
779
limitations and variability of radiology-based examinations, leading to increased
toxicity or untreated areas in rt.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_66.pdf:
we propose an unsupervised deep learning method to recon-
struct a 3d tomographic image from biplanar x-rays, to reduce the num-
ber of required projections, the patient dose, and the acquisition time.
keywords: image reconstruction · inverse problem · sparse
sampling · deep generative model · ct
1
introduction
tomographic imaging estimates body density using hundreds of x-ray projec-
tions, but it’s slow and harmful to patients.
acquisition time may be too high
for certain applications, and each projection adds dose to the patient.
this can improve image-guided therapies and preoperative planning, espe-
cially for radiotherapy, which requires precise patient positioning with minimal
radiation exposure.
compared to nerf-based methods, our method exploits prior
knowledge from many patients to require only two projections.
we evaluate our
method on reconstructing cancer patients’ head-and-neck cts, which involves
intricate and complicated structures.
we ﬁrst learn the low-
dimensional manifold of ct volumes of a target body region.
in practice, we take operator a as a 3d cone beam
projection that simulates x-ray attenuation across the patient, adapted from
[21,27].
we trained our model with a large dataset of 3500 cts of
patients with head-and-neck cancer, more exactly 2297 patients from the publicly
available the cancer imaging archive (tcia)
we focused
ct scans on the head and neck region above shoulders, with a resolution of
80 × 96 × 112, and centered on the mouth after automatic segmentation using a
pre-trained u-net [22].
to evaluate our approach, we used an external private cohort
of 80 patients who had undergone radiotherapy for head-and-neck cancer, with
their consent.
our
method achieves better ﬁtting of the patient structure, including bones, tissues,
and air separations, almost matching the real ct volume.
in some clinical procedures, an earlier
ct volume of the patient may be available and can be used as an additional input
for nerp

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf:
tα the deformed image, the optimization target can be
expressed in the following way:
f(α) =

p∈ω
w(p) s(f[p], m ◦ tα[p]),
(1)
where w(p) is the weight assigned to the point p, s(·, ·) deﬁnes a local similarity
and the [·] operator extracts a patch (or a pixel) at a given spatial location.
our neural network is trained using patches from the “gold atlas
- male pelvis - gentle radiotherapy” [14] dataset, which is comprised of 18
patients each with a ct, mr t1, and mr t2 volumes.
the dataset comprises 8 sets of mr and ct volumes, both depicting
the abdominal region of a single patient and exhibiting notable deformations.
we are using a heterogeneous dataset of 27 cases, comprising liver cancer
patients and healthy volunteers, diﬀerent ultrasound machines, as well as

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_6.pdf:
several methods have applied cyclegan to lever-
age unpaired data, but they often generate inaccurate mappings that shift
theanatomy.thisproblemisfurtherexacerbatedwhentheimagesfromthe
sourceandtargetmodalitiesareheavilymisaligned.recently,currentmeth-
ods have aimed to address this issue by incorporating a supplementary seg-
mentation network.
(c) attentiongan [12]
inﬂates the head area in the synthetic ct, which is inconsistent with the original mri.
unfortunately, ct imaging
exposes patients to ionizing radiation, which can damage dna and increase
cancer risk [9], especially in children and adolescents.
the
content branch synthesizes n −1 outputs for the foreground structures, denoted
as c. each output, ci, represents the synthetic content for the corresponding
foreground region that is masked by the attention mask ai.
we collected 270 volumetric t1-weighted mri and 267 thin-
slice ct head scans with bony reconstruction performed in pediatric patients
under routine scanning protocols1.
we targeted the age group from 6–24 months
since pediatric patients are more susceptible to ionizing radiation and experience
a greater cancer risk (up to 24% increase) from radiation exposure [7]. further-
more, surgery for craniosynostosis, a birth defect in which the skull bones fuse
too early, typically occurs during this age [5,16].
13 mri-
ct volumes from the same patients that were captured less than three months
apart are registered using rigid registration algorithms.
the scope of the
paper centers on theoretical development; clinical evaluations such as dose cal-
culation and treatment planning will be conducted in future work.
3.2
results and discussions
comparisons with state-of-the-art.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_5.pdf:
despite their success, these methods lack the ability
to quantify the contributions of diﬀerent input sequences and estimate
region-speciﬁc quality in generated images, making it hard to be practical.
hence, we propose an explainable task-speciﬁc synthesis network, which
adapts weights automatically for speciﬁc sequence generation tasks and
provides interpretability and reliability from two sides: (1) visualize and
quantify the contribution of each input sequence in the fusion stage by
a trainable task-speciﬁc weighted average module; (2) highlight the area
the network tried to reﬁne during synthesizing by a task-speciﬁc atten-
tion module.
the intensity contrast combination of multi-sequence mri provides
clinicians with diﬀerent characteristics of tissues, extensively used in disease diag-
nosis
however, some
acquired sequences are unusable or missing in clinical settings due to incorrect
machine settings, imaging artifacts, high scanning costs, time constraints, con-
trast agents allergies, and diﬀerent acquisition protocols between hospitals [5].
without rescanning or aﬀecting the downstream pipelines, the mri synthesis
technique can generate missing sequences by leveraging redundant shared infor-
mation between multiple sequences
by giving the task-speciﬁc code, tsf-
seq2seq can synthesize a target sequence from existing sequences, and meanwhile,
output the weight of input sequences ω and the task-speciﬁc enhanced map (tsem).
shows interpretability for fusion by quantifying the contribution of each input
sequence; (3) the network provides reliability for synthesis by highlighting the
area the network tried to reﬁne.
as shown in fig. 4, tsem has a higher
resolution than the attention maps and can highlight the tumor area which is
hard to be synthesized by the networks.
to assist the synthesis models deploying in clinical settings, tsem
can be used as an attention and uncertainty map to remind clinicians of the
possible unreliable synthesized area.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf:
the high-quality ct
images are important to improve the performance of diagnosis in clinic [27].
when a patient is
located in a ct equipment, a set of consecutive cross-sectional images are gen-
erated.
the main idea relies on the practical assumptions that the bright-
ness of the object more likely remains stable across consecutive frames, and the
brightness of the pixels in a local region are usually changed consistently
∇i = (∇iw, ∇ih) denotes spatial gradients of image brightness, and ∇it denotes
the temporal partial derivative of the corresponding region.
first, our proposed approaches are evaluated on the “mayo-clinic
low-dose ct grand challenge” (mayo-clinic) dataset of lung ct images [19].
the dataset contains 2250 two dimensional slices from 9 patients for training,
and the remaining 128 slices from 1 patient are reserved for testing.
we randomly select 4
patients with 1827 slices from the dataset.
the simulation process is identical to
that of mayo-clinic.
table 1 presents the results on the mayo-clinic dataset, where the ﬁrst
row represents diﬀerent parameter settings (i.e., the number of uniform views nv,
the number of detectors nd and the standard deviation of gaussian noise σ) for
simulating low-dose sinograms.
experimental results for mayo-clinic dataset.
to evaluate the stability and generalization of our
model and the baselines trained on mayo-clinic dataset, we also test them on
the rider dataset.
due to the bias in the
datasets collected from diﬀerent facilities, the performances of all the models are
declined to some extents.
3. reconstruction results on mayo-clinic dataset.
in future, we will
evaluate our network on real-world ct images from local hospital and use the
reconstructed images to support doctors for the diagnosis and recognition of lung
nodules.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf:
4dcbct ·
deep learning · unsupervised learning
1
introduction
radiotherapy (rt) is one of the cornerstones of cancer patients.
image
guided rt (igrt) is a technique to capture the anatomy of the day using in
room imaging in order to align the treatment beam with the tumor location [1].
from the projections, it is possible to extract a respi-
ratory signal [12], which indicates the position of the organs within the patient
during breathing.
we validated our method on publicly available data [15] against a supervised
approach [6] and applied it to an internal clinical dataset of 30 lung cancer
patients.
the spare varian dataset was used to provide performance results on pub-
licly available patient data.
to more closely resemble normal respiratory
motion per projection image, the 8 min scan has been used from each patient
(ﬁve such scans are available in the dataset).
training is performed over 4
patients while 1 patient is used as a test set.
486
s. papa et al.
2. an internal dataset (irb approved) of 30 lung cancer patients’ 4dcbcts
from 2020 to 2022, originally used for igrt, with 25 patients for training and
5 patients for testing.
the metrics in table 1 show
mean and standard deviation across all phases for a single patient.
1. qualitative comparison between methods using coronal view of the patient in
the test set.
values are mean and std computed across all phases of patient 1
of the spare varian dataset.
from fig. 1 and table 1, the supervised approach repro-
duces the noise that was seen during training, while noise2aliasing manages to
remove it consistently, outperforming the supervised approach, especially in the
soft tissue area around the lungs, where the noise aﬀects attenuation coeﬃcients
the most.
noise2alisting trained on 25 patients and tested on 5 achieved
mean psnr of 35.24 and ssim of 0.91, while the clinical method achieved
mean psnr of 29.97 and 0.74 ssim with p-value of 0.048 for the psnr and
0.0015 for the ssim, so noise2aliasing was signiﬁcantly better according to
both metrics.
overall, using more
patients results in better noise reduction and sharper reconstructions (see fig. 2),
488
s. papa et al.
fig.
with fewer
patients, the model is more conservative and tends to keep more noise, but also smudges
the interface between tissues and bones.
with more patients, more of the view-aliasing
is addressed, and the reconstruction is sharper, however, a few small anatomical struc-
tures tend to be suppressed by the model.
when applied to a clinical dataset, noise2aliasing beneﬁts from more
patients being included in the dataset, however, qualitatively good performance
is already achieved with 5 patients.
3. motion extent is accurately resolved by noise2aliasing when using 25 patients.
as future work, we plan to study
noise2aliasing in the presence of changes in the breathing frequency and ampli-
tude between patients and during a scan.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_3.pdf:
conventional ac techniques require additionally-acquired com-
puted tomography (ct) or magnetic resonance (mr) images to calculate
attenuation coeﬃcients, which increases imaging expenses, time costs, or
radiation hazards to patients, especially for whole-body scanners.
since pseudo ct is
convenient to be integrated into conventional ac processes, generating pseudo
ct images is feasible in clinics for ac.
[4] (https://www.cancerimagingarchive.net/collections/), where a series
of public datasets with diﬀerent types of lesions, patients, and scanners are
open-access.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_21.pdf:

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_24.pdf:
sparse-view computed tomography (ct) is a promising solu-
tion for expediting the scanning process and mitigating radiation expo-
sure to patients, the reconstructed images, however, contain severe streak
artifacts, compromising subsequent screening and diagnosis.
(4)
2.3
self-guided artifact reﬁnement network
areas heavily obscured by the artifact should be given more attention, which
is hard to achieve using only freenet.
3. overview of dual-domain counterpart of freeseed.
3
experiments
3.1
experimental settings
we conduct experiments on the dataset of “the 2016 nih-aapm mayo clinic
low dose ct grand challenge”
[8], which contains 5,936 ct slices in 1 mm
image thickness from 10 anonymous patients, where a total of 5,410 slices from
9 patients, resized to 256 × 256 resolution, are randomly selected for training
and the 526 slices from the remaining one patient for testing without patient
overlap.
specifying the distance from the x-ray source to the
rotation center as 59.5 cm and the number of detectors as 672, we generate
sinograms from full-dose images with multiple sparse views nv ∈ {18, 36, 72, 144}
uniformly sampled from full 720 views covering [0, 2π].

--------------------------------------------------------------------------------

