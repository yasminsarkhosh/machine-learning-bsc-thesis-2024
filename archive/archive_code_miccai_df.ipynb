{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive: code samples\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back matter\n",
    "#a = [x for x in page_numbers_str if any(bm in x for bm in back_matter_numbers_str)]\n",
    "#filtered_page_numbers_str = [x for x in page_numbers_str if x not in a]\n",
    "\n",
    "# front matter\n",
    "#a = [x for x in page_numbers_str if any(fm in x for fm in front_matter_numbers_str)]\n",
    "#filtered_page_numbers_str = [x for x in page_numbers_str if x not in a]\n",
    "\n",
    "# find the back matter in the document - used in the mining function \n",
    "text = 'Back Matter'\n",
    "back_matter = soup.find_all(lambda tag: tag.name == \"div\" and text in tag.text)\n",
    "\n",
    "#get the page numbers for back matter pages the in the document\n",
    "back_matter__str = []\n",
    "\n",
    "for element in back_matter:\n",
    "    line = str(element)\n",
    "    first_subline = 'Pages'\n",
    "    second_subline ='</span>'\n",
    "    line = line[(line.find(first_subline)):line.find(second_subline)]\n",
    "    if len(line) > 0: #ensures I don't add an empty string to the list\n",
    "        back_matter__str.append(line)\n",
    "\n",
    "back_matter__str = np.unique(back_matter__str)  #removes duplicates\n",
    "print(back_matter__str)\n",
    "\n",
    "# find front matter in the document\n",
    "text = 'Front Matter'\n",
    "front_matter = soup.find_all(lambda tag: tag.name == \"div\" and text in tag.text)\n",
    "\n",
    "#get the page numbers for front matter pages the in the document\n",
    "front_matter__str = []\n",
    "\n",
    "for element in front_matter:\n",
    "    line = str(element)\n",
    "    first_subline = 'Pages'\n",
    "    second_subline ='</span>'\n",
    "    line = line[(line.find(first_subline)):line.find(second_subline)]\n",
    "    if len(line) > 0: #ensures I don't add an empty string to the list\n",
    "        front_matter__str.append(line)\n",
    "\n",
    "front_matter__str = np.unique(front_matter__str)\n",
    "print(front_matter__str)\n",
    "\n",
    "'''\n",
    "# find the back matter in the document\n",
    "back_matter_numbers = soup.find_all('li', class_=\"c-card c-list-group__item c-card--flush u-pa-16 u-pl-0\")   \n",
    "\n",
    "#strips it back to simply the pages\n",
    "back_matter_numbers_str = []\n",
    "\n",
    "for element in back_matter_numbers:\n",
    "    line = str(element)\n",
    "    first_subline = 'Pages'\n",
    "    second_subline ='</span>'\n",
    "    line = line[(line.find(first_subline)):line.find(second_subline)]\n",
    "    if len(line) > 0: #ensures I don't add an empty string to the list\n",
    "        back_matter_numbers_str.append(line)\n",
    "\n",
    "back_matter_numbers_str\n",
    "'''\n",
    "\n",
    "# function to find back matters\n",
    "''' \n",
    "ll = []\n",
    "for element in back_matter_numbers_str:\n",
    "    string = element[6:] #removes the white spaces and \"Pages \"\n",
    "    print(string)\n",
    "    for x in ll:\n",
    "        if x in string:\n",
    "            ll.remove(x)\n",
    "\n",
    "print(ll)\n",
    "'''\n",
    "'''\n",
    "    for element in page_numbers_str:\n",
    "        for element in front_matter__str:\n",
    "            if element in front_matter__str:\n",
    "                print('fm', element)\n",
    "                string = element[6:] #removes the white spaces and \"Pages \"\n",
    "                page_numbers_str = page_numbers_str.remove(string)\n",
    "        for element in back_matter__str:\n",
    "            if len(back_matter) > 0 and element in back_matter__str:\n",
    "                print('bm', element)\n",
    "                string = element[6:]\n",
    "                page_numbers_str = page_numbers_str.remove(string)\n",
    "        \n",
    "    #updating the page numbers to remove the front and back matter\n",
    "    for front_matter_pages in front_matter__str:\n",
    "        if front_matter__str.size == 0:\n",
    "            print('no front matter')\n",
    "            print('front matter pages', front_matter_pages)\n",
    "    if front_matter__str.size > 0:\n",
    "        string = front_matter_pages[6:] #removes the white spaces and \"Pages \"\n",
    "        for element in page_numbers_str:\n",
    "            if element in string:\n",
    "                page_numbers_str.remove(element)\n",
    "  \n",
    "    \n",
    "    #updating the page numbers to remove the front and back matter\n",
    "    for back_matter_pages in back_matter__str:\n",
    "        if back_matter__str.size == 0:\n",
    "            print('no back matter')\n",
    "            print('back matter pages', back_matter_pages)\n",
    "    if back_matter__str.size > 0:    \n",
    "        string = back_matter_pages[6:] #removes the white spaces and \"Pages \"\n",
    "        for element in page_numbers_str:\n",
    "            if element in string:\n",
    "                page_numbers_str.remove(element)\n",
    "    '''\n",
    "\n",
    "# find the front matter in the document\n",
    "    text = 'Front Matter'\n",
    "    front_matter = soup.find_all(lambda tag: tag.name == \"div\" and text in tag.text)\n",
    "    front_matter__str = []\n",
    "\n",
    "    for element in front_matter:\n",
    "        line = str(element)\n",
    "        first_subline = 'Pages'\n",
    "        second_subline ='</span>'\n",
    "        line = line[(line.find(first_subline)):line.find(second_subline)]\n",
    "        if len(line) > 0: #ensures I don't add an empty string to the list\n",
    "            front_matter__str.append(line)\n",
    "\n",
    "    front_matter__str = np.unique(front_matter__str)\n",
    "    front_matter__str = front_matter__str.tolist()\n",
    "    #print('front matter', front_matter__str, len(front_matter__str))\n",
    "\n",
    "    text = 'Back Matter'\n",
    "    back_matter = soup.find_all(lambda tag: tag.name == \"div\" and text in tag.text)\n",
    "    back_matter__str = []\n",
    "\n",
    "    for element in back_matter:\n",
    "        line = str(element)\n",
    "        first_subline = 'Pages'\n",
    "        second_subline ='</span>'\n",
    "        line = line[(line.find(first_subline)):line.find(second_subline)]\n",
    "        if len(line) > 0: #ensures I don't add an empty string to the list\n",
    "            back_matter__str.append(line)\n",
    "\n",
    "    back_matter__str = np.unique(back_matter__str)  #removes duplicates\n",
    "    back_matter__str = back_matter__str.tolist()\n",
    "    #print('back matter', back_matter__str, len(back_matter__str))\n",
    "\n",
    "    #updating the page numbers to remove the front and back matter\n",
    "    #page_numbers_str = [x for x in page_numbers_str + front_matter__str if x not in page_numbers_str or x not in front_matter__str]\n",
    "    #print('res', len(res))\n",
    "\n",
    "    #page_numbers_str = [x for x in page_numbers_str + back_matter__str if x not in page_numbers_str or x not in back_matter__str]\n",
    "    #print('res2', len(res2))\n",
    "\n",
    "      #stores a list of pages that are in the back matter\n",
    "    ''' \n",
    "    # find the back matter in the document\n",
    "    back_matter_numbers = soup.find_all('li', class_=\"c-card c-list-group__item c-card--flush u-pa-16 u-pl-0\")   \n",
    "\n",
    "    back_matter_numbers_str = []\n",
    "    for element in back_matter_numbers:\n",
    "        line = str(element)\n",
    "        first_subline = 'Pages'\n",
    "        second_subline ='</span>'\n",
    "        line = line[(line.find(first_subline)):line.find(second_subline)]\n",
    "        if len(line) > 0: #ensures I don't add an empty string to the list\n",
    "            back_matter_numbers_str.append(line)\n",
    "    print('bm numbers', len(back_matter_numbers_str))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction: by url\n",
    "***\n",
    "\n",
    "<it>Front page with 730 links, where each link directs you to another page with a specific research paper.\n",
    "Code below extracts text from the second webpage. However, it is not able to extract text from the pdf that are linked in the second page. There are 2 different links for downloading the same pdf.</it>\n",
    "- DOI and \n",
    "- ShareIt\n",
    "\n",
    "\n",
    "```<p>\n",
    "DOI: \n",
    "<a href=\"https://doi.org/10.1007/978-3-031-43990-2_43\">https://doi.org/10.1007/978-3-031-43990-2_43</a>\n",
    "</p>\n",
    "<p>\n",
    "SharedIt: \n",
    "<a href=\"https://rdcu.be/dnwLY\">https://rdcu.be/dnwLY</a>\n",
    "<br><br>\n",
    "</p>´´´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import regex as re\n",
    "from urllib.request import urlopen\n",
    "from urllib import request as urllib2\n",
    "\n",
    "# specify the URL of the archive here \n",
    "archive_url = \"http://conferences.miccai.org/2023/papers/\"\n",
    "url = \"http://conferences.miccai.org\"\n",
    "\n",
    "\n",
    "list_of_pdfs = []\n",
    "# create response object \n",
    "r = requests.get(archive_url) \n",
    "\n",
    "# create beautiful-soup object \n",
    "soup = BeautifulSoup(r.content,'html')\n",
    "\n",
    "# find all links on web-page \n",
    "links = soup.findAll('a') \n",
    "\n",
    "# filter links ending with .html \n",
    "list_of_urls = [url + link['href'] for link in links if link['href'].endswith('html')] \n",
    "# print(len(paper_links)) gives us 730 papers in total\n",
    "\n",
    "\n",
    "# now that we have all the links to the papers, we can go through each paper and find the pdf link\n",
    "for link in list_of_urls:\n",
    "\t# create response object per link from the total list of links\n",
    "\tr = requests.get(link) \n",
    "\tsoup = BeautifulSoup(r.content,'html')\n",
    "\n",
    "\t# find all urls with 'rdcu.be' which contains the specific paper\n",
    "\tlinks = soup.findAll(attrs={'href': re.compile(\"rdcu.be\")})\n",
    "\tpdfs = [link['href'] for link in links]\t\n",
    "\tlist_of_pdfs.extend(pdfs)\n",
    "\n",
    "print(len(list_of_pdfs))\n",
    "\n",
    "for link in list_of_pdfs:\n",
    "\t# create response object per link from the total list of links\n",
    "\tr = requests.get(link) \n",
    "\tsoup = BeautifulSoup(r.content,'html')\n",
    "\t# find all urls with 'rdcu.be' which contains the specific paper\n",
    "\tlinks = soup.findAll(attrs={'href': re.compile(\"/content/pdf\")})\n",
    "\tfinal_list = [link['content'] for link in links]\t\n",
    "\n",
    "print(len(final_list))  \n",
    "for pdf in list_of_pdfs:\n",
    "    filename = pdf.split(\"/\")[-1]\n",
    "    res = requests.get(pdf)\n",
    "    pdf = open(\"pdfs/\" + filename, 'wb')\n",
    "    pdf.write(res.content)\n",
    "    pdf.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Original function \n",
    "#main mining function returning the initial dataframe \n",
    "\n",
    "def mining(html_doc, year, current_page, all_pages, part):\n",
    "    #opening the html document (copy pasted and saved as a .doc file)\n",
    "    doc = open(html_doc, \"r\", encoding = \"ISO-8859-1\") \n",
    "    soup = BeautifulSoup(doc, 'html.parser' )\n",
    "\n",
    "    list_of_doi = soup.find_all(href=has_doi)\n",
    "        \n",
    "    #getting the titles and the doi's from list generated helper function\n",
    "\n",
    "    titles = []\n",
    "    doi_str = []\n",
    "\n",
    "    for element in list_of_doi:\n",
    "        titles.append(element.get_text()) #returns the titles as the only text in the list\n",
    "        string = str(element)\n",
    "        first_substring = '/chapter'\n",
    "        second_substring ='\">'\n",
    "        #separates out the DOIS (added the +9 to remove /chapter/ from the beginning of all DOIS)\n",
    "        doi_str.append(string[(string.find(first_substring)+9):string.find(second_substring)]) \n",
    "            \n",
    "    ## now the lines containing author are found\n",
    "    authors = soup.find_all(\"li\", class_=\"c-author-list__item\")\n",
    "        \n",
    "    #keeping only the author names\n",
    "    authors_str = []\n",
    "    for element in authors:\n",
    "        string = str(element)\n",
    "        first_substring = 'item\">'\n",
    "        second_substring ='</li>'\n",
    "        authors_str.append(string[(string.find(first_substring)+6):string.find(second_substring)])\n",
    "            \n",
    "    #now the lines containing page numbers are found\n",
    "    page_numbers= soup.find_all('div', class_ = \"c-meta\")\n",
    "\n",
    "    back_matter_numbers = soup.find_all('li', class_=\"c-card c-list-group__item c-card--flush u-pa-16 u-pl-0\")   \n",
    "\n",
    "    #keeping only the page numbers\n",
    "    page_numbers_str = []\n",
    "\n",
    "    for element in page_numbers:\n",
    "        string = element.get_text()[6:-1] #removes the white spaces and \"Pages \"\n",
    "        both = string.split(\"-\")\n",
    "        if 'x' not in string: #filtering out front matters\n",
    "            try: \n",
    "                if int(both[1])-int(both[0]) > 1: \n",
    "                    page_numbers_str.append(string)\n",
    "            except:\n",
    "                if \"C\" in string or \"E\" in string: #including corrections and erratum, are removed later\n",
    "                    page_numbers_str.append(string)\n",
    "    #filtering out back matters, only an issue in 2021 \n",
    "    #if year == 2023 and int(current_page) == int(all_pages):\n",
    "       #page_numbers_str = page_numbers_str[:-1]\n",
    "            \n",
    "    #need to create a list of the year of publication to add to dataframe \n",
    "    year_of_pub = []\n",
    "    for element in titles:\n",
    "        year_of_pub.append(year)\n",
    "    \n",
    "    #will add the part of the publication to the dataframe as well\n",
    "    part_of_pub = []\n",
    "    for element in titles:\n",
    "        part_of_pub.append(part)\n",
    "        \n",
    "    \n",
    "    #creating the column names and content for the dataframe        \n",
    "    data = {'Title': titles,\n",
    "        'Authors': authors_str,\n",
    "        'Page numbers' : page_numbers_str,\n",
    "        'DOI': doi_str,\n",
    "        'Year of publication' : year_of_pub,\n",
    "        'Part of publication' : part_of_pub       }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining function: separated for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "part1 = open('/Users/yasminsarkhosh/Documents/miccai2023 papers/miccai2023 vol 06 page 2 of 4.doc', encoding = \"ISO-8859-1\")\n",
    "\n",
    "soup = BeautifulSoup(part1, 'html.parser')\n",
    "#parsing the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_doi(href):\n",
    "    return href and re.compile(\"chapter/\").search(href)\n",
    "\n",
    "list_of_doi = soup.find_all(href=has_doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_19\">DiffDP: Radiotherapy Dose Prediction via a Diffusion Model</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_20\">A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_21\">Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_22\">Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_23\">Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_24\">Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_25\">Improving Pathology Localization: Multi-series Joint Attention Takes the Lead</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_26\">Detection of Basal Cell Carcinoma in Whole Slide Images</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_27\">SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_28\">STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_29\">Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_30\">Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_31\">TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_32\">Vision Transformer Based Multi-class Lesion Detection in IVOCT</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_33\">Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_34\">AR2T: Advanced Realistic Rendering Technique for Biomedical Volumes</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_35\">Transformer-Based End-to-End Classification of Variable-Length Volumetric Data</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_36\">ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_37\">MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans</a>, <a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-031-43987-2_38\">Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays</a>]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#list_of_doi[:5]\n",
    "print(list_of_doi) #vol4 page 3 of 4 has dois from 39 to 58\n",
    "print(len(list_of_doi)) #20 dois in total for vol4 page 3 of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the titles and the doi's from above function\n",
    "\n",
    "titles = []\n",
    "doi_str = []\n",
    "\n",
    "for element in list_of_doi:\n",
    "    titles.append(element.get_text())\n",
    "    string = str(element)\n",
    "    first_substring = '/chapter'\n",
    "    second_substring ='\">'\n",
    "    doi_str.append(string[(string.find(first_substring)):string.find(second_substring)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DiffDP: Radiotherapy Dose Prediction via a Diffusion Model', 'A Novel Multi-task Model Imitating Dermatologists for\\xa0Accurate Differential Diagnosis of\\xa0Skin Diseases in\\xa0Clinical Images', 'Wall Thickness Estimation from\\xa0Short Axis Ultrasound Images via\\xa0Temporal Compatible Deformation Learning', 'Mitral Regurgitation Quantification from\\xa0Multi-channel Ultrasound Images via\\xa0Deep Learning', 'Progressive Attention Guidance for\\xa0Whole Slide Vulvovaginal Candidiasis Screening', 'Detection-Free Pipeline for\\xa0Cervical Cancer Screening of\\xa0Whole Slide Images', 'Improving Pathology Localization: Multi-series Joint Attention Takes the\\xa0Lead', 'Detection of\\xa0Basal Cell Carcinoma in\\xa0Whole Slide Images', 'SCOL: Supervised Contrastive Ordinal Loss for\\xa0Abdominal Aortic Calcification Scoring on\\xa0Vertebral Fracture Assessment Scans', 'STAR-Echo: A Novel Biomarker for\\xa0Prognosis of\\xa0MACE in\\xa0Chronic Kidney Disease Patients Using Spatiotemporal Analysis and\\xa0Transformer-Based Radiomics Models', 'Interpretable Deep Biomarker for\\xa0Serial Monitoring of\\xa0Carotid Atherosclerosis Based on\\xa0Three-Dimensional Ultrasound Imaging', 'Learning Robust Classifier for\\xa0Imbalanced Medical Image Dataset with\\xa0Noisy Labels by\\xa0Minimizing Invariant Risk', 'TCEIP: Text Condition Embedded Regression Network for\\xa0Dental Implant Position Prediction', 'Vision Transformer Based Multi-class Lesion Detection in\\xa0IVOCT', 'Accurate and\\xa0Robust Patient Height and\\xa0Weight Estimation in\\xa0Clinical Imaging Using a\\xa0Depth Camera', 'AR2T: Advanced Realistic Rendering Technique for\\xa0Biomedical Volumes', 'Transformer-Based End-to-End Classification of\\xa0Variable-Length Volumetric Data', 'ProtoASNet: Dynamic Prototypes for\\xa0Inherently Interpretable and\\xa0Uncertainty-Aware Aortic Stenosis Classification in\\xa0Echocardiography', 'MPBD-LSTM: A Predictive Model for\\xa0Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans', 'Diffusion-Based Hierarchical Multi-label Object Detection to\\xa0Analyze Panoramic Dental X-Rays']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#titles[:5]\n",
    "print(titles)\n",
    "print(len(titles)) #20 titles in total for vol4 page 3 of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now want to find authors - YES!\n",
    "authors = []\n",
    "\n",
    "authors = soup.find_all(\"li\", class_=\"c-author-list__item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li class=\"c-author-list__item\">Zhenghao Feng, Lu Wen, Peng Wang, Binyu Yan, Xi Wu, Jiliu Zhou et al.</li>, <li class=\"c-author-list__item\">Yan-Jie Zhou, Wei Liu, Yuan Gao, Jing Xu, Le Lu, Yuping Duan et al.</li>, <li class=\"c-author-list__item\">Ang Zhang, Guijuan Peng, Jialan Zheng, Jun Cheng, Xiaohua Liu, Qian Liu et al.</li>, <li class=\"c-author-list__item\">Keming Tang, Zhenyi Ge, Rongbo Ling, Jun Cheng, Wufeng Xue, Cuizhen Pan et al.</li>, <li class=\"c-author-list__item\">Jiangdong Cai, Honglin Xiong, Maosong Cao, Luyan Liu, Lichi Zhang, Qian Wang</li>, <li class=\"c-author-list__item\">Maosong Cao, Manman Fei, Jiangdong Cai, Luyan Liu, Lichi Zhang, Qian Wang</li>, <li class=\"c-author-list__item\">Ashwin Raju, Micha Kornreich, Colin Hansen, James Browning, Jayashri Pawar, Richard Herzog et al.</li>, <li class=\"c-author-list__item\">Hongyan Xu, Dadong Wang, Arcot Sowmya, Ian Katz</li>, <li class=\"c-author-list__item\">Afsah Saleem, Zaid Ilyas, David Suter, Ghulam Mubashar Hassan, Siobhan Reid, John T. Schousboe et al.</li>, <li class=\"c-author-list__item\">Rohan Dhamdhere, Gourav Modanwal, Mohamed H. E. Makhlouf, Neda Shafiabadi Hassani, Satvika Bharadwaj, Pingfu Fu et al.</li>, <li class=\"c-author-list__item\">Xueli Chen, Xinqi Fan, Bernard Chiu</li>, <li class=\"c-author-list__item\">Jinpeng Li, Hanqun Cao, Jiaze Wang, Furui Liu, Qi Dou, Guangyong Chen et al.</li>, <li class=\"c-author-list__item\">Xinquan Yang, Jinheng Xie, Xuguang Li, Xuechen Li, Xin Li, Linlin Shen et al.</li>, <li class=\"c-author-list__item\">Zixuan Wang, Yifan Shao, Jingyi Sun, Zhili Huang, Su Wang, Qiyong Li et al.</li>, <li class=\"c-author-list__item\">Birgi Tamersoy, Felix Alexandru Pîrvan, Santosh Pai, Ankur Kapoor</li>, <li class=\"c-author-list__item\">Elena Denisova, Leonardo Manetti, Leonardo Bocchi, Ernesto Iadanza</li>, <li class=\"c-author-list__item\">Marzieh Oghbaie, Teresa Araujo, Taha Emre, Ursula Schmidt-Erfurth, Hrvoje Bogunovic</li>, <li class=\"c-author-list__item\">Hooman Vaseli, Ang Nan Gu, S. Neda Ahmadi Amiri, Michael Y. Tsang, Andrea Fung, Nima Kondori et al.</li>, <li class=\"c-author-list__item\">Xueyang Li, Han Xiao, Weixiang Weng, Xiaowei Xu, Yiyu Shi</li>, <li class=\"c-author-list__item\">Ibrahim Ethem Hamamci, Sezgin Er, Enis Simsar, Anjany Sekuboyina, Mustafa Gundogar, Bernd Stadlinger et al.</li>]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(authors)\n",
    "print(len(authors)) # 19 authors for vol4 page 3 of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping only the author names\n",
    "authors_str = []\n",
    "for element in authors:\n",
    "    string = str(element)\n",
    "    first_substring = 'item\">'\n",
    "    second_substring ='</li>'\n",
    "    authors_str.append(string[(string.find(first_substring)+6):string.find(second_substring)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zhenghao Feng, Lu Wen, Peng Wang, Binyu Yan, Xi Wu, Jiliu Zhou et al.', 'Yan-Jie Zhou, Wei Liu, Yuan Gao, Jing Xu, Le Lu, Yuping Duan et al.', 'Ang Zhang, Guijuan Peng, Jialan Zheng, Jun Cheng, Xiaohua Liu, Qian Liu et al.', 'Keming Tang, Zhenyi Ge, Rongbo Ling, Jun Cheng, Wufeng Xue, Cuizhen Pan et al.', 'Jiangdong Cai, Honglin Xiong, Maosong Cao, Luyan Liu, Lichi Zhang, Qian Wang', 'Maosong Cao, Manman Fei, Jiangdong Cai, Luyan Liu, Lichi Zhang, Qian Wang', 'Ashwin Raju, Micha Kornreich, Colin Hansen, James Browning, Jayashri Pawar, Richard Herzog et al.', 'Hongyan Xu, Dadong Wang, Arcot Sowmya, Ian Katz', 'Afsah Saleem, Zaid Ilyas, David Suter, Ghulam Mubashar Hassan, Siobhan Reid, John T. Schousboe et al.', 'Rohan Dhamdhere, Gourav Modanwal, Mohamed H. E. Makhlouf, Neda Shafiabadi Hassani, Satvika Bharadwaj, Pingfu Fu et al.', 'Xueli Chen, Xinqi Fan, Bernard Chiu', 'Jinpeng Li, Hanqun Cao, Jiaze Wang, Furui Liu, Qi Dou, Guangyong Chen et al.', 'Xinquan Yang, Jinheng Xie, Xuguang Li, Xuechen Li, Xin Li, Linlin Shen et al.', 'Zixuan Wang, Yifan Shao, Jingyi Sun, Zhili Huang, Su Wang, Qiyong Li et al.', 'Birgi Tamersoy, Felix Alexandru Pîrvan, Santosh Pai, Ankur Kapoor', 'Elena Denisova, Leonardo Manetti, Leonardo Bocchi, Ernesto Iadanza', 'Marzieh Oghbaie, Teresa Araujo, Taha Emre, Ursula Schmidt-Erfurth, Hrvoje Bogunovic', 'Hooman Vaseli, Ang Nan Gu, S. Neda Ahmadi Amiri, Michael Y. Tsang, Andrea Fung, Nima Kondori et al.', 'Xueyang Li, Han Xiao, Weixiang Weng, Xiaowei Xu, Yiyu Shi', 'Ibrahim Ethem Hamamci, Sezgin Er, Enis Simsar, Anjany Sekuboyina, Mustafa Gundogar, Bernd Stadlinger et al.']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#authors_str[:5]\n",
    "print(authors_str)\n",
    "print(len(authors_str)) #19 authors for vol4 page 3 of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zhenghao Feng', ' Lu Wen', ' Peng Wang', ' Binyu Yan', ' Xi Wu', ' Jiliu Zhou et al.']\n",
      "['Yan-Jie Zhou', ' Wei Liu', ' Yuan Gao', ' Jing Xu', ' Le Lu', ' Yuping Duan et al.']\n",
      "['Ang Zhang', ' Guijuan Peng', ' Jialan Zheng', ' Jun Cheng', ' Xiaohua Liu', ' Qian Liu et al.']\n",
      "['Keming Tang', ' Zhenyi Ge', ' Rongbo Ling', ' Jun Cheng', ' Wufeng Xue', ' Cuizhen Pan et al.']\n",
      "['Jiangdong Cai', ' Honglin Xiong', ' Maosong Cao', ' Luyan Liu', ' Lichi Zhang', ' Qian Wang']\n",
      "['Maosong Cao', ' Manman Fei', ' Jiangdong Cai', ' Luyan Liu', ' Lichi Zhang', ' Qian Wang']\n",
      "['Ashwin Raju', ' Micha Kornreich', ' Colin Hansen', ' James Browning', ' Jayashri Pawar', ' Richard Herzog et al.']\n",
      "['Hongyan Xu', ' Dadong Wang', ' Arcot Sowmya', ' Ian Katz']\n",
      "['Afsah Saleem', ' Zaid Ilyas', ' David Suter', ' Ghulam Mubashar Hassan', ' Siobhan Reid', ' John T. Schousboe et al.']\n",
      "['Rohan Dhamdhere', ' Gourav Modanwal', ' Mohamed H. E. Makhlouf', ' Neda Shafiabadi Hassani', ' Satvika Bharadwaj', ' Pingfu Fu et al.']\n",
      "['Xueli Chen', ' Xinqi Fan', ' Bernard Chiu']\n",
      "['Jinpeng Li', ' Hanqun Cao', ' Jiaze Wang', ' Furui Liu', ' Qi Dou', ' Guangyong Chen et al.']\n",
      "['Xinquan Yang', ' Jinheng Xie', ' Xuguang Li', ' Xuechen Li', ' Xin Li', ' Linlin Shen et al.']\n",
      "['Zixuan Wang', ' Yifan Shao', ' Jingyi Sun', ' Zhili Huang', ' Su Wang', ' Qiyong Li et al.']\n",
      "['Birgi Tamersoy', ' Felix Alexandru Pîrvan', ' Santosh Pai', ' Ankur Kapoor']\n",
      "['Elena Denisova', ' Leonardo Manetti', ' Leonardo Bocchi', ' Ernesto Iadanza']\n",
      "['Marzieh Oghbaie', ' Teresa Araujo', ' Taha Emre', ' Ursula Schmidt-Erfurth', ' Hrvoje Bogunovic']\n",
      "['Hooman Vaseli', ' Ang Nan Gu', ' S. Neda Ahmadi Amiri', ' Michael Y. Tsang', ' Andrea Fung', ' Nima Kondori et al.']\n",
      "['Xueyang Li', ' Han Xiao', ' Weixiang Weng', ' Xiaowei Xu', ' Yiyu Shi']\n",
      "['Ibrahim Ethem Hamamci', ' Sezgin Er', ' Enis Simsar', ' Anjany Sekuboyina', ' Mustafa Gundogar', ' Bernd Stadlinger et al.']\n"
     ]
    }
   ],
   "source": [
    "# splitting the group of authors to check for missing authors\n",
    "for element in authors_str:\n",
    "    print(element.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DiffDP: Radiotherapy Dose Prediction via a Diffusion Model']\n",
      "['A Novel Multi-task Model Imitating Dermatologists for\\xa0Accurate Differential Diagnosis of\\xa0Skin Diseases in\\xa0Clinical Images']\n",
      "['Wall Thickness Estimation from\\xa0Short Axis Ultrasound Images via\\xa0Temporal Compatible Deformation Learning']\n",
      "['Mitral Regurgitation Quantification from\\xa0Multi-channel Ultrasound Images via\\xa0Deep Learning']\n",
      "['Progressive Attention Guidance for\\xa0Whole Slide Vulvovaginal Candidiasis Screening']\n",
      "['Detection-Free Pipeline for\\xa0Cervical Cancer Screening of\\xa0Whole Slide Images']\n",
      "['Improving Pathology Localization: Multi-series Joint Attention Takes the\\xa0Lead']\n",
      "['Detection of\\xa0Basal Cell Carcinoma in\\xa0Whole Slide Images']\n",
      "['SCOL: Supervised Contrastive Ordinal Loss for\\xa0Abdominal Aortic Calcification Scoring on\\xa0Vertebral Fracture Assessment Scans']\n",
      "['STAR-Echo: A Novel Biomarker for\\xa0Prognosis of\\xa0MACE in\\xa0Chronic Kidney Disease Patients Using Spatiotemporal Analysis and\\xa0Transformer-Based Radiomics Models']\n",
      "['Interpretable Deep Biomarker for\\xa0Serial Monitoring of\\xa0Carotid Atherosclerosis Based on\\xa0Three-Dimensional Ultrasound Imaging']\n",
      "['Learning Robust Classifier for\\xa0Imbalanced Medical Image Dataset with\\xa0Noisy Labels by\\xa0Minimizing Invariant Risk']\n",
      "['TCEIP: Text Condition Embedded Regression Network for\\xa0Dental Implant Position Prediction']\n",
      "['Vision Transformer Based Multi-class Lesion Detection in\\xa0IVOCT']\n",
      "['Accurate and\\xa0Robust Patient Height and\\xa0Weight Estimation in\\xa0Clinical Imaging Using a\\xa0Depth Camera']\n",
      "['AR2T: Advanced Realistic Rendering Technique for\\xa0Biomedical Volumes']\n",
      "['Transformer-Based End-to-End Classification of\\xa0Variable-Length Volumetric Data']\n",
      "['ProtoASNet: Dynamic Prototypes for\\xa0Inherently Interpretable and\\xa0Uncertainty-Aware Aortic Stenosis Classification in\\xa0Echocardiography']\n",
      "['MPBD-LSTM: A Predictive Model for\\xa0Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans']\n",
      "['Diffusion-Based Hierarchical Multi-label Object Detection to\\xa0Analyze Panoramic Dental X-Rays']\n"
     ]
    }
   ],
   "source": [
    "# splitting the group of authors to check for missing authors\n",
    "for element in titles:\n",
    "    print(element.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next up is page numbers\n",
    "#line to find:\n",
    "#<div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 1-9</span>\n",
    "\n",
    "page_numbers = []\n",
    "page_numbers= soup.find_all('div', class_ = \"c-meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 191-201</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 202-212</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 213-222</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 223-232</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 233-242</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 243-252</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 253-262</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 263-272</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 273-283</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 284-294</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 295-305</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 306-316</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 317-326</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 327-336</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 337-346</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 347-357</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 358-367</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 368-378</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 379-388</span>\n",
      "</div>, <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 389-399</span>\n",
      "</div>]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(page_numbers) #vol4 page 3 of 4 has pages starting from 405 ending with 621\n",
    "print(len(page_numbers)) #20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191-201\n",
      "202-212\n",
      "213-222\n",
      "223-232\n",
      "233-242\n",
      "243-252\n",
      "253-262\n",
      "263-272\n",
      "273-283\n",
      "284-294\n",
      "295-305\n",
      "306-316\n",
      "317-326\n",
      "327-336\n",
      "337-346\n",
      "347-357\n",
      "368-378\n",
      "379-388\n",
      "389-399\n"
     ]
    }
   ],
   "source": [
    "#now the lines containing page numbers are found\n",
    "#page_numbers= soup.find_all('div', class_ = \"c-meta\")\n",
    "    \n",
    "#keeping only the page numbers\n",
    "page_numbers_str = []\n",
    "\n",
    "\n",
    "# an element in page_numbers_str looks like this:\n",
    "'''' <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" \n",
    "data-test=\"page-number\"> Pages 618-627</span> </div> '''  \n",
    "\n",
    "for element in page_numbers:            \n",
    "    #removes white spaces, and everything within the div tag that is not the page numbers\n",
    "    string = element.get_text()[6:13]                       #618-627\n",
    "    print(string)\n",
    "    #splits the string into two numbers\n",
    "    both = string.split(\"-\")                                #['618', '627']\n",
    "    #filtering out front matters and back matters                                                   \n",
    "    if 'x' and '1-1' and '463-463' and '135-135' and '649-649' and '247-247' and '281-281' and '369-369' and '433-433' and '509-509' and '583-583' and '757-757' and '535-535' not in string:             \n",
    "        try: \n",
    "            if int(both[1])-int(both[0]) > 1:               #if True adds the string to the list\n",
    "                page_numbers_str.append(string)\n",
    "        except:\n",
    "            if \"C1\" in string or \"C\" in string:              #page numbers that are in the form of C<some number> \n",
    "                page_numbers_str.append(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(page_numbers_str))\n",
    "print(page_numbers_str) #vol4 page 3 of 4 has 20 page numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the front matter in the document\n",
    "text = 'Front Matter'\n",
    "front_matter = soup.find_all(lambda tag: tag.name == \"div\" and text in tag.text)\n",
    "front_matter__str = []\n",
    "\n",
    "for element in front_matter:\n",
    "    line = str(element)\n",
    "    first_subline = 'Pages'\n",
    "    second_subline ='</span>'\n",
    "    line = line[(line.find(first_subline)):line.find(second_subline)]\n",
    "    if len(line) > 0: #ensures I don't add an empty string to the list\n",
    "        front_matter__str.append(line)\n",
    "\n",
    "front_matter__str = np.unique(front_matter__str)\n",
    "front_matter__str = front_matter__str.tolist()\n",
    "#print('front matter', front_matter__str, len(front_matter__str))\n",
    "\n",
    "text = 'Back Matter'\n",
    "back_matter = soup.find_all(lambda tag: tag.name == \"div\" and text in tag.text)\n",
    "back_matter__str = []\n",
    "\n",
    "for element in back_matter:\n",
    "    line = str(element)\n",
    "    first_subline = 'Pages'\n",
    "    second_subline ='</span>'\n",
    "    line = line[(line.find(first_subline)):line.find(second_subline)]\n",
    "    if len(line) > 0: #ensures I don't add an empty string to the list\n",
    "        back_matter__str.append(line)\n",
    "\n",
    "back_matter__str = np.unique(back_matter__str)  #removes duplicates\n",
    "back_matter__str = back_matter__str.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(page_numbers))\n",
    "print(len(authors_str))\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strips it back to simply the pages\n",
    "page_numbers_str = []\n",
    "\n",
    "for element in page_numbers:\n",
    "    string = str(element)\n",
    "    first_substring = 'Pages'\n",
    "    second_substring ='</span>'\n",
    "    string = string[(string.find(first_substring)):string.find(second_substring)]\n",
    "    if len(string) > 0: #ensures I don't add an empty string to the list\n",
    "        page_numbers_str.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a check that I've found the same amount of authors, titles, dois and page numbers\n",
    "if len(page_numbers_str) == len(authors_str) == len(titles) == len(doi_str):\n",
    "    print(\"true\")\n",
    "else:\n",
    "    print(\"You have an error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(page_numbers_str))\n",
    "print(len(authors_str))\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_numbers_str = page_numbers_str[:-1]\n",
    "page_numbers_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a check that I've found the same amount of authors, titles, dois and page numbers\n",
    "if len(page_numbers_str) == len(authors_str) == len(titles) == len(doi_str):\n",
    "    print(\"true\")\n",
    "else:\n",
    "    print(\"You have an error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now want to combine everything in a pandas dataframe\n",
    "\n",
    "import pandas as pd\n",
    "#need to add the year (probably a better way to do this, but should be fine)\n",
    "year_of_pub = []\n",
    "for element in titles:\n",
    "    year_of_pub.append(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Title': titles,\n",
    "        'Authors': authors_str,\n",
    "        'Page numbers' : page_numbers_str,\n",
    "        'DOI': doi_str,\n",
    "        'Year of publication' : year_of_pub\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv('miccai_2023_part2.csv')\n",
    "#not sure why it seems to put \"\" around only the authors in the csv file, but good enough to start! \n",
    "#Now want to combine this into a more pretty script that I can then feed all the pages into it, will need to do some \n",
    "#combining at the end, adding everything together"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
