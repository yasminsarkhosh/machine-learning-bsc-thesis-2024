Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer Hexin Dong1,2,3, Jiawen Yao1,3(B),YuxingTang1 , Mingze Yuan1,2,3, Yingda Xia1 , Jian Zhou4,5,HongLu6 , Jingren Zhou1,3,Bin Dong2,7,LeLu1 , Zaiyi Liu8 , Li Zhang2(B), Yu Shi9(B), and Ling Zhang1 1 DAMO Academy, Alibaba Group, Hangzhou, China yaojiawen.yjw@alibaba-inc.com 2 Peking University, Beijing, China zhangli pku@pku.edu.cn 3 Hupan Lab, Hangzhou 310023, China 4 Sun Yat-sen University Cancer Center, Guangzhou, China 5 South China Hospital, Shenzhen University, Shenzhen, China 6 Tianjin Medical University Cancer Institute and Hospital, Tianjin, China 7 Peking University Changsha Institute for Computing and Digital Economy, Changsha, China 8 Guangdong Provincial People’s Hospital, Guangzhou, China 9 Shengjing Hospital, Shenyang, China 18940259980@163.com Abstract. Pancreatic ductal adenocarcinoma (PDAC) is a highly lethal cancer in which the tumor-vascular involvement greatly aﬀects the resectability and, thus, overall survival of patients. However, current prognostic prediction methods fail to explicitly and accurately investigate relationships between the tumor and nearby important vessels. This paper proposes a novel learnable neural distance that describes the precise relationship between the tumor and vessels in CT images of different patients, adopting it as a major feature for prognosis prediction. Besides, diﬀerent from existing models that used CNNs or LSTMs to exploit tumor enhancement patterns on dynamic contrast-enhanced CT imaging, we improved the extraction of dynamic tumor-related texture features in multi-phase contrast-enhanced CT by fusing local and global features using CNN and transformer modules, further enhancing the features extracted across multi-phase CT images. We extensively evaluated and compared the proposed method with existing methods in the multicenter (n = 4) dataset with 1,070 patients with PDAC, and statistical H. Dong—Work was done during an internship at Alibaba DAMO Academy. Supplementary Information The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 24. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 H. Greenspan et al. (Eds.): MICCAI 2023, LNCS 14224, pp. 241–251, 2023. https://doi.org/10.1007/978-3-031-43904-9_24 analysis conﬁrmed its clinical eﬀectiveness in the external test set consisting of three centers. The developed risk marker was the strongest predictor of overall survival among preoperative factors and it has the potential to be combined with established clinical factors to select patients at higher risk who might beneﬁt from neoadjuvant therapy. Keywords: Pancreatic ductal adenocarcinoma (PDAC) · Survival prediction · Texture-aware Transformer · Cross-attention · Nerual distance Introduction Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest forms of human cancer, with a 5-year survival rate of only 9% [16]. Neoadjuvant chemotherapy can increase the likelihood of achieving a margin-negative resection and avoid unnecessary surgery in patients with aggressive tumor types [23]. Providing accurate and objective preoperative biomarkers is crucial for triaging patients who are most likely to beneﬁt from neoadjuvant chemotherapy. However, current clinical markers such as larger tumor size and high carbohydrate antigen (CA) 19-9 level may not be suﬃcient to accurately tailor neoadjuvant treatment for patients [19]. Therefore, multi-phase contrast-enhanced CT has a great potential to enable personalized prognostic prediction for PDAC, leveraging its ability to provide a wealth of texture information that can aid in the development of accurate and eﬀective prognostic models [2,10]. Previous studies have utilized image texture analysis with hand-crafted features to predict the survival of patients with PDACs [1], but the representational Fig. 1. Two examples of spatial information between vessel (orange region) and tumor (green region). The minimum distance, which refers to the closest distance between the Superior Mesenteric Artery (SMA) and the PDAC tumor region, is almost identical in these two cases. We deﬁne the surface-to-surface distance based on point-to-surface distance (weighted-average of red lines from ♦ to ) instead of point-to-point distance (blue lines) to better capture the relationship between the tumor and the perivascular tissue. Here ♦ and &#3; are points sampled from subset Vc and Pc deﬁned in Eq. 4. The distances ˆˆ and weights shown in the ﬁgure is for illustration purposes only. (Color ﬁgure online) power of these features may be limited. In recent years, deep learning-based methods have shown promising results in prognosis models [3,6,12]. However, PDACs diﬀer signiﬁcantly from the tumors in these studies. A clinical investigation based on contrast-enhanced CT has revealed a dynamic correlation between the internal stromal fractions of PDACs and their surrounding vasculature [14]. Therefore, focusing solely on the texture information of the tumor itself may not be eﬀective for the prognostic prediction of PDAC. It is necessary to incorporate tumor-vascular involvement into the feature extraction process of the prognostic model. Although some studies have investigated tumor-vascular relationships [21,22], these methods may not be suﬃciently capable of capturing the complex dynamics between the tumor and its environment. We propose a novel approach for measuring the relative position relationship between the tumor and the vessel by explicitly using the distance between them. Typically, Chamfer distance [7], Hausdorﬀ distance [8], or other surfaceawareness metrics are used. However, as shown in Fig. 1, these point-to-point distances cannot diﬀerentiate the degree of tumor-vascular invasion [18]. To address this limitation, we propose a learnable neural distance that considers all relevant points on diﬀerent surfaces and uses an attention mechanism to compute a combined distance that is more suitable for determining the degree of invasion. Furthermore, to capture the tumor enhancement patterns across multi-phase CT images, we are the ﬁrst to combine convolutional neural networks (CNN) and transformer [4] modules for extracting the dynamic texture patterns of PDAC and its surroundings. This approach takes advantage of the visual transformer’s adeptness in capturing long-distance information compared to the CNN-onlybased framework in the original approach. By incorporating texture information between PDAC, pancreas, and peripancreatic vessels, as well as the local tumor information captured by CNN, we aim to improve the accuracy of our prognostic prediction model. In this study, we make the following contributions: (1) We propose a novel approach for aiding survival prediction in PDAC by introducing a learnable neural distance that explicitly evaluates the degree of vascular invasion between the tumor and its surrounding vessels. (2) We introduce a texture-aware transformer block to enhance the feature extraction approach, combining local and global information for comprehensive texture information. We validate that the cross-attention is utilized to capture cross-modality information and integrate it with in-modality information, resulting in a more accurate and robust prognostic prediction model for PDAC. (3) Through extensive evaluation and statistical analysis, we demonstrate the eﬀectiveness of our proposed method. The signature built from our model remains statistically signiﬁcant in multivariable analysis after adjusting for established clinical predictors. Our proposed model has the potential to be used in combination with clinical factors for risk stratiﬁcation and treatment decisions for patients with PDAC. 2 Methods As showninFig. 2, the proposed method consists of two main components. The ﬁrst component combines the CNN and transformer to enhance the extraction of tumor dynamic texture features. The second component proposes a neural distance metric between PDAC and important vessels to assess their involvements. 2.1 Texture-Aware Vision Transformer: Combination of CNN and Transformer Recently, self-attention models, speciﬁcally vision transformers (ViTs [4]), have emerged as an alternative to CNNs in survival prediction [15,25]. Our proposed texture-aware transformer, inspired by MobileViT [13], aims to combine both local information (such as PDAC texture) and global information (such as the relationship between PDAC and the pancreas). This approach is diﬀerent from previous methods that rely solely on either CNN-based or transformer-based backbones, focusing only on local or global information, respectively. The texture-aware transformer (Fig. 2) comprises three blocks, each consisting of a texture-aware CNN block and a texture-aware self-attention block. These ∈ RH×W ×D×Cblocks encode the input feature of an image Fi to the hidden ∈ RH×W ×D×Clfeature Fc using a 3 × 3 × 3 convolutional layer, followed by a Fig. 2. An overview of the proposed method. The texture-aware transformer captures texture information among PDAC, Pancreas and vessels around Pancreas with our proposed texture-aware transformer block and a cross-attention block to fusion cross-modality features. The structure-aware block extracts the structure relationship between PDAC and four related vessels. The neural distance calculates the distances between the PDAC surface and the vessel surface with our proposed neural distance. We ﬁrst select related points set from the closest sub-surface on PDAC and vessels respectively. Then we use a cross-attention block to obtain the neural distance. Finally, we concatenate features from three branches to obtain the survival outcome OOS. 1 × 1 × 1 convolutional layer. The 3 × 3 × 3 convolution captures local spatial information, while the 1 × 1 × 1 convolution maps the input tensor to a higherdimensional space (i.e., Cl >C). The texture-aware CNN block downsamples the input, and the texture-aware self-attention block captures long-range nonlocal dependencies through a patch-wise self-attention mechanism. In the texture-aware self-attention block, the input feature Fc is divided ∈ RV ×N×Cuinto N non-overlapping 3D patches Fu , where V = hwd and N = HWD/V is the number of patches, and h, w, d are the height, width, and depth of a patch, respectively. For each voxel position within a patch, we apply a multi-head self-attention block and a feed-forward block following [20] to obtain the output feature Fo. In this study, preoperative multi-phase CE-CT pancreatic imaging includes the non-contrast phase, the pancreatic phase and venous phase. Therefore, we obtain three outputs from the transformer block with the input of these phases, denoted as F1 , F2 , F3 ∈ RD×C , resulting in the concatenatedooo output Fo ∈ RD×3C . Instead of directly fusing the outputs as in previous work, we employ a 3-way cross-attention block to extract cross-modality information from these phases. The cross-attention is performed on the concatenated self-attention matrix with an extra mask M ∈{0, −∞}3C×3C , deﬁned as: Fcross = Softmax(QKT + M)V, &#2; −∞ kC <i,j ≤ (k +1)C, k =0, 1, 2, (1)M(i, j)= 0 otherwise, Here, Q, K, V are the query, key, and value matrices, respectively, obtained by linearly projecting the input FT ∈ R3C×D . The cross-modality output Fcross o and in-modality output FT are then concatenated and passed through an average o pooling layer to obtain the ﬁnal output feature of the texture branch, denoted as Ft ∈ RCt . 2.2 Neural Distance: Positional and Structural Information Between PDAC and Vessels The vascular involvement in patients with PDAC aﬀects the resectability and treatment planning [5]. In this study, we investigate four important vessels: portal vein and splenic vein (PVSV), superior mesenteric artery (SMA), superior mesenteric vein (SMV), and truncus coeliacus (TC). We used a semi-supervised nnUnet model to segment PDAC and the surrounding vessels, following recent work [11,21]. We deﬁne a general distance between the surface boundaries of PDAC (P) and the aforementioned four types of vessels (V)as D(V, P), which can be derived as follows: &#3;&#3;11D(V, P)= dss(V, P)+ dss(P, V)= dps(v, P)dv + dps(p, V)dp,V&#5; P&#5; VP (2) where v ∈Vand p ∈Pare points on the surfaces of blood vessels and PDAC, respectively. The point-to-surface distance dps(v,P) is the distance from a point 2v on Vto P, deﬁned as dps(v,P) = minp∈P v −p2, and vice versa. To numerically calculate the integrals in the previous equation, we uniformly sample from the surfaces Vand Pto obtain the sets Vˆ and Pˆ consisting of Nv points and Np points, respectively. The distance is then calculated between the two sets using the following equation: &#4;&#4; D(Vˆ,Pˆ)= 1 dps(v,Pˆ)+ 1 dps(p,Vˆ). (3)Nv Np v∈Vˆ p∈Pˆ However, the above distance treats all points equally and may not be ﬂexible enough to adapt to individualized prognostic predictions. Therefore, we improve the above equation in two ways. Firstly, we focus on the sub-sets Vˆ c and Pˆ c of Vˆ and Pˆ, respectively, which only contain the K closest points to the opposite surfaces Pˆ and Vˆ, respectively. The sub-sets are deﬁned as: K &#4; Vˆ c = argmin dps(vi,Pˆ),{v1,v2,··· ,vK }⊂Vˆ i=1 (4)K &#4; Pˆ c = argmin dps(pi,Vˆ).{p1,p2,··· ,pK }⊂Pˆ i=1 Secondly, we regard the entire sets Vˆ c and Pˆ c as sequences and calculate the distance using a 2-way cross-attention block (similar to Eq. 1) to build a neural distance based on the 3D spatial coordinates of each point: Dθ(Vˆ,Pˆ) = CrossAttention(Vˆ c,Pˆ c), Vˆ c,Pˆ c ∈RK×3 . (5) Neural distance allows for the ﬂexible assignment of weights to diﬀerent points and is able to ﬁnd positional information that is more suitable for PDAC prognosis prediction. In addition to neural distance, we use the 3D-CNN model introduced in [22] to extract the structural relationship between PDAC and the ∈R2×H×W ×Dvessels. Speciﬁcally, we concatenate each PDAC-vessel pair Xv ,s where v ∈{PVSV, SMV, SMA, TC}and obtain the structure feature Fs ∈RCs . Finally, we concatenate the features extracted from the two components and apply a fully-connected layer to predict the survival outcome, denoted as OOS , which is a value between 0 and 1. To optimize the proposed model, we use the negative log partial likelihood as the survival loss [9]. 3 Experiments Dataset. In this study, we used data from Shengjing Hospital to train our method with 892 patients, and data from three other centers, including Guangdong Provincial People’s Hospital, Tianjin Medical University and Sun Yatsen University Cancer Center for independent testing with 178 patients. The contrast-enhanced CT protocol included non-contrast, pancreatic, and portal venous phases. PDAC masks for 340 patients were manually labeled by a radiologist from Shengjing Hospital with 18 years of experience in pancreatic cancer, while the rest were predicted using self-learning models [11,24] and checked by the same annotator. Other vessel masks were generated using the same semisupervised segmentation models. C-index was used as our primary evaluation metric for survival prediction. We also reported the survival AUC, which estimates the cumulative area under the ROC curve for the ﬁrst 36 months. Implementation Details: We used nested 5-fold cross-validation and augmented the training data by rotating volumetric tumors in the axial direction and randomly selecting cropped regions with random shifts. We also set the output feature dimensions to Ct = 64 for the texture-aware transformer, Cs =64 for the structure extraction and K = 32 for the neural distance. The batch size was 16 and the maximum iteration was set to 1000 epochs, and we selected the model with the best performance on the validation set during training for testing. We implemented our experiments using PyTorch 1.11 and trained the models on a single NVIDIA 32G-V100 GPU. Ablation Study. We ﬁrst evaluated the performance of our proposed textureaware transformer (TAT) by comparing it with the ResNet18 CNN backbone and ViT transformer backbone, as shown in Table 1. Our model leverages the strengths of both local and global information in the pancreas and achieved the best result. Next, we compared diﬀerent methods for multi-phase stages, including LSTM, early fusion (Fusion), and cross-attention (Cross) in our method. Cross-attention is more eﬀective and lightweight than LSTM. Moreover, we separated texture features into in-phase features and cross-phase features, which is more reasonable than early fusion. Secondly, we evaluated each component in our proposed method, as shown in Fig. 2, and presented the results in Table 1. Combining the texture-aware transformer and regular structure information improved the results from 0.630 to 0.648, as tumor invasion strongly aﬀects the survival of PDAC patients. We also employed a simple 4-variable regression model that used only the Chamfer distance of the tumor and the four vessels for prognostic prediction. The resulting C-index of 0.611 conﬁrmed the correlation of the distance with the survival, which is consistent with clinical ﬁndings [18]. Explicitly adding the distance measure further improved the results. Our proposed neural distance metric outperformed traditional surface distance metrics like Chamfer distance, indicating its suitability for distinguishing the severity of PDAC. Comparisons. To further evaluate the performance of our proposed model, we compared it with recent deep prediction methods [17,21] and report the results in Table 2. We modiﬁed baseline deep learning models [12,17] and used their network architectures to take a single pancreatic phase or all three phases as inputs. DeepCT-PDAC [21] is the most recent method that considers both tumor-related and tumor-vascular relationships using 3D CNNs. Our proposed method, which uses the transformer and structure-aware blocks to capture tumor enhancement Table 1. Ablation tests with diﬀerent network backbones including ResNet18 (Res), ViT and texture-aware transformer (TAT) and methods for multi-phases including LSTM, early fusion (Fusion) and cross-attention (Cross). Network Backbone  Structural Info  Distance  Model Size (M)  C-index  Res-LSTM  –  –  65.06  0.618 ± 0.017  Res-Cross  –  –  43.54  0.625 ± 0.016  ViT-Cross  –  –  23.18  0.628 ± 0.018  TAT-Fusion  –  –  3.64  0.626 ± 0.022  TAT-Cross  –  –  15.13  0.630 ± 0.019  TAT-Cross  &#2;  –  15.90  0.648 ± 0.021  –  –  Chamfer distance  –  0.611 ± 0.029  TAT-Cross  &#2;  Chamfer distance  15.93  0.652 ± 0.019  TAT-Cross  &#2;  Nerual distance  16.28  0.656 ± 0.017  Table 2. Results of diﬀerent methods on nested 5-fold CV and independent set. Nested 5-fold CV (n = 892) Independent test (n = 178) C-index AUC C-index AUC 3DCNN-P [12] 0.630 ± 0.009 0.668 ± 0.019 0.674 0.740 Early Fusion [17] 0.635 ± 0.011 0.670 ± 0.024 0.696 0.779 DeepCT-PDAC [21] 0.640 ± 0.018 0.680 ± 0.036 0.697 0.773 Ours 0.656 ± 0.017 0.695 ± 0.023 0.710 0.792 patterns and tumor-vascular involvement, demonstrated its eﬀectiveness with better performance in both nested 5-fold cross-validation and the multi-center independent test set. In Table 3, we used univariate and multivariate Cox proportional-hazards models to evaluate our signature and other clinicopathologic factors in the independent test set. The proposed risk stratiﬁcation was a signiﬁcant prognostic factor, along with other factors like pathological TNM stages. After selecting signiﬁcant variables (p<0.05) in univariate analysis, our proposed staging remained strong in multivariable analysis after adjusting for important prognostic markers like pT and resection margins. Notably, our proposed marker remained the strongest among all pre-operative markers, such as tumor size and CA 19-9. Neoadjuvant Therapy Selection. To demonstrate the added value of our signature as a tool to select patients for neoadjuvant treatment before surgery, we plotted Kaplan-Meier survival curves in Fig. 3. We further stratify patients by our signature after grouping them by tumor size and CA19-9, two clinically used preoperative criteria for selection, and also age. Our signature could signiﬁcantly stratify patients in all cases and those in the high-risk group had worse outcomes and might be considered as potential neoadjuvant treatment candidates (e.g. 33 high-risk patients with larger tumor size and high CA19-9). Independent test set (n = 178)  Univariate Analysis  Multivariate Analysis  HR (95% CI)  p-value  HR (95% CI)  p-value  Proposed (High vs low risk)  2.42(1.64-3.58)  <0.0001  1.85(1.08-3.17)  0.027  Age (> 60 vs = 60)  1.49(1.01-2.20)  0.043  1.01(0.65-1.58)  0.888  Sex (Male vs Female)  1.28(0.86-1.90)  0.221  - - pT (pT3-pT4 vs pT1-pT2)  3.17(2.10-4.77)  <0.0001  2.44(1.54-3.86)  0.00015  pN (Positive ve Negative)  1.47(0.98-2.20)  0.008  1.34(0.85-2.12)  0.210  Resection margin (R1 vs R0)  2.84(1.64-4.93)  <0.0001  1.68(0.92-3.07)  0.091  CA19-9 (> 210 vs ≤ 210 U/mL)  0.94(0.64-1.39)  0.759  - - Tumor Size (> 25 vs ≤ 25 mm)  2.36(1.59-3.52)  <0.0001  0.99(0.52-1.85)  0.963  Tumor Location (Head vs Tail)  1.06(0.63-1.79)  0.819  - - Fig. 3. Kaplan-Meier analyses of overall survival according to the proposed signature in all patients in the independent test set (n = 178) and subgroups deﬁned by preoperative factors. High risk group indicated by the proposed method is the potential patient group that could beneﬁt from neoadjuvant treatment before surgery. 4 Conclusion In our paper, we propose a multi-branch transformer-based framework for predicting cancer survival. Our framework includes a texture-aware transformer that captures both local and global information about the PDAC and pancreas. We also introduce a neural distance to calculate a more reasonable distance between PDAC and vessels, which is highly correlated with PDAC survival. We have extensively evaluated and statistically analyzed our proposed method, demonstrating its eﬀectiveness. Furthermore, our model can be combined with established high-risk features to aid in the patient selections who might beneﬁt from neoadjuvant therapy before surgery. Acknowledgement. This work was supported by Alibaba Group through Alibaba Research Intern Program. Bin Dong and Li Zhang was partly supported by NSFC 12090022 and 11831002, and Clinical Medicine Plus X-Young Scholars Project of Peking University PKU2023LCXQ041. Yu Shi was supported by the National Natural Science Foundation of China (No. 82071885). References 1. Attiyeh, M.A., et al.: Survival prediction in pancreatic ductal adenocarcinoma by quantitative computed tomography image analysis. Ann. Surg. Oncol. 25(4), 1034–1042 (2018) 2. Bian, Y., et al.: Artiﬁcial intelligence to predict lymph node metastasis at CT in pancreatic ductal adenocarcinoma. Radiology 306(1), 160–169 (2023) 3. Cheng, N.M., et al.: Deep learning for fully automated prediction of overall survival in patients with oropharyngeal cancer using FDG-PET imaging. Clin. Cancer Res. 27(14), 3948–3959 (2021) 4. Dosovitskiy, A., et al.: An image is worth 16x16 words: transformers for image recognition at scale. In: ICLR (2021) 5. Ducreux, M., et al.: Cancer of the pancreas: ESMO clinical practice guidelines for diagnosis, treatment and follow-up. Ann. Oncol. 26, v56–v68 (2015) 6. Feng, Y., Wang, J., An, D., Gu, X., Xu, X., Zhang, M.: End-to-end evidentialeﬃcient net for radiomics analysis of brain MRI to predict oncogene expression and overall survival. In: Wang, L., Dou, Q., Fletcher, P.T., Speidel, S., Li, S. (eds.) MICCAI 2022. LNCS, vol. 13433, pp. 282–291. Springer, Cham (2022). https:// doi.org/10.1007/978-3-031-16437-8 27 7. Haoqiang Fan, H.S., Guibas, L.: A point set generation network for 3D object reconstruction from a single image. In: CVPR (2017) 8. Huttenlocher, D.P., Rucklidge, W.J., Klanderman, G.A.: Comparing images using the hausdorﬀ distance under translation. In: Proceedings 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (2002) 9. Katzman, J.L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., Kluger, Y.: Deepsurv: personalized treatment recommender system using a cox proportional hazards deep neural network. BMC Med. Res. Methodol. 18(1), 24 (2018) 10. Koay, E.J., et al.: Computed tomography-based biomarker outcomes in a prospective trial of preoperative folﬁrinox and chemoradiation for borderline resectable pancreatic cancer. JCO Precis. Oncol. 3, 1–15 (2019) 11. Koehler, G., Isensee, F., Maier-Hein, K.: A noisy nnU-Net student for semisupervised abdominal organ segmentation. In: Ma, J., Wang, B. (eds.) MICCAI 2022. LNCS, vol. 13816, pp. 128–138. Springer, Cham (2023). https://doi.org/10. 1007/978-3-031-23911-3 12 12. Lou, B., et al.: An image-based deep learning framework for individualising radiotherapy dose: a retrospective analysis of outcome prediction. Lancet Digit. Health 1(3), e136–e147 (2019) 13. Mehta, S., Rastegari, M.: Mobilevit: light-weight, general-purpose, and mobilefriendly vision transformer. In: ICLR (2022) 14. Prokesch, R.W., Chow, L.C., Beaulieu, C.F., Bammer, R., Jeﬀrey, R.B., Jr.: Isoattenuating pancreatic adenocarcinoma at multi-detector row CT: secondary signs. Radiology 224(3), 764–768 (2002) 15. Saeed, N., Sobirov, I., Al Majzoub, R., Yaqub, M.: TMSS: an end-to-end transformer-based multimodal network for segmentation and survival prediction. In: Wang, L., Dou, Q., Fletcher, P.T., Speidel, S., Li, S. (eds.) MICCAI 2022. LNCS, vol. 13437, pp. 319–329. Springer, Cham (2022). https://doi.org/10.1007/ 978-3-031-16449-1 31 16. Siegel, R.L., Miller, K.D., Jemal, A.: Cancer statistics, 2019. CA Cancer J. Clin. 69(1), 7–34 (2019) 17. Tang, Z., et al.: Deep learning of imaging phenotype and genotype for predicting overall survival time of glioblastoma patients. IEEE Trans. Med. Imaging 39(6), 2100–2109 (2020) 18. Tempero, M.A., et al.: Pancreatic adenocarcinoma, version 2.2021, NCCN clinical practice guidelines in oncology. J. Natl. Compr. Cancer Netw. 19(4), 439–457 (2021) 19. Tsai, S., et al.: Importance of normalization of ca19-9 levels following neoadjuvant therapy in patients with localized pancreatic cancer. Ann. Surg. 271(4), 740–747 (2020) 20. Vaswani, A., et al.: Attention is all you need. In: Guyon, I., et al. (eds.) NeurIPS, vol. 30. Curran Associates, Inc. (2017) 21. Yao, J., et al.: Deep learning for fully automated prediction of overall survival in patients undergoing resection for pancreatic cancer: a retrospective multicenter study. Ann. Surg. 278(1), e68–e79 (2023) 22. Yao, J., et al.: Deepprognosis: Preoperative prediction of pancreatic cancer survival and surgical margin via comprehensive understanding of dynamic contrastenhanced CT imaging and tumor-vascular contact parsing. Med. Image Anal. 73, 102150 (2021) 23. Yuan, M., et al.: Devil is in the queries: advancing mask transformers for realworld medical image segmentation and out-of-distribution localization. In: CVPR, pp. 23879–23889 (2023) 24. Zhang, L., et al.: Robust pancreatic ductal adenocarcinoma segmentation with multi-institutional multi-phase partially-annotated CT scans. In: Martel, A.L., et al. (eds.) MICCAI 2020. LNCS, vol. 12264, pp. 491–500. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-59719-1 48 25. Zheng, H., et al.: Multi-transSP: multimodal transformer for survival prediction of nasopharyngeal carcinoma patients. In: Wang, L., Dou, Q., Fletcher, P.T., Speidel, S., Li, S. (eds.) MICCAI 2022, pp. 234–243. Springer, Cham (2022). https://doi. org/10.1007/978-3-031-16449-1 23 