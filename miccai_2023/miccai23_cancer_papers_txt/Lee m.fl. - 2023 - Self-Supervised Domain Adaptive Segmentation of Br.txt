Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning Kyungsu Lee1,HaeyunLee2 , Georges El Fakhri3 , Jonghye Woo3 , and Jae Youn Hwang1(B) 1 Department of Electrical Engineering and Computer Science, Daegu Gyeongbuk Institute of Science and Technology, Daegu 42988, South Korea {ks_lee,jyhwang}@dgist.ac.kr2 Production Engineering Research Team, Samsung SDI, Yongin 17084, South Korea 3 Gordon Center for Medical Imaging, Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA 02114, USA Abstract. Unsupervised domain adaptation (UDA) has become increasingly popular in imaging-based diagnosis due to the challenge of labeling a large number of datasets in target domains. Without labeled data, well-trained deep learning models in a source domain may not per-form well when applied to a target domain. UDA allows for the use of large-scale datasets from various domains for model deployment, but it can face diﬃculties in performing adaptive feature extraction when dealing with unlabeled data in an unseen target domain. To address this, we propose an advanced test-time ﬁne-tuning UDA framework designed to better utilize the latent features of datasets in the unseen target domain by ﬁne-tuning the model itself during diagnosis. Our proposed framework is based on an auto-encoder-based network architecture that ﬁne-tunes the model itself. This allows our framework to learn knowledge speciﬁc to the unseen target domain during the ﬁne-tuning phase. In order to further optimize our framework for the unseen target domain, we introduce a re-initialization module that injects randomness into network parameters. This helps the framework to converge to a local minimum that is better-suited for the target domain, allowing for improved performance in domain adaptation tasks. To evaluate our framework, we carried out experiments on UDA segmentation tasks using breast cancer datasets acquired from multiple domains. Our experimental results demonstrated that our framework achieved state-of-the-art performance, outperforming other competing UDA models, in segmenting breast cancer on ultrasound images from an unseen domain, which supports its clinical potential for improving breast cancer diagnosis. Keywords: Unsupervised Domain Adaptation · Test-Time Tuning · Breast Cancer · Segmentation · Ultrasound Imaging Supplementary Information The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_52. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 H. Greenspan et al. (Eds.): MICCAI 2023, LNCS 14220, pp. 539–550, 2023. https://doi.org/10.1007/978-3-031-43907-0_52 1 Introduction In recent years, deep learning (DL) methods have demonstrated remarkable performance in detecting and localizing tumors on ultrasound images [2,27]. Compared with conventional image processing methods, DL methods provide an accurate feature extraction capability on ultrasound images, despite their low resolution and noise disturbance, leading to superior segmentation accuracy [2,5,14]. However, there are some limitations in developing a DL model in a source domain and deploying it in an unseen target domain. The primary limitation is that DL models require a large number of training samples to achieve accurate predictions [8,24]. Yet, acquiring large training datasets and their corresponding labels, especially from a cohort of patients, can be costly or even infeasible, which poses a signiﬁcant challenge in developing a DL model with high performance [7]. Second, even when large-scale datasets are available through collaborative research from multiple sites, DL models trained on such datasets may yield sub-optimal solutions due to domain gaps caused by diﬀerences in images acquired from diﬀerent sites [20]. Third, due to the small number of datasets from each domain, the images for each individual domain may not capture representative features, limiting the ability of DL models to generalize across domains [3]. Domain adaptation (DA) has been extensively studied to alleviate the aforementioned limitations, the goal of which is to reduce the domain gap caused by the diversity of datasets from diﬀerent domains [12,20,26,29,33]. Example solutions include transfer learning-and style transfer-based methods. Nonetheless, unlike natural images, generating labels can be a challenging task, making it difﬁcult to apply general DA methods; thus bridging domain gaps by DA methods remains limited [26,33]. This is due to sensitive privacy issues in patients’ data, particularly in collaborative research, which restricts access to labels from diﬀerent domains. As a result, conventional DA methods cannot be easily applied [10]. More recently, unsupervised domain adaptation (UDA) has been introduced to address this issue [16,33], aiming to generate semi-predictions (pseudo-labels) in target domains ﬁrst, followed by producing accurate predictions using the pseudo-labels. One critical limitation of pseudo-label-based UDA is the possibility of error accumulation due to mispredicted pseudo-labels. This can lead to signiﬁcant degradation of the performance of DL models, as errors can compound and become more pronounced over time [17,25]. To alleviate the problem of pseudo-label-based UDA, in this work, we propose an advanced UDA framework based on self-supervised DA with a test-time ﬁne-tuning network. Test-time adaptation methods have been developed [4,11,13,23] to improve the learning of knowledge in target domains. The distinctive feature of our test-time self-supervised DA is that it enables the DL network (i) to learn knowledge about the features of target domains by ﬁne-tuning the network itself during the test-time phase, rather than generating pseudo-labels and then (ii) to provide precise predictions on images in target domains, by using the ﬁne-tuned network. Speciﬁcally, we adopt self-supervised learning and verify the model via thorough mathematical analysis. Our framework was tested on the task of breast cancer segmentation in ultrasound images, but it could also be applied to other lesion segmentation tasks. To summarize, our contributions are three-fold: • We design a self-supervised DA framework that includes a parameter search method and provide a mathematical justiﬁcation for it. With our framework, we are able to identify the best-performing parameters that result in improved performance in DA tasks. • Our framework is eﬀective at preserving privacy, since it carries out DA using only pre-trained network parameters, without transferring any patient data. • We applied our framework to the task of segmenting breast cancer from ultrasound imaging data, demonstrating its superior performance over competing UDA methods. Our results indicate that our framework is eﬀective in improving the accuracy of breast cancer segmentation from ultrasound images, which could have potential implications for improving the diagnosis and treatment of breast cancer. Methodology Algorithm 1: Test-Time Fine-Tuning Scheme Input: E, H, C,and Dgen = Dseg 1: def Training_on_Source: 2: Sample batches of (s, s¯) ∼S 3: Update E and Dseg via LBCE((H ◦ Dseg ◦ E)(s),s¯)) Update E and Dgen via LGAN((Dgen ◦ E)(s),s) 4: return ES and DS = DS seg gen 5: End 6: def Fine_Tuning_on_Target: Sample batches of (t, ?) ∼T 7: Update DS via LGAN(DS (E(t)),t), then DS→T gen gen gen 8: Share parameters from DS→T gen to DFT DS→T 9: return DFT = seg 10: End 11: def Prediction_on_Target: 12: Sample batches of (t, ?) ∼T &#2;&#3; 13: tˆ= H ◦ (DS (t))seg ⊕ DFT) ◦ E 14: return yˆ 15: End Output: Predictions (yˆ)on T Fig. 1. Architecture of our TTFT network (Left) and its pipeline (Right). 2.1 Test-Time Fine-Tuning (TTFT) Network and Its Pipeline Network Architecture. Our proposed TTFT network is based on selfsupervised DA [31], which is a part of UDA and can be seen as multi-task learning, involving both the main and pretext tasks, as shown in Fig.1.In the main task, an encoder (E), a decoder for segmentation (Dseg), and a segmentation header (H) are included. The main task is the segmentation task, (H◦Dseg◦E)(x). In predicting segmentation labels in the target domain (T ), DFT is also involved in the main task, and the ﬁnal prediction after the ﬁne-tuning is &#2;&#3; provided by H ◦(Dseg ⊕DFT) ◦E (x),where ⊕is the concatenation operation. In the pretext task, E, a decoder for a generator, Dgen, and a discriminator, C, are involved. The pretext task aims to generate synthetic images, (Dgen ◦E)(t). Note that Dgen and Dseg share the same parameters to enable knowledge transfer. However, since the headers of image reconstruction and generating segmentation mask are diﬀerent (diﬀerent output), a new header incorporating DFT and Dseg is devised and leverages the outputs of two decoders. Besides, Dgen = DFT is ﬁne-tuned during the ﬁne-tuning step, and the DFT learns the knowledge of the input domain via image reconstruction. Two distinct knowledge (information) from Dseg and DFT enable the network to utilize target domain knowledge and predict precise predictions. Pre-training in Source Domain. The model M is ﬁrst trained in S in a supervised manner with (s, s¯) ∼Sin both main and pretext tasks as below: &#5;&#8; S ,Θm &#4; &#6; &#7;&#6;&#7; LBCE (H ◦ Dseg ◦ E)(s),s¯+ LGAN (Dgen ◦ E)(s),s , (1)pΘ = argmin S pmθ ,θsS S where LBCE and LGAN represent the loss functions for binary cross-entropy and generative adversarial network [6], respectively. Θm includes ES , DS ,and HS ,S seg while Θp includes ES , DS ,and CS . Additionally, DS = DS S gen seg gen. Fine-Tuning in Target Domain. Since the pre-trained model is likely to produce imprecise predictions in T, the model should learn domain knowledge about T. To this end, in the pretext task, for self-supervised learning, the model is ﬁne-tuned in T to generate synthetic images identical to the input images as below: , (2)&#4; SS S S→T &#6; &#7; ppLGAN (D ◦ E )(t),t ⇒ ⊇ E ∪ DΘ = argmin ΘT Tgen genpθtT where only Dgen is ﬁne-tuned to achieve memory eﬃciency and to decrease the ﬁne-tuning time, and DS is ﬁne-tuned as DS→T . Then, DS→T is transferred to gen gen gen DFT, and knowledge distillation via self-supervised learning is realized. Hence, &#2;&#3; the precise predictions in T could be provided by H ◦(DS ⊕DT (x).seg FT) ◦E Beneﬁts of Our Dual-Pipeline. Due to the symmetric property of mutual information in information entropy (H), we have I(X;Y )= H(X)+H(Y )− H(X, Y ). As a result, the predictions made by the ﬁne-tuned network in the target domain (T) lead to reduced entropy, as shown below: &#6; &#7;&#6;&#7;&#6;&#7;ST ST¯ ¯¯H (H ◦ (Dseg ⊕ DFT) ◦ E)(t),t ≤ H (H ◦ Dseg ◦ E)(t),t + H (H ◦ DFT ◦ E)(t),t . (3) Since DS is fully optimized for S in a supervised manner, it guarantees aseg baseline segmentation performance. Furthermore, since DT FT is ﬁne-tuned in T Fig. 2. Illustration of the local minimum of the source (a) and target (b) domains and parameter ﬂuctuation (c) using knowledge distillation, it can provide domain-speciﬁc information for T . As a result, the predictions made by the ﬁne-tuned model in T are jointly con-STstrained by the expectations of Dseg and DFT. This enables the ﬁnal model to provide precise predictions in T by taking into account both the source domain and target domain information. 2.2 Parameter Fluctuation: Parameter Randomization Method Since the loss function and its values can vary based on the distribution of inputs, and diﬀerent domains can have diﬀerent distributions, the local minimum identiﬁed in the source domain (S) cannot be considered as the same local minimum &#4;1in T , as illustrated in Fig.2. The y-axis of Fig.2 indicates L(M(x;θ),x¯),|X| x and the local minimum is diﬀerent in S and T as ΘS in Fig.2aand ΘT in Fig.2c, respectively. A longer ﬁne-tuning time is required to re-position ΘS to ΘT as in Fig. 2c than to re-position θT to ΘT . Therefore, eﬃcient ﬁne-tuning is necessary to re-position the local minimum in Fig. 2b and this process is known as parameter ﬂuctuation. Note that the parameter ﬂuctuation is followed by the ﬁne-tuning step. thSuppose Ci be the i convolution operator in Dseg with weight wi,then STCi(x)=wi · x.Since Dseg provides the baseline segmentation performance, DFT should provide similar feature maps to achieve the baseline performance. To this end, the mid-feature maps generated should be similar, i.e., ∀iCi(Fi)≈ C(F ),ii Twhere Ci represents the convolution in DFT, Fi represents ith feature map, and F0 = E(x). Suppose ∀i|Ci(Fi)− C(F )| <i &#7; 1, such that ∀iFi ≈ F byii i &#4; mathematical induction. Therefore, the sum of errors ( |Ci(Fi)− C(F )|)is &#4; ii approximated by |wiF0− wiF0| iﬀ ∀iFi ≈ Fi, which can be expressed as: &#4; &#4;&#4; (4)|wiF0 − w F0| <&#3; 1⇐|wiF0 − w F0|≈ 0⇔|wi − w | =0.i i i Here, we denote wi − wi =fi as the ﬂuctuation vector in the vector space, and &#4; the condition fi =0indicates that the sum of the ﬂuctuation vectors should be zero under the condition of |fi| <r&#7; 1. Hence, we achieve the condition for the parameter ﬂuctuation that the centers of parameters of ΘS and θT should be the same in the vector space, and the length of the ﬂuctuation vector should be less than a certain small threshold (0 <r&#7; 1). Therefore, the parameter ﬂuctuation aims to add random vectors of which length is less than 0 <r&#7; 1 on the parameters of ΘS, and the sum of vectors should be zero. To summarize, the parameter ﬂuctuation aims to add randomness on ΘS as follows: &#4; θT ={wi +fi| wi ∈ ΘS ,fi =0, 0<|fi| <r 1}. (5) 3 Experiments 3.1 Experimental Set-Ups To evaluate the segmentation performance of our TTFT framework, we used three diﬀerent ultrasound databases: BUS [32], BUSI [1], and BUV [18], which are considered to be diﬀerent domains. All three databases contain ultrasound imaging data and segmentation masks for breast cancer, with the masks labeled as 0 (background) and 1 (lesion) using a one-hot encoding. The BUS database consists of 163 images along with corresponding labels. The BUSI database contains 780 images, with 133 images belonging to the NORMAL class and having labels containing only 0 values. The BUV database originally consists of ultrasound videos, providing a total of 21,702 frames. While the database also provides labels for the detection task, we processed these labels as segmentation masks using a region growing method [15]. We employed diﬀerent deep-learning models for evaluation. Speciﬁcally, U-Net [22]and FusionNet[21] were employed as our baseline models, since U-Net is a widely used basic model for segmentation, and FusionNet contains advanced residual modules, compared with U-Net. Ours I and Ours II were based on U-Net and FusionNet as the baseline network, respectively. Additionally, MIB-Net [28], which is a state-of-the-art model for breast cancer segmentation using ultrasound images, was employed for comparison. Furthermore, CBST [33]and CT-Net [16] were employed as the comparison models for UDA methods. As the evaluation metrics, dice coeﬃcient (D. Coef), PRAUC, which is an area under a precision-recall curve, and cohen kappa (κ)wereemployed[30]. Our experimental set-ups included: (i) individual databases were used to assess the baseline segmentation performance (Appendix); (ii) the domain adaptive segmentation performance was assessed using the three databases, where two databases were regarded as the source domain, and the remaining database was regarded as the target domain; and (iii) the ablation study was carried out to evaluate the proposed network architecture along with the randomized re-initialization method. 3.2 Comparison Analysis Since all compared DL models show similar D. Coef, only UDA performance is comparable as a control in our experiments. In this experiment, two databases were used for training, and the remaining database was used for testing. For instance, BUS in Fig. 3 illustrates the BUSdatabase was used for testing, and Fig. 3. Comparison analysis of our framework and comparison models: performance comparison table (Left) and Box-and-Whisker plot (Right). Fig. 4. Precision-Recall curves by ours and comparison models on each database. Area under the precision-recall curve (PR-AUC) values were reported. the other two databases of BUSI and BUV were used for training. Figs. 3 and 4 show quantitative results, and Fig.5 shows the sample segmentation results. Unlike the experiment using the individual database, U-Net, FusionNet, and MIB-Net showed signiﬁcantly inferior scores due to domain gaps. In contrast, UDA methods of CBST and CT-Net showed superior scores, compared with others, and the scores were not strongly reduced, compared with the experiment with the single database. Note that, our TTFT framework achieved the best performance compared with other DL models. Additionally, Ours II, based on FusionNet, showed the best scores, potentially due to the advanced residual connection module. Furthermore, as illustrated in Fig 4, our framework provides superior precision scores in a long range of (0, 0.7), indicating that our frameworks estimated unnecessary mispredictions but precise predictions on cancer. Fig. 5. Segmentation results by ours and comparison models on each database. Fig. 6. Illustration of feature maps: style loss comparison (Left) and a T-SNE plot of generated images by diﬀerent decoders (Right) 3.3 Ablation Study In order to assess the eﬀectiveness of each of the proposed modules, including the parameter ﬂuctuation and ﬁne-tuning methods, the ablation study was carried out. Since our framework contains three types of decoders, including DS seg,and DS→T for the ﬁne-tuning, we mainly targeted those decoders seg, Dfl seg in our ablation study. Table 1 illustrates the quantitative results by diﬀerent types of decoders. The higher D. coef value (+3.4%) of Pre-train + PF than that of Pre-train + Random Init and Pre-train + Oﬀset conﬁrms the eﬀectiveness of the parameter ﬂuctuation in the UDA performance. Additionally, the higher score (+11%) of Fine-tuning than Pre-train shows an outstanding UDA performance of the ﬁne-tuning pipeline. Furthermore, the simultaneous utilization of the dual pipeline with DS and DS→T is justiﬁed by the scores of seg seg Pre-train + Fine-tuning. Using dual-pipeline and parameter ﬂuctuation yielded the best performance. However, the utilization of ensemble pipelines of multiple ﬁne-tuning modules was ineﬃcient, since negligible performance improvements (+0.002) were observed, despite the heavy memory utilization. Furthermore, Fig.6 shows the eﬀectiveness of the parameter ﬂuctuation and ﬁne-tuning methods. We ﬁrst compared the similarity of feature-maps by decoders, including DS ,and DS→T and DT,with DS seg, which was fully seg, Dseg ﬂ seg seg optimized decoder in T . Here, a style loss [9] was employed to measure the simi→ Dﬂ → DS→T larity of feature maps. Our framework was ﬁne-tuned as DS seg seg seg Table 1. Dice coeﬃcients by diﬀerent versions of our TTFT framework. Random Init is DFT is randomly initialized, and Oﬀset indicates DFT is initialized with the value of Dseg added by the oﬀset value. D. Coef (95% CI)  BUS  BUSI  BUV  Pre-train  0.664 (0.653–0.675)  0.664 (0.653–0.675)  0.664 (0.653–0.675)  Fine-tuning  0.774 (0.763–0.785)  0.774 (0.763–0.785)  0.774 (0.763–0.785)  Pre-train + Random Init  0.663 (0.653–0.673)  0.663 (0.653–0.673)  0.663 (0.653–0.673)  Pre-train + Oﬀset  0.676 (0.668–0.684)  0.676 (0.668–0.684)  0.676 (0.668–0.684)  Pre-train + PF  0.697 (0.686–0.707)  0.697 (0.686–0.707)  0.697 (0.686–0.707)  Pre-train + Fine-tuning  0.799 (0.789–0.809)  0.799 (0.789–0.809)  0.799 (0.789–0.809)  Pre-train + PF + Fine-tuning Pre-train + PF + N Fine-tuning  0.855 (0.844–0.866) 0.857 (0.842–0.872)  0.855 (0.844–0.866) 0.857 (0.842–0.872)  0.855 (0.844–0.866) 0.857 (0.842–0.872)  along which the similarity with DT of those decoders were increasing, and seg the feature-maps by DS→T were similar to those of DT seg, compared with DS seg seg, indicating UDA was successfully performed. Additionally, the generated images by decoders, including DS seg,and DS→T in S and T are plotted with Tseg, Dﬂ seg SNE, where the short distance represents the similar features [19]. The generated images became similar to T in order of DS seg,and DS→T , which conﬁrmed seg, Dﬂ seg the eﬀectiveness of the ﬁne-tuning method in terms of knowledge distillation. Additionally, the parameters were successfully re-positioned from the local minimum in S by parameter ﬂuctuation, which was conﬁrmed by the distances from S to DS and Dﬂ gen gen. Discussion and Conclusion In this work, we proposed a DL-based segmentation framework for multi-domain breast cancer segmentation on ultrasound images. Due to the low resolution of ultrasound images, manual segmentation of breast cancer is challenging even for expert clinicians, resulting in a sparse number of labeled data. To address this issue, we introduced a novel self-supervised DA network for breast cancer segmentation in ultrasound images. In particular, we proposed a test-time ﬁne-tuning network to learn domain-speciﬁc knowledge via knowledge distillation by self-supervised learning. Since UDA is susceptible to error accumulation due to imprecise pseudo-labels, which can lead to degraded performance, we employed a self-supervised learning-based pretext task. Speciﬁcally, we utilized an autoencoder-based network architecture to generate synthetic images that matched the input images. Moreover, we introduced a randomized re-initialization module that injects randomness into network parameters to reposition the network from the local minimum in the source domain to a local minimum that is better suited for the target domain. This approach enabled our framework to eﬃciently ﬁne-tune the network in the target domain and achieve better segmentation performance. Experimental results, carried out with three ultrasound databases from diﬀerent domains, demonstrated the superior segmentation performance of our framework over other competing methods. Additionally, our framework is well-suited to a scenario in which access to source domain data is limited, due to data privacy protocols. It is worth noting that we used vanilla U-Net [22] and FusionNet [21] as baseline models to evaluate the basic performance of our TTFT framework. However, the use of more advanced baseline models could lead to even better segmentation performance, which is a subject for our future work. Moreover, our proposed framework is not limited to breast cancer segmentation on ultrasound images acquired from diﬀerent domains. It can also be applied to other disease groups or imaging modalities such as MRI or CT. Acknowledgements. This work was partially supported by the Korea Medical Device Development Fund grant funded by the Korea government (the Ministry of Science and ICT, the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, the Ministry of Food and Drug Safety) (Project Number: 1711174564, RS-202200141185). Also, this work was partially supported by the Technology Innovation Program(20014214) funded By the Ministry of Trade, Industry & Energy(MOTIE, Korea). References 1. Al-Dhabyani, W., Gomaa, M., Khaled, H., Fahmy, A.: Dataset of breast ultrasound images. Data Brief 28, 104863 (2020) 2. Badawy, S.M., Mohamed, A.E.N.A., Hefnawy, A.A., Zidan, H.E., GadAllah, M.T., El-Banby, G.M.: Automatic semantic segmentation of breast tumors in ultrasound images based on combining fuzzy logic and deep learning-a feasibility study. PLoS ONE 16(5), e0251899 (2021) 3. Barbato, F., Toldo, M., Michieli, U., Zanuttigh, P.: Latent space regularization for unsupervised domain adaptation in semantic segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2835–2845 (2021) 4. Bateson, M., Kervadec, H., Dolz, J., Lombaert, H., Ayed, I.B.: Source-free domain adaptation for image segmentation. Med. Image Anal. 82, 102617 (2022) 5. van Beers, F., Lindström, A., Okafor, E., Wiering, M.A.: Deep neural networks with intersection over union loss for binary image segmentation. In: ICPRAM, pp. 438–445 (2019) 6. Goodfellow, I., et al.: Generative adversarial nets. In: Advances in Neural Information Processing Systems, pp. 2672–2680 (2014) 7. Guan, H., Liu, M.: Domain adaptation for medical image analysis: a survey. IEEE Trans. Biomed. Eng. 69(3), 1173–1185 (2021) 8. Ioﬀe, S., Szegedy, C.: Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015) 9. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). https://doi.org/10. 1007/978-3-319-46475-6_43 10. Kaissis, G.A., Makowski, M.R., Rückert, D., Braren, R.F.: Secure, privacypreserving and federated machine learning in medical imaging. Nat. Mach. Intell. 2(6), 305–311 (2020) 11. Karani, N., Erdil, E., Chaitanya, K., Konukoglu, E.: Test-time adaptable neural networks for robust medical image segmentation. Med. Image Anal. 68, 101907 (2021) 12. Kouw, W.M., Loog, M.: An introduction to domain adaptation and transfer learning. arXiv preprint arXiv:1812.11806 (2018) 13. Kundu, J.N., Kulkarni, A., Singh, A., Jampani, V., Babu, R.V.: Generalize then adapt: source-free domain adaptive semantic segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7046–7056 (2021) 14. Lee, H., Park, J., Hwang, J.Y.: Channel attention module with multi-scale grid average pooling for breast cancer segmentation in an ultrasound image. Ferroelectrics, and Frequency Control, IEEE Transactions on Ultrasonics (2020) 15. Lee, M.H., Kim, J.Y., Lee, K., Choi, C.H., Hwang, J.Y.: Wide-ﬁeld 3D ultrasound imaging platform with a semi-automatic 3D segmentation algorithm for quantitative analysis of rotator cuﬀ tears. IEEE Access 8, 65472–65487 (2020) 16. Lee, S., Hyun, J., Seong, H., Kim, E.: Unsupervised domain adaptation for semantic segmentation by content transfer. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 35, pp. 8306–8315 (2021) 17. Liang, J., He, R., Sun, Z., Tan, T.: Exploring uncertainty in pseudo-label guided unsupervised domain adaptation. Pattern Recogn. 96, 106996 (2019) 18. Lin, Z., Lin, J., Zhu, L., Fu, H., Qin, J., Wang, L.: A new dataset and a baseline model for breast lesion detection in ultrasound videos. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 614–623. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-16437-8_59 19. Van der Maaten, L., Hinton, G.: Visualizing data using t-SNE. J. Mach. Learn. Res. 9(11), 2579–2605 (2008) 20. Nam, H., Lee, H., Park, J., Yoon, W., Yoo, D.: Reducing domain gap by reducing style bias. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8690–8699 (2021) 21. Quan, T.M., Hildebrand, D.G., Jeong, W.K.: FusionNet: a deep fully residual convolutional neural network for image segmentation in connectomics. arXiv preprint arXiv:1612.05360 (2016) 22. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomedical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F. (eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-24574-4_28 23. Roy, S., Trapp, M., Pilzer, A., Kannala, J., Sebe, N., Ricci, E., Solin, A.: Uncertainty-guided source-free domain adaptation. In: Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXV. pp. 537–555. Springer, Cham (2022). https://doi.org/10.1007/ 978-3-031-19806-9_31 24. Ruder, S.: An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747 (2016) 25. Sun, Y., Tzeng, E., Darrell, T., Efros, A.A.: Unsupervised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825 (2019) 26. Toldo, M., Maracani, A., Michieli, U., Zanuttigh, P.: Unsupervised domain adaptation in semantic segmentation: a review. Technologies 8(2), 35 (2020) 27. Vakanski, A., Xian, M., Freer, P.E.: Attention-enriched deep learning model for breast tumor segmentation in ultrasound images. Ultrasound Med. Biol. 46(10), 2819–2833 (2020) 28. Wang, J., et al.: Information bottleneck-based interpretable multitask network for breast cancer classiﬁcation and segmentation. Med. Image Anal., 102687 (2022) 29. Wang, Q., Fink, O., Van Gool, L., Dai, D.: Continual test-time domain adaptation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211 (2022) 30. Wang, Y., Yao, Y.: Breast lesion detection using an anchor-free network from ultrasound images with segmentation-based enhancement. Sci. Rep. 12(1), 1–12 (2022) 31. Xu, J., Xiao, L., López, A.M.: Self-supervised domain adaptation for computer vision tasks. IEEE Access 7, 156694–156706 (2019) 32. Yap, M.H., et al.: Automated breast ultrasound lesions detection using convolutional neural networks. IEEE J. Biomed. Health Inform. 22(4), 1218–1226 (2017) 33. Zou, Y., Yu, Z., Kumar, B., Wang, J.: Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 289–305 (2018) 