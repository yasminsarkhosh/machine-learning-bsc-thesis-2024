<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data</title>
				<funder ref="#_FkktbER #_PTRJkqE #_bETTjnZ">
					<orgName type="full">National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Deeksha</forename><forename type="middle">M</forename><surname>Shama</surname></persName>
							<email>dshama1@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiasen</forename><surname>Jing</surname></persName>
							<email>jjing2@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Archana</forename><surname>Venkataraman</surname></persName>
							<email>archanav@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A5808CD404FD406AC76AAFCD0E6137E8</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_18</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Epilepsy</term>
					<term>EEG</term>
					<term>Multi-instance learning</term>
					<term>Trustworthy AI</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a robust deep learning framework to simultaneously detect and localize seizure activity from multichannel scalp EEG. Our model, called DeepSOZ, consists of a transformer encoder to generate global and channel-wise encodings. The global branch is combined with an LSTM for temporal seizure detection. In parallel, we employ attention-weighted multi-instance pooling of channel-wise encodings to predict the seizure onset zone. DeepSOZ is trained in a supervised fashion and generates high-resolution predictions on the order of each second (temporal) and EEG channel (spatial). We validate DeepSOZ via bootstrapped nested cross-validation on a large dataset of 120 patients curated from the Temple University Hospital corpus. As compared to baseline approaches, DeepSOZ provides robust overall performance in our multi-task learning setup. We also evaluate the intra-seizure and intra-patient consistency of DeepSOZ as a first step to establishing its trustworthiness for integration into the clinical workflow for epilepsy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Epilepsy is a debilitating neurological disorder characterized by spontaneous and recurring seizures <ref type="bibr" target="#b16">[17]</ref>. Roughly 30% of epilepsy patients are drug resistant, meaning they do not positively respond to anti-seizure medications. In such cases, the best alternative treatment is to identify and surgically resect the brain region responsible for triggering the seizures, i.e., the seizure onset zone (SOZ). Scalp electroencephalography (EEG) is the first and foremost modality used to monitor epileptic activity. However, seizure detection and SOZ localization from scalp EEG are based on expert visual inspection, which is time consuming and heavily prone to the subjective biases of the clinicians <ref type="bibr" target="#b7">[8]</ref>.</p><p>Computer-aided tools for scalp EEG almost exclusively focus on the task of (temporal) seizure detection. Early works approached the problem via feature engineering and explored spectral <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, entropy-based <ref type="bibr" target="#b8">[9]</ref>, and graphtheoretic <ref type="bibr" target="#b0">[1]</ref> features for the task. In general, these methods extract features from short time windows and use a machine learning classifier to discriminate between window-wise seizure and baseline activity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref>. More recently, deep learning models have shown promise in extracting generalizable information from noisy and heterogeneous datasets. Deep learning applications to EEG include convolutional neural networks (CNNs) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, graph convolutional networks(GCNs) <ref type="bibr" target="#b20">[21]</ref>, and a combination of attention-based feature extraction <ref type="bibr" target="#b9">[10]</ref> and recurrent layers to capture evolving dynamics <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. Transformers have also been used for seizure detection, both in combination with CNNs <ref type="bibr" target="#b13">[14]</ref> and directly on the EEG signals and their derived features <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>. While these methods have greatly advanced the problem of seizure detection, they provide little information about the SOZ, which is ultimately the more important clinical question.</p><p>A few works have explored the difficult task of localizing the SOZ via post hoc evaluations of deep networks trained for seizure detection. For example, the authors of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref> perform a cross-channel connectivity analysis of the learned representations to determine the SOZ. In contrast, the method of <ref type="bibr" target="#b1">[2]</ref> identifies the SOZ by dropping out nodes of the trained GCN until the seizure detection performance degrades below a threshold. Finally, the SZTrack model of <ref type="bibr" target="#b5">[6]</ref> jointly detects and tracks the spatio-temporal seizure spread by aggregating channelwise detectors; the predictions of this model are seen to correlate with the SOZ. While valuable, the post hoc nature of these unsupervised analyses means that the results may not generalize to unseen patients. The first supervised approach for SOZ localization was proposed by <ref type="bibr" target="#b2">[3]</ref> and uses probabilistic graphical models for simultaneous detection and localization. The more recent SZLoc model <ref type="bibr" target="#b4">[5]</ref> proposes an end-to-end deep architecture for SOZ localization along with a set of novel loss functions to weakly supervise the localization task from coarse inexact labels. While these two methods represent seminal contributions to the field, they are difficult to train and only report the localization performance on short (i.e., &lt; 2 min) EEG recordings around the time of seizure onset.</p><p>In this paper, we present DeepSOZ, a robust model for joint seizure detection and SOZ localization from multichannel scalp EEG. Our model consists of a spatial transformer encoder to combine cross-channel information and LSTM layers to capture dynamic activity for window-wise seizure detection. In parallel, we use a novel attention-weighted multi-instance pooling to supervise seizure-level SOZ localization at the single channel resolution. We curate a large evaluation dataset from the publicly available TUH seizure corpus by creating SOZ labels from the clinician notes for each patient. We perform extensive window-level, seizure-level, and patient-level evaluations of our model. Additionally, we analyze the consistency of predictions across seizure occurrences, which has not previously been reported for SOZ localization. Quantifying the error variance is the first step in establishing trust in DeepSOZ for clinical translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates our DeepSOZ architecture. The inputs to DeepSOZ are multichannel EEG data for a single seizure recording segmented into one-second windows. The outputs are a temporal sequence of predicted seizure versus baseline activity (detection) and a channel-wise posterior distribution for the SOZ (localization). Formally, let x t i denote the EEG data for channel i and time window t. Clinical EEG is recorded in the 10-20 system, which consists of 19 channels distributed across the scalp. For training, let S t ∈ {0, 1} denote the seizure versus baseline activity label for time window t, and let y ∈ {0, 1} 19×1 be a vector representing the clinician annotated SOZ. Below, we describe each component of DeepSOZ, along with our training and validation strategy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The DeepSOZ Model</head><formula xml:id="formula_0">Z t = LN (MHA( Xt ) + Xt ) h t i = LN (F F (z t i ) + z t i ),<label>(1)</label></formula><p>where LN (•) denotes layer normalization, and F F (•) represents a learned two layer feed-forward network with ReLU activation.</p><p>The MHA(•) operation uses parallel self attentions to map the input data into a set of projections, as guided by the other channels in the montage. Formally, let n index the attention head. The attention weights A t n ∈ R 20×20 captures global (1) and cross-channel <ref type="bibr" target="#b18">(19)</ref> similarities via the key matrix</p><formula xml:id="formula_1">K t n = W K n Xt and query matrix Q t n = W Q n</formula><p>Xt as follows:</p><formula xml:id="formula_2">A t n = ξ Q t n K tT n √ d , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where ξ(•) represents the softmax function, and d is our model dimension. The attention A t n is multiplied by the value matrix</p><formula xml:id="formula_4">V t n = W V n</formula><p>Xt to generate the output for head n. These outputs are concatenated and fed into a linear layer to produce MHA(•). Finally, these MHA outputs are passed into a two layer feed forward neural network with ReLU activation, post residual connections and layer normalization to generate the hidden encoding H t ∈ R 20×200 .</p><p>The matrices W Q n , W K n , and W V n are trained parameters of the encoder. For simplicity, we set the model dimension d to be the same as our input x t i (d = 200 in this work), and we specify 8 attention heads in the MHA operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM for Temporal Seizure Detection:</head><p>We use a bidirectional LSTM to capture evolving patterns in the global encodings of the one-second EEG windows, i.e., {h t 0 } T t=1 . We use a single LSTM layer with 100 hidden units to process the global encodings and capture both long-term and short-term dependencies. The output of the LSTM is passed into a linear layer, followed by a softmax function, to generate window-wise predictions Ŝt ∈ [0, 1]. Here, Ŝt represents the posterior probability of seizure versus baseline activity at window t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention-Weighted Multi-Instance Pooling for SOZ Localization:</head><p>We treat the localization task as a multi-instance learning problem to predict a channel-wise posterior distribution for the SOZ vector {y i } 19  i=1 by computing a weighted average of the hidden representations from the transformer. We first map the channel-wise encodings {h t i } 19 i=1 ∈ R 200 to scalars ŷt i using the same linear layer across channels. We use the predicted seizure probability Ŝt as our attention to compute the final SOZ prediction as follows:</p><formula xml:id="formula_5">ŷi = σ T t=1 Ŝt • ŷt i T t=1 Ŝt , i = 1, . . . , 19<label>(3)</label></formula><p>where σ(•) is the sigmoid function. The final patient-level predictions are obtained by averaging ŷi across all seizure recordings for that patient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Loss Function and Model Training</head><p>We train DeepSOZ in two stages. First, the transformer and LSTM layers are trained for window-wise seizure detection using weighted cross entropy loss:</p><formula xml:id="formula_6">L det = - 1 T T t=1 (1 -δ)(1 -S t ) log(1 -Ŝt ) + δ • S t log Ŝt , (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where the weight δ = 0.8 is fixed based on the ratio of non-seizure to seizure activity in the dataset. DeepSOZ is then finetuned for SOZ localization. To avoid catastrophic forgetting of the detection task, we freeze the LSTM layers and provide a weak supervision for detection via the loss function:</p><formula xml:id="formula_8">L soz = 0.1 • L det - 1<label>19</label></formula><formula xml:id="formula_9">19 i=1 ((1 -y i ) log(1 -ŷi ) + y i log ŷi ) + ŷ 1 ,<label>(5)</label></formula><p>where the ||.|| 1 penalizes the L1 norm to encourage sparsity in predicted ŷ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Validation</head><p>We evaluate DeepSOZ using bootstrapped 5-fold nested cross validation. Within each training fold, we select the learning rate and seizure detection threshold through a grid search with a fixed dropout of 0.15. We use PyTorch v1.9.0 with Adam <ref type="bibr" target="#b12">[13]</ref> for training with a batch size of one patient; early stopping is implemented using a validation set drawn from the training data. We re-sample the original 5-fold split three times and report the results across all 15 models<ref type="foot" target="#foot_0">1</ref> .</p><p>Seizure Detection: At the window level, we report sensitivity, specificity, and area under the receiver operating characteristic curve (AU-ROC). At the seizure level, we adopt the strategy of <ref type="bibr" target="#b3">[4]</ref> and select a detection threshold that ensures no more than 2 min of false positive detections per hour in the validation dataset.</p><p>To eliminate spikes, we smooth the output predictions using a 30 s window and count only the contiguous intervals beyond the calibrated detection threshold as seizure predictions. Following the standard of <ref type="bibr" target="#b3">[4]</ref>, we do not penalize post-ictal predictions. We report the false positive rate (FPR) per hour (min/hour), the sensitivity, and the latency (seconds) in seizure detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Comparisons:</head><p>We compare the performance of DeepSOZ with one model ablation and four state-of-the-art methods from the literature. Our ablation replaces the attention-weighted multi-instance pooling in DeepSOZ with a standard maxpool operation within the prediction seizure window (DeepSOZmax). Our baselines consist of the CNN-BLSTM model for seizure detection developed by <ref type="bibr" target="#b3">[4]</ref>, the SZTrack model proposed by <ref type="bibr" target="#b5">[6]</ref> that uses a convolutionalrecurrent architecture for each channel, the SZLoc model by <ref type="bibr" target="#b4">[5]</ref> consisting of CNN-transformer-LSTM layers, and the Temporal Graph Convolutional Network (TGCN) developed by <ref type="bibr" target="#b1">[2]</ref>. SZTrack and SZLoc are trained and evaluated for localization via the approach published by the authors which uses only 45 s of data around onset time. We modify the TGCN slightly to extract channel-wise prediction for localization task but evaluate it on the full 10-minute recordings like DeepSOZ. Finally, we note that the CNN-BLSTM can only be used for seizure detection, and SZLoc is only trained for SOZ localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and Preprocessing:</head><p>We validate DeepSOZ on 642 EEG recordings from 120 adult epilepsy patients in the publicly available Temple University Hospital (TUH) corpus <ref type="bibr" target="#b18">[19]</ref> with a well characterized unifocal seizure onset. We use the clinical notes to localize the SOZ to a subset of the 19 EEG channels. Table <ref type="table" target="#tab_1">1</ref> describes the seizure characteristics across patients in our curated subset. Following <ref type="bibr" target="#b3">[4]</ref>, we re-sample the raw EEG to 200 Hz for uniformity, filter the signals between 1.6-30 Hz, and clip them at two standard deviations from mean to remove high intensity artifacts. All signals are normalized to have zero mean and unit variance. We standardize the input lengths by cropping the signals to 10 min around the seizure interval, while ensuring that the onset times are uniformly distributed within this period. We segment the EEG into one second non-overlapping windows to obtain the model inputs x t i .</p><p>Seizure Detection Performance: Table <ref type="table" target="#tab_2">2</ref> reports the seizure detection performance averaged over the 15 bootstrapped testing folds. At the window level, both aggregation strategies for DeepSOZ (weighted posterior and max pooling) perform similarly and achieve higher AU-ROC values than the other baselines.</p><p>The TGCN and CNN-BLSTM baselines achieve notably worse AU-ROC values, establishing the power of a transformer encoder in extracting more meaningful features. SZTrack is trained using the published strategy in <ref type="bibr" target="#b5">[6]</ref> and fails to detect seizures effectively. The differences in AU-ROC between DeepSOZ and TGCN, SZTrack, and CNN-BLSTM are statistically significant per a De Long's test at p &lt; 0.05. At the seizure level, DeepSOZ achieves a good balance between sensitivity (0.81) and FPR (0.44 min/h). The negative latency of 18 s contributes towards the slightly elevated FPR. The TGCN and SZTrack have a high sensitivity, which comes at the cost of much higher FPR, while the CNN-BLSTM has a low detection sensitivity but comparable FPR.</p><p>SOZ Localization Performance: Table <ref type="table" target="#tab_3">3</ref> summarizes the SOZ localization performance across models. DeepSOZ performs the best at both patient and seizure levels. In contrast, the SZTrack and TGCN baselines are confident in their predictions but more often incorrect, once again highlighting the value of a transformer encoder. While the SZLoc model performs the best of the baselines, we note that both it and SZTrack have an unfair advantage of being trained and evaluated on 45 s EEG recordings around the seizure onset time. In contrast, DeepSOZ processes full 10-minute recordings for both tasks. Figure <ref type="figure" target="#fig_1">2</ref> aggregates the final predictions of DeepSOZ across the 120 patients into quadrants. As seen, DeepSOZ is adept at differentiating right-and lefthemisphere onsets but struggles to differentiate anterior and posterior SOZs. We hypothesize that this trend is due to the skew towards temporal epilepsy patients in the TUH dataset. A similar trend can be observed at the finer lobewise predictions. Figure <ref type="figure" target="#fig_2">3</ref> illustrate sample DeepSOZ outputs for two patients in the testing fold. As seen, DeepSOZ accurately detects the seizure interval in all cases but has two false positive detections for Patient 1. Nonetheless, DeepSOZ correctly localizes the seizure to the left frontal area. The localization for Patient 2 is more varied, which correlates with the patient notes that specify a right-posterior onset but epileptogenic activity quickly spreading to the left hemisphere. Overall, DeepSOZ is more uncertain about this patient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have introduced DeepSOZ for joint seizure detection and SOZ localization from scalp EEG. DeepSOZ leverages a self-attention mechanism to generate informative global and channel-wise latent representations that strategically fuse multi-channel information. The subsequent recurrent layers and attentionweighted pooling allow DeepSOZ to generalize across a heterogeneous cohort. We validate DeepSOZ on data from 120 epilepsy patients and report improved detection and localization performance over numerous baselines. Finally, we quantify the prediction uncertainty as a first step towards building trust in the model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic of our DeepSOZ model. Left: Transformer encoder that uses positional encoding and self attention to generate hidden representations. Top Right: Bidirectional LSTM for seizure detection. Bottom Right: Attention weighted multiinstance pooling for SOZ localization.</figDesc><graphic coords="3,53,79,54,23,316,48,192,49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Confusion matrices between the max channel-wise posterior and the true SOZ. Left: Quadrant-based aggregation (L: Left, R: Right, Ant: Anterior, Post: Posterior). Right: Functional region-based aggregation (F: Frontal, FC: Frontocentral, FT: Frontotemporal, T: Temporal, C: Central, P: Parietal, O: Occipital).</figDesc><graphic coords="8,56,46,54,50,339,19,124,21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization for two testing patients. Top: Temporal seizure detection. Blue lines correspond to the DeepSOZ prediction; horizontal orange lines denote the seizure detection threshold from training; shaded region is the ground-truth seizure interval. Bottom: Predicted SOZ for the above seizure projected onto a topological scalp plot. Side: Patient-level SOZ with ground-truth below.</figDesc><graphic coords="9,70,80,54,32,282,67,227,38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Architecture Spatial Transformer Encoder:</head><label></label><figDesc>For each time window t, the multichannel EEG data {x t is the indicator function for a one hot encoding at element i.The hidden representations are computed by the transformer encoder from the modified multichannel input Xt = [x t</figDesc><table /><note><p>i } 19 i=1 is passed to a transformer encoder consisting of multi-head attention (MHA) layers to generate both a global h t 0 and channel-wise {h t i } 19 i=1 encodings. Since the spatial orientation of these channels is crucial for tracking seizure activity, we add a positional embedding generated by a trainable linear layer W p , resulting in the modified input xt i = x t i + W T p 1(i), where 1(i) 1 . . . xt 19 ] as follows:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Description of patient demographics in the curated TUH dataset.</figDesc><table><row><cell></cell><cell>curated TUH dataset</cell></row><row><cell>Number of patients</cell><cell>120</cell></row><row><cell>Male/Female</cell><cell>55/65</cell></row><row><cell>Average age</cell><cell>55.2 ± 16.6</cell></row><row><cell>Min/Max age</cell><cell>19/91</cell></row><row><cell>Seizures per patient</cell><cell>14.7 ± 25.2</cell></row><row><cell>Min/Max seizures per patient</cell><cell>1/152</cell></row><row><cell cols="2">Average EEG duration per patient 79.8 ± 135 min</cell></row><row><cell>Average seizure duration</cell><cell>88.0 ± 123.5 s</cell></row><row><cell>Min/Max seizure duration</cell><cell>7.5 ± 1121 s</cell></row><row><cell>Temporal/Extra-temporal Onset</cell><cell>72/48</cell></row><row><cell>Right/Left onset zone</cell><cell>59/61</cell></row></table><note><p>SOZ Localization: By construction, DeepSOZ processes each seizure recording separately to find the SOZ. Patient-level SOZ predictions are obtained by averaging across all seizure recordings for that patient. The SOZ is correctly localized if the maximum channel-wise probability lies in the neighborhood determined by the clinician. We quantify the prediction variance at the seizure level by generating Monte Carlo samples during test via active dropout. At the patient level, we compute the prediction variance across all seizures for that patient.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Temporal seizure detection performance on the TUH dataset. Window-level metrics are calculated for each one-second windows. Seizure-level metrics are aggregated over the duration of seizure after post-processing. .045 .464 ± .300 .535 ± .303 2.06 ± .844 .799 ± .135 -50.5 ± 71.5 CNN-BLSTM [4] .876 ± .044 .664 ± .135 .876 ± .055 .351 ± .45 .42 ± .281 28.89 ± 154.88</figDesc><table><row><cell>Model</cell><cell cols="2">Window-Level</cell><cell>Seizure-Level</cell></row><row><cell></cell><cell>AU-ROC</cell><cell cols="2">Sensitivity Specificity FPR</cell><cell>Sensitivity Latency</cell></row><row><cell>DeepSOZ</cell><cell cols="3">.901 ± .027 .679 ± .100 .890 ± .030 .44 ± .23</cell><cell>.808 ± .106 -18.45 ± 15.67</cell></row><row><cell>DeepSOZ-max</cell><cell cols="3">.907 ± .032 .676 ± .079 .909 ± .029 .288 ± .153 .700 ± .105 -15.39 ± 9.91</cell></row><row><cell>TGCN [2]</cell><cell cols="3">.887 ± .032 .711 ± .148 .835 ± .085 .808 ± .591 .869 ± .085 -36.01 ± 29.49</cell></row><row><cell>SZTrack [6]</cell><cell>.5202 ±</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>SOZ localization metrics. The seizure-level results are calculated independently on all seizure recordings. Patient-level results are aggregated over multiple seizures of the patients. Number of model parameters is also given.</figDesc><table><row><cell>Model</cell><cell>Seizure-Level</cell><cell>Patient-Level</cell><cell># Params</cell></row><row><cell></cell><cell cols="2">Accuracy Uncertainty Accuracy Uncertainty</cell><cell></cell></row><row><cell>DeepSOZ</cell><cell cols="3">.731 ± .061 .009 ± .001 .744 ± .058 .142 ± .013 510K</cell></row><row><cell cols="2">DeepSOZ-max .513 ± .154 .0 ± .0</cell><cell cols="2">.411 ± .076 .023 ± .007 510K</cell></row><row><cell>TGCN [2]</cell><cell>.479 ± .07 .0 ± .0</cell><cell cols="2">.486 ± .123 .153 ± .015 1.16M</cell></row><row><cell>SZTrack [6]</cell><cell cols="3">.454 ± .065 .003 ± .001 .450 ± .142 .017 ± .007 19K</cell></row><row><cell>SZLoc [5]</cell><cell cols="3">.682 ± .094 .008 ± .001 .740 ± .056 .074 ± .008 491K</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our code and data can be accessed at https://github.com/deeksha-ms/DeepSOZ. git.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Institutes of Health</rs> <rs type="grantNumber">R01 EB029977 (PI Caffo</rs>), the <rs type="funder">National Institutes of Health</rs> <rs type="grantNumber">R01 HD108790</rs> (<rs type="affiliation">PI Venkataraman</rs>), and the <rs type="funder">National Institutes of Health</rs> <rs type="grantNumber">R21 CA263804 (PI Venkataraman</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FkktbER">
					<idno type="grant-number">R01 EB029977 (PI Caffo</idno>
				</org>
				<org type="funding" xml:id="_PTRJkqE">
					<idno type="grant-number">R01 HD108790</idno>
				</org>
				<org type="funding" xml:id="_bETTjnZ">
					<idno type="grant-number">R21 CA263804 (PI Venkataraman</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 18.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for seizure detection using effective connectivity, graph theory, and multi-level modular network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Akbarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Sig. Process. Control</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101878</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Temporal graph convolutional networks for automatic seizure detection</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Covert</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="160" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automated noninvasive seizure detection and localization using switching markov models and convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Craley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jouny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Venkataraman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-928" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="253" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automated inter-patient seizure detection using multichannel convolutional and recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Craley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Sig. Process. Control</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page">102360</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SZLoc: a multi-resolution architecture for automated epileptic seizure localization from scalp EEG</title>
		<author>
			<persName><forename type="first">J</forename><surname>Craley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated seizure activity tracking and onset zone localization from scalp EEG using deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E A</forename><surname>Craley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">264537</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometric deep learning for subject independent epileptic seizure prediction using scalp EEG signals</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dissanayake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="527" to="538" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Value of the electroencephalogram in adult patients with untreated idiopathic first seizures</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Van Donselaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Neurol</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="237" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent neural networks employing Lyapunov exponents for EEG signals classification</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Güler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="506" to="514" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial-temporal seizure detection with graph attention network and bi-directional LSTM architecture</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Sig. Process. Control</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">103908</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-channel vision transformer for epileptic seizure prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hussein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedicines</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1551</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal onset seizure prediction using convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marcuse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2109" to="2118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">EEG-based seizure prediction via transformer guided CNN</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">203</biblScope>
			<biblScope unit="page">111948</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalp EEG epileptogenic zone recognition and localization based on long-term recurrent convolutional network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">396</biblScope>
			<biblScope unit="page" from="569" to="576" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online EEG seizure detection and localization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mansouri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">176</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Epilepsy</publisher>
			<pubPlace>Hoboken</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TABS: transformer based seizure detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pedoeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bar Yosef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abittan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keene</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-99383-2_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-99383-24" />
	</analytic>
	<monogr>
		<title level="m">Biomedical Sensing and Analysis</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Obeid</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Picone</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Selesnick</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The temple university hospital seizure detection corpus</title>
		<author>
			<persName><forename type="first">V</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="https://isip.piconepress.com/projects/tuheeg/html/downloads.shtml" />
	</analytic>
	<monogr>
		<title level="j">Front. Neuroinform</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep recurrent neural network for seizure detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vidyaratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Glandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Iftekharuddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1202" to="1207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">EEG-GCNN: augmenting electroencephalogram-based neurological disease diagnosis using a domain-guided graph convolutional neural network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wagh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Health</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="367" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic epileptic EEG detection using convolutional neural network with improvements in time-domain</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Sig. Process. Control</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page">101551</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A multi-view deep learning framework for EEG seizure detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="94" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automated real-time epileptic seizure detection in scalp EEG recordings using an algorithm based on wavelet packet transform</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Zandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1639" to="1651" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Integration of 24 feature types to accurately detect and predict seizures using scalp EEG signals</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1372</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
