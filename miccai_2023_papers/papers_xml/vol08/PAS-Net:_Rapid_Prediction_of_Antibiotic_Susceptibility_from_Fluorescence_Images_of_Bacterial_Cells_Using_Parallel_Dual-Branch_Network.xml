<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network</title>
				<funder ref="#_mHq2YPK #_gsB4pV4 #_J5Z9DsC #_FJHueJe">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_JEdVMCd #_srk8Dz3 #_9ax8Qa3 #_HmkGZRD">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_Aq2GMGK">
					<orgName type="full">Shenzhen Key Basic Research Project</orgName>
				</funder>
				<funder ref="#_M6M4qyj">
					<orgName type="full">National Natural Science Foundation of Guangdong Province</orgName>
				</funder>
				<funder ref="#_xJnRCqP">
					<orgName type="full">Shenzhen Peacock Plan Team</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging</orgName>
								<orgName type="institution" key="instit1">National-Regional Key Technology Engineering Laboratory for MedicalUltrasound</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaiwei</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Medicine</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Medicine</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baiying</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging</orgName>
								<orgName type="institution" key="instit1">National-Regional Key Technology Engineering Laboratory for MedicalUltrasound</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9A91EDA5B1053495FA71A8A51712ECF6</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Parallel dual-branch network</term>
					<term>Feature interaction unit</term>
					<term>Hierarchical multi-head self-attention</term>
					<term>Antibiotic susceptibility prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, the emergence and rapid spread of multi-drug resistant bacteria has become a serious threat to global public health. Antibiotic susceptibility testing (AST) is used clinically to determine the susceptibility of bacteria to antibiotics, thereby guiding physicians in the rational use of drugs as well as slowing down the process of bacterial resistance. However, traditional phenotypic AST methods based on bacterial culture are time-consuming and laborious (usually 24-72 h). Because delayed identification of drug-resistant bacteria increases patient morbidity and mortality, there is an urgent clinical need for a rapid AST method that allows physicians to prescribe appropriate antibiotics promptly. In this paper, we present a parallel dual-branch network (i.e., PAS-Net) to predict bacterial antibiotic susceptibility from fluorescent images. Specifically, we use the feature interaction unit (FIU) as a connecting bridge to align and fuse the local features from the convolutional neural network (CNN) branch (C-branch) and the global representations from the Transformer branch (T-branch) interactively and effectively. Moreover, we propose a new hierarchical multi-head self-attention (HMSA) module that reduces the computational overhead while maintaining the global relationship modeling capability of the T-branch. PAS-Net is experimented on a fluorescent image dataset of clinically isolated Pseudomonas aeruginosa (PA) with promising prediction performance. Also, we verify the generalization performance of our algorithm in fluorescence image classification on two HEp-2 cell public datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, the overuse and misuse of antibiotics have led to an increase in the rate of bacterial antibiotic resistance worldwide <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The increasing number of multidrug resistant strains not only poses a serious threat to human health, but also poses great difficulties in clinical anti-infection treatment <ref type="bibr" target="#b2">[3]</ref>. To address this issue, clinicians rely on antibiotic susceptibility testing (AST) to determine bacterial susceptibility to antibiotics, thus guiding rational drug use. However, the traditional AST method requires overnight culture of the bacteria in the presence of antibiotics, which is time-consuming and laborious (usually 24-72 h). Such delays prevent physicians from determining effective antibiotic treatments promptly. Therefore, there is an urgent clinical need for a rapid AST method that allows physicians to prescribe appropriate antibiotics in an informed manner, which is essential to improve patient outcomes, shorten the treatment duration, and slow down the progression of bacterial resistance.</p><p>In this paper, we take Pseudomonas aeruginosa (PA) as the research object and observe the difference in shape and distribution of bacterial aggregates formed by sensitive and multi-drug resistant bacteria through fluorescent images, so we want to use image recognition technology to distinguish these two types of bacteria for the purpose of rapid prediction of antibiotic susceptibility. However, we recognize that this classification task presents several challenges (as shown in Fig. <ref type="figure" target="#fig_0">1</ref>). Firstly, in images of sensitive PA and multi-drug resistant Pseudomonas aeruginosa (MDRPA), inter-class variation is low, but intra-class variation is high. Secondly, some images have exposure problems due to the high intensity of bacterial aggregation. Lastly, there are low signal-to-noise ratio of the images, coupled with possible image artifacts resulting from inhomogeneous staining or inappropriate manipulation. In recent years, deep learning techniques have made a splash in the field of image recognition with their impressive performance and have provided powerful support for a wide range of applications in biomedical research and clinical practice. Notably, deep learning methods based on convolutional neural network (CNN, e.g., ResNet <ref type="bibr" target="#b3">[4]</ref>, ResNeXt <ref type="bibr" target="#b4">[5]</ref>, ResNeSt <ref type="bibr" target="#b5">[6]</ref>) are widely used in microscopic image classification tasks. For instance, Waisman et al. <ref type="bibr" target="#b6">[7]</ref> utilized transmission light microscopy images to train a CNN to distinguish pluripotent stem cells from early differentiated cells. Riasatian et al. <ref type="bibr" target="#b7">[8]</ref> proposed a novel network based on DenseNet <ref type="bibr" target="#b8">[9]</ref> and fine-tuned and trained it with various configurations of histopathology images. Recently, due to the successful application of ViT <ref type="bibr" target="#b9">[10]</ref> to image classification tasks, many research efforts (e.g., DeiT <ref type="bibr" target="#b10">[11]</ref>, PVT <ref type="bibr" target="#b11">[12]</ref>, Swin Transformer <ref type="bibr" target="#b12">[13]</ref>) have attempted to introduce the power of selfattention mechanism <ref type="bibr" target="#b13">[14]</ref> into computer vision. For example, He et al. <ref type="bibr" target="#b14">[15]</ref> applied a spatial pyramidal Transformer network to learn long-range contextual information for skin lesion analysis for skin disease classification.</p><p>The above studies show that two deep learning frameworks, CNN and Transformer, are effective in microscopy image classification tasks. CNN is good at extracting local features, but its receptive field is limited by the size of the convolution kernel and cannot effectively capture the global information in the image. Meanwhile, in visual Transformer, its self-attention module is good at capturing feature dependencies over long distances, but ignores local feature information. However, these two kinds of feature information are very important for the classification of microscope images with complex features. To tackle this issue, this paper builds a hybrid model that maximizes the advantages of CNN and Transformer, thus enhancing the feature representation of the network. To achieve the complementary advantages of these two techniques, we propose a parallel dual-branch network named PAS-Net, specifically designed to enable rapid prediction of bacterial antibiotic susceptibility. The main contributions of this study are as follows:</p><p>1) We develop a parallel dual-branch classification network to realize the interactive learning of features throughout the whole process through feature interaction unit (FIU), which can better integrate local features of CNN branch (C-branch) and global representations of Transformer branch (T-branch).</p><p>2) We propose a more efficient hierarchical multi-head self-attention (HMSA) module, which utilizes a local-to-global attention mechanism to simulate the global information of an image, while effectively reducing the computational costs and memory consumption.</p><p>To the best of our knowledge, this study represents the first attempt to use deep learning techniques to realize rapid AST based on PA fluorescence images, which provides a new perspective for predicting bacterial antibiotic susceptibility.  Feature dimension mismatch exists between feature map from C-branch and vector sequence from T-branch. Therefore, our network use FIU as a bridge to effectively combine the local features and the global representation in an interactive manner to eliminate the misalignment between the two features, as shown in Fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN→Transformer:</head><p>The feature map is first aligned with the dimensions of the patch embedding by 1 × 1 convolution. Then, the feature resolution is adjusted using the downsampling module to complete the alignment of the spatial dimensions. Finally, the feature maps are summed with the patch embedding of the T-branch.</p><p>Transformer→CNN: After going through the HMSA module and FFN, the patch embedding is fed back from the T-branch to the C-branch. An up-sampling module needs to be used first for the patch embedding to align the spatial scales. The patch embedding is then aligned to the number of channels of the feature map by 1 × 1 convolution, and added to the feature map of the C-branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Hierarchical Multi-head Self-attention</head><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the detailed structure of HMSA module. To be able to compute attention in a hierarchical manner, we reshape the input patch embedding E back to the patch map E p . Firstly, the patch map E p is divided into small grids of size G × G, i.e., each grid contains G × G (set G = 4 in this paper) pixel points. Then, a 1 × 1 pointwise convolution is performed on E p to obtain three matrices</p><formula xml:id="formula_0">Q = E p W Q 、K = E p W K and V = E p W V ,</formula><p>respectively, where W Q , W K and W V are three learnable weight matrices with shared parameters that are updated together with the model parameters during training. After that, we compute local attention A 0 within each small grid using the self-attention mechanism, which can be defined as:</p><formula xml:id="formula_1">Attention(Q, K, V) = Softmax QK T √ d V. (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>Then Eq. ( <ref type="formula" target="#formula_1">1</ref>) is applied once more on the basis of A 0 to obtain global attention A 1 . We reshape them back to the shape of the input E p . The final output of HMSA is</p><formula xml:id="formula_3">HMSA(E) = Transpose(Flatten(A 1 + A 0 + E p )).</formula><p>(</p><formula xml:id="formula_4">)<label>2</label></formula><p>The original MSA module computes attention map over the entire input feature, and its computational complexity scale quadratically with spatial dimension N, which can be calculated as:</p><formula xml:id="formula_5">(MSA) = 4ND 2 + 2N 2 D.</formula><p>(</p><formula xml:id="formula_6">)<label>3</label></formula><p>In contrast, our HMSA module computes attention map in a hierarchical manner so that A 0 and A 1 are computed within small G × G grids. The computational complexity of HMSA is</p><formula xml:id="formula_7">(HMSA) = 3ND 2 + 2NG 2 D. (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>With this approach, only a limited number of image blocks need to be processed in each step, thus significantly reducing the computational effort of the module from O(N 2 ) to O(NG 2 ), where G 2 is much smaller than N. For example, the size of the input image is 224 × 224, if the patch is divided according to the size of 4 × 4, the division will get (224 / 4) 2 = 3136 patches, i.e., N = 3136. However, we set G to 4, so the computational complexity of the HMSA module is greatly reduced. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Experimental Setup</head><p>The fluorescent images of PA come from a local medical school. We screen out 12 multidrug resistant strains and 11 sensitive strains. Our dataset has 2625 fluorescent images of PA, 1233 images of sensitive PA and 1392 images of MDRPA. We randomly divide the data into a training set and a test set in a ratio of 9:1. To better train the network model and prevent overfitting, we perform five data enhancement operations on each image, including horizontal flip, vertical flip and rotation at different angles (90°, 180°, 270°). Finally, our data volume is expanded to 15,750 images, including 14,178 training images and 1,572 test images.</p><p>To achieve comprehensive and objective assessment of the classification performance of the proposed method, we select eight classification evaluation metrics, including accuracy (Acc), precision (Pre), recall, specificity (Spec), F1-score (F1), Kappa, area under the receiver operating characteristic (ROC) curve (AUC). All experiments are implemented by configuring the PyTorch framework on NVIDIA GTX 2080Ti GPU with 11 GB of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Results</head><p>In this paper, we adopt Conformer <ref type="bibr" target="#b16">[17]</ref> as the baseline of our network, and then make adjustments and improvements to optimize its performance on the PA fluorescence image dataset. Table <ref type="table" target="#tab_0">1</ref> shows the results of the ablation experiments for different modules in the network. Among them, "Baseline + CB" indicates that the ResNet block in the original C-branch is replaced by the ConvNeXt block, reflecting the impact of the performanceenhanced C-branch on the classification performance. "Baseline + CB + Stem" replaces the convolutional module of the standard ResNet network in baseline with the Stem module on top of the modified C-branch. "Baseline + CB + Stem + HMSA" represents the replacement of the traditional MSA module in Baseline with the efficient HMSA module proposed in this paper on the basis of "Baseline + CB + Stem". The proposed HMSA module replaces the traditional MSA module in baseline, which achieves the improvement of network efficiency and classification performance.</p><p>In order to evaluate the classification performance of the proposed method, we choose ten state-of-the-art image classification methods for comparison, including 5 CNN networks: ResNet50 <ref type="bibr" target="#b3">[4]</ref>, ResNeXt50 <ref type="bibr" target="#b4">[5]</ref>, ResNeSt50 <ref type="bibr" target="#b5">[6]</ref>, ConvNeXt-T <ref type="bibr" target="#b15">[16]</ref> and DenseNet121 <ref type="bibr" target="#b8">[9]</ref>, and 5 Transformer-related networks: ViT-B/16 <ref type="bibr" target="#b9">[10]</ref>, DeiT-S <ref type="bibr" target="#b10">[11]</ref>, PVT-M <ref type="bibr" target="#b11">[12]</ref>, Swin-T <ref type="bibr" target="#b12">[13]</ref> and CeiT-S <ref type="bibr" target="#b17">[18]</ref>. The results of the comparative experiment are illustrated in Table <ref type="table" target="#tab_1">2</ref>. We can observe that our dual-branch network achieves the best performance on our dataset, and outperforms the CeiT-S by 7.08%, 6.7%, and 6.66% in accuracy, recall and F1-score, respectively.</p><p>To further analyze and compare the computational complexity of different methods, we compare the number of model parameters (#Param) and the number of floating-point operations per second (FLOPs). In general, the higher the number of parameters and operations, the higher the performance of the model, but at the same time, the greater the computational and storage overhead. It can be seen that the accuracy of ViT is 5% lower than that of ResNet50, but its model complexity is about three times higher. The number of model parameters of PVT-M is similar to that of our PAS-Net, but the accuracy is much worse. The number of parameters of our proposed PAS-Net is 43.4M and FLOPs is 23.37G, indicating that the network achieves a good balance between the number of parameters, FLOPs, accuracy and classification consistency.  To verify the interpretability of the proposed PAS-Net and understand its classification effect more intuitively and effectively, we visualize the results using Grad-CAM, as shown in Fig. <ref type="figure" target="#fig_4">5</ref>. From the second column vertically, we can see that the C-branch only focuses on local edge parts or incorrectly highlights some regions that are not relevant to the discrimination, as shown in the heat map in the first and second rows. From the third column, we can see that the T-branch can obtain the global attention map, but at the same time it produces some worthless and redundant features. A sideby-side comparison shows that the heat map in the fourth column can focus well on some discriminative regions with distinct features and reflect the correlation between local regions. For example, our network can effectively capture the bacterial aggregates with clear edges and largest area in the first image, and also establish the long-range feature dependencies among the three small bacterial aggregates near the lower right corner; for the second image, our dual-branch network corrects the error of focusing the C-branch to the exposure position because of the Transformer's ability to learn the global feature representation For the third and fourth images, the network nicely combines the discriminative regions focused on by the C-branch and T-branch, capturing both the local features of larger bacterial aggregates and learning the distributional dependencies among bacterial aggregates. This shows to some extent that our proposed model effectively exploits the advantages of CNN and Transformer and maximizes the retention of local features and global representation.</p><p>We also use the t-SNE dimensionality reduction algorithm to map the feature vectors learned from the last feature extraction layer of different networks onto a twodimensional plane, as shown in Fig. <ref type="figure" target="#fig_5">6</ref>. The visualization allows us to observe the clustering of the image features extracted by these networks. Compared with other models, the features extracted by our proposed PAS-Net can better distinguish the sensitive bacteria (blue) from the multi-drug resistant bacteria (pink).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Robustness to HEp-2 Dataset</head><p>To further verify the effectiveness of PAS-Net in fluorescent image classification tasks, we also apply our method to two HEp-2 cell public datasets, ICPR 2012 and I3A Task1. ICPR 2012 dataset uses average class accuracy (ACA) as the evaluation metric, which is the same concept as the accuracy mentioned above, while I3A Task1 uses mean class accuracy (MCA). We select four deep learning techniques for classification of HEp-2 cells for comparison, respectively, and the results are shown in Table <ref type="table" target="#tab_3">3</ref>. Without using pre-trained weights for migration learning and data augmentation, our network achieves 81.61% and 98.71% accuracy on ICPR 2012 dataset and I3A Task1 dataset, respectively, and the experimental results demonstrate the generalizability of the proposed PAS-Net for fluorescent image classification tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusion</head><p>In this paper, we develop a PAS-Net framework for rapid prediction of antibiotic susceptibility from bacterial fluorescence images only. PAS-Net is a parallel dual-branch feature interaction network. FIU is a connecting bridge to align and fuse the local features from the C-branch and the global representation from the T-branch, which enhances the feature representation ability of the network. We design a HMSA module with less computational overhead to improve the computational efficiency of the model. The experimental results demonstrate that our method is feasible and effective in PA fluorescence image classification task, and can assist clinicians in determining bacterial antibiotic susceptibility.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of fluorescent images of sensitive PA (first row) and MDRPA (second row).</figDesc><graphic coords="2,63,48,314,72,325,48,64,36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overall architecture of the proposed PAS-Net. DwConv: Depthwise convolution. PwConv: Pointwise convolution. FFN: Feed forward network.</figDesc><graphic coords="3,77,79,372,26,268,36,163,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 Fig. 3 .</head><label>23</label><figDesc>Figure 2 shows the overview of our proposed PAS-Net. The model consists of four parts: the Stem module, the parallel C-branch and T-branch, and the FIU connecting the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of proposed HMSA. (a) Hierarchical structure of the HMSA module. (b) Implementation details of the HMSA module.</figDesc><graphic coords="5,65,79,455,87,292,96,76,72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Grad-cam is used to highlight the discriminant regions of interest for predicting sensitive and multi-drug resistant bacteria. The first column shows four images from the test set, the first two for sensitive PA and the last two for MDRPA.</figDesc><graphic coords="9,45,30,136,31,333,52,79,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Two-dimensional t-SNE maps of different models on our test set. Pink points represent MDRPA, blue points represent sensitive PA.</figDesc><graphic coords="9,105,81,275,66,212,20,148,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation experiments of different modules in PAS-Net (%).</figDesc><table><row><cell></cell><cell>± 1.30</cell><cell>± 1.04</cell><cell></cell><cell>± 0.45</cell><cell></cell><cell>± 0.37</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AUC</cell><cell>97.46</cell><cell>98.48</cell><cell></cell><cell>98.11</cell><cell></cell><cell>99.42</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Youden</cell><cell>83.42 ± 5.34</cell><cell>87.95 ± 4.90</cell><cell></cell><cell>88.18 ± 3.28</cell><cell></cell><cell>91.97 ± 2.64</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kappa</cell><cell>83.38 ± 5.41</cell><cell>88.09 ± 4.82</cell><cell></cell><cell>88.32 ± 3.30</cell><cell></cell><cell>92.04 ± 2.70</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>± 2.64</cell><cell>± 2.19</cell><cell></cell><cell>± 1.55</cell><cell></cell><cell>± 1.30</cell><cell></cell><cell></cell><cell></cell></row><row><cell>F1</cell><cell>92.14</cell><cell>94.49</cell><cell></cell><cell>94.59</cell><cell></cell><cell>96.28</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>± 3.75</cell><cell>± 4.14</cell><cell></cell><cell>± 2.25</cell><cell></cell><cell>± 1.91</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Spec</cell><cell>91.59</cell><cell>92.32</cell><cell></cell><cell>92.45</cell><cell></cell><cell>95.18</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall</cell><cell>91.83 ± 4.38</cell><cell>95.63 ± 2.74</cell><cell></cell><cell>95.73 ± 2.19</cell><cell></cell><cell>96.80 ± 2.36</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>± 2.98</cell><cell>± 3.23</cell><cell></cell><cell>± 1.84</cell><cell></cell><cell>± 1.59</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pre</cell><cell>92.59</cell><cell>93.46</cell><cell></cell><cell>93.50</cell><cell></cell><cell>95.81</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>± 2.71</cell><cell>± 2.39</cell><cell></cell><cell>± 1.64</cell><cell></cell><cell>± 1.35</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Acc</cell><cell>91.72</cell><cell>94.08</cell><cell></cell><cell>94.19</cell><cell></cell><cell>96.04</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Baseline</cell><cell>Baseline +</cell><cell>CB</cell><cell>Baseline +</cell><cell>CB + Stem</cell><cell>+ Baseline</cell><cell>CB +</cell><cell>Stem +</cell><cell>HMSA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Classification performance camparison of state-of-the-art methods on the test set (%).</figDesc><table><row><cell>#Param FLOPs</cell><cell>± 1.23 23.5M 8.45G</cell><cell>± 0.71 23.0M 8.78G</cell><cell>± 0.75 25.4M 11.02G</cell><cell>± 0.44 27.8M 8.90G</cell><cell>± 0.99 7.0M 5.94G</cell><cell>± 1.23 86.0M 37.30G</cell><cell>± 1.12 21.7M 10.13G</cell><cell>± 0.54 43.7M 14.09G</cell><cell>± 0.49 29.8M 4.45G</cell><cell>± 0.55 23.9M 10.51G</cell><cell></cell></row><row><cell>Kappa AUC</cell><cell>± 1.94 75.86 ± 3.43 95.58</cell><cell>± 1.51 79.14 ± 2.98 95.50</cell><cell>± 1.90 81.23 ± 3.79 96.38</cell><cell>± 0.83 75.55 ± 1.88 95.85</cell><cell>± 2.18 79.74 ± 3.68 96.31</cell><cell>± 2.18 65.08 ± 5.07 90.84</cell><cell>± 1.48 71.31 ± 3.21 94.14</cell><cell>± 0.54 73.16 ± 1.19 93.52</cell><cell>± 2.11 77.09 ± 3.26 95.52</cell><cell>± 1.40 77.82 ± 2.55 95.41</cell><cell></cell></row><row><cell>F1</cell><cell>± 4.17 88.62</cell><cell>± 2.41 90.31</cell><cell>± 3.10 91.34</cell><cell>± 2.38 89.23</cell><cell>± 3.90 90.53</cell><cell>± 6.72 83.81</cell><cell>± 6.54 86.21</cell><cell>± 2.94 88.07</cell><cell>± 4.64 89.09</cell><cell>± 2.08 89.62</cell><cell></cell></row><row><cell>Recall Spec</cell><cell>± 2.89 88.66 ± 5.15 87.19</cell><cell>± 1.73 91.31 ± 3.03 87.71</cell><cell>± 2.31 92.96 ± 3.76 88.07</cell><cell>± 1.43 94.41 ± 1.91 80.57</cell><cell>± 2.53 91.65 ± 6.09 87.95</cell><cell>± 4.03 84.75 ± 4.72 80.25</cell><cell>± 4.81 84.52 ± 4.93 86.98</cell><cell>± 1.68 92.46 ± 2.37 80.23</cell><cell>± 3.67 88.58 ± 5.92 88.58</cell><cell>± 1.43 90.10 ± 3.21 87.67</cell><cell></cell></row><row><cell>Pre</cell><cell>± 1.74 88.85</cell><cell>± 1.50 89.40</cell><cell>± 1.90 89.88</cell><cell>± 0.93 84.63</cell><cell>± 1.89 89.77</cell><cell>± 2.47 83.20</cell><cell>± 1.57 88.44</cell><cell>± 0.58 84.16</cell><cell>± 1.68 90.07</cell><cell>± 1.29 89.24</cell><cell>± 1.</cell></row><row><cell>Model Acc</cell><cell>ResNet50 87.97</cell><cell>ResNeXt50 89.62</cell><cell>ResNeSt50 90.67</cell><cell>ConvNeXt-T 87.91</cell><cell>DenseNet121 89.92</cell><cell>ViT-B/16 82.63</cell><cell>DeiT-S 85.67</cell><cell>PVT-M 86.72</cell><cell>Swin-T 88.57</cell><cell>CeiT-S 88.96</cell><cell>Ours 96.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>35 95.81 ± 1.59 96.80 ± 2.36 95.18 ± 1.91 96.28 ± 1.30 92.04</head><label></label><figDesc></figDesc><table><row><cell>23.37G</cell></row><row><cell>43.4M</cell></row><row><cell>± 0.37</cell></row><row><cell>± 2.70 99.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Algorithm comparison on ICPR2012 dataset and I3A Task1 dataset (%).</figDesc><table><row><cell>ICPR 2012</cell><cell>Method</cell><cell cols="2">ACA I3A Task1</cell><cell>Method</cell><cell>MCA</cell></row><row><cell>Gao et al. [19]</cell><cell cols="2">Seven layers CNN 74.8</cell><cell cols="2">Gao et al. [19] Seven layers</cell><cell>96.76</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CNN</cell><cell></cell></row><row><cell cols="2">Phan et al. [20] VGG-16 + SVM</cell><cell>77.1</cell><cell>Jia et al. [21]</cell><cell>VGG-like</cell><cell>98.26</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>network</cell><cell></cell></row><row><cell>Jia et al. [21]</cell><cell cols="3">VGG-like network 79.29 Li et al. [22]</cell><cell>Deep residual</cell><cell>98.37</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>inception model</cell><cell></cell></row><row><cell>Liu et al. [23]</cell><cell>DACN</cell><cell>81.2</cell><cell>Lei et al. [24]</cell><cell>Cross-modal</cell><cell>98.42</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>transfer learning</cell><cell></cell></row><row><cell>Ours</cell><cell>PAS-Net</cell><cell cols="2">81.61 Ours</cell><cell>PAS-Net</cell><cell>98.71</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported <rs type="funder">National Natural Science Foundation of China</rs> (Nos. <rs type="grantNumber">62101338</rs>, <rs type="grantNumber">61871274</rs>, <rs type="grantNumber">32270196</rs> and <rs type="grantNumber">U1902209</rs>), <rs type="funder">National Natural Science Foundation of Guangdong Province</rs> (<rs type="grantNumber">2019A1515111205</rs>), <rs type="funder">Shenzhen Key Basic Research Project</rs> (<rs type="grantNumber">KCXFZ20201221173213036</rs>, <rs type="grantNumber">JCYJ20220818095809021</rs>, <rs type="grantNumber">SGDX202011030958020-07</rs>, <rs type="grantNumber">JCYJ201908081556188-06</rs>, and <rs type="grantNumber">JCYJ20190808145011 -259</rs>), <rs type="funder">Shenzhen Peacock Plan Team</rs> Project (grants number <rs type="grantNumber">KQTD20200909113758-004</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mHq2YPK">
					<idno type="grant-number">62101338</idno>
				</org>
				<org type="funding" xml:id="_gsB4pV4">
					<idno type="grant-number">61871274</idno>
				</org>
				<org type="funding" xml:id="_J5Z9DsC">
					<idno type="grant-number">32270196</idno>
				</org>
				<org type="funding" xml:id="_FJHueJe">
					<idno type="grant-number">U1902209</idno>
				</org>
				<org type="funding" xml:id="_M6M4qyj">
					<idno type="grant-number">2019A1515111205</idno>
				</org>
				<org type="funding" xml:id="_Aq2GMGK">
					<idno type="grant-number">KCXFZ20201221173213036</idno>
				</org>
				<org type="funding" xml:id="_JEdVMCd">
					<idno type="grant-number">JCYJ20220818095809021</idno>
				</org>
				<org type="funding" xml:id="_srk8Dz3">
					<idno type="grant-number">SGDX202011030958020-07</idno>
				</org>
				<org type="funding" xml:id="_9ax8Qa3">
					<idno type="grant-number">JCYJ201908081556188-06</idno>
				</org>
				<org type="funding" xml:id="_xJnRCqP">
					<idno type="grant-number">JCYJ20190808145011 -259</idno>
				</org>
				<org type="funding" xml:id="_HmkGZRD">
					<idno type="grant-number">KQTD20200909113758-004</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding the mechanisms and drivers of antimicrobial resistance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">387</biblScope>
			<biblScope unit="page" from="176" to="187" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Antimicrobial resistance: implications and costs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dadgostar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infect. Drug Resist</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3903" to="3910" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Antimicrobial resistance: a global emerging threat to public health systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ranucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Romagnoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Giaccone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Crit. Rev. Food Sci. Nutr</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2857" to="2876" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Resnest: split-attention networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2736" to="2746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning neural networks highly predict very early onset of pluripotent stem cell differentiation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stem Cell Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="845" to="859" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fine-Tuning and training of densenet for histopathology image representation using TCGA diagnostic slides</title>
		<author>
			<persName><forename type="first">A</forename><surname>Riasatian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">102032</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno>arXiv:.11929</idno>
		<title level="m">An image is worth 16 × 16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training dataefficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: a versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. neural inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully transformer network for skin lesion analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">102357</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conformer: local features coupling global representations for visual recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HEp-2 cell image classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE j. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="416" to="428" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transfer learning of a convolutional neural network for HEp-2 cell image classification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 13th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1208" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network based HEp-2 cell classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="77" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A deep residual inception network for HEp-2 cell classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M R S</forename><surname>Tavares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-67558-9_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-67558-9_2" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2017</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10553</biblScope>
			<biblScope unit="page" from="12" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HEp-2 cell classification based on a deep autoencoding-classification convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garibaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 14th International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="1019" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A deeply supervised residual network for HEp-2 cell classification via crossmodal transfer learning. Pattern Recogn</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="290" to="302" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
