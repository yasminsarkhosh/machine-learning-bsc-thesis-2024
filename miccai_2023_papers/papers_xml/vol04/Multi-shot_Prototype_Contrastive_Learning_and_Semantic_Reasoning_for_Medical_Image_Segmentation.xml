<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation</title>
				<funder ref="#_NVZ5FSX #_JG4FzUR">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_9Ne7PEH #_tstYu9N">
					<orgName type="full">University Synergy Innovation Program of Anhui Province</orgName>
				</funder>
				<funder ref="#_Pxn3rtU">
					<orgName type="full">Anhui Provincial Natural Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuhui</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Institute</orgName>
								<orgName type="institution">Anhui University</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiuquan</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Institute</orgName>
								<orgName type="institution">Anhui University</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanping</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Institute</orgName>
								<orgName type="institution">Anhui University</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenchu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence Institute</orgName>
								<orgName type="institution">Anhui University</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="578" to="588"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B8E0D198952B998700B8DE1F8CC1683A</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_55</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical image segmentation</term>
					<term>Multi-shot learning</term>
					<term>Prototype contrastive learning</term>
					<term>Semantic reasoning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the remarkable achievements made by deep convolutional neural networks in medical image segmentation, the limitation that they rely heavily on high-precision and intensively annotated samples makes it difficult to adapt to novel classes that have not been seen before. Few-shot learning is introduced to solve these challenges by learning the generalized representation of a semantic class from very few annotated support samples that can be used as a reference for unannotated query samples. In this paper, instead of averaging multiple support prototypes, we propose a multi-shot prototype contrastive learning and semantic reasoning network (MPSNet) for medical image segmentation. The multi-shot learning network exists independently within the support set, obtains effective semantic features for support images and gives priority to training the core segmentation model of prototype contrastive learning. We also propose a semantic reasoning network that takes the prior semantic features and prior segmentation model learned from the support set as the immediate and necessary conditions for the query image to deduce its segmentation mask. The proposed method is verified to be superior to the state-of-the-art methods on three public datasets, revealing its powerful segmentation and generalization abilities. Code: https://github.com/H51705/FSS MPSNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The successes of deep convolutional neural networks (CNNs) <ref type="bibr" target="#b8">[9]</ref> in image tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> rely heavily on a large number of intensively annotated training samples, and obtaining these annotated images is time-consuming, laborious and costly. In addition to the extreme lack of training data, these segmentation models also have low generalization ability due to the specificity of training classes. This limitation is more significant in medical images. Therefore, few-shot learning (FSL) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref> emerged to deal with these challenges. By learning the generalized representation of a certain semantic class from a few labeled samples (i.e., support images) to guide the segmentation of that class in a large number of unlabeled images (i.e., query images).</p><p>The development of FSL has derived many effective medical image segmentation methods, including pure CNNs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> and CNNs with a nonparametric learning module embedded <ref type="bibr" target="#b0">[1]</ref>, among which the prototype network is the most widely used <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref>. For example, Yu et al. <ref type="bibr" target="#b22">[23]</ref> proposed a location-sensitive local prototype network. Tang et al. <ref type="bibr" target="#b14">[15]</ref> proposed a recurrent mask refinement algorithm based on a prototype network. In addition, Ouyang et al. <ref type="bibr" target="#b7">[8]</ref> introduced a self-supervised super-pixel few-shot segmentation (FSS) method to get rid of the demand for labeled data in the training stage. Although FSS methods based on prototype networks have the ability to achieve image segmentation, when the prototype learned from the support image is used to judge the pixel category of the query image, it still cannot avoid the problem of insufficient prototype discriminability caused by the differences between the images as well as the complexity of the target. Furthermore, it is easy to lose important information when the target features are taken as a prototype by GAP (global average pooling). In this regard, Wang et al. <ref type="bibr" target="#b17">[18]</ref> introduced a prototype alignment regularization between the support image and the query image to better utilize the information of the support set. Meanwhile, compared with one-shot mode, the multi-shot prototype strategy is widely used to improve segmentation performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>. However, the existing multi-shot strategies usually take the average of multiple support prototypes as the final prototype, which not only weakens the independent contribution of multiple support images but also destroys the semantic information of support prototypes. Especially for medical images with lower image intensity and contrast, prototype alignment regularization and the mean prototype strategy do not significantly increase the discriminant ability and prediction accuracy of prototypes. Especially for medical images with lower image intensity and contrast, prototype alignment regularization and mean prototype strategy may aggravate the confusion of target pixel categories. For pure prototype technology, Zhou et al. <ref type="bibr" target="#b24">[25]</ref> explored a better non-parametric segmentation model based on non-learnable prototypes in their latest work and successfully applied it to the semantic segmentation of natural images, but this method has not been verified in medical images.</p><p>In this paper, we propose a prototype contrastive learning and semantic reasoning network based on multi-shot strategy (MPSNet). To our knowledge, this is a great improvement in medical image analysis. First, we design a multi-shot learning network (MLN) within the support set to generate prior semantic features and a prior segmentation model for the query image. Cross-validation mode is used to construct the support-query image pairs within the support set, and a complete FSS model is formed by prototype contrastive learning and supervised training. Second, we propose a prototype contrastive learning module (PCLM) to ascertain the positive contribution of multiple support images to the query image in segmentation guidance, leading to better segmentation performance of the query image. Third, we design a semantic reasoning network (SRN) that is convenient to directly transfer the prior semantic features and prior segmentation model to the query image to deduce its segmentation mask quickly. Our contributions can be summarized as follows:</p><p>1) A novel FSS method based on a multi-shot prototype strategy is proposed for the first time to replace the commonly used mean prototype method to improve the guidance ability of the support images to the query images in segmentation. 2) A multi-shot prototype contrastive learning network within the support set is constructed with supervised training to generate prior semantic features and a prior segmentation model, and transfer them to the query image to deduce its segmentation mask.</p><p>3) The proposed method achieves the latest performance on three public datasets that is superior to the state-of-the-art (SOTA) methods.  </p><formula xml:id="formula_0">S train = {(I s k , M s k )} K s k=1 and Q train = {(I q n , M q n )} N q</formula><p>n=1 . The support images, masks and the query image constitute the model input Input model = {I s , M s , I q }, query mask M q as the supervisory information. At testing stage, we set episodes with the same mode, that is, we use the trained FSS model to segment the query set under the function of the support set. Since our research is carried out based on multi-shot strategy, we mainly take 5-shot as an example to verify the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-shot Learning Network</head><p>The framework of MLN is shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), it is a supervised learning network that exists independently within the support set, mainly including 5 same structures but different input and output of 4-shot learning branch {P ath l } 5 l=1 . When one support image is used as the fake query image, the remaining 4 support images are used as the fake support images. Therefore, in the support set, each support image can be used as a fake query image to form a 4-shot learning network. The specific implementation process can be expressed as follows:</p><p>msi / mfq = P CLM(GAP (RN (</p><formula xml:id="formula_1">x fsi 4 i=1 ), m fsi 4 i=1 ), RN(x fq )),<label>(1)</label></formula><p>where RN (•) refers to the shared feature extractor ResNet101, GAP (•) refers to global average pooling, P CLM (•) refers to the proposed prototype contrastive learning module. x fsi , m fsi , x fq represent the fake support image, mask and query image. We use cross-entropy (CE) loss to supervise the training process:</p><formula xml:id="formula_2">L s seg = 5 i=1 L si seg = 5 i=1 CE( msi , m si ). (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>As shown in the lower part of Fig. <ref type="figure" target="#fig_0">1</ref>, we take P ath 1 branch as an example in detail the specific structures. Fake support images x fs i 4 i=1 ∈ R 3×H×W and fake query image x fq ∈ R 3×H×W are fed into ResNet101 for support features F ea fs i 4 i=1 ∈ R C×h×w and query features F ea fq ∈ R C×h×w . Under the function of support masks m fs i 4 i=1 ∈ R 1×H×W , the support prototypes P = {P i } 4 i=1 have been extracted from the support features, denoted as P i ∈ R C×1×1 , this process is implemented by GAP:</p><formula xml:id="formula_4">Pi = F ea fs i m fs i m fs i , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where support features F ea fs i are resized to the mask size (H, W ) , (h, w) denotes the shape of the feature maps, C denotes the channel number of the feature maps, denotes the Hadamard product. The obtained multi-shot prototypes P and query features F ea fq are sent into PCLM to get the final segmentation of the query image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Prototype Contrastive Learning Module</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1(c</ref>), PCLM is embedded in MLN and SRN as a core segmentation module, assumes the task of predicting the pixel-level categories of the query image by taking the multi-shot prototypes as evaluation criteria. Following Hansen et al. <ref type="bibr" target="#b2">[3]</ref>, we consider using a single foreground prototype as the feature compression of the target region, and adopt the anomaly score thresholding method to segment the query image. This method not only avoids the construction of the prototype for the background with more impurities, but also saves the calculation cost compared with the decoder. First of all, the similarity maps S i (x, y) 4 i=1 are obtained by doing cosine similarity between the multishot prototypes P and the query features F ea fq :</p><formula xml:id="formula_6">S i (x, y) = -α F ea fq (x, y) • Pi F ea fq (x, y) Pi . (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>The foreground prediction mask is then obtained:</p><formula xml:id="formula_8">m fq fore i = 1 -δ S i (x, y) -T fq = 1 -δ S i (x, y) -g θ F ea fq ,<label>(5)</label></formula><p>where (x, y) refers to the spatial location of the query image, and the prototype generates a similarity coefficient for each pixel. δ (•) denotes the Sigmoid function with a steepness parameter k = 0.5. T fq is the correlation threshold obtained by the query features F ea fq through the full connection layers g θ (•). Then, we assume that the prediction mask is no different from the real mask, and GAP it with F ea fq to obtain the hypothetical prototypes P h = P h i 4 i=1 :</p><formula xml:id="formula_9">P h i = F ea fq m fq fore i m fq fore i . (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>Theoretically, under the assumption that there is no pixel intensity and contrast difference between the query image and the support image, the hypothetical prototypes P h should be similar to the multi-shot prototypes P . But in reality, it is difficult to make the query image and the support images have a consistent feature hierarchy. That is to say, the prior knowledge provided by multiple support images for the query image is not equally important. We need to assign a weight factor w i to each support image to represent its positive contribution to query image segmentation. To this end, we determine the weight of each foreground prediction mask by comparing the hypothetical prototypes P h with real support prototypes P :</p><formula xml:id="formula_11">w i = P h i • Pi P h i Pi . (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>At this time, the foreground and background segmentation mask can be modified as:</p><formula xml:id="formula_13">m fq fore = 4 i=1 w i • m fq fore i 4 i=1 w i , (<label>8</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">m fq back = 1 -m fq fore , (<label>9</label></formula><formula xml:id="formula_16">)</formula><formula xml:id="formula_17">m fq = concatenate m fq fore , m fq back , (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>we take m fq as the output of PCLM that is also the final segmentation mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Semantic Reasoning Network</head><p>In our 5-shot segmentation task, the query image needs to be accurately segmented under the guidance of the given 5 support images. Such an FSS task can be realized through the migration of the prior semantic features and prior segmentation model from MLN. Specifically, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b), we directly prototype the support features obtained from MLN with the corresponding support masks. It is worth noting that the support images in each supervised training path are gathered together to form the support feature-mask cluster to obtain the prototype cluster. The purpose of this is to make the FSS network designed for query images conform to the laws of prior semantic features and the prior segmentation model. At the same time, prior prototypes are obtained by internal averaging of the prototype cluster. On this basis, the implementation of the same PCLM module as MLN can output the segmentation mask mq of the query image, and use the query mask m q to conduct supervised training:</p><formula xml:id="formula_19">mq = P CLM(GAP ({F ea si } 5 i=1 ), {m si } 5 i=1 ), RN(x q )),<label>(11)</label></formula><p>L q seg = CE( mq , m q ), ( <ref type="formula">12</ref>)</p><formula xml:id="formula_20">L seg = L s seg + L q seg , (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>so far, the overall loss of the proposed segmentation network MPSNet is L seg .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Datasets: We evaluate the proposed MPSNet on three public datasets: the Combined (CT-MR) Healthy Abdominal Organ Segmentation (CHAOS) dataset <ref type="bibr" target="#b3">[4]</ref> (Abd-MRI) with 20 3D T2-SPIR MRI scan images, the MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge <ref type="bibr" target="#b4">[5]</ref> (Abd-CT) with 30 3D CT scan images and the MICCAI 2019 Multi-sequence Cardiac MRI Segmentation Challenge (bSSFP fold) <ref type="bibr" target="#b25">[26]</ref> (Cardiac-MRI) with 35 3D cardiac MRI scans. Our task is to segment the liver, right kidney (RK), left kidney (LK), and spleen from Abd-MRI and Abd-CT, and the left ventricle (LV), myocardium (Myo), and right ventricle (RV) from Cardiac MRI. In the preprocessing of these datasets, we follow Ouyang et al.'s scheme <ref type="bibr" target="#b7">[8]</ref> to make a fair comparison with the SOTA methods. Similarly, we also use the super-pixels technology to generate the pseudo-labels for the preprocessed datasets.</p><p>Implementation Details: We use 5-fold cross validation to conduct training and testing. We chose the same allocation strategy for the support slices and the query slices as Roy et al. <ref type="bibr" target="#b9">[10]</ref>, and we randomly selected 1000 support-query image pairs to form the training set. All codes are based on PyTorch. We use the stochastic gradient descent (SGD) optimizer with a batch size of 1 and set the initial learning rate to 0.001, momentum to 0.9 and weight decay to 0.0005. The total number of iterations of the training is 30,000, and it takes an average of 9h to complete end-to-end training on an Nvidia RTX 3090 graphics card. To facilitate direct comparison with the SOTA methods, we follow the practice of most papers, using the pretrained ResNet101 framework as the feature encoder and the mean Sørensen-Dice coefficient (DSC) as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Superior Performance over the SOTA Methods</head><p>We compare the quantitative results of our method with the SOTA methods, including SE-Net <ref type="bibr" target="#b9">[10]</ref>, RP-Net <ref type="bibr" target="#b14">[15]</ref>, GCN-DE <ref type="bibr" target="#b12">[13]</ref>, ADNet <ref type="bibr" target="#b2">[3]</ref>, ALPNet <ref type="bibr" target="#b7">[8]</ref> and RSCNet <ref type="bibr" target="#b18">[19]</ref> are FSS methods specially proposed for medical images that have been verified on the datasets used by us. The remaining methods are classical FSS methods that are used in natural image. In particular, PANet <ref type="bibr" target="#b17">[18]</ref> contains a prototype alignment strategy that is also applicable to medical images. As shown in Table <ref type="table" target="#tab_1">1</ref>, our method has obtained the best mean DSC of 82.01%, 74.72% and 77.90% for Abd-MRI, Abd-CT and Cardiac MRI respectively. Compared with the second-best method ALPNet <ref type="bibr" target="#b7">[8]</ref>, the mean DSC achieved by our method increased by 3.17%, 1.37% and 1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Studies</head><p>The specific implementation scheme of the ablation studies can be divided into three aspects: 1) MLN is removed from MPSNet to prove that the prior semantic   prototype network is reverted, that is, MLN and PCLM are removed, and the multi-shot support prototypes are directly averaged for similarity measurement with the query features (PNet). Note that SRN cannot be implemented when MLN and PCLM are absent.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the quantitative results of the ablation studies. In the absence of MLN, the average DSC is 8.11%, 8.67% and 9.78% lower than MPSNet. In the absence of PCLM, the average DSC is 1.26%, 1.26% and 2.66% lower than MPSNet. More significantly, our method has improved average DSC by 6.28%, 5.48% and 11.15% over PNet due to the inclusion of MLN and PCLM. Ablation studies fully demonstrate the effectiveness of the proposed method and the necessity of the internal parts. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, one subject is randomly selected for each dataset to present the visualization results. In Fig. <ref type="figure" target="#fig_2">3</ref>, we plot the training loss of the support images and the query image. It's obvious that the query loss decreases faster and becomes more stable than the support loss, indicating that our proposed MLN has a positive effect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel FSS method MPSNet for medical image segmentation to replace the approach of averaging multiple support prototypes in the existing FSS methods. This is the first time that a multi-shot learning pattern is built within the support set and applied to the query image to significantly improve segmentation performance. Compared with the SOTA methods, our method is evaluated on three public datasets and achieves improvements in DSC of 3.17%, 1.37% and 1%, respectively. Relevant ablation studies also demonstrate the necessity and validity of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The proposed MPSNet consists of three parts: (a) the multi-shot learning network (MLN), (b) the semantic reasoning network (SRN) and (c) the prototype contrastive learning module (PCLM). PCLM is a core segmentation module that is embedded in both MLN and SRN. MPSNet follows a 1-way 5-shot learning mode. For each episode, the input is 5 support image-mask pairs and a query image, and the output is the segmentation mask of the query image.</figDesc><graphic coords="3,70,29,254,48,283,51,265,75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of query image segmentation in test set.</figDesc><graphic coords="8,55,98,373,64,340,18,194,47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of training loss between the support images and the query image. The value of the abscissa represents the loss measured every 100 iterations.</figDesc><graphic coords="9,63,30,255,89,297,34,51,76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>2 Method 2.1 Problem Setting In</head><label></label><figDesc>FSS problem, a segmentation model is trained on a training set D train , then the model is used to evaluate a test set D test . Assumes that the semantic classes in D train is C train and in D test is C test , the particular definition of FSL is C train C test = φ. The training set and test set have a common intrinsic pattern that both contain a support set S and a query set Q. At training stage, we use episode mode to set N -way K-shot segmentation task, where N represents the number of semantic classes contained in each episode and K represents the number of images contained in each class. For the task of few-shot medical image segmentation, N is usually set to 1, that is, each episode only sets the support set and the query set for a specific class c. For one episode, suppose that the support set and the query set are</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison between MPSNet and the SOTA methods.</figDesc><table><row><cell>Dataset</cell><cell>Abd-MRI</cell><cell></cell><cell cols="2">Abd-CT</cell><cell></cell><cell></cell><cell></cell><cell>Cardiac-MRI</cell></row><row><cell>Methods</cell><cell>Liver RK</cell><cell>LK</cell><cell cols="2">Spleen Mean Liver RK</cell><cell>LK</cell><cell cols="3">Spleen Mean LV</cell><cell>Myo RV</cell><cell>Mean</cell></row><row><cell>SE-Net [10]</cell><cell cols="8">27.43 61.32 62.11 51.80 50.66 38.20 23.60 32.70 32.53 31.76 58.04 25.18 12.86 32.03</cell></row><row><cell>RP-Net [15]</cell><cell cols="8">73.51 85.78 81.40 76.35 79.26 79.62 70.00 70.48 69.85 72.48 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GCN-DE [13]</cell><cell cols="8">49.47 83.03 76.07 60.63 67.30 46.77 75.50 68.13 56.53 61.73 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PANet [18]</cell><cell cols="8">66.59 42.18 44.32 54.59 51.90 69.16 36.42 38.83 42.40 46.70 70.62 46.03 67.16 61.27</cell></row><row><cell>PPNet [7]</cell><cell cols="3">73.12 71.78 62.13 66.57 68.40 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>67.78 42.61 60.80 57.06</cell></row><row><cell>CANet [24]</cell><cell cols="3">72.88 77.15 69.53 67.05 71.65 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>78.99 43.61 61.10 61.07</cell></row><row><cell>ADNet [3]</cell><cell cols="3">80.81 83.28 75.28 75.92 78.82 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.53 62.43 77.31 75.76</cell></row><row><cell>ALPNet [8]</cell><cell cols="8">76.10 85.18 81.92 72.18 78.84 78.29 71.81 72.36 70.96 73.35 83.99 66.74 79.96 76.90</cell></row><row><cell>RSCNet [19]</cell><cell cols="8">75.55 84.24 77.07 73.73 77.65 73.63 63.37 67.39 67.36 67.94 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="9">MPSNet (Ours) 81.88 87.26 82.10 76.80 82.01 81.13 69.56 70.26 77.96 74.72 87.98 65.08 80.63 77.90</cell></row><row><cell cols="9">features and the prior segmentation model are more effective than the indepen-</cell></row><row><cell cols="9">dent segmentation based on the prototype network (w/o MLN). 2) We use PCLM</cell></row><row><cell cols="9">to replace the mean prototype strategy to verify its improvement in segmenta-</cell></row><row><cell cols="9">tion performance (w/o PCLM). 3) The initial segmentation model based on the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results of the ablation studies.</figDesc><table><row><cell>Dataset</cell><cell>Abd-MRI</cell><cell></cell><cell>Abd-CT</cell><cell></cell><cell cols="2">Cardiac-MRI</cell><cell></cell></row><row><cell>Methods</cell><cell>Liver RK</cell><cell>LK</cell><cell>Spleen Mean Liver RK</cell><cell>LK</cell><cell>Spleen Mean LV</cell><cell>Myo RV</cell><cell>Mean</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>MPSNet 81.88 87.26 82.10 76.80 82.01 81.13 69.56 70.26 77.96 74.72 87.98 65.08 80.63 77.90</head><label></label><figDesc></figDesc><table /><note><p>w/o MLN 79.18 78.88 70.62 66.90 73.90 82.25 57.21 58.48 66.28 66.05 84.04 51.83 68.47 68.12 w/o PCLM 80.52 86.51 81.26 74.72 80.75 80.89 69.29 68.29 75.37 73.46 86.01 62.51 77.19 75.24 PNet 81.13 78.63 74.35 68.81 75.73 82.85 62.48 64.20 67.43 69.24 83.42 50.08 66.76 66.75</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported in part by The <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">62106001</rs>, <rs type="grantNumber">U1908211</rs>), The <rs type="funder">University Synergy Innovation Program of Anhui Province</rs> (<rs type="grantNumber">GXXT-2021-007</rs>, <rs type="grantNumber">GXXT-2022-052</rs>), and The <rs type="funder">Anhui Provincial Natural Science Foundation</rs> (<rs type="grantNumber">2208085Y19</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NVZ5FSX">
					<idno type="grant-number">62106001</idno>
				</org>
				<org type="funding" xml:id="_JG4FzUR">
					<idno type="grant-number">U1908211</idno>
				</org>
				<org type="funding" xml:id="_9Ne7PEH">
					<idno type="grant-number">GXXT-2021-007</idno>
				</org>
				<org type="funding" xml:id="_tstYu9N">
					<idno type="grant-number">GXXT-2022-052</idno>
				</org>
				<org type="funding" xml:id="_Pxn3rtU">
					<idno type="grant-number">2208085Y19</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactive few-shot learning: Limited supervision, better medical image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2575" to="2588" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-level semantic adaptation for few-shot segmentation on cardiac image sequences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">102170</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Anomaly detectioninspired few-shot medical image segmentation through self-supervision with supervoxels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">102385</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chaos challenge-combined (ct-mr) healthy abdominal organ segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kavur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Conze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101950</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Miccai multi atlas labeling beyond the cranial vault-workshop and challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Igelsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Langerak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive prototype learning and allocation for few-shot segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8334" to="8343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Part-aware prototype network for few-shot semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58545-7_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58545-79" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12354</biblScope>
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Self-supervision with superpixels: training few-shot medical image segmentation without annotation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Biffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="762" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">&apos;squeeze &amp; excite&apos; guided few-shot segmentation of volumetric images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Polsterl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101587</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Foundation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Two-stage segmentation network with feature aggregation and multi-level attention mechanism for multi-modality heart images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">102054</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Few-shot medical image segmentation using a global correlation network with discriminative embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">105067</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to compare: relation network for few-shot learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent mask refinement for few-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3918" to="3928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prior guided feature enrichment network for few-shot segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Foundation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Panet: few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9197" to="9206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Few-shot medical image segmentation regularized with self-reference and contrastive learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-849" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 202</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="514" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video salient object detection using dual-stream spatiotemporal attention</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Albuquerque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">107433</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bmanet: boundary mining with adversarial learning for semisupervised 2d myocardial infarction segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="96" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Synthesis of gadolinium-enhanced liver tumors on nonenhanced liver mr images using pixel-level graph reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101976</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A location-sensitive local prototype network for few-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Symposium on Biomedical Imaging</title>
		<imprint>
			<biblScope unit="page" from="262" to="266" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Canet: class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5217" to="5226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation: a prototype view</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2582" to="2593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multivariate mixture model for myocardial segmentation combining multi-source images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2933" to="2946" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
