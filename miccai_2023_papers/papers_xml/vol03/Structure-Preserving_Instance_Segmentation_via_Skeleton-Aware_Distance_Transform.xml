<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform</title>
				<funder>
					<orgName type="full">Google</orgName>
				</funder>
				<funder ref="#_q4m9Cpd #_n3t8VCc">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Zudi</forename><surname>Lin</surname></persName>
							<email>linzudi@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Donglai</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aarush</forename><surname>Gupta</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">CMU</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Google Research</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="529" to="539"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">22E924B7CBBB970326D70F1EF53ECD3A</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_51</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Objects with complex structures pose significant challenges to existing instance segmentation methods that rely on boundary or affinity maps, which are vulnerable to small errors around contacting pixels that cause noticeable connectivity change. While the distance transform (DT) makes instance interiors and boundaries more distinguishable, it tends to overlook the intra-object connectivity for instances with varying width and result in over-segmentation. To address these challenges, we propose a skeleton-aware distance transform (SDT) that combines the merits of object skeleton in preserving connectivity and DT in modeling geometric arrangement to represent instances with arbitrary structures. Comprehensive experiments on histopathology image segmentation demonstrate that SDT achieves state-of-the-art performance.</p><p>Z. Lin-Currently affiliated with Amazon Alexa. Work was done before joining Amazon. A. Gupta-Work was done during an internship at Harvard University.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Instances with complex shapes arise in many biomedical domains, and their morphology carries critical information. For example, the structure of gland tissues in microscopy images is essential in accessing the pathological stages for cancer diagnosis and treatment. These instances, however, are usually closely in touch with each other and have non-convex structures with parts of varying widths (Fig. <ref type="figure" target="#fig_0">1a</ref>), posing significant challenges for existing segmentation methods.</p><p>In the biomedical domain, most methods <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b22">22]</ref> first learns intermediate representations and then convert them into masks with standard segmentation algorithms like connected-component labeling and watershed transform. These representations are not only efficient to predict in one model forward pass but also able to capture object geometry (i.e., precise instance boundary), which are hard for top-down methods using low-resolution features for mask generation. However, existing representations have several restrictions. For example, boundary map is usually learned as a pixel-wise binary classification task, which makes the model conduct relatively local predictions and consequently become vulnerable to small errors that break the connectivity between adjacent instances (Fig. <ref type="figure" target="#fig_0">1b</ref>). To improve the boundary map, Deep Watershed Transform (DWT) <ref type="bibr" target="#b0">[1]</ref> predicts the Euclidean distance transform (DT) of each pixel to the instance boundary. This representation is more aware of the structure for convex objects, as the energy value for centers is significantly different from pixels close to the boundary. However, for objects with non-convex morphology, the boundary-based distance transform produces multiple local optima in the energy landscape (Fig. <ref type="figure" target="#fig_0">1c</ref>), which tends to break the intra-instance connectivity when applying thresholding and results in over-segmentation.</p><p>To preserve the connectivity of instances while keeping the precise instance boundary, in this paper, we propose a novel representation named skeleton-aware distance transform (SDT). Our SDT incorporate object skeleton, a concise and connectivity-preserving representation of object structure, into the traditional boundary-based distance transform (DT) (Fig. <ref type="figure" target="#fig_0">1d</ref>). In quantitative evaluations, we show that our proposed SDT achieves leading performance on histopathology image segmentation for instances with various sizes and complex structures. Specifically, under the Hausdorff distance for evaluating shape similarity, our approach improves the previous state-of-the-art method by relatively 10.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Instance Segmentation. Bottom-up instance segmentation approaches have become de facto for many biomedical applications due to the advantage in segmenting objects with arbitrary geometry. U-Net <ref type="bibr" target="#b14">[14]</ref> and DCAN <ref type="bibr" target="#b4">[4]</ref> use fully convolutional models to predict the boundary map of instances. Since the boundary map is not robust to small errors that can significantly change instance structure, shape-preserving loss <ref type="bibr" target="#b22">[22]</ref> adds a curve fitting step in the loss function to enforce boundary connectivity. In order to further distinguish closely touching instances, deep watershed transform (DWT) <ref type="bibr" target="#b0">[1]</ref> predicts the distance transform (DT) that represents each pixel as its distance to the closest boundary. However, for complex structure with parts of varying width, the boundary-based DT tends to produce relatively low values for thin connections and consequently causes oversegmentation. Compared to DWT, our SDT incorporates object skeleton (also known as medial axis) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b24">24]</ref> that concisely captures the topological connectivity into standard DT to enforce both the geometry and connectivity.</p><p>Object Skeletonization. Object skeleton <ref type="bibr" target="#b15">[15]</ref> is a one-pixel wide representation of object masks that can be calculated by topological thinning <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b24">24]</ref> or medial axis transform <ref type="bibr" target="#b1">[2]</ref>. The vision community has been working on direct object skeletonization from images <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b19">19</ref>]. Among the works, only Shen et al. <ref type="bibr" target="#b16">[16]</ref> shows the application of the skeleton on segmenting single-object images. We instead focus on the more challenging instance segmentation task with multiple objects closely touching each other. Object skeletons are also used to correct errors in pre-computed segmentation masks <ref type="bibr" target="#b11">[11]</ref>. Our SDT framework instead use the skeleton in the direct segmentation from images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Skeleton-Aware Distance Transform</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SDT Energy Function</head><p>Given an image, we aim to design a new representation E for a model to learn, which is later decoded into instances with simple post-processing. Specifically, a good representation for capturing complex-structure masks should have two desired properties: precise geometric boundary and robust topological connectivity.</p><p>Let Ω denote an instance mask, and Γ b be the boundary of the instance (pixels with other object indices in a small local neighborhood). The boundary (or affinity) map is a binary representation where E| Γ b = 0 and E| Ω\Γ b = 1. Taking the merits of DT in modeling the geometric arrangement and skeleton in preserving connectivity, we propose a new representation E that satisfies:</p><formula xml:id="formula_0">0 = E| Γ b &lt; E| Ω\(Γ b ∪Γs) &lt; E| Γs = 1<label>(1)</label></formula><p>Here E| Ω\Γs &lt; E| Γs = 1 indicates that there is only one global maximum for each instance, and the value is assigned to a pixel if and only if the pixel is on the skeleton. This property avoids ambiguity in defining the object interior and preserve connectivity. Besides, E| Ω\Γ b &gt; E| Γ b = 0 ensures that boundary is distinguishable as the standard DT, which produces precise geometric boundary.</p><p>For the realization of E, let x be a pixel in the input image, and d be the metric, e.g., Euclidean distance. The energy function for distance transform (DT) is defined as E DT (x) = d(x, Γ b ), which starts from 0 at object boundary and increases monotonically when x is away from the boundary. Similarly, we can define an energy function d(x, Γ s ) representing the distance from the skeleton. It vanishes to 0 when the pixel approaches the object skeleton. Formally, we define the energy function of the skeleton-aware distance transform (Fig. <ref type="figure" target="#fig_1">2</ref>) as</p><formula xml:id="formula_1">E SDT (x) = d(x, Γ b ) d(x, Γ s ) + d(x, Γ b ) α , α &gt; 0 (2)</formula><p>where α controls the curvature of the energy surface<ref type="foot" target="#foot_0">1</ref> . When 0 &lt; α &lt; 1, the function is concave and decreases faster when being close to the boundary, and vice versa when α &gt; 1. In the ablation studies, we demonstrate various patterns of the model predictions given different α.</p><p>Besides, since common skeletonization algorithms can be sensitive to small perturbations on the object boundary and produce unwanted branches, we smooth the masks before computing the object skeleton by Gaussian filtering and thresholding to avoid complex branches.</p><p>Learning Strategy. Given the ground-truth SDT energy map, there are two ways to learn it using a CNN model. The first way is to regress the energy using L 1 or L 2 loss. In the regression mode, the output is a single-channel image. The second way is to quantize the [0, 1] energy space into K bins and rephrase the regression task into a classification task <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">18]</ref>, which makes the model robust to small perturbations in the energy landscape. For the classification mode, the model output has (K +1) channels with one channel representing the background region. We fix the bin size to 0.1 without tweaking, making K = 10. Softmax is applied before calculating the cross-entropy loss. We test both learning strategies in the experiments to illustrate the optimal setting for SDT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SDT Network</head><p>Network Architecture. Directly learning the energy function with a fully convolutional network (FCN) can be challenging. Previous approaches either first regress an easier direction field representation and then use additional layers to predict the desired target <ref type="bibr" target="#b0">[1]</ref>, or take the multi-task learning approach to predict additional targets at the same time <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22]</ref>. Fortunately, with recent progress in FCN architectures, it becomes feasible to learn the target energy map in an end-to-end fashion. Specifically, in all the experiments, we use a DeepLabV3 model <ref type="bibr" target="#b5">[5]</ref> with a ResNet <ref type="bibr" target="#b6">[6]</ref> backbone to directly learn the SDT energy without additional targets (Fig. <ref type="figure" target="#fig_2">3</ref>, Training Phase). We also add a CoordConv <ref type="bibr" target="#b10">[10]</ref> layer before the 3rd stage in the backbone network to introduce spatial information into the segmentation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target SDT Generation.</head><p>There is an inconsistency problem in object skeleton generation: part of the complete instance skeleton can be different from the skeleton of the instance part (Fig. <ref type="figure" target="#fig_3">4</ref>). Some objects may touch the image border due to either a restricted field of view (FoV) of the imaging devices or spatial data augmentation like the random crop. If pre-computing the skeleton, we will get local skeleton (Fig. <ref type="figure" target="#fig_3">4c</ref>) for objects with missing masks due to imaging restrictions, and partial skeleton (Fig. <ref type="figure" target="#fig_3">4b</ref>) due to spatial data augmentation, which causes ambiguity. Therefore we calculate the local skeleton for SDT on-the-fly after all spatial transformations instead of pre-computing to prevent the model from hallucinating the structure of parts outside of the currently visible region. In inference, we always run predictions on the whole images to avoid inconsistent predictions. We use the skeletonization algorithm in Lee et al. <ref type="bibr" target="#b8">[8]</ref>, which is less sensitive to small perturbations and produces skeletons with fewer branches.</p><p>Instance Extraction from SDT. In the SDT energy map, all boundary pixels share the same energy value and can be processed into segments by direct thresholding and connected component labeling, similar to DWT <ref type="bibr" target="#b0">[1]</ref>. However, since the prediction is never perfect, the energy values along closely touching boundaries are usually not sharp and cause split-errors when applying a higher threshold or merge-errors when applying a lower threshold. Therefore we utilize a skeleton-aware instance extraction (Fig. <ref type="figure" target="#fig_2">3</ref>, Inference Phase) for SDT. Specifically, we set a threshold θ = 0.7 so that all pixels with the predicted energy bigger than θ are labeled as skeleton pixels. We first perform connected component labeling of the skeleton pixels to generate seeds and run the watershed algorithm on the reversed energy map using the seeds as basins (local optima) to generate the final segmentation. We also follow previous works <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b22">22]</ref> and refine the segmentation by hole-filling and removing small spurious objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Histopathology Instance Segmentation</head><p>Accurate instance segmentation of gland tissues in histopathology images is essential for clinical analysis, especially cancer diagnosis. The diversity of object appearance, size, and shape makes the task challenging.</p><p>Dataset and Evaluation Metric. We use the gland segmentation challenge dataset <ref type="bibr" target="#b17">[17]</ref> that contains colored light microscopy images of tissues with a wide range of histological levels from benign to malignant. There are 85 and 80 images in the training and test set, respectively, with ground truth annotations provided by pathologists. According to the challenge protocol, the test set is further divided into two splits with 60 images of normal and 20 images of abnormal tissues for evaluation. Three evaluation criteria used in the challenge include instance-level F1 score, Dice index, and Hausdorff distance, which measure the performance of object detection, segmentation, and shape similarity, respectively. For the instance-level F1 score, an IoU threshold of 0.5 is used to decide the correctness of a prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods in Comparison.</head><p>We compare SDT with previous state-of-the-art segmentation methods, including DCAN <ref type="bibr" target="#b4">[4]</ref>, multi-channel network (MCN) <ref type="bibr" target="#b21">[21]</ref>, shape-preserving loss (SPL) <ref type="bibr" target="#b22">[22]</ref> and FullNet <ref type="bibr" target="#b13">[13]</ref>. We also compare with suggestive annotation (SA) <ref type="bibr" target="#b23">[23]</ref>, and SA with model quantization (QSA) <ref type="bibr" target="#b20">[20]</ref>, which use multiple FCN models to select informative training samples from the dataset. With the same training settings as our SDT, we also report the performance of skeleton with scales (SS) and traditional distance transform (DT). Training and Inference. Since the training data is relatively limited due to the challenges in collecting medical images, we apply pixel-level and spatial-level augmentations, including random brightness, contrast, rotation, crop, and elastic transformation, to alleviate overfitting. We set α = 0.8 for our SDT in Eq. 2. We use the classification learning strategy and optimize a model with 11 output channels (10 channels for energy quantized into ten bins and one channel for Table <ref type="table">1</ref>. Comparison with existing methods on the gland segmentation. Our SDT achieves better or on par F1 score and Dice Index, and significantly better Hausdorff distance for evaluating shape similarity. DT and SS represent distance transform and skeleton with scales. <ref type="bibr" target="#b4">[4]</ref> 0.912 0.716 0.897 0.781 45.42 160.35 MCN <ref type="bibr" target="#b21">[21]</ref> 0.893 0.843 0.908 0.833 44.13 116.82 SPL <ref type="bibr" target="#b22">[22]</ref> 0.924 0.844 0.902 0.840 49.88 106.08 SA <ref type="bibr" target="#b23">[23]</ref> 0.921 0.855 0.904 0.858 44.74 96.98 FullNet <ref type="bibr" target="#b13">[13]</ref>  Specifically for SS, we set the number of output channels to two, with one channel predicting skeleton probability and the other predicting scales. Since the scales are non-negative, we add a ReLU activation for the second channel and calculate the regression loss. Masks are generated by morphological dilation. We do not quantize the scales as DT and SDT since even ground-truth scales can yield masks unaligned with the instance boundary with quantization.</p><formula xml:id="formula_2">Method F1 Score ↑ Dice Index ↑ Hausdorff ↓ Part A Part B Part A Part B Part A Part B DCAN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>Our SDT framework achieves state-of-the-art performance on 5 out of 6 evaluation metrics on the gland segmentation dataset (Table <ref type="table">1</ref>). With the better distinguishability of object interior and boundary, SDT can unambiguously separate closely touching instances (Fig. <ref type="figure" target="#fig_4">5</ref>, first two rows), performs better than previous methods using object boundary representations <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b22">22]</ref>. Besides, under the Hausdorff distance for evaluating shape-similarity between ground-truth and predicted masks, our SDT reports an average score of 44.82 across two test splits, which improves the previous state-of-the-art approach (i.e., FullNet with an average score of 50.15) by 10.6%. We also notice the different sensitivities of the three evaluation metrics. Taking the instance D (Fig. <ref type="figure" target="#fig_4">5</ref>, 3rd row) as an example: both SDT and FullNet <ref type="bibr" target="#b13">[13]</ref> have 1.0 F1-score (IoU threshold 0.5) for the correct detection; SDT has a slightly higher Dice Index (0.956 vs. 0.931) for better pixel-level classification; and our SDT has significantly lower Hausdorff distance (24.41 vs. 48.81) as SDT yields a mask with much more accurate morphology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Studies</head><p>Loss Function. We compare the regression mode using L1 and L2 losses with the classification mode using cross-entropy loss. There is a separate channel for background under the classification mode where the energy values are quantized into bins. However, for regression mode, if the background value is 0, we need to use a threshold τ &gt; 0 to decide the foreground region, which results in shrank masks. To separate the background region from the foreground objects, we assign an energy value of -b to the background pixels (b ≥ 0). To facilitate the regression, given the predicted value ŷi for pixel i, we apply a sigmoid function (σ) and affine transformation so that ŷ i = (1+b) • σ(ŷ i )b has a range of (-b, 1). We set b = 0.1 for the experiments. We show that under the same settings, the model trained with quantized energy reports the best results (Table <ref type="table" target="#tab_0">2</ref>). We also notice that the model trained with L 1 loss produces a much sharper energy surface than the model trained with L 2 loss, which is expected.</p><p>Curvature. We also compare different α in Eq. 2 that controls the curvature of the energy landscape. Table <ref type="table" target="#tab_0">2</ref> shows that α = 0.8 achieves the best overall performance, which is slightly better than α = 1.0. Decreasing α to 0.6 introduces more merges and make the results worse.</p><p>Global/Local Skeleton. In Sect. 2.2 we show the inconsistency problem of global and local skeletons. In this study, we set α = 0.8 and let the model learn the pre-computed SDT energy for the training set. The results show that pre-computed SDT significantly degrades performance (Table <ref type="table" target="#tab_0">2</ref>). We argue this is because pre-computed energy not only introduces inconsistency for instances touching the image border but also restricts the diversity of SDT energy maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we introduce the skeleton-aware distance transform (SDT) to capture both the geometry and topological connectivity of instance masks with complex shapes. For multi-class problems, we can use class-aware semantic segmentation to mask the SDT energy trained for all objects that is agnostic to their classes. We hope this work can inspire more research on not only better representations of object masks but also novel models that can better predict those representations with shape encoding. We will also explore the application of SDT in the more challenging 3D instance segmentation setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Skeleton-aware distance transform (SDT). Given (a) instance masks, (b) the boundary map is prone to false merge errors at object contact pixels while (c) the distance transform (DT) struggles to preserve object connectivity. (d) Our SDT can both separate touching instances and enforce object connectivity.</figDesc><graphic coords="2,66,81,53,78,291,04,113,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the SDT energy function. (a) Given an instance mask Ω, (b) we calculate the distances of a pixel to both the skeleton and boundary. (c) Our energy function ensures a uniform maximum value of 1 on the skeleton and minimum value of 0 on the boundary, with a smooth interpolation in between.</figDesc><graphic coords="4,77,31,53,93,269,92,87,76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of the SDT framework. (a) Training Phase: target SDT is calculated conditioned on the distance to both the boundary and skeleton. A FCN maps the image into the energy space to minimize the loss. (b) Inference Phase: we threshold the SDT to generate skeleton segments, which is processed into seeds with the connected component labeling. Finally, the watershed transform algorithm takes the seeds and the reversed SDT energy to yield the masks.</figDesc><graphic coords="5,62,46,154,73,327,28,141,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Skeleton generation rule. (a) Given an instance and the global skeleton, (b) the partial skeleton cropped from the global skeleton can be different from (c) the local skeleton generated from the cropped mask. For SDT, we calculate the local skeleton to prevent the model from extrapolating the unseen parts.</figDesc><graphic coords="6,84,30,202,37,255,64,115,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visual comparison on histopathology image segmentation. (First 2 rows) Compared with shape-preserving loss (SPL) [22], our SDT unambiguously separates closely touching objects while preserving the structure of complicated masks. (The 3rd row) Compared with FullNet [13], our model infers the SDT energy of instance masks from a global structure perspective instead of boundary that relies on relatively local predictions, which produces high-quality masks.</figDesc><graphic coords="7,57,48,304,94,337,36,131,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Ablations studies on the gland dataset. The results suggest that the model trained with cross-entropy loss with α = 0.8 and local skeleton generation achieves the best performance. We train the model for 20k iterations with an initial learning rate of 5 × 10 -4 and a momentum of 0.9. The same settings are applied to DT. At inference time, we apply argmax to get the corresponding bin index of each pixel and transform the energy value to the original data range. Finally, we apply the watershed-based instance extraction rule described in Sect. 2.2.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Setting F1 Score ↑</cell><cell>Dice Index ↑ Hausdorff ↓</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Part A Part B Part A Part B Part A Part B</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell>L1</cell><cell>0.916 0.842 0.903 0.850 39.76</cell><cell>94.83</cell></row><row><cell></cell><cell></cell><cell></cell><cell>L2</cell><cell>0.896 0.833 0.885 0.837 49.11 110.24</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CE</cell><cell>0.931 0.866 0.919 0.851 32.29</cell><cell>82.40</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Curvature</cell></row><row><cell></cell><cell></cell><cell></cell><cell>α = 0.6 0.912 0.845 0.914 0.855 36.25 α = 0.8 0.931 0.866 0.919 0.851 32.29 α = 1.0 0.926 0.858 0.907 0.849 35.73</cell><cell>91.24 82.40 86.73</cell></row><row><cell>QSA [20] SS</cell><cell cols="2">0.924 0.853 0.914 0.856 37.28 0.930 0.862 0.914 0.859 41.78 0.872 0.765 0.853 0.797 54.86 116.33 88.75 97.39</cell><cell>Skeleton Partial 0.899 0.831 0.896 0.837 47.50 105.19 Local 0.931 0.866 0.919 0.851 32.29 82.40</cell></row><row><cell>DT</cell><cell>0.918 0.846 0.896 0.848 41.84</cell><cell>90.86</cell></row><row><cell>SDT</cell><cell>0.931 0.866 0.919 0.851 32.29</cell><cell>82.40</cell></row><row><cell cols="2">background).</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We add = 10 -6 to the denominator to avoid dividing by 0 for the edge case where a pixel is both instance boundary and skeleton (i.e., a one-pixel wide part).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work has been partially supported by <rs type="funder">NSF</rs> awards <rs type="grantNumber">IIS-2239688</rs> and <rs type="grantNumber">IIS-2124179</rs>. This work has also been partially supported by gift funding and GCP credits from <rs type="funder">Google</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_q4m9Cpd">
					<idno type="grant-number">IIS-2239688</idno>
				</org>
				<org type="funding" xml:id="_n3t8VCc">
					<idno type="grant-number">IIS-2124179</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5221" to="5229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<title level="m">A Transformation for Extracting New Descriptors of Shape</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maximin affinity learning of image segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Briggman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Helmstaedter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Turaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1865" to="1873" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dcan: deep contour-aware networks for accurate gland segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2487" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Srn: side-output residual network for object symmetry detection in the wild</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1068" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building skeleton models via 3-d medial surface axis thinning algorithms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP: Graph. Models Image Process</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="462" to="478" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Linear span network for object skeleton detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="133" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="9605" to="9616" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Biologicallyconstrained graphs for global connectomics reconstruction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Matejek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Parag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">2d parallel thinning and shrinking based on sufficient conditions for topology preservation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Németh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Palágyi</surname></persName>
		</author>
		<idno type="DOI">10.14232/actacyb.20.1.2011.10</idno>
		<ptr target="https://doi.org/10.14232/actacyb.20.1.2011.10" />
	</analytic>
	<monogr>
		<title level="j">Acta Cybernetica</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="144" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving nuclei/gland instance segmentation in histopathology images by full resolution neural network and spatial constrained loss</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Riedlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-7" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey on skeletonization algorithms and their applications</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Borgefors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Di Baja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="3" to="12" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepskeleton: learning multi-task scale-associated deep side outputs for object skeleton extraction in natural images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5298" to="5311" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gland segmentation in colon histology images: the glas challenge contest</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="489" to="502" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep distance transform for tubular structure segmentation in ct scans</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3833" to="3842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepflux for skeletons in the wild</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsogkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5287" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quantization of fully convolutional networks for accurate biomedical image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8300" to="8308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gland instance segmentation using deep multichannel neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2901" to="2912" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A deep model with shape-preserving loss for gland instance segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-T</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00934-2_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00934-216" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11071</biblScope>
			<biblScope unit="page" from="138" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Suggestive annotation: a deep active learning framework for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66179-7_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66179-7" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Duchesne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10435</biblScope>
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A fast parallel algorithm for thinning digital patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="236" to="239" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
