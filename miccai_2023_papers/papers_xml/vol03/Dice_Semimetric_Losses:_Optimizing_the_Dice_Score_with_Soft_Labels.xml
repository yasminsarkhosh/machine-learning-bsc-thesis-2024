<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels</title>
				<funder ref="#_mU66Hrw #_zVTeksR">
					<orgName type="full">Research Foundation -Flanders (FWO)</orgName>
				</funder>
				<funder>
					<orgName type="full">Flemish Government</orgName>
				</funder>
				<funder>
					<orgName type="full">VSC (Flemish Supercomputer Center)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zifu</forename><surname>Wang</surname></persName>
							<email>zifu.wang@kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI, KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Teodora</forename><surname>Popordanoska</surname></persName>
							<email>teodora.popordanoska@kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI, KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeroen</forename><surname>Bertels</surname></persName>
							<email>jeroen.bertels@kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI, KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robin</forename><surname>Lemmens</surname></persName>
							<email>robin.lemmens@kuleuven.be</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Neurosciences</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Neurology</orgName>
								<orgName type="institution">UZ Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
							<email>matthewb.blaschko@kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI, KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="475" to="485"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">38A42A00C03612A319E5D45B5A6FC479</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_46</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dice Score</term>
					<term>Dice Loss</term>
					<term>Soft Labels</term>
					<term>Model Calibration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The soft Dice loss (SDL) has taken a pivotal role in numerous automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct utilization in scenarios involving soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be employed in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calibration, which supports the wider adoption of DMLs in practice. The code is available at https://github.com/zifuwanggg/JDTLosses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image segmentation is a fundamental task in medical image analysis. One of the key design choices in many segmentation pipelines that are based on neural networks lies in the selection of the loss function. In fact, the choice of loss function goes hand in hand with the metrics chosen to assess the quality of the predicted segmentation <ref type="bibr" target="#b46">[46]</ref>. The intersection-over-union (IoU) and the Dice score are commonly used metrics because they reflect both size and localization agreement, and they are more in line with perceptual quality compared to, e.g., pixel-wise accuracy <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">27]</ref>. Consequently, directly optimizing the IoU or the Dice score using differentiable surrogates as (a part of) the loss function has become prevalent in semantic segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b47">47]</ref>. In medical imaging in particular, the Dice score and the soft Dice loss (SDL) <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b42">42]</ref> have become the standard practice, and some reasons behind its superior functioning have been uncovered and further optimizations have been explored <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b45">45]</ref>.</p><p>Another mechanism to further improve the predicted segmentation that has gained significant interest in recent years, is the use of soft labels during training. Soft labels can be the result of data augmentation techniques such as label smoothing (LS) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">43]</ref> and are integral to regularization methods such as knowledge distillation (KD) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">36]</ref>. Their role is to provide additional regularization so as to make the model less prone to overfitting <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">43]</ref> and to combat overconfidence <ref type="bibr" target="#b13">[14]</ref>, e.g., providing superior model calibration <ref type="bibr" target="#b31">[31]</ref>. In medical imaging, soft labels emerge not only from LS or KD, but are also present inherently due to considerable intra-and inter-rater variability. For example, multiple annotators often disagree on organ and lesion boundaries, and one can average their annotations to obtain soft label maps <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b41">41]</ref>.</p><p>This work investigates how the medical imaging community can combine the use of SDL with soft labels to reach a state of synergy. While the original SDL surrogate was posed as a relaxed form of the Dice score, naively inputting soft labels to SDL is possible (e.g. in open-source segmentation libraries <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b51">51]</ref>), but it tends to push predictions towards 0-1 outputs rather than make them resemble the soft labels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b47">47]</ref>. Consequently, the use of SDL when dealing with soft labels might not align with a user's expectations, with potential adverse effects on the Dice score, model calibration and volume estimation <ref type="bibr" target="#b2">[3]</ref>.</p><p>Motivated by this observation, we first (in Sect. 2) propose two probabilistic extensions of SDL, namely, Dice semimetric losses (DMLs). These losses satisfy the conditions of a semimetric and are fully compatible with soft labels. In a standard setting with hard labels, DMLs are identical to SDL and can safely replace SDL in existing implementations. Secondly (in Sect. 3), we perform extensive experiments on the public QUBIQ, LiTS and KiTS benchmarks to empirically confirm the potential synergy of DMLs with soft labels (e.g. averaging, LS, KD) over hard labels (e.g. majority voting, random selection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We adopt the notation from <ref type="bibr" target="#b47">[47]</ref>. In particular, we denote the predicted segmentation as ẋ ∈ {1, ..., C} p and the ground-truth segmentation as ẏ ∈ {1, ..., C} p , where C is the number of classes and p the number of pixels. For a class c, we define the set of predictions as x c = { ẋ = c}, the set of ground-truth as y c = { ẏ = c}, the union as u c = x c ∪y c , the intersection as v c = x c ∩y c , the symmetric difference (i.e., the set of mispredictions) as x c i the cardinality of the relevant set. Moreover, when the context is clear, we will drop the superscript c.</p><formula xml:id="formula_0">m c = (x c \ y c ) ∪ (y c \ x c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Existing Extensions</head><p>If we want to optimize the Dice score, hence, minimize the Dice loss Δ Dice = 1 -Dice in a continuous setting, we need to extend Δ Dice with Δ Dice such that it can take any predicted segmentation x ∈ [0, 1] p as input. Hereinafter, when there is no ambiguity, we will use x and x interchangeably.</p><p>The soft Dice loss (SDL) <ref type="bibr" target="#b42">[42]</ref> extends Δ Dice by realizing that when x, y ∈ {0, 1} p , |v| = x, y , |x| = x 1 and |y| = y 1 . Therefore, SDL replaces the set notation with vector functions:</p><formula xml:id="formula_1">Δ SDL : x ∈ [0, 1] p , y ∈ {0, 1} p → 1 - 2 x, y x 1 + y 1 . (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>The soft Jaccard loss (SJL) <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b37">37]</ref> can be defined in a similar way:</p><formula xml:id="formula_3">Δ SJL : x ∈ [0, 1] p , y ∈ {0, 1} p → 1 - x, y x 1 + y 1 -x, y .<label>(2)</label></formula><p>A major limitation of loss functions based on L 1 relaxations, including SDL, SJL, the soft Tversky loss <ref type="bibr" target="#b39">[39]</ref> and the focal Tversky loss <ref type="bibr" target="#b0">[1]</ref>, as well as those relying on the Lovasz extension, such as the Lovasz hinge loss <ref type="bibr" target="#b49">[49]</ref>, the Lovasz-Softmax loss <ref type="bibr" target="#b1">[2]</ref> and the PixIoU loss <ref type="bibr" target="#b50">[50]</ref>, is that they cannot handle soft labels <ref type="bibr" target="#b47">[47]</ref>. That is, when y is also in [0, 1] p . In particular, both SDL and SJL do not reach their minimum at x = y, but instead they drive x towards the vertices {0, 1} p <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b47">47]</ref>. Take for example y = 0.5; it is straightforward to verify that SDL achieves its minimum at x = 1, which is clearly erroneous.</p><p>Loss functions that utilize L 2 relaxations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">30]</ref> do not exhibit this problem <ref type="bibr" target="#b47">[47]</ref>, but they are less commonly employed in practice and are shown to be inferior to their L 1 counterparts <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b47">47]</ref>. To address this, Wang and Blaschko <ref type="bibr" target="#b47">[47]</ref> proposed two variants of SJL termed as Jaccard Metric Losses (JMLs). These two variants, Δ JML,1 and</p><formula xml:id="formula_4">Δ JML,2 : [0, 1] p × [0, 1] p → [0, 1] are defined as Δ JML,1 = 1 - x + y 1 -x -y 1 x + y 1 + x -y 1 , Δ JML,2 = 1 - x y 1 x y 1 + x -y 1 .<label>(3)</label></formula><p>JMLs are shown to be a metric on [0, 1] p , according to the definition below.</p><formula xml:id="formula_5">Definition 1 (Metric [8]). A mapping f : [0, 1] p ×[0, 1] p → R is called a metric if it satisfies the following conditions for all a, b, c ∈ [0, 1] p : (i) (Reflexivity). f (a, a) = 0. (ii) (Positivity). If a = b, then f (a, b) &gt; 0. (iii) (Symmetry). f (a, b) = f (b, a). (iv) (Triangle inequality). f (a, c) ≤ f (a, b) + f (b, c).</formula><p>Note that reflexivity and positivity jointly imply x = y ⇔ f (x, y) = 0, hence, a loss function that satisfies these conditions will be compatible with soft labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dice Semimetric Losses</head><p>We focus here on the Dice loss. For the derivation of the Tversky loss and the focal Tversky loss, please refer to our full paper on arXiv. Since Dice</p><formula xml:id="formula_6">= 2IoU 1+IoU ⇒ 1 -Dice = 1-IoU 2-(1-IoU) , we have Δ Dice = Δ IoU</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-Δ IoU</head><p>. There exist several alternatives to define Δ IoU , but not all of them are feasible, e.g., SJL. Generally, it is easy to verify the following proposition:</p><p>Proposition 1. Δ Dice satisfies reflexivity and positivity iff Δ IoU does.</p><p>Among the definitions of Δ IoU , Wang and Blaschko <ref type="bibr" target="#b47">[47]</ref> found only two candidates as defined in Eq. ( <ref type="formula" target="#formula_4">3</ref>) satisfy reflexivity and positivity. Following Proposition 1, we transform these two IoU losses and define Dice semimetric losses (DMLs)</p><formula xml:id="formula_7">Δ DML,1 , Δ DML,2 : [0, 1] p × [0, 1] p → [0, 1] as Δ DML,1 = 1 - x + y 1 -x -y 1 x + y 1 , Δ DML,2 = 1 - 2 xy 1 2 xy 1 + x -y 1 . (4)</formula><p>Δ Dice that is defined over integers does not satisfy the triangle inequality <ref type="bibr" target="#b10">[11]</ref>, which is shown to be helpful in KD <ref type="bibr" target="#b47">[47]</ref>. Nonetheless, we can consider a weaker form of the triangle inequality:</p><formula xml:id="formula_8">f (a, c) ≤ ρ(f (a, b) + f (b, c)).</formula><p>(</p><p>Functions that satisfy the relaxed triangle inequality for some fixed scalar ρ and conditions (i)-(iii) of a metric are called semimetrics. Δ Dice is a semimetric on {0, 1} p <ref type="bibr" target="#b10">[11]</ref>. Δ DML,1 and Δ DML,2 , which extend Δ Dice to [0, 1] p , remain semimetrics in the continuous space:</p><p>Theorem 1. Δ DML,1 and Δ DML,2 are semimetrics on [0, 1] p .</p><p>The proof can be found in Appendix A. Moreover, DMLs have properties that are similar to JMLs and they are presented as follows:</p><formula xml:id="formula_10">Theorem 2. ∀x ∈ [0, 1] p , y ∈ {0, 1} p and x ∈ {0, 1} p , y ∈ [0, 1] p , Δ SDL = Δ DML,1 = Δ DML,2 . ∃x, y ∈ [0, 1] p , Δ SDL = Δ DML,1 = Δ DML,2 . Theorem 3. ∀x, y ∈ [0, 1] p , Δ DML,1 ≤ Δ DML,2 .</formula><p>The proofs are similar to those given in <ref type="bibr" target="#b47">[47]</ref>. Importantly, Theorem 2 indicates that we can safely substitute the existing implementation of SDL with DMLs and no change will be incurred, as they are identical when only hard labels are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we provide empirical evidence of the benefits of using soft labels. In particular, using QUBIQ <ref type="bibr" target="#b29">[29]</ref>, which contains multi-rater information, we show that models trained with averaged annotation maps can significantly surpass those trained with majority votes and random selections. Leveraging LiTS <ref type="bibr" target="#b3">[4]</ref> and KiTS <ref type="bibr" target="#b15">[16]</ref>, we illustrate the synergistic effects of integrating LS and KD with DMLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>QUBIQ is a recent challenge held at MICCAI 2020 and 2021, specifically designed to evaluate the inter-rater variability in medical imaging. Following <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b41">41]</ref>, we use QUBIQ 2020, which contains 7 segmentation tasks in 4 different CT and MR datasets: Prostate (55 cases, 2 tasks, 6 raters), Brain Growth (39 cases, 1 task, 7 raters), Brain Tumor (32 cases, 3 tasks, 3 raters), and Kidney (24 cases, 1 task, 3 raters). For each dataset, we calculate the average Dice score between each rater and the majority votes in Table <ref type="table" target="#tab_0">1</ref>. In some datasets, such as Brain Tumor T2, the inter-rater disagreement can be quite substantial. In line with <ref type="bibr" target="#b23">[23]</ref>, we resize all images to 256 × 256. LiTS contains 201 high-quality CT scans of liver tumors. Out of these, 131 cases are designated for training and 70 for testing. As the ground-truth labels for the test set are not publicly accessible, we only use the training set. Following <ref type="bibr" target="#b36">[36]</ref>, all images are resized to 512×512 and the HU values of CT images are windowed to the range of <ref type="bibr">[-60, 140]</ref>. KiTS includes 210 annotated CT scans of kidney tumors from different patients. In accordance with <ref type="bibr" target="#b36">[36]</ref>, all images are resized to 512 × 512 and the HU values of CT images are windowed to the range of <ref type="bibr">[-200, 300</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We adopt a variety of backbones including ResNet50/18 <ref type="bibr" target="#b14">[15]</ref>, EfficientNetB0 <ref type="bibr" target="#b44">[44]</ref> and MobileNetV2 <ref type="bibr" target="#b40">[40]</ref>. All these models that have been pretrained on Ima-geNet <ref type="bibr" target="#b6">[7]</ref> are provided by timm library <ref type="bibr" target="#b48">[48]</ref>. We consider both UNet <ref type="bibr" target="#b38">[38]</ref> and DeepLabV3+ <ref type="bibr" target="#b4">[5]</ref> as the segmentation method.</p><p>We train the models using SGD with an initial learning rate of 0.01, momentum of 0.9, and weight decay of 0.0005. The learning rate is decayed in a poly policy with an exponent of 0.9. The batch size is set to 8 and the number of epochs is 150 for QUBIQ, 60 for both LiTS and KiTS. We leverage a mixture of CE and DMLs weighted by 0.25 and 0.75, respectively. Unless otherwise specified, we use Δ DML,1 by default.</p><p>In this work, we are mainly interested in how models can benefit from the use of soft labels. The superiority of SDL over CE has been well established in the medical imaging community <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>, and our preliminary experiments also confirm this, as shown in Table <ref type="table">5</ref> (Appendix C). Therefore, we do not include any further comparison with CE in this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>We report both the Dice score and the expected calibration error (ECE) <ref type="bibr" target="#b13">[14]</ref>. For QUBIQ experiments, we additionally present the binarized Dice score (BDice), which is the official evaluation metrics used in the QUBIQ challenge. To compute BDice, both predictions and soft labels are thresholded at different probability levels (0.1, 0.2, ..., 0.8, 0.9). We then compute the Dice score at each level and average these scores with all thresholds.</p><p>For all experiments, we conduct 5-fold cross validation, making sure that each case is presented in exactly one validation set, and report the mean values in the aggregated validation set. We perform statistical tests according to the procedure detailed in <ref type="bibr" target="#b8">[9]</ref> and highlight results that are significantly superior (with a significance level of 0.05) in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results on QUBIQ</head><p>In Table <ref type="table" target="#tab_1">2</ref>, we compare different training methods on QUBIQ using UNet-ResNet50. This comparison includes both hard labels, obtained through (i) majority votes <ref type="bibr" target="#b25">[25]</ref> and (ii) random sampling each rater's annotation <ref type="bibr" target="#b21">[22]</ref>, as well as soft labels derived from (i) averaging across all annotations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b41">41]</ref> and (ii) label smoothing <ref type="bibr" target="#b43">[43]</ref>.</p><p>In the literature <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b41">41]</ref>, annotations are usually averaged with uniform weights. We additionally consider weighting each rater's annotation by its Dice score with respect to the majority votes, so that a rater who deviates far from the majority votes receives a low weight. Note that for all methods, the Dice score and ECE are computed with respect to the majority votes, while BDice is calculated as illustrated in Sect. 3.3.</p><p>Generally, models trained with soft labels exhibit improved accuracy and calibration. In particular, averaging annotations with uniform weights obtains the highest BDice, while a weighted average achieves the highest Dice score. It is worth noting that the weighted average significantly outperforms the majority votes in terms of the Dice score which is evaluated based on the majority votes themselves. We hypothesize that this is because soft labels contain extra interrater information, which can ease the network optimization at those ambiguous regions. Overall, we find the weighted average outperforms other methods, with the exception of Brain Tumor T2, where there is a high degree of disagreement among raters.</p><p>We compare our method with state-of-the-art (SOTA) methods using UNet-ResNet50 in Table <ref type="table" target="#tab_2">3</ref>. In our method, we average annotations with uniform weights for Brain Tumor T2 and with each rater's Dice score for all other datasets. Our method, which simply averages annotations to produce soft labels obtains superior results compared to methods that adopt complex architectures or training techniques.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results on LiTS and KiTS</head><p>Wang and Blaschko <ref type="bibr" target="#b47">[47]</ref> empirically found that a well-calibrated teacher can distill a more accurate student. Concurrently, Menon et al. <ref type="bibr" target="#b28">[28]</ref> argued that the effectiveness of KD arises from the teacher providing an estimation of the Bayes class-probabilities p * (y|x) and this can lower the variance of the student's empirical loss. </p><formula xml:id="formula_11">(y|x) -f (x)]| ≤ E[|E[y|f (x)] -f (x)|].</formula><p>That is, the bias of the estimation is bounded above by the calibration error and this explains why the calibration of the teacher would be important for the student. Inspired by this, we apply a recent kernel density estimator (KDE) <ref type="bibr" target="#b35">[35]</ref> that provides consistent estimation of E[y|f (x)]. We then adopt it as a post-hoc calibration method to replace the temperature scaling to calibrate the teacher in order to improve the performance of the student. For more details of KDE, please refer to our full paper on arXiv.</p><p>In Table <ref type="table" target="#tab_3">4</ref>, we compare models trained with hard labels, LS <ref type="bibr" target="#b43">[43]</ref> and KD <ref type="bibr" target="#b16">[17]</ref> on LiTS and KiTS, respectively. For all KD experiments, we use UNet-ResNet50 as the teacher. Again, we obtain noticeable improvements in both the Dice score and ECE. It is worth noting that for UNet-ResNet18 and UNet-EfficientNetB0 on LiTS, the student's Dice score exceeds that of the teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Ablation Studies</head><p>In Table <ref type="table">6</ref> (Appendix C), we compare SDL with DMLs. For QUBIQ, we train UNet-ResNet50 with soft labels obtained from weighted average and report BDice. For LiTS and KiTS, we train UNet-ResNet18 with KD and present the Dice score. For a fair comparison, we disable KDE in all KD experiments.</p><p>We find models trained with SDL can still benefit from soft labels to a certain extent because (i) models are trained with a mixture of CE and SDL, and CE is compatible with soft labels; (ii) although SDL pushes predictions towards vertices, it can still add some regularization effects in a binary segmentation setting. However, SDL is notably outperformed by DMLs. As for DMLs, we find Δ DML,1 is slightly superior to Δ DML,2 and recommend using Δ DML,1 in practice.</p><p>In Table <ref type="table">7</ref> (Appendix C), we ablate the contribution of each KD term on LiTS and KiTS with a UNet-ResNet18 student. In the table, CE and DML represent adding the CE and DML term between the teacher and the student, respectively. In Table <ref type="table">8</ref> (Appendix C), we illustrate the effect of bandwidth that controls the smoothness of KDE. Results shown in the tables verify the effectiveness of the proposed loss and the KDE method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Future Works</head><p>In this study, our focus is on extending the Dice loss within the realm of medical image segmentation. It may be intriguing to apply DMLs in the context of longtailed classification <ref type="bibr" target="#b26">[26]</ref>. Additionally, while we employ DMLs in the label space, it holds potential for measuring the similarity of two feature vectors <ref type="bibr" target="#b17">[18]</ref>, for instance, as an alternative to cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we introduce the Dice semimetrics losses (DMLs), which are identical to the soft Dice loss (SDL) in a standard setting with hard labels, but are fully compatible with soft labels. Our extensive experiments on the public QUBIQ, LiTS and KiTS benchmarks validate that incorporating soft labels leads to higher Dice score and lower calibration error, indicating that these losses can find wide application in diverse medical image segmentation problems. Hence, we suggest to replace the existing implementation of SDL with DMLs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>), the Jaccard index as IoU c = |v c | |u c | , and the Dice score as Dice c = 2IoU c 1+IoU c = 2|v c | |x c |+|y c | . In what follows, we will represent sets as binary vectors x c , y c , u c , v c , m c ∈ {0, 1} p and denote |x c | = p i=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The number of raters and the averaged Dice score between each rater and</figDesc><table><row><cell cols="7">the majority votes for each QUBIQ dataset. D1: Prostate T1, D2: Prostate T2, D3:</cell></row><row><cell cols="7">Brain Growth T1, D4: Brain Tumor T1, D5: Brain Tumor T2, D6: Brain Tumor T3,</cell></row><row><cell>D7: Kidney T1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset D1</cell><cell>D2</cell><cell>D3</cell><cell>D4</cell><cell>D5</cell><cell>D6</cell><cell>D7</cell></row><row><cell># Raters 6</cell><cell>6</cell><cell>7</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell cols="7">Dice (%) 96.49 92.17 91.20 95.44 68.73 92.71 97.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparing hard labels with soft labels on QUBIQ using UNet-ResNet50.</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell cols="5">Majority Random Uniform Weighted LS</cell></row><row><cell></cell><cell>Dice (%)</cell><cell>95.65</cell><cell>95.80</cell><cell>95.74</cell><cell>95.99</cell><cell>95.71</cell></row><row><cell>Prostate T1</cell><cell cols="2">BDice (%) 94.72</cell><cell>95.15</cell><cell>95.19</cell><cell>95.37</cell><cell>94.91</cell></row><row><cell></cell><cell>ECE (%)</cell><cell>0.51</cell><cell>0.39</cell><cell>0.22</cell><cell>0.20</cell><cell>0.36</cell></row><row><cell></cell><cell>Dice (%)</cell><cell>89.39</cell><cell>88.87</cell><cell>89.57</cell><cell>89.79</cell><cell>89.82</cell></row><row><cell>Prostate T2</cell><cell cols="2">BDice (%) 88.31</cell><cell>88.23</cell><cell>89.35</cell><cell>89.66</cell><cell>88.85</cell></row><row><cell></cell><cell>ECE (%)</cell><cell>0.52</cell><cell>0.47</cell><cell>0.26</cell><cell>0.25</cell><cell>0.41</cell></row><row><cell></cell><cell>Dice (%)</cell><cell>91.09</cell><cell>90.65</cell><cell>90.94</cell><cell>91.46</cell><cell>91.23</cell></row><row><cell>Brain Growth</cell><cell cols="2">BDice (%) 88.72</cell><cell>88.81</cell><cell>89.89</cell><cell>90.40</cell><cell>89.88</cell></row><row><cell></cell><cell>ECE (%)</cell><cell>1.07</cell><cell>0.85</cell><cell>0.27</cell><cell>0.34</cell><cell>0.41</cell></row><row><cell></cell><cell>Dice (%)</cell><cell>86.46</cell><cell>87.24</cell><cell>87.74</cell><cell>87.78</cell><cell>87.84</cell></row><row><cell>Brain Tumor T1</cell><cell cols="2">BDice (%) 85.74</cell><cell>86.59</cell><cell>86.67</cell><cell>86.92</cell><cell>86.91</cell></row><row><cell></cell><cell>ECE (%)</cell><cell>0.62</cell><cell>0.55</cell><cell>0.38</cell><cell>0.36</cell><cell>0.37</cell></row><row><cell></cell><cell>Dice (%)</cell><cell>58.58</cell><cell>48.86</cell><cell>52.42</cell><cell>61.01</cell><cell>61.23</cell></row><row><cell>Brain Tumor T2</cell><cell cols="2">BDice (%) 38.68</cell><cell>49.19</cell><cell>55.11</cell><cell>44.23</cell><cell>40.61</cell></row><row><cell></cell><cell>ECE (%)</cell><cell>0.25</cell><cell>0.81</cell><cell>0.74</cell><cell>0.26</cell><cell>0.22</cell></row><row><cell></cell><cell>Dice (%)</cell><cell>53.54</cell><cell>54.64</cell><cell>53.45</cell><cell>56.75</cell><cell>57.01</cell></row><row><cell>Brain Tumor T3</cell><cell cols="2">BDice (%) 52.33</cell><cell>53.53</cell><cell>51.98</cell><cell>53.90</cell><cell>55.26</cell></row><row><cell></cell><cell>ECE (%)</cell><cell>0.17</cell><cell>0.17</cell><cell>0.14</cell><cell>0.09</cell><cell>0.11</cell></row><row><cell></cell><cell>Dice (%)</cell><cell>62.96</cell><cell>68.10</cell><cell>71.33</cell><cell>76.18</cell><cell>71.21</cell></row><row><cell>Kidney</cell><cell cols="2">BDice (%) 62.47</cell><cell>67.69</cell><cell>70.82</cell><cell>75.67</cell><cell>70.41</cell></row><row><cell></cell><cell>ECE (%)</cell><cell>0.88</cell><cell>0.78</cell><cell>0.67</cell><cell>0.53</cell><cell>0.62</cell></row><row><cell></cell><cell>Dice (%)</cell><cell>76.80</cell><cell>76.30</cell><cell>77.31</cell><cell>79.85</cell><cell>79.15</cell></row><row><cell>All</cell><cell cols="2">BDice (%) 72.99</cell><cell>75.59</cell><cell>77.00</cell><cell>76.59</cell><cell>75.26</cell></row><row><cell></cell><cell>ECE (%)</cell><cell>0.57</cell><cell>0.57</cell><cell>0.38</cell><cell>0.29</cell><cell>0.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparing SOTA methods with ours on QUBIQ using UNet-ResNet50. All results are BDice (%).</figDesc><table><row><cell>Dataset</cell><cell cols="5">Dropout [10] Multi-head [13] MRNet [23] SoftSeg [12, 25] Ours</cell></row><row><cell>Prostate T1</cell><cell>94.91</cell><cell>95.18</cell><cell>95.21</cell><cell>95.02</cell><cell>95.37</cell></row><row><cell>Prostate T2</cell><cell>88.43</cell><cell>88.32</cell><cell>88.65</cell><cell>88.81</cell><cell>89.66</cell></row><row><cell>Brain Growth</cell><cell>88.86</cell><cell>89.01</cell><cell>89.24</cell><cell>89.36</cell><cell>90.40</cell></row><row><cell cols="2">Brain Tumor T1 85.98</cell><cell>86.45</cell><cell>86.33</cell><cell>86.41</cell><cell>86.92</cell></row><row><cell cols="2">Brain Tumor T2 48.04</cell><cell>51.17</cell><cell>51.82</cell><cell>52.56</cell><cell>55.11</cell></row><row><cell cols="2">Brain Tumor T3 52.49</cell><cell>53.68</cell><cell>54.22</cell><cell>52.43</cell><cell>53.90</cell></row><row><cell>Kidney</cell><cell>66.53</cell><cell>68.00</cell><cell>68.56</cell><cell>69.83</cell><cell>75.67</cell></row><row><cell>All</cell><cell>75.03</cell><cell>75.97</cell><cell>76.18</cell><cell>76.34</cell><cell>78.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparing hard labels with LS and KD on LiTS and KiTS.In line with these findings, in Appendix B, we prove |E[p *</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Metric</cell><cell>LiTS Hard LS</cell><cell>KD</cell><cell>KiTS Hard LS</cell><cell>KD</cell></row><row><cell>UNet</cell><cell>ResNet50</cell><cell cols="3">Dice (%) 59.79 60.59 -ECE (%) 0.51 0.49 -</cell><cell cols="2">72.66 73.92 -0.39 0.33 -</cell></row><row><cell>UNet</cell><cell>ResNet18</cell><cell cols="5">Dice (%) 57.92 58.60 60.30 67.96 69.09 71.34 ECE (%) 0.52 0.48 0.50 0.44 0.38 0.44</cell></row><row><cell>UNet</cell><cell>EfficientNetB0</cell><cell cols="5">Dice (%) 56.90 57.66 60.11 70.31 71.12 71.73 ECE (%) 0.56 0.47 0.52 0.39 0.35 0.39</cell></row><row><cell>UNet</cell><cell>MobileNetV2</cell><cell cols="5">Dice (%) 56.16 57.20 58.92 67.46 68.19 68.85 ECE (%) 0.54 0.48 0.50 0.42 0.38 0.41</cell></row><row><cell cols="2">DeepLabV3+ ResNet18</cell><cell cols="5">Dice (%) 56.10 57.07 59.12 69.95 70.61 70.80 ECE (%) 0.53 0.50 0.52 0.40 0.38 0.40</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We acknowledge support from the <rs type="funder">Research Foundation -Flanders (FWO)</rs> through project numbers <rs type="grantNumber">G0A1319N</rs> and <rs type="grantNumber">S001421N</rs>, and funding from the <rs type="funder">Flemish Government</rs> under the <rs type="institution">Onderzoeksprogramma Artificiële Intelligentie (AI) Vlaanderen</rs> programme. The resources and services used in this work were provided by the <rs type="funder">VSC (Flemish Supercomputer Center)</rs>, funded by the <rs type="funder">Research Foundation -Flanders (FWO)</rs> and the <rs type="funder">Flemish Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mU66Hrw">
					<idno type="grant-number">G0A1319N</idno>
				</org>
				<org type="funding" xml:id="_zVTeksR">
					<idno type="grant-number">S001421N</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_46.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A novel focal Tversky loss function with improved attention U-Net for lesion segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISBI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Lovasz-softmax loss: a tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theoretical analysis and experimental validation of volume bias of soft dice optimized segmentation maps in the context of inherent uncertainty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bertels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suetens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIA</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101833</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The liver tumor segmentation benchmark (LiTS)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIA</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">102680</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01234-2_49" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">MMSegmentation: OpenMMLab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Encyclopedia of Distances</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Deza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deza</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-00234-2</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-00234-2" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimization for medical image segmentation: theory and practice when evaluating with dice score or Jaccard index</title>
		<author>
			<persName><forename type="first">T</forename><surname>Eelbode</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3679" to="3690" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relaxed triangle inequality ratio of the Sørensen-Dice and Tversky indexes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gragera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Suppakitpaisarn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCS</title>
		<imprint>
			<biblScope unit="volume">718</biblScope>
			<biblScope unit="page" from="37" to="45" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SoftSeg: advantages of soft versus binary training for image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lemay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIA</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102038</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Who said what: modeling individual labelers improves classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: results of the KiTS19 challenge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Heller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIA</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101821</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Masked distillation with receptive tokens</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Iakubovskii</surname></persName>
		</author>
		<ptr target="https://github.com/qubvel/segmentation_models.pytorch" />
		<title level="m">Segmentation models PyTorch</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Meth</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatially varying label smoothing: capturing uncertainty from expert annotations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPMI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving uncertainty estimation in convolutional neural networks using inter-rater agreement</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Jørgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jalaboi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="540" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_59</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-9_59" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning calibrated medical image segmentation via multi-rater agreement modeling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Segment anything</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Label fusion and training methods for reliable representation of inter-rater uncertainty</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lemay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Karthik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MELBA</title>
		<imprint>
			<biblScope unit="volume">031</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dice loss for data-imbalanced NLP tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Metrics reloaded: recommendations for image analysis validation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Statistical Perspective on Distillation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quantification of uncertainties in biomedical image quantification challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Becker</surname></persName>
		</author>
		<ptr target="https://qubiq.grand-challenge.org" />
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">3D</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">When does label smoothing help?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Noisy image segmentation with soft-dice</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nordström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Löfman</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimal decisions from probabilistic models: the intersection-overunion case</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">On the relationship between calibrated predictors and unbiased volume estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Popordanoska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bertels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A consistent and differentiable Lp canonical calibration error estimator</title>
		<author>
			<persName><forename type="first">T</forename><surname>Popordanoska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient medical image segmentation based on knowledge distillation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optimizing intersection-over-union in deep neural networks for image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-50835-1_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-50835-1_22" />
	</analytic>
	<monogr>
		<title level="m">ISVC 2016</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">10072</biblScope>
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Tversky loss function for image segmentation using 3D fully convolutional deep networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MICCAI Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">MobileNetV2: inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Using soft labels to model uncertainty in medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>MICCAI Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MICCAI Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">EfficientNet: rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The dice loss in the context of missing or empty labels: introducing Φ and</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tilborghs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bertels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Maes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4757-3264-1</idno>
		<ptr target="https://doi.org/10.1007/978-1-4757-3264-1" />
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Jaccard metric losses: optimizing the Jaccard index with soft labels</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The Lovász hinge: a novel convex surrogate for submodular losses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="735" to="748" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning generalized intersection over union for dense pixelwise prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deep learning for medical image segmentation: tricks, challenges and future directions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
