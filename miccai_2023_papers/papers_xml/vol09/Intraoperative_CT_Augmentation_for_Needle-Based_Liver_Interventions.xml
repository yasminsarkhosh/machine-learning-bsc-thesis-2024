<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intraoperative CT Augmentation for Needle-Based Liver Interventions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sidaty</forename><surname>El Hadramy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<address>
									<settlement>Strasbourg</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><surname>Verde</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<address>
									<settlement>Strasbourg</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Stéphane</forename><surname>Cotin</surname></persName>
							<email>stephane.cotin@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intraoperative CT Augmentation for Needle-Based Liver Interventions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF5A4F671DAFCA14A993502EE756EA6A</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_28</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Liver tumor ablation</term>
					<term>Needle-based procedures</term>
					<term>Patient-specific interventions</term>
					<term>CT-guidance</term>
					<term>Medical image augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the need for improved CT-guidance during needle-based liver procedures (i.e., tumor ablation), while reduces the need for contrast agent injection during such interventions. To achieve this objective, we augment the intraoperative CT with the preoperative vascular network deformed to match the current acquisition. First, a neural network learns local image features in a non-contrasted CT image by leveraging the known preoperative vessel tree geometry and topology extracted from a matching contrasted CT image. Then, the augmented CT is generated by fusing the labeled vascular tree and the non-contrasted intraoperative CT. Our method is trained and validated on porcine data, achieving an average dice score of 0.81 on the predicted vessel tree instead of 0.51 when a medical expert segments the non-contrasted CT. In addition, vascular labels can also be transferred to provide additional information. Source code of this work is publicly available at https://github.com/Sidaty1/Intraoperative CT augmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Needle-based liver tumor ablation techniques (e.g., radiofrequency, microwave, laser, cryoablation) have a great potential for local curative tumor control <ref type="bibr" target="#b0">[1]</ref>, with comparable results to surgery in the early stages for both primary and secondary cancers. Furthermore, as it is minimally invasive, it has a low rate of major complications and procedure-specific mortality, and is tissue-sparing, thus, its indications are growing exponentially and extending the limits to more advanced tumors <ref type="bibr" target="#b2">[3]</ref>. CT-guidance is a widely used imaging modality for placing the needles, monitoring the treatment, and following up patients. However, it is limited by the exposure to ionizing radiation and the need for intravenous injection of contrast agents to visualize the intrahepatic vessels and the target tumor(s).</p><p>In standard clinical settings, the insertion of each needle requires multiple check points during its progression, fine-tune maneuvers, and eventual repositioning. This leads to multiple CT acquisitions to control the progression of the needle with respect to the vessels, the target, and other sensible structures <ref type="bibr" target="#b25">[26]</ref>. However, intrahepatic vessels (and some tumors) are only visible after contrast-enhancement, which has a short lifespan and dose-related deleterious kidney effects. It makes it impossible to perform each of the control CT acquisitions under contrast injection. A workaround to shortcut these limitations is to perform an image fusion between previous contrasted and intraoperative noncontrasted images. However, such a solution is only available in a limited number of clinical settings, and the registration is only rigid, usually deriving into bad results. In this work, we propose a method for visualizing intrahepatic structures after organ motion and needle-induced deformations, in non-injected images, by exploiting image features that are generally not perceivable by the human eye in common clinical workflows.</p><p>To address this challenge, two main strategies could be considered: image fusion and image processing techniques. Image fusion typically relies on the estimation of rigid or non-rigid transformations between 2 images, to bring into the intraoperative image structures of interest only visible in the preoperative data. This process is often described as an optimization problem <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> which can be computationally expensive when dealing with non-linear deformations, making their use in a clinical workflow limited. Recent deep learning approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref> have proved to be a successful alternative to solve image fusion problems, even when a large non-linear mapping is required. When ground-truth displacement fields are not known, state-of-the-art methods use unsupervised techniques, usually an encoder-decoder architecture <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>, to learn the unknown displacement field between the 2 images. However, such unsupervised methods fail at solving our problem due to lack of similar image features between the contrasted (CCT) and non-contrasted (NCCT) image in the vascular tree region (see Sect. <ref type="bibr">3.3)</ref>.</p><p>On the other hand, deep learning techniques have proven to be very efficient at solving image processing challenges <ref type="bibr" target="#b14">[15]</ref>. For instance, image segmentation <ref type="bibr" target="#b15">[16]</ref>, image style transfer <ref type="bibr" target="#b16">[17]</ref>, or contrast-enhancement to cite a few. Yet, segmenting vessels from non-contrasted images remains a challenge for the medical imaging community <ref type="bibr" target="#b15">[16]</ref>. Style transfer aims to transfer the style of one image to another while preserving its content <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. However, applying such methods to generate a contrasted intraoperative CT is not a sufficiently accurate solution for the problem that we address. Contrast-enhancement methods could be an alternative. In the method proposed by Seo et al. <ref type="bibr" target="#b19">[20]</ref>, a deep neural network synthesizes contrast-enhanced CT from non contrast-enhanced CT. Nevertheless, results obtained by this method are not sufficiently robust and accurate to provide an augmented intraoperative CT on which needle-based procedures can be guided.</p><p>In this paper we propose an alternative approach, where a neural network learns local image features in a NCCT image by leveraging the known preoperative vessel tree geometry and topology extracted from a matching (undeformed) CCT. Then, the augmented CT is generated by fusing the deformed vascular tree with the non-contrasted intraoperative CT. Section 2 presents the method and its integration in the medical workflow. Section 3 presents and discusses the results, and finally we conclude in Sect. 4 and highlight some perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we present our method and its compatibility with current clinical workflows. A few days or a week before the intervention, a preoperative diagnostic multiphase contrast-enhanced image (MPCECT) is acquired (Fig. <ref type="figure" target="#fig_0">1</ref>, yellow box). The day of the intervention, a second MPCECT image is acquired before starting the needle insertion, followed by a series of standard, non-injected acquisitions to guide the needle insertion (Fig. <ref type="figure" target="#fig_0">1</ref>, blue box). Using such a noncontrasted intraoperative image as input, our method performs a combined non-rigid registration and augmentation of the intraoperative CT by adding anatomical features (mainly intrahepatic vessels and tumors) from the preoperative image to the current image. To achieve this result, our method only requires to process and train on the baseline MPCECT image (Fig. <ref type="figure" target="#fig_0">1,</ref><ref type="figure">red box</ref>). An overview of the method is shown in the Fig <ref type="figure" target="#fig_1">2</ref> and the following sections describe its main steps.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vessel Map Extraction</head><p>We call Vessel Map (VM) the region of interest defining the vascular tree in the NCCT. Since vascular structures are not visible in non-contrasted images, the extraction of this map is done by segmenting the CCT and then using this segmentation as a mask in the NCCT. Mathematical morphology operators, in particular a dilation operation <ref type="bibr" target="#b22">[23]</ref>, are performed on the segmented region of interest to slightly increase its dimensions. This is needed to compensate for segmentation errors and the slight anatomical motion that may exist between the contrasted and non-contrasted image acquisitions. In practice, the acquisition protocols limit the shift between the NCCT and CCT acquisitions, and only a few sequential dilation operations are needed to ensure we capture the true vessel fingerprint in the NCCT image. Note that the resulting vessel map is not a binary mask, but a subset of the image limited to the volume covered by the vessels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Augmentation</head><p>The preoperative MPCECT provides a couple of registered NCCT and CCT images. This is obviously not sufficient for training purposes, as they do not represent the possible soft tissue deformation that may occur during the procedure. Therefore, we augment the data set by applying multiple random deformations to the original images. Random deformations are created by considering a predefined set of control points for which we define a displacement field with a random normal distribution. The displacement field of the full volume is then obtained by linearly interpolating the control points' displacement field to the rest of the volume. All the deformations are created using the same number of control points and characteristics of the normal distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Neural Network</head><p>Predicting the vascular tree location in the deformed intraoperative NCCT is done using a U-net <ref type="bibr" target="#b4">[5]</ref> architecture. The neural network takes as input the preoperative vessel map and the intraoperative NCCT, and outputs the intraoperative vessel map. Our network learns to find the image features (or vessel fingerprint) present in the vessel map, in a given NCCT assuming the knowledge of its geometry, topology, and the distribution of contrast from the preoperative MPCECT. The architecture of our network is illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. It consists of a four layers analysis (left side) and synthesis (right side) paths that provide a non-linear mapping between low resolution input and output images. Both paths include four 3 × 3 × 3 unpadded convolutions, each followed by a Leaky Rectified Linear Unit (LeakyReLU) activation function. The analysis includes a 2 × 2 × 2 max pooling with a stride of 1, while the synthesis follows each convolution by a 2 × 2 × 2 up-convolution with a stride of 1. Shortcut connections from layers of equal resolution in the analysis path provide the essential high-resolution features to the synthesis path. In the last layer, a 1 × 1 × 1 convolution reduces the number of output channels to one, yielding the vessel map in the intraoperative image. Our network is trained by minimizing the mean square error between the predicted and ground truth vessel map. Training details are presented in Sect. 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Augmented CT</head><p>Once the network has been trained on the patient-specific preoperative data, the next step is to augment and visualize the intraoperative NCCT. This is done in 3 steps:</p><p>-The dilatation operations introduced in Sect. 2.1 are not reversible (i.e. the segmented vessel tree cannot be recovered from the VM by applying the same number of erosion operations). Also, neighboring branches in the vessel tree could end up being fused, thus changing the topology of the vessel map. Therefore, to retrieve the correct segmented (yet deformed) vascular tree, we compute a displacement field between the pre-and intraoperative VMs. This is done with the Elastix library <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The resulting displacement field is applied on the preoperative segmentation to retrieve the intraoperative vessel tree segmentation. This is illustrated in Fig. <ref type="figure">4</ref>. -The augmented image is obtained by fusing the predicted intraoperative segmentation with the intraoperative NCCT image. The augmented vessels are displayed in green to ensure the clinician is aware this is not a true CCT image (see Fig. <ref type="figure">5</ref>). -It is also possible to add anatomical labels to the intraoperative augmented CT to further assist the clinician. To achieve this objective, we compute a graph data structure from the preoperative segmentation. We first extract the vessel centerlines as described in <ref type="bibr" target="#b3">[4]</ref>. To define the associated graph structure, we start by selecting all branches with either no parent or no children. The branch with the highest radius is then selected as the root edge. An oriented graph is created using a Breadth First Search algorithm starting from the root edge. Nodes and edges correspond respectively to vessel tree bifurcations and branches. We use the graph structure to associate each anatomical label (manually defined) with a Strahler <ref type="bibr" target="#b5">[6]</ref> graph ordering. The same process is applied to the predicted intraoperative segmentation. This makes it possible to correctly map the preoperative anatomical labels (e.g. vessel name) and display them on the augmented image.</p><p>Fig. <ref type="figure">4</ref>. This figure illustrates the different stages of the pipeline adopted to generate the VM and show how the vessel tree topology is retrieved from the predicted intraoperative VM by computing a displacement field between the preoperative VM and the predicted VM. This field is applied to the preoperative segmentation to get the intraoperative one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>To validate our approach, 4 couples of MPCECT abdominal porcine images were acquired from 4 different subjects. For a given subject, each couple corresponds to a preoperative and an intraoperative MPCECT. We recall that an MPCECT contains a set of registered NCCT and CCT images. These images are then cropped and down-sampled to 256 × 256 × 256, and the voxels intensities are scaled between 0 and 255. Finally, we extract the VM from each MPCECT sample and apply 3 dilation operations, which demonstrated the best performance in terms of prediction accuracy and robustness on our data. We note that public data sets such as DeepLesion <ref type="bibr" target="#b23">[24]</ref>, 3Dircadb-01 <ref type="bibr" target="#b24">[25]</ref> and others do not fit our problem since they do not include the NCCT images. Aiming at a patientspecific prediction, we only train on a "subject" at a time. For a given subject, we generate 100 displacement fields using the data augmentation strategy explained above with 50 voxels for the control points spacing in the three spatial directions and a standard deviation of 5 voxels for the normal distributions. The resulting deformation is applied to the preoperative MPCECT and its corresponding VM. Thus, we end up with a set of 100 triplets (NCCT, CCT and VM). Two out of the 100 triplets are used for each training batch, where one is considered as the pre-operative MPCECT and the other as the intraoperative one. This makes it possible to generate up to 4950 training and validation samples. The intraoperative MPCECT of the same subject is used to test the network. Our method is implemented in Tensorflow 2.4, on a GeForce RTX 3090. We use an Adam optimizer (β 1 = 0.001, β 2 = 0.999) with a learning rate of 10 -4 . The training process converges in about 1,000 epochs with a batch size of 1 and 200 steps per epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>To assess our method, we use a dice score to measure the overlap between our predicted segmentation and the ground truth. Being a commonly used metric for segmentation problems, Dice aligns the nature of our problem as well as the clinical impact of our solution. We ha performed tests on 4 different (porcine) data sets. Results are reported in Table <ref type="table" target="#tab_0">1</ref>. The method achieved a mean dice score of 0.81. An example of a subject intraoperative augmented CT is illustrated in Fig. <ref type="figure">5</ref>, where the three images correspond respectively to the initial non injected CT, the augmented CT without and with labels. Figure <ref type="figure">6</ref> illustrates the results of our method for Subject 1. The green vessels correspond to the ground truth intraoperative segmentation, the orange ones to the predicted intraoperative segmentation and finally the gray vessel tree corresponds to the preoperative CCT vessel tree. Such results demonstrate the ability of our method to perform very well even in the presence of large deformations.  Qualitative Assessment: To further demonstrate the value of our method, we have asked two clinicians to manually segment the NCCT images in the intraoperative MPCECT data. Their results (mean and standard deviation) are reported in Table <ref type="table" target="#tab_0">1</ref>. Our method outperforms the results of both clinicians, with an average dice score of 0.81 against 0.51 as a mean for the clinical experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study and Additional Results</head><p>Vessel Map: We have removed the VM from the network input to demonstrate its impact on our results. Using the data of the Subject 1, a U-net was trained to segment the vessel tree of the intraoperative NCCT image. The network only managed to segment a small portion of the main portal vein branch. Thus, achieving a dice score of 0.16 vs 0.79 when adding the preoperative VM as additional input. We also studied the influence of the diffusion kernel applied to the initial segmentation. We have seen, on our experimental data, that 3 dilation operations were sufficient to compensate for the possible motion between NCCT and CCT acquisitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with VoxelMorph:</head><p>The problem that we address can be seen from different angles. In particular, we could attempt to solve it by registering the preoperative NCCT to the intraoperative one and then applying the resulting displacement field to the known preoperative segmentation. However, state-of-the-art registration methods such as VoxelMorph <ref type="bibr" target="#b6">[7]</ref> and others do not necessarily guarantee a diffeomorphic <ref type="bibr" target="#b7">[8]</ref> displacement field that ensures the continuity of the displacement field inside the parenchyma where the intensity is quite homogeneous on the NCCT. To assess this assumption, a VoxelMorph<ref type="foot" target="#foot_0">1</ref> network was trained on the Subject 1 of our porcine data sets. We trained the network with both MSE and smoothness losses during 100 epochs and given a batch of size 4. Results are illustrated below in Fig. <ref type="figure" target="#fig_4">7</ref>. While the VoxelMorph network accurately registers the liver shape, the displacement field is almost null in the region of vessels inside the parenchyma. Therefore, the preoperative vessel segmentation is not correctly transferred into the intraoperative image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed a method for augmenting intra-operative NCCT images as a means to improve needle CT-guided techniques while reducing the need for contrast agent injection during tumor ablation procedures, or other needle-based procedures. Our method uses a U-net architecture to learn local vessel tree image features in the NCCT by leveraging the known vessel tree geometry and topology extracted from a matching CCT image. The augmented CT is generated by fusing the predicted vessel tree with the NCCT. Our method is validated on several porcine images, achieving an average dice score of 0.81 on the predicted vessel tree location. In addition, it demonstrates robustness even in the presence of large deformations between the preoperative and intraoperative images. Our future steps will essentially involve applying this method to patient data and perform a small user study to evaluate the usefulness and limitations of our approach.</p><p>Aknowledgments. This work was partially supported by French state funds managed by the ANR under reference ANR-10-IAHU-02 (IHU Strasbourg). The authors would like to thank Paul Baksic and Robin Enjalbert for proofreading the manuscript.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Integration of our method in the clinical workflow. The neural network trained on preoperative MPCECT avoids contrast agent injections during the intervention.</figDesc><graphic coords="3,60,96,463,52,330,94,77,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The neural network takes as input the preoperative vessel map (VM) and the intraoperative NCCT, and outputs the intraoperative vessel map (VM) from which we extract the deformed vascular tree. Finally, the augmented CT is created by fusing the segmented image and labels with the intraoperative NCCT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Our neural network uses a four-path encoder-decoder architecture and takes as input a two-channel image corresponding to the intraoperative NCCT image concatenated with the preoperative vessel map. The output is the intraoperative vessel map.</figDesc><graphic coords="5,72,96,100,91,306,64,101,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. In this figure we show the original NCCT on the left. The middle image shows the augmented CT with the predicted vessel tree (in green). The rightmost image shows the augmented image with anatomical labels transferred from the preoperative image segmentation and labelling. (Color figure online)</figDesc><graphic coords="8,48,30,206,09,327,10,85,54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Illustration of Voxelmorph registration between NCCT preoperative and intraoperative images. The prediction is the output of VoxelMorph method. DF stands for the displacement fields on x and y predicted by VoxelMorph method.</figDesc><graphic coords="9,57,96,208,01,336,40,58,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>This table presents our results over 4 subjects in terms of dice score. For each subject the network was trained on the preoperative MPCECT and tested on the intraoperative MPCECT. We achieve a mean dice score of 0.81 vs. 0.51 for the clinical experts.</figDesc><table><row><cell>Dice score</cell><cell cols="5">Subject 1 Subject 2 Subject 3 Subject 4 Mean Std</cell></row><row><cell>Ours</cell><cell>0.8</cell><cell>0.77</cell><cell>0.79</cell><cell>0.90</cell><cell>0.81 0.04</cell></row><row><cell cols="2">Expert clinician 0.52</cell><cell>0.45</cell><cell>0.53</cell><cell>0.52</cell><cell>0.51 0.03</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/voxelmorph/voxelmorph.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Risk factors for distant recurrence of hepatocellular carcinoma in the liver after complete coagulation by microwave or radiofrequency ablation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Izumi</surname></persName>
		</author>
		<idno type="PMID">11251946</idno>
	</analytic>
	<monogr>
		<title level="j">Cancer</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="949" to="956" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Early-stage hepatocellular carcinoma in patients with cirrhosis: long-term results of percutaneous image-guided radiofrequency ablation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lencioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">234</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="961" to="967" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Safety and efficacy of stereotactic radiofrequency ablation for very large (gt 8 cm) primary and metastatic liver tumors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schullian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Putzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Laimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1618</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Centerline computation and geometric analysis of branching tubular surfaces with application to blood vessel modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ene-Iordache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WSCG</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the Horton-Strahler number for random tries</title>
		<author>
			<persName><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informatique théorique et Applications/Theoretical Informaties and Applications</title>
		<imprint>
			<biblScope unit="volume">305</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="443" to="456" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">VoxelMorph: a learning framework for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1788" to="1800" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large deformation diffeomorphic metric mapping of vector fields</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Winslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1216" to="1230" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Elastix: a toolbox for intensity-based medical image registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="196" to="205" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A reproducible evaluation of ants similarity metric performance in brain image registration</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2033" to="2044" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning based inter-modality image registration supervised by intra-modality similarity</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00919-9_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00919-97" />
	</analytic>
	<monogr>
		<title level="m">MLMI 2018</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11046</biblScope>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Quicksilver: fast predictive image registration-a deep learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="378" to="396" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CycleMorph: cycle consistent unsupervised deformable image registration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102036</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonrigid image registration using multi-scale 3D convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sokooti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P F</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66182-7_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66182-727" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Duchesne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10433</biblScope>
			<biblScope unit="page" from="232" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A review on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suganyadevi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Seethalakshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Balasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Multimed. Info. Retr</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="19" to="38" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A review of medical image segmentation algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Rajest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EAI Endorsed Trans Perv Health Tech</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<title level="m">A neural algorithm of artistic style</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46475-6_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46475-6" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural contrast enhancement of CT image</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting><address><addrLine>Waikoloa, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">elastix: a toolbox for intensity based medical image registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P W</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="196" to="205" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast parallel image registration on CPU and GPU for diagnostic classification of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Shamonin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Bron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P F</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neuroinform</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image analysis using mathematical morphology</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Sternberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="550" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JMI.5.3.036501</idno>
		<ptr target="https://doi.org/10.1117/1.JMI.5.3.036501" />
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging (Bellingham)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36501</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">3D image reconstruction for comparison of algorithm database: a patient specific anatomical and medical image database</title>
		<author>
			<persName><forename type="first">L</forename><surname>Soler</surname></persName>
		</author>
		<idno>IRCAD</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Strasbourg, France</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">System for CT-guided needle placement in the thorax and abdomen: a design for clinical acceptability, applicability and usability: system for CT-guided needle placement in the thorax and abdomen</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Arnolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buijze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A M J</forename><surname>Broeders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Robot. Comput. Assist. Surg</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1877</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
