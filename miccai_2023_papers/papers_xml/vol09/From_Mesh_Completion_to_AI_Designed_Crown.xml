<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Mesh Completion to AI Designed Crown</title>
				<funder>
					<orgName type="full">Kerenor Dental Studio, Intellident Dentaire Inc.</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Golriz</forename><surname>Hosseinimanesh</surname></persName>
							<email>golriz.hosseinimanesh@polymtl.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Polytechnique Montréal University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Farnoosh</forename><surname>Ghadiri</surname></persName>
							<email>farnoosh.ghadiri@jacobb.ca</email>
							<idno type="ORCID">0000-0002-7232-1888</idno>
							<affiliation key="aff1">
								<orgName type="laboratory">Centre d&apos;intelligence artificielle appliquée (JACOBB)</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francois</forename><surname>Guibault</surname></persName>
							<email>francois.guibault@polymtl.ca</email>
							<idno type="ORCID">0000-0002-3934-4631</idno>
							<affiliation key="aff0">
								<orgName type="institution">Polytechnique Montréal University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Farida</forename><surname>Cheriet</surname></persName>
							<email>farida.cheriet@polymtl.ca</email>
							<idno type="ORCID">0000-0001-6170-5627</idno>
							<affiliation key="aff0">
								<orgName type="institution">Polytechnique Montréal University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julia</forename><surname>Keren</surname></persName>
							<idno type="ORCID">0000-0001-6170-5627</idno>
							<affiliation key="aff2">
								<orgName type="institution">Intellident Dentaire Inc</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">From Mesh Completion to AI Designed Crown</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">905633F31C0DDB5C371A75E8EDA797D6</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_53</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Mesh completion</term>
					<term>Transformer</term>
					<term>3D shape generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Designing a dental crown is a time-consuming and laborintensive process. Our goal is to simplify crown design and minimize the tediousness of making manual adjustments while still ensuring the highest level of accuracy and consistency. To this end, we present a new end-to-end deep learning approach, coined Dental Mesh Completion (DMC), to generate a crown mesh conditioned on a point cloud context. The dental context includes the tooth prepared to receive a crown and its surroundings, namely the two adjacent teeth and the three closest teeth in the opposing jaw. We formulate crown generation in terms of completing this point cloud context. A feature extractor first converts the input point cloud into a set of feature vectors that represent local regions in the point cloud. The set of feature vectors is then fed into a transformer to predict a new set of feature vectors for the missing region (crown). Subsequently, a point reconstruction head, followed by a multi-layer perceptron, is used to predict a dense set of points with normals. Finally, a differentiable point-to-mesh layer serves to reconstruct the crown surface mesh. We compare our DMC method to a graph-based convolutional neural network which learns to deform a crown mesh from a generic crown shape to the target geometry. Extensive experiments on our dataset demonstrate the effectiveness of our method, which attains an average of 0.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>If a tooth is missing, decayed, or fractured, its treatment may require a dental crown. Each crown must be customized to the individual patient in a process, as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. The manual design of these crowns is a time-consuming and labor-intensive task, even with the aid of computer-assisted design software.</p><p>Designing natural grooves and ensuring proper contact points with the opposing jaw present significant challenges, often requiring technicians to rely on trial and error. As such, an automated approach capable of accelerating this process and generating crowns with comparable morphology and quality to that of a human expert would be a groundbreaking advancement for the dental industry. A limited number of studies have focused on how to automate dental crown designs. In <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b1">3]</ref>, a conditional generative adversarial based network (GAN) is applied to a 2D depth image obtained from a 3D scan to generate a crown for a prepared tooth. Depth images created from a 3D scan can be used directly with 2D convolutional neural networks (CNNs) such as pix2pix <ref type="bibr" target="#b2">[4]</ref>, which are wellestablished in computer vision. However, depth images are limited in their ability to capture fine-grained details and can suffer from noise and occlusion issues. By contrast, point clouds have the advantage of being able to represent arbitrary 3D shapes and can capture fine-grained details such as surface textures and curvatures. <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b6">8]</ref> use point cloud-based networks to generate crowns in the form of 3D point clouds. Input point clouds used by <ref type="bibr" target="#b3">[5]</ref> are generated by randomly removing a tooth from a given jaw; then the network estimates the missing tooth by utilizing a feature points-based multi-scale generating network. <ref type="bibr" target="#b6">[8]</ref> propose a more realistic setting by generating a crown for a prepared tooth instead of a missing tooth. They also incorporate margin line information extracted from the prepared tooth in their network to have a more accurate prediction in the margin line area. The crowns generated by both approaches are represented as point clouds, so another procedure must convert these point clouds into meshes. Creating a high-quality mesh that accurately represents the underlying point cloud data is a challenging task which is not addressed by these two works. <ref type="bibr" target="#b4">[6]</ref> proposed a transformer-based network to generate a surface mesh of the crown for a missing tooth. They use two separate networks, one responsible for generating a point cloud and the other for reconstructing a mesh given the crown generated by the first network. Similar to <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b6">8]</ref>, the point completion network used by <ref type="bibr" target="#b4">[6]</ref> only uses the Chamfer Distance (CD) loss to learn crown features. This metric's ability to capture shape details in point clouds is limited by the complexity and density of the data.</p><p>Although all aforementioned methods are potentially applicable to the task of dental crown design, most of them fail to generate noise-free point clouds, which is critical for surface reconstruction. One way to alleviate this problem is to directly generate a crown mesh. In <ref type="bibr" target="#b26">[28]</ref>, the authors develop a deep learningbased network that directly generates personalized cardiac meshes from sparse contours by iteratively deforming a template mesh, mimicking the traditional 3D shape reconstruction method. To our knowledge, however, the approach in <ref type="bibr" target="#b26">[28]</ref> has not been applied to 3D dental scans.</p><p>In this paper, we introduce Dental Mesh Completion (DMC), a novel endto-end network for directly generating dental crowns without using generic templates. The network employs a transformer-based architecture with self-attention to predict features from a 3D scan of dental preparation and surrounding teeth. These features deform a 2D fixed grid into a 3D point cloud, and normals are computed using a simple MLP. A differentiable point-to-mesh module reconstructs the 3D surface. The process is supervised using an indicator grid function and Chamfer loss from the target crown mesh and point cloud. Extensive experiments validate the effectiveness of our approach, showing superior performance compared to existing methods as measured by the CD metric. In summary, our main contributions include proposing the first end-to-end network capable of generating crown meshes for all tooth positions, employing a non-templatebased method for mesh deformation (unlike previous works), and showcasing the advantages of using a differentiable point-to-mesh component to achieve highquality surface meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the field of 3D computer vision, completing missing regions of point clouds or meshes is a crucial task for many applications. Various methods have been proposed to tackle this problem. Since the introduction of PointNet <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b12">14]</ref>, numerous methods have been developed for point cloud completion <ref type="bibr" target="#b10">[12]</ref>. The recent works PoinTr <ref type="bibr" target="#b9">[11]</ref> and SnowflakeNet <ref type="bibr" target="#b14">[16]</ref> leverage a transformer-based architecture with geometry-aware blocks to generate point clouds. It is hypothesized that using transformers preserves detailed information for point cloud completion. Nonetheless, the predicted point clouds lack connections between points, which complicates the creation of a smooth surface for mesh reconstruction.</p><p>Mesh completion methods are usually useful when there are small missing regions or large occlusions in the original mesh data. Common approaches based on geometric priors, self-similarity, or patch encoding can be used to fill small missing regions, as demonstrated in previous studies <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b25">27]</ref>, but are not suitable for large occlusions. <ref type="bibr" target="#b16">[18]</ref> propose a model-based approach that can capture the variability of a particular shape category and enable the completion of large missing regions. However, the resulting meshes cannot achieve the necessary precision required by applications such as dental crown generation. Having a mesh prior template can also be a solution to generate a complete mesh given a sparse point cloud or a mesh with missing parts. In <ref type="bibr" target="#b26">[28]</ref>, cardiac meshes are reconstructed from sparse point clouds using several mesh deformation blocks. Their network can directly generate 3D meshes by deforming a template mesh under the guidance of learned features.</p><p>We combine the advantages of point cloud completion techniques with a differentiable surface reconstruction method to generate a dental mesh. Moreover, we used the approach in <ref type="bibr" target="#b26">[28]</ref> to directly produce meshes from 3D dental points and compared those results with our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>Our method is an end-to-end supervised framework to generate a crown mesh conditioned on a point cloud context. The overview of our network is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. The network is characterized by two main components: a transformer encoder-decoder architecture and a mesh completion layer. The following sections explain each part of the network. Transformer Encoder-Decoder. We adapt the transformer encoder-decoder architecture from <ref type="bibr" target="#b9">[11]</ref> to extract global and local 3D features from our input (context) using the encoder and generate crown points via the decoder. A dynamic graph convolution network (DGCNN) <ref type="bibr" target="#b27">[29]</ref> is used to group the input points into a smaller set of feature vectors that represent local regions in the context. The generated feature vectors are then fed into the encoder with a geometry-aware block. This block is used to model the local geometric relationships explicitly. The self-attention layer in the encoder updates the feature vectors using both long-range and short-range information. The feature vectors are further updated by a multi-layer perceptron. The decoder's role is to reason about the crown based on the learnable pairwise interactions between features of the input context and the encoder output. The decoder incorporates a series of transformer layers with a self-attention and cross-attention mechanisms to learn structural knowledge. The output of the transformer decoder is fed into a folding-based decoder <ref type="bibr" target="#b13">[15]</ref> to deform a canonical 2D grid onto the underlying 3D surface of the crown points.</p><p>Mesh Completion Layer. In this stage, to directly reconstruct the mesh from the crown points, we use a differentiable Poisson surface reconstruction (DPSR) method introduced by <ref type="bibr" target="#b15">[17]</ref>. We reconstruct a 3D surface as the zero level set of an indicator function. The latter consists in a regular 3D point grid associated with values indicating whether a point is inside the underlying shape or not. To compute this function, We first densify the input unoriented crown points. This is done by predicting additional points and normals for each input point by means of an MLP network. After upsampling the point cloud and predicting normals, the network solves a Poisson partial differential equation (PDE) to recover the indicator function from the densified oriented point cloud. We represent the indicator function as a discrete Fourier basis on a dense grid (of resolution 128 3 ) and solve the Poisson equation (PDE) with the spectral solver method in <ref type="bibr" target="#b15">[17]</ref>.</p><p>During training, we obtain the estimated indicator grid from the predicted point cloud by using the differentiable Poisson solver. We similarly acquire the ground truth indicator grid on a dense point cloud sampled from the ground truth mesh, together with the corresponding normals. The entire pipeline is differentiable, which enables the updating of various elements such as point offsets, oriented normals, and network parameters during the training process. At inference time, we leverage our trained model to predict normals and offsets using Differentiable Poisson Surface Reconstruction (DPSR) <ref type="bibr" target="#b15">[17]</ref>, solve for the indicator grid, and finally apply the Marching Cubes algorithm <ref type="bibr" target="#b20">[22]</ref> to extract the final mesh.</p><p>Loss Function. We use the mean Chamfer Distance (CD) <ref type="bibr" target="#b18">[20]</ref> to constrain point locations. The CD measures the mean squared distance between two point clouds S 1 and S 2 . Individual distances are measured between each point and its closest point in the other point set, as described in Eq. (1). In addition, we minimize the L 2 distance between the predicted indicator function x and a ground truth indicator function x gt , each obtained by solving a Poisson PDE <ref type="bibr" target="#b15">[17]</ref> on a dense set of points and normals. We can express the Mean Square Error (MSE) loss as Eq. ( <ref type="formula">2</ref>), where f θ (X) represents a neural network (MLP) with parameters θ conditioned on the input point cloud X, D is the training data distribution, along with indicator functions x i and point samples X i on the surface of shapes. The sum of the CD and MSE losses is used to train the overall network.</p><formula xml:id="formula_0">CD(S 1 , S 2 ) = 1 |S 1 | x∈S1 min y∈S2 |x -y| 2 + 1 |S 2 | y∈S2 min x∈S1 |y -x| 2</formula><p>(1)</p><formula xml:id="formula_1">L DP SR (θ) = E Xi,xi∼D ||P oisson(f θ (X i )) -x i || 2 2 (2)</formula><p>4 Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Preprocessing</head><p>Our dataset consisted of 388 training, 97 validation, and 71 test cases, which included teeth in various positions in the jaw such as molars, canines, and incisors. The first step in the preprocessing was to generate a context from a given 3D scan. To determine the context for a specific prepared tooth, we employed a pre-trained semantic segmentation model <ref type="bibr" target="#b19">[21]</ref> to separate the 3D scan into 14 classes representing the tooth positions in each jaw. From the segmentations, we extracted the two adjacent and three opposing teeth of a given prepared tooth, as well as the surrounding gum tissue. To enhance the training data, we conducted data augmentation on the entire dental context, which included the master arch, opposing arch, and shell, treated as a single entity. Data augmentation involved applying 3D translation, scaling, and rotation, thereby increasing the training set by a factor of 10. To enrich our training set, we randomly sampled 10,240 cells from each context to form the input during training. We provide two types of ground truth: mesh and point cloud crowns. To supervise network training using the ground truth meshes, we calculate the gradient from a loss on an intermediate indicator grid. We use the spectral method from <ref type="bibr" target="#b15">[17]</ref> to compute the indicator grid for our ground truth mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We adapted the architecture of <ref type="bibr" target="#b9">[11]</ref> for our transformer encoder-decoder module. For mesh reconstruction, we used Differentiable Poisson Surface Reconstruction (DPSR) from <ref type="bibr" target="#b15">[17]</ref>. All models were implemented in PyTorch with the AdamW optimizer <ref type="bibr" target="#b21">[23]</ref>, using a learning rate of 5e-4 and a batch size of 16. Training the model took 400 epochs and 22 h on an NVIDIA A100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation and Metrics</head><p>To evaluate the performance of our network and compare it with point cloudbased approaches, we used the Chamfer distance to measure the dissimilarity between the predicted and ground truth points. We employed two versions of CD: CD L1 uses the L 1 -norm, while CD L2 uses the L 2 -norm to calculate the distance between two sets of points. Additionally, we used the F-score <ref type="bibr" target="#b22">[24]</ref> with a distance threshold of 0.3, chosen based on the distance between the predicted and ground truth point clouds. We also used the Mean Square Error (MSE) loss <ref type="bibr" target="#b15">[17]</ref> to calculate the similarity between the predicted and ground truth indicator grids or meshes.</p><p>We conducted experiments to compare our approach with two distinct approaches from the literature, as shown in Table <ref type="table" target="#tab_0">1</ref>. The first such approach, PoinTr+margin line <ref type="bibr" target="#b6">[8]</ref>, uses the PoinTr <ref type="bibr" target="#b9">[11]</ref> point completion method as a baseline and introduces margin line information to their network. To compare our work to <ref type="bibr" target="#b6">[8]</ref>, we used its official implementation provided by the author. In the second experiment, PoinTr+graph, we modified the work of <ref type="bibr" target="#b26">[28]</ref> to generate a dental crown mesh. To this end, we use deformation blocks in <ref type="bibr" target="#b26">[28]</ref> to deform a generic template mesh to output a crown mesh under the guidance of the learned features from PoinTr. The deformation module included three Graph Convolutional Networks (GCNs) as in <ref type="bibr" target="#b7">[9]</ref>. All experiments used the same dataset, which included all tooth positions, and were trained using the same methodology. To compare the results of the different experiments, we extracted points from the predicted meshes of our proposed network (DMC), as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. Table <ref type="table" target="#tab_0">1</ref> shows that DMC outperforms the two other networks in terms of both CD and F-score. PoinTr+graph achieves poor CD and F-score results compared to the other methods. While the idea of using graph convolutions seems interesting, features extracted from the point cloud completion network don't carry enough information to deform the template into an adequate final crown. Therefore, these methods are highly biased toward the template shape and need extensive pre-processing steps to scale and localize the template. In the initial two experiments, the MSE metric was not applicable as it was calculated on the output meshes. Figure <ref type="figure" target="#fig_3">4</ref> showcases the visual results obtained from our proposed network (DMC). Furthermore, Fig. <ref type="figure" target="#fig_4">5</ref> presents a visual comparison of mesh surfaces generated by various methods for a sample molar tooth.    <ref type="bibr" target="#b9">[11]</ref> for point cloud and Shape as points <ref type="bibr" target="#b15">[17]</ref> for mesh; c) Proposed method (DMC); d) Ground truth shape. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We conducted an ablation study to evaluate the components of our architecture. We started with the baseline PoinTr <ref type="bibr" target="#b6">[8]</ref>, a point completion method. To enhance it, we integrated Shape as Points (SAP) <ref type="bibr" target="#b15">[17]</ref> as a separate network for mesh reconstruction from the PoinTr-generated point cloud. Next, we tested our proposed method (DMC) by excluding the Mean Square Error (MSE) loss function. Finally, we assessed DMC's performance, including the MSE loss function. The results, shown in Table <ref type="table" target="#tab_1">2</ref>, demonstrate the consistent improvements achieved by our full model across all evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Existing deep learning-based dental crown design solutions require additional steps to reconstruct a surface mesh from the generated point cloud. In this study, we propose a new end-to-end approach that directly generates high-quality crown meshes for all tooth positions. By utilizing transformers and a differentiable Poisson surface reconstruction solver, we effectively reason about the crown points and convert them into mesh surfaces using Marching Cubes. Our experimental results demonstrate that our approach produces accurately fitting crown meshes with superior performance. In the future, incorporating statistical features into our deep learning method for chewing functionality, such as surface contacts with adjacent and opposing teeth, could be an interesting avenue to explore.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Dental crown design process: a) Dentist prepares the tooth; b) Technician designs the crown; c) Dentist places the crown on the prepared tooth.</figDesc><graphic coords="2,43,80,137,03,336,73,118,06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pipeline of our proposed network.</figDesc><graphic coords="4,41,79,319,43,340,21,112,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Crown mesh predicted by DMC (a) and extracted point set (b).</figDesc><graphic coords="7,164,97,462,74,122,44,51,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of mesh completions by the proposed architecture (DMC). a) Input context containing master arch, prepped tooth and opposing arch; b) Generated mesh in its context; c) Ground truth mesh in its context.</figDesc><graphic coords="8,73,29,277,79,277,96,55,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Qualitative comparison of crown mesh generation approaches: a) Standard Poisson surface reconstruction; b) PoinTr<ref type="bibr" target="#b9">[11]</ref> for point cloud and Shape as points<ref type="bibr" target="#b15">[17]</ref> for mesh; c) Proposed method (DMC); d) Ground truth shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of proposed method (DMC) with two alternate methods. Evaluation metrics: CDL 1 , CDL 2 ; MSE on output meshes; F-Score@0.3 .</figDesc><table><row><cell>Method</cell><cell cols="4">CD-L1 (↓) CD-L2 (↓) MSE (↓) F 1 0.3 (↑)</cell></row><row><cell cols="2">PoinTr + margin line [8] 0.065</cell><cell>0.018</cell><cell></cell><cell>0.54</cell></row><row><cell>PoinTr + graph</cell><cell>1.99</cell><cell>1.51</cell><cell></cell><cell>0.08</cell></row><row><cell>DMC</cell><cell>0.0623</cell><cell>0.011</cell><cell>0.0028</cell><cell>0.70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of ablation study. Metrics are the same as in Table1.</figDesc><table><row><cell>PoinTr [11]</cell><cell>0.070</cell><cell>0.023</cell><cell></cell><cell>0.24</cell></row><row><cell>PoinTr + SAP</cell><cell>0.067</cell><cell>0.021</cell><cell>0.031</cell><cell>0.50</cell></row><row><cell cols="2">DMC without MSE 0.0641</cell><cell>0.015</cell><cell></cell><cell>0.65</cell></row><row><cell cols="2">DMC (Full Model) 0.0623</cell><cell>0.011</cell><cell cols="2">0.0028 0.70</cell></row></table><note><p><p>Method</p>CD-L1 (↓) CD-L2 (↓) MSE (↓) F 1 0.3 (↑)</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was funded by <rs type="funder">Kerenor Dental Studio, Intellident Dentaire Inc.</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning beyond human expertise with generative models for dental restorations</title>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azernikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Personalized design technique for the dental occlusal surface based on conditional generative adversarial networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Numer. Methods Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3321</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dental restoration using a multiresolution deep learning approach</title>
		<author>
			<persName><forename type="first">O</forename><surname>Lessard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guibault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cheriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging, (ISBI)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ToothCR: a two-stage completion and reconstruction approach on 3D dental model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-05981-0_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-05981-0_13" />
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Teng</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13282</biblScope>
		</imprint>
	</monogr>
	<note>PAKDD 2022</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-attention implicit function networks for 3D dental data completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Aided Geometr. Design</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">102026</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving the quality of dental crown using a transformer-based method</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hosseinimanesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics of Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">12463</biblScope>
			<date type="published" when="2023">2023. 2023</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pixel2Mesh: generating 3D mesh models from single RGB images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01252-6_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01252-6_4" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11215</biblScope>
			<biblScope unit="page" from="55" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PF-Net: point fractal network for 3D point cloud completion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7662" to="7670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PoinTr: diverse point cloud completion with geometry-aware transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="12498" to="12507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Comprehensive review of deep learning-based 3D point clouds completion processing and analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>arXiv Prepr. arXiv2203.03311</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PointNet: deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PointNet++: deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FoldingNet: point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SnowflakeNet: point cloud completion by snowflake point deconvolution with skip-transformer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2108.04444</idno>
		<ptr target="https://doi.org/10.48550/arxiv.2108.04444" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5499" to="5509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shape as points: a differentiable poisson solver</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deformable shape completion with graph convolutional autoencoders</title>
		<author>
			<persName><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1886" to="1895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural template: topology-aware reconstruction and disentangled generation of 3D meshes</title>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18572" to="18582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">PCN: point completion network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<editor>3DV</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="728" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised segmentation of tooth from 3D scanned dental arches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alsheghri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>SPIE</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Marching cubes: a high-resolution 3D surface construction algorithm</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 14th Annual Conference on Computer Graphics and Interactive Techniques<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="163" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">What do single-view 3D reconstruction networks learn?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>CoRR, abs/1905.03678</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Clarkson: intraoperative liver surface completion with graph convolutional VAE</title>
		<author>
			<persName><forename type="first">S</forename><surname>Foti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, and Graphs in Biomedical Image Analysis</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="198" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning quadrangulated patches for 3D shape parameterization and completion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Johns Hopkins University: screened poisson surface reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shape registration with learned deformations for 3D shape reconstruction from sparse and incomplete point clouds</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">102228</biblScope>
			<date type="published" when="2021">2021</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic Graph CNN for Learning on Point Clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
