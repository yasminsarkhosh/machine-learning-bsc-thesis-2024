<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues</title>
				<funder ref="#_UjSq8tr">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_GVApBcU">
					<orgName type="full">Shanghai Municipal Science and Technology Major</orgName>
				</funder>
				<funder>
					<orgName type="full">Multi-Scale Medical Robotics Centre InnoHK, CUHK Shun Hing Institute of Advanced Engineering</orgName>
				</funder>
				<funder>
					<orgName type="full">Acknowledgment</orgName>
				</funder>
				<funder ref="#_Y4xATWg">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_NxpBgja">
					<orgName type="full">Natural Science Foundation of Shanghai</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chen</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution">AI Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kailing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution">AI Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuehao</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Sha Tin, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution">AI Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
							<email>wei.shen@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution">AI Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4F099C881F46503D74C27707BF42B2CD</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fast 3D Reconstruction</term>
					<term>Neural Rendering</term>
					<term>Robotic Surgery</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reconstructing deformable tissues from endoscopic stereo videos in robotic surgery is crucial for various clinical applications. However, existing methods relying only on implicit representations are computationally expensive and require dozens of hours, which limits further practical applications. To address this challenge, we introduce Ler-Plane, a novel method for fast and accurate reconstruction of surgical scenes under a single-viewpoint setting. LerPlane treats surgical procedures as 4D volumes and factorizes them into explicit 2D planes of static and dynamic fields, leading to a compact memory footprint and significantly accelerated optimization. The efficient factorization is accomplished by fusing features obtained through linear interpolation of each plane and enables using lightweight neural networks to model surgical scenes. Besides, LerPlane shares static fields, significantly reducing the workload of dynamic tissue modeling. We also propose a novel sample scheme to boost optimization and improve performance in regions with tool occlusion and large motions. Experiments on DaVinci robotic surgery videos demonstrate that LerPlane accelerates optimization by over 100× while maintaining high quality across various non-rigid deformations, showing significant promise for future intraoperative surgery applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reconstructing deformable tissues in surgical scenes accurately and efficiently from endoscope stereo videos is a challenging and active research topic. Such techniques can facilitate constructing virtual surgery environments for surgery robot learning and AR/VR surgery training and provide vivid and specific training for medics on human tissues. Moreover, real-time reconstruction further expands its applications to intraoperative use, allowing surgeons to navigate and precisely control surgical instruments while having a complete view of the surgical scene. This capability could reduce the need for invasive follow-up procedures and address the challenge of operating within a confined field of view.</p><p>Neural Radiance Fields (NeRFs) <ref type="bibr" target="#b15">[16]</ref>, a promising approach for 3D reconstruction, have demonstrated strong potential in accurately reconstructing deformable tissues in dynamic surgical scenes from endoscope stereo videos. EndoNeRF <ref type="bibr" target="#b25">[26]</ref>, a recent representative approach, represents deformable surgical scenes using a canonical neural radiance field and a time-dependent neural displacement field, achieving impressive reconstruction of deformable tissues. However, the optimization for dynamic NeRFs is computationally intensive, often taking dozens of hours, as each generated pixel requires hundreds of neural network calls. This computational bottleneck significantly constrains the widespread application of these methods in surgical procedures.</p><p>Recently, explicit and hybrid methods have been developed for modeling static scenes, achieving significant speedups over NeRF by employing explicit spatial data structures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">27]</ref> or features decoded by small MLPs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25]</ref>. Nevertheless, these methods have only been applied to static scenes thus far. Adopting these methods to surgical scenes presents significant challenges for two primary reasons. Firstly, encoding temporal information is essential for modeling surgical scenes while naively adding a temporal dimension to the explicit data structure can significantly increase memory and computational requirements. Secondly, dynamic surgical scene reconstruction suffers from limited viewpoints, often providing only one view per timestep, as opposed to static scenes, which can fully use multi-view consistency for further regularization. This condition requires sharing information across disjoint timesteps for better reconstruction.</p><p>To address the aforementioned challenges, we propose a novel method for fast and accurate reconstruction of deformable tissues in surgical procedures, Neural LerPlane (Linear Interpolation Plane), by leveraging explicitly represented multi-plane fields. Specifically, we treat surgical procedures as 4D volumes, where the time axis is orthogonal to 3D spatial coordinates. LerPlane factorizes 4D volumes into 2D planes and uses space planes to form static fields and space-time planes to form dynamic fields. This factorization results in a compact memory footprint and significantly accelerates optimization compared to previous methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref>, which rely on pure MLPs, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. LerPlane enables information sharing across timesteps within the static field, thereby reducing the negative impact of limited viewpoints. Moreover, considering the surgical instrument occlusion, we develop a novel sample approach based on tool masks and contents, which assigns higher sampling probability to tissue pixels that have been occluded by tools or have a more extensive motion range. By targeting these regions, our approach allows for more efficient sampling during the training, leading to higher-quality results and faster optimization.</p><p>We summarize our contributions:</p><p>1. A fast deformable tissue reconstruction method, with rendering quality comparable to or better than the previous method in just 3 min, which is over 100× faster. 2. An efficient representation of surgical scenes, which includes static and dynamic fields, enabling fast optimization and high reconstruction quality. 3. A novel sampling method that boosts optimization and improves the rendering quality. Compared to previous methods, our LerPlane, achieves much faster optimization with superior quantitative and qualitative performance on 3D reconstruction and deformation tracking of surgical scenes, providing significant promise for further applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>LerPlane represents surgical procedures using static and dynamic fields, each of which is made up of three orthogonal planes (Sect. 2.3). It starts by using spatiotemporal importance sampling to identify high-priority tissue pixels and build corresponding rays (Sect. 2.4). Then we sample points along each ray and query features using linear interpolation to construct fused features. The fused features and encoded coordinate-time information are input to a lightweight MLP, which predicts color and density for each point (Sect. 2.5). To better optimize LerPlane, we introduce some training schemes, including sample-net, various regularizers, and a warm-up training strategy (Sect. 2.6). Finally, we apply volume rendering to produce predicted color and depth values for each chosen ray. The overall framework is illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preliminaries</head><p>Neural Radiance Field (NeRF) <ref type="bibr" target="#b15">[16]</ref> is a coordinate-based neural scene representation optimized through a differentiable rendering loss. NeRF maps the 3D coordinate and view direction of each point in the space into its color values c and volume density σ via neural networks Φ r .</p><p>c, σ = Φ r (x, y, z, θ, φ).</p><p>(1)</p><p>It calculates the expected color Ĉ(r) and the expected depth D(r) of a pixel in an image captured by a camera by tracing a ray r(t) = o + td from the camera center to the pixel. Here, o is the ray origin, d is the ray direction, and t is the distance from a point to the o, ranging from a pre-defined near bound t n to a far bound t f . w(t) represents a weight function that accounts for absorption and scattering during the propagation of light rays. The pixel color is obtained by classical volume rendering techniques <ref type="bibr" target="#b9">[10]</ref>, which involve sampling a series of points along the ray.  </p><formula xml:id="formula_0">Ĉ(r) = t f tn w(t)c(t)dt, D(r) =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Neural LerPlane Representations for Deformable Tissues</head><p>A surgical procedure can be represented as a 4D volume, and we factorize the volume into 2D planes. Specifically, We represent a surgical scene using six orthogonal feature planes, consisting of three space planes (i.e., XY, YZ, and XZ ) for the static field and three space-time planes (i.e., XT, YT, and ZT ) for the dynamic field. Each space plane has a shape of N × N × D, and each space-time plane owns a shape of N × M × D, where N and M represent spatial and temporal resolution, respectively, and D is the size of the feature.</p><p>To extract features from an image pixel p ij with color C at a specific timestep τ , we first cast a ray r(t) from o to the pixel. We then sample spatial-temporal points along the ray, obtaining their 4D coordinates. We acquire a feature vector for a point P(x, y, z, τ ) by projecting it onto each plane and using bilinear interpolation B to query features from the six feature planes. v(x, y, z, τ ) = B(F XY , x, y) B(F YZ , y, z) . . . B(F YT , y, τ) B(F ZT , z, τ), <ref type="bibr" target="#b2">(3)</ref> where the represents element-wise multiplication, inspired by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref>. The fused feature vector v is then passed to a tiny MLP Θ, which predicts the color c and density σ of the point. Finally, we leverage the Eq. 2 to get the predicted color Ĉ(r). Inspired by <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref>, we build the feature planes with multi-resolution planes, e.g. F XY is represented by planes with N = 128 and 256.</p><p>Existing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref> for reconstructing surgical procedures using pure implicit representations, requires traversing all possible positions in space-time, which is highly computationally and time-intensive. In contrast, LerPlane decomposes the surgical scene into six explicitly posed planes, resulting in a significant reduction in complexity and a much more efficient representation. This reduces the computational cost from O(N 4 ) to O(N 2 ) and enables the use of smaller neural networks, leading to a considerable acceleration in the training period. Besides, methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref> using a single displacement field to supplement the static field struggle with handling variations in scene topology, such as non-linear deformations. In contrast, LerPlane can naturally model these situations using a dynamic field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Spatiotemporal Importance Sampling</head><p>Tool occlusion in robotic surgery poses a challenge for reconstructing occluded tissues due to their infrequent occurrence in the training set, resulting in varied learning difficulties for different pixels. Besides, we observe that many tissues remain stationary over time, and therefore repeated training on these pixels contributes minor to the convergence, reducing efficiency. We design a novel spatiotemporal importance sampling strategy to address the issues above. In particular, we utilize binary masks {M i } T i=1 and temporal differences among frames to generate sampling weight maps {W} T i=1 . These weight maps represent the sampling probabilities for each pixel/ray, drawing inspiration from <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. One sampling weight map W i can be determined by:</p><formula xml:id="formula_1">W i = min( max i-n&lt;j &lt;i+n ( I i M i -I j M j 1 )/3, α) • Ω i , Ω i = β(M i T / T i=1 M i ), (<label>4</label></formula><formula xml:id="formula_2">)</formula><p>where α is a lower-bound to avoid zero weight among unchanged pixels, Ω i specifies higher importance scaling for those tissue areas with higher occlusion frequencies, and β is a hyper-parameter for balancing augmentation among frequently occluded areas and time-variant areas. By unitizing spatiotemporal importance sampling, LerPlane concentrates on tissue areas and speeds up training, improving the rendering quality of occluded areas and prioritizing tissue areas with higher occlusion frequencies and temporal variability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Coordinate-Time Encoding</head><p>Previous methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref> apply positional encoded view direction γ(d) to model view-dependent appearance. However, during endoscopic operations, camera movements are restricted. The view direction changes are typically minimal. Instead of view encoding, we propose using γ(x, y, z, τ ) to enhance the spatiotemporal information. Specifically, the encoding along with the fused features v from feature planes is input to the MLP Θ, which predicts σ and c of each point. Then we utilize Eq. 2 to render the expected color Ĉ and depth D of one specific ray.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Optimization</head><p>We adopt a joint supervision approach to optimize the tiny MLP Θ and feature planes using rendered color and depth. To further improve the optimization process, we propose several optimization schemes, including a sample-net for better-sampled points, a warm-up strategy to address outliers, and several regularizers.</p><p>Sample-Net. The sampling of spatiotemporal points is crucial for volume rendering, with a particular focus on sampling around tissue regions for optimal performance. We replaced the conventional two-stage time-consuming sampling strategy with a single sample-net and train it using histogram loss <ref type="bibr" target="#b2">[3]</ref>. The sample-net is a lightweight single-resolution LerPlane model that provides more accurate sampling points for the full model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularizers.</head><p>We apply some regularization to address the limited information available in surgical scene reconstruction. We adopt 2D total variation (TV) loss for space planes in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref> and 1D TV loss on the space axis for space-time planes and a similar smooth loss on the time axis. Additionally, we introduce a minor time-invariant loss to separate the static and dynamic fields as much as possible, encouraging the features in space-time planes to remain unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Warm-Up Training Strategy.</head><p>Since single-view captures cannot provide valid scale information, we leverage pseudo ground truth depth maps D(r) generated by STTR-light <ref type="bibr" target="#b12">[13]</ref> from stereo images to guide the optimization. Specifically, we apply a Huber loss for depth regularization:</p><formula xml:id="formula_3">L D = 0.5ΔD(r) 2 , if |ΔD(r)| &lt; δ δ • (ΔD(r) -0.5 • δ) , otherwise<label>(5)</label></formula><p>where ΔD(r) = | D(r) -D(r)| represents the absolute depth difference among valid depth values, δ is a threshold at which to change loss type. Considering that the predicted depth maps encounter a lot of unreliable depth values and missing areas <ref type="bibr" target="#b25">[26]</ref>, we design a simple by effective warm-up training strategy. Specifically, we apply the L D to depths from both the sample-net and the full model during the first half of the training. In the remaining iterations, we disable the L D and use other regularization to refine unreliable depths.</p><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation Metrics</head><p>We evaluate our proposed method on the EndoNeRF dataset <ref type="bibr" target="#b25">[26]</ref>, a collection of typical robotic surgery stereo videos captured from stereo cameras at a single viewpoint during in-house DaVinci robotic prostatectomy procedures, which is designed to capture challenging surgical scenes with non-rigid deformation and tool occlusion. We evaluate our proposed method by comparing it to existing methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26]</ref> using standard image quality metrics following <ref type="bibr" target="#b25">[26]</ref>, including PSNR, SSIM, and LPIPS. Additionally, to measure the consistency of the underlying 3D scene, we supplement these metrics using the FLIP metric <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. For qualitative evaluation, we follow the exhibition method from <ref type="bibr" target="#b25">[26]</ref>.</p><formula xml:id="formula_4">=1.07s =2.07s =3.73s E-DSSR EndoNeRF-14h</formula><p>Ours-10min Reference Fig. <ref type="figure">3</ref>. Qualitative results on scene "traction" from different timesteps τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We normalize the scene into device coordinates (NDC) to handle single-view endoscopy videos and then project rays within the NDC space. The video duration is normalized to [-1, 1]. We use a two-stage sampling network with 128 and 256 dimensional plane features for the sample-net. Oneblob encoding <ref type="bibr" target="#b17">[18]</ref> is applied to encode the spatiotemporal information. The full model consists of four resolutions, 64, 128, 256, and 512 dimensions among space. Hyperparameters include D = 32 for feature planes, j = 25 for spatiotemporal importance sampling, ξ = 16 for Oneblob encoding dimensionality, and δ = 0.2 for depth loss across all experiments. An Adam <ref type="bibr" target="#b10">[11]</ref> optimizer is used with default values for optimization. In each iteration, 2048 rays are randomly sampled from the whole dataset to form a batch. The initial learning rate is set to 0.01. We apply a cosine schedule with a 512 iterations warming-up stage. We train all scenes with 9k and 32k iterations, which take around 3 and 10 minutes, respectively, on a single RTX 3090 GPU running the Ubuntu 20.04. Our LerPlane is implemented with pure Pytorch <ref type="bibr" target="#b20">[21]</ref>. The code is available at https://github.com/ Loping151/LerPlane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>We compare our proposed method, LerPlane, against two existing SOTA methods: the surfel warping-based method, E-DSSR <ref type="bibr" target="#b14">[15]</ref> and NeRF-based method EndoNeRF <ref type="bibr" target="#b25">[26]</ref>. We find that E-DSSR struggles to completely reconstruct surgical scenes, resulting in many holes and noisy points (see Fig. <ref type="figure">3</ref>), which leads to poor numerical performance. In contrast, EndoNeRF achieves high-fidelity reconstruction of deformable tissues but requires around 14 h of optimization, which is computationally expensive and constrains intraoperative use. LerPlane, on the other hand, achieves comparable results to EndoNeRF with only 3 min of optimization, providing nearly 280-fold acceleration. Moreover, with a longer optimization time of 10 min, LerPlane outperforms both E-DSSR and EndoNeRF in terms of all metrics, as shown in Table <ref type="table" target="#tab_0">1</ref>. Our novel importance sampling and encoding strategies further enhance the ability of LerPlane to preserve details and produce accurate visualizations of deformable tissues, as demonstrated in Fig. <ref type="figure">3</ref>. Our results demonstrate that LerPlane achieves significantly faster optimization without compromising reconstruction quality, showing great potential for future clinical applications in robotic surgery. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>We conduct ablation studies on the EndoNeRF dataset to understand the key components and demonstrate their effectiveness. Table <ref type="table" target="#tab_0">1</ref> shows the performance of all experiments.</p><p>1. Sampling Strategy. We compare with two different methods: naively avoiding tool masks, assigning equal weights to other pixels (Ours-NS), and assigning higher probabilities to highly occluded areas (Ours-TS), as in <ref type="bibr" target="#b25">[26]</ref>. Our method effectively prioritizes time-variant and highly occluded areas, significantly improving convergence speed. 2. Encoding Strategy. Experiments showed that coordinate-time encoding achieves better performance in all metrics compared to no encoding (Ours-NE) or direction encoding (Ours-VE), showing the effectiveness of the proposed encoding.</p><p>Further analysis of the optimization schemes is available in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this paper, we introduced LerPlane, a fast and accurate method for reconstructing deformable tissues from endoscopic videos. By utilizing multi-plane fields and spatiotemporal importance sampling, we can handle tool occlusion and large motion while significantly accelerating optimization. Our experiments show that LerPlane achieves rendering quality comparable to or better than EndoNeRF in just three minutes, which is over 100× faster. We believe that LerPlane could improve robotic surgery scene understanding, benefiting various clinical-oriented tasks and intraoperative surgery applications. Currently, the inference speed of our Lerplane is slow, In our future research endeavors, our main emphasis will revolve around improving the inference time of our approach, with the primary goal of efficiently supporting intraoperative operations. Moreover, we will dedicate our efforts to reducing the input data requirements, thereby aiming to broaden the applicability of LerPlane to a wider range of surgical scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Performance along with training time. We show the results of EndoNeRF (top) and LerPlane (bottom) with the same training time. LerPlane exhibits remarkable restoration of surgical scenes with just a few minutes of optimization.</figDesc><graphic coords="2,57,48,54,95,318,64,98,17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t f tn w(t)tdt, w(t) = exp -t tn σ(s)ds σ(t).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of our fast 4D reconstruction method, LerPlane.</figDesc><graphic coords="4,75,96,276,08,291,67,160,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,67,08,235,46,300,16,237,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on the EndoNeRF dataset. Please refer to Sect. 3.4 for explanations of the acronyms used.</figDesc><table><row><cell>Methods</cell><cell>PSNR↑</cell><cell>SSIM↑</cell><cell>LPIPS↓</cell><cell>FLIP↓</cell><cell>Time↓</cell></row><row><cell>E-DSSR [15]</cell><cell cols="4">13.398 ± 1.387 0.630 ± 0.057 0.423 ± 0.047 \</cell><cell>\</cell></row><row><cell cols="6">EndoNeRF [26] 29.272 ± 2.836 0.921 ± 0.022 0.088 ± 0.020 0.085 ± 0.018 14 h</cell></row><row><cell>Ours-NS</cell><cell cols="5">31.532 ± 1.665 0.886 ± 0.021 0.142 ± 0.020 0.112 ± 0.016 3 min</cell></row><row><cell>Ours-TS</cell><cell cols="5">31.544 ± 1.669 0.886 ± 0.021 0.142 ± 0.020 0.111 ± 0.015 3 min</cell></row><row><cell>Ours-VE</cell><cell cols="5">32.353 ± 1.742 0.897 ± 0.022 0.131 ± 0.024 0.103 ± 0.012 3 min</cell></row><row><cell>Ours-NE</cell><cell cols="5">32.230 ± 1.655 0.895 ± 0.023 0.131 ± 0.020 0.102 ± 0.012 3 min</cell></row><row><cell>Ours-9k</cell><cell cols="5">32.589 ± 1.451 0.901 ± 0.021 0.126 ± 0.028 0.103 ± 0.014 3 min</cell></row><row><cell>Ours-32k</cell><cell>35.504</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>± 3.076 0.935 ± 0.026 0.083 ± 0.022 0.075 ± 0.031 10 min</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p><rs type="person">Data Use Declaration</rs> and <rs type="funder">Acknowledgment</rs>. This work was supported in part by the <rs type="funder">National Key R&amp;D Program of China</rs> <rs type="grantNumber">2022YFF1202600</rs>, in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62176159</rs>, in part by the <rs type="funder">Natural Science Foundation of Shanghai</rs> <rs type="grantNumber">21ZR1432200</rs>, and in part by the <rs type="funder">Shanghai Municipal Science and Technology Major</rs> Project <rs type="grantNumber">2021SHZDZX0102</rs>. This paper uses the EndoN-eRF dataset, which is supported by <rs type="funder">Multi-Scale Medical Robotics Centre InnoHK, CUHK Shun Hing Institute of Advanced Engineering</rs>, and <rs type="institution">Shenzhen-HK Collaborative Development Zone</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Y4xATWg">
					<idno type="grant-number">2022YFF1202600</idno>
				</org>
				<org type="funding" xml:id="_UjSq8tr">
					<idno type="grant-number">62176159</idno>
				</org>
				<org type="funding" xml:id="_NxpBgja">
					<idno type="grant-number">21ZR1432200</idno>
				</org>
				<org type="funding" xml:id="_GVApBcU">
					<idno type="grant-number">2021SHZDZX0102</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_5.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FLIP: a difference evaluator for alternating images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Akenine-Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oskarsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Åström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Fairchild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Comput. Graph. Interact. Tech</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="15" to="16" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visualizing errors in rendered high dynamic range images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Akenine-Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mip-NeRF 360: unbounded anti-aliased neural radiance fields</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Verbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5470" to="5479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient geometry-aware 3D generative adversarial networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16123" to="16133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tensorf: tensorial radiance fields</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19824-3_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19824-3_20" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022, Part XXXII</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13692</biblScope>
			<biblScope unit="page" from="333" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Corona-Figueroa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frawley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bond-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bethapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
		</author>
		<title level="m">MedNeRF: medical neural radiance fields for reconstructing 3D-aware CT-projections from a single x-ray</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3843" to="3848" />
		</imprint>
	</monogr>
	<note>2022 44th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast dynamic radiance fields with time-aware neural voxels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2022 Conference Papers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Plenoxels: radiance fields without neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5501" to="5510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Baking neural radiance fields for real-time view synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5875" to="5884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ray tracing volume densities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kajiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Von Herzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="165" to="174" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural 3d video synthesis from multi-view video</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5521" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6197" to="6206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zaw Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural sparse voxel fields</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15651" to="15663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">E-DSSR: efficient dynamic surgical scene reconstruction with transformer-based stereoscopic depth perception</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_40</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-1_40" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part IV</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="415" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">NeRF: representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Instant neural graphics primitives with a multiresolution hash encoding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (ToG)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural importance sampling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rousselle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novák</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (ToG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">NeRFies: deformable neural radiance fields</title>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5865" to="5874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13228</idno>
		<title level="m">HyperNeRF: a higher-dimensional representation for topologically varying neural radiance fields</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional occupancy networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58580-8_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58580-8_31" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020, Part III</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12348</biblScope>
			<biblScope unit="page" from="523" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">D-NeRF: neural radiance fields for dynamic scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10318" to="10327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07695</idno>
		<title level="m">VoxGRAF: fast 3Daware image synthesis with sparse voxel grids</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Direct voxel grid optimization: super-fast convergence for radiance fields reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5459" to="5469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural rendering for stereo 3D reconstruction of deformable tissues in robotic surgery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="431" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_41" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PlenOctrees for real-time rendering of neural radiance fields</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5752" to="5761" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
