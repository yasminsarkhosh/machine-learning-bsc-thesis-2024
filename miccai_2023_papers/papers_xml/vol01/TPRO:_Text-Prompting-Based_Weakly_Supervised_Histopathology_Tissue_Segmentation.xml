<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation</title>
				<funder ref="#_YmAKMcV">
					<orgName type="full">Ningbo Clinical Research Center for Medical Imaging</orgName>
				</funder>
				<funder ref="#_dpDhPFg">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_WZgFR5v">
					<orgName type="full">Key Technologies Research and Development Program</orgName>
				</funder>
				<funder ref="#_dvw6qAN">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_QJJkgfy">
					<orgName type="full">Natural Science Foundation of Ningbo City, China</orgName>
				</funder>
				<funder ref="#_EYrMUmC">
					<orgName type="full">Science and Technology Innovation Committee of Shenzhen Municipality, China</orgName>
				</funder>
				<funder ref="#_64DgVex">
					<orgName type="full">Key Research and Development Program of Shaanxi Province, China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaoteng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ningbo Institute of Northwestern Polytechnical University</orgName>
								<address>
									<postCode>315048</postCode>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Research and Development Institute of Northwestern</orgName>
								<orgName type="institution">Polytechnical University</orgName>
								<address>
									<postCode>518057</postCode>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianpeng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yutong</forename><surname>Xie</surname></persName>
							<email>yutong.xie678@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<settlement>Adelaide</settlement>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Xia</surname></persName>
							<email>yxia@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Ningbo Institute of Northwestern Polytechnical University</orgName>
								<address>
									<postCode>315048</postCode>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Research and Development Institute of Northwestern</orgName>
								<orgName type="institution">Polytechnical University</orgName>
								<address>
									<postCode>518057</postCode>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="109" to="118"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">237A2325351C3D43F465C32E85B65F0D</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_11</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Histopathology Tissue Segmentation</term>
					<term>Weakly-Supervised Semantic Segmentation</term>
					<term>Vision-Language</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing weakly-supervised segmentation methods rely on class activation maps (CAM) to generate pseudo-labels for training segmentation models. However, CAM has been criticized for highlighting only the most discriminative parts of the object, leading to poor quality of pseudo-labels. Although some recent methods have attempted to extend CAM to cover more areas, the fundamental problem still needs to be solved. We believe this problem is due to the huge gap between image-level labels and pixel-level predictions and that additional information must be introduced to address this issue. Thus, we propose a text-prompting-based weakly supervised segmentation method (TPRO), which uses text to introduce additional information. TPRO employs a vision and label encoder to generate a similarity map for each image, which serves as our localization map. Pathological knowledge is gathered from the internet and embedded as knowledge features, which are used to guide the image features through a knowledge attention module. Additionally, we employ a deep supervision strategy to utilize the network's shallow information fully. Our approach outperforms other weakly supervised segmentation methods on benchmark datasets LUAD-HistoSeg and BCSS-WSSS datasets, setting a new state of the art. Code is available at: https://github.com/zhangst431/TPRO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated segmentation of histopathological images is crucial, as it can quantify the tumor micro-environment, provide a basis for cancer grading and prognosis, and improve the diagnostic efficiency of clinical doctors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b19">19]</ref>. However, pixellevel annotation of images is time-consuming and labor-intensive, especially for histopathology images that require specialized knowledge. Therefore, there is an urgent need to pursue weakly supervised solutions for pixel-wise segmentation. Nonetheless, weakly supervised histopathological image segmentation presents a challenge due to the low contrast between different tissues, intra-class variations, and inter-class similarities <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. Additionally, the tissue structures in histopathology images can be randomly arranged and dispersed, which makes it difficult to identify complete tissues or regions of interest <ref type="bibr" target="#b6">[7]</ref>.</p><p>Ours CAM Under the microscope, tumor epithelial Ɵssue may appear as solid nests, acinar structures, or papillary formaƟons. The cells may have enlarged and irregular nuclei.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tumor epithelial Ɵssue</head><p>Necrosis Ɵssue Tumor-associated stroma</p><p>Necrosis may appear as areas of pink, amorphous material under the microscope, and may be surrounded by viable tumor cells and stroma.</p><p>Tumor-associated stroma Ɵssue is the connecƟve Ɵssue that surrounds and supports the tumor epithelial Ɵssue.</p><p>Fig. <ref type="figure">1</ref>. Comparison of activation maps extracted from CAM and our method, from left to right: origin image, ground truth, three activation maps of tumor epithelial (red), necrosis (green), and tumor-associated stroma (orange) respectively. On the right side, there are some examples of the related language knowledge descriptions used in our method. It shows that CAM only highlights a small portion of the target, while our method, which incorporates external language knowledge, can encompass a wider and more precise target tissue. (Color figure online)</p><p>Recent studies on weakly supervised segmentation primarily follow class activation mapping (CAM) <ref type="bibr" target="#b20">[20]</ref>, which localizes the attention regions and then generates the pseudo labels to train the segmentation network. However, the CAM generated based on the image-level labels can only highlight the most discriminative region, but fail to locate the complete object, leading to defective pseudo labels, as shown in Fig. <ref type="figure">1</ref>. Accordingly, many attempts have been made to enhance the quality of CAM and thus boost the performance of weakly supervised segmentation. Han et al. <ref type="bibr" target="#b6">[7]</ref> proposed an erasure-based method that continuously expands the scope of attention areas to obtain rich content of pseudo labels. Li et al. <ref type="bibr" target="#b10">[11]</ref> utilized the confidence method to remove any noise that may exist in the pseudo labels and only included the confident pixel labels for the segmentation training. Zhang et al. <ref type="bibr" target="#b18">[18]</ref> leveraged the Transformer to model the long-distance dependencies on the whole histopathological images to improve the CAM's ability to find more complete regions. Lee et al. <ref type="bibr" target="#b9">[10]</ref> utilized the ability of an advanced saliency detection model to assist CAM in locating more precise targets. However, these improved variants still face difficulties in capturing the complete tissues. The primary limitation is that the symptoms and manifestations of histopathological subtypes cannot be comprehensively described by an abstract semantic category. As a result, the image-level label supervision may not be sufficient to pinpoint the complete target area.</p><p>To remedy the limitations of image-level supervision, we advocate for the integration of language knowledge into weakly supervised learning to provide reliable guidance for the accurate localization of target structures. To this end, we propose a text-prompting-based weakly supervised segmentation method (TPRO) for accurate histopathology tissue segmentation. The text information originates from the task's semantic labels and external descriptions of subtype manifestations. For each semantic label, a pre-trained medical language model is utilized to extract the corresponding text features that are matched to each feature point in the image spatial space. A higher similarity represents a higher possibility of this location belonging to the corresponding semantic category. Additionally, the text representations of subtype manifestations, including tissue morphology, color, and relationships to other tissues, are extracted by the language model as external knowledge. The discriminative information can be explored from the text knowledge to help identify and locate complete tissues accurately by jointly modeling long-range dependencies between image and text. We conduct experiments on two weakly supervised histological segmentation benchmarks, LUAD-HistoSeg and BCSS-WSSS, and demonstrate the superior quality of pseudo labels produced by our TPRO model compared to other CAM-based methods.</p><p>Our contributions are summarized as follows: (1) To the best of our knowledge, this is the first work that leverages language knowledge to improve the quality of pseudo labels for weakly-supervised histopathology image segmentation. <ref type="bibr" target="#b1">(2)</ref> The proposed text prompting models the correlation between image representations and text knowledge, effectively improving the quality of pseudo labels. (3) The effectiveness of our approach has been effectively validated by two benchmarks, setting a new state of the art. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Figure <ref type="figure" target="#fig_0">2</ref> displays the proposed TPRO framework, a classification network designed to train a suitable model and extract segmentation pseudo-labels. The framework comprises a knowledge attention module and three encoders: one vision encoder and two text encoders (label encoder and knowledge encoder).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Classification with Deep Text Guidance</head><p>Vision Encoder. The vision encoder is composed of four stages that encode the input image into image features. The image features are denoted as T s ∈ R Ms×Cs , where 2 ≤ s ≤ 4 indicates the stage number.</p><p>Label Encoder. The label encoder encodes the text labels in the dataset into N label features, denoted as L ∈ R N ×C l , where N represents the number of classes in the dataset and C l represents the dimension of label features. Since the label features will be used to calculate the similarity with image features, it is important to choose a language model that has been pre-trained on image-text pairs. Here we use MedCLIP<ref type="foot" target="#foot_0">1</ref> as our label encoder, which is a model fine-tuned on the ROCO dataset <ref type="bibr" target="#b12">[12]</ref> based on CLIP <ref type="bibr" target="#b14">[14]</ref>.</p><p>Knowledge Encoder. The knowledge encoder is responsible for embedding the descriptions of subtype manifestations into knowledge features, denoted as K ∈ R N ×C k . The knowledge features guide the image features to focus on regions relevant to the target tissue. To encode the subtype manifestations description into more general semantic features, we employ ClinicalBert <ref type="bibr" target="#b1">[2]</ref> as our knowledge encoder. ClinicalBert is a language model that has been fine-tuned on the MIMIC-III <ref type="bibr" target="#b7">[8]</ref> dataset based on BioBert <ref type="bibr" target="#b8">[9]</ref>.</p><p>Adaptive Layer. We freeze the label and knowledge encoders for training efficiency but add an adaptive layer after the text encoders to better tailor the text features to our dataset. The adaptive layer is a simple FC-ReLU-FC block that allows for fine-tuning of the features extracted from the text encoders.</p><p>Label-Pixel Correlation. After the input image and text labels are embedded. We employ the inner product to compute the similarity between image features and label features, denoted as F s . Specially, we first reshape the image features from a token format into feature maps. We denote the feature map as I s ∈ R Hs×Ws×Cs , where H s and W s mean the height and width of the feature map. F s is computed with the below formula</p><formula xml:id="formula_0">F s [i, j, k] = I s [i, j] • L[k] ∈ R Hs×Ws×N .</formula><p>(1)</p><p>Then, we perform a global average-pooling operation on the produced similarity map to obtain the class prediction, denoted as P s ∈ R 1×N . We then calculate the binary cross-entropy loss between the class label Y ∈ R 1×N and the class prediction P s to supervise the model training, which is formulated as:</p><formula xml:id="formula_1">L s = - 1 N N n=1 Y [n]log σ(P s [n]) + (1 -Y [n])log[1 -σ(P s [n])]<label>(2)</label></formula><p>Deep Supervision. To leverage the shallow features in the network, we employ a deep supervision strategy by calculating the similarity between the image features from different stages and the label features from different adaptive layers. Class predictions are derived from these similarity maps. The loss of the entire network is computed as:</p><formula xml:id="formula_2">L = λ 2 L 2 + λ 3 L 3 + λ 4 L 4 .</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Attention Module</head><p>To enhance the model's understanding of the color, morphology, and relationships between different tissues, we gather text representations of different subtype manifestations from the Internet and encode them into external knowledge via the knowledge encoder. The knowledge attention module uses this external knowledge to guide the image features toward relevant regions of the target tissues.</p><p>The knowledge attention module, shown in Fig. <ref type="figure" target="#fig_0">2</ref>, consists of two multi-head self-attention modules. The image features T 4 ∈ R M4×C4 and knowledge features after adaptive layer K ∈ R N ×C4 are concatenated in the token dimension to obtain T fuse ∈ R (M4+N )×C4 . This concatenated feature is then fed into the knowledge attention module for self-attention calculation. The output tokens are split, and the part corresponding to the image features is taken out. Noted that the knowledge attention module is added only after the last stage of the vision encoder to save computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pseudo Label Generation</head><p>In the classification process, we calculate the similarity between image features and label features to obtain a similarity map F , and then directly use the result of global average pooling on the similarity map as a class prediction. That is, the value at position (i, j, k) of F represents the probability that pixel (i, j) is classified into the k th class. Therefore we directly use F as our localization map. We first perform min-max normalization on it, the formula is as follows</p><formula xml:id="formula_3">F c fg = F c -min(F c ) max(F c ) -min(F c ) , (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where 1 ≤ c ≤ N means c th class in the dataset. Then we calculate the background localization map by the following formula:</p><formula xml:id="formula_5">F bg (i, j) = {1 -max c∈[0,C) F c fg (i, j)} α , (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where α ≥ 1 denotes a hyper-parameter that adjusts the background confidence scores. Referring to <ref type="bibr" target="#b0">[1]</ref> and combined with our own experiments, we set α to 10. Then we stitch together the localization map of foreground and background, denoted as F . In order to make full use of the shallow information of the network, we perform weighted fusion on the localization maps from different stages by the following formula:</p><formula xml:id="formula_7">F all = γ 2 F2 + γ 3 F4 + γ 4 F4 . (6)</formula><p>Finally, we perform argmax operation on F all to obtain the final pseudo-label.</p><p>3 Experiments </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>For the classification part, we adopt MixTransformer <ref type="bibr" target="#b17">[17]</ref> pretrained on Ima-geNet, MedCLIP, and ClinicalBert <ref type="bibr" target="#b1">[2]</ref> as our vision encoder, label encoder, and knowledge encoder, respectively. The hyperparameters during training and evaluation can be found in the supplementary materials. We conduct all of our experiments on 2 NVIDIA GeForce RTX 2080 Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Compare with State-of-the-Arts</head><p>Comparison on Pseudo-Labels. Table <ref type="table" target="#tab_1">1</ref> compares the quality of our pseudolabels with those generated by previous methods. CAM <ref type="bibr" target="#b20">[20]</ref> and Grad-CAM <ref type="bibr" target="#b15">[15]</ref> were evaluated using the same ResNet38 <ref type="bibr" target="#b16">[16]</ref> classifier, and the results showed that CAM <ref type="bibr" target="#b20">[20]</ref> outperformed Grad-CAM <ref type="bibr" target="#b15">[15]</ref>, with mIoU values of 70.44% and 56.52% on the LUAD-HistoSeg and BCSS-WSSS datasets, respectively. Tran-sWS <ref type="bibr" target="#b18">[18]</ref> consists of a classification and a segmentation branch, and Table <ref type="table" target="#tab_1">1</ref> displays the pseudo-label scores generated by the classification branch. Despite using CAM <ref type="bibr" target="#b20">[20]</ref> for pseudo-label extraction, TransWS <ref type="bibr" target="#b18">[18]</ref> yielded inferior results compared to CAM <ref type="bibr" target="#b20">[20]</ref>. This could be due to the design of TransWS <ref type="bibr" target="#b18">[18]</ref> for single-label image segmentation, with the segmentation branch simplified to binary segmentation to reduce the difficulty, while our dataset consists of multilabel images. Among the compared methods, MLPS <ref type="bibr" target="#b6">[7]</ref> was the only one to surpass CAM <ref type="bibr" target="#b20">[20]</ref> in terms of the quality of the generated pseudo-labels, with its proposed progressive dropout attention effectively expanding the coverage of target regions beyond what CAM <ref type="bibr" target="#b20">[20]</ref> can achieve. Our proposed method outperformed all previous methods on both LUAD-HistoSeg and BCSS-WSSS datasets, with improvements of 2.64% and 5.42% over the second-best method, respectively (Table <ref type="table" target="#tab_2">2</ref>). Comparison on Segmentation Results. To further evaluate our proposed method, we trained a segmentation model using the extracted pseudo-labels and compared its performance with previous methods. Due to its heavy reliance on dataset-specific post-processing steps, HistoSegNet <ref type="bibr" target="#b4">[5]</ref> failed to produce the desired results on our datasets. As we have previously analyzed since the datasets we used are all multi-label images, it was challenging for the segmentation branch of TransWS <ref type="bibr" target="#b18">[18]</ref> to perform well, and it failed to provide an overall benefit to the model. Experimental results also indicate that the IoU scores of its segmentation branch were even lower than the pseudo-labels of the classification branch. By training the segmentation model of OEEM <ref type="bibr" target="#b10">[11]</ref> using the pseudo-labels extracted by CAM <ref type="bibr" target="#b20">[20]</ref> in Table <ref type="table" target="#tab_1">1</ref>, we can observe a significant improvement in the final segmentation results. The final segmentation results of MLPS <ref type="bibr" target="#b6">[7]</ref> showed some improvement compared to its pseudo-labels, indicating the effectiveness of the Multi-layer Pseudo Supervision and Classification Gate Mechanism strategy proposed by MLPS <ref type="bibr" target="#b6">[7]</ref>. Our segmentation performance surpassed all previous methods. Specifically, our mIoU scores exceeded the second-best method by 3.17% and 3.09% on LUAD-HistoSeg and BCSS-WSSS datasets, respectively. Additionally, it is worth noting that we did not use any strategies specifically designed for the segmentation stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>The results of our ablation experiments are presented in Table <ref type="table" target="#tab_3">3</ref>. We set the baseline as the framework shown in Fig. <ref type="figure" target="#fig_0">2</ref> with all text information and deep supervision strategy removed. It is evident that the addition of textual information increases our pseudo-label mIoU by 2.50%. Furthermore, including the deep supervision strategy and knowledge attention module improves our pseudo-label by 0.98% and 2.74%, respectively. These findings demonstrate the significant contribution of each proposed module to the overall improvement of the results. In order to demonstrate the effectiveness of fusing pseudo-labels from the last three stages, we have presented in Table <ref type="table" target="#tab_4">4</ref> the IoU scores for each stage's pseudolabels as well as the fused pseudo-labels. It can be observed that after fusing the pseudo-labels, not only have the IoU scores for each class substantially increased, but the mIoU score has also increased by 0.91% compared to the fourth stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose the TPRO to address the limitation of weakly supervised semantic segmentation on histopathology images by incorporating text supervision and external knowledge. We argue that image-level labels alone cannot provide sufficient information and that text supervision and knowledge attention can provide additional guidance to the model. The proposed method achieves the best results on two public datasets, LUAD-HistoSeg and BCSS-WSSS, demonstrating the superiority of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The framework of the proposed TPRO.</figDesc><graphic coords="3,56,46,435,92,339,37,130,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the pseudo labels generated by our proposed method and those generated by previous methods.</figDesc><table><row><cell>Dataset</cell><cell cols="2">LUAD-HistoSeg</cell><cell>BCSS-WSSS</cell></row><row><cell>Method</cell><cell>TE</cell><cell cols="2">NEC LYM TAS mIoU TUM STR LYM NEC mIoU</cell></row><row><cell>CAM [20]</cell><cell cols="3">69.66 72.62 72.58 66.88 70.44 66.83 58.71 49.41 51.12 56.52</cell></row><row><cell>Grad-CAM [15]</cell><cell cols="3">70.07 66.01 70.18 64.76 67.76 65.96 56.71 43.36 30.04 49.02</cell></row><row><cell cols="4">TransWS (CAM) [18] 65.92 60.16 73.34 69.11 67.13 64.85 58.17 44.96 50.60 54.64</cell></row><row><cell>MLPS [7]</cell><cell cols="3">71.72 76.27 73.53 67.67 72.30 70.76 61.07 50.87 52.94 58.91</cell></row><row><cell>TPRO (Ours)</cell><cell cols="3">74.82 77.55 76.40 70.98 74.94 77.18 63.77 54.95 61.43 64.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the final segmentation results between our method and the methods in previous years.</figDesc><table><row><cell>Dataset</cell><cell cols="2">LUAD-HistoSeg</cell><cell>BCSS-WSSS</cell></row><row><cell>Method</cell><cell>TE</cell><cell cols="2">NEC LYM TAS mIoU TUM STR LYM NEC mIoU</cell></row><row><cell>HistoSegNet [5]</cell><cell cols="3">45.59 36.30 58.28 50.82 47.75 33.14 46.46 29.05 1.91</cell><cell>27.64</cell></row><row><cell cols="4">TransWS (seg) [18] 57.04 49.98 59.46 58.59 56.27 44.71 36.49 41.72 38.08 40.25</cell></row><row><cell>OEEM [11]</cell><cell cols="3">73.81 70.49 71.89 69.48 71.42 74.86 64.68 48.91 61.03 62.37</cell></row><row><cell>MLPS [7]</cell><cell cols="3">73.90 77.48 73.61 69.53 73.63 74.54 64.45 52.54 58.67 62.55</cell></row><row><cell>TPRO (Ours)</cell><cell cols="3">75.80 80.56 78.14 72.69 76.80 77.95 65.10 54.55 64.96 65.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison the effectiveness of label text(LT), knowledge text(KT), and deep supervision(DS).</figDesc><table><row><cell>LT DS KT TE</cell><cell>NEC LYM TAS mIoU</cell></row><row><cell cols="2">68.11 75.24 64.95 66.57 68.72</cell></row><row><cell cols="2">72.39 72.44 71.37 68.67 71.22</cell></row><row><cell cols="2">72.41 72.11 74.21 70.07 72.20</cell></row><row><cell cols="2">74.82 77.55 76.40 70.98 74.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison of pseudo labels extracted from the single stage and our fused version.</figDesc><table><row><cell>TE</cell><cell>NEC LYM TAS mIoU</cell></row><row><cell cols="2">stage2 67.16 65.28 67.38 55.09 63.73</cell></row><row><cell cols="2">stage3 72.13 70.83 73.47 69.46 71.47</cell></row><row><cell cols="2">stage4 72.69 77.57 76.06 69.81 74.03</cell></row><row><cell cols="2">fusion 74.82 77.55 76.40 70.98 74.94</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/Kaushalya/medclip.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://drive.google.com/drive/folders/1E3Yei3Or3xJXukHIybZAgochxfn6FJpr.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://drive.google.com/drive/folders/1iS2Z0DsbACqGp7m6VDJbAcgzeXNEFr77.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported in part by the <rs type="funder">Natural Science Foundation of Ningbo City, China</rs>, under Grant <rs type="grantNumber">2021J052</rs>, in part by the <rs type="funder">Ningbo Clinical Research Center for Medical Imaging</rs> under Grant <rs type="grantNumber">2021L003</rs> (<rs type="projectName">Open</rs> Project <rs type="grantNumber">2022LYK-FZD06</rs>), in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62171377</rs>, in part by the <rs type="funder">Key Technologies Research and Development Program</rs> under Grant <rs type="grantNumber">2022YFC2009903/2022YFC2009900</rs>, in part by the <rs type="funder">Key Research and Development Program of Shaanxi Province, China</rs>, under Grant <rs type="grantNumber">2022GY-084</rs>, and in part by the <rs type="funder">Science and Technology Innovation Committee of Shenzhen Municipality, China</rs>, under Grants <rs type="grantNumber">JCYJ20220530161616036</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QJJkgfy">
					<idno type="grant-number">2021J052</idno>
				</org>
				<org type="funded-project" xml:id="_YmAKMcV">
					<idno type="grant-number">2021L003</idno>
					<orgName type="project" subtype="full">Open</orgName>
				</org>
				<org type="funding" xml:id="_dpDhPFg">
					<idno type="grant-number">2022LYK-FZD06</idno>
				</org>
				<org type="funding" xml:id="_WZgFR5v">
					<idno type="grant-number">62171377</idno>
				</org>
				<org type="funding" xml:id="_64DgVex">
					<idno type="grant-number">2022YFC2009903/2022YFC2009900</idno>
				</org>
				<org type="funding" xml:id="_EYrMUmC">
					<idno type="grant-number">2022GY-084</idno>
				</org>
				<org type="funding" xml:id="_dvw6qAN">
					<idno type="grant-number">JCYJ20220530161616036</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_11.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4981" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03323</idno>
		<title level="m">Publicly available clinical bert embeddings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured crowdsourcing enables convolutional segmentation of histology images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Amgad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3461" to="3467" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comprehensive analysis of weaklysupervised semantic segmentation in different image domains</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="361" to="384" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histosegnet: semantic segmentation of histological tissue type in whole slide images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rowsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Damaskinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10662" to="10671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dcan: deep contour-aware networks for accurate gland segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2487" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-layer pseudo-supervision for histopathology tissue semantic segmentation using patch-level classification labels</title>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102487</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Railroad is not a train: saliency as pseudopixel supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5495" to="5505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online easy example mining for weaklysupervised gland segmentation from histology images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, Part IV</title>
		<meeting>Part IV<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 18-22, 2022</date>
			<biblScope unit="page" from="578" to="587" />
		</imprint>
	</monogr>
	<note>Assisted Intervention-MICCAI 2022: 25th International Conference</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_55</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_55" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Radiology Objects in COntext (ROCO): a multimodal image dataset</title>
		<author>
			<persName><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01364-6_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01364-6_20" />
	</analytic>
	<monogr>
		<title level="m">LABELS/CVII/STENT -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11043</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast and accurate tumor segmentation of histology images using persistent homology and deep convolutional features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Qaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradcam: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wider or deeper: revisiting the resnet model for visual recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Segformer: simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12077" to="12090" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transws: Transformer-based weakly supervised histology image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-21014-3_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-21014-3_38" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Medical Imaging: 13th International Workshop, MLMI 2022, Held in Conjunction with MICCAI 2022, Singapore</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-18">September 18, 2022. 2022</date>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Triple u-net: hematoxylin-aware nuclei segmentation with progressive dense feature aggregation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101786</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
