<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anatomy-Driven Pathology Detection on Chest X-rays</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Philip</forename><surname>Müller</surname></persName>
							<email>philip.j.mueller@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for AI in Medicine</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felix</forename><surname>Meissen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for AI in Medicine</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Johannes</forename><surname>Brandt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for AI in Medicine</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Georgios</forename><surname>Kaissis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for AI in Medicine</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Helmholtz Zentrum Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for AI in Medicine</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Anatomy-Driven Pathology Detection on Chest X-rays</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="57" to="66"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">DED3A82A1FB51F5A02D4EA45CFA7764B</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pathology detection</term>
					<term>Anatomical regions</term>
					<term>Chest X-rays</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pathology detection and delineation enables the automatic interpretation of medical scans such as chest X-rays while providing a high level of explainability to support radiologists in making informed decisions. However, annotating pathology bounding boxes is a timeconsuming task such that large public datasets for this purpose are scarce. Current approaches thus use weakly supervised object detection to learn the (rough) localization of pathologies from image-level annotations, which is however limited in performance due to the lack of bounding box supervision. We therefore propose anatomy-driven pathology detection (ADPD), which uses easy-to-annotate bounding boxes of anatomical regions as proxies for pathologies. We study two training approaches: supervised training using anatomy-level pathology labels and multiple instance learning (MIL) with image-level pathology labels. Our results show that our anatomy-level training approach outperforms weakly supervised methods and fully supervised detection with limited training samples, and our MIL approach is competitive with both baseline approaches, therefore demonstrating the potential of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chest radiographs (chest X-rays) represent the most widely utilized type of medical imaging examination globally and hold immense significance in the detection of prevalent thoracic diseases, including pneumonia and lung cancer, making them a crucial tool in clinical care <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref>. Pathology detection and localization -for brevity we will use the term pathology detection throughout this workenables the automatic interpretation of medical scans such as chest X-rays by predicting bounding boxes for detected pathologies. Unlike classification, which only predicts the presence of pathologies, it provides a high level of explainability supporting radiologists in making informed decisions. However, while image classification labels can be automatically extracted from electronic health records or radiology reports <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref>, this is typically not possible for bounding boxes, thus limiting the availability of large datasets for pathology detection. Additionally, manually annotating pathology bounding boxes is a time-consuming task, further exacerbating the issue. The resulting scarcity of large, publicly available datasets with pathology bounding boxes limits the use of supervised methods for pathology detection, such that current approaches typically follow weakly supervised object detection approaches, where only classification labels are required for training. However, as these methods are not guided by any form of bounding boxes, their performance is limited.</p><p>We, therefore, propose a novel approach towards pathology detection that uses anatomical region bounding boxes, solely defined on anatomical structures, as proxies for pathology bounding boxes. These region boxes are easier to annotate -the physiological shape of a healthy subject's thorax can be learned relatively easily by medical students -and generalize better than those of pathologies, such that huge labeled datasets are available <ref type="bibr" target="#b20">[21]</ref>. In summary:</p><p>-We propose anatomy-driven pathology detection (ADPD), a pathology detection approach for chest X-rays, trained with pathology classification labels together with anatomical region bounding boxes as proxies for pathologies. -We study two training approaches: using localized (anatomy-level) pathology labels for our model Loc-ADPD and using image-level labels with multiple instance learning (MIL) for our model MIL-ADPD. -We train our models on the Chest ImaGenome <ref type="bibr" target="#b20">[21]</ref> dataset and evaluate on NIH ChestX-ray 8 <ref type="bibr" target="#b19">[20]</ref>, where we found that our Loc-ADPD model outperforms both, weakly supervised methods and fully supervised detection with a small training set, while our MIL-ADPD model is competitive with supervised detection and slightly outperforms weakly supervised approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Weakly Supervised Pathology Detection. Due to the scarcity of bounding box annotations, pathology detection on chest X-rays is often tackled using weakly supervised object detection with Class Activation Mapping (CAM) <ref type="bibr" target="#b24">[25]</ref>, which only requires image-level classification labels. After training a classification model with global average pooling (GAP), an activation heatmap is computed by classifying each individual patch (extracted before pooling) with the trained classifier, before thresholding this heatmap for predicting bounding boxes. Inspired by this approach, several methods have been developed for chest X-rays <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>. While CheXNet <ref type="bibr" target="#b13">[14]</ref> follows the original approach, the method provided with the NIH ChestX-ray 8 dataset <ref type="bibr" target="#b19">[20]</ref> and the STL method <ref type="bibr" target="#b5">[6]</ref> use Logsumexp (LSE) pooling <ref type="bibr" target="#b12">[13]</ref>, while the MultiMap model <ref type="bibr" target="#b22">[23]</ref> uses max-min pooling as first proposed for the WELDON <ref type="bibr" target="#b2">[3]</ref> method. Unlike our method, none of these methods utilize anatomical regions as proxies for predicting pathology bounding boxes, therefore leading to inferior performance. Localized Pathology Classification. Anatomy-level pathology labels have been utilized before to train localized pathology classifiers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref> or to improve weakly supervised pathology detection <ref type="bibr" target="#b23">[24]</ref>. Along with the Chest ImaGenome dataset <ref type="bibr" target="#b20">[21]</ref> several localized pathology classification models have been proposed which use a Faster R-CNN <ref type="bibr" target="#b15">[16]</ref> to extract anatomical region features before predicting observed pathologies for each region using either a linear model or a GCN model based on pathology co-occurrences. This approach has been further extended to use GCNs on anatomical region relationships <ref type="bibr" target="#b0">[1]</ref>. While utilizing the same form of supervision as our method, these methods do not tackle pathology detection.</p><p>In AGXNet <ref type="bibr" target="#b23">[24]</ref>, anatomy-level pathology classification labels are used to train a weakly-supervised pathology detection model. Unlike our and the other described methods, it does however not use anatomical region bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>Figure <ref type="figure" target="#fig_0">1</ref> provides an overview of our method. Given a chest X-ray, we apply a DenseNet121 <ref type="bibr" target="#b4">[5]</ref> backbone and extract patch-wise features by using the feature map after the last convolutional layer (before GAP). We then apply a lightweight object detection model consisting of a single DETR <ref type="bibr" target="#b1">[2]</ref> decoder layer to detect anatomical regions. Following <ref type="bibr" target="#b1">[2]</ref>, we use learned query tokens attending to patch features in the decoder layer, where each token corresponds to one predicted bounding box. As no anatomical region can occur more than once in each chest X-ray, each query token is assigned to exactly one pre-defined anatomical region, such that the number of tokens equals the number of anatomical regions. This one-to-one assignment of tokens and regions allows us to remove the Hungarian matching used in <ref type="bibr" target="#b1">[2]</ref>. As described next, the resulting per-region features from the output of the decoder layer will be used for predictions on each region.</p><p>For predicting whether the associated region is present, we use a binary classifier with a single linear layer, for bounding box prediction we use a three-layer MLP followed by sigmoid. We consider the prediction of observed pathologies as a multi-label binary classification task and use a single linear layer (followed by sigmoid) to predict the probabilities of all pathologies. Each of these predictors is applied independently to each region with their weights shared across regions.</p><p>We experimented with more complex pathology predictors like an MLP or a transformer layer but did not observe any benefits. We also did not observe improvements when using several decoder layers and observed degrading performance when using ROI pooling to compute region features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference</head><p>During inference, the trained model predicts anatomical region bounding boxes and per-region pathology probabilities, which are then used to predict pathology bounding boxes in two steps, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. In step (i), pathology probabilities are first thresholded and for each positive pathology (with probability larger than the threshold) the bounding box of the corresponding anatomical region is predicted as its pathology box, using the pathology probability as box score. This means, if a region contains several predicted pathologies, then all of its predicted pathologies share the same bounding box during step (i). In step (ii), weighted box fusion (WBF) <ref type="bibr" target="#b18">[19]</ref> merges bounding boxes of the same pathology with IoU-overlaps above 0.03 and computes weighted averages (using box scores as weights) of their box coordinates. As many anatomical regions are at least partially overlapping, and we use a small IoU-overlap threshold, this allows the model to either pull the predicted boxes to relevant subparts of an anatomical region or to predict that pathologies stretch over several regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>The anatomical region detector is trained using the DETR loss <ref type="bibr" target="#b1">[2]</ref> with fixed oneto-one matching (i.e. without Hungarian matching). For training the pathology classifier, we experiment with two different levels of supervision (Fig. <ref type="figure" target="#fig_2">3</ref>). For our Loc-ADPD model, we utilize anatomy-level pathology classification labels. Here, the target set of observed pathologies is available for each anatomical region individually such that the pathology observation prediction can directly be trained for each anatomical region. We apply the ASL <ref type="bibr" target="#b16">[17]</ref> loss function independently on each region-pathology pair and average the results over all regions and pathologies. The decoder feature dimension is set to 512.</p><p>For our MIL-ADPD model, we experiment with a weaker form of supervision, where pathology classification labels are only available on the per-image level. We utilize multiple instance learning (MIL), where an image is considered a bag of individual instances (i.e. the anatomical regions), and only a single label (per pathology) is provided for the whole bag, which is positive if any of its instances is positive. To train using MIL, we first aggregate the predicted pathology probabilities of each region over all detected regions in the image using LSE pooling <ref type="bibr" target="#b12">[13]</ref>, acting as a smooth approximation of max pooling. The resulting per-image probability for each pathology is then trained using the ASL <ref type="bibr" target="#b16">[17]</ref> loss. In this model, the decoder feature dimension is set to 256.</p><p>In both models, the ASL loss is weighted by a factor of 0.01 before adding it to the DETR loss. We train using AdamW <ref type="bibr" target="#b11">[12]</ref> with a learning rate of 3e-5 (Loc-ADPD) or 1e-4 (MIL-ADPD) and weight decay 1e-5 (Loc-ADPD) or 1e-4 (MIL-ADPD) in batches of 128 samples with early stopping (with 20 000 steps patience) for roughly 7 h on a single Nvidia RTX A6000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dataset</head><p>Training Dataset. We train on the Chest ImaGenome dataset [4,21,22]<ref type="foot" target="#foot_0">1</ref> , consisting of roughly 240 000 frontal chest X-ray images with corresponding scene graphs automatically constructed from free-text radiology reports. It is derived from the MIMIC-CXR dataset <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, which is based on imaging studies from 65 079 patients performed at Beth Israel Deaconess Medical Center in Boston, US. Amongst other information, each scene graph contains bounding boxes for 29 unique anatomical regions with annotated attributes, where we consider positive anatomical finding and disease attributes as positive labels for pathologies, leading to binary anatomy-level annotations for 55 unique pathologies. We consider the image-level label for a pathology to be positive if any region is positively labeled with that pathology.</p><p>We use the provided jpg-images <ref type="bibr" target="#b10">[11]</ref> 2 and follow the official MIMIC-CXR training split but only keep samples containing a scene graph with at least five valid region bounding boxes, resulting in a total of 234 307 training samples.</p><p>During training, we use random resized cropping with size 224 × 224, apply contrast and brightness jittering, random affine augmentations, and Gaussian blurring.</p><p>Evaluation Dataset and Class Mapping. We evaluate our method on the subset of 882 chest X-ray images with pathology bounding boxes, annotated by radiologists, from the NIH ChestXray-8 (CXR8) dataset <ref type="bibr" target="#b19">[20]</ref> 3 from the National Institutes of Health Clinical Center in the US. We use 50% for validation and keep the other 50% as a held-out test set. Note that for evaluation only pathology bounding boxes are required (to compute the metrics), while during training only anatomical region bounding boxes (without considering pathologies) are required. All images are center-cropped and resized to 224 × 224.</p><p>The dataset contains bounding boxes for 8 unique pathologies. While partly overlapping with the training classes, a one-to-one correspondence is not possible for all classes. For some evaluation classes, we therefore use a many-to-one mapping where the class probability is computed as the mean over several training classes. We refer to the supp. material for a detailed study on class mappings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup and Baselines</head><p>We compare our method against several weakly supervised object detection methods (CheXNet <ref type="bibr" target="#b13">[14]</ref>, STL <ref type="bibr" target="#b5">[6]</ref>, GradCAM <ref type="bibr" target="#b17">[18]</ref>, CXR <ref type="bibr" target="#b19">[20]</ref>, WELDON <ref type="bibr" target="#b2">[3]</ref>, Mul-tiMap Model <ref type="bibr" target="#b22">[23]</ref>, LSE Model <ref type="bibr" target="#b12">[13]</ref>), trained on the CXR8 training set using only image-level pathology labels. Note that some of these methods focus on (imagelevel) classification and do not report quantitative localization results. Nevertheless, we compare their localization approaches quantitatively with our method. We also use AGXNet <ref type="bibr" target="#b23">[24]</ref> for comparison, a weakly supervised method trained using anatomy-level pathology labels but without any bounding box supervision. It was trained on MIMIC-CXR (sharing the images with our method) with labels from RadGraph <ref type="bibr" target="#b7">[8]</ref> and finetuned on the CXR8 training set with imagelevel labels. Additionally, we also compare with a Faster-RCNN <ref type="bibr" target="#b15">[16]</ref> trained on a small subset of roughly 500 samples from the CXR8 training set that have been 2 https://physionet.org/content/mimic-cxr-jpg/2.0.0/ (PhysioNet Credentialed Health Data License 1.5.0). annotated with pathology bounding boxes by two medical experts, including one board-certified radiologist.</p><p>Table <ref type="table">1</ref>. Results on the NIH ChestX-ray 8 dataset <ref type="bibr" target="#b19">[20]</ref>. Our models Loc-ADPD and MIL-ADPD, trained using anatomy (An) bounding boxes, both outperform all weakly supervised methods trained with image-level pathology (Pa) and anatomy-level pathology (An-Pa) labels by a large margin. MIL-ADPD is competitive with the supervised baseline trained with pathology (Pa) bounding boxes, while Loc-ADPD outperforms it by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Supervision IoU@10-70 IoU@10 IoU@30 IoU@50 Box Class mAP AP loc-acc AP loc-acc AP loc-acc MIL-ADPD (ours) An Pa For all models, we only consider the predicted boxes with the highest box score per pathology, as the CXR8 dataset never contains more than one box per pathology. We report the standard object detection metrics average precision (AP) at different IoU-thresholds and the mean AP (mAP) over thresholds (0.1, 0.2, . . . , 0.7), commonly used thresholds on this dataset <ref type="bibr" target="#b19">[20]</ref>. Additionally, we report the localization accuracy (loc-acc) <ref type="bibr" target="#b19">[20]</ref>, a common localization metric on this dataset, where we use a box score threshold of 0.7 for our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pathology Detection Results</head><p>Comparison with Baselines. Table <ref type="table">1</ref> shows the results of our MIL-ADPD and Loc-ADPD models and all baselines on the CXR8 test set. Compared to the best weakly supervised method with image-level supervision (CheXNet) our methods improve by large margins (MIL-ADPD by Δ+35.2%, Loc-ADPD by Δ+87.8% in mAP). Improvements are especially high when considering larger IoU-thresholds and huge improvements are also achieved in loc-acc at all thresholds. Both models also outperform AGXNet (which uses anatomy-level supervision) by large margins (MIL-ADPD by Δ + 47.9% and Loc-ADPD by Δ + 105.5% mAP), while improvements on larger thresholds are smaller here. Even when compared to Faster R-CNN trained on a small set of fully supervised samples, MIL-ADPD is competitive (Δ + 6.5%), while Loc-ADPD improves by Δ + 48.0%. However, on larger thresholds (IoU@50) the supervised baseline slightly outperforms MIL-ADPD, while Loc-ADPD is still superior. This shows that using anatomical regions as proxies is an effective approach to tackle pathology detection. While using image-level annotations (MIL-ADPD) already gives promising results, the full potential is only achieved using anatomy-level supervision (Loc-ADPD). Unlike Loc-ADPD and MIL-ADPD, all baselines were either trained or finetuned on the CXR8 dataset, showing that our method generalizes well to unseen datasets and that our class mapping is effective.</p><p>For detailed results per pathology we refer to the supp. material. We found that the improvements of MIL-ADPD are mainly due to improved performance on Cardiomegaly and Mass detection, while Loc-ADPD consistently outperforms all baselines on all classes except Nodule, often by a large margin.</p><p>Ablation Study. In Table <ref type="table">1</ref> we also show the results of different ablation studies. Without WBF, results degrade for both of our models, highlighting the importance of merging region boxes. Combining the training strategies of Loc-ADPD and MIL-ADPD does not lead to an improved performance. Different class mappings between training and evaluation set are studied in the supp. material.</p><p>Qualitative Results. As shown in Fig. <ref type="figure" target="#fig_4">4</ref> Loc-ADPD detects cardiomegaly almost perfectly, as it is always exactly localized at one anatomical region. Other pathologies are detected but often with too large or too small boxes as they only cover parts of anatomical regions or stretch over several of them, which cannot be completely corrected using WBF. Detection also works well for predicting several overlapping pathologies. For qualitative comparisons between Loc-ADPD and MIL-ADPD, we refer to the supp. material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>Limitations. While our proposed ADPD method outperforms all competing models, it is still subject to limitations. First, due to the dependence on region proxies, for pathologies covering only a small part of a region, our models predict the whole region, as highlighted by their incapability to detect nodules. We however note that in clinical practice, chest X-rays are not used for the final diagnosis of such pathologies and even rough localization can be beneficial. Additionally, while not requiring pathology bounding boxes, our models still require supervision in the form of anatomical region bounding boxes, and Loc-ADPD requires anatomy-level labels. However, anatomical bounding boxes are easier to annotate and predict than pathology bounding boxes, and the used anatomylevel labels were extracted automatically from radiology reports <ref type="bibr" target="#b20">[21]</ref>. While our work is currently limited to chest X-rays, we see huge potential for modalities where abnormalities can be assigned to meaningful regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>We proposed a novel approach tackling pathology detection on chest X-rays using anatomical region bounding boxes. We studied two training approaches, using anatomy-level pathology labels and using image-level labels with MIL. Our experiments demonstrate that using anatomical regions as proxies improves results compared weakly supervised methods and supervised training on little data, thus providing a promising direction for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Overview of our method. Anatomical regions are first detected using a CNN backbone and a shallow detector. For each region, observed pathologies are predicted using a shared classifier. Bounding boxes for each pathology are then predicted by considering regions with positive predictions and fusing overlapping boxes.</figDesc><graphic coords="3,81,51,76,97,202,60,117,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Inference. For each pathology, the regions with pathology probability above a threshold are predicted as bounding boxes, which are then fused if overlapping.</figDesc><graphic coords="4,65,79,53,81,292,66,68,05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Training. Loc-ADPD: Pathology predictions of regions are directly trained using anatomy-level supervision. MIL-ADPD: Region predictions are first aggregated using LSE pooling and then trained using image-level supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>3 https://www.kaggle.com/datasets/nih-chest-xrays/data (CC0: Public Domain).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Qualitative results of Loc-ADPD, with predicted (solid) and target (dashed) boxes. Cardiomegaly (red) is detected almost perfectly, as it is always exactly localized at one anatomical region. Other pathologies like atelectasis (blue), effusion (green), or pneumonia (cyan) are detected but often with non-perfect overlapping boxes. Detection also works well for predicting several overlapping pathologies (second from left). (Color figure online)</figDesc><graphic coords="8,47,31,53,90,329,95,77,80" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://physionet.org/content/chest-imagenome/1.0.0 (PhysioNet Credentialed Health Data License 1.5.0).</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_6.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">AnaXNet: anatomy aware multi-label finding classification in chest X-ray</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Agu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_77</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_77" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-8_13" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Weldon: weakly supervised learning of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.513</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.513" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4743" to="4752" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="215" to="e220" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.243" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-transfer learning for weakly supervised lesion localization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-E</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="239" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">CheXpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.3301590</idno>
		<idno>01.3301590</idno>
		<ptr target="https://doi.org/10.1609/aaai" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
		<respStmt>
			<orgName>AAAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Radgraph: extracting clinical entities and relations from radiology reports</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mimic-cxr database (version 2.0.0)</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PhysioNet</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07042</idno>
		<title level="m">Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298780</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298780" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">CheXnet: radiologist-level pneumonia detection on chest xrays with deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1711.05225</idno>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1711.05225" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interpretation of plain chest roentgenogram</title>
		<author>
			<persName><forename type="first">S</forename><surname>Raoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feigin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Irugulpati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Rosenow</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chest</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="545" to="558" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Asymmetric loss for multi-label classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV48922.2021.00015</idno>
		<ptr target="https://doi.org/10.1109/ICCV48922.2021.00015" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="82" to="91" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Grad-CAM: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.74</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.74" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weighted boxes fusion: ensembling boxes from different object detection models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Solovyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gabruseva</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.imavis.2021.104117</idno>
		<ptr target="https://doi.org/10.1016/j.imavis.2021.104117" />
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">104117</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Chestx-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.369</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.369" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Chest imagenome dataset for clinical reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chest imagenome dataset (version 1.0.0)</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PhysioNet</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Weakly supervised deep learning for thoracic disease classification and localization on chest x-rays</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ACM BCB</publisher>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Anatomy-guided weaklysupervised abnormality localization in chest x-rays</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deible</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_63</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_63" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="658" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.319</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.319" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
