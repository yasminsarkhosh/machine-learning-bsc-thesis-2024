<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction</title>
				<funder ref="#_BdQ75g4">
					<orgName type="full">National Key Technology Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_V4gbc3q">
					<orgName type="full">Key Research and Development Program of Zhejiang Province</orgName>
				</funder>
				<funder ref="#_WMB6eHG">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_AHn7Q2H">
					<orgName type="full">Talent Program of Zhejiang Province</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rui</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Optical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Modern Optical Instrumentation</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">The Center for Advanced Medical Computing and Analysis</orgName>
								<orgName type="institution" key="instit1">Massachusetts General Hospital</orgName>
								<orgName type="institution" key="instit2">Harvard Medical School</orgName>
								<address>
									<postCode>02114</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunmei</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<postCode>32611</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyungsang</forename><surname>Kim</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">The Center for Advanced Medical Computing and Analysis</orgName>
								<orgName type="institution" key="instit1">Massachusetts General Hospital</orgName>
								<orgName type="institution" key="instit2">Harvard Medical School</orgName>
								<address>
									<postCode>02114</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marcio</forename><forename type="middle">Aloisio Bezerra Cavalcanti</forename><surname>Rockenbach</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Quanzheng</forename><surname>Li</surname></persName>
							<email>li.quanzheng@mgh.harvard.edu</email>
							<affiliation key="aff2">
								<orgName type="department">The Center for Advanced Medical Computing and Analysis</orgName>
								<orgName type="institution" key="instit1">Massachusetts General Hospital</orgName>
								<orgName type="institution" key="instit2">Harvard Medical School</orgName>
								<address>
									<postCode>02114</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Data Science Office</orgName>
								<orgName type="institution">Massachusetts General Brigham</orgName>
								<address>
									<postCode>02116</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huafeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Optical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Modern Optical Instrumentation</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="153" to="162"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">02706486053DA1EF5F1F452A1AB80AE7</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_15</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Image reconstruction • Positron emission tomography</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning based PET image reconstruction methods have achieved promising results recently. However, most of these methods follow a supervised learning paradigm, which rely heavily on the availability of high-quality training labels. In particular, the long scanning time required and high radiation exposure associated with PET scans make obtaining these labels impractical. In this paper, we propose a dual-domain unsupervised PET image reconstruction method based on learned descent algorithm, which reconstructs high-quality PET images from sinograms without the need for image labels. Specifically, we unroll the proximal gradient method with a learnable l2,1 norm for PET image reconstruction problem. The training is unsupervised, using measurement domain loss based on deep image prior as well as image domain loss based on rotation equivariance property. The experimental results demonstrate the superior performance of proposed method compared with maximum-likelihood expectation-maximization (MLEM), total-variation regularized EM (EM-TV) and deep image prior based method (DIP).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Positron Emission Tomography (PET) is a widely used modality in functional imaging for oncology, cardiology, neurology, and medical research <ref type="bibr" target="#b0">[1]</ref>. However, PET images often suffer from a high level of noise due to several physical degradation factors as well as the ill-conditioning of the PET reconstruction problem.</p><p>As a result, the quality of PET images can be compromised, leading to difficulties in accurate diagnosis. Deep learning (DL) techniques, especially supervised learning, have recently garnered considerable attention and show great promise in PET image reconstruction compared with traditional analytical methods and iterative methods. Among them, four primary approaches have emerged: DL-based postdenoising <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, end-to-end direct learning <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, deep learning regularized iterative reconstruction <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> and deep unrolled methods <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>.</p><p>DL-based post denoising methods are relatively straightforward to implement but can not reduce the lengthy reconstruction time and its results are significantly affected by the pre-reconstruction algorithm. End-to-end direct learning methods utilize deep neural networks to learn the directing mapping from measurement sinogram to PET image. Without any physical constraints, these methods can be unstable and extremely data-hungry. Deep learning regularized iterative reconstruction methods utilize a deep neural network as a regularization term within the iterative reconstruction process to regularize the image estimate and guide the reconstruction process towards a more accurate and stable solution. Despite the incorporation of deep learning, the underlying mathematical framework and assumptions of deep learning regularized iterative methods still rely on the conventional iterative reconstruction methods. Deep unrolled methods utilize a DNN to unroll the iterative reconstruction process and to learn the mapping from sinogram to the reconstructed PET images, which potentially result in more accurate and explainable image reconstruction. Deep unrolled methods have demonstrated improved interpretabillity and yielded inspiring outcomes.</p><p>However, the aforementioned approaches for PET image reconstruction depend on high quality ground truths as training labels, which can be diffi-cult and expensive to obtain. This challenge is further compounded by the high dose exposure associated with PET imaging. Unsupervised/self supervised learning has gained considerable interest in medical imaging, owing to its ability to mitigate the need for high-quality training labels. Gong et al. proposed a PET image reconstruction approach using the deep image prior (DIP) framework <ref type="bibr" target="#b14">[15]</ref>, which employed a randomly initialized Unet as a prior. In another study, Fumio et al. proposed a simplified DIP reconstruction framework with a forward projection model, which reduced the network parameters <ref type="bibr" target="#b15">[16]</ref>. Shen et al. proposed a DeepRED framework with an approximate Bayesian framework for unsupervised PET image reconstruction <ref type="bibr" target="#b16">[17]</ref>. These methods all utilize generative models to generate PET images from random noise or MRI prior images and use sinogram to design loss functions. However, these generative models tend to favor low frequencies and sometimes lack of mathematical interpretability. In the absence of anatomic priors, the network convergence can take a considerable amount of time, resulting in prolonged reconstruction times. Recently, equivariant property <ref type="bibr" target="#b17">[18]</ref> of medical imaging system is proposed to train the network without labels, which shows the potential for the designing of PET reconstruction algorithms.</p><p>In this paper, we propose a dual-domain unsupervised learned descent algorithm for PET image reconstruction, which is the first attempt to combine unsupervised learning and deep unrolled method for PET image reconstruction. The main contributions of this work are summarized as follows: 1) a novel model based deep learning method for PET image reconstruction is proposed with a learnable l 2,1 norm for more general and robust feature sparsity extraction of PET images; 2) a dual domain unsupervised training strategy is proposed, which is plug-and-play and does not need paired training samples; 3) without any anatomic priors, the proposed method shows superior performance both quantitatively and visually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods and Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>As a typical inverse problem, PET image reconstruction can be modeled in a variational form and cast as an optimization task, as follows:</p><formula xml:id="formula_0">min φ(x; y, θ) = -L(y|x) + P (x; θ) (1) L(y|x) = i y i log y i - i y i (2) y = Ax + b (<label>3</label></formula><formula xml:id="formula_1">)</formula><p>where y is the measured sinogram data, y is the mean of the measured sinogram.</p><p>x is the PET activity image to be reconstructed, L(y|x) is the Poisson loglikelihood of measured sinogram data. P (x; θ) is the penalty term with learnable parameter θ. A ∈ R I×J is the system response matrix, with A ij representing the probabilities of detecting an emission from voxel j at detector i.</p><p>We expect that the parameter θ in penalty term P can be learned from the training data like many other deep unrolling methods. However, most of these methods directly replace the penalty term <ref type="bibr" target="#b13">[14]</ref> or its gradient <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref> with a network, which loses some mathematical rigor and interpretablities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parametric Form of Learnable Regularization</head><p>We choose to parameterize P as the l 2,1 norm with a feature extraction operator g(x) to be learned in the training data. The smooth nonlinear mapping g is used to extract sparse features and the l 2,1 norm is used as a robust and effective sparse feature regularization. Specifically, we formulate P as follows <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_2">P (x; θ) = ||g θ (x)|| 2,1 = m i=1 ||g i,θ (x)||<label>(4)</label></formula><p>where g i,θ (x) is i-th feature vector. We choose g as a multi-layered CNN with nonlinear activation function σ, and σ is a smoothed ReLU:</p><formula xml:id="formula_3">σ(x) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0, if x -δ, 1 4δ x 2 + 1 2 x + σ 4 , if -δ &lt; x &lt; δ, x, if x δ,<label>(5)</label></formula><p>In this case, the gradient ∇g can be computed directly. The Nesterov's smoothing technique is used in P for the derivative calculation of the l 2,1 norm through smooth approximation:</p><formula xml:id="formula_4">P ε (x) = 1 2ε ||g i (x)|| 2 + (||g i (x) - ε 2 ||) (6) ∇P ε (x) = ∇g i (x) T g i (x) ε + ∇g i (x) T g i (x) ||g i (x)||<label>(7)</label></formula><p>where parameter ε controls how close the approximation P ε to the original P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1. Learned Descent Algorithm for PET image reconstruction</head><p>Input: Image initialization x0, ρ, γ ∈ (0, 1), ε0, σ, τ &gt; 0, maximum number of iteration I, total phase numbers K and measured Sinogram y 1:</p><formula xml:id="formula_5">for i ∈ [1, I] do 2: r k = x k-1 + α k-1 ( A T y A x k-1 +b -A T 1) 3: u k = r k -τ k-1 ∇P ε k-1 (r k ) 4: repeat 5: v k = x k-1 -α k-1 ∇(-L(y|x k-1 )) -α k-1 ∇Pε k-1 (x k-1 ) 6: until φε k-1 (v k ) φε k-1 (x k-1 ) 7: If φ(u k ) φ(v k ), x k = u k ; otherwise, x k = v k 8: If ||∇φε k-1 (x k )|| &lt; σγε k-1 , ε k = γε k-1 ; otherwise, ε k = ε k-1 9: end for 10: return x K ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learned Descent Algorithm for PET</head><p>With the parametric form of learnable regularization given above, we rewrite Eq. 1 as the objective function: min φ(x; y, θ) = -L(y|x) + P ε (x; θ) <ref type="bibr" target="#b7">(8)</ref> We unrolled the learned descent algorithm in several phases as shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>In each phase k -1, we apply the proximal gradient step in Eq. 8:</p><formula xml:id="formula_6">r k = x k-1 -α k-1 ∇(-L(y|x)) = x k-1 + α k-1 ( A T y Ax k-1 + b - A T 1)<label>(9)</label></formula><formula xml:id="formula_7">x k = prox α k-1 Pε k-1 (r k )<label>(10)</label></formula><p>where the proximal operator is defined as:</p><formula xml:id="formula_8">prox αP (r) = arg min x { 1 2α ||x -r|| 2 + P (x)}<label>(11)</label></formula><p>In order to have a close form solution of the proximal operator, we perform a Taylor approximation of P ε k-1 :</p><formula xml:id="formula_9">Pε k-1 (x) = P ε k-1 (r k ) + (x -r k ) • ∇ Pε k-1 (r k ) + 1 2β k-1 ||x -r k || 2<label>(12)</label></formula><p>After discarding higher-order constant terms, we can simplify the Eq. 10 as:</p><formula xml:id="formula_10">u k = prox α k-1 Pε k-1 (r k ) = r k -τ k-1 ∇ Pε k-1 (r k )<label>(13)</label></formula><p>where α k-1 and β k-1 are two parameters greater than 0 and</p><formula xml:id="formula_11">τ k-1 = α k-1 β k-1 α k-1 +β k-1 .</formula><p>We also calculate a close-form safeguard v k as:</p><formula xml:id="formula_12">v k = x k-1 -α k-1 ∇(-L(y|x k-1 )) -α k-1 ∇P ε k-1 (x k-1 )<label>(14)</label></formula><p>The line search strategy is used by shrinking α k-1 to ensure objective function decay. We choose the u k or v k with smaller objection function value φ ε k-1 to be the next x k . The smoothing parameter ε k-1 is shrinkage by γ ∈ (0, 1) if the ||∇φ ε k-1 (x k )|| &lt; σγε k-1 is satisfied. The whole flow is shown in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dual-Domain Unsupervised Training</head><p>The whole reconstruction network is indicated by f θ with learned parameter θ.</p><p>Inspired by Deep image prior <ref type="bibr" target="#b19">[20]</ref> and equivariance <ref type="bibr" target="#b17">[18]</ref> of PET imaging system, the proposed dual-domain unsupervised training loss function is formulated as:</p><formula xml:id="formula_13">L dual = L image + λL measure (<label>15</label></formula><formula xml:id="formula_14">)</formula><p>where λ is the parameter that controls the ratio of different domain loss function, which was set to 0.1 in the experiments. For image domain loss L image , the equivariance constraint is used. For example, if the test sample x t first undergoes an equivariant transformation, such as rotation, we obtain x tr . Subsequently, we perform a PET scan to obtain the sinogram data of x tr and x t . The image reconstructed by the f θ of these two sinogram should also keep this rotation properties. The L image is formulate as: where T r denotes the rotation operator, A is the forward projection which also can be seen as a measurement operator. For sinogram domain loss L measure , the data argumentation with random noise ξ is performed on y:</p><formula xml:id="formula_15">L image = ||T r f θ (y) xt -f θ (A(T r (f θ (y)) xtr ))|| 2<label>(16)</label></formula><formula xml:id="formula_16">L measure = ||(y + ξ) -Af θ (y + ξ)|| 2<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Implementation Details and Reference Methods</head><p>We implemented DULDA using Pytorch 1.7 on a NVIDIA GeForce GTX Titan X. The Adam optimizer with a learning rate of 10 -4 was used and trained for 100 epochs with batch size of 8. The total unrolled phase was 4. The image x 0 was initialized with the values of one. The smoothing parameter ε 0 and δ were initialized to be 0.001 and 0.002. The step-size α 0 and β 0 were initialized to be 0.01 and 0.02. The system matrix was computed by using Michigan Image Reconstruction Toolbox (MIRT) with a strip-integral model <ref type="bibr" target="#b20">[21]</ref>. The proposed DULDA was compared with MLEM <ref type="bibr" target="#b21">[22]</ref>, total variation regularized EM (EM-TV) <ref type="bibr" target="#b22">[23]</ref> and deep image prior method (DIP) <ref type="bibr" target="#b15">[16]</ref>. For both MLEM and EM-TV, 25 iterations were adopted. The penalty parameter for EM-TV was 2e -5 .</p><p>For DIP, we used random noise as input and trained 14000 epochs with the same training settings as DULDA to get the best results before over-fitting.</p><p>The proposed method can also be trained in a fully supervised manner (we call it SLDA). The loss is the mean square error between the output and the label image. To further demonstrate the effectiveness, we compared SLDA with DeepPET <ref type="bibr" target="#b4">[5]</ref> and FBSEM <ref type="bibr" target="#b10">[11]</ref>, the training settings remained the same. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Evaluations</head><p>Forty 128 × 128 × 40 3D Zubal brain phantoms <ref type="bibr" target="#b23">[24]</ref> were used in the simulation study as ground truth, and one clinical patient brain images with different dose level were used for the robust analysis. Two tumors with different size were added in each Zubal brain phantom. The ground truth images were firstly forward-projected to generate the noise-free sinogram with count of 10 6 for each transverse slice and then Poisson noise were introduced. 20 percent of uniform random events were simulated. In total, 1600 (40 × 40) 2D sinograms were generated. Among them, 1320 (33 samples) were used in training, 200 (5 samples) for testing, and 80 (2 samples) for validation. A total of 5 realizations were simulated and each was trained/tested independently for bias and variance calculation <ref type="bibr" target="#b14">[15]</ref>. We used peak signal to noise ratio (PSNR), structural similarity index (SSIM) and root mean square error (RMSE) for overall quantitative analysis. The contrast recovery coefficient (CRC) <ref type="bibr" target="#b24">[25]</ref> was used for the comparison of reconstruction results in the tumor region of interest (ROI) area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Figure <ref type="figure" target="#fig_1">2</ref> shows three different slices of the reconstructed brain PET images using different methods. The DIP method and proposed DULDA have lower noise compared with MLEM and EM-TV visually. However, the DIP method shows unstable results cross different slices and fails in the recovery of the small cortex region. The proposed DULDA can recover more structural details and the white matter appears to be more sharpen. The quantitative and bias-variance results are shown in Table <ref type="table" target="#tab_0">1</ref>. We noticed that DIP method performs even worse than MLEM without anatomic priors. The DIP method demonstrates a certain ability to reduce noise by smoothing the image, but this leads to losses in important structural information, which explains the lower PSNR and SSIM. Both DIP method and DULDA have a better CRC and Bias performance compared with MLEM and EM-TV. In terms of supervised training, SLDA also performs best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>To test the robustness of proposed DULDA, we forward-project one patient brain image data with different dose level and reconstructed it with the trained DULDA model. The results compared with MLEM are shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The patient is scanned with a GE Discovery MI 5-ring PET/CT system. The real image has very different cortex structure and some deflection compared with the training data. It can be observed that DULDA achieves excellent reconstruction results in both details and edges across different dose level and different slices.Table <ref type="table" target="#tab_1">2</ref> shows the ablation study on phase numbers and loss function for DULDA. It can be observed that the dual domain loss helps improve the performance and when the phase number is 4, DULDA achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we proposed a dual-domain unsupervised model-based deep learning method (DULDA) for PET image reconstruction by unrolling the learned descent algorithm. Both quantitative and visual results show the superior performance of DULDA when compared to MLEM, EM-TV and DIP based method. Future work will focus more on clinical aspects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Diagram of the proposed DULDA for PET image reconstruction. The LDA was unrolled into several phases with the learnable l2,1 norm, where each phase includes the gradient calculation of both likelihood and regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Reconstruction results of MLEM, EMTV, DIP, proposed DULDA, DeepPET, FBSEM and proposed SLDA on different slices of the test set.</figDesc><graphic coords="4,42,30,152,51,339,10,125,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The robust analysis on proposed DULDA with one clinical patient brain sample with different dose level. From left to right: MLEM results and DULDA results with quarter dose sinogram, half dose sinogram and full dose sinogram.</figDesc><graphic coords="6,162,60,281,15,51,28,62,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative analysis and bias-variance analysis for the reconstruction results of MLEM, EM-TV, DIP, Proposed DULDA, DeepPET, FBSEM and proposed SLDA.</figDesc><table><row><cell>Methods</cell><cell>PSNR(dB)↑ SSIM↑</cell><cell>RMSE↓</cell><cell>CRC↑ Bias↓ Variance↓</cell></row><row><cell>MLEM</cell><cell cols="3">20.02 ± 1.91 0.889 ± 0.015 0.160 ± 0.045 0.6517 0.5350 0.2311</cell></row><row><cell>EM-TV</cell><cell cols="3">20.28 ± 2.21 0.904 ± 0.014 0.154 ± 0.044 0.8027 0.5389 0.2340</cell></row><row><cell>DIP</cell><cell cols="3">19.96 ± 1.50 0.873 ± 0.012 0.187 ± 0.047 0.8402 0.2540 0.2047 0.2047 0.2047</cell></row><row><cell cols="4">Our DULDA 20.80 ± 1.77 20.80 ± 1.77 20.80 ± 1.77 0.910 ± 0.011 0.910 ± 0.011 0.910 ± 0.011 0.148 ± 0.011 0.148 ± 0.011 0.148 ± 0.011 0.8768 0.8768 0.8768 0.2278 0.2278 0.2278 0.2449</cell></row><row><cell>DeepPET</cell><cell cols="3">23.40 ± 2.87 0.962 ± 0.011 0.135 ± 0.021 0.8812 0.1470 0.1465 0.1465 0.1465</cell></row><row><cell>FBSEM</cell><cell cols="3">23.59 ± 1.50 0.954 ± 0.008 0.122 ± 0.017 0.8825 0.1593 0.2034</cell></row><row><cell>Our SLDA</cell><cell cols="3">24.21 ± 1.83 24.21 ± 1.83 24.21 ± 1.83 0.963 ± 0.007 0.963 ± 0.007 0.963 ± 0.007 0.104 ± 0.013 0.104 ± 0.013 0.104 ± 0.013 0.9278 0.9278 0.9278 0.1284 0.1284 0.1284 0.1820</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study for different phase numbers and loss function type of DULDA on the test datasets.</figDesc><table><row><cell>Settings</cell><cell>PSNR↑</cell><cell>SSIM↑</cell><cell>MSE↓</cell></row><row><cell cols="4">phase numbers 2 14.53 ± 1.45 0.769 ± 0.024 0.314 ± 0.047</cell></row><row><cell></cell><cell cols="3">4 20.80 ± 1.77 0.910 ± 0.011 0.148 ± 0.011</cell></row><row><cell></cell><cell cols="3">6 20.29 ± 1.16 0.903 ± 0.014 0.156 ± 0.016</cell></row><row><cell></cell><cell cols="3">8 19.94 ± 1.31 0.884 ± 0.012 0.180 ± 0.013</cell></row><row><cell></cell><cell cols="3">10 15.33 ± 0.65 0.730 ± 0.020 0.313 ± 0.050</cell></row><row><cell>only Limage</cell><cell cols="3">15.41 ± 0.69 0.729 ± 0.008 0.324 ± 0.048</cell></row><row><cell>only Lmeasure</cell><cell cols="3">19.61 ± 1.49 0.881 ± 0.012 0.181 ± 0.011</cell></row><row><cell cols="4">Limage + Lmeasure 20.80 ± 1.77 0.910 ± 0.011 0.148 ± 0.011</cell></row><row><cell cols="2">3 Experiment and Results</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported in part by the <rs type="funder">National Key Technology Research and Development Program of China</rs> (No: <rs type="grantNumber">2021YFF0501503</rs>), the <rs type="funder">Talent Program of Zhejiang Province</rs> (No: <rs type="grantNumber">2021R51004</rs>), the <rs type="funder">Key Research and Development Program of Zhejiang Province</rs> (No: <rs type="grantNumber">2021C03029</rs>) and by <rs type="funder">NSF</rs> grants: <rs type="grantNumber">DMS2152961</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BdQ75g4">
					<idno type="grant-number">2021YFF0501503</idno>
				</org>
				<org type="funding" xml:id="_AHn7Q2H">
					<idno type="grant-number">2021R51004</idno>
				</org>
				<org type="funding" xml:id="_V4gbc3q">
					<idno type="grant-number">2021C03029</idno>
				</org>
				<org type="funding" xml:id="_WMB6eHG">
					<idno type="grant-number">DMS2152961</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The use of PET in Alzheimer disease</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nordberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rinne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Langström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="78" to="87" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PET image denoising using unsupervised deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Nucl. Med. Mol. Imaging</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="2780" to="2789" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Anatomical-guided attention enhances unsupervised PET image denoising performance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Onishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">102226</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image reconstruction by domaintransform manifold learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">555</biblScope>
			<biblScope unit="page" from="487" to="492" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DeepPET: a deep encoder-decoder network for directly solving the PET image reconstruction inverse problem</title>
		<author>
			<persName><forename type="first">I</forename><surname>Häggström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmidtlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Campanella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="253" to="262" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deep neural network for parametric image reconstruction on a large axial field-of-view PET</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Nucl. Med. Mol. Imaging</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="701" to="714" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative PET image reconstruction using convolutional neural network representation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="675" to="685" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Penalized PET reconstruction using deep learning prior and local linear fitting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1478" to="1487" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep kernel representation for image reconstruction in PET</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="3029" to="3038" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural KEM: a kernel method with deep coefficient prior for PET image reconstruction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="785" to="796" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-based deep learning PET image reconstruction using forward-backward splitting expectation-maximization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mehranian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Radiat. Plasma Med. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="54" to="64" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved low-count quantitative PET reconstruction with an iterative neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dewaraja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3512" to="3522" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TransEM: residual swin-transformer based regularized PET image reconstruction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part IV</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="184" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MAPEM-Net: an unrolled neural network for Fully 3D PET image reconstruction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th International Meeting on Fully Three-dimensional Image Reconstruction in Radiology And Nuclear Medicine</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11072</biblScope>
			<biblScope unit="page" from="109" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PET image reconstruction using deep image prior</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Catana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1655" to="1665" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PET image reconstruction incorporating deep image prior and a forward projection model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Onishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Radiat. Plasma Med. Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="841" to="846" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised Bayesian PET reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Radiat. Plasma Med. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="75" to="190" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust equivariant imaging: a fully unsupervised framework for learning to image from noisy and partial measurements</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tachella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5647" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learnable descent algorithm for nonsmooth nonconvex image reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imag. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1564" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9446" to="9454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistical sinogram restoration in dual-energy CT for PET attenuation correction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kinahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1688" to="1702" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum likelihood reconstruction for emission tomography</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="113" to="122" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Total variation regularization in positron emission tomography</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jonsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAM Report</title>
		<imprint>
			<biblScope unit="page">9848</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Computerized three-dimensional segmented human anatomy</title>
		<author>
			<persName><forename type="first">I</forename><surname>Zubal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Rattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hoffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="299" to="302" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A theoretical study of the contrast recovery and variance of MAP reconstructions from PET data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="293" to="305" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
