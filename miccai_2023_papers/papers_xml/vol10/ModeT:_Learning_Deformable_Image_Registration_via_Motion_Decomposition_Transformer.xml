<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer</title>
				<funder ref="#_n5nfNVK">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_GM7xdrz">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
				<funder ref="#_C277V7G #_TngV2sd #_Uw62ERK #_xuygPFh">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haiqiao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="laboratory">Smart Medical Imaging, Learning and Engineering (SMILE) Lab, Medical UltraSound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution" key="instit1">Shenzhen University Medical School</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Ni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="laboratory">Smart Medical Imaging, Learning and Engineering (SMILE) Lab, Medical UltraSound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution" key="instit1">Shenzhen University Medical School</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="laboratory">Smart Medical Imaging, Learning and Engineering (SMILE) Lab, Medical UltraSound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution" key="instit1">Shenzhen University Medical School</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="740" to="749"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">5408F694832E028D28DA0E6EC001F611</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_70</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deformable image registration</term>
					<term>Motion decomposition</term>
					<term>Transformer</term>
					<term>Attention</term>
					<term>Pyramid structure</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Transformer structures have been widely used in computer vision and have recently made an impact in the area of medical image registration. However, the use of Transformer in most registration networks is straightforward. These networks often merely use the attention mechanism to boost the feature learning as the segmentation networks do, but do not sufficiently design to be adapted for the registration task. In this paper, we propose a novel motion decomposition Transformer (ModeT) to explicitly model multiple motion modalities by fully exploiting the intrinsic capability of the Transformer structure for deformation estimation. The proposed ModeT naturally transforms the multihead neighborhood attention relationship into the multi-coordinate relationship to model multiple motion modes. Then the competitive weighting module (CWM) fuses multiple deformation sub-fields to generate the resulting deformation field. Extensive experiments on two public brain magnetic resonance imaging (MRI) datasets show that our method outperforms current state-of-the-art registration networks and Transformers, demonstrating the potential of our ModeT for the challenging nonrigid deformation estimation problem. The benchmarks and our code are publicly available at https://github.com/ZAX130/SmileCode.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deformable image registration has always been an important focus in the society of medical imaging, which is essential for the preoperative planning, intraoperative information fusion, disease diagnosis and follow-ups <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">23]</ref>. The deformable registration is to solve the non-rigid deformation field to warp the moving image, so that the warped image can be anatomically similar to the fixed image. Let I f , I m ∈ R H×W ×L be the fixed and moving images (H, W, L denote image size), in the deep-learning-based registration paradigm, it is often necessary to employ a spatial transformer network (STN) <ref type="bibr" target="#b12">[13]</ref> to apply the estimated sampling grid G ∈ R H×W ×L×3 to the moving image, where G is obtained by adding the regular grid and the deformation field. For any position p ∈ R 3 in the sampling grid, G(p) represents the corresponding relation, which means that the voxel at position p in the fixed image corresponds to the voxel at position G(p) in the moving image. That is to say, image registration can be understood as finding the corresponding voxels between the moving and fixed images, and converting this into the relative positional relationship between voxels, which is very similar to the calculation method of Transformer <ref type="bibr" target="#b7">[8]</ref>.</p><p>Transformers have been successfully used in the society of computer vision and have recently made an impact in the field of medical image computing <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>. In medical image registration, there are also several related studies that employ Transformers to enhance network structures to obtain better registration performance, such as Transmorph <ref type="bibr" target="#b4">[5]</ref>, Swin-VoxelMorph <ref type="bibr" target="#b27">[26]</ref>, Vit-V-Net <ref type="bibr" target="#b5">[6]</ref>, etc. The use of Transformer in these networks, however, often merely leverages the self-attention mechanism in Transformers to boost the feature learning (the same as the segmentation tasks do), but does not sufficiently design for the registration tasks. Some other methods use cross-attention to model the corresponding relationship between moving and fixed images, such as Attention-Reg <ref type="bibr" target="#b22">[22]</ref> and Xmorpher <ref type="bibr" target="#b21">[21]</ref>. The cross-attention Transformer (CAT) module is used in the bottom layer of Attention-Reg <ref type="bibr" target="#b22">[22]</ref> and each layer in Xmorpher <ref type="bibr" target="#b21">[21]</ref> to establish the relationship between the features of moving and fixed images. However, the usage of Transformer in <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref> is still limited to improving the feature learning, with no additional consideration given to the relationship between the attention mechanism and the deformation estimation. Furthermore, due to the large network structure of <ref type="bibr" target="#b21">[21]</ref>, only small windows can be created for similarity calculation, which may result in performance degradation. Few studies consider the relationship between attention and deformation estimation, such as Coordinate Translator <ref type="bibr" target="#b17">[18]</ref> and Deformer <ref type="bibr" target="#b3">[4]</ref>. Deformer <ref type="bibr" target="#b3">[4]</ref> uses the calculation mode of multiplication of attention map and Value matrix in Transformer to weight the predicted basis to generate the deformation field, but its attention map calculation is only the concatenation and projection of moving and fixed feature maps, without using similarity calculation part. Coordinate Translator <ref type="bibr" target="#b17">[18]</ref> calculates the matching score of the fixed feature map and the moving feature map. Then the computed scores are employed to re-weight the deformation field. However, for feature maps with coarse-level resolution, a voxel often has multiple possibilities of different motion modes <ref type="bibr" target="#b26">[25]</ref>, which is not considered in <ref type="bibr" target="#b17">[18]</ref>. Traditional methods have explored multiple modes of deformations, e.g., probabilistic registration <ref type="bibr" target="#b11">[12]</ref>, to improve the performance.</p><p>In this study, we propose a novel motion decomposition Transformer (ModeT) to explicitly model multiple motion modalities by fully exploiting the intrinsic capability of the Transformer structure for deformation estimation. Experiments on two public brain magnetic resonance imaging (MRI) datasets demonstrate our method outcompetes several cutting-edge registration networks and Transformers. The main contributions of our work are summarized as follows:</p><p>• We propose to leverage the Transformer structure to naturally model the correspondence between images and convert it into the deformation field, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network Overview</head><p>The proposed deformable registration network is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. We employ a pyramidal registration structure, which has the advantage of reducing the scope of attention calculation required at each decoding level and therefore alleviating the computational consumption. Given the fixed image I f and moving image I m as input, the encoder extracts hierarchical features using a 5-layer convolutional block, which doubles the number of channels in each layer. This generates two sets of feature maps</p><formula xml:id="formula_0">F 1 , F 2 , F 3 , F 4 , F 5 and M 1 , M 2 , M 3 , M 4 , M 5 .</formula><p>The feature maps M 5 and F 5 are sent into the ModeT to generate multiple deformation subfields, and then the generated deformation sub-fields are input into the CWM to obtain the fused deformation field ϕ 1 of the coarsest decoding layer as the initial of the total deformation field φ. The moving feature map M 4 is deformed using φ, and the deformed moving feature map is fed into the ModeT along with F 4 to generate multiple sub-fields, which are input into the CWM to get ϕ 2 . Then ϕ 2 is compounded with previous total deformation field to generate the updated φ. The feature maps M 3 and F 3 go through the similar operations. As the decoding feature maps become finer, the number of motion modes at position p decreases, along with the number of attention heads we need to model. At the F 2 /M 2 and F 1 /M 1 levels, we no longer generate multiple deformation sub-fields, i.e., the number of attention heads in ModeT is 1. Finally, the obtained total deformation field φ is used to warp I m to obtain the registered image.</p><p>To guide the network training, the normalized cross correlation L ncc <ref type="bibr" target="#b19">[19]</ref> and the deformation regularization L reg <ref type="bibr" target="#b2">[3]</ref> is used:</p><formula xml:id="formula_1">L train = L ncc (I f , I m • φ) + λL reg (φ), (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where • is the warping operation, and λ is the weight of the regularization term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motion Decomposition Transformer (ModeT)</head><p>In deep-learning-based registration networks, a position p in the low-resolution feature map contains semantic information of a large area in the original image and therefore may often have multiple possibilities of different motion modalities.</p><p>To model these possibilities, we employ a multi-head neighborhood attention mechanism to decompose different motion modalities at low-resolution level.</p><p>The illustration of the motion decomposition is shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Let F, M ∈ R c×h×w×l stand for the fixed and moving feature maps from a specific level of the hierarchical encoder, where h, w, l denote feature map size and c is the channel number. The feature maps F and M go through linear projection (proj) and LayerNorm (LN ) <ref type="bibr" target="#b1">[2]</ref> to get Q (query) and K (key):</p><formula xml:id="formula_3">Q = LN (proj(F )), K = LN (proj(M )),</formula><p>Q ={Q (1) , Q (2) , . . . , Q (S) }, K ={K (1) , K (2) , . . . , K (S) }, <ref type="bibr" target="#b1">(2)</ref> where the projection operation is shared weight, and the weight initialization is sampled from N (0, 1e -5 ), the bias is initialized to 0. The Q and K are then divided according to channels, and S represents the number of divided heads.</p><p>We then calculate the neighborhood attention map. We use c(p) to denote the neighborhood of voxel p. For a neighborhood of size n × n × n, ||c(p)|| = n 3 . The neighborhood attention map of multiple heads is obtained by: where B ∈ R S×n×n×n is a learnable relative positional bias, initialized to all zeros. We pad the moving feature map with zeros to calculate boundary voxels because the registration task sometimes requires voxels outside the field-of-view to be warped. Equation (3) shows how the neighborhood attention is computed for the s-th head at position p, so that the semantic information of voxels on low resolution can be decomposed to compute similarity one by one, in preparation for modeling different motion modalities. Moreover, the neighborhood attention operation narrows the scope of attention calculation to reduce the computational effort, which is very friendly to volumetric processing. The next step is to obtain the multiple sub-fields at this level by computing the regular displacement field weighted via the neighborhood attention map:</p><formula xml:id="formula_4">NA(p, s) = sof tmax(Q (s) p • K (s)T c(p) + B (s) ),<label>(3)</label></formula><formula xml:id="formula_5">ϕ (s) p = NA(p, s)V,<label>(4)</label></formula><p>where ϕ (s) ∈ R h×w×l×3 , V ∈ R n×n×n , and V (value) represents the relative position coordinates for the neighborhood centroid, which is not learned so that the multi-head attention relationship can be naturally transformed into a multicoordinate relationship. With the above steps, we obtain a series of deformation sub-fields for this level: ϕ (1) , ϕ (2) , . . . , ϕ (S) (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Competitive Weighting Module (CWM)</head><p>Multiple low-resolution deformation fields need to be reasonably fused when deforming a high-resolution feature map. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, we first upsample these deformation sub-fields, then convolve them in three layers to get the score of each sub-field, and use softmax to compete the motion modality for each voxel. The convolution uses 3 × 3 × 3 convolution rather than direct projection because deformation fields often require correlation of adjacent displacements to determine if they are reasonable. We formulate above competitive weighting operation to obtain the deformation field ϕ at this level as follows:</p><p>w (1) , w (2) , . . . , w (S) = W Conv(cat(ϕ (1) , ϕ (2) , . . . , ϕ (S) )),</p><formula xml:id="formula_6">ϕ = w (1) ϕ (1) + w (2) ϕ (2) +, . . . , +w (S) ϕ (S) , (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where w (s) ∈ R h×w×l , and ϕ (s) has already been upsampled. W Conv represents the ConvBlock used to calculate weights, as shown in the right part of Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets. Experiments were carried on two public brain MRI datasets, including LPBA <ref type="bibr" target="#b20">[20]</ref> and Mindboggle <ref type="bibr" target="#b15">[16]</ref>. For LPBA, each MRI volume contains 54 manually labeled region-of-interests (ROIs). All volumes in LPBA were rigidly pre-aligned to mni305. 30 volumes (30×29 pairs) were employed for training and 10 volumes (10×9 pairs) were used for testing. For Mindboggle, each volume contains 62 manually labeled ROIs. All volumes in Mindboggle were affinely aligned to mni152. 42 volumes (42 × 41 pairs from the NKI-RS-22 and NKI-TRT-20 subsets) were employed for training, and 20 volumes from OASIS-TRT-20 (20 × 19 pairs) were used for testing. All volumes were pre-processed by min-max normalization, and skull-stripping using FreeSurfer <ref type="bibr" target="#b8">[9]</ref>. The final size of each volume was 160 × 192 × 160 after a center-cropping operation.</p><p>Evaluation Metrics. To quantitatively evaluate the registration performance, Dice score (DSC) <ref type="bibr" target="#b6">[7]</ref> was calculated as the primary similarity metric to evaluate the degree of overlap between corresponding regions. In addition, the average symmetric surface distance (ASSD) <ref type="bibr" target="#b25">[24]</ref> was evaluated, which can reflect the similarity of the region contours. The quality of the predicted deformation φ was assessed by the percentage of voxels with non-positive Jacobian determinant (i.e., folded voxels). All above metrics were calculated in 3D. A better registration shall have larger DSC, and smaller ASSD and Jacobian. </p><formula xml:id="formula_8">(%) ASSD %|J φ | ≤ 0 DSC (%) ASSD %|J φ | ≤ 0 SyN [1]</formula><p>56.7 ± 1.5 1.38 ± 0.09 &lt; 0.00001% 70.1 ± 6.2 1.72 ± 0.12 &lt; 0.0004% VM <ref type="bibr" target="#b2">[3]</ref> 56.0 ± 1.6 1.49 ± 0.11 &lt; 1% 64.3 ± 3.2 2.03 ± 0.21 &lt; 0.7% TM <ref type="bibr" target="#b4">[5]</ref> 60.7 ± 1.5 1.35 ± 0.10 &lt; 0.9% 67.0 ± 3.0 1.90 ± 0.20 &lt; 0.6% I2G <ref type="bibr" target="#b17">[18]</ref> 59.8 ± 1.3 1.30 ± 0.07 &lt; 0.03% 71.0 ± 1.4 1.64 ± 0.10 &lt; 0.01% PR++ <ref type="bibr" target="#b13">[14]</ref> 61.1 ± 1.4 1.34 ± 0.10 &lt; 0.5% 69.5 ± 2.2 1.76 ± 0.17 &lt; 0.2% XM <ref type="bibr" target="#b21">[21]</ref> 53.6 ± 1.5  Implementation Details. Our method was implemented with PyTorch, using a GPU of NVIDIA Tesla V100 with 32GB memory. The regularization term λ and neighborhood size n were set as 1 and 3. For the encoder part, we used the same convolution structure as <ref type="bibr" target="#b17">[18]</ref>. In the pyramid decoder, from coarse to fine, the number of attention heads were set as 8, 4, 2, 1, 1, respectively. We used 6 channels for each attention head. The Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with a learning rate decay strategy was employed as follows:</p><formula xml:id="formula_9">lr m = lr init • (1 - m -1 M ) 0.9 , m = 1, 2, ..., M<label>(7)</label></formula><p>where lr m represents the learning rate of m-th epoch and lr init = 1e -4 represents the learning rate of initial epoch. We set the batch size as 1, M as 30 for training. In the inference phase, our method averagely took 0.56 second and 9GB memory to register a volume pair of size 160 × 192 × 160.</p><p>Comparison Methods. We compared our method with several state-of-theart registration methods: (1) SyN <ref type="bibr" target="#b0">[1]</ref>: a classical traditional approach, using the SyN Only setting in ANTS. ( <ref type="formula">2</ref>) VoxelMorph(VM) <ref type="bibr" target="#b2">[3]</ref>: a popular single-stage registration network. ( <ref type="formula" target="#formula_4">3</ref>) TransMorph(TM) <ref type="bibr" target="#b4">[5]</ref>: a single-stage registration network with SwinTransformer enhanced encoder. ( <ref type="formula" target="#formula_5">4</ref>) PR++ <ref type="bibr" target="#b13">[14]</ref>: a pyramid registration network using 3D correlation layer. ( <ref type="formula">5</ref>) XMorpher(XM) <ref type="bibr" target="#b21">[21]</ref>: a registration network using CAT modules for each level of encoder and decoder. ( <ref type="formula" target="#formula_6">6</ref>) Im2grid(I2G) <ref type="bibr" target="#b17">[18]</ref>: a pyramid network using a coordinate translator. ( <ref type="formula" target="#formula_9">7</ref>) DMR [4]: a registration network using a Deformer and a multi-resolution refinement module.</p><p>Quantitative and Qualitative Analysis. The numerical results of different methods on datasets Mindboggle and LPBA are reported in Table <ref type="table" target="#tab_0">1</ref>. It can be observed that our method consistently attained the best registration accuracy with respect to DSC and ASSD metrics. For the DSC results, our method surpassed the second-best networks by 1.7% and 1.1% on Mindboggle and LPBA, respectively. We further investigated the statistical significance of our method over comparison methods on DSC and ASSD metrics, by conducting the paired and two-sided Wilcoxon signed-rank test. The null hypotheses for all pairs (our method v.s. other method) were not accepted at the 0.05 level. As a result, our method can be regarded as significantly better than all comparison methods on DSC and ASSD metrics. Table <ref type="table" target="#tab_0">1</ref> also lists the percentage of voxels with nonpositive Jacobian determinant (%|J φ | ≤ 0). Our method achieved satisfactory performance, which was the best among all deep-learning-based networks. Figure <ref type="figure" target="#fig_3">4</ref> visualizes the registered images from different methods on two datasets. Our method generated more accurate registered images, and internal structures can be consistently preserved using our method. Figure <ref type="figure" target="#fig_4">5</ref> takes the registration of one image pair as an example to show the multi-level deformation fields generated by our method. Our ModeT effectively modeled multiple motion modalities and our CWM fused them together at low-resolution levels. The final deformation field φ accurately warped the moving image to registered with the fixed image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We present a motion decomposition Transformer (ModeT) to naturally model the correspondence between images and convert this into the deformation field, which improves the interpretability of the deep-learning-based registration network. The proposed ModeT employs the multi-head neighborhood attention mechanism to identify various motion patterns of a voxel in the low-resolution feature map. Then with the help of competitive weighting module and pyramid structure, the motion modes contained in a voxel can be gradually fused and determined in the coarse-to-fine pyramid decoder. The experimental results have proven the superior performance of the proposed method. In our future study, we attempt to implement our ModeT in a more efficient way, and also investigate more effective fusion strategy to combine the displacement field from multiple attention heads.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the proposed deformable registration network. The encoder takes the fixed image I f and moving image Im as input to extract hierarchical features F1-F5 and M1-M5. The motion decomposition Transformer (ModeT) is used to generate multiple deformation sub-fields and the competitive weighting module (CWM) fuses them. Finally the decoding pyramid outputs the total deformation field φ.</figDesc><graphic coords="3,44,79,54,14,334,48,173,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the proposed motion decomposition Transformer, which employs the multi-head neighborhood attention mechanism to decompose different motion modalities. (S = 3 in this illustration)</figDesc><graphic coords="5,47,79,54,14,328,54,153,58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the proposed competitive weighting module (CWM).</figDesc><graphic coords="6,61,98,54,08,328,54,95,62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visualized registration results from different methods on Mindboggle (top row) and LPBA (bottom row).</figDesc><graphic coords="7,44,79,223,25,334,57,86,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visualization of the generated multi-level deformation fields (ϕ1-ϕ5) to register one image pair. At low-resolution levels, multiple deformation sub-fields are decomposed to effectively model different motion modalities.</figDesc><graphic coords="8,58,98,54,59,334,42,74,11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The numerical results of different registration methods on two datasets.</figDesc><table><row><cell>Mindboggle (62 ROIs)</cell><cell>LPBA (54 ROIs)</cell></row><row><cell>DSC</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1.46 ± 0.09 &lt; 1% 66.3 ± 2.0 1.92 ± 0.15 &lt; 0.1% DMR [4] 60.6 ± 1.4 1.34 ± 0.09 &lt; 0.7% 69.2 ± 2.4 1.79 ± 0.18 &lt; 0.4%</figDesc><table><row><cell>Ours</cell><cell>62.8 ± 1.2 1.22 ± 0.07 &lt; 0.03%</cell><cell>72.1 ± 1.4 1.58 ± 0.11 &lt; 0.007%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grants <rs type="grantNumber">62071305</rs>, <rs type="grantNumber">61701312</rs>, <rs type="grantNumber">81971631</rs> and <rs type="grantNumber">62171290</rs>, in part by the <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> under Grant <rs type="grantNumber">2022A1515011241</rs>, and in part by the <rs type="funder">Shenzhen Science and Technology Program</rs> (No. <rs type="grantNumber">SGDX 20201103095613036</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_C277V7G">
					<idno type="grant-number">62071305</idno>
				</org>
				<org type="funding" xml:id="_TngV2sd">
					<idno type="grant-number">61701312</idno>
				</org>
				<org type="funding" xml:id="_Uw62ERK">
					<idno type="grant-number">81971631</idno>
				</org>
				<org type="funding" xml:id="_xuygPFh">
					<idno type="grant-number">62171290</idno>
				</org>
				<org type="funding" xml:id="_n5nfNVK">
					<idno type="grant-number">2022A1515011241</idno>
				</org>
				<org type="funding" xml:id="_GM7xdrz">
					<idno type="grant-number">SGDX 20201103095613036</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</title>
		<author>
			<persName><forename type="first">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="41" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VoxelMorph: a learning framework for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1788" to="1800" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformer: towards displacement field learning for unsupervised medical image registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-014" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="141" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transmorph: transformer for unsupervised medical image registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Segars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102615</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ViT-V-Net: vision transformer for unsupervised volumetric medical image registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06468</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FreeSurfer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="774" to="781" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning in medical image registration: a review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno>20TR01</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">20</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transformers in medical image analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Med</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="78" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deformable image registration by combining uncertainty estimates from supervoxel belief propagation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Papież</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="57" to="71" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual-stream pyramid registration network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">102379</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">101 labeled brain images and a consistent human cortical labeling protocol</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tourville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">171</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transforming medical imaging with transformers? A comparative review of key properties, current progresses, and future perspectives</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">102762</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coordinate translator for learning deformable medical image registration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMMI 2022</title>
		<editor>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Huo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Leahy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13594</biblScope>
			<biblScope unit="page" from="98" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-18814-5_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-18814-510" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Application of normalized cross correlation to image registration</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Prathapani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nagabhooshanam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Res. Eng. Technol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Construction of a 3D probabilistic atlas of human cortical structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Shattuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1064" to="1080" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Xmorpher: full transformer for deformable medical image registration via cross attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-021" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-modal attention for MRI and ultrasound volume registration</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-17" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deformable medical image registration: a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1153" to="1190" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Papiez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04290</idno>
		<title level="m">Residual aligner network</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Swin-VoxelMorph: a symmetric unsupervised learning model for deformable medical image registration using swin transformer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-08" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
