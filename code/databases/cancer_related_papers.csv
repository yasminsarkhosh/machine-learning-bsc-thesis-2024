Paper Title,Header Number,Header Title,Text
Anatomy-Driven Pathology Detection on Chest X-rays,1.0,Introduction,"chest radiographs (chest x-rays) represent the most widely utilized type of medical imaging examination globally and hold immense significance in the detection of prevalent thoracic diseases, including pneumonia and lung cancer, making them a crucial tool in clinical care  we, therefore, propose a novel approach towards pathology detection that uses anatomical region bounding boxes, solely defined on anatomical structures, as proxies for pathology bounding boxes. these region boxes are easier to annotate -the physiological shape of a healthy subject's thorax can be learned relatively easily by medical students -and generalize better than those of pathologies, such that huge labeled datasets are available  -we propose anatomy-driven pathology detection (adpd), a pathology detection approach for chest x-rays, trained with pathology classification labels together with anatomical region bounding boxes as proxies for pathologies. -we study two training approaches: using localized (anatomy-level) pathology labels for our model loc-adpd and using image-level labels with multiple instance learning (mil) for our model mil-adpd. -we train our models on the chest imagenome "
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2.4,Dataset,"the dataset is composed of 23 oncological patients with different tumor types. dpet data was acquired on a biograph vision quadra for 65 min, over 62 frames. the exposure duration of the frames were 2 × 10 s, 30 × 2 s, 4 × 10 s, 8 × 30 s, 4 × 60 s, 5 × 120 s and 9 × 300 s. the pet volumes were reconstructed with an isotropic voxel size of 1.6 mm. the dataset included the label maps of 7 organs (bones, lungs, heart, liver, kidneys, spleen, aorta) and one image-derived input function a(t) [bq/ml] from the descending aorta per patient. further details on the dataset are presented elsewhere  the pet frames and the label map were resampled to an isotropic voxel size of 2.5 mm. then, the dataset was split patient-wise into training, validation, and test set, with 10, 4, and 9 patients respectively. details on the dataset split are available in the supplementary material (table "
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,1.0,Introduction,"deep learning techniques have greatly improved medical image segmentation by automatically extracting specific tissue or substance location information, which facilitates accurate disease diagnosis and assessment. however, most deep learning approaches for segmentation require fully or partially labeled training datasets, which can be time-consuming and expensive to annotate. to address this issue, recent research has focused on developing segmentation frameworks that require little or no segmentation labels. to meet this need, many researchers have devoted their efforts to weakly-supervised semantic segmentation (wsss)  the literature has not adequately addressed the issue of low-resolution class-activation maps (cams), especially for medical images. some existing methods, such as dilated residual networks  in this paper, we propose an attentive multiple-exit cam (ame-cam) for brain tumor segmentation in magnetic resonance imaging (mri). different from recent cam methods, ame-cam uses a classification model with multipleexit training strategy applied to optimize the internal outputs. activation maps from the outputs of internal classifiers, which have different resolutions, are then aggregated using an attention model. the model learns the pixel-wise weighted sum of the activation maps by a novel contrastive learning method. our proposed method has the following contributions: -to tackle the issues in existing cams, we propose to use multiple-exit classification networks to accurately capture all the internal activation maps of different resolutions. -we propose an attentive feature aggregation to learn the pixel-wise weighted sum of the internal activation maps. -we demonstrate the superiority of ame-cam over state-of-the-art cam methods in extracting segmentation results from classification networks on the 2021 brain tumor segmentation challenge (brats 2021) "
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,3.1,Dataset,we evaluate our method on the brain tumor segmentation challenge (brats) dataset 
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,4.1,Quantitative and Qualitative Comparison with State-of-the-Art,"in this section, we compare the segmentation performance of the proposed ame-cam with five state-of-the-art weakly-supervised segmentation methods, namely grad-cam  compared to the unsupervised baseline (ul), c&f is unable to separate the tumor and the surrounding tissue due to low contrast, resulting in low dice scores in all experiments. with pixel-wise labels, the dice of supervised c&f improves significantly. without any pixel-wise label, the proposed ame-cam outperforms supervised c&f in all modalities. the fully supervised (fsl) optimized u-net achieves the highest dice score and iou score in all experiments. however, even under different levels of supervision, there is still a performance gap between the weakly supervised cam methods and the fully supervised state-of-the-art. this indicates that there is still potential room for wsss methods to improve in the future. qualitatively, fig. "
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,Selected Exit,"dice effect of single-exit and multiple-exit: table  the comparisons show that the activation map obtained from the shallow layer m 1 and the deepest layer m 4 result in low dice scores, around 0.15. this is because the network is not deep enough to learn the tumor region in the shallow layer, and the resolution of the activation map obtained from the deepest layer is too low to contain sufficient information to make a clear boundary for the tumor. results of the internal classifiers from the middle of the network (m 2 and m 3 ) achieve the highest dice score and iou, both of which are around 0.5. to evaluate whether using results from all internal classifiers leads to the highest performance, we further apply the proposed method to the two internal classifiers with the highest dice scores, i.e., m 2 and m 3 , called m 2 + m 3 . compared with using all internal classifiers (m 1 to m 4 ), m 2 + m 3 results in 18.6% and 22.1% lower dice and iou, respectively. in conclusion, our ame-cam still achieves the optimal performance among all the experiments of single-exit and multiple-exit. other ablation studies are presented in the supplementary material due to space limitations."
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,5.0,Conclusion,"in this work, we propose a brain tumor segmentation method for mri images using only class labels, based on an attentive multiple-exit class activation mapping (ame-cam). our approach extracts activation maps from different exits of the network to capture information from multiple resolutions. we then use an attention model to hierarchically aggregate these activation maps, learning pixel-wise weighted sums. experimental results on the four modalities of the 2021 brats dataset demonstrate the superiority of our approach compared with other cam-based weakly-supervised segmentation methods. specifically, ame-cam achieves the highest dice score for all patients in all datasets and modalities. these results indicate the effectiveness of our proposed approach in accurately segmenting brain tumors from mri images using only class labels."
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,4.3,Results,"we compare our method with state-of-the-art unsupervised 3d structure discovery approaches including clustering using 3d feature learning  for the synthetic datasets, we used k = 2 (background and cell) for level 1, k = 4 (background, cell, vesicle, mitochondria) for level 2, and k = 8 (background, cell, vesicle, mitochondria, and 4 small protein aggregates) for level 3 predictions. the evaluation metric is the average dice score on the annotated test labels. as the label order may differ we use the hungarian algorithm to match the predicted masks with the ground truth segmentations. table  for the brain tumor segmentation (brats'19) dataset, we use the whole tumor (wt) segmentation mask for evaluation, which is detectable based on the flair images alone. we train segmentation models with k = 3 parts (background, brain, tumor). the evaluation metric, as in the brats'19 challenge  we perform ablation studies on the brats'19 dataset (table  this might be due to the fact that predictive modeling involves learning from a distribution of images and a model may therefore extract useful knowledge from a collection of images. to evaluate the significance of the diffusion features, we replaced our diffusion feature extractor with a 3d resnet from med3d "
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,5.0,Conclusion,"in this work, we showed that features from 3d generative diffusion models using a ladder-like u-net-based architecture can discover intrinsic 3d structures in biomedical images. we trained predictive unsupervised segmentation models using losses that encourage the decomposition of biomedical volumes into nested subvolumes aligned with their hierarchical structures. our method outperforms existing unsupervised segmentation approaches and discovers meaningful hierarchical concepts on challenging biologically-inspired synthetic datasets and on the brats brain tumor dataset. while we tested our approach for unsupervised image segmentation it is conceivable that it could also be useful in semisupervised settings and that could be applied to data types other than images."
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,1.0,Introduction,colorectal cancer is a leading cause of cancer-related deaths worldwide  dual-branch learning has been widely adopted in annotation-efficient learning to encourage mutual consistency through co-teaching. while existing approaches are typically designed for learning in the spatial domain 
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,1.0,Introduction,"endoscopy is an important medical procedure with many applications, from routine screening to detection of early signs of cancer and minimally invasive treatment. automatic analysis and understanding of these videos raises many opportunities for novel assistive and automatization tasks on endoscopy procedures. obtaining 3d models from the intracorporeal scenes captured in endoscopies is an essential step to enable these novel tasks and build applications, for example, for improved monitoring of existing patients or augmented reality during training or real explorations. 3d reconstruction strategies have been studied for long, and one crucial step in these strategies is feature detection and matching which serves as input for structure from motion (sfm) pipelines. endoscopic images are a challenging case for feature detection and matching, due to several well known challenges for these tasks, such as lack of texture, or the presence of frequent artifacts, like specular reflections. these problems are accentuated when all the elements in the scene are deformable, as it is the case in most endoscopy scenarios, and in particular in the real use case studied in our work, the lower gastrointestinal tract explored with colonoscopies. existing 3d reconstruction pipelines are able to build small 3d models out of short clips from real and complete recordings  this work introduces superpoint-e, a new model to extract interest points from endoscopic images. we build on the well known superpoint architecture "
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,3.0,Experiments,"dataset and setting: we collect four pathology image datasets to validate our proposed approach. firstly, we acquire 50 images from a cohort of patients with triple negative breast cancer (tnbc), which is released by naylor et al  experimental results: to validate our method, we compare it with the following approaches: (1) cellsegssda  in addition, the experimental results also show that simply combining multiple source data into a traditional single source will result in performance degradation in some cases, which also proves the importance of studying multi-source domain adaptation methods. ablation study: to evaluate the impact of our proposed methods of weighted logits(wl), pseudo-cutout label(pcl) and maximize mutual information(mmi) on the model performance, we conduct an ablation study. we compare the baseline model with the models that added these three methods separately. we chose crc, kirc and brca as our source domains, and tnbc as our target domain. the results of these experiments, presented in the table "
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,1.0,Introduction,"in recent years, deep learning (dl) methods have demonstrated remarkable performance in detecting and localizing tumors on ultrasound images  domain adaptation (da) has been extensively studied to alleviate the aforementioned limitations, the goal of which is to reduce the domain gap caused by the diversity of datasets from different domains  to alleviate the problem of pseudo-label-based uda, in this work, we propose an advanced uda framework based on self-supervised da with a test-time finetuning network. test-time adaptation methods have been developed  to summarize, our contributions are three-fold: • we design a self-supervised da framework that includes a parameter search method and provide a mathematical justification for it. with our framework, we are able to identify the best-performing parameters that result in improved performance in da tasks. • our framework is effective at preserving privacy, since it carries out da using only pre-trained network parameters, without transferring any patient data. • we applied our framework to the task of segmenting breast cancer from ultrasound imaging data, demonstrating its superior performance over competing uda methods. our results indicate that our framework is effective in improving the accuracy of breast cancer segmentation from ultrasound images, which could have potential implications for improving the diagnosis and treatment of breast cancer. sample batches of (t, ?) ∼ t"
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,4.0,Discussion and Conclusion,"in this work, we proposed a dl-based segmentation framework for multi-domain breast cancer segmentation on ultrasound images. due to the low resolution of ultrasound images, manual segmentation of breast cancer is challenging even for expert clinicians, resulting in a sparse number of labeled data. to address this issue, we introduced a novel self-supervised da network for breast cancer segmentation in ultrasound images. in particular, we proposed a test-time finetuning network to learn domain-specific knowledge via knowledge distillation by self-supervised learning. since uda is susceptible to error accumulation due to imprecise pseudo-labels, which can lead to degraded performance, we employed a self-supervised learning-based pretext task. specifically, we utilized an autoencoder-based network architecture to generate synthetic images that matched the input images. moreover, we introduced a randomized re-initialization module that injects randomness into network parameters to reposition the network from the local minimum in the source domain to a local minimum that is better suited for the target domain. this approach enabled our framework to efficiently fine-tune the network in the target domain and achieve better segmentation performance. experimental results, carried out with three ultrasound databases from different domains, demonstrated the superior segmentation performance of our framework over other competing methods. additionally, our framework is well-suited to a scenario in which access to source domain data is limited, due to data privacy protocols. it is worth noting that we used vanilla u-net "
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,1.0,Introduction,"positron emission tomography (pet) is a sensitive nuclear imaging technique, and plays an essential role in early disease diagnosis, such as cancers and alzheimer's disease  in recent years, many enhancement algorithms have been proposed to improve pet image quality. among the earliest are filtering-based methods such as non-local mean (nlm) filter  fortunately, the recent glowing diffusion model  taking all into consideration, we propose the spet-only unsupervised pet enhancement (upete) framework based on the latent diffusion model. specifically, upete has an encoder-<diffusion model>-decoder structure that first uses the encoder to compress input the lpet/spet images into latent representations, then uses the latent diffusion model to learn/estimate the distribution of spet latent representations, and finally uses the decoder to recover spet images from the estimated spet latent representations. the keys of our upete include 1) compressing the 3d pet images into a lower dimensional space for reducing the computational cost of diffusion model, 2) adopting the poisson noise, which is the dominant noise in pet imaging  our work had three main features/contributions: i) proposing a clinicallyapplicable unsupervised pet enhancement framework, ii) designing three targeted strategies for improving the diffusion model, including pet image compression, poisson diffusion, and ct-guided cross-attention, and iii) achieving better performance than state-of-the-art methods on the collected pet datasets."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,1.0,Introduction,"medical image segmentation is a critical task in computer-aided diagnosis and treatment planning. it involves the delineation of anatomical structures or pathological regions in medical images, such as magnetic resonance imaging (mri) or computed tomography (ct) scans. accurate and efficient segmentation is essential for various medical applications, including tumor detection, surgical planning, and monitoring disease progression. however, manual medical imaging annotation is time-consuming and expensive because it requires the domain knowledge from medical experts. therefore, there is a growing interest in developing semi-supervised learning that leverages both labeled and unlabeled data to improve the performance of image segmentation models  existing semi-supervised segmentation methods exploit smoothness assumption, e.g., the data samples that are closer to each other are more likely to to have the same label. in other words, the smoothness assumption encourages the model to generate invariant outputs under small perturbations. we have seen such perturbations being be added to natural input images at data-level  in this paper, we propose a novel cross-adversarial local distribution regularization for semi-supervised medical image segmentation for smoothness assumption enhancement"
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.4,Dataset and Quantitative Metrics,"we use publicly available data collected from a breast phantom (model 059, cirs: tissue simulation & phantom technology, norfolk, va) using an alpinion e-cube r12 research us machine (bothell, wa, usa). the center frequency was 8 mhz and the sampling frequency was 40 mhz. the young's modulus of the experimental phantom was 20 kpa and contains several inclusions with young's modulus of higher than 40 kpa. this data is available online at http://code.sonography.ai in  in vivo data was collected at johns hopkins hospital from patients with liver cancer during open-surgical rf thermal ablation by a research antares siemens system using a vf 10-5 linear array with the sampling frequency of 40 mhz and the center frequency of 6.67 mhz. the institutional review board approved the study with the consent of the patients. we selected 600 rf frame pairs of this dataset for the training of the networks. two well-known metrics of contrast to noise ratio (cnr) and strain ratio (sr) are utilized to evaluate the compared methods. two regions of interest (roi) are selected to compute these metrics and they can be defined as  where the subscript t and b denote the target and background rois. the sr is only sensitive to the mean (s x ), while cnr depends on both the mean and the standard deviation (σ x ) of rois. for stiff inclusions as the target, higher cnr correlates with better target visibility, and lower sr translates to a higher difference between the target and background strains."
SLPD: Slide-Level Prototypical Distillation for WSIs,1.0,Introduction,"in computational histopathology, visual representation extraction is a fundamental problem  more recently, some works propose to close the gap via directly learning slidelevel representations in pre-training. for instance, hipt  in this paper, we propose to encode the intra-and inter-slide semantic structures by modeling the mutual-region/slide relations, which is called slpd: slide-level prototypical distillation for wsis. specifically, we perform the slide-level clustering for the 4096 × 4096 regions within each wsi to yield the prototypes, which characterize the medically representative patterns of the tumor (e.g., morphological phenotypes). in order to learn this intra-slide semantic structure, we encourage the region representations to be closer to the assigned prototypes. by representing each slide with its prototypes, we further select semantically simi- lar slides by the set-to-set distance of prototypes. then, we learn the inter-slide semantic structure by building correspondences between region representations and cross-slide prototypes. we conduct experiments on two benchmarks, nsclc subtyping and brca subtyping. slpd achieves state-of-the-art results on multiple slide-level tasks, demonstrating that representation learning of semantic structures of slides can make a suitable proxy task for wsi analysis. we also perform extensive ablation studies to verify the effectiveness of crucial model components."
SLPD: Slide-Level Prototypical Distillation for WSIs,2.3,Slide-Level Clustering,"many histopathologic features have been established based on the morphologic phenotypes of the tumor, such as tumor invasion, anaplasia, necrosis and mitoses, which are then used for cancer diagnosis, prognosis and the estimation of response-to-treatment in patients  to characterize the histopathologic features underlying the slides, a straightforward practice is the global clustering, i.e., clustering the region embeddings from all the wsis, as shown in the left of fig. "
SLPD: Slide-Level Prototypical Distillation for WSIs,2.5,Inter-Slide Distillation,"tumors of different patients can exhibit morphological similarities in some respects  where cos(•, •) measures the cosine similarity between two vectors, and s m enumerates the permutations of m elements. the optimal permutation σ * can be computed efficiently with the hungarian algorithm  the inter-slide distillation can encode the sldie-level information complementary to that of intra-slide distillation into the region embeddings. the overall learning objective of the proposed slpd is defined as: where the loss scale is simply set to α 1 = α 2 = 1. we believe the performance can be further improved by tuning this."
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,1.0,Introduction,"nuclei segmentation in histopathology images is an important task for cancer diagnosis and immune response prediction  thousands of instances are tedious and the ambiguous nature of nuclei boundaries requires high-level expert annotators. to address this, weakly-supervised nuclei segmentation methods  to overcome these challenges, we propose a novel weakly supervised instance segmentation method that effectively distinguishes adjacent nuclei and is robust to point shifts. the proposed model consists of three modules responsible for binary segmentation, boundary delineation, and instance separation. to train the binary segmentation module, we generate pseudo binary segmentation masks using geodesic distance-based voronoi labels and cluster labels from point annotations. geodesic distance provides more precise nuclei shape information than previous euclidean distance-based schemes. to train the offset map module, we generate pseudo offset maps by computing the offset distance between binary segmentation pixel predictions and the point label. the offset information facilitates precise delineation of the boundaries between adjacent nuclei. to make the model robust to center point shifts, we introduce an expectation maximization (em)  the contributions of this paper are as follows:  (2) by utilizing geodesic distance, we produce more detailed voronoi and cluster labels that precisely delineate the boundary between adjacent nuclei. (3) we introduce an em algorithm-based refinement process to encourage model robustness on center-shifted point labels. (4) ablation and evaluation studies on two public datasets demonstrate our model's ability to outperform state-of-the-art techniques not only with ideal labels but also with shifted labels."
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,1.0,Introduction,"automated segmentation of histopathological images is crucial, as it can quantify the tumor micro-environment, provide a basis for cancer grading and prognosis, and improve the diagnostic efficiency of clinical doctors  ours cam under the microscope, tumor epithelial ɵssue may appear as solid nests, acinar structures, or papillary formaɵons. the cells may have enlarged and irregular nuclei."
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,,Tumor epithelial Ɵssue,"necrosis ɵssue tumor-associated stroma necrosis may appear as areas of pink, amorphous material under the microscope, and may be surrounded by viable tumor cells and stroma. tumor-associated stroma ɵssue is the connecɵve ɵssue that surrounds and supports the tumor epithelial ɵssue. fig.  recent studies on weakly supervised segmentation primarily follow class activation mapping (cam)  to remedy the limitations of image-level supervision, we advocate for the integration of language knowledge into weakly supervised learning to provide reliable guidance for the accurate localization of target structures. to this end, we propose a text-prompting-based weakly supervised segmentation method (tpro) for accurate histopathology tissue segmentation. the text information originates from the task's semantic labels and external descriptions of subtype manifestations. for each semantic label, a pre-trained medical language model is utilized to extract the corresponding text features that are matched to each feature point in the image spatial space. a higher similarity represents a higher possibility of this location belonging to the corresponding semantic category. additionally, the text representations of subtype manifestations, including tissue morphology, color, and relationships to other tissues, are extracted by the language model as external knowledge. the discriminative information can be explored from the text knowledge to help identify and locate complete tissues accurately by jointly modeling long-range dependencies between image and text. we conduct experiments on two weakly supervised histological segmentation benchmarks, luad-histoseg and bcss-wsss, and demonstrate the superior quality of pseudo labels produced by our tpro model compared to other cam-based methods. our contributions are summarized as follows: (1) to the best of our knowledge, this is the first work that leverages language knowledge to improve the quality of pseudo labels for weakly-supervised histopathology image segmentation. "
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,1.0,Introduction,"transfer learning has become a standard practice in medical image analysis as collecting and annotating data in clinical scenarios can be costly. the pre-trained parameters endow better generalization to dnns than the models trained from scratch  diversity between domains and tasks and privacy concerns related to pre-training data. consequently, recent work  previous studies have shown that the transferability of lower layers is often higher than higher layers that are near the model output  to search for optimal layer combinations for fine-tuning, manually selecting transferable layers  in summary, this work makes the following three contributions. 1) we introduce metalr, a meta-learning-based lr tuner that can adaptively adjust layerwise lrs based on transfer learning feedback from various medical domains. 2) we enhance metalr with a proportional hyper-lr and a validation scheme using batched training data to improve the algorithm's stability and efficacy. 3) extensive experiments on both lesion detection and tumor segmentation tasks were conducted to demonstrate the superior efficiency and performance of met-alr compared to current sota medical fine-tuning techniques."
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,3.0,Method,"our approach aims to promote the information flow through the different employed resolutions. while existing works  feature extraction. our work exploits dino, the self-supervised learning approach proposed in  architecture. the representations yield by dino provide a detailed description of the local patterns in each patch; however, they retain poor knowledge of the surrounding context. to grasp a global guess about the entire slide, we allow patches to exchange local information. we achieve it through a pyramidal graph neural network (pgnn) in which each node represents an individual wsi patch seen at different scales. each node is connected to its neighbors (8-connectivity) in the euclidean space and between scales following the relation ""part of"" in general terms, such a module takes as input multi-scale patch-level representations x = [x 1 x 2 ], where x 1 ∈ r n1×f and x 2 ∈ r n2×f are respectively the representations of the lower and higher scale. the input undergoes two graph layers: while the former treats the two scales as independent subgraphs a 1 ∈ r n1×n1 and a 2 ∈ r n2×n2 , the latter process them jointly by considering the entire graph a (see fig.  where h ≡ [h 1 h 2 ] stands for the output of the pgnn obtained by concatenating the two scales. these new contextualized patch representations are then fed to the attention-based mil module proposed in  aligning scales with (self ) knowledge distillation. we have hence obtained two distinct sets of predictions for the two resolutions: namely, a bag-level score (e.g., a tumor is either present or not) and a patch-level one (e.g., which instances contribute the most to the target class). however, as these learned metrics are inferred from different wsi zooms, a disagreement may emerge: indeed, we have observed (see table  formally, the first term seeks to align bag predictions from the two scales through (self) knowledge distillation  where kl stands for the kullback-leibler divergence and τ is a temperature that lets secondary information emerge from the teaching signal. the second aligning term regards the instance scores. it encourages the two resolutions to assign criticality scores in a consistent manner: intuitively, if a lowresolution patch has been considered critical, then the average score attributed to its children patches should be likewise high. we encourage such a constraint by minimizing the euclidean distance between the low-resolution criticality grid map z 1 and its subsampled counterpart computed by the high-resolution branch: (2) in the equation above, graphpooling identifies a pooling layer applied over the higher scale: to do so, it considers the relation ""part of"" between scales and then averages the child nodes, hence allowing the comparison at the instance level. overall objective. to sum up, the overall optimization problem is formulated as a mixture of two objectives: the one requiring higher conditional likelihood w.r.t. ground truth labels y and carried out through the cross-entropy loss l ce (•; y); the other one based on knowledge distillation: where λ is a hyperparameter weighting the tradeoff between the teaching signals provided by labels and the higher resolution, while β balances the contributions of the consistency regularization introduced in eq. ( "
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,,TCGA Lung Dataset.,"it is available on the gdc data transfer portal and comprises two subsets of cancer: lung adenocarcinoma (luad) and lung squamous cell carcinoma (lusc), counting 541 and 513 wsis, respectively. the aim is to classify luad vs lusc; we follow the split proposed by dsmil "
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,4.2,Model Analysis,"on the impact of knowledge distillation. to assess its merits, we conducted several experiments varying the values of the corresponding balancing coefficients (see table  single-scale vs multi-scale.  the impact of the feature extractors and gnns. table  h 2 -mil exploits a global pooling layer (ihpool) that fulfils only the spatial structure of patches: as a consequence, if non-tumor patches surround a tumor patch, its contribution to the final prediction is likely to be outweighed by the ihpool module of h 2 -mil. differently, our approach is not restricted in such a way, as it can dynamically route the information across the hierarchical structure (also based on the connections with the critical instance)."
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,1.0,Introduction,"medical image segmentation often relies on supervised model training  secondly, the resulting models may not generalize well to unseen data domains. even small changes in the task may result in a significant drop in performance, requiring re-training from scratch  self-supervised learning (ssl) is a promising solution to these limitations. ssl pre-trains a model backbone to extract informative representations from unlabeled data. then, a simple linear or non-linear head on top of the frozen pre-trained backbone can be trained for various downstream tasks in a supervised manner (linear or non-linear probing). alternatively, the backbone can be finetuned for a downstream task along with the head. pre-training the backbone in a self-supervised manner enables scaling to larger datasets across multiple data and task domains. in medical imaging, this is particularly useful given the growing number of available datasets. in this work, we focus on contrastive learning  several works have implemented contrastive learning of dense representations in medical imaging  the common weakness of all the above works is that they do not evaluate their ssl models in linear or non-linear probing setups, even though these setups are de-facto standards for evaluation of ssl methods in natural images  our contributions are threefold. first, we propose vox2vec, a framework for contrastive learning of voxel-level representations. our simple negative sampling strategy and the idea of storing voxel-level representations in a feature pyramid form result in high-dimensional, fine-grained, multi-scale representations suitable for the segmentation of different organs and tumors in full resolution. second, we employ vox2vec to pre-train a fpn architecture on a diverse collection of six unannotated datasets, totaling over 6,500 ct images of the thorax and abdomen. we make the pre-trained model publicly available to simplify the reproduction of our results and to encourage practitioners to utilize this model as a starting point for the segmentation algorithms training. finally, we compare the pretrained model with the baselines on 22 segmentation tasks on seven ct datasets in three setups: linear probing, non-linear probing, and fine-tuning. we show that vox2vec performs slightly better than sota models in the fine-tuning setup and outperforms them by a huge margin in the linear and non-linear probing setups. to the best of our knowledge, this is the first successful attempt to evaluate dense ssl methods in the medical imaging domain in linear and non-linear probing regimes."
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,5.0,Results,"the mean value and standard deviation of dice score across 5 folds on the btcv dataset for all models in all evaluation setups are presented in table  nevertheless, vox2vec-fpn significantly outperforms other models in linear and non-linear regimes. on top of that, we observe that in non-linear probing regime, it performs (within the standard deviation) as well as the fpn trained from scratch while having x50 times fewer trainable parameters (see fig.  we reproduce the key results on msd challenge ct datasets, which contain tumor and organ segmentation tasks. table "
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,6.0,Conclusion,"in this work, we present vox2vec -a self-supervised framework for voxel-wise representation learning in medical imaging. our method expands the contrastive learning setup to the feature pyramid architecture allowing to pre-train effective representations in full resolution. by pre-training a fpn backbone to extract informative representations from unlabeled data, our method scales to large datasets across multiple task domains. we pre-train a fpn architecture on more than 6500 ct images and test it on various segmentation tasks, including different organs and tumors segmentation in three setups: linear probing, nonlinear probing, and fine-tuning. our model outperformed existing methods in all regimes. moreover, vox2vec establishes a new state-of-the-art result on the linear and non-linear probing scenarios. still, this work has a few limitations to consider. we plan to investigate further how the performance of vox2vec scales with the increasing size of the pre-training dataset and the pre-trained architecture size. another interesting research direction is exploring the effectiveness of vox2vec with regard to domain adaptation to address the challenges of domain shift between different medical imaging datasets obtained from different sources. a particular interest is a lowshot scenario when only a few examples from the target domain are available."
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,1.0,Introduction,"deep learning has brought medical image segmentation into the era of datadriven approaches, and has made significant progress in this field  domain adaptation (da) has been proposed and investigated to combat distribution shift in medical image segmentation. many researchers proposed using adversarial learning to tackle distribution shift problems  to tackle the aforementioned issues, we propose utilizing prompt learning to take full advantage of domain information. prompt learning  in this paper, we introduce a domain prompt learning method (prompt-da) to tackle distribution shift in multi-target domains. different from the recent prompt learning methods, we generate domain-specific prompts in the encoding feature space instead of the image space. as a consequence, it can improve the quality of the domain prompts, more importantly, we can easily consolidate the prompt learning with the other da methods, for instance, adversarial learning based da. in addition, we propose a specially designed fusion module to reinforce the respective characteristics of the encoder features and domain-specific prompts, and thus generate domain-aware features. as a way to prove the prompt-da is compatible with other das, a very simple adversarial learning module is jointly adopted in our method to further enhance the model's generalization ability (we denote this model as comb-da). we evaluate our proposed method on two multi-domain datasets: 1). the infant brain mri dataset for cross-age segmentation; 2). the brats2018 dataset for cross-grade tumor segmentation. experiments show our proposed method outperforms state-of-the-art methods. moreover, ablation study demonstrates the effectiveness of the proposed domain prompt learning and the feature fusion module. our claim about the successful combination of prompt learning with adversarial learning is also well-supported by experiments."
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,3.1,Datasets,"our proposed method was evaluated using two medical image segmentation da datasets. the first dataset, i.e., cross-age infant segmentation  the first dataset is for infant brain segmentation (white matter, gray matter and cerebrospinal fluid). to build the cross-age dataset, we take advantage 10 brain mris of 6-month-old from iseg2019  the 2nd dataset is for brain tumor segmentation (enhancing tumor, peritumoral edema and necrotic and non-enhancing tumor core), which has 285 mri samples (210 hgg and 75 lgg). we take hgg as the source domain and lgg as the target domain. "
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,3.2,Comparison with State-of-the-Art (SOTA) Method,"we compared our method with four sota methods: adda  for fair comparison, we have replaced the backbone of these models with the same we used in our approach. the quantitative comparison results of cross-age infant brain segmentation is presented in table  when transferring to a single target domain in the brain tumor segmentation task, our proposed da solution improves about 3.09 dice in the target lgg domain. also, the proposed method shows considerable improvements over adda and cycada, but very subtle improvements to the sifa and adr methods (although adr shows a small advantage on the whole category). we also visualize the segmentation results on a typical test sample of the infant brain dataset in fig. "
Gall Bladder Cancer Detection from US Images with only Image Level Labels,2.0,Datasets,"gallbladder cancer detection in ultrasound images: we use the public gbc us dataset  note that, we use only the image labels for training. we report results on 5-fold cross-validation. we did the cross-validation splits at the patient level, and all images of any patient appeared either in the train or validation split. polyp detection in colonoscopy images: we use the publicly available kvasir-seg  since the patient information is not available with the data, we use random stratified splitting for 5-fold cross-validation."
Structured State Space Models for Multiple Instance Learning in Digital Pathology,1.0,Introduction,"precision medicine efforts are shifting cancer care standards by providing novel personalised treatment plans with promising outcomes. patient selection for such treatment regimes is based principally on the assessment of tissue biopsies and the characterisation of the tumor microenvironment. this is typically performed by experienced pathologists, who closely inspect chemically stained histopathological whole slide images (wsis). increasingly, clinical centers are investing in the digitisation of such tissue slides to enable both automatic processing as well as research studies to elucidate the underlying biological processes of cancer. the resulting images are of gigapixel size, rendering their computational analysis challenging. to deal with this issue, multiple instance learning (mil) schemes based on weakly supervised training are used for wsi classification tasks. in such schemes, the wsi is typically divided into a grid of patches, with general purpose features derived from pretrained imagenet  state space models are designed to efficiently model long sequences, such as the sequences of patches that arise in wsi mil. in this paper, we present the first use of state space models for wsi mil. extensive experiments on three publicly available datasets show the potential of such models for the processing of gigapixel-sized images, under both weakly and multi-task schemes. moreover, comparisons with other commonly used mil schemes highlight their robust performance, while we demonstrate empirically the superiority of state space models in processing the longest of wsi sequences with respect to commonly used mil methods."
Structured State Space Models for Multiple Instance Learning in Digital Pathology,4.1,Data,"camelyon16  tcga-luad is a tcga lung adenocarcinoma dataset that contains 541 wsis along with genetic information about each patient. we obtained genetic information for this cohort using xena browser  tcga-rcc is a tcga dataset for three kidney cancer subtypes (denoted kich, kirc, and kirp). it consists of 936 wsis (121 kich, 518 kirc, and 297 kirp). the average sequence length is 12234 (ranging from 319 to 62235)."
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,1.0,Introduction,"longitudinal lesion or tumor tracking is a fundamental task in treatment monitoring workflows, and for planning of re-treatments in radiation therapy. based on longitudinal imaging for a given patient it requires establishing which lesions are corresponding (i.e., same lesion, observed at different timepoints), which lesions have disappeared and which are new compared to prior scanning. this information can be leveraged to assess treatment response, e.g., by analyzing the evolution of size and morphology for a given tumor  in practice, the development of automatic and reliable lesion tracking solutions is hindered by the complexity of the data (over different modalities), the absence of large, annotated datasets, and the difficulties associated with lesion identification (i.e., varying sizes, poses, shapes, and sparsely distributed locations). in this work, we present a multi-scale self-supervised learning solution for lesion tracking in longitudinal studies using the capabilities of contrastive learning  our proposed method brings two elements of novelty from a technical point of view: (1) the multi-scale approach for the anatomical embedding learning and (2) a positive sampling approach that incorporates anatomically significant landmarks across different subjects. with these two strategies, the goal is to ensure a high degree of robustness in the computation of the lesion matching across different lesion sizes and varying anatomies. furthermore, a significant focus and contribution of our research is the experimental study at a very large scale: we (1) train a pixel-wise self-supervised system using a very large and diverse dataset of 52,487 ct volumes and (2) evaluate on two publicly available datasets. notably, one of the datasets, nlst, presents challenging cases with 68% of lesions being very small (i.e., radius < 5 mm)."
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,5.0,Conclusion,"in conclusion, this paper presents an effective method for longitudinal lesion tracking based on multi-scale self-supervised learning. the method is generic, it does not require expert annotations or longitudinal data for training and can generalize to different types of tumors/organs/modalities. the multi-scale approach ensures a high degree of robustness and accuracy for small lesions. through large-scale experiments and validation on two longitudinal datasets, we highlight the superiority of the proposed method in comparison to state-of-theart. we found that adopting a multi-scale approach (instead of the global/local approach as proposed in  disclaimer: the concepts and information presented in this paper/presentation are based on research results that are not commercially available. future commercial availability cannot be guaranteed."
Geometry-Invariant Abnormality Detection,1.0,Introduction,"the use of machine learning for anomaly detection in medical imaging analysis has gained a great deal of traction over previous years. most recent approaches have focused on improvements in performance rather than flexibility, thus limiting approaches to specific input types -little research has been carried out to generate models unhindered by variations in data geometries. often, research assumes certain similarities in data acquisition parameters, from image dimensions to voxel dimensions and fields-of-view (fov). these restrictions are then carried forward during inference  unsupervised methods have become an increasingly prominent field for automatic anomaly detection by eliminating the necessity of acquiring accurately labelled data  even though these methods are state-of-the-art, they have stringent data requirements, such as having a consistent geometry of the input data, e.g., in a whole-body imaging scenario, it is not possible to crop a region of interest and feed it to the algorithm, as this cropped region will be wrongly detected as an anomaly. this would happen even in the case that a scan's original fov was restricted  as such, we propose a geometric-invariant approach to anomaly detection, and apply it to cancer detection in whole-body pet via an unsupervised anomaly detection method with minimal spatial labelling. through adapting the vq-vae transformer approach in "
Geometry-Invariant Abnormality Detection,4.0,Results,"the proposed model was trained on the data described in sect. 3.3, with random crops applied while training. model and anomaly detection hyperparameter tuning was done on our validation samples using the best dice scores. we then test our model and baselines on 4 hold-out test sets: a low-resolution whole-body set, a low-resolution cropped set, a high-resolution rotated set and a high-resolution test set of pet images with varying cancers. the visual results shown in fig. "
Geometry-Invariant Abnormality Detection,5.0,Conclusion,"detection and segmentation of anomalous regions, particularly for cancer patients, is essential for staging, treatment and intervention planning. generally, the variation scanners and acquisition protocols can cause failures in models trained on data from single sources. in this study, we proposed a system for anomaly detection that is robust to variances in geometry. not only does the proposed model showcase strong and statistically-significant performance improvements on varying image resolutions and fov, but also on whole-body data. through this, we demonstrate that one can improve the adaptability and flexibility to varying data geometries while also improving performance. such flexibility also increases the pool of potential training data, as they dont require the same fov. we hope this work serves as a foundation for further exploration into geometry-invariant deep-learning methods for medical-imaging."
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,3.0,Experiments,"data. the proposed approach is evaluated using the publicly available lidc-idri dataset consisting of 1018 clinical thoracic ct scans from patients with non-small cell lung cancer (nsclc)  experiment designs. to ensure comparability with previous work  the algorithm was implemented using the pytorch framework version 1.13 and cuda version 11.6. a learning rate of 0.5 was chosen for the prototype vectors and 0.02 for the other learnable parameters. the batch size was set to 128 and the optimizer was adam  besides pure performance, the effect of reduced availability of attribute annotations was investigated. this was done by using attribute information only for a randomly selected fraction of the nodules during the training. to investigate the effect of prototypes on the network performance, an ablation study was performed. three networks were compared: proto-caps (proposed) including learning and applying prototypes during inference, proto-caps w/o use where prototypes are only learned but ignored for inference, and proto-caps w/o learn using the proposed architecture without any prototypes."
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,1.0,Introduction,"colonoscopy is a critical tool for identifying adenomatous polyps and reducing rectal cancer mortality. deep learning methods have shown powerful abilities in automatic colonoscopy analysis, including polyp segmentation "
Synthetic Augmentation with Large-Scale Unconditional Pre-training,3.0,Experiments,"datasets. we employ three public datasets of histopathology images during the large-scale pre-training procedure. the first one is the h&e breast cancer dataset  to replicate a scenario where only a small annotated dataset is available for training, we have opted to utilize a subset of 5,000 (5%) samples for finetuning. this subset has been carefully selected through an even sampling without replacement from each tissue type present in the train set. it is worth noting that the labels for these samples have been kept, which allows the fine-tuning process to be guided by labeled data, leading to better predictions on the specific task or domain being trained. by ensuring that the fine-tuning process is representative of the entire dataset through even sampling from each tissue type, we can eliminate bias towards any particular tissue type. we evaluate the fine-tuned model on the official test set. the related data use declaration and acknowledgment can be found in our supplementary materials. evaluation metrics. we employ fréchet inception distance (fid) score  model implementation. all the patches are resized to 256 × 256 × 3 before being passed into the models. our implementation of histodiffusion basically follows the ldm-4  we use the same architecture for the auxiliary image classifier ϕ. for downstream evaluation, we implement the classifier using the vit-b/16 architecture  comparison to state-of-the-art. we compare our proposed histodiffusion with the current state-of-the-art cgan-based method "
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,1.0,Introduction,"a must-have ingredient for training a deep neural network (dnn) is a large number of labelled data that is not always available in real-world applications. this challenge of data annotation becomes even worse for medical image segmentation tasks that require pixel-level annotation by experts. data augmentation (da) is a recognized approach to tackle this challenge. common da strategies create new samples by using predefined transformations such as rotation, translation, and colour jitter to existing data, where the performance gains heavily relies on the choice of augmentation operations and parameters  to mitigate this reliance, recent efforts have focused on learning optimal augmentation operations for a given task and dataset  however, to date, all existing approaches to learning deformable registrationbased da assume a perfect alignment of image pairs to learn the transformations. in other words, the deformation-based transformations are learned globally for the image. this assumption is restrictive and associated with several challenges. first, the learning of a global image-level transformation requires image alignment that may be non-trivial in many scenarios, such as the alignment of tumours that can appear at different locations of an image, or alignment of images from different modalities. the learning of transformations itself is also complicated by the presence of other objects in the image and is best suited when the object of interest is always in the same (and often centre) location in all the images, i.e., images are globally aligned a priori  intuitively, object-centric transformations and augmentations have the potential to overcome the challenges associated with global image-level transformations. recently, an object-centric augmentation method termed as tumorcp  similarly, other existing works on object-level augmentation of lesions have mostly focused on position, orientation, and random transformations of the lesion on different backgrounds  in this paper, we present a novel approach for learning and transferring object-centric deformations for da in medical image segmentation tasks. as illustrated in fig.  -a generative model of object-centric deformations -constrained to c1 diffeomorphism for better dnn training -to describe shape variability learned from paired patches of objects of interest. this allows the learning to focus on the shape variations of an object regardless of its positions and sizes in the image, thus bypassing the requirement for image alignment. -an online augmentation strategy to sample transformations from the generative model and to augment the objects of interest in place without distorting the surrounding content in the image. this allows us to add shape diversity to the objects of interest in an image regardless of their positions or sizes, eventually facilitating transferring the learned variations across datasets. we demonstrated the effectiveness of the presented object-centric diffeomorphic augmentation in kidney tumour segmentation, including using shape variations of kidney tumours learned from the same dataset (kits "
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,2.0,Methods,we focus on da for tumour segmentation because tumours can occur at different locations of an organ with substantially different orientations and sizes. it thus presents a challenging scenario where global image-level deformable transformations cannot apply. mentation approach comprises as outlined in below we describe the two key methodological elements.
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,2.1,Object-Centric Diffeomorphism as a Generative Model,"the goal of this element is to learn to generate diffeomorphic transformation parameters θ that describe shape variations -in the form of deformable transformations t θ -that are present within training instances of tumour x. to realize this, we train a generative model g(.) for θ such that, when given two instances of tumours (x src , x tgt ), it is asked to generate θ from the encoded latent representations z in order to deform x src through t θ (x src ) to x tgt . transformations: in order to model shape deformations between x src and x tgt , we need highly expressive transformations to capture rich shape variations in tumour pairs. we assume a spatial transformation t θ in the form of pixel-wise displacement field u as t θ (x) = x + u. inspired from  where the integration can be done via a specialized solver  generative modeling: the data generation process can be described as: where z is the latent variable assumed to follow an isotropic gaussian prior, p φ (θ|z) is modeled by a neural network parameterized by φ, and p(x tgt |θ, x src ) follows the deformable transformation as described in equation (  we define variational approximations of the posterior density as q ψ (z|x src , x tgt ), modeled by a convolutional neural network that expects two inputs x src and x tgt . passing a tuple of x src and x tgt as the input helps the latent representations to learn the spatial difference between two tumour samples. alternatively, the generative model as described can be considered as a conditional model where both the generative and inference model is conditioned on the source tumour sample x src . variational inference: the parameters ψ and φ are optimized by the modified evidence lower bound (elbo) of the log-likelihood log p(x tgt |x src ): where the first term in the elbo takes the form of similarity loss: l 2 norm on the difference between x tgt and xsrc = t θ (x src ) synthesized using the θ from g(z). the second kl term constrains our approximated posterior q ψ (z|x src , x tgt ) to be closer to the isotropic gaussian prior p(z), and its contribution to the overall loss is scaled by the hyperparameter β. to further ensure that xsrc looks realistic, we discourage g(z) from generating overly-expressive transformations by adding a regularization term over the l 2 norm of the displacement field u with a tunable hyperparameter λ reg . the final objective function becomes: object-centric learning: to learn object-centric spatial transformations, x src and x tgt are in the forms of image patches that solely contain tumours. given an image and its corresponding tumour segmentation mask (x, y ), we first extract a bounding box around the tumour by applying skimage.measure.regionprops from the scikit-image package to y . we then use this bounding box to carve out the tumour x from the image x, masking out all the regions within the bounding box that do not belong to the tumour. all the tumour patches are then resized to the same scale, such that tumours of different sizes can be described by the same tesselation resolution. when pairing tumour patches, we pair each tumour with its k nearest neighbour tumours based on their euclidean distance -this again avoids learning overly expressive transformation when attempting to deform between significantly different tumour shapes."
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,2.2,Online Augmentations with Generative Models,"the goal of this element is to sample random object-centric transformations of t θ from g(z), to generate diverse augmentations of different instances of tumours in place. however, if we only transform the tumour and keep the rest of the image identical, the transformed tumour may appear unrealistic and out of place. to ensure that the entire transformed image appears smooth, we use a hybrid strategy to construct a deformation field for the entire image x that combines tumour-specific deformations with an identity transform for the rest of the image. specifically, we fill a small region around the tumour with displacements of diminishing magnitudes, achieved by propagating the deformations from the boundaries of the deformation fields from g(z) to their neighbours with reduced magnitudes. repeating this process ensures that the change at the boundaries is smooth and that the transformed region appears naturally as part of the image. "
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,,Model:,"the encoder of g(z) consisted of five convolutional layers and three fully connected layers, with a latent dimension of 12 for z. the decoder consisted of five fully connected layers to output the parameters θ for t θ . we trained the g(z) for a total of 400 epochs and a batch size of 16. we also implemented early stopping if the validation loss does not improve for 20 epochs. we used adam optimizer  results: we evaluated g(z) with two criteria. first, the model needs to be able to reconstruct x tgt by generating θ to transform x src . second, the model needs to be able to generate diverse transformed tumour samples for a given tumour sample. figure "
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,3.2,Deformation-Based da for Kidney Tumour Segmentation,"data: we then used g(z) to generate deformation-based augmentations to increase the size and diversity of training samples for kidney tumour segmentation on kits. to assess the effect of augmentations on different sizes of labelled data, we considered training using 25%, 50%, 75%, and 100% of the kits training set. we considered two da scenarios: augment with transformations learned from kits (within-data augmentation) versus from lits (cross-data augmentation). models: for the base segmentation network, we adopted nnu-net  results: we use sørensen-dice coefficient (dice) to measure segmentation network performance. dice measures the overlap between prediction and ground truth. as summarized in table "
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,4.0,Discussion and Conclusions,"in this work, we presented a novel diffeomorphism-based object-centric augmentation that can be learned and used to augment the objects of interest regardless of their position and size in an image. as demonstrated by the experimental results, this allows us to not only introduce new variations to unfixed objects like tumours in an image but also transfer the knowledge of shape variations across datasets. an immediate next step will be to extend the presented approach to learn and transfer 3d transformations for 3d segmentation tasks, and to enrich the shape-based transformation with appearance-based transformations. in the long term, it would be interesting to explore ways to transfer knowledge about more general forms of variations across datasets."
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,1.0,Introduction,"globally, cancer is a leading cause of death and the burden of cancer incidence and mortality is rapidly growing  recently, many computational tools have shown to be effective in analyzing pathology images  in this study, we propose a centroid-aware feature recalibration network (cafenet) for accurate and robust cancer grading in pathology images. cafenet is built based upon three major components: 1) a feature extractor, 2) a centroid update (cup) module, and 3) a centroid-aware feature recalibration (cafe) module. the feature extractor is utilized to obtain the feature representation of pathology images. cup module obtains and updates the centroids of class labels, i.e., cancer grades. cafe module adjusts the input embedding vectors with respect to the class centroids (i.e., training data distribution). assuming that the classes are well separated in the feature space, the centroid embedding vectors can serve as reference points to represent the data distribution of the training data. this indicates that the centroid embedding vectors can be used to recalibrate the input embedding vectors of pathology images. during inference, we fix the centroid embedding vectors so that the recalibrated embedding vectors do not vary much compared to the input embedding vectors even though the data distribution substantially changes, leading to improved stability and robustness of the feature representation. in this manner, the feature representations of the input pathology images are re-calibrated and stabilized for a reliable cancer classification. the experimental results demonstrate that cafenet achieves the state-of-the-art cancer grading performance in colorectal cancer grading datasets. the source code of cafenet is available at https://github.com/col in19950703/cafenet."
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,2.1,Centroid-Aware Feature Recalibration,"let {x i , y i } n i=1 be a set of pairs of pathology images and ground truth labels where n is the number of pathology image-ground truth label pairs, x i ∈ r h×w×c is the i th pathology image, y i ∈ {c 1 , . . . , c m } represents the corresponding ground truth label. h, w, and c denote the height, width, and the number of channels, respectively. m is the cardinality of the class labels. given x i , a deep neural network f maps x i into an embedding space, producing an embedding vector e i ∈ r d . the embedding vector e i is fed into 1) a centroid update (cup) module and 2) a centroid-aware feature recalibration (cafe) module. cup module obtains and updates the centroid of the class label in the embedding space e c ∈ r m ×d . cafe module adjusts the embedding vector e i in regard to the embedding vectors of the class centroids and produces a recalibrated embedding vector e r i . e i and e r i are concatenated together and is fed into a classification layer to conduct cancer grading.  from the centroid embedding vectors e c by using a linear layer. then, attention scores are computed via a dot product between q e and k c followed by a softmax operation. multiplying the attention scores by v c , we obtain the recalibrated feature representation e r for the input embedding vectors e. the process can be formulated as follows: finally, cafe concatenates e and e r and produces them as the output."
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.1,Datasets,two publicly available colorectal cancer datasets 
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.2,Comparative Experiments,"we conducted a series of comparative experiments to evaluate the effectiveness of cafenet for cancer grading, in comparison to several existing methods: 1) three dcnnbased models: resnet "
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.4,Result and Discussions,"we evaluated the performance of colorectal cancer grading by the proposed cafenet and other competing models using five evaluation metrics, including accuracy (acc), precision, recall, f1-score (f1), and quadratic weighted kappa (κ w ). table  were the best performing models. among dcnn-based models, resnet was superior to other dcnn-based models. metric learning was able to improve the classification performance. effcientnet was the worst model among them, but with the help of triplet loss (triplet) or supervised contrastive loss (sc), the overall performance increased by ≥2.8% acc, ≥0.023 precision, ≥0.001 recall, ≥0.010 f1, and ≥0.047 κ w . among the transformer-based models, swin was one of the best performing models, but vit showed much lower performance in all evaluation metrics. moreover, we applied the same models to c testii to test the generalizability of the models. we note that c testi originated from the same set with c train and c validation and c testii was obtained from different time periods and using a different slide scanner. table  we conducted ablation experiments to investigate the effect of the cafe module on cancer classification. the results are presented in table "
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,4.0,Conclusions,"herein, we propose an attention mechanism-based deep neural network, called cafenet, for cancer classification in pathology images. the proposed approach proposes to improve the feature representation of deep neural networks by re-calibrating input embedding vectors via an attention mechanism in regard to the centroids of cancer grades. in the experiments on colorectal cancer datasets against several competing models, the proposed network demonstrated that it has a better learning capability as well as a generalizability in classifying pathology images into different cancer grades. however, the experiments were only conducted on two public colorectal cancer datasets from a single institute. additional experiments need to be conducted to further verify the findings of our study. therefore, future work will focus on validating the effectiveness of the proposed network for other types of cancers and tissues in pathology images."
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,3.1,Dynamic Contrast-Enhanced Liver CT,"our xai technique was applied to explain the mtann model's decision in a liver tumor segmentation task  in addition, since most liver tumors' shape is ellipsoidal, the liver tumors can also be enhanced by the hessian-based method and utilized in the model to improve the performance  seven cases and 24 cases in the dynamic contrast-enhanced ct scans dataset were used for training and testing, respectively. 10,000 patches were randomly selected from the liver mask region in each case, summing up to a total of 70,000 training samples for training. the number of input units in the mtann model with one hidden layer was 250. the structure optimization process started with 80 hidden units in the hidden layer. the binary cross-entropy (bce) loss function was used to train the model. the mtann model classified the input patches into tumor or non-tumor classes, and the output pixels represented the probability of being a tumor class. during the structure optimization process, the f1 score on the training patches and the dice coefficient on the training images were also calculated as the reference to select a suitable compact model that performed equivalently to the original large model. as observed in the four evaluation metric curves in fig.  then, we applied the unsupervised hierarchical clustering algorithm to the weighted function maps from the optimized compact model with 9 hidden units. figure "
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,4.0,Conclusion,"in this study, we proposed a novel xai approach to explain the functions and behavior of an mtann model for semantic segmentation of liver tumors in ct. our structure optimization algorithm refined the structure and made every hidden unit in the model have a clear, meaningful function by removing redundant hidden units and ""condensing"" the functions into fewer hidden units, which solved the issue of unstable xai results with conventional xai methods. the unsupervised hierarchical clustering algorithm in our xai approach grouped the hidden units with a similar function into one group so as to explain their functions by group. through the experiments, we successfully proved that the mtann model was explainable by functions."
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,1.0,Introduction,"liver cancer is one of the most deadly cancers and has the second highest fatality rate  a single-phase lesion annotation means the annotation of both lesion position and its class. in hospitals, collected multi-phase cts are normally grouped by patients rather than lesions, which makes single-phase lesion annotation insufficient for feature fusion learning. however, the number of lesions inside a single patient can vary from one to dozens and they can be of different types in realistic cases. multi-phase cts are also not co-registered in most cases, therefore, it is necessary to make sure the lesions extracted from different phases are somehow aligned for feature fusion, which is called as multi-phase lesion annotation. moreover, while most works have attached much importance to liver lesion segmentation  self-attention based transformers  in this paper, we construct a hybrid framework with vit backbone for liver lesion classification, transliver. we design a pre-processing unit to reduce the annotation cost, where we obtain lesion area on multi-phase cts from annotations marked on a single phase. to alleviate the limitations of pure transformers, we propose a multi-stage pyramid structure and add convolutional layers to the original transformer encoder. we use additional cross phase tokens at the last stage to complete a multi-phase fusion, which can focus on cross-phase communication and improve the fusion effectiveness as compared with conventional modes. while most multi-phase liver lesion classification studies use datasets with no more than three phases (without dl phase for its difficulty of collection) or no more than six lesion classes, we validate the whole framework on an in-house dataset with four phases of abdominal ct and seven classes of liver lesions. considering the disproportion of axial lesion slice number and the relatively small scale of the dataset, we adopt a 2-d network in classification part instead of 3-d in pre-processing part and achieve a 90.9% accuracy."
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,1.0,Introduction,"semantic segmentation on histological whole slide images (wsis) allows precise detection of tumor boundaries, thereby facilitating the assessment of metastases  we use region-based active learning (al)  this work focuses on region selection methods, a topic that has been largely neglected in literature until now, but which we show to have a great impact on al sampling efficiency (i.e., the annotated area required to reach the full annotation performance). we discover that the sampling efficiency of the aforementioned standard method decreases as the al step size (i.e., the annotated area at each al cycle, determined by the multiplication of the region size and the number of selected regions per wsi) increases. to avoid extensive al step size tuning, we propose an adaptive region selection method with reduced reliance on this al hyperparameter. specifically, our method dynamically determines an annotation region by first identifying an informative area with connected component detection and then detecting its bounding box. we test our method using a breast cancer metastases segmentation task on the public camelyon16 dataset and demonstrate that determining the selected regions individually provides greater flexibility and efficiency than selecting regions with a uniform predefined shape and size, given the variability in histological tissue structures. results show that our method consistently outperforms the standard method by providing a higher sampling efficiency, while also being more robust to al step size choices. additionally, our method is especially beneficial for settings where a large al step size is desirable due to annotator availability or computational restrictions."
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,2.3,WSI Semantic Segmentation Framework,"this section describes the breast cancer metastases segmentation task we use for evaluating the al region selection methods. the task is performed with patch-wise classification, where the wsi is partitioned into patches, each patch is classified as to whether it contains metastases, and the results are assembled. training. the patch classification model h(x, w) : r d×d -→ [0, 1] takes as input a patch x and outputs the probability p(y = 1|x, w) of containing metastases, where w denotes model parameters. patches are extracted from the annotated regions at 40× magnification (0.25 µm px ) with d = 256 pixels. following "
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,3.3,Results,"full annotation performance. to validate our segmentation framework, we first train on the fully-annotated data (average performance of five repetitions reported). with a patch extraction stride s = 256 pixels, our framework yields an froc score of 0.760 that is equivalent to the challenge top 2, and an miou (tumor) of 0.749, which is higher than the most comparable method in "
Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,1.0,Introduction,"when using a machine learning (ml) model during intraoperative tissue characterisation, it is vital that the surgeon is able to assess how reliable a model's prediction is  despite being speedy, easy to deploy and able to localise semantic features, the above methods lack trustworthiness due to the training strategy of their underlying model. deep learning (dl) models trained with empirical risk minimisation (erm) are overconfident in prediction  in this paper, we propose the first approach which incorporates risk estimation into a pa method. a classification model is trained with dropout and a pa method is used to generate a pa map. at test time, the classification model is employed with the dropout enabled. in this work, we propose to repeat this process for a number of iterations creating a volume of pa maps. this volume is used to generate a pixel-wise distribution of pa values from which we can infer risk. more specifically, we introduce a method to generate an enhanced pa map by estimating the expectation values of the pixel-wise distributions. in addition, the coefficient of variation (cv) is used to estimate pixel-wise risk of this enhanced pa map. this provides an improved explanation of the model's prediction by clearly presenting to the surgeon which salient areas to trust in the model's enhanced pa map. in this work, we focus on the explainability of the classification of brain tumours using probe-based confocal laser endomicroscopy (pcle) data. performance evaluation on pcle data shows that our improved explainability method outperforms the sota. "
Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,3.0,Experiments and Analysis,"dataset. the developed explainability framework has been validated on an in vivo and ex vivo pcle dataset of meningioma, glioblastoma and metastases of an invasive ductal carcinoma (idc). all studies on human subjects were performed according to the requirements of the local ethic committee and in agreement with the declaration of helsinki (no. cle-001 nr: 2014480). the cellvizio c by mauna kea technologies, paris, france has been used in combination with the mini laser probe cystoflex c uhd-r. the distinguishing characteristic of the meningioma is the psammoma body with concentric circles that show various degrees of calcification. regarding glioblastomas, the pcle images allow for the visualization of the characteristic hypercellularity, evidence of irregular nuclei with mitotic activities or multinuclear appearance with irregular cell shape. when examining metastases of an idc, the tumor presents as egg-shaped cells with uniform evenly spaced nuclei. our dataset includes 38 meningioma videos, 24 glioblastoma and 6 idc. each pcle video represents one tumour type and corresponds to a different patient. the data has been curated to remove noisy images and similar frames. this resulted in a training dataset of 2500 frames per class (7500 frames in total) and a testing dataset of the same size. the dataset is split into a training and testing subset, with the division done on the patient level. implementation. to implement the dl models we use the open-source framework pytorch  evaluation metrics. evaluating a pa method is not a trivial task because a pa map may not need to be inline with what a human deems ""reasonable""  where, x = x f s ( ŷ (x). the above equation measures the effect on the output score of the classification model if we only include the pixels which the pa method scored highly. a minimum average drop is desired. as average drop was found to not be sufficient on its own, the unified method adcc  coherency is the pearson correlation coefficient which ensures that the remaining pixels after dropping are still important, defined as: where, cov(., .) is the covariance and σ is the standard deviation. a higher coherency is better. complexity is the l1 norm of the output pa map. complexity is used to measure how cluttered a pa map is. for a good pa map, complexity should be a minimum. as it has been shown in the literature, the metrics in eqs. ( "
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,1.0,Introduction,"humans inherently learn in an incremental manner, acquiring new concepts over time without forgetting previous ones. in contrast, deep learning models suffer from catastrophic forgetting  the medical domain faces a similar problem: the ability to dynamically extend a model to new classes is critical for multiple organ and tumor segmentation, wherein the key obstacle lies in mitigating 'forgetting.' a typical strategy involves retaining some previous data. for instance, liu et al.  therefore, we identify two main open questions that must be addressed when designing a multi-organ and tumor segmentation framework. q1: can we relieve the forgetting problem without needing previous data and annotations? q2: can we design a new model architecture that allows us to share more parameters among different continual learning steps? to tackle the above questions, in this paper, we propose a novel continual multi-organ and tumor segmentation method that overcomes the forgetting problem with little memory and computation overhead. first, inspired by knowledge distillation methods in continual learning  we focus on organ/tumor segmentation because it is one of the most critical tasks in medical imaging  segment liver tumors in the lits dataset. on the private dataset, the learning trajectory is to first segment 13 organs, followed by continual segmentation of three gastrointestinal tracts and four cardiovascular system structures. in our study, we review and compare three popular continual learning baselines that apply knowledge distillation to predictions "
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Image-Aware Organ-Specific Heads:,"the vanilla swin unetr has a softmax layer as the output layer that predicts the probabilities of each class. we propose to replace the output layer with multiple image-aware organ-specific heads. we first use a global average pooling (gap) layer on the last encoder features to obtain a global feature f of the current image x. then for each organ class k, a multilayer perceptron (mlp) module is learned to map the global image feature to a set of parameters θ k : where e(x) denotes the encoder feature of image x. an output head for organ class k is a sequence of convolution layers that use parameters θ k as convolution kernel parameters. these convolution layers are applied to the decoder features, which output the segmentation prediction for organ class k: where e is the encoder, d is the decoder, σ is the sigmoid non-linear layer and p (y k j = 1) denotes the predicted probability that pixel j belongs to the organ class k. the predictions for each class are optimized by binary cross entropy loss. the separate heads allow independent probability prediction for newly introduced and previously learned classes, therefore minimizing the impact of new classes on old ones during continual learning. moreover, this design allows multi-label prediction for cases where a pixel belongs to more than one class (e.g., a tumor on an organ)."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,4.0,Conclusion,"in this paper, we propose a method for continual multiple organ and tumor segmentation in 3d abdominal ct images. we first empirically verified the effectiveness of high-quality pseudo labels in retaining previous knowledge. then, we propose a new model design that uses organ-specific heads for segmentation, which allows easy extension to new classes and brings little computational cost in the meantime. the segmentation heads are further strengthened by utilizing the clip text embeddings that encode the semantics of organ or tumor classes. numerical results on an in-house dataset and two public datasets demonstrate that the proposed method outperforms the continual learning baseline methods in the challenging multiple organ and tumor segmentation tasks."
Efficient Subclass Segmentation in Medical Images,2.0,Method,"problem definition. we start by considering a set of r coarse classes, denoted by y c = {y 1 , ..., y r }, such as background and brain tumor, and a set of n training images, annotated with y c , denoted by d c = {(x l , y l )|y l i ∈ y c } n l=1 . each pixel i in image x l is assigned a superclass label y l i . to learn a finer segmentation model, we introduce a set of fine subclass kr }, such as background, enhancing tumor, tumor core, and whole tumor. we assume that only a small subset of n training images have pixel-wise subclass labels z ∈ y f denoted by our goal is to train a segmentation network f (x l ) that can accurately predict the subclass labels for each pixel in the image x l , even when n n . without specification, we consider r = 2 (background and foreground) and extend the foreground class to multi subclass in this work. prior concatenation. one direct way to leverage the superclass and subclass annotations simultaneously is using two 1×1×1 convolution layers as superclass and subclass classification heads for the features extracted from the network. the superclassification and subclassification heads are individually trained by superclass p c (x l ) labels and subclass labels p f (x l ). with enough superclass labels, the feature maps corresponding to different superclasses should be well separated. however, this coerces the subclassification head to discriminate among k subclasses under the mere guidance from few subclass annotations, making it prone to overfitting. another common method to incorporate the information from superclass annotations into the subclassification head is negative learning  to make use of superclass labels without affecting the training of the subclass classification head, we propose a simple yet effective method called prior concatenation (pc): as shown in fig.  separate normalization. intuitively, given sufficient superclass labels in supervised learning, the superclassification head tends to reduce feature distance among samples within the same superclass, which conflicts with the goal of increasing the distance between subclasses within the same superclass. to alleviate this issue, we aim to enhance the internal diversity of the distribution within the same superclass while preserving the discriminative features among superclasses. to achieve this, we propose separate normalization(sn) to separately process feature maps belonging to hierarchical foreground and background divided by superclass labels. as a superclass and the subclasses within share the same background, the original conflict between classifiers is transferred to finding the optimal transformations that separate foreground from background, enabling the network to extract class-specific features while keeping the features inside different superclasses well-separated. our framework is shown in fig.  hierarchicalmix. given the scarcity of subclass labels, we intend to maximally exploit the existent subclass supervision to guide the segmentation of coarsely labeled samples. inspired by guidedmix  as shown in fig.  next, we adopt image mixup by cropping the bounding box of foreground pixels in x , resizing it to match the size of foreground in x, and linearly overlaying them by a factor of α on x. this semantically mixed image x mix has subclass labels z = resize(α • z ) from the fine-labeled image x . then, we pass it through the network to obtain a segmentation result f (x mix ). this segmentation result is supervised by the superposition of the pseudo label map z pse and subclass labels z, with weighting factor α: the intuition behind this framework is to simultaneously leverage the information from both unlabeled and labeled data by incorporating a more robust supervision from transform-invariant pseudo labels. while mixing up only the semantic foreground provides a way of exchanging knowledge between similar foreground objects while lifting the confirmation bias in pseudo labeling "
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,1.0,Introduction,"despite the success of artificial intelligence (ai) in aiding diagnosis, its application to medical education remains limited. trainee physicians require several years of experience with a diverse range of clinical cases to develop sufficient skills and expertise. however, designing educational materials solely based on real-world data poses several challenges. for example, although small but significant disease characteristics (e.g., depth of cancer invasion) can sometimes alter diagnosis and treatment, collecting pairs with and without these characteristics is cumbersome. another major challenge is longitudinal tracking of pathological progression over time (e.g., from the early stage of cancer to the advanced stage), which is difficult to understand because medical images are often snapshots. privacy is also a concern since images of educational materials are widely distributed. therefore, medical image editing that allows users to generate their intended disease characteristics is useful for precise medical education  image editing can synthesize low-or high-level image contents  several types of image editing techniques for medical imaging have been introduced, mainly using generative adversarial networks [5] and, more recently, diffusion models  here, we propose a novel framework for image editing called u3-net that allows the generation of anatomical elements with precise conditions. the core technique is self-supervised segmentation, which aims to achieve pixel-wise clustering without manually annotated labels "
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,4.0,Conclusion,"in this study, we propose a medical image-editing framework to edit fine-grained anatomical elements. the self-supervised segmentation extracted low-to midlevel content of medical images, which corresponded well to the clinically meaningful substructures of organs and diseases. the majority of the edited images with intended characteristics were perceived as natural medical images by several expert physicians. our medical image editing method can be applied to medical education, which has been overlooked as an application of ai. future challenges include improving scalability with fewer manual operations, validating segmentation maps from a more objective perspective, and comparing our proposed algorithm with existing methods, such as those based on superpixels  data use declaration and acknowledgment: the pelvic mri and chest ct datasets were collected from the national cancer center hospital. the study, data use, and data protection procedures were approved by the ethics committee of the national cancer center, tokyo, japan (protocol number 2016-496). our implementation and all synthesized images will be available here: https:// github.com/kaz-k/medical-image-editing."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.0,Experimental Results,"impact of selection strategies. in fig.  our results explain why random selection remains a strong competitor for 3d segmentation tasks in cold-start scenarios, as no strategy evaluated in our benchmark consistently outperforms the random selection average performance. however, we observe that typiclust (shown as orange) achieves comparable or superior performance compared to random selection across all tasks in our benchmark, whereas other approaches can significantly under-perform on certain tasks, especially challenging ones like the liver dataset. hence, typi-clust stands out as a more robust cold-start selection strategy, which can achieve at least a comparable (sometimes better) performance against the mean of random selection. we further note that typiclust largely mitigates the risk of 'unlucky' random selection as it consistently performs better than the low-performing random samples (red dots below the dashed line). impact of different budgets. in fig.  impact of different rois. in fig.  limitations. for the segmentation tasks that include tumors (  th columns on fig. "
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,,MICCAI FeTS2021,"training data. the real-world dataset used in experiments is provided by the fets challenge organizer, which is the training set of the whole dataset about brain tumor segmentation. in order to evaluate the performance of fedgrav, we partition the dataset composed of 341 data samples "
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,3.2,Results,"experiment results on the cifar-10. we first validate the proposed method on the cifar-10 dataset. table  experiment results on miccai fets2021 training dataset. in order to verify the robustness of our method and its performance in real-world data, we conduct the experiment on the miccai fets2021 training dataset. we evaluate the performance of our algorithm by comparing six indicators: the dice similarity coefficient(dsc) and hausdorff distance-95th percentile(hd95) of whole tumor(wt), enhancing tumor(et), and tumor core(tc). as is shown in table  the visualization results are shown in fig. "
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,4.0,Conclusion,"in this paper, we introduced fedgrav, a novel aggregation strategy inspired by the law of universal gravitation in physics. fedgrav improves local model aggregation by considering both the differences in sample size and discrepancies among local models. it can adaptively adjust the aggregation weights and explore the internal correlations of local models more effectively. we evaluated our method on cifar-10 and real-world miccai federated tumor segmentation challenge (fets) datasets, and the superior results demonstrated the effectiveness and robustness of our fedgrav."
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,1.0,Introduction,"magnetic resonance imaging (mri) of the brain is an essential imaging modality to accurately diagnose various neurological diseases ranging from inflammatory t. pinetz and a. effland-are funded the german research foundation under germany's excellence strategy -exc-2047/1 -390685813 and -exc2151 -390873048 and r. haase is funded by a research grant (bonfor; o-194.0002.1). t. pinetz and e. kobler-contributed equally to this work. lesions to brain tumors and metastases. for accurate depictions of said pathologies, gadolinium-based contrast agents (gbca) are injected intravenously to highlight brain-blood barrier dysfunctions. however, these contrast agents are expensive and may cause nephrogenic systemic fibrosis in patients with severely reduced kidney function  driven by this recommendation, several research groups have recently published dose-reduction techniques focusing on maintaining image quality. complementary to the development of higher relaxivity contrast agents  in recent years, generative models have been used to overcome data scarcity in the computer vision and medical imaging community. frequently, generative adversarial networks (gans)  learning conditional distributions between images can be accomplished by additionally feeding a condition (additional scans, dose level, etc.) into both the generator and discriminator. in particular, for image-to-image translation tasks, these conditional gans have been successfully applied using paired  with this in mind, the contributions of this work are as follows: -synthesis of gbca behavior at various doses using conditional gans, -loss enabling interpolation of dose levels present in training data, -noise-preserving content loss function to generate realistic synthetic images."
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,1.0,Introduction,"ultrasound imaging is a very effective technique for breast lesion diagnosis, which has high sensitivity. automatically detecting breast lesions is a challenging problem with a potential to aid in improving the efficiency of radiologists in ultrasound-based breast cancer diagnosis  most existing breast lesion detection methods can be categorized into imagebased  to address the aforementioned issues, we propose a spatial-temporal deformable attention based network, named stnet, for detecting the breast lesions in ultrasound videos. within our stnet, we introduce a spatial-temporal deformable attention module to fuse multi-scale spatial-temporal information among different frames, and further integrate it into each layer of the encoder and decoder. in this way, different from the recent cva-net, our proposed stnet performs both deep and local feature fusion. in addition, we introduce multiframe prediction with encoder feature shuffle operation that shares the backbone and encoder features, and only perform multi-frame prediction in the decoder. this enables us to significantly accelerate the detection speed of the proposed approach. we conduct extensive experiments on a public breast lesion ultrasound video dataset, named bluvd-186 "
DeDA: Deep Directed Accumulator,3.3,Discussions,"medical images often require processing of a primary target or region of interest (roi), such as rims, left ventricles, or tumors. these rois frequently exhibit distinct geometric structures  while the study focuses on rim+ lesion identification, the proposed deda can be extended to other applications. these include the utilization of polar transformation for skin lesion recognition/segmentation, symmetric circular transformation for cardiac image registration "
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,1.0,Introduction,"deep learning techniques have achieved unprecedented success in the field of medical image classification, but this is largely due to large amount of annotated data  active learning (al) is an effective approach to address this issue from a data selection perspective, which selects the most informative samples from an unlabeled sample pool for experts to label and improves the performance of the trained model with reduced labeling cost  recently, ning et al.  in this paper, we propose a novel al framework under an open-set scenario, and denote it as openal, which cannot only query as many target class samples as possible but also query the most informative samples from the target classes. openal adopts an iterative query paradigm and uses a two-stage sample selection strategy in each query. in the first stage, we do not rely on a detection network to select target class samples and instead, we propose a feature-based target sample selection strategy. specifically, we first train a feature extractor using all samples in a self-supervised learning manner, and map all samples to the feature space. there are three types of samples in the feature space, the unlabeled samples, the target class samples labeled in previous iterations, and the non-target class samples queried in previous iterations but not being labeled. then we select the unlabeled samples that are close to the target class samples and far from the non-target class samples to form a candidate set. in the second stage, we select the most informative samples from the candidate set by utilizing a model-based informative sample selection strategy. in this stage, we measure the uncertainty of all unlabeled samples in the candidate set using the classifier trained with the target class samples labeled in previous iterations, and select the samples with the highest model uncertainty as the final selected samples in this round of query. after the second stage, the queried samples are sent for annotation, which includes distinguishing target and non-target class samples and giving a fine-grained label to every target class sample. after that, we train the classifier again using all the fine-grained labeled target class samples. we conducted two experiments with different matching ratios (ratio of the number of target class samples to the total number of samples) on a public 9-class colorectal cancer pathology image dataset. the experimental results demonstrate that openal can significantly improve the query quality of target class samples and obtain higher performance with equivalent labeling cost compared with the current state-of-the-art al methods. to the best of our knowledge, this is the first open-set al work in the field of pathology image analysis."
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,3.1,"Dataset, Settings, Metrics and Competitors","to validate the effectiveness of openal, we conducted two experiments with different matching ratios (the ratio of the number of samples in the target class to the total number of samples) on a 9-class public colorectal cancer pathology image classification dataset (nct-crc-he-100k)  metrics. following  as defined in eq. 5, precision is the proportion of the target class samples among the total samples queried in each query and recall is the ratio of the number of the queried target class samples to the number of all the target class samples in the unlabeled sample pool. where k m denotes the number of target class samples queried in the mth query, l m denotes the number of non-target class samples queried in the mth query, and n target denotes the number of target class samples in the original unlabeled sample pool. obviously, the higher the precision and recall are, the more target class samples are queried, and the more effective the trained target class classifier will be. we measure the final performance of each al method using the accuracy of the final classifier on the test set of target class samples. competitors. we compare the proposed openal to random sampling and five al methods, lfosa "
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,,GFF.,"in the experiments on brain tumor segmentation, we compare sfusion with a gated feature fusion block (gff)  our implementations are on an nvidia rtx 3090(24g) with pytorch 1.8.1."
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,3.3,Results,"activity recognition. we compare sfusion with the embracenet  brain tumor segmentation. the quantitative segmentation results are shown in table  ablation experiments. the correlation extraction (ce) module and the modal attention (ma) module are two key components in sfusion. we evaluate the sfusion without ce and ma, respectively. sfusion without ce denotes that feature representations are directly fed into the ma module (fig. "
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,4.0,Conclusion,"in this paper, we propose a self-attention based n-to-one fusion block sfusion to tackle the problem of multimodal missing modalities fusion. as a data-dependent fusion strategy, sfusion can automatically learn the latent correlations between different modalities and builds a shared feature representation. the entire fusion process is based on available data without simulating missing modalities. in addition, sfusion has compatibility with any kind of upstream processing model and downstream decision model, making it universally applicable to different tasks. we show that it can be integrated into existing backbone networks by replacing their fusion operation or block to improve activity recognition and achieve brain tumor segmentation performance. in particular, by integrating with sfusion, sf_fdgf achieves the state-of-the-art performance. in the future, we will explore other tasks related to variable multimodal fusion with sfusion."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,1.0,Introduction,"skin cancer is one of the most common cancers all over the world. serious skin diseases such as melanoma can be life-threatening, making early detection and treatment essential  accuracy and robustness requirements in applications, which is hard to suffice due to the long-tailed occurrence of diseases in the real-world. long-tailed problem is usually caused by differences in incidence rate and difficulties in data collection. some diseases are common while others are rare, making it difficult to collect balanced data  to tackle the challenge of learning unbiased classifiers with imbalanced data, many previous works focus on three main ideas, including re-sampling data  recently, contrastive learning (cl) methods pose great potential for representation learning when trained on imbalanced data  to address the above issues, we propose a class-enhancement contrastive learning (ecl) method for skin lesion classification, differences between scl and ecl are illustrated in fig.  our contributions can be summarized as follows: "
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,3.1,Experimental Settings,"datasets and pre-trained model. we conducted experiments on automating liver tumor segmentation in contrast-enhanced ct scans, a crucial task in liver cancer diagnosis and surgical planning  collecting large-scale data from our hospital and training a new model will be expensive. therefore, we can use the model trained from them as a starting point and use slpt to adapt it to our hospital with minimum cost. we collected a dataset from our in-house hospital comprising 941 ct scans with eight categories: hepatocellular carcinoma, cholangioma, metastasis, hepatoblastoma, hemangioma, focal nodular hyperplasia, cyst, and others. it covers both major and rare tumor types. our objective is to segment all types of lesions accurately. we utilized a pre-trained model for liver segmentation using supervised learning on two public datasets  metrics. we evaluated lesion segmentation performance using pixel-wise and lesion-wise metrics. for pixel-wise evaluation, we used the dice per case, a commonly used metric  training setup. we conducted the experiments using the pytorch framework on a single nvidia tesla v100 gpu. the nnunet "
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,1.0,Introduction,"recent years have witnessed the remarkable success of deep learning in medical image segmentation. however, although the performance of deep learning models even surpasses the accuracy of human exports on some segmentation tasks, two challenges still persist. (1) different segmentation tasks are usually tackled separately by specialized networks (see fig.  several strategies have been attempted to address both challenges. first, multi-head networks (see fig.  in this paper, we propose a prompt-driven universal segmentation model (uniseg) to segment multiple organs, tumors, and vertebrae on 3d medical images with diverse modalities and domains. uniseg contains a vision encoder, a fusion and selection (fuse) module, and a prompt-driven decoder. the fuse module is devised to generate the task-specific prompt, which enables the model to be 'aware' of the ongoing task (see fig.  our contributions are three-fold: "
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,4.0,Conclusion,"this study proposes a universal model called uniseg (a single model) to perform multiple organs, tumors, and vertebrae segmentation on images with multiple modalities and domains. to solve two limitations existing in preview universal models, we design the universal prompt to describe correlations among all tasks and make the model 'aware' of the ongoing task early, boosting the training of the whole decoder instead of just the last few layers. thanks to both designs, our uniseg achieves superior performance on 11 upstream datasets and two downstream datasets, setting a new record. in our future work, we plan to design a universal model that can effectively process multiple dimensional data."
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,1.0,Introduction,"histopathological image analysis is an important step towards cancer diagnosis. however, shortage of pathologists worldwide along with the complexity of histopathological data make this task time consuming and challenging. therefore, developing automatic and accurate histopathological image analysis methods that leverage recent progress in deep learning has received significant attention in recent years. in this work, we investigate the problem of diagnosing colorectal cancer, which is one of the most common reason for cancer deaths around the world and particularly in europe and america  existing deep learning-based colorectal tissue classification methods  while generative adversarial networks (gans) "
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,2.0,Related Work,"the ability of generative models  our approach: while the aforementioned works explore fs generation in natural images, to the best of our knowledge, we are the first to investigate fs generation in colorectal tissue images. in this work, we look into multi-class colorectal tissue analysis problem, with low and high-grade tumors included in the set. the corresponding dataset "
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,4.0,Experiments,we conduct experiments on human colorectal cancer dataset 
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,1.0,Introduction,"instances with complex shapes arise in many biomedical domains, and their morphology carries critical information. for example, the structure of gland tissues in microscopy images is essential in accessing the pathological stages for cancer diagnosis and treatment. these instances, however, are usually closely in touch with each other and have non-convex structures with parts of varying widths (fig.  in the biomedical domain, most methods  to preserve the connectivity of instances while keeping the precise instance boundary, in this paper, we propose a novel representation named skeleton-aware distance transform (sdt). our sdt incorporate object skeleton, a concise and connectivity-preserving representation of object structure, into the traditional boundary-based distance transform (dt) (fig. "
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,3.1,Histopathology Instance Segmentation,"accurate instance segmentation of gland tissues in histopathology images is essential for clinical analysis, especially cancer diagnosis. the diversity of object appearance, size, and shape makes the task challenging. dataset and evaluation metric. we use the gland segmentation challenge dataset "
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,4.3,KiTS'19 Experiments,"to verify the effectiveness and generalization of our searched architectures from dast, we validate the searched architecture (from pancreas data set) on this challenging task. metrics for kidneys and tumors are the average dice score per case. finally, we evaluate our single-fold model as well as the ensemble from 5 cross-validation models on the public test leaderboard table "
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,1.0,Introduction,"computed tomography (ct) and magnetic resonance (mr) are two widely used imaging techniques in clinical practice. ct imaging uses x-rays to produce detailed, cross-sectional images of the body, which is particularly useful for imaging bones and detecting certain types of cancers with fast imaging speed. however, ct imaging has relatively high radiation doses that can pose a risk of radiation exposure to patients. low-dose ct techniques have been developed to address this concern by using lower doses of radiation, but the image quality is degraded with increased noise, which may compromise diagnostic accuracy  mr imaging, on the other hand, uses a strong magnetic field and radio waves to create detailed images of the body's internal structures, which can produce high-contrast images for soft tissues and does not involve ionizing radiation. this makes mr imaging safer for patients, particularly for those who require frequent or repeated scans. however, mr imaging typically has a lower resolution than ct  motivated by the aforementioned, there is a pressing need to improve the quality of low-dose ct images and low-resolution mr images to ensure that they provide the necessary diagnostic information. numerous algorithms have been developed for ct and mr image enhancement, with deep learning-based methods emerging as a prominent trend  these algorithms are capable of improving image quality, but they have two significant limitations. first, paired images are required for training, e.g., low-dose and full-dose ct images; low-resolution and high-resolution mr images). however, acquiring such paired data is challenging in real clinical scenarios. although it is possible to simulate low-quality images from high-quality images, the models derived from such data may have limited generalization ability when applied to real data  recently, pre-trained diffusion models  in this paper, we aim at addressing the limitations of existing image enhancement methods and the scarcity of pre-trained diffusion models for medical images. specifically, we provide two well-trained diffusion models on full-dose ct images and high-resolution heart mr images, suitable for a range of applications including image generation, denoising, and super-resolution. motivated by the existing plug-and-play image restoration methods "
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,1.0,Introduction,"detecting out-of-distribution (ood) samples is crucial in real-world applications of machine learning, especially in medical imaging analysis where misdiagnosis can pose significant risks  deep learning-based ood detection methods with uncertainty estimation, such as evidential deep learning (edl)  to address this limitation, we propose an evidence reconciled neural network (ernn), which aims to reliably detect those samples that are similar to the training data but still with different distributions (near ood), while maintain accuracy for in-distribution (id) classification. concretely, we introduce a module named evidence reconcile block (erb) based on evidence offset. this module cancels out the conflict evidences obtained from the evidential head, maximizes the uncertainty of derived opinions, thus minimizes the error of uncertainty calibration in ood detection. with the proposed method, the decision boundary of the model is restricted, the capability of medical outlier detection is improved and the risk of misdiagnosis in medical images is mitigated. extensive experiments on both isic2019 dataset and in-house pancreas tumor dataset demonstrate that the proposed ernn significantly improves the reliability and accuracy of ood detection for clinical applications. code for ernn can be found at https://github.com/kelladoe/ernn."
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,3.3,Ablation Study,"in this section, we conduct a detailed ablation study to clearly demonstrate the effectiveness of our major technical components, which consist of evaluation of evidential head, evaluation of the proposed evidence reconcile block on both isic 2019 dataset and our in-house pancreas tumor dataset. since the evidence reconcile block is based on the evidential head, thus there are four combinations, but only three experimental results were obtained. as shown in table "
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,1.0,Introduction,"lung cancer is the main cause of cancer death worldwide  benign or malignant tumors  segmenting nodules is a tedious task that requires significant human labor. computer aided diagnosis (cad) systems can significantly reduce such heavy workloads. the accuracy of the existing nodule detection model reaches 96.1%  several studies have proposed solutions to tackle the large scale span challenges at both the input and feature level. for instance, some approaches adopt multi-scale inputs  recently, some click-based lesion segmentation methods  in this paper, we propose a scale-aware test-time click adaptation (sattca) method, which simply utilizes easily obtainable lesion click (i.e., the center detected nodule) to adjust the parameters of the network normalization layers "
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,1.0,Introduction,"colorectal cancer (crc) has become a major threat to health worldwide. since most crcs originate from colorectal polyps, early screening for polyps is necessary. given its significance, automatic polyp segmentation models  all above models are fully supervised and require pixel-level annotations. however, pixel-by-pixel labeling is time-consuming and expensive, which hampers practical clinical usage. besides, many polyps do not have welldefined boundaries. pixel-level labeling inevitably introduces subjective noise. to address the above limitations, a generalized polyp segmentation model is urgently needed. in this paper, we achieve this goal by a weakly supervised polyp segmentation model (named weakpolyp) that only uses coarse bounding box annotations. figure  however, bounding box annotations are much coarser than pixel-level ones, which can not describe the shape of polyps. simply adopting these box annotations as supervision introduces too much background noise, thereby leading to suboptimal models. as a solution, boxpolyp  weakpolyp is mainly enabled by two novel components: mask-to-box (m2b) transformation and scale consistency (sc) loss. in practice, m2b is applied to transform the predicted mask into a box-like mask by projection and backprojection. then, this transformed mask is supervised by the bounding box annotation. this indirect supervision avoids the misleading of box-shape bias of annotations. however, many regions in the predicted mask are lost in the projection and therefore get no supervision. to fully explore these regions, we propose the sc loss to provide a pixel-level self-supervision while requiring no annotations at all. specifically, the sc loss explicitly reduces the distance between predictions of the same image at different scales. by forcing feature alignment, it inhibits the excessive diversity of predictions, thus improving the model generalization. in summary, our contributions are three-fold: "
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,1.0,Introduction,"axillary lymph node (aln) metastasis is a severe complication of cancer that can have devastating consequences, including significant morbidity and mortality. early detection and timely treatment are crucial for improving outcomes and reducing the risk of recurrence. in breast cancer diagnosis, accurately segmenting breast lesions in ultrasound (us) videos is an essential step for computer-aided diagnosis systems, as well as breast cancer diagnosis and treatment. however, this task is challenging due to several factors, including blurry lesion boundaries, inhomogeneous distributions, diverse motion patterns, and dynamic changes in lesion sizes over time  although the existing benchmark method dpstt "
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,1.0,Introduction,"prostate cancer is a leading cause of cancer-related deaths in adult males, as reported in studies, such as  due to the relatively low spatial resolution and soft tissue contrast in ct images compared to mri images, manual prostate segmentation in ct images can be time-consuming and may result in significant variations between operators  in spite of the improved performance for the aforementioned vit-based networks, these methods utilize the standard or shifted-window-based sa, which is the fine-grained local sa and may overlook the local and global interactions  recently, focal transformer "
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,5.0,Discussion and Conclusion,"the results reported before reveal that approaching the problem of segmentation uncertainty prediction via a regression task, where the uncertainty is expressed in terms of landmark location, is globally better than via pixel-based segmentation methods. it also shows that our method (n 1 , n 2 and sn 2 ) is better than the commonly-used mc-dropout. it can also be said that our method is more interpretable as is detailed in sect. 2.2 and shown in fig.  the choice of distribution has an impact when considering the shape of the predicted contour. for instance, structures such as the left ventricle and the myocardium wall in the ultrasound datasets have large components of their contour oriented along the vertical direction which allows the univariate and bivariate models to perform as well, if not better, than the asymmetric model. however, the lungs and heart in chest x-rays have contours in more directions and therefore the uncertainty is better modeled with the asymmetric model. furthermore, it has been demonstrated that skewed uncertainty is more prevalent when tissue separation is clear, for instance, along the septum border (camus) and along the lung contours (jsrt). the contrast between the left ventricle and myocardium in the images of the private cardiac us dataset is small, which explains why the simpler univariate and bivariate models perform well. this is why on very noisy and poorly contrasted data, the univariate or the bivariate model might be preferable to using the asymmetric model. while our method works well on the tasks presented, it is worth noting that it may not be applicable to all segmentation problems like tumour segmentation. nevertheless, our approach is broad enough to cover many applications, especially related to segmentation that is later used for downstream tasks such as clinical metric estimation. future work will look to expand this method to more general distributions, including bi-modal distributions, and combine the aleatoric and epistemic uncertainty to obtain the full predictive uncertainty."
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,1.0,Introduction,"primary liver cancer is one of the most common and deadly cancer diseases in the world, and liver resection is a highly effective treatment  however, automatic and accurate couinaud segmentation from ct images is a challenging task. since it is defined based on the anatomical structure of live vessels, even no intensity contrast (fig.  in this paper, to tackle the aforementioned challenges, we propose a pointvoxel fusion framework that represents the liver ct in continuous points to better learn the spatial structure, while performing the convolutions in voxels to obtain the complementary semantic information of the couinaud segments. specifically, the liver mask and vessel attention maps are first extracted from the ct images, which allows us to randomly sample points embedded with vessel structure prior in the liver space and voxelize them into a voxel grid. subsequently, points and voxels pass through two branches to extract features. the point-based branch extracts the fine-grained feature of independent points and explores spatial topological relations. the voxel-based branch is composed of a series of convolutions to learn semantic features, followed by de-voxelization to convert them back to points. through the operation of voxelization and devoxelization at different resolutions, the features extracted by these two branches can achieve multi-scale fusion on point-based representation, and finally output the couinaud segment category of each point. extensive experiments on two publicly available datasets named 3dircadb "
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,1.0,Introduction,"segmentation of the pulmonary vessels is the foundation for the clinical diagnosis of pulmonary vascular diseases such as pulmonary embolism (pe), pulmonary hypertension (ph) and lung cancer  in the literature, several conventional methods  to summarize, there exist several challenges for pulmonary vessel segmentation in non-contrast ct images:  to address the above challenges, we propose a h ierarchical e nhancement n etwork (henet) for pulmonary vessel segmentation in non-contrast ct images by enhancing the representation of vessels at both image-and feature-level. for the input ct images, we propose an auto contrast enhancement (ace) module to automatically adjust the range of hu values in different areas of ct images. it mimics the radiologist in setting the window level (wl) and window width (ww) to better enhance vessels from surrounding voxels, as shown in fig. "
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.4,Results on QUBIQ,"in table  in the literature  generally, models trained with soft labels exhibit improved accuracy and calibration. in particular, averaging annotations with uniform weights obtains the highest bdice, while a weighted average achieves the highest dice score. it is worth noting that the weighted average significantly outperforms the majority votes in terms of the dice score which is evaluated based on the majority votes themselves. we hypothesize that this is because soft labels contain extra interrater information, which can ease the network optimization at those ambiguous regions. overall, we find the weighted average outperforms other methods, with the exception of brain tumor t2, where there is a high degree of disagreement among raters. we compare our method with state-of-the-art (sota) methods using unet-resnet50 in table "
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,5.0,Conclusion,"we present a novel data augmentation method for semantic segmentation using a flexible anti-adversarial consistency regularization. in particular, our method is tailored for medical images that contain small and underrepresented key objects such as a polyp and tumor. with object-level perturbations, our method effectively expands discriminative regions on challenging samples while preserving the morphological characteristics of key objects. extensive experiments with various backbones and datasets confirm the effectiveness of our method."
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,1.0,Introduction,head and neck (han) cancer is a prevalent type of cancer 
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,5.0,Conclusions,"in this study, we introduced mfm, a fusion module that aligns fms from an auxiliary modality (e.g. mr) to fms from the primary modality (e.g. ct). the proposed mfm is versatile, as it can be applied to any multimodal segmentation network. however, it has to be noted that it is not symmetrical, and therefore requires the user to specify the primary modality, which is typically the same as the primary modality used in manual delineation (i.e. in our case ct). we evaluated the performance of mfm combined with the nnu-net backbone for segmentation of oars in the han region, an important task in rt cancer treatment planning. the obtained results indicate that the performance of mfm is similar to other state-of-the-art methods, but it outperforms other multimodal methods in scenarios with one missing modality."
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,1.0,Introduction,"integrating multi-modality medical images for tumor segmentation is crucial for comprehensive diagnosis and surgical planning. in the clinic, the consistent information and complementary information in multi-modality medical images provide the basis for tumor diagnosis. for instance, the consistent anatomical structure information offers the location feature for tumor tracking  existing methods for multi-modality medical image integration can be categorized into three groups:  dempster-shafer theory (dst)  in this paper, we propose an evidence-identified ddpm (ei-ddpm) with contextual discounting for tumor segmentation via integrating multi-modality medical images. our basic assumption is that we can learn the segmentation feature on single modality medical images using ddpm and parse the reliability of different modalities medical images by evidence theory with a contextual discounting mechanism. specifically, the ei-ddpm first utilizes parallel conditional ddpm to learn the segmentation feature from a single modality image. next, the evidence-identified layer (eil) preliminarily integrates multi-modality images by comprehensively using the multi-modality uncertain information. lastly, the contextual discounting operator (cdo) performs the final integration of multimodality images by parsing the reliability of information from multi-modality medical images. the contributions of this work are: -our ei-ddpm achieves tumor segmentation by using ddpm under the guidance of evidence theory. it provides a solution to integrate multi-modality medical images when deploying the ddpm algorithm. -the proposed eil and cdo apply contextual discounting guided dst to parse the reliability of information from different modalities of medical images. this allows for the integration of multi-modality medical images with learned weights corresponding to their reliability. -we conducted extensive experiments using the brats 2021 "
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,2.0,Method,"the ei-ddpm achieves tumor segmentation by parsing the reliability of multimodality medical images. specifically, as shown in fig. "
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,4.0,Conclusion,"in this paper, we proposed a novel ddpm-based framework for tumor segmentation under the condition of multi-modality medical images. the eil and cdo enable our ei-ddpm to capture the reliability of different modality medical images with respect to different tumor regions. it provides a way of deploying contextual discounted dst to parse the reliability of multi-modality medical images. extensive experiments prove the superiority of ei-ddpm for tumor segmentation on multi-modality medical images, which has great potential to aid in clinical diagnosis. the weakness of ei-ddpm is that it takes around 13 s to predict one segmentation image. in future work, we will focus on improving sampling steps in parallel ddpm paths to speed up ei-ddpm."
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,1.0,Introduction,"breast cancer is the leading cause of cancer-related fatalities among women. currently, it holds the highest incidence rate of cancer among women in the u.s., and in 2022 it accounted for 31% of all newly diagnosed cancer cases  in the past decade, deep learning-based approaches achieved remarkable advancements in bus tumor classification  vision transformer (vit)  accordingly, numerous prior studies introduced modifications to the original vit network specifically designed for bus image classification  multitask learning leverages shared information across related tasks by jointly training the model. it constrains models to learn representations that are relevant to all tasks rather than learning task-specific details. moreover, multitask learning acts as a regularizer by introducing inductive bias and prevents overfitting  in this study, we introduce a hybrid multitask approach, hybrid-mt-estan, which encompasses tumor classification as a primary task and tumor segmentation as a secondary task. hybrid-mt-estan combines the advantages of cnns and transformers in a framework incorporating anatomical tissue information in bus images. specifically, we designed a novel attention block named anatomy-aware attention (aaa), which modifies the attention block of swin transformer by considering the breast anatomy. the anatomy of the human breast is categorized into four primary layers: the skin, premammary (subcutaneous fat), mammary, and retromammary layers, where each layer has a distinct texture and generates different echo patterns. the primary layers in bus images are arranged in a vertical stack, with similar echo patterns appearing horizontally across the images. the kernels in the introduced aaa attention blocks are organized in rows and columns to capture the anatomical structure of the breast tissue. in the published literature, the closest approach to ours is the work by iqbal et al.  where f l and f l are the output features of the mlp module and the (s)w-msa module for block l, respectively; in the proposed anatomy-aware attention (aaa) block, we redesigned the swin blocks to enhance their ability to model both global and local features by adding an attention block based on the breast anatomy (see fig.  concretely, we first reconstruct the i-th feature map (y i ) by merging (m ) all patches, and afterward, we applied average pooling (avg-p) and max pooling (max-p) layers with size (2, 2). the outputs of (avg-p) and (max-p) layers are concatenated and up-sampled (u ) with size (2, 2) and stride (2, 2). rowcolumn-wise kernels (a) with size (9 , 1) and "
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,2.4,Loss Function,"we applied a multitask loss function (l mt ) that aggregates two terms: a focal loss l f ocal for the classification task and dice loss l dice for the segmentation task. therefore, the composite loss function is l mt = w 1 • l f ocal + l dice , where the weight coefficient w 1 is set to apply greater importance to the classification task as the primary task. since in medical image diagnosis achieving high sensitivity places emphasis on the detection of malignant lesions, we employed the focal loss for the classification task to trade off between sensitivity and specificity. because malignant tumors are more challenging to detect due to greater differences in margin, shape, and appearance in bus images, focal loss forces the model to focus more on difficult predictions. specifically, focal loss adds a factor (1 -p i ) γ to the cross-entropy loss where γ is a focusing parameter, resulting in in the formulation, α is a weighting coefficient, n denotes the number of image samples, t i is the target label of the i th training sample, and p i denotes the prediction. the segmentation loss is calculated using the commonly-employed dice loss (l dice ) function."
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,4.0,Conclusion,"in this paper, we introduced the hybrid-mt-estan, a multitask learning approach for bus image analysis that alleviates the lack of global contextual infor-mation in the low-level layers of cnn-based approaches. hybrid-mt-estan concurrently performs bus tumor classification and segmentation, with a hybrid architecture that employs cnn-based and swin transformer layers. the proposed approach exploits multi-scale local patterns and global long-range dependencies provided by mt-esta and aaa transformer blocks for learning feature representations, resulting in improved generalization. experimental validation demonstrated significant performance improvement by hybrid-mt-estan in comparison to current state-of-the-art models for bus classification."
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,1.0,Introduction,"breast cancer is the most common cause of cancer-related deaths among women all around the world  to better support the radiologists with breast cancer diagnosis, various segmentation algorithms have been developed  although  in this study, we propose a simple yet effective weakly-supervised strategy, by using extreme points as annotations (see fig. "
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,3.0,Experiments,"dataset. we evaluated our method on an in-house breast dce-mri dataset collected from the cancer center of sun yat-sen university. in total, we collected 206 dce-mri scans with biopsy-proven breast cancers. all mri scans were examined with 1.5t mri scanner. the dce-mri sequences (tr/te = 4.43 ms/1.50 ms, and flip angle = 10 • ) using gadolinium-based contrast agent were performed with the t1-weighted gradient echo technique, and injected 0.2 ml/kg intravenously at 2.0 ml/s followed by 20 ml saline. the dce-mri volumes have two kinds of resolution, 0.379×0.379×1.700 mm 3 and 0.511×0.511×1.000 mm 3 . all cancerous regions and extreme points were manually annotated by an experienced radiologist via itk-snap  implementation details. the framework was implemented in pytorch, using a nvidia geforce gtx 1080 ti with 11gb of memory. we employed 3d unet  • train: the network was trained by stochastic gradient descent (sgd) for 200 epochs, with an initial learning rate η = 0.01. the ploy learning policy was used to adjust the learning rate, (1epoch/200) 0.9 . the batch size was 2, consisting of a random foreground patch and a random background patch located via initial segmentation y init . such setting can help alleviate class imbalance issue. the patch size was 128 × 128 × 96. for the contrastive loss, we set n = 100, temperature parameter τ = 0.1. • fine-tune: we initialized the network with the trained weights. we trained it by sgd for 100 iterations, with η = 0.0001. the ploy learning policy was also used. for the simple strategy, we set n = 100, λ = 0.96, α = 0.96, w = 0.1. quantitative and qualitative analysis. we first verified the efficacy of our simple in the training stage. figure "
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,4.0,Conclusion,we introduce a simple yet effective weakly-supervised learning method for breast cancer segmentation in dce-mri. the primary attribute is to fully exploit the simple trainfine-tuneretrain process to optimize the segmentation network via only extreme point annotations. this is achieved by employing a similarityaware propagation learning (simple) strategy to update the pseudo-masks. experimental results demonstrate the efficacy of the proposed simple strategy for weakly-supervised segmentation.
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,1.0,Introduction,"medical image segmentation is always a critical task as it can be used for disease diagnosis, treatment planning, and anomaly monitoring. weakly supervised semantic segmentation attracts significant attention from medical image community since it greatly reduces the cost of dense pixel-wise labeling to get segmentation mask. in wsss, the training labels are usually easier and faster to obtain, like image-level tags, bounding boxes, scribbles, or point annotations. this work only focuses on wsss with image-level tags, like whether a tumor presents or not. in this field, previous wsss works  meanwhile, denoising diffusion models "
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,3.0,Experiments and Results,"brain tumor segmentation. brats (brain tumor segmentation challenge)  only classification labels are used during training the diffusion models, and segmentation masks are used for evaluation in the test stage. for both datasets, we repeat the evaluation protocols for four times and report the average metrics and their standard deviation on test set."
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,3.2,Results,"comparison with state of the arts. we benchmark our methods against previous wsss works on two datasets in table  from the results, we can make several key observations. firstly, our proposed method, even without classifier guidance, outperform all other wsss methods including the classifier guided diffusion model cg-diff on both datasets for all three metrics. when classifier guidance is provided, the improvement gets even bigger, and cg-cdm can beat other methods regarding segmentation accuracy. secondly, all wsss methods have performance drop on kidney dataset compared with brats dataset. this demonstrates that the kidney segmentation task is a more challenging task for wsss than brain tumor task, which may be caused by the small training size and diverse appearance across slices in the chaos dataset. time efficiency. regarding inference time for different methods, as shown in table  ablation studies. there are several important hyperparameters in our framework, noise level q, number of iterations r, moving weight τ , and gradient scale s. the default setting is cg-cdm on brats dataset with q = 400, r = 10, τ = 0.95, and s = 10. we evaluate the influence of one hyperparameter at a time by keeping other parameters at their default values. as illustrated in fig. "
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,1.0,Introduction,"glioma is one of the most common malignant brain tumors with varying degrees of invasiveness  with the rise of deep learning, researchers have begun to study deep learning-based image analysis methods  in recent years, models based on the self-attention mechanism, such as transformer, have received widespread attention due to their excellent performance in natural language processing (nlp)  while transformer-based models have shown effectiveness in capturing long-range dependencies, designing a transformer architecture that performs well on the samm-bts task remains challenging. first, modeling relationships between 3d voxel sequences is much more difficult than 2d pixel sequences. when applying 2d models, 3d images need to be sliced along one dimension. however, the data in each slice is related to three views, discarding any of them may lead to the loss of local information, which may cause the degradation of performance  the contributions of our proposed method can be described as follows: 1) based on transformer, we construct dual-branch encoder and decoder layers that assemble two attention mechanisms, being able to model close-window and distant-window dependencies without any extra computational cost. 2) in addition to the traditional skipconnection structure, in the dual-branch decoder, we also establish an extra path to facilitate the decoding process. we design a shifted-w-mca-based global branch to build a bridge between the decoder and encoder, maintaining affluent information of the segmentation target during the decoding process. 3) for the multi-modal data adopt in the task of samm-bts, we improve the channel attention mechanism in se-net by applying se-weights to features from both branches in the encoder and decoder layers. by this means, we implicitly consider the importance of multiple mri modalities and two window-based attention branches, thereby strengthening the fusion effect of the multi-modal information from a global perspective."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,3.0,Experiments and Results,"datasets. we use the multimodal brain tumor segmentation challenge (brats 2021  comparative experiments. to evaluate the effectiveness of the proposed dbtrans, we compare it with the state-of-the-art brain tumor segmentation methods including six transformer-based networks swin-unet "
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,2.1,M-GenSeg: Semi-supervised Segmentation,"in order to disentangle the information common to a and p, and the information specific to p, we split the latent representation of each image into a common code c and a unique code u. essentially, the common code contains information inherent to both domains, which represents organs and other structures, while the unique code stores features like tumor shapes and location. in the two fol-lowing paragraphs, we explain p→a and a→p translations for source images. the same process is applied for target images by replacing s notation with t . presence to absence translation. given an image s p of modality s in the presence domain p, we use an encoder e s to compute the latent representation [c s p , u s p ]. a common decoder g s com takes as input the common code c s p and generates a healthy version s pa of that image by removing the apparent tumor region. simultaneously, both common and unique codes are used by a residual decoder g s res to output a residual image δ s pp , which corresponds to the additive change necessary to shift the generated healthy image back to the presence domain. in other words, the residual is the disentangled tumor that can be added to the generated healthy image to create a reconstruction s pp of the initial diseased image:  like approaches in  modality translation. our objective is to learn to segment tumor lesions in a target modality by reusing potentially scarce image annotations in a source modality. note that for each modality m ∈ {s, t }, m-genseg holds a segmentation decoder g m seg that shares most of its weights with the residual decoder g m res , but has its own set of normalization parameters and a supplementary classifying layer. thus, through the absence and presence translations, these segmenters have already learned how to disentangle the tumor from the background. however, supervised training on a few example annotations is still required to learn how to transform the resulting residual representation into appropriate segmentation maps. while this is a fairly straightforward task for the source modality using pixel-level annotations, achieving this for the target modality is more complex, justifying the second unsupervised translation objective between source and target modalities. based on the cyclegan  for the t→s→t cycle. note that to perform the domain adaptation, training the model to segment only the pseudo-target images generated by the s→t modality generator would suffice (in addition to the diseased/healthy target translation). however, training the segmentation on diseased source images also imposes additional constraints on encoder e s , ensuring the preservation of tumor structures. this constraint proves beneficial for the translation decoder g t as it generates pseudo-target tumoral samples that are more reliable. segmentation is therefore trained on both diseased source images s p and their corresponding synthetic target images s t p , when provided with annotations y s . to such an extent, two segmentation masks are predicted ŷs = g s seg • e s (s p ) and ŷst = g t seg • e t (s t p )."
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,4.0,Conclusion,"we propose m-genseg, a new framework for unpaired cross-modality tumor segmentation. we show that m-genseg is an annotation-efficient framework that greatly reduces the performance gap due to domain shift in cross-modality tumor segmentation. we claim that healthy tissues, if adequately incorporated to the training process of neural networks like in m-genseg, can help to better delineate tumor lesions in segmentation tasks. however, top performing methods on brats are 3d models. thus, future work will explore the use of full 3d images rather than 2d slices, along with more optimal architectures. our code is available: https://github.com/maloadba/mgenseg_2d."
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,1.0,Introduction,"simultaneous multi-index quantification (i.e., max diameter (md), center point coordinates (x o , y o ), and area), segmentation, and uncertainty prediction of liver tumor have essential significance for the prognosis and treatment of patients  recently, an increasing number of works have been attempted on liver tumor segmentation or quantification  to the best of our knowledge, although many works focus on the simultaneous quantization, segmentation, and uncertainty in medical images (i.e., heart  in this study, we propose an edge-aware multi-task network (eamtnet) that integrates the multi-index quantification (i.e., center point, max-diameter (md), and area), segmentation, and uncertainty. our basic assumption is that the model should capture the long-range dependency of features between multimodality and enhance the boundary information for quantification, segmentation, and uncertainty of liver tumors. the two parallel cnn encoders first extract local feature maps of multi-modality ncmri. meanwhile, to enhance the weight of tumor boundary information, the sobel filters are employed to extract edge maps that are fed into edge-aware feature aggregation (eafa) as prior knowledge. then, the eafa module is designed to select and fuse the information of multi-modality, making our eamtnet edge-aware by capturing the long-range dependency of features maps and edge maps. lastly, the proposed method estimates segmentation, uncertainty prediction, and multi-index quantification simultaneously by combining multi-task and cross-task joint loss. the contributions of this work mainly include: (1) for the first time, multiindex quantification, segmentation, and uncertainty of the liver tumor on multimodality ncmri are achieved simultaneously, providing a time-saving, reliable, and stable clinical tool. (2) the edge information extracted by the sobel filter enhances the weight of the tumor boundary by connecting the local feature as prior knowledge. (3) the novel eafa module makes our eamtnet edge-aware by capturing the long-range dependency of features maps and edge maps for feature fusion. the source code will be available on the author's website."
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,2.0,Method,"the eamtnet employs an innovative approach for simultaneous tumor multiindex quantification, segmentation, and uncertainty prediction on multimodality ncmri. as shown in fig. "
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,2.3,Multi-task Prediction,"in step 3 of fig.  where z i is the probability of pixel x belonging to category i. when a pixel has high entropy, it means that the network is uncertain about its classification. therefore, pixels with high entropy are more likely to be misclassified. in other words, its entropy will decrease when the network is confident in a pixel's label. under the constraints of uncertainty, the eamtnet can effectively rectify the errors in tumor segmentation because the uncertainty estimation can avoid overconfidence and erroneous quantification  where ŷs represents the prediction, and y i represents the ground truth label. the sum is performed on s pixels, ŷi task represents the predicted multi-index value, and y i task represents the ground truth of multi-index value, task ∈ {md, x, y , area}."
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,4.0,Conclusion,"in this paper, we have proposed an eamtnet for the simultaneous segmentation and multi-index quantification of liver tumors on multi-modality ncmri. the new eafa enhances edge awareness by utilizing boundary information as prior knowledge while capturing the long-range dependency of features to improve feature selection and fusion. additionally, multi-task leverages the prediction discrepancy to estimate uncertainty, thereby improving segmentation and quantification performance. extensive experiments have demonstrated the proposed model outperforms the sota methods in terms of dsc and mae, with great potential to be a diagnostic tool for doctors."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,1.0,Introduction,"automatic detection of brain tumors from magnetic resonance imaging (mri) is complex, tedious, and time-consuming because there are a lot of missed, misinterpreted, and misleading tumor-like lesions in the images of the brain tumors  with the rapid development of cnns, the accuracies of different visual tasks are constantly improved. however, the increasingly complex network architecture in cnn-based models, such as resnet  repvgg "
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.1,Dataset Details,"to evaluate the proposed rcs-yolo model, we used the brain tumor detection 2020 dataset (br35h) "
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.4,Results,"to highlight the accuracy and rapidity of the proposed model for the detection of brain tumor medical image data set, table  it can be seen that rcs-yolo with the advantages of incorporating the rcs-osa module performs well. compared with yolov7, the flops of the object detectors of this paper decrease by 8.8g, and the inference speed improves by 43.4 fps. in terms of detection rate, precision improves by 0.024; ap 50 increases by 0.01; ap 50:95 by 0.006. also, rcs-yolo is faster and more accurate than yolov6-l v3.0 and yolov8l. although the ap 50:95 of rcs-yolo equals that of yolov8l, it doesn't obscure the essential advantage of rcs-yolo. the results clearly show the superior performance and efficiency of our method, compared to the state-of-the-art for brain tumor detection. as shown in supplementary material fig. "
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,4.0,Conclusion,"we developed an rcs-yolo network for fast and accurate medical object detection, by leveraging the reparameterized convolution operator rcs based on channel shuffle in the yolo architecture. we designed an efficient one-shot aggregation module rcs-osa based on rcs, which serves as a computational unit in the backbone and neck of a new yolo network. evaluation of the brain mri dataset shows superior performance for brain tumor detection in terms of both speed and precision, as compared to yolov6, yolov7, and yolov8 models."
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,1.0,Introduction,"prostate segmentation from magnetic resonance imaging (mri) is a crucial step for diagnosis and treatment planning of prostate cancer. recently, deep learningbased approaches have greatly improved the accuracy and efficiency of automatic prostate mri segmentation  regarding quantity , the abundance of unlabeled data serves as a way to regularize the model and alleviate overfitting to the limited labeled data. unfortunately, such ""abundance"" may be unobtainable in practice, i.e., the local unlabeled pool is also limited due to restricted image collection capabilities or scarce patient samples. as a specific case shown in table  here, we define this new ssl scenario as multi-site semi-supervised learning (ms-ssl), allowing to enrich the unlabeled pool with multi-site heterogeneous images. being an under-explored scenario, few efforts have been made. to our best knowledge, the most relevant work is ahdc  in this work, we propose a more generalized framework called categorylevel regularized unlabeled-to-labeled (cu2l) learning, as depicted in fig. "
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,1.0,Introduction,"segmenting the prostate anatomy and detecting tumors is essential for both diagnostic and treatment planning purposes. hence, the task of developing domain generalisable prostate mri segmentation models is essential for the safe translation of these models into clinical practice. deep learning models are susceptible to textural shifts and artefacts which is often seen in mri due to variations in the complex acquisition protocols across multiple sites  the most common approach to tackle domain shifts is with data augmentation  the contributions of this paper are summarized as follows: 1. this work considers shape compositionality to enhance the generalisability of deep learning models to segment the prostate on mri. 2. we use cellular sheaves to aid compositionality for segmentation as well as improve tumour localisation."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,3.0,Related Work,there have been various deep learning based architectures developed for prostate tumour segmentation  randconv 
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,6.0,Conclusion,"in conclusion, we propose shape compositionality as a way to improve the generalisability of segmentation models for prostate mri. we devise a method to learn texture invariant and shape equivariant features used to create a dictionary of shape components. we use cellular sheaf theory to help model the composition of sampled shape components from this dictionary in order to produce more anatomically meaningful segmentations and improve tumour localisation."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Comparison:,"we compare our method with the nnunet  training: in all our experiments, the models were trained using adam optimization with a learning rate of 0.0001 and weight decay of 0.05. training was run for up to 500 epochs on three nvidia rtx 2080 gpus. the performance of the models was evaluated using the dice score, betti error  in our ablation studies, the minimum number of shape components required in d for the zonal and zonal + tumour segmentation experiments was 64 and 192 respectively before segmentation performance dropped. see supplementary material for ablation experiments analysing each component of our framework."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,1.0,Introduction,"transformers  the convnext architecture marries the scalability and long-range spatial representation learning capabilities of vision  in this work, we maximize the potential of a convnext design while uniquely addressing challenges of limited datasets in medical image segmentation. we present the first fully convnext 3d segmentation network, mednext, which is a scalable encoder-decoder network, and make the following contributions: mednext achieves state-of-the-art performance against baselines consisting of transformer-based, convolutional and large kernel networks. we show performance benefits on 4 tasks of varying modality (ct, mri) and sizes (ranging from 30 to 1251 samples), encompassing segmentation of organs and tumors. we propose mednext as a strong and modernized alternative to standard convnets for building deep networks for medical image segmentation."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,3.2,Datasets,"we use 4 popular tasks, encompassing organ as well as tumor segmentation tasks, to comprehensively demonstrate the benefits of the mednext architecture -1) beyond-the-cranial-vault (btcv) abdominal ct organ segmentation "
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,1.0,Introduction,"fluorodeoxyglucose positron emission tomography (pet) is widely recognized as an essential tool in oncology  in the last decade, cnns have demonstrated remarkable achievements in medical image segmentation tasks. this is primarily due to their ability to learn informative hierarchical features directly from data. however, as illustrated in  beyond tumour delineation, another important use of functional images, such as pet images is their use for designing imrt dose painting (dp). in particular, dose painting uses functional images to paint optimised dose prescriptions based on the spatially varying radiation sensitivities of tumours, thus enhancing the efficacy of tumour control  to address both tumour delineation and corresponding dose painting challenges, we propose to combine the expressiveness of deep cnns with the versa-tility of kspc in a unified framework, which we call kspc-net. in the proposed kspc-net, a cnn is employed to learn directly from the data to produce the pixel-wise bandwidth feature map and initial segmentation map, which are used to define the tuning parameters in the kspc module. our framework is completely automatic and differentiable. more specifically, we use the classic unet "
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,3.1,Dataset,the dataset is from the hecktor challenge in miccai 2021 (head and neck tumor segmentation challenge). the hecktor training dataset consists of 224 patients diagnosed with oropharyngeal cancer 
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,5.0,Conclusion,"in this paper, we present a novel network, kspc-net, for the segmentation in 2d pet images, which integrates kspc into the unet architecture in an end-toend differential manner. the kspc-net utilizes the benefits of kspc to deliver both contour-based and grid-based segmentation outcomes, leading to improved precision in the segmentation of contours. promising performance was achieved by our proposed kspc-net compared to the state-of-the-art approaches on the miccai 2021 challenge dataset (hecktor). it is worth mentioning that the architecture of our kspc-net is not limited to head & neck cancer type and can be broadcast to different cancer types. additionally, a byproduct application of our kspc-net is to construct probability contours, which enables probabilistic interpretation of contours. the subregions created by probability contours allow for a strategy planning for the assigned dose boosts, which is a necessity for the treatment planning of radiation therapy for cancers."
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,1.0,Introduction,"extracting brain tumors from medical image scans plays an important role in further analysis and clinical diagnosis. typically, a brain tumor includes peritumoral edema, enhancing tumor, and non-enhancing tumor core. since different modalities present different clarity of brain tumor components, we often use multi-modal image scans, such as t1, t1c, t2, and flair, in the task of brain tumor segmentation  current image segmentation methods for handling missing modalities can be divided into three categories, including: 1) brute-force methods: designing individual segmentation networks for each possible modality combination  to handle various numbers of modal inputs, hemis  due to the complexity of current models, we tend to develop a simple model, which adopts a simple average fusion and attention mechanism. these two techniques are demonstrated to be effective in handling missing modalities and multimodal fusion  -we propose a simple multi-modal fusion network, a2fseg, for brain tumor segmentation, which is general and can be extended to any number of modalities for incomplete image segmentation. -we conduct experiments on the brats 2020 dataset and achieve the sota segmentation performance, having a mean dice core of 89.79% for the whole tumor, 82.72% for the tumor core, and 66.71% for the enhancing tumor."
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,2.0,Method,"figure  modality-specific feature extraction (msfe) module. before fusion, we first extract features for every single modality, using the nnunet model  here, the number of channels is c = 32; h f , w f , and d f are the height, width, and depth of feature maps f m , which share the same size as the input image. for every single modality, each msfe module is supervised by the image segmentation mask to fasten its convergence and provide a good feature extraction for fusion later. all four msfes have the same architecture but with different weights. average fusion module. to aggregate image features from different modalities and handle the possibility of missing one or more modalities, we use the average of the available features from different modalities as the first fusion result. that is, we obtain a fused average feature here, n m is the number of available modalities. for example, as shown in fig.  adaptive fusion module. since each modality contributes differently to the final tumor segmentation, similar to maml  here, f m is a convolutional layer for this specific modality m, and θ m represents the parameters of this layer, and σ is a sigmoid function. that is, we have an individual convolution layer f m for each modality to generate different weights. due to the possibility of missing modalities, we will have different numbers of feature maps for fusion. to address this issue, we normalize the different attention weights by using a softmax function: that is, we only consider feature maps from those available modalities but normalize their contribution to the final fusion result, so that, the fused one has a consistent value range, no matter how many modalities are missing. then, we perform voxel-wise multiplication of the attention weight with the corresponding modal feature maps. as a result, the adaptively fused feature maps f is calculated by the weighted sum of each modal feature: here, ⊗ indicates the voxel-wise multiplication. loss function. we have multiple segmentation heads, which are distributed in each module of a2fseg. for each segmentation head, we use the combination of the cross-entropy and the soft dice score as the basic loss function, which is defined as where ŷ and y represent the segmentation prediction and the ground truth, respectively. based on this basic one, we have the overall loss function defined as where the first term is the basic segmentation loss for each modality m after feature extraction; the second term is the loss for the segmentation output of the average fusion module; and the last term is the segmentation loss for the final output from the adaptive fusion module. 3 experiments"
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,3.1,Dataset,"our experiments are conducted on brats2020, which contains 369 multicontrast mri scans with four modalities: t1, t1c, t2, and flair. these images went through a sequence of preprocessing steps, including co-registration to the same anatomical template, resampling to the same resolution (1 mm 3 ), and skullstripping. the segmentation masks have three labels, including the whole tumor (abbreviated as complete), tumor core (abbreviated as core), and enhancing tumor (abbreviated as enhancing). these annotations are manually provided by one to four radiologists according to the same annotation protocol."
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,4.0,Discussion and Conclusion,"in this paper, we propose an average and adaptive fusion segmentation network (a2fseg) for the incomplete multi-model brain tumor segmentation task. the essential components of our a2fseg network are the two stages of feature fusion, including an average fusion and an adaptive fusion. compare to existing complicated models, our model is much simpler and more effective, which is demonstrated by the best performance on the brats 2020 brain tumor segmentation task. the experimental results demonstrate the effectiveness of two techniques, i.e., the average fusion and the attention-based adaptive one, for incomplete modal segmentation tasks. our study brings up the question of whether having complicated models is necessary. if there is no huge gap between different modalities, like in our case where all four modalities are images, the image feature maps are similar and a simple fusion like ours can work. otherwise, we perhaps need an adaptor or an alignment strategy to fuse different types of features, such as images and audio. also, we observe that a good feature extractor is essential for improving the segmentation results. in this paper, we only explore a reduced unet for feature extraction. in future work, we will explore other feature extractors, such as vision transformer (vit) or other pre-trained visual foundation models "
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,1.0,Introduction,"diffuse glioma is a common malignant tumor with highly variable prognosis across individuals. to improve survival outcomes, many pre-operative survival prediction methods have been proposed with success. based on the prediction results, personalized treatment can be achieved. for instance, isensee et al.  despite the promising results of existing pre-operative survival prediction methods, they often overlook clinical knowledge that could aid in improving the prediction accuracy. notably, tumor types have been found to be strongly correlated with the prognosis of diffuse glioma  our method is evaluated using pre-operative multimodal mr brain images of 1726 diffuse glioma patients collected from cooperation hospitals and a public dataset brats2019 "
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.0,Methods,"diffuse glioma can be classified into three histological types: the oligodendroglioma, the astrocytoma, and the glioblastoma  the tumor subtyping network is trained independently before being integrated into the backbone. to solve the inherent issue of imbalanced tumor type in the training data collected in clinic, a novel ordinal manifold mixup based feature augmentation is applied in the training of the tumor subtyping network. it is worth noting that the ground truth of tumor types, which is determined after craniotomy, is available in the training data, while for the testing data, tumor types are not required, because tumor-type-related features can be learned from the pre-operative multimodal mr brain images."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.1,The Survival Prediction Backbone,"the architecture of the survival prediction backbone, depicted in fig.  from the tumor subtyping network (discussed later), and m is the vector dimension which is set to 128. as f type i has strong correlation with prognosis, the performance of the backbone can be improved. in addition, information of patient age and tumor position is also used. to encode the tumor position, the brain is divided into 3 × 3 × 3 blocks, and the tumor position is represented by 27 binary values (0 or 1) with each value for one block. if a block contains tumors, then the corresponding binary value is 1, otherwise is 0. the backbone is based on the deep cox proportional hazard model, and the loss function is defined as: where h θ (x i ) represents the risk of the i-th patient predicted by the backbone, θ stands for the parameters of the backbone, x i is the input multimodal mr brain images of the i-th patient, r(t i ) is the risk group at time t i , which contains all patients who are still alive before time t i , t i is the observed time (time of death happened) of x i , and δ i = 0/1 for censored/non-censored patient."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.2,The Tumor Subtyping Network,"the tumor subtyping network has almost the same structure as the backbone. it is responsible for learning tumor-type-related features from each input preoperative multimodal mr brain image x i and classifying the tumor into oligodendroglioma, astrocytoma, or glioblastoma. the cross entropy is adopted as the loss function of the tumor subtyping network, which is defined as: where y k i and p k i are the ground truth (0 or 1) and the prediction (probability) of the k-th tumor type (k = 1, 2, 3) of the i-th patient, respectively. the learned tumor-type-related feature f type i ∈ r m is fed to the survival prediction backbone and concatenated with f cox i learned in the backbone to predict the risk. in the in-house dataset, the proportions of the three tumor types are 20.9% (oligodendroglioma), 28.7% (astrocytoma), and 50.4% (glioblastoma), which is consistent with the statistical report in "
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.3,The Ordinal Manifold Mixup,"in the original manifold mixup  where y k i and y k j stand for the labels of the k-th tumor type of the i-th and j-th patients, respectively, and λ ∈ [0, 1] is a weighting factor. for binary classification, the original manifold mixup can effectively enhance the network performance, however, for the classification of more than two classes, e.g., tumor types, there exists a big issue. as shown in fig. "
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,,right).,"normally, the feature distribution of each tumor type is assumed to be independent normal distribution, so their joint distribution is given by: where f k , k = 1, 2, 3 represents the feature set of oligodendroglioma, astrocytoma, and glioblastoma, respectively, μ k and σ 2 k are mean and variance of f k . to impose the ordinal constraint, we define the desired feature distribution of each tumor type as n (μ 1 , σ2 1 ) for k = 1, and n (μ k-1 + δ k , σ2 k ) for k = 2 and 3. in this way, the feature distribution of each tumor type depends on its predecessor, and the mean feature of each tumor type μk (except μ1 ) is equal to the mean feature of its predecessor μk-1 shifted by δ k . note that δ k is set to be larger than 3 × σk to ensure the desired ordering  which can be represented as: where μ1 and σk , k = 1, 2, 3 can be learned by the tumor subtyping network. finally, the ordinal loss, which is in the form of kl divergence, is defined as: in our method, μ k and σ 2 k are calculated by where φ θ and g are the encoder and gap of the tumor subtyping network, respectively, θ is the parameter set of the encoder, stands for the subset containing the pre-operative multimodal mr brain images of the patients with the k-th tumor type, n k is the patient number in d k . so we impose the ordinal loss l kl to the features after the gap of the tumor subtyping network as shown in fig.  the tumor subtyping network is first trained before being integrated into the survival prediction backbone. in the training stage of the tumor subtyping network, each input batch contains pre-operative multimodal mr brain images of n patients and can be divided into k = 3 subsets according to their corresponding tumor types, i.e., d k , k = 1, 2, 3. with the ordinal constrained feature distribution, high consistent features can be augmented between neighboring tumor types. based on the original and augmented features, the performance of the tumor subtyping network can be enhanced. once the tumor subtyping network has been trained, it is then integrated into the survival prediction backbone, which is trained under the constraint of the cox proportional hazard loss l cox ."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,3.0,Results,"in our experiment, both in-house and public datasets are used to evaluate our method. specifically, the in-house dataset collected in cooperation hospitals contains pre-operative multimodal mr images, including t1, t1 contrast enhanced (t1c), t2, and flair, of 1726 patients (age 49.7 ± 13.1) with confirmed diffuse glioma types. the patient number of each tumor type is 361 (oligodendroglioma), 495 (astrocytoma), and 870 (glioblastoma), respectively. in the 1726 patients, 743 have the corresponding overall survival time (dead, non-censored), and 983 patients have the last visiting time (alive, censored). besides the inhouse dataset, a public dataset brats2019, including pre-operative multimodal mr images of 210 non-censored patients (age 61.4 ± 12.2), is adopted as the external independent testing dataset. all images of the in-house and brats2019 datasets go through the same pre-processing stage, including image normalization and affine transformation to mni152  besides our method, four state-of-the-art methods, including random forest based method (rf)  where d = {x 1 , ..., x n } is the dataset containing all patients, t i and t j are ground truth of survival times of the i-th and j-th patients, r i and r j are the days predicted by rf, mcsp, and pgsp or risks predicted by the deep cox proportional hazard models (i.e., deepconvsurv and our method), 1 x<y = 1 if x < y, else 0, and δ i = 0 or 1 when the i-th patient is censored or non-censored. as rf, mcsp, and pgsp cannot use the censored data in the in-house dataset, 80% of the non-censored data (594 patients) are randomly selected as the training data, and the rest 20% non-censored data (149 patients) are for testing. while deepconvsurv and our method are deep cox models, both censored and non-censored patients can be utilized. so besides the 80% non-censored patients, all censored data (983 patients) are also included in the training data. table "
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,3.1,Ablation Study of Survival Prediction,"to show the effect of the tumor subtyping network and the ordinal manifold mixup in survival prediction, our method without the tumor subtyping network (baseline-1) and our method with the tumor subtyping network (using original manifold mixup instead, baseline-2) are evaluated. for the in-house dataset, the resulting c-indices are 0.744 (baseline-1) and 0.735 (baseline-2). so our method make the improvement of c-index more than 8% comparing with baseline-2. for the external independent testing dataset brats2019, the resulting c-indices are 0.738 (baseline-1) and 0.714 (baseline-2), and our method still has more than 6% improvement comparing with baseline-2. figure "
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,4.0,Conclusions,"we proposed a new method for pre-operative survival prediction of diffuse glioma patients, where a tumor subtyping network is integrated into the prediction backbone. based on the tumor subtyping network, tumor type information, which are only available after craniotomy, can be derived from the pre-operative multimodal mr images to boost the survival prediction performance. moreover, a novel ordinal manifold mixup was presented, where ordinal constraint is imposed to make feature distribution of different tumor types in the order of risk grade, and feature augmentation only takes place between neighboring tumor types. in this way, inconsistency between the augmented features and corresponding labels can be effectively reduced. both in-house and public datasets containing 1936 patients were used in the experiment. our method outperformed the state-of-the-art methods in terms of the concordance-index."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,1.0,Introduction,segmentation of skin lesions from dermoscopy images is a critical task in disease diagnosis and treatment planning of skin cancers 
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,1.0,Introduction,"accurate tumor segmentation from medical images is essential for quantitative assessment of cancer progression and preoperative treatment planning  recently, multimodal tumor segmentation has attracted the interest of many researchers. with the emergence of multimodal datasets (e.g., brats  in addition to the progress on the fusion of multimodal features, improving the model representation ability is also an effective way to boost segmentation performance. in the past few years, transformer structure  although remarkable performance has been accomplished with these efforts, there still exist several challenges to be resolved. most existing methods are either limited to specific modality numbers due to the design of asymmetric connections or suffer from large computational complexity because of the huge amount of model parameters. therefore, how to improve model ability while ensuring computational efficiency is the main focus of this paper. to this end, we propose an efficient multimodal tumor segmentation solution named hybrid densely connected network (h-denseformer). first, our method leverages transformer to enhance the global contextual information of different modalities. second, h-denseformer integrates a transformer-based multi-path parallel embedding (mpe) module, which can extract and fuse multimodal image features as a complement to naive input-level fusion structure. specifically, mpe assigns an independent encoding path to each modality, then merges the semantic features of all paths and feeds them to the encoder of the segmentation network. this decouples the feature representations of different modalities while relaxing the input constraint on the specific number of modalities. finally, we design a lightweight, densely connected transformer (dct) module to replace the standard transformer to ensure performance and computational efficiency. extensive experimental results on two publicly available datasets demonstrate the effectiveness of our proposed method. as the auxiliary extractor of multimodal fusion features, while the latter is used to generate predictions. specifically, given a multimodal image input x 3d ∈ r c×h×w ×d or x 2d ∈ r c×h×w with a spatial resolution of h × w , the depth dimension of d (number of slices) and c channels (number of modalities), we first utilize mpe to extract and fuse multimodal image features. then, the obtained features are progressively upsampled and delivered to the encoder of the segmentation network to enhance the semantic representation. finally, the segmentation network generates multi-scale outputs, which are used to calculate deep supervision loss as the optimization target."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,2.4,Segmentation Backbone Network,"the h-denseformer adopts a u-shaped encoder-decoder structure as its backbone. as shown in fig.  , where i ∈ [0, 1, 2, 3], and c = 2 (tumor and background) represents the number of segmentation classes. to mitigate the pixel imbalance problem, we use a combined loss of focal loss  where n refers to the total number of pixels, p t and q t denote the predicted probability and ground truth of the t-th pixel, respectively, and r = 2 is the modulation factor. thus, ds loss can be calculated as follows: where g i represents the ground truth after resizing and has the same size as o i . α is a weighting factor to control the proportion of loss corresponding to the output at different scales. this approach can improve the convergence speed and performance of the network."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,4.0,Conclusion,"in this paper, we proposed an efficient hybrid model (h-denseformer) that combines transformer and cnn for multimodal tumor segmentation. concretely, a multi-path parallel embedding module and a densely connected transformer block were developed and integrated to balance accuracy and computational complexity. extensive experimental results demonstrated the effectiveness and superiority of our proposed h-denseformer. in future work, we will extend our method to more tasks and explore more efficient multimodal feature fusion methods to further improve computational efficiency and segmentation performance."
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,1.0,Introduction,"accurate cancer diagnosis, grading, and treatment decisions from medical images heavily rely on the analysis of underlying complex nuclei structures  in the literature work, the sole-decoder design in these unet variants (fig.  additionally, existing methods are cnn-based, and their intrinsic convolution operation fails to capture global spatial information or the correlation amongst nuclei "
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,1.0,Introduction,"gliomas are the most commonly seen central nervous system malignancies with aggressive growth and low survival rates  most previous studies of segmentation qc only provide subject-level quality assessment by either directly predicting segmentation-quality metrics or their surrogates. specifically, wang et al.  multiple studies have also explored regression-based methods to directly predict segmentation-quality metrics, e.g., dice similarity coefficient (dsc). for example, kohlberger et al.  in summary, while numerous efforts have been devoted to segmentation qc, most works were in the context of cardiac mri segmentation with few works tackling segmentation qc of brain tumors, which have more complex and heterogeneous appearances than the heart. furthermore, most of the existing methods do not localize segmentation errors, which is meaningful for both auditing purposes and guiding manual refinement. to address these challenges, we propose a novel framework for joint subject-level and voxel-level prediction of segmentation quality from multimodal mri. the contribution of this work is four-fold. first, we proposed a predictive model (qcresunet) that simultaneously predicts dsc and localizes segmentation errors at the voxel level. second, we devised a datageneration approach, called seggen, that generates a wide range of segmentations of varying quality, ensuring unbiased model training and testing. third, our end-to-end predictive model yields fast inference. fourth, the proposed method achieved a good performance in predicting subject-level segmentation quality and identifying voxel-level segmentation failures."
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,2.0,Method,"given four imaging modalities denoted as [x 1 , x 2 , x 3 , x 4 ] and a predicted multiclass brain tumor segmentation mask (s pred ), the goal of our approach is to automatically assess the tumor segmentation quality by simultaneously predicting dsc and identifying segmentation errors as a binary mask (s err ). toward this end, we proposed a 3d encoder-decoder architecture termed qcresunet (see fig.  the resnet-34 encoder enables the extraction of semantically rich features that are useful for characterizing the quality of the segmentation. we maintained the main structure of the vanilla 2d resnet-34  the building block of the decoder consisted of an upsampling by a factor of two, which was implemented by a nearest neighbor interpolation in the feature map, followed by two convolutional blocks that halve the number of feature maps. each convolutional block comprised a 3 × 3 × 3 convolutional layer followed by an instance normalization layer and a leaky relu activation  the objective function for training qcresunet consists of two parts. the first part corresponds to the dsc regression task. it consists of a mean absolute error (mae) loss (l mae ) term that penalizes differences between ground truth (dsc gt ) and predicted dsc (dsc pred ): where n denotes the number of samples in a batch. the second part of the objective function corresponds to the segmentation error prediction. it consists of a dice loss  where s errgt , s err pred denote the binary ground-truth segmentation error map and the predicted error segmentation map from the sigmoid output of the decoder, respectively. the dice loss and cross-entropy loss were averaged across the number of pixels i in a batch. the two parts are combined using a weight parameter λ to balance the different loss components: 3 experiments for this study, pre-operative multimodal mri scans of varying grades of glioma were obtained from the 2021 brain tumor segmentation (brats) challenge "
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,5.0,Conclusion,"in this work, we proposed a novel cnn architecture called qcresunet to perform automatic brain tumor segmentation qc in multimodal mri scans. qcre-sunet simultaneously provides subject-level segmentation-quality prediction and localizes segmentation failures at the voxel level. it achieved superior dsc prediction performance compared to all baselines. in addition, the ability to localize segmentation errors has the potential to guide the refinement of predicted segmentations in a clinical setting. this can significantly expedite clinical workflows, thus improving the overall clinical management of gliomas."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,1.0,Introduction,"multi-modal learning has become a popular research area in computer vision and medical image analysis, with modalities spanning across various media types, including texts, audio, images, videos and multiple sensor data. this approach has been utilised in robot control  the missing modality issue is a significant challenge in the multi-modal domain, and it has motivated the community to develop approaches that attempt to address this problem. havaei et al.  aiming at this issue, we propose the non-dedicated training model -we propose the learnable cross-modal knowledge distillation (lckd) model to address missing modality problem in multi-modal learning. it is a simple yet effective model designed from the viewpoint of distilling crossmodal knowledge to maximise the performance for all tasks; -the lckd approach is designed to automatically identify the important modalities per task, which helps the cross-modal knowledge distillation process. it also can handle missing modality during both training and testing. the experiments are conducted on the brain tumour segmentation benchmark brats2018 "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.1,Overall Architecture,"let us represent the n -modality data with m l = {x ∈ x denotes the l th data sample and the superscript (i) indexes the modality. to simplify the notation, we omit the subscript l when that information is clear from the context. the label for each set m is represented by y ∈ y, where y represents the ground-truth annotation space. the framework of lckd is shown in fig.  multi-modal segmentation is composed not only of multiple modalities, but also of multiple tasks, such as the three types of tumours in brats2018 dataset that represent the three tasks. take one of the tasks for example. our model undergoes an external teacher election procedure prior to processing all modalities {x (i) } n i=1 ∈ m in order to select the modalities that exhibit promising performance as teachers. this is illustrated in fig.  in the next sections, we explain each module of the proposed learnable crossmodal knowledge distillation model training and testing with full and missing modalities."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.2,Teacher Election Procedure,"usually, one of the modalities is more useful than others for a certain task, e.g. for brain tumour segmentation, t1c scan clearly displays the enhanced tumour, but it does not clearly show edema  more specifically, in the teacher election procedure, a validation process is applied: for each task k (for k ∈ {1, ..., k}), the modality with the best performance is selected as the teacher t (k) . formally, we have: where i indexes different modalities, f (•; θ) is the lckd segmentation model parameterised by θ, including the encoder and decoder parameters {θ enc , θ dec } ∈ θ, and d(•, •) is the function to calculate the dice score. based on the elected teachers for different tasks, a list of unique teachers (i.e., repetitions are not allowed in the list, so for brats, {t1c, t1c, flair} would be reduced to {t1c, flair}) are generated with: t = φ(t (1) , t (2) , ..., t (k) , ..., t (k) ), "
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,1.0,Introduction,"malignant melanoma is one of the most rapidly growing cancers in the world. as estimated by the american cancer society, there were approximately 100,350 new cases and over 6,500 deaths in 2020  transunet  prior works have enhanced performance by introducing intricate modules, but neglected the constraint of computational resources in real medical settings. hence, there is an urgent need to design a low-parameter and low-computational load model for segmentation tasks in mobile healthcare. recently, unext  to be specific, ege-unet leverages two key modules: the group multi-axis hadamard product attention module (ghpa) and group aggregation bridge module (gab). on the one hand, recent models based on vit  in summary, our contributions are threefold: (1) ghpa and gab are proposed, with the former efficiently acquiring and integrating multi-perspective information and the latter accepting features at different scales, along with an auxiliary mask for efficient multi-scale feature fusion. (2) we propose ege-unet, an extremely lightweight model designed for skin lesion segmentation. (3) we conduct extensive experiments, which demonstrate the effectiveness of our methods in achieving state-of-the-art performance with significantly lower resource requirements."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,1.0,Introduction,"dynamic contrast-enhanced magnetic resonance imaging (dce-mri) revealing tumor hemodynamics information is often applied to early diagnosis and treatment of breast cancer  recently, denoising diffusion probabilistic model (ddpm)  based on the above observations, we innovatively consider the underlying relation between hemodynamic response function (hrf) and denoising diffusion process (ddp). as shown in fig.  once the diffusion module is pretrained, the latent kinetic code can be easily generated with only pre-contrast images, which is fed into a segmentation module to annotate cancers. to verify the effectiveness of the latent kinetic code, the sm adopts a simple u-net-like structure, with an encoder to simultaneously conduct semantic feature encoding and kinetic code fusion, along with a decoder to obtain voxel-level classification. in this manner, our latent kinetic code can be interpreted to provide tic information and hemodynamic characteristics for accurate cancer segmentation. we verify the effectiveness of our proposed diffusion kinetic model (dkm) on dce-mri-based breast cancer segmentation using breast-mri-nact-pilot dataset  • we propose a diffusion kinetic model that implicitly exploits hemodynamic priors in dce-mri and effectively generates high-quality segmentation maps only requiring pre-contrast images. • we first consider the underlying relation between hemodynamic response function and denoising diffusion process and provide a ddpm-based solution to capture a latent kinetic code for hemodynamic knowledge. • compared to the existing approaches with complete sequences, the proposed method yields higher cancer segmentation performance even with pre-contrast images."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,2.2,Segmentation Module,"once pretrained, the dm outputs multi-scale latent kinetic code f dm from intermediate layers, which is fed into the sm to guide cancer segmentation. as shown in fig.  where * represents 1 × 1 based convolution operation, w is the weight matrix, bn represents batch normalization, φ represents relu activation function and c is concatenation operation. in this way, the hemodynamic knowledge can be incorporated into the sm to capture more expressive representations to improve segmentation performance."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,2.3,Model Training,"to maintain training stability, the proposed dkm adopts a two-step training procedure for cancer annotation. in the first step, the dm is trained to transform pre-contrast images into post-contrast images for a latent space where hemodynamic priors are exploited. in particular, the diffusion loss for the reverse diffusion process can be formulated as follows: where θ represents the denoising model that employs an u-net structure, x 0 and x k are the pre-contrast and post-contrast images, respectively, is gaussian distribution data ∼ n (0, i), and t is a timestep. for a second step, we train the sm that integrates the previously learned latent kinetic code to provide tumor hemodynamic information for voxel-level prediction. considering the varying sizes, shapes and appearances of tumors that results from intratumor heterogeneity and results in difficulties of accurate cancer annotation, we design the segmentation loss as follows: where l ssim is used to evaluate tumor structural characteristics, s and g represents segmentation map and ground truth, respectively; μ s is the mean of s and μ g is the mean of g; ϕ s represents the variance of s and ϕ g represents the variance of g; c 1 and c 2 denote the constant to hold training stable  where k 1 is set as 0.01, k 2 is set as 0.03 and l is set as the range of voxel values."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,3.0,Experiments,"dataset: to demonstrate the effectiveness of our proposed dkm, we evaluate our method on 4d dce-mri breast cancer segmentation using the breast-mri-nact-pilot dataset "
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,4.0,Conclusion,we propose a diffusion kinetic model by exploiting hemodynamic priors in dce-mri to effectively generate high-quality segmentation results only requiring precontrast images. our models learns the hemodynamic response function based on the denoising diffusion process and estimates the latent kinetic code to guide the segmentation task. experiments demonstrate that our proposed framework has the potential to be a promising tool in clinical applications to annotate cancers.
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,1.0,Introduction,"accurate gland segmentation from whole slide images (wsis) plays a crucial role in the diagnosis and prognosis of cancer, as the morphological features of glands can provide valuable information regarding tumor aggressiveness  to reduce the annotation cost, developing annotation-efficient methods for semantic-level gland segmentation has attracted much attention  one potential solution is to adopt unsupervised semantic segmentation (uss) methods which have been successfully applied to medical image research and natural image research. on the one hand, existing uss methods have shown promising results in various medical modalities, e.g., magnetic resonance images  x-ray images  to tackle the above challenges, our solution is to incorporate an empirical cue about gland morphology as additional knowledge to guide gland segmentation. the cue can be described as: each gland is comprised of a border region with high gray levels that surrounds the interior epithelial tissues. to this end, we propose a novel morphology-inspired method via selective semantic grouping, abbreviated as mssg. to begin, we leverage the empirical cue to selectively mine out proposals for the two gland sub-regions with variant appearances. then, considering that our segmentation target is the gland, we employ a morphology-aware semantic grouping module to summarize the semantic information about glands by explicitly grouping the semantics of the sub-region proposals. in this way, we not only prioritize and dedicate extra attention to the target gland regions, thus avoiding under-segmentation; but also exploit the valuable morphology information hidden in the empirical cue, and force the segmentation network to recognize entire glands despite the excessive variance among the sub-regions, thus preventing over-segmentation. ultimately, our method produces well-delineated and complete predictions; see fig.  our contributions are as follows: (1) we identify the major challenge encountered by prior unsupervised semantic segmentation (uss) methods when dealing with gland images, and propose a novel mssg for unsupervised gland segmentation. "
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,1.0,Introduction,"accurate segmentation of brain tumors from mri images is of great significance as it enables more accurate assessment of tumor morphology, size, location, and distribution range, thereby providing clinicians with a reliable basis for diagnosis and treatment  cnn-based networks, such as unet  considerable advancement has been achieved in the field of natural image segmentation by focusing on the edge information  in this paper, we propose an edge-oriented transformer (eoformer), for efficient and accurate brain tumor segmentation. we design a cnn-transformer based encoder for more effective feature representation, called efficient hybrid encoder (ehe). specifically, the input image is first processed by the cnn blocks to extract low-level local features. then, the extracted features are fed into the transformer blocks to create long-range dependencies, resulting in the formation of high-level semantic features. in addition, to provide more accurate edge predictions, we design two edge sharpening modules in the decoder, called edgeoriented sobel (eos) and laplacian (eol) modules. by implicitly embedding sobel and laplacian filters into the convolution layers to extract 1st-order and 2nd-order differential features, the two modules could enhance the edge information contained in the feature maps. in order to reduce the computational and memory complexity of the model, we replace the vanilla attention module with our extended efficient attention module "
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,4.0,Conclusion,"in this paper, we propose the eoformer, a novel approach for brain tumor segmentation. our method comprises the efficient hybrid encoder and the edgeoriented transformer decoder. the encoder effectively extracts features from images by striking a balance between cnn and transformer architectures. the decoder integrates the sobel and laplacian edge detection filters into our edgeoriented modules that enhance the extraction capability of edge and texture information. besides, we introduce the efficient attention mechanism and the re-parameterization technology to improve the model efficiency. our eoformer outperforms other state-of-the-art methods on both brats 2020 and medseg. our model is computationally efficient and can be readily applied to other 3d medical image segmentation tasks."
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,1.0,Introduction,"medical image analysis has greatly benefited from advances in ai  one important diagnosis task is to segment lyme lesion, particularly the em pattern, from benign skins. such dl-assisted segmentation not only helps clinicians in pre-screening patients but also improves downstream tasks such as lesion classification. however, while lyme disease lesion segmentation is intuitively simple, it is challenging due to the following reasons. first, there lacks of a well-segmented dataset with manual labels on lyme disease. on one hand, some datasets-such as ham10000  second, the segmentation of lyme lesion is itself challenging due to the nature of em pattern. specifically, a typical lyme lesion exhibits a bull's eye pattern with one central redness and one outer circle, which is different from darkness lesion in cancer-related skin disease like melanoma. furthermore, clinical data collected for training is usually imbalanced in some properties, e.g., more samples with light skins compared with dark skins. therefore, existing skin disease segmentation  in this paper, we present the first lyme disease dataset that contains labeled segmentation and skin tones. our lyme disease dataset contains two parts: (i) a classification dataset, composed of more than 3,000 diseased skin images that are either obtained from public resources or clinicians with patient-informed consent, and (ii) a segmentation dataset containing 185 samples that are manually annotated for three regions-i.e., background, skin (light vs. dark), and lesionconducted under clinician supervision and institutional review boards (irb) approval. our dataset with manual labels is available at this url  secondly, we design a simple yet novel data preprocessing and alternation method, called edgemixup, to improve lyme disease segmentation and diagnosis fairness on samples with different skin-tones. the key insight is to alter a skin image with a linear combination of the source image and a detected lesion boundary so that the lesion structure is preserved while minimizing skin tone information. such an improvement is an iterative process that gradually improves lesion edge detection and segmentation fairness until convergence. then, the detected, converged edge in the first step also helps classification of lyme diseases via mixup with improved fairness. our source code is available at this url  we evaluate edgemixup for skin disease segmentation and classification tasks. our results show that edgemixup is able to increase segmentation utility and improve fairness. we also show that the improved segmentation further improves classification fairness as well as joint fairness-utility metrics compared to existing debiasing methods, e.g., ad "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,1.0,Introduction,"head and neck (h&n) cancer refers to malignant tumors in h&n regions, which is among the most common cancers worldwide  traditional survival prediction methods are usually based on radiomics  firstly, existing deep survival models are underdeveloped in utilizing complementary multi-modality information, such as the metabolic and anatomical information in pet and ct images. for survival prediction in h&n cancer, existing methods usually use single imaging modality  secondly, although deep survival models have advantages in performing end-to-end survival prediction without requiring tumor masks, this also incurs difficulties in extracting region-specific information, such as the prognostic information in primary tumor (pt) and metastatic lymph node (mln) regions. to address this limitation, recent deep survival models adopted multi-task learning for joint tumor segmentation and survival prediction, to implicitly guide the model to extract features related to tumor regions  in this study, we design an x-shape merging-diverging hybrid transformer network (named xsurv, fig. "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3.1,Dataset and Preprocessing,"we adopted the training dataset of hecktor 2022 (refer to https://hecktor.grand-cha llenge.org/), including 488 h&n cancer patients acquired from seven medical centers  we resampled pet-ct images into isotropic voxels where 1 voxel corresponds to 1 mm 3 . each image was cropped to 160 × 160 × 160 voxels with the tumor located in the center. pet images were standardized using z-score normalization, while ct images were clipped to "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,5.0,Conclusion,"we have outlined an x-shape merging-diverging hybrid transformer network (xsurv) for survival prediction from pet-ct images in h&n cancer. within the xsurv, we propose a merging-diverging learning framework, a hybrid parallel cross-attention (hpca) block, and a region-specific attention gate (rag) block, to learn complementary information from multi-modality images and extract region-specific prognostic information for survival prediction. extensive experiments have shown that the proposed framework and blocks enable our xsurv to outperform state-of-the-art survival prediction methods on the well-benchmarked hecktor 2022 dataset."
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,1.0,Introduction,"the examination of tissue and cells using microscope (referred to as histology) has been a key component of cancer diagnosis and prognostication since more than a hundred years ago. histological features allow visual readout of cancer biology as they represent the overall impact of genetic changes on cells  the great rise of deep learning in the past decade and our ability to digitize histopathology slides using high-throughput slide scanners have fueled interests in the applications of deep learning in histopathology image analysis. the majority of the efforts, so far, focus on the deployment of these models for diagnosis and classification  in the machine learning domain, patient prognostication can be treated as a weakly supervised problem, which a model would predict the outcome (e.g., time to cancer recurrence) based on the histopathology images. their majority have utilized multiple instance learning (mil)  to address this issue, graph neural networks (gnn) have recently received more attention in histology. they can model patch relations  this paper aims to investigate the potential of extracting fine and coarse features from histopathology slides and integrating them for risk stratification in cancer patients. therefore, the contributions of this work can be summarized as: 1) a novel graph-based model for predicting survival that extracts both local and global properties by identifying morphological super-nodes; 2) introducing a fine-coarse feature distillation module with 3 various strategies to aggregate interactions at different scales; 3) outperforming sota approaches in both risk prediction and patient stratification scenarios on two datasets; 4) publishing two large and rare prostate cancer datasets containing more than 220 graphs for active surveillance and 240 graphs for brachytherapy cases. the code and graph embeddings are publicly available at https://github.com/pazadimo/all-in 2 related works"
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4.1,Dataset,"we utilize two prostate cancer (pca) datasets to evaluate the performance of our proposed model. the first set (pca-as) includes 179 pca patients who were managed with active surveillance (as). radical therapy is considered overtreatment in these patients, so they are instead monitored with regular serum prostate-specific antigen (psa) measurements, physical examinations, sequential biopsies, and magnetic resonance imaging  the second dataset (pca-bt) includes 105 pca patients with low to high risk disease who went through brachytherapy. this treatment involves placing a radioactive material inside the body to safely deliver larger dose of radiation at one time "
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,1.0,Introduction,"accurate spatial characterization of tumor immune microenvironment is critical for precise therapeutic stratification of cancer patients (e.g. via immunotherapy). currently, this characterization is done manually by individual pathologists on standard hematoxylin-and-eosin (h&e) or singleplex immunohistochemistry (ihc) stained images. however, this results in high interobserver variability among pathologists, primarily due to the large (> 50%) disagreement among pathologists for immune cell phenotyping  in this paper, we introduce a new dataset that can be readily used out-ofthe-box with any artificial intelligence (ai)/deep learning algorithms for spatial characterization of tumor immune microenvironment and several other use cases. to date, only two denovo stained datasets have been released publicly: bci h&e and singleplex ihc her2 dataset "
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.0,Dataset,the complete staining protocols for this dataset are given in the accompanying supplementary material. images were acquired at 20× magnification at moffitt cancer center. the demographics and other relevant information for all eight head-and-neck squamous cell carcinoma patients is given in table 
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.1,Region-of-Interest Selection and Image Registration,"after scanning the full images at low resolution, nine regions of interest (rois) from each slide were chosen by an experienced pathologist on both mif and mihc images: three in the tumor core (tc), three at the tumor margin (tm), and three outside in the adjacent stroma (s) area. the size of the rois was standardized at 1356×1012 pixels with a resolution of 0.5 µm/pixel for a total surface area of 0.343 mm 2 . hematoxylin-stained rois were first used to align all the mihc marker images in the open source fiji software using affine registration. after that, hematoxylin-and dapi-stained rois were used as references to align mihc and mif rois again using fiji and subdivided into 512×512 patches, resulting in total of 268 co-registered mihc and mif patches (∼33 co-registered mif/mihc images per patient)."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,4.0,Conclusions and Future Work,"we have released the first ai-ready restained and co-registered mif and mihc dataset for head-and-neck squamous cell carcinoma patients. this dataset can be used for virtual phenotyping given standard clinical hematoxylin images, virtual clinical ihc dab generation with ground truth segmentations (to train highquality segmentation models across multiple cancer types) created from cleaner mif images, as well as for generating standardized clean mif images from neighboring h&e and ihc sections for registration and 3d reconstruction of tissue specimens. in the future, we will release similar datasets for additional cancer types as well as release for this dataset corresponding whole-cell segmentations via impartial https://github.com/nadeemlab/impartial."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Data use Declaration and Acknowledgment:,"this study is not human subjects research because it was a secondary analysis of results from biological specimens that were not collected for the purpose of the current study and for which the samples were fully anonymized. this work was supported by msk cancer center support grant/core grant (p30 ca008748) and by james and esther king biomedical research grant (7jk02) and moffitt merit society award to c. h. chung. it is also supported in part by the moffitt's total cancer care initiative, collaborative data services, biostatistics and bioinformatics, and tissue core facilities at the h. lee moffitt cancer center and research institute, an nci-designated comprehensive cancer center (p30-ca076292)."
Detection of Basal Cell Carcinoma in Whole Slide Images,1.0,Introduction,"skin cancer, the most prevalent cancer globally, has seen increasing incidences over recent decades  in this study, we utilized the nas approach to identify the optimal network for skin cancer detection. to improve the efficiency and accuracy of the search, we developed a new framework named sc-net, which focuses on identifying highly valuable architectures. we observed that conventional nas methods often overlook fairness ranking during the search, hindering the search for optimal solutions. our sc-net framework addresses this by ensuring fair training and precise ranking. the efficacy of sc-net was confirmed by our experimental results, with our resnet50 achieving 96.2% top-1 accuracy and 96.5% auc, outperforming baseline methods by 4.8% and 4.7% respectively. figure "
Detection of Basal Cell Carcinoma in Whole Slide Images,3.2,Performance Evaluation,"we validated our algorithm using the curated skin cancer dataset and sc-net as a supernet, testing both heavy and light models. we performed a search on resnet50 and mobilenetv2 models, compared against original resnet50 (ori resnet50) and mobilenetv2 (ori mobilenetv2) models as baselines. the resulting models are denoted as s resnet50 and s mobilenetv2. comparison with related methods. to ensure a fair comparison on our dataset, we selected several papers in the field of pathological image analysis, such as  evaluation metrics. our model was evaluated on: as shown in table "
Detection of Basal Cell Carcinoma in Whole Slide Images,4.0,Conclusion and Future Work,"in this paper, we introduce sc-net, a novel nas framework for skin cancer detection in pathology images. by formulating sc-net as a balanced supernet, we ensure fair ranking and treatment of all potential architectures. with scnet and evolutionary search, we obtained optimal architectures, achieving 96.2% top-1 and 96.5% accuracy on a skin cancer dataset, improvements of 4.8% and 4.7% over baselines. future work will apply our approach to larger datasets for wider-scale validation."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,3.1,Dataset,"we evaluate our model with three datasets. (1) luad-gm dataset: the objective is to predict the epidermal growth factor receptor (egfr) gene mutations in patients with lung adenocarcinoma (luad) using 723 whole slide image (wsi) slices, where 47% of cases have egfr mutations. (2) tcga-nsclc and tcga-rcc datasets: cancer type classification is performed using the cancer genome atlas (tcga) dataset. the tcga-nsclc dataset comprised two subtypes, lung squamous cell carcinoma (lusc) and lung adenocarcinoma (luad), while the tcga-rcc dataset included three subtypes: renal chromophobe cell carcinoma (kich), renal clear cell carcinoma (kirc), and renal papillary cell carcinoma (kirp)."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,1.0,Introduction,"histopathological images are regarded as the 'gold standard' in the diagnosis of cancers. with the advent of the whole slide image (wsi) scanner, deep learning has gained its reputation in the field of computational pathology  to address this issue, multiple instance learning (mil) has been successfully applied to the wsi classification task as a weakly supervised learning problem  recently, prototypical learning is applied in wsi analysis to identify representative instances in the bag  on the other hand, when pathologists analysis the wsis, they always observe the tissues at various resolutions  in this work, we propose a multi-scale prototypical transformer (mspt) for wsi classification. the mspt includes two key components: a prototypical transformer (pt) and a multi-scale feature fusion module (mffm). the specifically developed pt uses a clustering algorithm to extract instance prototypes from the bags, and then re-calibrates these prototypes at each scale with the self-attention mechanism in transformer  the contributions of this work are summarized as follows: 1) a novel prototypical transformer (pt) is proposed to learn superior prototype representation for wsi classification by integrating prototypical learning into the transformer architecture. it can effectively re-calibrate the cluster prototypes as well as reduce the computational complexity of the transformer. 2) a new multi-scale feature fusion module (mffm) is developed based on the mlp-mixer to enhance the information communication among phenotypes. it can effectively capture multi-scale information in wsi to improve the performance of wsi classification."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,2.2,Microvessel Infiltration Awareness (MIA),"we design a mia module to learn the infiltrative areas of microvessel. the tumors and margin depicted by ceus may be larger than those depicted by gray us because of continuous infiltrative expansion. inspired by the continuous infiltrative expansion, a series of flexible sigmoid alpha functions (saf) simulate the infiltrative expansion of microvessels by establishing the distance maps from the pixel to lesion boundary. here, the distance maps "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,1.0,Introduction,"the tumor microenvironment (tme) is comprised of cancer, immune (e.g. b lymphocytes, and t lymphocytes), stromal, and other cells together with noncellular tissue components "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.1,Dataset,"the cohort employed in this study was composed of pre-treatment tumor biopsy specimens from patients with nsclc from five centers (two centers for training (s t ) and three centers for independent validation (s v )). the entire analysis was carried out using 122 patients in experiment 1 (73 in s t , and 49 in s v ) and 135 patients in experiment 2 (81 in s t , and 54 in s v ). specimens were analyzed with a multiplexed quantitative immunofluorescence (qif) panel using the method described in "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,Results:,"the two top predictive triangil features were found to be the number of edges between stroma and cd4+ cells, and the number of edges between stroma and tumor cells with more interactions between stromal cells and both cd4+ and tumor cells being associated with response to io. this finding is concordant with other studies  result: figure "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,5.0,Concluding Remarks,"we presented a new approach, triangular analysis of geographical interplay of lymphocytes (triangil), to quantitatively chartacterize the spatial arrangement and relative geographical interplay of multiple cell families across pathological images. compared to previous spatial graph-based methods, triangil quantifies the spatial interplay between multiple cell families, providing a more comprehensive portrait of the tumor microenvironment. triangil was predictive of response after io (n = 122) and also demonstrated a strong correlation with os in nsclc patients treated with io (n = 135). triangil outperformed other graph-and dl-based approaches, with the added benefit of provoding interpretability with regard to the spatial interplay between cell families. for instance, triangil yielded the insight that more interactions between stromal cells and both cd4+ and tumor cells appears to be associated with better response to io. although five cell families were studies in this work, triangil is flexible and could include other cell types (e.g., macrophages). future work will entail larger validation studies and also evaluation on other use cases."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,1.0,Introduction,"histopathology relies on hematoxylin and eosin (h&e) stained biopsies for microscopic inspection to identify visual evidence of diseases. hematoxylin has a deep blue-purple color and stains acidic structures such as dna in cell nuclei. eosin, alternatively, is red-pink and stains nonspecific proteins in the cytoplasm and the stromal matrix. pathologists then examine highlighted tissue characteristics to diagnose diseases, including different cancers. a correct diagnosis, therefore, is dependent on the pathologist's training and prior exposure to a wide variety of disease subtypes "
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,1.0,Introduction,"radiotherapy, one of the mainstream treatments for cancer patients, has gained notable advancements in past decades. for promising curative effect, a high-quality radiotherapy plan is demanded to distribute sufficient dose of radiation to the planning target volume (ptv) while minimizing the radiation hazard to organs at risk (oars). to achieve this, radiotherapy plans need to be manually adjusted by the dosimetrists in a trial-and-error manner, which is extremely labor-intensive and time-consuming  recently, the blossom of deep learning (dl) has promoted the automatic medical image processing tasks  although the above methods have achieved good performance in predicting dose distribution, they suffer from the over-smoothing problem. these dl-based dose prediction methods always apply the l 1 or l 2 loss to guide the model optimization which calculates a posterior mean of the joint distribution between the predictions and the ground truth  in this paper, we investigate the feasibility of applying a diffusion model to the dose prediction task and propose a diffusion-based model, called diffdp, to automatically predict the clinically acceptable dose distribution for rectum cancer patients. specifically, the diffdp consists of a forward process and a reverse process. in the forward process, the model employs a markov chain to gradually transform dose distribution maps with complex distribution into gaussian distribution by progressively adding pre-defined noise. then, in the reverse process, given a pure gaussian noise, the model gradually removes the noise in multiple steps and finally outputs the predicted dose map. in this procedure, a noise predictor is trained to predict the noise added in the corresponding step of the forward process. to further ensure the accuracy of the predicted dose distribution for both the ptv and oars, we design a dl-based structure encoder to extract the anatomical information from the ct image and the segmentation masks of the ptv and oars. such anatomical information can indicate the structure and relative position of organs. by incorporating the anatomical information, the noise predictor can be aware of the dose constraints among ptv and oars, thus distributing more appropriate dose to them and generating more accurate dose distribution maps. overall, the contributions of this paper can be concluded as follows: "
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,3.0,Experiments and Results,"dataset and evaluations. we measure the performance of our model on an in-house rectum cancer dataset which contains 130 patients who underwent volumetric modulated arc therapy (vmat) treatment at west china hospital. concretely, for every patient, the ct images, ptv segmentation, oars segmentations, and the clinically planned dose distribution are included. additionally, there are four oars of rectum cancer containing the bladder, femoral head r, femoral head l, and small intestine. we randomly select 98 patients for model training, 10 patients for validation, and the remaining 22 patients for test. the thickness of the cts is 3 mm and all the images are resized to the resolution of 256 × 256 before the training procedure. we measure the performance of our proposed model with multiple metrics. considering dm represents the minimal absorbed dose covering m% percentage volume of ptv, we involve d 98 , d 2 , maximum dose (d max ), and mean dose (d mean ) as metrics. besides, the heterogeneity index (hi) is used to quantify dose heterogeneity  comparison with state-of-the-art methods. to verify the superior accuracy of our proposed model, we select multiple state-of-the-art (sota) models in dose prediction, containing unet (2017)  besides the quantitative results, we also present the dvh curves derived by compared methods in fig.  ablation study. to study the contributions of key components of the proposed method, we conduct the ablation experiments by 1) removing the structure encoder from the proposed method and concatenating the anatomical images x and noisy image y t together as the original input for diffusion model (denoted as baseline); 2) the proposed diffdp model. the quantitative results are given in table "
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,4.0,Conclusion,"in this paper, we introduce a novel diffusion-based dose prediction (diffdp) model for predicting the radiotherapy dose distribution of cancer patients. the proposed method involves a forward and a reverse process to generate accurate prediction by progressively transferring the gaussian noise into a dose distribution map. moreover, we propose a structure encoder to extract anatomical information from patient anatomy images and enable the model to concentrate on the dose constraints within several essential organs. extensive experiments on an in-house dataset with 130 rectum cancer patients demonstrate the superiority of our method."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,1.0,Introduction,"in the past few years, the development of histopathological whole slide image (wsi) analysis methods has dramatically contributed to the intelligent cancer diagnosis  generally, multiple instance learning (mil) is one of the most popular solutions for wsi analysis  however, hipt is a hierarchical learning framework based on a greedy training strategy. the bias and error generated in each level of the representation model will accumulate in the final decision model. moreover, the vit  in this paper, we propose a novel whole slide image representation learning framework named position-aware masked autoencoder (pama), which achieves slide-level representation learning by reconstructing the local representations of the wsi in the patch feature space. pama can be trained end-to-end from the local features to the wsi-level representation. moreover, we designed a position-aware cross-attention mechanism to guarantee the correlation of localto-global information in the wsis while saving computational resources. the proposed approach was evaluated on a public tcga-lung dataset and an in-house endometrial dataset and compared with 6 state-of-the-art methods. the results have demonstrated the effectiveness of the proposed method. the contribution of this paper can be summarized into three aspects. (1) we propose a novel whole slide image representation learning framework named position-aware masked autoencoder (pama). pama can make full use of abundant unlabeled wsis to learn discriminative wsi representations. (2) we propose a position-aware cross-attention (paca) module with a kernel reorientation (kro) strategy, which makes the framework able to maintain the spatial integrity and semantic enrichment of slide representation during the selfsupervised training. "
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,end end end,"tcga-lung dataset is collected from the cancer genome atlas (tcga) data portal. the dataset includes a total of 3,064 wsis, which consist of three categories, namely tumor-free (normal), lung adenocarcinoma (luad), and lung squamous cancer (lusc), endometrial dataset includes 3,654 wsis of endometrial pathology, which includes 8 categories, namely well/moderately/low-differentiated endometrioid adenocarcinoma, squamous differentiation carcinoma, plasmacytoid carcinoma, clear cell carcinoma, mixed-cell adenocarcinoma, and benign tumor. each dataset was randomly divided into training, validation and test sets according to 6:1:3 while keeping each category of data proportionally. we conducted wsi multi-type classification experiments on the two datasets. the validation set was used to perform an early stop. the results of the test set were reported for comparison."
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,1.0,Introduction,"cervical cancer is a common and severe disease that affects millions of women globally, particularly in developing countries  several computer-aided cervical cancer screening methods have been proposed for whole slide images (wsis) in the literature. most of them are detectionbased methods, which typically contain a detection model as well as some postprocessing modules in their frameworks. for instance, zhou et al.  some methods improve the final classification performance by improving the detection model to identify positive cells more reliably. cao et al.  these methods have achieved good results through continuous improvement on the detection-based pipeline, but there are some common drawbacks. first, they are not able to get rid of their reliance on detection models, which means they have a high need for expensive detection data labeling to train the detection model. cervical cancer cell detection datasets involve labeling individual and small bounding boxes in a large number of cells. it often requires multiple experienced pathologists to annotate  to address the aforementioned issues, we propose a detection-free pipeline in this paper, which does not rely on any detection model. instead, our pipeline requires only sample-level diagnosis labels, which are naturally available in clinical scenarios and thus get rid of additional image labeling. to attain this goal, we have designed a two-stage pipeline as in fig. "
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,3.0,Experiment and Results,"dataset and experimental setup. in this study, we have collected 5384 cervical cytopathological wsi by 20x lens, each with 20000 × 20000 pixels, from our collaborating hospitals. among them, there 2853 negative samples, and 2531 positive samples (962 ascus, and 1569 high-level positive samples). all wsis only have diagnosis labels at the sample level, without annotation boxes at the cell level. and all sample labels are strictly diagnosed according to the tbs  comparison to sota methods. in this section, we experiment to compare our method with popular state-of-the-art (sota) methods, which are all fully supervised and detection-based. to the best of our knowledge, there are few good methods to train cervical cancer classification models in weakly supervised or unsupervised learning ways. no methods can achieve the detection-free goal either. all the detection-based methods are evaluated in the following way. first, we label a dataset with cell-level bounding boxes to train a detection model. the detection dataset has 3761 images and 7623 cell-level annotations. after obtaining the suspicious cell patches provided by the detection model, we use the subsequent classification models used in these sota works to classify them and obtain the final classification results. as shown in table  ablation study. in this section, we experiment to demonstrate the effectiveness of all the proposed parts in our pipeline. we divide all 5384 samples into five independent parts for five-fold cross-validation, and the results are shown in table "
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,1.0,Introduction,"colorectal cancer is a prevalent form of cancer characterized by colorectal adenocarcinoma, which develops in the colon or rectum's inner lining and exhibits glandular structures  recently, diffusion model  in this paper, we propose a new method for gland instance segmentation based on the diffusion model. (1) our method utilizes a diffusion model to perform denoising and tackle the task of gland instance segmentation in histology images. the noise boxes are generated from gaussian noise, and the predicted ground truth (gt) boxes and segmentation masks are performed during the diffusion process. (2) to improve segmentation, we use instance-aware techniques to recover lost details during denoising. this includes employing a filter and a multi-scale mask branch to create a global mask and refine finer segmentation details. (3) to enhance object-background differentiation, we utilize conditional encoding to augment intermediate features with the original image encoding. this method effectively integrates the abundant information from the original image, thereby enhancing the distinction between the objects and the surrounding background. our proposed method was trained and tested on the 2015 mic-cai gland segmentation (glas) challenge dataset "
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,1.0,Introduction,"immunohistochemical (ihc) staining is a widely used technique in pathology for visualizing abnormal cells that are often found in tumors. ihc chromogens highlight the presence of certain antigens or proteins by staining their corresponding antibodies. for instance, the her2 (human epidermal growth factor receptor 2) biomarker is associated with aggressive breast tumor development and is essential in forming a precise treatment plan. despite its capability to provide highly valuable diagnostic information, the process of ihc staining is very labor-intensive, time-consuming and requires specialized histotechnologists and laboratory equipments  at the other end of the spectrum, h&e (hematoxylin and eosin) staining, as the gold standard in histological staining, highlights the tissue structures and cell morphology. in routine diagnostics, on account of its much lower cost, an h&e-stained slide is prepared by pathologists in order to determine whether or not to also apply the ihc stains for a more precise assessment of the disease. therefore, it is of great interest to have an algorithm that can automatically translate an h&e-stained slide into one that could be considered to have been stained with ihc while accurately predicting the target expression levels. to that end, researchers have recently proposed to use gan-based image-to-image translation (i2it) algorithms for transforming h&e-stained slides into ihc. despite the progress, the outstanding challenge in training such i2it frameworks is the lack of aligned h&e-ihc image pairs, or in other words, the inconsistencies in the h&e-ihc groundtruth pairs. to explain, since re-staining a slice is physically infeasible, a matching pair of h&e-ihc slices are taken from two depth-wise consecutive cuts of the same tissue then stained and scanned separately. this inevitably prevents pixel-perfect image correspondences due to the slice-to-slice changes in cell morphology, staining-induced degradation (e.g. tissue-tearing), imaging artifacts that may vary among slices (e.g. camera out-offocus) and multi-slice registration errors. an example pair of patches is shown in fig.  as a result, recent advances in h&e-to-ihc i2it have mostly avoided using the inconsistent gt pairs and instead have imposed the cycle-consistency constraint  in this paper, we argue that the ihc slides, despite the disparities vis-a-vis their h&e counterparts, can still serve as useful targets for stain translation. the work we present in this paper is based on the important realization that even when pairs of consecutive tissue slices do not yield images that are pixel-perfect aligned, it is highly likely that the corresponding patches in the two stains share the same diagnostic label. for example, if the levels of expression in a region of the her2 slide are high, the corresponding region in the h&e slide is highly likely to contain a high density of cancerous cells. therefore, we set our goal to meaningfully leverage such correlations to benefit the h&e-to-ihc i2it while being resilient to any inconsistencies. toward this goal, we propose a supervised patchwise contrastive loss named the adaptive supervised patchnce (asp) loss. our formulation of this loss was inspired by the recent research findings that contrastive loss benefits model robustness under label noise  lastly, to support further research in virtual ihc-restaining, we present the multi-ihc stain translation (mist) as a new public dataset. the mist dataset contains 4k+ training and 1k testing aligned h&e-ihc patches for each of the following ihc stains that are critical for breast cancer diagnostics: her2, ki67, er (estrogen receptor) and pr (progesterone receptor). we evaluated existing i2it methods and ours for multiple ihc stains and demonstrate the superior performance achieved by our method both qualitatively and quantitatively. "
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,3.0,Experiments,"datasets. the following datasets are used in our experiments: the breast cancer immunohistochemical (bci) challenge dataset  implementation details. for all of our models, we used resnet-6blocks as the generator and a 5-layer patchgan as the discriminator. we trained our networks with random 512 × 512 crops and a batch size of one. the adam optimizer  evaluation metrics. we compare the methods using both paired and unpaired evaluation metrics. to compare a pair of images, generated and groundtruth, we use the standard ssim (structural similarity index measure) and phv (perceptual hash value) as described in  qualitative evaluations. in fig. "
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,1.0,Introduction,"histopathology is considered the gold standard for diagnosing and treating many cancers  different resolution levels in the wsi pyramids contain different and complementary information  in this paper, we present a novel hierarchical interaction graph-transformer framework (i.e., higt) to simultaneously capture both local and global information from wsi pyramids with a novel bidirectional interaction module. specifically, we abstract the multi-resolution wsi pyramid as a heterogeneous hierarchical graph and devise a hierarchical interaction graph-transformer architecture to learn both short-range and long-range correlations among different image patches within different resolutions. considering that the information from different resolutions is complementary and can benefit each other, we specially design a bidirectional interaction block in our hierarchical interaction vit mod- ule to establish communication between different resolution levels. moreover, a fusion block is proposed to aggregate features learned from the different levels for slide-level prediction. to reduce the tremendous computation and memory cost, we further adopt the efficient pooling operation after the hierarchical gnn part to reduce the number of tokens and introduce the separable self-attention mechanism in hierarchical interaction vit modules to reduce the computation burden. the extensive experiments with promising results on two public wsi datasets from tcga projects, i.e., kidney carcinoma (kica) and esophageal carcinoma (esca), validate the effectiveness and efficiency of our framework on both tumor subtyping and staging tasks. the codes are available at https:// github.com/hku-medai/higt."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,3.0,Experiments,"datasets and evaluation metrics. we assess the efficacy of the proposed higt framework by testing it on two publicly available datasets (kica and esca) from the cancer genome atlas (tcga) repository. the datasets are described below in more detail: -kica dataset. the kica dataset consists of 371 cases of kidney carcinoma, of which 279 are classified as early-stage and 92 as late-stage. for the tumor typing task, 259 cases are diagnosed as kidney renal papillary cell carcinoma, while 112 cases are diagnosed as kidney chromophobe. -esca dataset. the esca dataset comprises 161 cases of esophageal carcinoma, with 96 cases classified as early-stage and 65 as late-stage. for the tumor typing task, there are 67 squamous cell carcinoma cases and 94 adenocarcinoma cases. experimental setup. the proposed framework was implemented by pytorch  ablation analysis. we further conduct an ablation study to demonstrate the effectiveness of the proposed components. the results are shown in table "
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,1.0,Introduction,"colorectal cancer is the third most common malignant tumor, and nearly half of all patients with colorectal cancer develop liver metastasis during the course of the disease  extensive existing works have demonstrated the power of deep learning on various spatial-temporal data, and can potentially be applied towards the problem of crlm. for example, originally designed for natural data, several mainstream models such as e3d-lstm  however, all these methods have only demonstrated their effectiveness towards 3d/4d data (i.e., time-series 2d/3d images), and it is not clear how to best extend them to work with the 5d cect data. part of the reason is due to the lack of public availability of such data. when extending these models towards 5d cect data, some decisions need to be made, for example: 1) what is the most effective way to incorporate the phase information? simply concatenating different phases together may not be the optimal choice, because the positional information of the same ct slice in different phases would be lost. 2) shall we use uni-directional lstm or bi-direction lstm? e3d-lstm  in this paper, we investigate how state-of-art deep learning models can be applied to the crlm prediction task using our 5d cect dataset. we evaluate the effectiveness of bi-directional lstm and explore the possible method of incorporating different phases in the cect dataset. specifically, we show that the best prediction accuracy can be achieved by enhancing e3d-lstm "
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,2.1,Dataset,"our dataset follows specific inclusion criteria: -no tumor appears on the ct scans. that means patients have not been diagnosed as crlm when they took the scans. -patients were previously diagnosed with colorectal cancer tnm stage i to stage iii, and recovered from colorectal radical surgery. -patients have two or more times of cect scans. -we already determined whether or not the patients had liver metastases within 2 years after the surgery, and manually labeled the dataset based on this. -no potential focal infection in the liver before the colorectal radical surgery. -no metastases in other organs before the liver metastases. -no other malignant tumors. our retrospective dataset includes two cohorts from two hospitals. the first cohort consists of 201 patients and the second cohort includes 68 patients. each scan contains three phases and 100 to 200 ct slices with a resolution of 512×512. patients may have different numbers of ct scans, ranging from 2 to 6, depending on the number of follow-up visits. ct images are collected with the following acquisition parameters: window width 150, window level 50, radiation dose 120 kv, slice thickness 1 mm, and slice gap 0.8 mm. all images underwent manual quality control to exclude any scans with noticeable artifacts or blurriness and to verify the completeness of all slices. additional statistics on our dataset are presented in table "
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,1.0,Introduction,accurate diagnosis plays an important role in achieving the best treatment outcomes for people with cancer  the introduction of digital pathology (dp) has enabled application of machine learning approaches to extract otherwise inaccessible diagnostic and prognostic information from h&e-stained whole slide images (wsis) 
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,3.1,Data,"we tested our feature representation method in several classification tasks involving wsis of h&e-stained histopathology slides. the number of slides per class for each classification task are shown in fig.  for these two tasks we used artifact-free tiles from tumor regions detected with an in-house tumor detection model. for breast cancer metastasis detection in lymph node tissue, we used wsis of h&estained healthy lymph node tissue and lymph node tissue with breast cancer metastases from the publicly available camelyon16 challenge data set  for cell of origin (coo) prediction of activated b-cell like (abc) or germinal center b-cell like (gcb) tumors in diffuse large b-cell lymphoma (dlbcl), we used data from the phase 3 goya (nct01287741) and phase 2 cavalli (nct02055820) clinical trials, hereafter referred to as ct1 and ct2, respectively. all slides were h&e-stained and scanned using ventana dp200 scanners at 40× magnification. ct1 was used for training and testing the classifier and ct2 was used only as an independent holdout data set. for these data sets we used artifact-free tiles from regions annotated by expert pathologists to contain tumor tissue."
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Cellular Explainability Method. The cellular average embedding is,"where e ij ∈ r 256 is the cellular embedding extracted from every detected cell in the tile j i ∈ 1, 2, . . . , n j where n j is the number of cells in the tile j. this can be rewritten as a weighted average of the cellular embeddings where w i ∈ r 256 are the per cell attention weights that if initialized to 0 result in the original cellular average embedding. the re-formulation does not change the result of the forward pass since w i are not all equal. note that the weights are not learned through training but calculated per cell at inference time to get the per cell contribution. we computed the gradient of the output category (of the classification method applied on top of the computed embedding) with respect to the attention weights w i : grad i = ∂score i /∂w i and visualized cells that received positive and negative gradients using different colors. visual example results. examples of our cellular explainability method applied to weakly supervised tumor detection on wsis from the camelyon16 data set using a-mil are shown in fig.  in this case, cells with positive attention gradients that shifted the output towards a classification of gcb were labeled green and cells with negative attention gradients that shifted the classification towards abc were labeled red. cells with positive attention gradients were mostly smaller lymphoid cells with low grade morphology or were normal lymphocytes, whereas cells with negative attention gradients were more frequently larger lymphoid cells with high grade morphology (fig. "
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,5.0,Conclusions,"we describe a method to capture both cellular and texture feature representations from wsis that can be plugged into any mil architecture (e.g., cnn or xformer-based), as well as into fully supervised models (e.g., tile classification models). our method is more flexible than other methods (e.g., hierarchical image pyramid transformer) that usually capture the hierarchical structure in wsis by aggregating features at multiple levels in a complex set of steps to perform the final classification task. in addition, we describe a method to explain the output of the classification model that evaluates the contributions of histologically identifiable cells to the slide-level classification. tilelevel embeddings result in good performance for detection of tumor metastases in lymph nodes. however, introducing more cell-level information, using combined embeddings, resulted in improved classification performance. in her2 and er prediction tasks for breast cancer we demonstrate that addition of a cell-level embedding summary to tilelevel embeddings can boost model performance by up to 8%. finally, for coo prediction in dlbcl and breast cancer metastasis detection in lymph nodes, we demonstrated the potential of our explainability method to gain insights into previously unknown associations between cellular morphology and disease biology."
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,2.2,Transformer with Skip Self-Attention (SSA),"we design a novel skip self-attention (ssa) module to fuse discriminative features of candida from different scales. at a fine-grained level, the hyphae and spores of candida are usually the basis for judging. yet we need to distinguish them from easily distorting factors such as contaminants in wsis and edges of nearby cells. at a coarse-grained level, there is the phenomenon that a candida usually links multiple host cells and yields a string of them. thus it is necessary to combine long-range visual cues that span several cells to derive the decision related to candida. cnn-based methods have achieved excellent performance in computer-aided diagnosis including cervical cancer "
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,1.0,Introduction,"cervical cancer accounts for 6.6% of the total cancer deaths in females worldwide, making it a global threat to healthcare  nowadays, thin-prep cytologic test (tct)  after scanning whole-slide images (wsis) from tct samples, automatic tct screening is highly desired due to the large population versus the limited number of pathologists. as the wsi data per sample has a huge size, the idea of identifying abnormal cells in a hierarchical manner has been proposed and investigated by several studies using deep learning  to alleviate the shortage of sufficient data to supervise classification, one may adopt traditional data augmentation techniques, which yet may bring little improvement due to scarcely expanded data diversity  aiming at augmenting the performance of cervical abnormality screening, we develop a novel conditional generative adversarial network in this paper, namely cellgan, to synthesize cytopathological images for various cell types. we leverage fastgan "
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,1.0,Introduction,"thyroid cancer is the most common cancer of the endocrine system, accounting for 2.1% of all malignant cancers  however, nuclei segmentation in thyroid cytopathology is still challenged by the varying cellularity of images from different tbsrtc categories  to narrow the gap discussed, we propose a novel tbsrtc-category-aware nuclei segmentation framework. our contributions are three-fold. "
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,1.0,Introduction,"breast cancer (bc) is one of the most common malignant tumors in women worldwide and it causes nearly 0.7 million deaths in 2020  recently, with the development of transformer, multi-modal pre-training has achieved great success in the fields of computer vision (cv) and natural language processing (nlp). according to the data format, there are two main multi-modal pre-training approaches, as shown in fig.  in this paper, we propose a multi-modal pre-training method based on masked autoencoders for bc downstream tasks. our model consists of three parts, i.e., the modal-fusion encoder, the mixed attention, and the modal-specific decoder. we choose paired h&e and ihc (only her2) staining images, which are cropped into non-overlapped patches as the input of our model. we randomly mask some patches by a ratio and feed the remaining patches into the modalfusion encoder to get corresponding tokens. then the mixed attention module is used to take the intra-modal and inter-modal correlation into account. finally, we use modal-specific decoders to reconstruct the original h&e and ihc staining images respectively. our contributions are summarized as follows: we propose a multi-modal pre-training via masked autoencoders mmp-mae for bc diagnosis. to our best knowledge, this is the first pre-training work based on multi-modal pathological data. we evaluate the proposed method on two public datasets as herohe challenge and bci challenge, which shows that our method achieves state-of-theart performance. i=1 and {yi} λ 2 n i=1 into the modal-fusion encoder to extract the patch tokens {fi} λ 1 n i=1 and {gi} λ 2 n i=1 . then we use intra-modal attention and inter-modal attention to take patch correlation into account. x and y are reconstructed by modal-specific decoders respectively."
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,3.1,Datasets,acrobat challenge. the automatic registration of breast cancer tissue (acrobat) challenge  bci challenge. breast cancer immunohistochemical image generation challenge 
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,1.0,Introduction,"automatic segmentation of tumor lesions from pathological images plays an important role in accurate diagnosis and quantitative evaluation of cancers. recently, deep learning has achieved remarkable performance in pathological image segmentation when trained with a large and well-annotated dataset  semi-supervised learning (ssl) is a potential technique to reduce the annotation cost via learning from a limited number of labeled data along with a large amount of unlabeled data. existing ssl methods can be roughly divided into two categories: consistency-based  in this work, we propose a novel and efficient method based on cross distillation with multiple attentions (cdma) for semi-supervised pathological image segmentation. firstly, a multi-attention tri-branch network (mtnet) is proposed to efficiently obtain diverse outputs for a given input. unlike mc-net+  the contribution of this work is three-fold: 1) a novel framework named cdma based on mtnet is introduced for semi-supervised pathological image segmentation, which leverages different attention mechanisms for generating diverse and complementary predictions for unlabeled images; 2) a cross decoder knowledge distillation method is proposed for robust and efficient learning from noisy pseudo labels, which is combined with an average prediction-based uncertainty minimization to improve the model's performance; 3) experimental results show that the proposed cdma outperforms eight state-of-the-art ssl methods on the public digestpath dataset "
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,1.0,Introduction,"automatic identification of lesions from dermoscopic images is of great importance for the diagnosis of skin cancer  one approach to address the above problem is novel class discovery (ncd)  in this paper, we propose a new novel class discovery framework to automatically discover novel disease categories. specifically, we first use contrastive learning to pretrain the model based on all data from known and unknown categories to learn a robust and general semantic feature representation. then, we propose an uncertainty-aware multi-view cross-pseudo-supervision strategy to perform clustering. it first uses a self-labeling strategy to generate pseudo-labels for unknown categories, which can be treated homogeneously with ground truth labels. the cross-pseudo-supervision strategy is then used to force the model to maintain consistent prediction outputs for different views of unlabeled images. in addition, we propose to use prediction uncertainty to adaptively adjust the contribution of the pseudo labels to mitigate the effects of noisy pseudo labels. finally, to encourage local neighborhood alignment and further refine the pseudo labels, we propose a local information aggregation module to aggregate the information of the neighborhood samples to boost the clustering performance. we conducted extensive experiments on the dermoscopy dataset isic 2019, and the experimental results show that our method outperforms other state-of-the-art comparison algorithms by a large margin. in addition, we also validated the effectiveness of different components through extensive ablation experiments."
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,1.0,Introduction,"cancers are a group of heterogeneous diseases reflecting deep interactions between pathological and genomics variants in tumor tissue environments  the major goal of multimodal data learning is to extract complementary contextual information across modalities  to tackle above challenges, we propose a pathology-and-genomics multimodal framework (i.e., pathomics) for survival prediction (fig. "
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,3.0,Experiments and Results,"datasets. all image and genomics data are publicly available. we collected wsis from the cancer genome atlas colon adenocarcinoma (tcga-coad) dataset (cc-by-3.0)  experimental settings and implementations. we implement two types of settings that involve internal and external datasets for model pretraining and finetuning. as shown in fig  the number of epochs for pretraining and finetuning are 25, the batch size is 1, the optimizer is adam "
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,4.0,Conclusion,"developing data-efficient multimodal learning is crucial to advance the survival assessment of cancer patients in a variety of clinical data scenarios. we demonstrated that the proposed pathomics framework is useful for improving the survival prediction of colon and rectum cancer patients. importantly, our approach opens up perspectives for exploring the key insights of intrinsic genotypephenotype interactions in complex cancer data across modalities. our finetuning"
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,1.0,Introduction,"cervical cancer is the second most common cancer among adult women. if diagnosed early, it can be effectively treated and cured  with the development of deep learning  although the above-mentioned attempts can improve the screening performance significantly, there are several issues that need to be addressed: 1) object detection methods often require accurate annotated data to guarantee performance with robustness and generalization. however, due to legal limitations, the scarcity of positive samples, and especially the subjectivity differences between cytopathologists for manual annotations  to address these issues, we propose a novel method for cervical abnormal cell detection using distillation from local-scale consistency refinement. inspired by knowledge distillation, we construct a pre-trained patch correction network (pcn), which is designed to exploit the supervised information from the pcn to reduce the impact of noisy labels and utilize the contextual relationships between cells. in our approach, we begin by utilizing retinanet "
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,4.0,Discussion,"in this work, we proposed and examined novel data augmentation strategies based on the idea of interpolations of feature vectors in the mil setting. instance-based mil did not show any competitive scores. obviously the model reducing each patch to a single value is not adequate for the classification of frozen or paraffin sections from thyroid cancer tissues. the considered dual-stream approach, including an embedding and instance-based stream, exhibited slightly improved average scores, compared to embedding-based mil only. in our analysis, we focused on the embedding-based configuration and on the balanced combined approach (referred to as 2/2). with the baseline data augmentation approaches, the maximum improvements were 0.03, and 0.02 for the frozen, and 0.01, and 0.05 for the paraffin data set. the inter-mixup approach did not show any systematic improvements. independently of the chosen strategy (v1, v2), concerning the combination within or between classes, we did not notice any positive trend. the multilinear intra-mixup method, however, exhibited the best scores for 3 out of 4 combinations and the best overall mean accuracy for both, the frozen and the paraffin data set. also a clear trend with increasing scores in the case of an increasing ratio of augmented data (β) is visible. the linear method showed a similar, but less pronounced trend. obviously, the straightforward application of the mixup scheme (as in case of the inter-mixup approach), is inappropriate for the considered setting. an inhibiting factor could be a high inter-wsi variability leading to incompatible feature vectors (which are too far away from realistic samples in the feature space). to particularly investigate this effect, we performed 2 different inter-mixup settings (v1 & v2), with the goal of identifying the effect of mixed (and thereby more dissimilar) or similar classes during interpolation. the analysis of the distance distributions between patch representations confirmed that, the variability between wsis is clearly larger than the variability within wsis. in addition, the results showed that the variability between classes is, on patch-level, not clearly larger than the variability within a class. obviously variability due to the acquisition outweigh any disease specific variability. this could provide an explanation for the effectiveness of intra-mixup approach compared to the (similarly) poorly performing inter-mixup settings. we expect that stain normalization methods (but not stain augmentation) could be utilized to align the different wsis to provide a more appropriate basis for inter-wsi interpolation. with regard to the different data sets, we noticed a stronger, positive effect in case of the frozen section data set. this is supposed to be due to the clearly higher variability of the frozen sections corresponding with a need for a higher variability in the training data. we also noticed a stronger effect of the solely embedding-based architecture (also showing the best overall scores). we suppose that this is due to the fact that the additional loss of the dual-stream architecture exhibits a valuable regularization tool to reduce the amount of needed training data. with the proposed intra-mixup augmentation strategy, this effect diminishes, since the amount and quality of training data is increased. to conclude, we proposed novel data augmentation strategies based on the idea of interpolations of image descriptors in the mil setting. based on the experimental results, the multilinear intra-mixup setting proved to be highly effective, while the inter-mixup method showed inferior scores compared to a state-of-the-art baseline. we learned that there is a clear difference between combinations within and between wsis with a noticeable effect on the final classification accuracy. this is supposedly due to the high variability between the wsis compared to a rather low variability within the wsis. in the future, additional experiments will be conducted including stain normalization methods and larger benchmark data sets to provide further insights."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,1.0,Introduction,"breast cancer (bc) is the most common cancer diagnosed among females and the second leading cause of cancer death among women after lung cancer  among different types of imaging biomarkers, histopathological images are generally considered the golden standard for bc prognosis since they can confer important cell-level information that can reflect the aggressiveness of bc  to deal with the above challenges, several researchers began to design domain adaption algorithms, which utilize the labeled data from a related cancer subtype to help predict the patients' survival in the target domain. specifically, alirezazadeh et al  although much progress has been achieved, most of the existing studies applied the feature alignment strategy to reduce the distribution difference between source and target domains. however, such transfer learning methods neglected to take the interaction among different types of tissues into consideration. for example, it is widely recognized that tumor-infiltrating lymphocytes (tils) and its correlation with tumors reveal a similar role in the prognosis of different brca subtypes. for instance, kurozumi et al  based on the above considerations, in this paper, we proposed a tils-tumor interactions guided unsupervised domain adaptation (t2uda) algorithm to predict the patients' survival on the target bc subtype. specifically, t2uda first applied the graph attention network (gats) to learn node embeddings and the spatial interactions between tumor and tils patches in wsi. in order to preserve the node-level and interaction-level similarities across different domains, we not only aligned the embedding for different types of nodes but also designed a novel tumor-tils interaction alignment (ttia) module to ensure that the distribution of the interaction weights are similar in both domains. we evaluated the performance of our method on the breast invasive carcinoma (brca) cohort derived from the cancer genome atlas (tcga), and the experimental results indicated that t2uda outperforms other domain adaption methods for predicting patients' clinical outcomes."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Calculating TILs-Tumor Interaction via Graph Attention Networks(GATs).,"to characterize the interaction between different tils and tumor patches, we employed gat  we calculated the attention coefficients among different nodes, which can be formulated as: furthermore, a softmax function was then adopted to normalize the attention coefficients e ij : where n i represents all neighbors of node i. the new feature vector v i for node i was calculated via a weighted sum: finally, the output features of each gat layer were aggregated in the readout layer. we fed the generated output features from each readout layer into the cox hazard proportional regression model for the final prognosis predictions. feature alignment. in the proposed gat-based transfer learning framework, the feature alignment component was employed on its first two layers. then, for the node embeddings with different types (tils and tumor) in both the source and target domain, we performed a mean pooling operation to obtain their aggregated features. next, we aligned the aggregated tumor or tils features from the two domains separately using maximum mean discrepancy(mmd)  here, we adopted mmd for feature alignment due to its ability to measure the distance between two distributions without explicit assumptions on the data distribution, we showed the objective function of mmd in our method as follows: where h is a hilbert space, f represents the features from the source, f represents the feature from the target, r represents the layer number, k ∈ {l, t } referred to tils or tumor node. in addition, n denotes the number of source samples, while m refers to the number of target samples.  tils-tumor interaction alignment. to accurately characterize the interaction between tils and tumors, we further analyzed the extracted interaction weights by dividing them into 10 intervals (i.e., bins). for each interval, we calculated the sum of all source domain interaction weights as i s k and the sum of all target domain interaction weights as i t k , where k represents the k-th interval. consequently, we obtained two vectors and applied softmax on each of them for normalization that can be denoted as . in order to measure the dissimilarity between p i and q i , the kullback-leibler (kl) divergence is adapted on the third layer of gat, which can be formulated as: according to eq.( "
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3.0,Experiments and Results,"datasets. we conducted our experiments on the breast invasive carcinoma (brca) dataset from the cancer genome atlas (tcga). specifically, the brca dataset includes 661 patients with hematoxylin and eosin (he)-stained pathological imaging and corresponding survival information. among the collected brca patients in tcga, the number of er positive(er+) and er negative(er-) patients are 515 and 146, respectively. we hope to investigate if the proposed t2uda could be used to help improve the prognosis performance of (er+) or (er-) with the aid of the survival information on its counterpart."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3.2,Result and Discussion,"in this study, we compared the performance of our proposed model with several existing domain adaptation methods, including 1) ddc  the results presented in table  we also evaluated the contributions of the key components of our framework and found that t2uda performed better than source only and t2uda-v1, which shows the advantage of minimizing differences in tils-tumor interaction weights. in addition, we also evaluated the patient stratification performance of different methods. as shown in fig.  we also examined the consistency of important edges in each group of stratified patients based on the tils-tumor interaction weights calculated by the gat-based framework in the source and target domains. as seen in fig. "
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,4.0,Conclusion,"in this paper, we presented an unsupervised domain adaptation algorithm that leverages tils-tumor interactions to predict patients' survival in a target bc subtype(t2uda). our results demonstrated that the relationship between tils and tumors is transferable and can be effectively used to improve the accuracy of survival prediction models. to the best of our knowledge, t2uda was the first method to successfully achieve interrelationship transfer between tils and tumors across different cancer subtypes for prognosis tasks."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,1.0,Introduction,"pathological image-omic analysis is the cornerstone of modern medicine and demonstrates promise in a variety of different tasks such as cancer diagnosis and prognosis  though deep learning techniques have revolutionized medical imaging, designing a task-specific algorithm for image-omic multi-modality analysis is challenging.  specifically, to our knowledge, most multi-modality techniques have been designed for modalities such as chest x-ray and reports  in this paper, we propose a task-specific framework dubbed gene-induced multimodal pre-training (gimp) for image-omic classification. concretely, we first propose a transformer-based gene encoder, group multi-head self attention (groupmsa), to capture global structured features in gene expression cohorts. next, we design a pre-training paradigm for wsis, masked patch modeling (mpm), masking random patch embeddings from a fixed-length contiguous subsequence of a wsi. we assume that one patch-level feature embedding can be reconstructed by its adjacent patches, and this process enhances the learning ability for pathological characteristics of different tissues. our mpm only needs to recover the masked patch embeddings in a fixed-length subsequence rather than processing all patches from wsis. furthermore, to model the high-order relevance of the two modalities, we combine cls tokens of paired image and genomic data to form unified representations and propose a triplet learning module to differentiate patient-level positive and negative samples in a mini-batch. it is worth mentioning that although our unified representation fuses features from the whole gene expression cohort and partial wsis in a mini-batch, we can still learn high-order relevance and discriminative patient-level information between these two modalities in pre-training thanks to the triplet learning module. in addition, note that our proposed method is different from self-supervised pre-training. specifically, we focus not only on superior representation learning capability, but also category-related feature distributions, w.r.t. intra-and inter-class variation. with the training process going on, complete information from wsis can be integrated and the fused multimodal representations with high discrimination will make it easier for the classifier to find the classification hyperplane. experimental results demonstrate that our gimp achieves significant improvement in accuracy than other image-omic competitors, and our multimodal framework shows competitive performance even without pre-training."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.1,Experimental Setup,"datasets. we verify the effectiveness of our method on the caner genome atlas (tcga) non-small cell lung cancer (nsclc) dataset, which contains two cancer subtypes, i.e., lung squamous cell carcinoma (lusc) and lung adenocarcinoma (luad). after pre-processing  implementation details. the pre-training process of all algorithms is conducted on the training set, without any extra data augmentation. note that our genetic encoder, groupmsa, is fully supervised pre-trained on unimodal genetic data to accelerate convergence and it is frozen during gimp training process. the maximum pre-training epoch for all methods is set to 100 and we finetune the models at the last epoch. during fine-tuning, we evaluate the model on the validation set after every epoch, and save the parameters when it performs the best. adamw "
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.1,Datasets,"we tested our proposed method on two different tasks: (1) intrahepatic cholangiocarcinomas(ihccs) subtype classification and (2) liver cancer type classification. the dataset for the former task was collected from 168 patients with 332 wsis from seoul national university hospital. ihccs can be further categorized into small duct type (sdt) and large duct type (ldt). using gene mutation information as prior knowledge, we collected wsis with wild kras and mutated idh genes for use as training samples in sdt, and wsis with mutated kras and wild idh genes for use in ldt. the rest of the wsis were used as testing samples. the liver cancer dataset for the latter task was composed of 323 wsis, in which the wsis can be further classified into hepatocellular carcinomas (hccs) (collected from pathology ai platform "
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.2,Implementation Detail,"we used a pre-trained vgg16 with imagenet as the initial encoder, which was further modified via deep manifold model training using the proposed manifold and cross-entropy loss functions. the number of nearest neighbors k and the number of sub-classes n were set to 5 and 10, respectively. in the deep manifold embedding learning model, the learning rates were set to 1e-4 with a decay rate of 1e-6 for the ihccs subtype classification and to 1e-5 with a decay rate of 1e-8 for the liver cancer type classification. the k-nearest neighbors graph and the geodesic distance matrix are updated once every five training epochs, which is empirically chosen to balance running time and accuracy. to train the mil classifier, we set the learning rate to 1e-3 and the decay rate to 1e-6. we used batch sizes 64 and 4 for training the deep manifold embedding learning model and the mil classification model, respectively. the number of epochs for the deep manifold embedding learning model was 50, while 50 and 200 epochs for the ihccs subtype classification and liver cancer type classification, respectively. as for the optimizer, we used stochastic gradient decay for both stages. the result shown in the tables is the average result from 10 iterations of the mil classification model."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,1.0,Introduction,"prostate cancer (pca) diagnosis and grading rely on histopathology analysis of biopsy slides  field effect refers to the spread of genetic and epigenetic alterations from a primary tumor site to surrounding normal tissues, leading to the formation of secondary tumors. understanding field effect is essential for cancer research as it provides insights into the mechanisms underlying tumor development and progression. tumor-associated stroma, which consists of various cell types, such as fibroblasts, smooth muscle cells, and nerve cells, is an integral component of the tumor microenvironment that plays a critical role in tumor development and progression. reactive stroma, a distinct phenotype of stromal cells, arises in response to signaling pathways from cancerous cells and is characterized by altered stromal cells and increased extracellular matrix components  manual review for tumor-associated stroma is time-consuming and lacks quantitative metrics  analyzing tumor-associated stroma in prostate cancer requires combining whole-mount and biopsy histopathology slides. biopsy slides provide information on the presence of pca, while whole-mount slides provide information on the extent and distribution of pca, including more information on tumor-associated stroma. combining the information from both modalities can provide a more accurate understanding of the tumor microenvironment. in this work, we explore the field effect in prostate cancer by analyzing tumor-associated stroma in multimodal histopathological images. our main contributions can be summarized as follows: -to the best of our knowledge, we present the first deep-learning approach to characterize prostate tumor-associated stroma by integrating histological image analysis from both whole-mount and biopsy slides. our research offers a promising computational framework for in-depth exploration of the field effect and cancer progression in prostate cancer. -we proposed a novel approach for stroma classification with spatial graphs modeling, which enable more accurate and efficient analysis of tumor microenvironment in prostate cancer pathology. given the spatial nature of cancer field effect and tumor microenvironment, our graph-based method offers valuable insights into stroma region analysis. -we developed a comprehensive pipeline for constructing tumor-associated stroma datasets across multiple data sources, and employed adversarial training and neighborhood consistency regularization techniques to learn robust multimodal-invariant image representations."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.1,Stroma Tissue Segmentation,"accurately analyzing tumor-associated stroma requires a critical pre-processing step of segmenting stromal tissue from the background, including epithelial tissue. this segmentation task is challenging due to the complex and heterogeneous appearance of the stroma. to address this, we propose utilizing the pointrend model "
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.2,Stroma Classification with Spatial Patch Graphs,"to capture the spatial nature of field effect and analyze tumor-associated stroma, modeling spatial relationships between stroma patches is essential. the spatial relationship can reveal valuable information about the tumor microenvironment, and neighboring stroma cells can undergo similar phenotypic changes in response to cancer. therefore, we propose using a spatial patch graph to capture the highorder relationship among stroma tissue regions. we construct the stroma patch graph using a k-nearest neighbor (knn) graph and neighbor sampling. the knn graph connects each stroma patch to its k nearest neighboring patches. given a central stroma patch, we iteratively add neighboring patches to construct  the patch graph until we reach a specified layer number l to control the subgraph size. this process results in a tree-like subgraph with each layer representing a different level of spatial proximity to the central patch. the use of neighbor sampling enables efficient processing of large images and allows for stochastic training of the model. to predict tumor-associated binary labels of stroma patches, we employ a message-passing approach that propagates patch features in the spatial graph. to achieve this, we use graph convolutional networks with attention, also known as graph attention networks (gats)  where w ∈ r m ×n is a learnable matrix transforming n -dimensional features to m -dimensional features. n e vi is the neighborhood of the node v i connected by e in g. gat uses attention mechanism to construct the weighting coefficients as: where t represents transposition, is the concatenation operation, and ρ is leakyrelu function. the final output of gat module is the tumor-associated probability of the input patch. and the module was optimized using the crossentropy loss l gat in an end-to-end fashion."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.3,Neighbor Consistency Regularization for Noisy Labels,"the labeling of tumor-associated stroma can be affected by various factors, which can result in noisy labels. one of the reasons for noisy labels is the irregular distribution of the field effect, which makes it challenging to define a clear boundary between the tumor-associated and normal stroma regions. additionally, the presence of tumor heterogeneity and the varied distribution of tumor foci can further complicate the labeling process. to address this issue, we propose applying neighbor consistency regularization (ncr)  where d kl is the kl-divergence loss to quantify the discrepancy between two probability distributions, t represents the temperature and nn k (v i ) is the set of k nearest neighbors of v i in the feature space."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.4,Adversarial Multi-modal Learning,"biopsy and whole-mount slides provide complementary multi-modal information on the tumor microenvironment, and combining them can provide a more comprehensive understanding of tumor-associated stroma. however, using data from multiple modalities can introduce systematic shifts, which can impact the performance of a deep learning model. specifically, whole-mount slides typically contain larger tissue sections and are processed using different protocols than biopsy slides, which can result in differences in image quality, brightness, and contrast. these technical differences can affect the pixel intensity distributions of the images, leading to systematic shifts in the features that the deep learning model learns to associate with tumor-associated stroma. for instance, a model trained on whole-mount slides only may not generalize well to biopsy slides due to systematic shifts, hindering model performance in the clinical application scenario. to address the above issues, we propose an adversarial multi-modal learning (aml) module to force the feature extractor to produce multimodal-invariant representations on multiple source images. specifically, we incorporate a source discriminator adversarial neural network as auxiliary classifier. the module takes the stroma embedding as an input and predicts the source of the image (biopsy or whole-mount) using multilayer perceptron (mlp) with cross-entropy loss function l aml . the overall loss function of the entire model is computed as: where hyper-parameters α and β control the impact of each loss term. all modules were concurrently optimized in an end-to-end manner. the stroma classifier and source discriminator are trained simultaneously, aiming to effectively classify tumor-associated stroma while impeding accurate source prediction by the discriminator. the optimization process aims to achieve a balance between these two goals, resulting in an embedding space that encodes as much information as possible about tumor-associated stroma identification while not encoding any information on the data source. by adopting the adversarial learning strategy, our model can maintain the correlated information and shared characteristics between two modalities, which will enhance the model's generalization and robustness."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,3.1,Dataset,"in our study, we utilized three datasets for tumor-associated stroma analysis. (1) dataset a comprises 513 tiles extracted from the whole mount slides of 40 patients, sourced from the archives of the pathology department at cedars-sinai medical center (irb# pro00029960). it combines two sets of tiles: 224 images from 20 patients featuring stroma, normal glands, low-grade and highgrade cancer  (2) dataset b included 97 whole mount slides with an average size of over 174,000×142,000 pixels at 40x magnification. the prostate tissue within these slides had an average tumor area proportion of 9%, with an average tumor area of 77 square mm. an expert pathologist annotated the tumor region boundaries at the region-level, providing exhaustive annotations for all tumor foci. (3) dataset c comprised 6134 negative biopsy slides obtained from 262 patients' biopsy procedures, where all samples were diagnosed as negative. these slides are presumed to contain predominantly normal stroma tissues without phenotypic alterations in response to cancer. dataset a was utilized for training the stroma segmentation model. extensive data augmentation techniques, such as image scaling and staining perturbation, were employed during the training process. the model achieved an average test dice score of 95.57 ± 0.29 through 5-fold cross-validation. this model was then applied to generate stroma masks for all slides in datasets b and c. to precisely isolate stroma tissues and avoid data bleeding from epithelial tissues, we only extracted patches where over 99.5% of the regions were identified as stroma at 40x magnification to construct the stroma classification dataset. for positive tumor-associated stroma patches, we sampled patches near tumor glands within annotated tumor region boundaries, as we presumed that tumor regions represent zones in which the greatest amount of damage has progressed. for negative stroma patches, we calculated the tumor distance for each patch by measuring the euclidean distance from the patch center to the nearest edge of the labeled tumor regions. negative stroma patches were then sampled from whole mount slides with a gleason group smaller than 3 and a tumor distance larger than 5 mm. this approach aims to minimize the risk of mislabeling tumor-associated stroma as normal tissue. setting a 5mm threshold accounts for the typically minimal inflammatory responses induced by prostate cancers, particularly in lower-grade cases. to incorporate multi-modal information, we randomly sampled negative stroma patches from all biopsy slides in dataset c. overall, we selected over 1.1 million stroma patches of size 256×256 pixels at 40x magnification for experiments. during model training and testing, we performed stain normalization and standard image augmentation methods."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,4.0,Results and Discussions,"in conclusion, our study introduced a deep learning approach to accurately characterize the tumor-associated stroma in multi-modal prostate histopathology slides. our experimental results demonstrate the feasibility of using deep learning algorithms to identify and quantify subtle stromal alterations, offering a promising tool for discovering new diagnostic and prognostic biomarkers of prostate cancer. through exploring field effect in prostate cancer, our work provides a computational system for further analysis of tumor development and progression. future research can focus on validating our approach on larger and more diverse datasets and expanding the method to a patient-level prediction system, ultimately improving prostate cancer diagnosis and treatment."
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,1.0,Introduction,"the ability to predict the future risk of patients with cancer can significantly assist clinical management decisions, such as treatment and monitoring  over the years, deep learning has greatly promoted the development of computational pathology, including wsi analysis  in summary, to better capture the prognosis-related information in wsi, two technical key points should be fully investigated: "
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2.0,Methodology,"figure  however, conventional patch-level analysis cannot model complex pathological patterns (e.g., tumor lymphocyte infiltration, immune cell composition, etc.), resulting in limited cancer survival prediction performance. to this end, we proposed a novel learning network, i.e., hgt, which utilized the spatial and semantic priors mined by a multi-scope analysis strategy (i.e., in-slide superpixel and cross-slide clustering) to represent and capture the contextual interaction of pathological components. our framework consists two modules: a hierarchical graph convolutional network and a transformer-based prediction head."
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2.1,Hierarchical Graph Convolutional Network,"unlike cancer diagnosis and subtyping, cancer survival prediction is a quite more challenging task, as it is a future event prediction task which needs to consider complex pathological structures  patch graph convolutional layer. based on the spatial topology extracted by in-slide superpixel, the patch graph convolutional layer (patch gcl) is designed to learn the feature of the fine-grained microenvironment (e.g., cell) through the message passing between adjacent patches, which can be represented as: where σ(•) denotes the activation function, such as relu. graphconv denotes the graph convolutional operation, e.g., gcnconv  tissue graph convolutional layer. third, based on the spatial assignment matrix a spa , the learned patch-level features can be aggregated to the tissue-level features which contain the information of necrosis, epithelium, etc. where [•] t denote the matrix transpose operation. the tissue graph convolutional layer (tissue gcl) is further designed to learn the feature of this coarse-grained microenvironment, which can be represented as:"
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2.2,Transformer-Based Prediction Head,"clinical studies have shown that cancer survival prediction requires considering not only the biological morphology but also the contextual interactions of tumor and surrounding tissues  cross-slide clustering. as shown in fig.  transformer architecture. under the guidance of the semantic prior identified by cross-slide clustering, the learned tissue features v tissue can be further aggregated, forming a series meaningful component embeddings p specific to the cancer type. then we employed a transformer  where p out is the output of transformer, mhsa is the multi-headed self-attention "
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,4.0,Conclusion,"in this paper, we propose a novel learning framework, i.e., multi-scope analysis driven hgt, to effectively represent and capture the contextual interaction of pathological components for improving the effectiveness and interpretability of wsi-based cancer survival prediction. experimental results on three clinical cancer cohorts demonstrated our model achieves better performance and richer interpretability over the existing models. in the future, we will evaluate our framework on more tasks and further statistically analyze the interpretability of our model to find more pathological biomarkers related to cancer prognosis."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,1.0,Introduction,"ultrasound is a widely-used imaging modality for clinical cancer screening. deep learning has recently emerged as a promising approach for ultrasound lesion detection. while previous works focused on lesion detection in still images  previous general-purpose detectors  to address this issue, we propose a novel ultradet model to leverage ntc. for each region of interest (roi) r proposed by a basic detector, we extract temporal contexts from previous frames. to compensate for inter-frame motion, we generate deformed grids by applying inverse optical flow to the original regular roi grids, illustrated in fig.  our contributions are four-fold. "
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,5.0,Conclusion,"in this paper, we address the clinical challenge of real-time ultrasound lesion detection. we propose a novel negative temporal context aggregation (ntca) module, imitating radiologists' diagnosis processes to suppress fps. the ntca module leverages negative temporal contexts that are essential for fp suppression but ignored in previous works, thereby being more effective in suppressing fps. we plug the ntca module into a basicdet to form the ultradet model, which significantly improves the precision and fp rates over previous state-ofthe-arts while achieving real-time inference speed. the ultradet has the potential to become a real-time lesion detection application and assist radiologists in more accurate cancer diagnosis in clinical practice."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,3.1,Datasets,"our experiments utilized two datasets, with the first being the publicly available breast cancer dataset, camelyon16  the second dataset is a private hepatocellular carcinoma (hcc) dataset collected from sir run run shaw hospital, hangzhou, china. this dataset comprises a total of 1140 valid tumor wsis scanned at 40× magnification, and the objective is to identify the severity of each case based on the edmondson-steiner (es) grading. the ground truth labels are binary classes of low risk and high risk, which were provided by experienced pathologists."
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,1.0,Introduction,"histology is critical for accurately diagnosing all cancers in modern medical imaging analysis. however, the complex scanning procedure for histological wholeslide images (wsis) digitization may result in the alteration of tissue structures, due to improper removal, fixation, tissue processing, embedding, and storage  in real clinical practice, rescanning the wsis that contain artifacts can partially address this issue. however, it may require multiple attempts before obtaining a satisfactory wsi, which can lead to a waste of time, medical resources, and deplete tissue samples. discarding the local region with artifacts for deep learning models is another solution, but it may result in the loss of critical contextual information. therefore, learning-based artifact restoration approaches have gained increasing attention. for example, cyclegan  the major contributions are two-fold. "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,1.0,Introduction,"head and neck (h&n) cancer refers to malignant tumors in h&n regions, which is among the most common cancers worldwide  traditional survival prediction methods are usually based on radiomics  firstly, existing deep survival models are underdeveloped in utilizing complementary multi-modality information, such as the metabolic and anatomical information in pet and ct images. for survival prediction in h&n cancer, existing methods usually use single imaging modality  secondly, although deep survival models have advantages in performing end-to-end survival prediction without requiring tumor masks, this also incurs difficulties in extracting region-specific information, such as the prognostic information in primary tumor (pt) and metastatic lymph node (mln) regions. to address this limitation, recent deep survival models adopted multi-task learning for joint tumor segmentation and survival prediction, to implicitly guide the model to extract features related to tumor regions  in this study, we design an x-shape merging-diverging hybrid transformer network (named xsurv, fig. "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3.1,Dataset and Preprocessing,"we adopted the training dataset of hecktor 2022 (refer to https://hecktor.grand-cha llenge.org/), including 488 h&n cancer patients acquired from seven medical centers  we resampled pet-ct images into isotropic voxels where 1 voxel corresponds to 1 mm 3 . each image was cropped to 160 × 160 × 160 voxels with the tumor located in the center. pet images were standardized using z-score normalization, while ct images were clipped to "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,5.0,Conclusion,"we have outlined an x-shape merging-diverging hybrid transformer network (xsurv) for survival prediction from pet-ct images in h&n cancer. within the xsurv, we propose a merging-diverging learning framework, a hybrid parallel cross-attention (hpca) block, and a region-specific attention gate (rag) block, to learn complementary information from multi-modality images and extract region-specific prognostic information for survival prediction. extensive experiments have shown that the proposed framework and blocks enable our xsurv to outperform state-of-the-art survival prediction methods on the well-benchmarked hecktor 2022 dataset."
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,1.0,Introduction,"the examination of tissue and cells using microscope (referred to as histology) has been a key component of cancer diagnosis and prognostication since more than a hundred years ago. histological features allow visual readout of cancer biology as they represent the overall impact of genetic changes on cells  the great rise of deep learning in the past decade and our ability to digitize histopathology slides using high-throughput slide scanners have fueled interests in the applications of deep learning in histopathology image analysis. the majority of the efforts, so far, focus on the deployment of these models for diagnosis and classification  in the machine learning domain, patient prognostication can be treated as a weakly supervised problem, which a model would predict the outcome (e.g., time to cancer recurrence) based on the histopathology images. their majority have utilized multiple instance learning (mil)  to address this issue, graph neural networks (gnn) have recently received more attention in histology. they can model patch relations  this paper aims to investigate the potential of extracting fine and coarse features from histopathology slides and integrating them for risk stratification in cancer patients. therefore, the contributions of this work can be summarized as: 1) a novel graph-based model for predicting survival that extracts both local and global properties by identifying morphological super-nodes; 2) introducing a fine-coarse feature distillation module with 3 various strategies to aggregate interactions at different scales; 3) outperforming sota approaches in both risk prediction and patient stratification scenarios on two datasets; 4) publishing two large and rare prostate cancer datasets containing more than 220 graphs for active surveillance and 240 graphs for brachytherapy cases. the code and graph embeddings are publicly available at https://github.com/pazadimo/all-in 2 related works"
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4.1,Dataset,"we utilize two prostate cancer (pca) datasets to evaluate the performance of our proposed model. the first set (pca-as) includes 179 pca patients who were managed with active surveillance (as). radical therapy is considered overtreatment in these patients, so they are instead monitored with regular serum prostate-specific antigen (psa) measurements, physical examinations, sequential biopsies, and magnetic resonance imaging  the second dataset (pca-bt) includes 105 pca patients with low to high risk disease who went through brachytherapy. this treatment involves placing a radioactive material inside the body to safely deliver larger dose of radiation at one time "
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,1.0,Introduction,"accurate spatial characterization of tumor immune microenvironment is critical for precise therapeutic stratification of cancer patients (e.g. via immunotherapy). currently, this characterization is done manually by individual pathologists on standard hematoxylin-and-eosin (h&e) or singleplex immunohistochemistry (ihc) stained images. however, this results in high interobserver variability among pathologists, primarily due to the large (> 50%) disagreement among pathologists for immune cell phenotyping  in this paper, we introduce a new dataset that can be readily used out-ofthe-box with any artificial intelligence (ai)/deep learning algorithms for spatial characterization of tumor immune microenvironment and several other use cases. to date, only two denovo stained datasets have been released publicly: bci h&e and singleplex ihc her2 dataset "
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.0,Dataset,the complete staining protocols for this dataset are given in the accompanying supplementary material. images were acquired at 20× magnification at moffitt cancer center. the demographics and other relevant information for all eight head-and-neck squamous cell carcinoma patients is given in table 
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.1,Region-of-Interest Selection and Image Registration,"after scanning the full images at low resolution, nine regions of interest (rois) from each slide were chosen by an experienced pathologist on both mif and mihc images: three in the tumor core (tc), three at the tumor margin (tm), and three outside in the adjacent stroma (s) area. the size of the rois was standardized at 1356×1012 pixels with a resolution of 0.5 µm/pixel for a total surface area of 0.343 mm 2 . hematoxylin-stained rois were first used to align all the mihc marker images in the open source fiji software using affine registration. after that, hematoxylin-and dapi-stained rois were used as references to align mihc and mif rois again using fiji and subdivided into 512×512 patches, resulting in total of 268 co-registered mihc and mif patches (∼33 co-registered mif/mihc images per patient)."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,4.0,Conclusions and Future Work,"we have released the first ai-ready restained and co-registered mif and mihc dataset for head-and-neck squamous cell carcinoma patients. this dataset can be used for virtual phenotyping given standard clinical hematoxylin images, virtual clinical ihc dab generation with ground truth segmentations (to train highquality segmentation models across multiple cancer types) created from cleaner mif images, as well as for generating standardized clean mif images from neighboring h&e and ihc sections for registration and 3d reconstruction of tissue specimens. in the future, we will release similar datasets for additional cancer types as well as release for this dataset corresponding whole-cell segmentations via impartial https://github.com/nadeemlab/impartial."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Data use Declaration and Acknowledgment:,"this study is not human subjects research because it was a secondary analysis of results from biological specimens that were not collected for the purpose of the current study and for which the samples were fully anonymized. this work was supported by msk cancer center support grant/core grant (p30 ca008748) and by james and esther king biomedical research grant (7jk02) and moffitt merit society award to c. h. chung. it is also supported in part by the moffitt's total cancer care initiative, collaborative data services, biostatistics and bioinformatics, and tissue core facilities at the h. lee moffitt cancer center and research institute, an nci-designated comprehensive cancer center (p30-ca076292)."
Detection of Basal Cell Carcinoma in Whole Slide Images,1.0,Introduction,"skin cancer, the most prevalent cancer globally, has seen increasing incidences over recent decades  in this study, we utilized the nas approach to identify the optimal network for skin cancer detection. to improve the efficiency and accuracy of the search, we developed a new framework named sc-net, which focuses on identifying highly valuable architectures. we observed that conventional nas methods often overlook fairness ranking during the search, hindering the search for optimal solutions. our sc-net framework addresses this by ensuring fair training and precise ranking. the efficacy of sc-net was confirmed by our experimental results, with our resnet50 achieving 96.2% top-1 accuracy and 96.5% auc, outperforming baseline methods by 4.8% and 4.7% respectively. figure "
Detection of Basal Cell Carcinoma in Whole Slide Images,3.2,Performance Evaluation,"we validated our algorithm using the curated skin cancer dataset and sc-net as a supernet, testing both heavy and light models. we performed a search on resnet50 and mobilenetv2 models, compared against original resnet50 (ori resnet50) and mobilenetv2 (ori mobilenetv2) models as baselines. the resulting models are denoted as s resnet50 and s mobilenetv2. comparison with related methods. to ensure a fair comparison on our dataset, we selected several papers in the field of pathological image analysis, such as  evaluation metrics. our model was evaluated on: as shown in table "
Detection of Basal Cell Carcinoma in Whole Slide Images,4.0,Conclusion and Future Work,"in this paper, we introduce sc-net, a novel nas framework for skin cancer detection in pathology images. by formulating sc-net as a balanced supernet, we ensure fair ranking and treatment of all potential architectures. with scnet and evolutionary search, we obtained optimal architectures, achieving 96.2% top-1 and 96.5% accuracy on a skin cancer dataset, improvements of 4.8% and 4.7% over baselines. future work will apply our approach to larger datasets for wider-scale validation."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,3.1,Dataset,"we evaluate our model with three datasets. (1) luad-gm dataset: the objective is to predict the epidermal growth factor receptor (egfr) gene mutations in patients with lung adenocarcinoma (luad) using 723 whole slide image (wsi) slices, where 47% of cases have egfr mutations. (2) tcga-nsclc and tcga-rcc datasets: cancer type classification is performed using the cancer genome atlas (tcga) dataset. the tcga-nsclc dataset comprised two subtypes, lung squamous cell carcinoma (lusc) and lung adenocarcinoma (luad), while the tcga-rcc dataset included three subtypes: renal chromophobe cell carcinoma (kich), renal clear cell carcinoma (kirc), and renal papillary cell carcinoma (kirp)."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,1.0,Introduction,"histopathological images are regarded as the 'gold standard' in the diagnosis of cancers. with the advent of the whole slide image (wsi) scanner, deep learning has gained its reputation in the field of computational pathology  to address this issue, multiple instance learning (mil) has been successfully applied to the wsi classification task as a weakly supervised learning problem  recently, prototypical learning is applied in wsi analysis to identify representative instances in the bag  on the other hand, when pathologists analysis the wsis, they always observe the tissues at various resolutions  in this work, we propose a multi-scale prototypical transformer (mspt) for wsi classification. the mspt includes two key components: a prototypical transformer (pt) and a multi-scale feature fusion module (mffm). the specifically developed pt uses a clustering algorithm to extract instance prototypes from the bags, and then re-calibrates these prototypes at each scale with the self-attention mechanism in transformer  the contributions of this work are summarized as follows: 1) a novel prototypical transformer (pt) is proposed to learn superior prototype representation for wsi classification by integrating prototypical learning into the transformer architecture. it can effectively re-calibrate the cluster prototypes as well as reduce the computational complexity of the transformer. 2) a new multi-scale feature fusion module (mffm) is developed based on the mlp-mixer to enhance the information communication among phenotypes. it can effectively capture multi-scale information in wsi to improve the performance of wsi classification."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,2.2,Microvessel Infiltration Awareness (MIA),"we design a mia module to learn the infiltrative areas of microvessel. the tumors and margin depicted by ceus may be larger than those depicted by gray us because of continuous infiltrative expansion. inspired by the continuous infiltrative expansion, a series of flexible sigmoid alpha functions (saf) simulate the infiltrative expansion of microvessels by establishing the distance maps from the pixel to lesion boundary. here, the distance maps "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,1.0,Introduction,"the tumor microenvironment (tme) is comprised of cancer, immune (e.g. b lymphocytes, and t lymphocytes), stromal, and other cells together with noncellular tissue components "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.1,Dataset,"the cohort employed in this study was composed of pre-treatment tumor biopsy specimens from patients with nsclc from five centers (two centers for training (s t ) and three centers for independent validation (s v )). the entire analysis was carried out using 122 patients in experiment 1 (73 in s t , and 49 in s v ) and 135 patients in experiment 2 (81 in s t , and 54 in s v ). specimens were analyzed with a multiplexed quantitative immunofluorescence (qif) panel using the method described in "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,Results:,"the two top predictive triangil features were found to be the number of edges between stroma and cd4+ cells, and the number of edges between stroma and tumor cells with more interactions between stromal cells and both cd4+ and tumor cells being associated with response to io. this finding is concordant with other studies  result: figure "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,5.0,Concluding Remarks,"we presented a new approach, triangular analysis of geographical interplay of lymphocytes (triangil), to quantitatively chartacterize the spatial arrangement and relative geographical interplay of multiple cell families across pathological images. compared to previous spatial graph-based methods, triangil quantifies the spatial interplay between multiple cell families, providing a more comprehensive portrait of the tumor microenvironment. triangil was predictive of response after io (n = 122) and also demonstrated a strong correlation with os in nsclc patients treated with io (n = 135). triangil outperformed other graph-and dl-based approaches, with the added benefit of provoding interpretability with regard to the spatial interplay between cell families. for instance, triangil yielded the insight that more interactions between stromal cells and both cd4+ and tumor cells appears to be associated with better response to io. although five cell families were studies in this work, triangil is flexible and could include other cell types (e.g., macrophages). future work will entail larger validation studies and also evaluation on other use cases."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,1.0,Introduction,"histopathology relies on hematoxylin and eosin (h&e) stained biopsies for microscopic inspection to identify visual evidence of diseases. hematoxylin has a deep blue-purple color and stains acidic structures such as dna in cell nuclei. eosin, alternatively, is red-pink and stains nonspecific proteins in the cytoplasm and the stromal matrix. pathologists then examine highlighted tissue characteristics to diagnose diseases, including different cancers. a correct diagnosis, therefore, is dependent on the pathologist's training and prior exposure to a wide variety of disease subtypes "
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,1.0,Introduction,"radiotherapy, one of the mainstream treatments for cancer patients, has gained notable advancements in past decades. for promising curative effect, a high-quality radiotherapy plan is demanded to distribute sufficient dose of radiation to the planning target volume (ptv) while minimizing the radiation hazard to organs at risk (oars). to achieve this, radiotherapy plans need to be manually adjusted by the dosimetrists in a trial-and-error manner, which is extremely labor-intensive and time-consuming  recently, the blossom of deep learning (dl) has promoted the automatic medical image processing tasks  although the above methods have achieved good performance in predicting dose distribution, they suffer from the over-smoothing problem. these dl-based dose prediction methods always apply the l 1 or l 2 loss to guide the model optimization which calculates a posterior mean of the joint distribution between the predictions and the ground truth  in this paper, we investigate the feasibility of applying a diffusion model to the dose prediction task and propose a diffusion-based model, called diffdp, to automatically predict the clinically acceptable dose distribution for rectum cancer patients. specifically, the diffdp consists of a forward process and a reverse process. in the forward process, the model employs a markov chain to gradually transform dose distribution maps with complex distribution into gaussian distribution by progressively adding pre-defined noise. then, in the reverse process, given a pure gaussian noise, the model gradually removes the noise in multiple steps and finally outputs the predicted dose map. in this procedure, a noise predictor is trained to predict the noise added in the corresponding step of the forward process. to further ensure the accuracy of the predicted dose distribution for both the ptv and oars, we design a dl-based structure encoder to extract the anatomical information from the ct image and the segmentation masks of the ptv and oars. such anatomical information can indicate the structure and relative position of organs. by incorporating the anatomical information, the noise predictor can be aware of the dose constraints among ptv and oars, thus distributing more appropriate dose to them and generating more accurate dose distribution maps. overall, the contributions of this paper can be concluded as follows: "
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,3.0,Experiments and Results,"dataset and evaluations. we measure the performance of our model on an in-house rectum cancer dataset which contains 130 patients who underwent volumetric modulated arc therapy (vmat) treatment at west china hospital. concretely, for every patient, the ct images, ptv segmentation, oars segmentations, and the clinically planned dose distribution are included. additionally, there are four oars of rectum cancer containing the bladder, femoral head r, femoral head l, and small intestine. we randomly select 98 patients for model training, 10 patients for validation, and the remaining 22 patients for test. the thickness of the cts is 3 mm and all the images are resized to the resolution of 256 × 256 before the training procedure. we measure the performance of our proposed model with multiple metrics. considering dm represents the minimal absorbed dose covering m% percentage volume of ptv, we involve d 98 , d 2 , maximum dose (d max ), and mean dose (d mean ) as metrics. besides, the heterogeneity index (hi) is used to quantify dose heterogeneity  comparison with state-of-the-art methods. to verify the superior accuracy of our proposed model, we select multiple state-of-the-art (sota) models in dose prediction, containing unet (2017)  besides the quantitative results, we also present the dvh curves derived by compared methods in fig.  ablation study. to study the contributions of key components of the proposed method, we conduct the ablation experiments by 1) removing the structure encoder from the proposed method and concatenating the anatomical images x and noisy image y t together as the original input for diffusion model (denoted as baseline); 2) the proposed diffdp model. the quantitative results are given in table "
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,4.0,Conclusion,"in this paper, we introduce a novel diffusion-based dose prediction (diffdp) model for predicting the radiotherapy dose distribution of cancer patients. the proposed method involves a forward and a reverse process to generate accurate prediction by progressively transferring the gaussian noise into a dose distribution map. moreover, we propose a structure encoder to extract anatomical information from patient anatomy images and enable the model to concentrate on the dose constraints within several essential organs. extensive experiments on an in-house dataset with 130 rectum cancer patients demonstrate the superiority of our method."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,1.0,Introduction,"in the past few years, the development of histopathological whole slide image (wsi) analysis methods has dramatically contributed to the intelligent cancer diagnosis  generally, multiple instance learning (mil) is one of the most popular solutions for wsi analysis  however, hipt is a hierarchical learning framework based on a greedy training strategy. the bias and error generated in each level of the representation model will accumulate in the final decision model. moreover, the vit  in this paper, we propose a novel whole slide image representation learning framework named position-aware masked autoencoder (pama), which achieves slide-level representation learning by reconstructing the local representations of the wsi in the patch feature space. pama can be trained end-to-end from the local features to the wsi-level representation. moreover, we designed a position-aware cross-attention mechanism to guarantee the correlation of localto-global information in the wsis while saving computational resources. the proposed approach was evaluated on a public tcga-lung dataset and an in-house endometrial dataset and compared with 6 state-of-the-art methods. the results have demonstrated the effectiveness of the proposed method. the contribution of this paper can be summarized into three aspects. (1) we propose a novel whole slide image representation learning framework named position-aware masked autoencoder (pama). pama can make full use of abundant unlabeled wsis to learn discriminative wsi representations. (2) we propose a position-aware cross-attention (paca) module with a kernel reorientation (kro) strategy, which makes the framework able to maintain the spatial integrity and semantic enrichment of slide representation during the selfsupervised training. "
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,end end end,"tcga-lung dataset is collected from the cancer genome atlas (tcga) data portal. the dataset includes a total of 3,064 wsis, which consist of three categories, namely tumor-free (normal), lung adenocarcinoma (luad), and lung squamous cancer (lusc), endometrial dataset includes 3,654 wsis of endometrial pathology, which includes 8 categories, namely well/moderately/low-differentiated endometrioid adenocarcinoma, squamous differentiation carcinoma, plasmacytoid carcinoma, clear cell carcinoma, mixed-cell adenocarcinoma, and benign tumor. each dataset was randomly divided into training, validation and test sets according to 6:1:3 while keeping each category of data proportionally. we conducted wsi multi-type classification experiments on the two datasets. the validation set was used to perform an early stop. the results of the test set were reported for comparison."
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,1.0,Introduction,"cervical cancer is a common and severe disease that affects millions of women globally, particularly in developing countries  several computer-aided cervical cancer screening methods have been proposed for whole slide images (wsis) in the literature. most of them are detectionbased methods, which typically contain a detection model as well as some postprocessing modules in their frameworks. for instance, zhou et al.  some methods improve the final classification performance by improving the detection model to identify positive cells more reliably. cao et al.  these methods have achieved good results through continuous improvement on the detection-based pipeline, but there are some common drawbacks. first, they are not able to get rid of their reliance on detection models, which means they have a high need for expensive detection data labeling to train the detection model. cervical cancer cell detection datasets involve labeling individual and small bounding boxes in a large number of cells. it often requires multiple experienced pathologists to annotate  to address the aforementioned issues, we propose a detection-free pipeline in this paper, which does not rely on any detection model. instead, our pipeline requires only sample-level diagnosis labels, which are naturally available in clinical scenarios and thus get rid of additional image labeling. to attain this goal, we have designed a two-stage pipeline as in fig. "
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,3.0,Experiment and Results,"dataset and experimental setup. in this study, we have collected 5384 cervical cytopathological wsi by 20x lens, each with 20000 × 20000 pixels, from our collaborating hospitals. among them, there 2853 negative samples, and 2531 positive samples (962 ascus, and 1569 high-level positive samples). all wsis only have diagnosis labels at the sample level, without annotation boxes at the cell level. and all sample labels are strictly diagnosed according to the tbs  comparison to sota methods. in this section, we experiment to compare our method with popular state-of-the-art (sota) methods, which are all fully supervised and detection-based. to the best of our knowledge, there are few good methods to train cervical cancer classification models in weakly supervised or unsupervised learning ways. no methods can achieve the detection-free goal either. all the detection-based methods are evaluated in the following way. first, we label a dataset with cell-level bounding boxes to train a detection model. the detection dataset has 3761 images and 7623 cell-level annotations. after obtaining the suspicious cell patches provided by the detection model, we use the subsequent classification models used in these sota works to classify them and obtain the final classification results. as shown in table  ablation study. in this section, we experiment to demonstrate the effectiveness of all the proposed parts in our pipeline. we divide all 5384 samples into five independent parts for five-fold cross-validation, and the results are shown in table "
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,1.0,Introduction,"colorectal cancer is a prevalent form of cancer characterized by colorectal adenocarcinoma, which develops in the colon or rectum's inner lining and exhibits glandular structures  recently, diffusion model  in this paper, we propose a new method for gland instance segmentation based on the diffusion model. (1) our method utilizes a diffusion model to perform denoising and tackle the task of gland instance segmentation in histology images. the noise boxes are generated from gaussian noise, and the predicted ground truth (gt) boxes and segmentation masks are performed during the diffusion process. (2) to improve segmentation, we use instance-aware techniques to recover lost details during denoising. this includes employing a filter and a multi-scale mask branch to create a global mask and refine finer segmentation details. (3) to enhance object-background differentiation, we utilize conditional encoding to augment intermediate features with the original image encoding. this method effectively integrates the abundant information from the original image, thereby enhancing the distinction between the objects and the surrounding background. our proposed method was trained and tested on the 2015 mic-cai gland segmentation (glas) challenge dataset "
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,1.0,Introduction,"immunohistochemical (ihc) staining is a widely used technique in pathology for visualizing abnormal cells that are often found in tumors. ihc chromogens highlight the presence of certain antigens or proteins by staining their corresponding antibodies. for instance, the her2 (human epidermal growth factor receptor 2) biomarker is associated with aggressive breast tumor development and is essential in forming a precise treatment plan. despite its capability to provide highly valuable diagnostic information, the process of ihc staining is very labor-intensive, time-consuming and requires specialized histotechnologists and laboratory equipments  at the other end of the spectrum, h&e (hematoxylin and eosin) staining, as the gold standard in histological staining, highlights the tissue structures and cell morphology. in routine diagnostics, on account of its much lower cost, an h&e-stained slide is prepared by pathologists in order to determine whether or not to also apply the ihc stains for a more precise assessment of the disease. therefore, it is of great interest to have an algorithm that can automatically translate an h&e-stained slide into one that could be considered to have been stained with ihc while accurately predicting the target expression levels. to that end, researchers have recently proposed to use gan-based image-to-image translation (i2it) algorithms for transforming h&e-stained slides into ihc. despite the progress, the outstanding challenge in training such i2it frameworks is the lack of aligned h&e-ihc image pairs, or in other words, the inconsistencies in the h&e-ihc groundtruth pairs. to explain, since re-staining a slice is physically infeasible, a matching pair of h&e-ihc slices are taken from two depth-wise consecutive cuts of the same tissue then stained and scanned separately. this inevitably prevents pixel-perfect image correspondences due to the slice-to-slice changes in cell morphology, staining-induced degradation (e.g. tissue-tearing), imaging artifacts that may vary among slices (e.g. camera out-offocus) and multi-slice registration errors. an example pair of patches is shown in fig.  as a result, recent advances in h&e-to-ihc i2it have mostly avoided using the inconsistent gt pairs and instead have imposed the cycle-consistency constraint  in this paper, we argue that the ihc slides, despite the disparities vis-a-vis their h&e counterparts, can still serve as useful targets for stain translation. the work we present in this paper is based on the important realization that even when pairs of consecutive tissue slices do not yield images that are pixel-perfect aligned, it is highly likely that the corresponding patches in the two stains share the same diagnostic label. for example, if the levels of expression in a region of the her2 slide are high, the corresponding region in the h&e slide is highly likely to contain a high density of cancerous cells. therefore, we set our goal to meaningfully leverage such correlations to benefit the h&e-to-ihc i2it while being resilient to any inconsistencies. toward this goal, we propose a supervised patchwise contrastive loss named the adaptive supervised patchnce (asp) loss. our formulation of this loss was inspired by the recent research findings that contrastive loss benefits model robustness under label noise  lastly, to support further research in virtual ihc-restaining, we present the multi-ihc stain translation (mist) as a new public dataset. the mist dataset contains 4k+ training and 1k testing aligned h&e-ihc patches for each of the following ihc stains that are critical for breast cancer diagnostics: her2, ki67, er (estrogen receptor) and pr (progesterone receptor). we evaluated existing i2it methods and ours for multiple ihc stains and demonstrate the superior performance achieved by our method both qualitatively and quantitatively. "
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,3.0,Experiments,"datasets. the following datasets are used in our experiments: the breast cancer immunohistochemical (bci) challenge dataset  implementation details. for all of our models, we used resnet-6blocks as the generator and a 5-layer patchgan as the discriminator. we trained our networks with random 512 × 512 crops and a batch size of one. the adam optimizer  evaluation metrics. we compare the methods using both paired and unpaired evaluation metrics. to compare a pair of images, generated and groundtruth, we use the standard ssim (structural similarity index measure) and phv (perceptual hash value) as described in  qualitative evaluations. in fig. "
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,1.0,Introduction,"histopathology is considered the gold standard for diagnosing and treating many cancers  different resolution levels in the wsi pyramids contain different and complementary information  in this paper, we present a novel hierarchical interaction graph-transformer framework (i.e., higt) to simultaneously capture both local and global information from wsi pyramids with a novel bidirectional interaction module. specifically, we abstract the multi-resolution wsi pyramid as a heterogeneous hierarchical graph and devise a hierarchical interaction graph-transformer architecture to learn both short-range and long-range correlations among different image patches within different resolutions. considering that the information from different resolutions is complementary and can benefit each other, we specially design a bidirectional interaction block in our hierarchical interaction vit mod- ule to establish communication between different resolution levels. moreover, a fusion block is proposed to aggregate features learned from the different levels for slide-level prediction. to reduce the tremendous computation and memory cost, we further adopt the efficient pooling operation after the hierarchical gnn part to reduce the number of tokens and introduce the separable self-attention mechanism in hierarchical interaction vit modules to reduce the computation burden. the extensive experiments with promising results on two public wsi datasets from tcga projects, i.e., kidney carcinoma (kica) and esophageal carcinoma (esca), validate the effectiveness and efficiency of our framework on both tumor subtyping and staging tasks. the codes are available at https:// github.com/hku-medai/higt."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,3.0,Experiments,"datasets and evaluation metrics. we assess the efficacy of the proposed higt framework by testing it on two publicly available datasets (kica and esca) from the cancer genome atlas (tcga) repository. the datasets are described below in more detail: -kica dataset. the kica dataset consists of 371 cases of kidney carcinoma, of which 279 are classified as early-stage and 92 as late-stage. for the tumor typing task, 259 cases are diagnosed as kidney renal papillary cell carcinoma, while 112 cases are diagnosed as kidney chromophobe. -esca dataset. the esca dataset comprises 161 cases of esophageal carcinoma, with 96 cases classified as early-stage and 65 as late-stage. for the tumor typing task, there are 67 squamous cell carcinoma cases and 94 adenocarcinoma cases. experimental setup. the proposed framework was implemented by pytorch  ablation analysis. we further conduct an ablation study to demonstrate the effectiveness of the proposed components. the results are shown in table "
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,1.0,Introduction,"colorectal cancer is the third most common malignant tumor, and nearly half of all patients with colorectal cancer develop liver metastasis during the course of the disease  extensive existing works have demonstrated the power of deep learning on various spatial-temporal data, and can potentially be applied towards the problem of crlm. for example, originally designed for natural data, several mainstream models such as e3d-lstm  however, all these methods have only demonstrated their effectiveness towards 3d/4d data (i.e., time-series 2d/3d images), and it is not clear how to best extend them to work with the 5d cect data. part of the reason is due to the lack of public availability of such data. when extending these models towards 5d cect data, some decisions need to be made, for example: 1) what is the most effective way to incorporate the phase information? simply concatenating different phases together may not be the optimal choice, because the positional information of the same ct slice in different phases would be lost. 2) shall we use uni-directional lstm or bi-direction lstm? e3d-lstm  in this paper, we investigate how state-of-art deep learning models can be applied to the crlm prediction task using our 5d cect dataset. we evaluate the effectiveness of bi-directional lstm and explore the possible method of incorporating different phases in the cect dataset. specifically, we show that the best prediction accuracy can be achieved by enhancing e3d-lstm "
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,2.1,Dataset,"our dataset follows specific inclusion criteria: -no tumor appears on the ct scans. that means patients have not been diagnosed as crlm when they took the scans. -patients were previously diagnosed with colorectal cancer tnm stage i to stage iii, and recovered from colorectal radical surgery. -patients have two or more times of cect scans. -we already determined whether or not the patients had liver metastases within 2 years after the surgery, and manually labeled the dataset based on this. -no potential focal infection in the liver before the colorectal radical surgery. -no metastases in other organs before the liver metastases. -no other malignant tumors. our retrospective dataset includes two cohorts from two hospitals. the first cohort consists of 201 patients and the second cohort includes 68 patients. each scan contains three phases and 100 to 200 ct slices with a resolution of 512×512. patients may have different numbers of ct scans, ranging from 2 to 6, depending on the number of follow-up visits. ct images are collected with the following acquisition parameters: window width 150, window level 50, radiation dose 120 kv, slice thickness 1 mm, and slice gap 0.8 mm. all images underwent manual quality control to exclude any scans with noticeable artifacts or blurriness and to verify the completeness of all slices. additional statistics on our dataset are presented in table "
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,1.0,Introduction,accurate diagnosis plays an important role in achieving the best treatment outcomes for people with cancer  the introduction of digital pathology (dp) has enabled application of machine learning approaches to extract otherwise inaccessible diagnostic and prognostic information from h&e-stained whole slide images (wsis) 
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,3.1,Data,"we tested our feature representation method in several classification tasks involving wsis of h&e-stained histopathology slides. the number of slides per class for each classification task are shown in fig.  for these two tasks we used artifact-free tiles from tumor regions detected with an in-house tumor detection model. for breast cancer metastasis detection in lymph node tissue, we used wsis of h&estained healthy lymph node tissue and lymph node tissue with breast cancer metastases from the publicly available camelyon16 challenge data set  for cell of origin (coo) prediction of activated b-cell like (abc) or germinal center b-cell like (gcb) tumors in diffuse large b-cell lymphoma (dlbcl), we used data from the phase 3 goya (nct01287741) and phase 2 cavalli (nct02055820) clinical trials, hereafter referred to as ct1 and ct2, respectively. all slides were h&e-stained and scanned using ventana dp200 scanners at 40× magnification. ct1 was used for training and testing the classifier and ct2 was used only as an independent holdout data set. for these data sets we used artifact-free tiles from regions annotated by expert pathologists to contain tumor tissue."
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Cellular Explainability Method. The cellular average embedding is,"where e ij ∈ r 256 is the cellular embedding extracted from every detected cell in the tile j i ∈ 1, 2, . . . , n j where n j is the number of cells in the tile j. this can be rewritten as a weighted average of the cellular embeddings where w i ∈ r 256 are the per cell attention weights that if initialized to 0 result in the original cellular average embedding. the re-formulation does not change the result of the forward pass since w i are not all equal. note that the weights are not learned through training but calculated per cell at inference time to get the per cell contribution. we computed the gradient of the output category (of the classification method applied on top of the computed embedding) with respect to the attention weights w i : grad i = ∂score i /∂w i and visualized cells that received positive and negative gradients using different colors. visual example results. examples of our cellular explainability method applied to weakly supervised tumor detection on wsis from the camelyon16 data set using a-mil are shown in fig.  in this case, cells with positive attention gradients that shifted the output towards a classification of gcb were labeled green and cells with negative attention gradients that shifted the classification towards abc were labeled red. cells with positive attention gradients were mostly smaller lymphoid cells with low grade morphology or were normal lymphocytes, whereas cells with negative attention gradients were more frequently larger lymphoid cells with high grade morphology (fig. "
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,5.0,Conclusions,"we describe a method to capture both cellular and texture feature representations from wsis that can be plugged into any mil architecture (e.g., cnn or xformer-based), as well as into fully supervised models (e.g., tile classification models). our method is more flexible than other methods (e.g., hierarchical image pyramid transformer) that usually capture the hierarchical structure in wsis by aggregating features at multiple levels in a complex set of steps to perform the final classification task. in addition, we describe a method to explain the output of the classification model that evaluates the contributions of histologically identifiable cells to the slide-level classification. tilelevel embeddings result in good performance for detection of tumor metastases in lymph nodes. however, introducing more cell-level information, using combined embeddings, resulted in improved classification performance. in her2 and er prediction tasks for breast cancer we demonstrate that addition of a cell-level embedding summary to tilelevel embeddings can boost model performance by up to 8%. finally, for coo prediction in dlbcl and breast cancer metastasis detection in lymph nodes, we demonstrated the potential of our explainability method to gain insights into previously unknown associations between cellular morphology and disease biology."
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,2.2,Transformer with Skip Self-Attention (SSA),"we design a novel skip self-attention (ssa) module to fuse discriminative features of candida from different scales. at a fine-grained level, the hyphae and spores of candida are usually the basis for judging. yet we need to distinguish them from easily distorting factors such as contaminants in wsis and edges of nearby cells. at a coarse-grained level, there is the phenomenon that a candida usually links multiple host cells and yields a string of them. thus it is necessary to combine long-range visual cues that span several cells to derive the decision related to candida. cnn-based methods have achieved excellent performance in computer-aided diagnosis including cervical cancer "
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,1.0,Introduction,"cervical cancer accounts for 6.6% of the total cancer deaths in females worldwide, making it a global threat to healthcare  nowadays, thin-prep cytologic test (tct)  after scanning whole-slide images (wsis) from tct samples, automatic tct screening is highly desired due to the large population versus the limited number of pathologists. as the wsi data per sample has a huge size, the idea of identifying abnormal cells in a hierarchical manner has been proposed and investigated by several studies using deep learning  to alleviate the shortage of sufficient data to supervise classification, one may adopt traditional data augmentation techniques, which yet may bring little improvement due to scarcely expanded data diversity  aiming at augmenting the performance of cervical abnormality screening, we develop a novel conditional generative adversarial network in this paper, namely cellgan, to synthesize cytopathological images for various cell types. we leverage fastgan "
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,1.0,Introduction,"thyroid cancer is the most common cancer of the endocrine system, accounting for 2.1% of all malignant cancers  however, nuclei segmentation in thyroid cytopathology is still challenged by the varying cellularity of images from different tbsrtc categories  to narrow the gap discussed, we propose a novel tbsrtc-category-aware nuclei segmentation framework. our contributions are three-fold. "
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,1.0,Introduction,"breast cancer (bc) is one of the most common malignant tumors in women worldwide and it causes nearly 0.7 million deaths in 2020  recently, with the development of transformer, multi-modal pre-training has achieved great success in the fields of computer vision (cv) and natural language processing (nlp). according to the data format, there are two main multi-modal pre-training approaches, as shown in fig.  in this paper, we propose a multi-modal pre-training method based on masked autoencoders for bc downstream tasks. our model consists of three parts, i.e., the modal-fusion encoder, the mixed attention, and the modal-specific decoder. we choose paired h&e and ihc (only her2) staining images, which are cropped into non-overlapped patches as the input of our model. we randomly mask some patches by a ratio and feed the remaining patches into the modalfusion encoder to get corresponding tokens. then the mixed attention module is used to take the intra-modal and inter-modal correlation into account. finally, we use modal-specific decoders to reconstruct the original h&e and ihc staining images respectively. our contributions are summarized as follows: we propose a multi-modal pre-training via masked autoencoders mmp-mae for bc diagnosis. to our best knowledge, this is the first pre-training work based on multi-modal pathological data. we evaluate the proposed method on two public datasets as herohe challenge and bci challenge, which shows that our method achieves state-of-theart performance. i=1 and {yi} λ 2 n i=1 into the modal-fusion encoder to extract the patch tokens {fi} λ 1 n i=1 and {gi} λ 2 n i=1 . then we use intra-modal attention and inter-modal attention to take patch correlation into account. x and y are reconstructed by modal-specific decoders respectively."
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,3.1,Datasets,acrobat challenge. the automatic registration of breast cancer tissue (acrobat) challenge  bci challenge. breast cancer immunohistochemical image generation challenge 
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,1.0,Introduction,"automatic segmentation of tumor lesions from pathological images plays an important role in accurate diagnosis and quantitative evaluation of cancers. recently, deep learning has achieved remarkable performance in pathological image segmentation when trained with a large and well-annotated dataset  semi-supervised learning (ssl) is a potential technique to reduce the annotation cost via learning from a limited number of labeled data along with a large amount of unlabeled data. existing ssl methods can be roughly divided into two categories: consistency-based  in this work, we propose a novel and efficient method based on cross distillation with multiple attentions (cdma) for semi-supervised pathological image segmentation. firstly, a multi-attention tri-branch network (mtnet) is proposed to efficiently obtain diverse outputs for a given input. unlike mc-net+  the contribution of this work is three-fold: 1) a novel framework named cdma based on mtnet is introduced for semi-supervised pathological image segmentation, which leverages different attention mechanisms for generating diverse and complementary predictions for unlabeled images; 2) a cross decoder knowledge distillation method is proposed for robust and efficient learning from noisy pseudo labels, which is combined with an average prediction-based uncertainty minimization to improve the model's performance; 3) experimental results show that the proposed cdma outperforms eight state-of-the-art ssl methods on the public digestpath dataset "
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,1.0,Introduction,"automatic identification of lesions from dermoscopic images is of great importance for the diagnosis of skin cancer  one approach to address the above problem is novel class discovery (ncd)  in this paper, we propose a new novel class discovery framework to automatically discover novel disease categories. specifically, we first use contrastive learning to pretrain the model based on all data from known and unknown categories to learn a robust and general semantic feature representation. then, we propose an uncertainty-aware multi-view cross-pseudo-supervision strategy to perform clustering. it first uses a self-labeling strategy to generate pseudo-labels for unknown categories, which can be treated homogeneously with ground truth labels. the cross-pseudo-supervision strategy is then used to force the model to maintain consistent prediction outputs for different views of unlabeled images. in addition, we propose to use prediction uncertainty to adaptively adjust the contribution of the pseudo labels to mitigate the effects of noisy pseudo labels. finally, to encourage local neighborhood alignment and further refine the pseudo labels, we propose a local information aggregation module to aggregate the information of the neighborhood samples to boost the clustering performance. we conducted extensive experiments on the dermoscopy dataset isic 2019, and the experimental results show that our method outperforms other state-of-the-art comparison algorithms by a large margin. in addition, we also validated the effectiveness of different components through extensive ablation experiments."
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,1.0,Introduction,"cancers are a group of heterogeneous diseases reflecting deep interactions between pathological and genomics variants in tumor tissue environments  the major goal of multimodal data learning is to extract complementary contextual information across modalities  to tackle above challenges, we propose a pathology-and-genomics multimodal framework (i.e., pathomics) for survival prediction (fig. "
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,3.0,Experiments and Results,"datasets. all image and genomics data are publicly available. we collected wsis from the cancer genome atlas colon adenocarcinoma (tcga-coad) dataset (cc-by-3.0)  experimental settings and implementations. we implement two types of settings that involve internal and external datasets for model pretraining and finetuning. as shown in fig  the number of epochs for pretraining and finetuning are 25, the batch size is 1, the optimizer is adam "
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,4.0,Conclusion,"developing data-efficient multimodal learning is crucial to advance the survival assessment of cancer patients in a variety of clinical data scenarios. we demonstrated that the proposed pathomics framework is useful for improving the survival prediction of colon and rectum cancer patients. importantly, our approach opens up perspectives for exploring the key insights of intrinsic genotypephenotype interactions in complex cancer data across modalities. our finetuning"
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,1.0,Introduction,"cervical cancer is the second most common cancer among adult women. if diagnosed early, it can be effectively treated and cured  with the development of deep learning  although the above-mentioned attempts can improve the screening performance significantly, there are several issues that need to be addressed: 1) object detection methods often require accurate annotated data to guarantee performance with robustness and generalization. however, due to legal limitations, the scarcity of positive samples, and especially the subjectivity differences between cytopathologists for manual annotations  to address these issues, we propose a novel method for cervical abnormal cell detection using distillation from local-scale consistency refinement. inspired by knowledge distillation, we construct a pre-trained patch correction network (pcn), which is designed to exploit the supervised information from the pcn to reduce the impact of noisy labels and utilize the contextual relationships between cells. in our approach, we begin by utilizing retinanet "
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,4.0,Discussion,"in this work, we proposed and examined novel data augmentation strategies based on the idea of interpolations of feature vectors in the mil setting. instance-based mil did not show any competitive scores. obviously the model reducing each patch to a single value is not adequate for the classification of frozen or paraffin sections from thyroid cancer tissues. the considered dual-stream approach, including an embedding and instance-based stream, exhibited slightly improved average scores, compared to embedding-based mil only. in our analysis, we focused on the embedding-based configuration and on the balanced combined approach (referred to as 2/2). with the baseline data augmentation approaches, the maximum improvements were 0.03, and 0.02 for the frozen, and 0.01, and 0.05 for the paraffin data set. the inter-mixup approach did not show any systematic improvements. independently of the chosen strategy (v1, v2), concerning the combination within or between classes, we did not notice any positive trend. the multilinear intra-mixup method, however, exhibited the best scores for 3 out of 4 combinations and the best overall mean accuracy for both, the frozen and the paraffin data set. also a clear trend with increasing scores in the case of an increasing ratio of augmented data (β) is visible. the linear method showed a similar, but less pronounced trend. obviously, the straightforward application of the mixup scheme (as in case of the inter-mixup approach), is inappropriate for the considered setting. an inhibiting factor could be a high inter-wsi variability leading to incompatible feature vectors (which are too far away from realistic samples in the feature space). to particularly investigate this effect, we performed 2 different inter-mixup settings (v1 & v2), with the goal of identifying the effect of mixed (and thereby more dissimilar) or similar classes during interpolation. the analysis of the distance distributions between patch representations confirmed that, the variability between wsis is clearly larger than the variability within wsis. in addition, the results showed that the variability between classes is, on patch-level, not clearly larger than the variability within a class. obviously variability due to the acquisition outweigh any disease specific variability. this could provide an explanation for the effectiveness of intra-mixup approach compared to the (similarly) poorly performing inter-mixup settings. we expect that stain normalization methods (but not stain augmentation) could be utilized to align the different wsis to provide a more appropriate basis for inter-wsi interpolation. with regard to the different data sets, we noticed a stronger, positive effect in case of the frozen section data set. this is supposed to be due to the clearly higher variability of the frozen sections corresponding with a need for a higher variability in the training data. we also noticed a stronger effect of the solely embedding-based architecture (also showing the best overall scores). we suppose that this is due to the fact that the additional loss of the dual-stream architecture exhibits a valuable regularization tool to reduce the amount of needed training data. with the proposed intra-mixup augmentation strategy, this effect diminishes, since the amount and quality of training data is increased. to conclude, we proposed novel data augmentation strategies based on the idea of interpolations of image descriptors in the mil setting. based on the experimental results, the multilinear intra-mixup setting proved to be highly effective, while the inter-mixup method showed inferior scores compared to a state-of-the-art baseline. we learned that there is a clear difference between combinations within and between wsis with a noticeable effect on the final classification accuracy. this is supposedly due to the high variability between the wsis compared to a rather low variability within the wsis. in the future, additional experiments will be conducted including stain normalization methods and larger benchmark data sets to provide further insights."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,1.0,Introduction,"breast cancer (bc) is the most common cancer diagnosed among females and the second leading cause of cancer death among women after lung cancer  among different types of imaging biomarkers, histopathological images are generally considered the golden standard for bc prognosis since they can confer important cell-level information that can reflect the aggressiveness of bc  to deal with the above challenges, several researchers began to design domain adaption algorithms, which utilize the labeled data from a related cancer subtype to help predict the patients' survival in the target domain. specifically, alirezazadeh et al  although much progress has been achieved, most of the existing studies applied the feature alignment strategy to reduce the distribution difference between source and target domains. however, such transfer learning methods neglected to take the interaction among different types of tissues into consideration. for example, it is widely recognized that tumor-infiltrating lymphocytes (tils) and its correlation with tumors reveal a similar role in the prognosis of different brca subtypes. for instance, kurozumi et al  based on the above considerations, in this paper, we proposed a tils-tumor interactions guided unsupervised domain adaptation (t2uda) algorithm to predict the patients' survival on the target bc subtype. specifically, t2uda first applied the graph attention network (gats) to learn node embeddings and the spatial interactions between tumor and tils patches in wsi. in order to preserve the node-level and interaction-level similarities across different domains, we not only aligned the embedding for different types of nodes but also designed a novel tumor-tils interaction alignment (ttia) module to ensure that the distribution of the interaction weights are similar in both domains. we evaluated the performance of our method on the breast invasive carcinoma (brca) cohort derived from the cancer genome atlas (tcga), and the experimental results indicated that t2uda outperforms other domain adaption methods for predicting patients' clinical outcomes."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Calculating TILs-Tumor Interaction via Graph Attention Networks(GATs).,"to characterize the interaction between different tils and tumor patches, we employed gat  we calculated the attention coefficients among different nodes, which can be formulated as: furthermore, a softmax function was then adopted to normalize the attention coefficients e ij : where n i represents all neighbors of node i. the new feature vector v i for node i was calculated via a weighted sum: finally, the output features of each gat layer were aggregated in the readout layer. we fed the generated output features from each readout layer into the cox hazard proportional regression model for the final prognosis predictions. feature alignment. in the proposed gat-based transfer learning framework, the feature alignment component was employed on its first two layers. then, for the node embeddings with different types (tils and tumor) in both the source and target domain, we performed a mean pooling operation to obtain their aggregated features. next, we aligned the aggregated tumor or tils features from the two domains separately using maximum mean discrepancy(mmd)  here, we adopted mmd for feature alignment due to its ability to measure the distance between two distributions without explicit assumptions on the data distribution, we showed the objective function of mmd in our method as follows: where h is a hilbert space, f represents the features from the source, f represents the feature from the target, r represents the layer number, k ∈ {l, t } referred to tils or tumor node. in addition, n denotes the number of source samples, while m refers to the number of target samples.  tils-tumor interaction alignment. to accurately characterize the interaction between tils and tumors, we further analyzed the extracted interaction weights by dividing them into 10 intervals (i.e., bins). for each interval, we calculated the sum of all source domain interaction weights as i s k and the sum of all target domain interaction weights as i t k , where k represents the k-th interval. consequently, we obtained two vectors and applied softmax on each of them for normalization that can be denoted as . in order to measure the dissimilarity between p i and q i , the kullback-leibler (kl) divergence is adapted on the third layer of gat, which can be formulated as: according to eq.( "
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3.0,Experiments and Results,"datasets. we conducted our experiments on the breast invasive carcinoma (brca) dataset from the cancer genome atlas (tcga). specifically, the brca dataset includes 661 patients with hematoxylin and eosin (he)-stained pathological imaging and corresponding survival information. among the collected brca patients in tcga, the number of er positive(er+) and er negative(er-) patients are 515 and 146, respectively. we hope to investigate if the proposed t2uda could be used to help improve the prognosis performance of (er+) or (er-) with the aid of the survival information on its counterpart."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3.2,Result and Discussion,"in this study, we compared the performance of our proposed model with several existing domain adaptation methods, including 1) ddc  the results presented in table  we also evaluated the contributions of the key components of our framework and found that t2uda performed better than source only and t2uda-v1, which shows the advantage of minimizing differences in tils-tumor interaction weights. in addition, we also evaluated the patient stratification performance of different methods. as shown in fig.  we also examined the consistency of important edges in each group of stratified patients based on the tils-tumor interaction weights calculated by the gat-based framework in the source and target domains. as seen in fig. "
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,4.0,Conclusion,"in this paper, we presented an unsupervised domain adaptation algorithm that leverages tils-tumor interactions to predict patients' survival in a target bc subtype(t2uda). our results demonstrated that the relationship between tils and tumors is transferable and can be effectively used to improve the accuracy of survival prediction models. to the best of our knowledge, t2uda was the first method to successfully achieve interrelationship transfer between tils and tumors across different cancer subtypes for prognosis tasks."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,1.0,Introduction,"pathological image-omic analysis is the cornerstone of modern medicine and demonstrates promise in a variety of different tasks such as cancer diagnosis and prognosis  though deep learning techniques have revolutionized medical imaging, designing a task-specific algorithm for image-omic multi-modality analysis is challenging.  specifically, to our knowledge, most multi-modality techniques have been designed for modalities such as chest x-ray and reports  in this paper, we propose a task-specific framework dubbed gene-induced multimodal pre-training (gimp) for image-omic classification. concretely, we first propose a transformer-based gene encoder, group multi-head self attention (groupmsa), to capture global structured features in gene expression cohorts. next, we design a pre-training paradigm for wsis, masked patch modeling (mpm), masking random patch embeddings from a fixed-length contiguous subsequence of a wsi. we assume that one patch-level feature embedding can be reconstructed by its adjacent patches, and this process enhances the learning ability for pathological characteristics of different tissues. our mpm only needs to recover the masked patch embeddings in a fixed-length subsequence rather than processing all patches from wsis. furthermore, to model the high-order relevance of the two modalities, we combine cls tokens of paired image and genomic data to form unified representations and propose a triplet learning module to differentiate patient-level positive and negative samples in a mini-batch. it is worth mentioning that although our unified representation fuses features from the whole gene expression cohort and partial wsis in a mini-batch, we can still learn high-order relevance and discriminative patient-level information between these two modalities in pre-training thanks to the triplet learning module. in addition, note that our proposed method is different from self-supervised pre-training. specifically, we focus not only on superior representation learning capability, but also category-related feature distributions, w.r.t. intra-and inter-class variation. with the training process going on, complete information from wsis can be integrated and the fused multimodal representations with high discrimination will make it easier for the classifier to find the classification hyperplane. experimental results demonstrate that our gimp achieves significant improvement in accuracy than other image-omic competitors, and our multimodal framework shows competitive performance even without pre-training."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.1,Experimental Setup,"datasets. we verify the effectiveness of our method on the caner genome atlas (tcga) non-small cell lung cancer (nsclc) dataset, which contains two cancer subtypes, i.e., lung squamous cell carcinoma (lusc) and lung adenocarcinoma (luad). after pre-processing  implementation details. the pre-training process of all algorithms is conducted on the training set, without any extra data augmentation. note that our genetic encoder, groupmsa, is fully supervised pre-trained on unimodal genetic data to accelerate convergence and it is frozen during gimp training process. the maximum pre-training epoch for all methods is set to 100 and we finetune the models at the last epoch. during fine-tuning, we evaluate the model on the validation set after every epoch, and save the parameters when it performs the best. adamw "
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.1,Datasets,"we tested our proposed method on two different tasks: (1) intrahepatic cholangiocarcinomas(ihccs) subtype classification and (2) liver cancer type classification. the dataset for the former task was collected from 168 patients with 332 wsis from seoul national university hospital. ihccs can be further categorized into small duct type (sdt) and large duct type (ldt). using gene mutation information as prior knowledge, we collected wsis with wild kras and mutated idh genes for use as training samples in sdt, and wsis with mutated kras and wild idh genes for use in ldt. the rest of the wsis were used as testing samples. the liver cancer dataset for the latter task was composed of 323 wsis, in which the wsis can be further classified into hepatocellular carcinomas (hccs) (collected from pathology ai platform "
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.2,Implementation Detail,"we used a pre-trained vgg16 with imagenet as the initial encoder, which was further modified via deep manifold model training using the proposed manifold and cross-entropy loss functions. the number of nearest neighbors k and the number of sub-classes n were set to 5 and 10, respectively. in the deep manifold embedding learning model, the learning rates were set to 1e-4 with a decay rate of 1e-6 for the ihccs subtype classification and to 1e-5 with a decay rate of 1e-8 for the liver cancer type classification. the k-nearest neighbors graph and the geodesic distance matrix are updated once every five training epochs, which is empirically chosen to balance running time and accuracy. to train the mil classifier, we set the learning rate to 1e-3 and the decay rate to 1e-6. we used batch sizes 64 and 4 for training the deep manifold embedding learning model and the mil classification model, respectively. the number of epochs for the deep manifold embedding learning model was 50, while 50 and 200 epochs for the ihccs subtype classification and liver cancer type classification, respectively. as for the optimizer, we used stochastic gradient decay for both stages. the result shown in the tables is the average result from 10 iterations of the mil classification model."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,1.0,Introduction,"prostate cancer (pca) diagnosis and grading rely on histopathology analysis of biopsy slides  field effect refers to the spread of genetic and epigenetic alterations from a primary tumor site to surrounding normal tissues, leading to the formation of secondary tumors. understanding field effect is essential for cancer research as it provides insights into the mechanisms underlying tumor development and progression. tumor-associated stroma, which consists of various cell types, such as fibroblasts, smooth muscle cells, and nerve cells, is an integral component of the tumor microenvironment that plays a critical role in tumor development and progression. reactive stroma, a distinct phenotype of stromal cells, arises in response to signaling pathways from cancerous cells and is characterized by altered stromal cells and increased extracellular matrix components  manual review for tumor-associated stroma is time-consuming and lacks quantitative metrics  analyzing tumor-associated stroma in prostate cancer requires combining whole-mount and biopsy histopathology slides. biopsy slides provide information on the presence of pca, while whole-mount slides provide information on the extent and distribution of pca, including more information on tumor-associated stroma. combining the information from both modalities can provide a more accurate understanding of the tumor microenvironment. in this work, we explore the field effect in prostate cancer by analyzing tumor-associated stroma in multimodal histopathological images. our main contributions can be summarized as follows: -to the best of our knowledge, we present the first deep-learning approach to characterize prostate tumor-associated stroma by integrating histological image analysis from both whole-mount and biopsy slides. our research offers a promising computational framework for in-depth exploration of the field effect and cancer progression in prostate cancer. -we proposed a novel approach for stroma classification with spatial graphs modeling, which enable more accurate and efficient analysis of tumor microenvironment in prostate cancer pathology. given the spatial nature of cancer field effect and tumor microenvironment, our graph-based method offers valuable insights into stroma region analysis. -we developed a comprehensive pipeline for constructing tumor-associated stroma datasets across multiple data sources, and employed adversarial training and neighborhood consistency regularization techniques to learn robust multimodal-invariant image representations."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.1,Stroma Tissue Segmentation,"accurately analyzing tumor-associated stroma requires a critical pre-processing step of segmenting stromal tissue from the background, including epithelial tissue. this segmentation task is challenging due to the complex and heterogeneous appearance of the stroma. to address this, we propose utilizing the pointrend model "
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.2,Stroma Classification with Spatial Patch Graphs,"to capture the spatial nature of field effect and analyze tumor-associated stroma, modeling spatial relationships between stroma patches is essential. the spatial relationship can reveal valuable information about the tumor microenvironment, and neighboring stroma cells can undergo similar phenotypic changes in response to cancer. therefore, we propose using a spatial patch graph to capture the highorder relationship among stroma tissue regions. we construct the stroma patch graph using a k-nearest neighbor (knn) graph and neighbor sampling. the knn graph connects each stroma patch to its k nearest neighboring patches. given a central stroma patch, we iteratively add neighboring patches to construct  the patch graph until we reach a specified layer number l to control the subgraph size. this process results in a tree-like subgraph with each layer representing a different level of spatial proximity to the central patch. the use of neighbor sampling enables efficient processing of large images and allows for stochastic training of the model. to predict tumor-associated binary labels of stroma patches, we employ a message-passing approach that propagates patch features in the spatial graph. to achieve this, we use graph convolutional networks with attention, also known as graph attention networks (gats)  where w ∈ r m ×n is a learnable matrix transforming n -dimensional features to m -dimensional features. n e vi is the neighborhood of the node v i connected by e in g. gat uses attention mechanism to construct the weighting coefficients as: where t represents transposition, is the concatenation operation, and ρ is leakyrelu function. the final output of gat module is the tumor-associated probability of the input patch. and the module was optimized using the crossentropy loss l gat in an end-to-end fashion."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.3,Neighbor Consistency Regularization for Noisy Labels,"the labeling of tumor-associated stroma can be affected by various factors, which can result in noisy labels. one of the reasons for noisy labels is the irregular distribution of the field effect, which makes it challenging to define a clear boundary between the tumor-associated and normal stroma regions. additionally, the presence of tumor heterogeneity and the varied distribution of tumor foci can further complicate the labeling process. to address this issue, we propose applying neighbor consistency regularization (ncr)  where d kl is the kl-divergence loss to quantify the discrepancy between two probability distributions, t represents the temperature and nn k (v i ) is the set of k nearest neighbors of v i in the feature space."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.4,Adversarial Multi-modal Learning,"biopsy and whole-mount slides provide complementary multi-modal information on the tumor microenvironment, and combining them can provide a more comprehensive understanding of tumor-associated stroma. however, using data from multiple modalities can introduce systematic shifts, which can impact the performance of a deep learning model. specifically, whole-mount slides typically contain larger tissue sections and are processed using different protocols than biopsy slides, which can result in differences in image quality, brightness, and contrast. these technical differences can affect the pixel intensity distributions of the images, leading to systematic shifts in the features that the deep learning model learns to associate with tumor-associated stroma. for instance, a model trained on whole-mount slides only may not generalize well to biopsy slides due to systematic shifts, hindering model performance in the clinical application scenario. to address the above issues, we propose an adversarial multi-modal learning (aml) module to force the feature extractor to produce multimodal-invariant representations on multiple source images. specifically, we incorporate a source discriminator adversarial neural network as auxiliary classifier. the module takes the stroma embedding as an input and predicts the source of the image (biopsy or whole-mount) using multilayer perceptron (mlp) with cross-entropy loss function l aml . the overall loss function of the entire model is computed as: where hyper-parameters α and β control the impact of each loss term. all modules were concurrently optimized in an end-to-end manner. the stroma classifier and source discriminator are trained simultaneously, aiming to effectively classify tumor-associated stroma while impeding accurate source prediction by the discriminator. the optimization process aims to achieve a balance between these two goals, resulting in an embedding space that encodes as much information as possible about tumor-associated stroma identification while not encoding any information on the data source. by adopting the adversarial learning strategy, our model can maintain the correlated information and shared characteristics between two modalities, which will enhance the model's generalization and robustness."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,3.1,Dataset,"in our study, we utilized three datasets for tumor-associated stroma analysis. (1) dataset a comprises 513 tiles extracted from the whole mount slides of 40 patients, sourced from the archives of the pathology department at cedars-sinai medical center (irb# pro00029960). it combines two sets of tiles: 224 images from 20 patients featuring stroma, normal glands, low-grade and highgrade cancer  (2) dataset b included 97 whole mount slides with an average size of over 174,000×142,000 pixels at 40x magnification. the prostate tissue within these slides had an average tumor area proportion of 9%, with an average tumor area of 77 square mm. an expert pathologist annotated the tumor region boundaries at the region-level, providing exhaustive annotations for all tumor foci. (3) dataset c comprised 6134 negative biopsy slides obtained from 262 patients' biopsy procedures, where all samples were diagnosed as negative. these slides are presumed to contain predominantly normal stroma tissues without phenotypic alterations in response to cancer. dataset a was utilized for training the stroma segmentation model. extensive data augmentation techniques, such as image scaling and staining perturbation, were employed during the training process. the model achieved an average test dice score of 95.57 ± 0.29 through 5-fold cross-validation. this model was then applied to generate stroma masks for all slides in datasets b and c. to precisely isolate stroma tissues and avoid data bleeding from epithelial tissues, we only extracted patches where over 99.5% of the regions were identified as stroma at 40x magnification to construct the stroma classification dataset. for positive tumor-associated stroma patches, we sampled patches near tumor glands within annotated tumor region boundaries, as we presumed that tumor regions represent zones in which the greatest amount of damage has progressed. for negative stroma patches, we calculated the tumor distance for each patch by measuring the euclidean distance from the patch center to the nearest edge of the labeled tumor regions. negative stroma patches were then sampled from whole mount slides with a gleason group smaller than 3 and a tumor distance larger than 5 mm. this approach aims to minimize the risk of mislabeling tumor-associated stroma as normal tissue. setting a 5mm threshold accounts for the typically minimal inflammatory responses induced by prostate cancers, particularly in lower-grade cases. to incorporate multi-modal information, we randomly sampled negative stroma patches from all biopsy slides in dataset c. overall, we selected over 1.1 million stroma patches of size 256×256 pixels at 40x magnification for experiments. during model training and testing, we performed stain normalization and standard image augmentation methods."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,4.0,Results and Discussions,"in conclusion, our study introduced a deep learning approach to accurately characterize the tumor-associated stroma in multi-modal prostate histopathology slides. our experimental results demonstrate the feasibility of using deep learning algorithms to identify and quantify subtle stromal alterations, offering a promising tool for discovering new diagnostic and prognostic biomarkers of prostate cancer. through exploring field effect in prostate cancer, our work provides a computational system for further analysis of tumor development and progression. future research can focus on validating our approach on larger and more diverse datasets and expanding the method to a patient-level prediction system, ultimately improving prostate cancer diagnosis and treatment."
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,1.0,Introduction,"the ability to predict the future risk of patients with cancer can significantly assist clinical management decisions, such as treatment and monitoring  over the years, deep learning has greatly promoted the development of computational pathology, including wsi analysis  in summary, to better capture the prognosis-related information in wsi, two technical key points should be fully investigated: "
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2.0,Methodology,"figure  however, conventional patch-level analysis cannot model complex pathological patterns (e.g., tumor lymphocyte infiltration, immune cell composition, etc.), resulting in limited cancer survival prediction performance. to this end, we proposed a novel learning network, i.e., hgt, which utilized the spatial and semantic priors mined by a multi-scope analysis strategy (i.e., in-slide superpixel and cross-slide clustering) to represent and capture the contextual interaction of pathological components. our framework consists two modules: a hierarchical graph convolutional network and a transformer-based prediction head."
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2.1,Hierarchical Graph Convolutional Network,"unlike cancer diagnosis and subtyping, cancer survival prediction is a quite more challenging task, as it is a future event prediction task which needs to consider complex pathological structures  patch graph convolutional layer. based on the spatial topology extracted by in-slide superpixel, the patch graph convolutional layer (patch gcl) is designed to learn the feature of the fine-grained microenvironment (e.g., cell) through the message passing between adjacent patches, which can be represented as: where σ(•) denotes the activation function, such as relu. graphconv denotes the graph convolutional operation, e.g., gcnconv  tissue graph convolutional layer. third, based on the spatial assignment matrix a spa , the learned patch-level features can be aggregated to the tissue-level features which contain the information of necrosis, epithelium, etc. where [•] t denote the matrix transpose operation. the tissue graph convolutional layer (tissue gcl) is further designed to learn the feature of this coarse-grained microenvironment, which can be represented as:"
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2.2,Transformer-Based Prediction Head,"clinical studies have shown that cancer survival prediction requires considering not only the biological morphology but also the contextual interactions of tumor and surrounding tissues  cross-slide clustering. as shown in fig.  transformer architecture. under the guidance of the semantic prior identified by cross-slide clustering, the learned tissue features v tissue can be further aggregated, forming a series meaningful component embeddings p specific to the cancer type. then we employed a transformer  where p out is the output of transformer, mhsa is the multi-headed self-attention "
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,4.0,Conclusion,"in this paper, we propose a novel learning framework, i.e., multi-scope analysis driven hgt, to effectively represent and capture the contextual interaction of pathological components for improving the effectiveness and interpretability of wsi-based cancer survival prediction. experimental results on three clinical cancer cohorts demonstrated our model achieves better performance and richer interpretability over the existing models. in the future, we will evaluate our framework on more tasks and further statistically analyze the interpretability of our model to find more pathological biomarkers related to cancer prognosis."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,1.0,Introduction,"ultrasound is a widely-used imaging modality for clinical cancer screening. deep learning has recently emerged as a promising approach for ultrasound lesion detection. while previous works focused on lesion detection in still images  previous general-purpose detectors  to address this issue, we propose a novel ultradet model to leverage ntc. for each region of interest (roi) r proposed by a basic detector, we extract temporal contexts from previous frames. to compensate for inter-frame motion, we generate deformed grids by applying inverse optical flow to the original regular roi grids, illustrated in fig.  our contributions are four-fold. "
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,5.0,Conclusion,"in this paper, we address the clinical challenge of real-time ultrasound lesion detection. we propose a novel negative temporal context aggregation (ntca) module, imitating radiologists' diagnosis processes to suppress fps. the ntca module leverages negative temporal contexts that are essential for fp suppression but ignored in previous works, thereby being more effective in suppressing fps. we plug the ntca module into a basicdet to form the ultradet model, which significantly improves the precision and fp rates over previous state-ofthe-arts while achieving real-time inference speed. the ultradet has the potential to become a real-time lesion detection application and assist radiologists in more accurate cancer diagnosis in clinical practice."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,3.1,Datasets,"our experiments utilized two datasets, with the first being the publicly available breast cancer dataset, camelyon16  the second dataset is a private hepatocellular carcinoma (hcc) dataset collected from sir run run shaw hospital, hangzhou, china. this dataset comprises a total of 1140 valid tumor wsis scanned at 40× magnification, and the objective is to identify the severity of each case based on the edmondson-steiner (es) grading. the ground truth labels are binary classes of low risk and high risk, which were provided by experienced pathologists."
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,1.0,Introduction,"histology is critical for accurately diagnosing all cancers in modern medical imaging analysis. however, the complex scanning procedure for histological wholeslide images (wsis) digitization may result in the alteration of tissue structures, due to improper removal, fixation, tissue processing, embedding, and storage  in real clinical practice, rescanning the wsis that contain artifacts can partially address this issue. however, it may require multiple attempts before obtaining a satisfactory wsi, which can lead to a waste of time, medical resources, and deplete tissue samples. discarding the local region with artifacts for deep learning models is another solution, but it may result in the loss of critical contextual information. therefore, learning-based artifact restoration approaches have gained increasing attention. for example, cyclegan  the major contributions are two-fold. "
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,1.0,Introduction,"breast cancer is the most prevalent form of cancer among women and can have serious physical and mental health consequences if left unchecked  deep neural networks have been widely adopted for breast cancer diagnosis to alleviate the workload of radiologists. however, these models often require a large number of manual annotations and lack interpretability, which can prevent their broader applications in breast cancer diagnosis. radiologists typically focus on areas with breast lesions during mammogram reading  radiologists' eye movements can be automatically and unobtrusively recorded during the process of reading mammograms, providing a valuable source of data without the need for manual labeling. previous studies have incorporated radiologists' eye-gaze as a form of weak supervision, which directs the network's attention to the regions with possible lesions  mammography primarily detects two types of breast lesions: masses and microcalcifications  in this work, we propose a novel diagnostic model, namely mammo-net, which integrates radiologists' gaze data and interactive information between cc-view and mlo-view to enhance diagnostic performance. to the best of our knowledge, this is the first work to integrate gaze data into multi-view mammography classification. we utilize class activation map (cam)  our contributions can be summarized as follows: • we emphasize the significance of low-cost gaze to provide weakly-supervised positioning and visual interpretability for the model. additionally, we develop a pyramid loss that adapts to the supervised process. • we propose a novel breast cancer diagnosis model, namely mammo-net. this model employs transformer-based attention to mutualize information and uses bfl to integrate task-related information to make accurate predictions. • we demonstrate the effectiveness of our approach through experiments using mammography datasets, which show the superiority of mammo-net. 2 proposed method"
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,4.0,Conclusion and Discussion,"in this paper, we have developed a breast cancer diagnosis model to mimic the radiologist's decision-making process. to achieve this, we integrate gaze data as a form of weak supervision for both lesion positioning and interpretability of the model. we also utilize transformer-based attention to mutualize multi-view information and further develop bfl to fully fuse multi-view information. our experimental results on mammography datasets demonstrate the superiority of our proposed model. in future work, we intend to explore the use of scanning path analysis as a means of obtaining insights into the pathology-relevant regions of lesions."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,1.0,Introduction,"colorectal cancer (crc) remains a major health burden with elevated mortality worldwide  traditional machine learning approaches in polyp segmentation primarily focus on learning low-level features, such as texture, shape, or color distribution  despite significant progress made by these binary mask supervised models, challenges remain in accurately locating polyps, particularly in complex clinical scenarios, due to their insensitivity to complex lesions and high false-positive rates. more specifically, most polyps have an elliptical shape with well-defined boundaries. however, supervised segmentation learning solely based on binary masks may not be effective in discriminating polyps in complex clinical scenarios. endoscopic images often contain pseudo-polyp objects with strong boundaries, such as colon folds, blood vessels, and air bubbles, which can result in false positives. in addition, sessile and flat polyps have ambiguous and challenging boundaries to delineate. to address these limitations, qadir et al.  therefore, the primary challenge lies in enhancing polyp segmentation performance in complex scenarios by precisely preserving the polyp segmentation boundaries, while simultaneously maximizing the decoder's attention on the overall pattern of the polyps. in this paper, we propose a novel transformer-based polyp segmentation framework, petnet, which addresses the aforementioned challenges and achieves sota performance in locating polyps with high precision. our contributions are threefold: • we propose a novel gaussian-probabilistic guided semantic fusion method for polyp segmentation, which improves the decoder's global perception of polyp locations and discrimination capability for polyps in complex scenarios. • we evaluate the performance of petnet on five widely adopted datasets, demonstrating its superior ability to identify polyp camouflage and small polyp scenes, achieving state-of-the-art performance in locating polyps with high precision. furthermore, we show that petnet can achieve a speed of about 27fps in edge computing devices (nvidia jetson orin). • we design several polyp instance-level evaluation metrics, considering that conventional pixel-level calculation methods cannot explicitly and comprehensively evaluate the overall performance of polyp segmentation algorithms."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,1.0,Introduction,"breast cancer (bc) is the most common cancer in women and incidence is increasing  authenticated by the bi-rads lexicon  the question of ""what the bi-mg would look like if they were symmetric?"" is often considered when radiologists determine the symmetry of bi-mg. it can provide valuable diagnostic information and guide the model in learning the diagnostic process akin to that of a human radiologist. recently, two studies explored generating healthy latent features of target mammograms by referencing contralateral mammograms, achieving state-of-the-art (sota) classification performance  in this work, we present a novel end-to-end framework, disasymnet, which consists of an asymmetric transformer-based classification (asyc) module and an asymmetric abnormality disentanglement (asyd) module. the asyc emulates the radiologist's analysis process of checking unilateral and comparing bi-mg for abnormalities classifying. the asyd simulates the process of disentangling the abnormalities and normal glands on pixel-level. additionally, we leverage a self-adversarial learning scheme to reinforce two modules' capacity, where the feedback from the asyc is used to guide the asyd's disentangling, and the asyd's output is used to refine the asyc in detecting subtle abnormalities. to "
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,2.3,Asymmetric Synthesis for Supervised Reconstruction,"to alleviate the lack of annotation pixel-wise asymmetry annotations, in this study, we propose a random synthesis method to supervise disentanglement. training with synthetic artifacts is a low-cost but efficient way to supervise the model to better reconstruct images  the alpha weights α k is a 2d gaussian distribution map, in which the co-variance is determined by the size of k-th tumor t, representing the transparency of the pixels of the tumor. some examples are shown in fig.  when training the model on other datasets, we use the tumor set collected from the inbreast dataset. thus, the supervised reconstruction loss is l syn = l l1 (x|real, x n |fake), where x|real is the real image before synthesis and x n |fake is the disentangled normal image from the synthesised image x|fake."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,1.0,Introduction,"over 430,000 new cases of renal cancer were reported in 2020 in the world  segmentation of kidney tumors on ncct images adds challenges compared to contrast-enhanced ct (cect) images, due to low contrast and lack of multiphase images. on cect images, the kidney tumors have different intensity values compared to the normal tissues. there are several works that demonstrated successful segmentation of kidney tumors with high precision  3d u-net  in this work, we present a novel framework that is capable of capturing the protuberances in the kidneys. our goal is to segment kidney tumors including isodensity types on ncct images. to achieve this goal, we create a synthetic dataset, which has separate annotations for normal kidneys and protruded regions, and train a segmentation network to separate the protruded regions from the normal kidney regions. in order to segment whole tumors, our framework consists of three networks. the first is a base network, which extracts kidneys and an initial tumor region masks. the second protuberance detection network receives the kidney region mask as its input and predicts a protruded region mask. the last fusion network receives the initial tumor mask and the protruded region mask to predict a final tumor mask. this proposed framework enables a better segmentation of isodensity tumors and boosts the performance of segmentation of kidney tumors on ncct images. the contribution of this work is summarized as follows: 1. present a pioneering work for segmentation of kidney tumors on ncct images. 2. propose a novel framework that explicitly captures protuberances in a kidney to enable a better segmentation of tumors including isodensity types on ncct images. this framework can be extended to other organs (e.g. adrenal gland, liver, pancreas). 3. verify that the proposed framework achieves a higher dice score compared to the standard 3d u-net using a publicly available dataset. "
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,2.0,Related Work,"the release of two public ct image datasets with kidney and tumor masks from the 2019/2021 kidney and kidney tumor segmentation challenge  looking at the top 3 teams from each challenge  in terms of focusing on protruded regions in kidneys, our work is close to  the second protuberance detection network is the same as the base network except it starts from 8 channels instead of 16. we train this network using synthetic datasets. the details of the dataset and training procedures are described in sect. 3.2. the last fusion network combines the outputs from the base network and the protuberance detection network and makes the final tumor prediction. in detail, we perform a summation of the initial tumor mask and the protruded region mask, and then concatenate the result with the input image. this is the input of the last fusion network, which also has the same architecture as the base network with an exception of having two input channels. this fusion network do not just combine the outputs but also is responsible for removing false positives from the base network and the protuberance detection network. our combined three network is fully differentiable, however, to train efficiently, we train the model in 3 steps."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,3.1,Step1: Training Base Network,"in the first step, we train the base network, which is a standard segmentation network, to extract kidney and tumor masks from the images. we use a sigmoid function for the last layer. and as a loss function, we use the dice loss "
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,3.2,Step2: Training Protuberance Detection Network,"in the second step, we train the protuberance detection network alone to separate protruded regions from the normal kidney masks. here, we only use the crossentropy loss and label smoothing with a smoothing factor of = 0.01. synthetic dataset. to enable a segmentation of protruded regions only, a separate annotation of each region is usually required. however, annotating such areas is time-consuming and preparing a large number of data is challenging. alternatively, we create a synthetic dataset that mimics a kidney with protrusions. the synthetic dataset is created through the following steps: 1. randomly sample a kidney mask without protuberance and a tumor mask. 2. apply random rotation and scaling to the tumor mask. 3. randomly insert the tumor mask into the kidney mask. 4. if both of the following conditions are met, append to the dataset. where k i is a voxel value (0 or 1) in the kidney mask and t i is a voxel value in the tumor mask. equation 1 ensures that only up to 30% of the kidney is covered with a tumor. equation "
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,3.3,Step3: End-to-End Training with Fusion Network,"in the final step, we train the complete network jointly. although our network is fully differentiable, since there is no separate annotation for protruded regions other from the synthetic dataset, we freeze the parameters in protuberance detection network. the output of the protuberance detection network will likely have more false positives than the base network since it has no access to the input image. thus, when the output of the protuberance detection network is concatenated with the output of the base network, the fusion network can easily reduce the loss by ignoring the protuberance detection network's output, which is suboptimal. to avoid this issue, we perform summation not concatenation to avoid the model from ignoring all output from the protuberance detection network. we then clip the value of the mask to the range of 0 and 1. as a result, the input to the fusion network has two channels. the first channel is the input image, and the second channel is the result of summation of the initial tumor mask and the protruded region mask. we concatenate the input image so that the last network can remove false positives from the predicted masks as well as predicting the missing tumor regions from the protuberance detection network. we use the dice loss and the cross-entropy loss as loss functions for the fusion network. we also keep the loss functions in the base network for predicting kidneys and tumors. the loss function for tumors in the base network acts like an intermediate supervision. our network shares some similarities with the stacked hourglass network "
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,6.0,Conclusion,"in this paper, we proposed a novel framework for kidney tumor segmentation on ncct images. to cope with isodensity tumors, which have similar intensity values to their surrounding tissues, we created a synthetic dataset to train a network that extracts protuberance from the kidney masks. we combined this network with the base network and fusion network. we evaluated our method using the publicly available kits19 dataset, and showed that the proposed method can achieve a higher sensitivity than existing approach. our framework is not limited to kidney tumors but can also be extended to other organs (e.g., adrenal gland, liver, pancreas)."
Skin Lesion Correspondence Localization in Total Body Photography,4.0,Conclusions and Limitations,"the evolution of a skin lesion is an important sign of a potentially cancerous growth and total body photography is useful to keep track of skin lesions longitudinally. we proposed a novel framework that leverages geometric and texture information to effectively find lesion correspondence across tbp scans. the framework is evaluated on a private dataset and a public dataset with success rates that are comparable to those of the state-of-the-art method. the proposed method assumes that the local texture enclosing the lesion and its surroundings should be similar from scan to scan. this may not hold when the appearance of the lesion changes dramatically (e.g. if the person acquires a tattoo). also, the resolution of the mesh affects the precision of the positions of landmarks and lesions. in addition, the method may not work well with longitudinal data that has non-isometric deformation due to huge variations in body shape, inconsistent 3d reconstruction, or a dramatic change in pose and, therefore, topology, such as an open armpit versus a closed one. in the future, the method needs to be evaluated on longitudinal data with longer duration and new lesions absent in the target. in addition, an automatic method to determine accurate landmarks is desirable. note that although we rely on the manual selection of landmarks, the framework is still preferable over manually annotating lesion correspondences when a subject has hundreds of lesions. as the 3d capture of the full body becomes more prevalent with better quality in tbp, we expect that the proposed method will serve as a valuable step for the longitudinal tracking of skin lesions."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,1.0,Introduction,"achieving complete tumor resection in surgical oncology like breast conserving surgery (bcs) is challenging as boundaries of tumors are not always visible/ palpable  the success of clinical deployment of learning models heavily relies on approaches that are not only accurate but also interpretable. therefore, it should be clear how models reach their decisions and the confidence they have in such decision. studies suggest that one way to improve these factors is through data centric approaches i.e. to focus on appropriate representation of data. specifically, representation of data as graphs has been shown to be effective for medical diagnosis and analysis  biological data, specially those acquired intra-opertively, are heterogeneous by nature. while the use of ex-vivo data collected under specific protocols are beneficial to develop baseline models, intra-operative deployment of these models is challenging. for iknife, the ex-vivo data is usually collected from homogeneous regions of resected specimens under the guidance of a trained pathologist, versus the intra-operative data is recorded continuously while the surgeon cutting through tissues with different heterogeneity and pathology. therefore, beyond predictive power and explainable decision making, intra-operative models must be able to handle mixed and unseen pathology labels. uncertainty-aware models in computer-assisted interventions can provide clinicians with feedback on prediction confidence to increase their reliability during deployment. deep ensembles "
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2.1,Data Curation,"ex-vivo: data is collected from fresh breast tissue samples from the patients referred to bcs at kingston health sciences center over two years. the study is approved by the institutional research ethics board and patients consent to be included. peri-operatively, a pathologist guides and annotates the ex-vivo pointburns, referred to as spectra, from normal or cancerous breast tissue immediately after excision. in addition to spectral data, clinicopathological details such as the status of hormone receptors is also provided post-surgically. in total 51 cancer and 149 normal spectra are collected and stratified into five folds (4 for cross validation and 1 prospectively) with each patient restricted to one fold only."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Evidential Graph Transformer:,"evidential deep learning provides a welldefined theoretical framework to jointly quantify classification prediction and uncertainty modeling by assuming the class probability follows a dirichlet distribution  in the context of surgical margin assessment, the attentions reveal the relevant metabolic ranges to cancerous tissue, while uncertainty helps identify and filter data with unseen pathology. specifically, the attentions affect the predictions by selectively emphasizing the contributions of relevant nodes, enabling the model to make more accurate predictions. on the other hand, the spread of the outcome probabilities as modeled by the dirichlet distribution represents the confidence in the final predictions. combining the two provides interpretable predictions along with the uncertainty estimation. mathematically, the dirichlet distribution is characterized by α = [α 1 , ..., α c ] where c is the number of classes in the classification task. the parameters can be estimates as α = f (x i |θ) + 1 where f (x i |θ) is the output of the evidential graph transformer parameterized by θ for each sample(x i ). then, the expected probability for the c-th class p c and the total uncertainty u for each sample (x i ) can be calculated as p c = αc s , and u = c s , respectively, where s = c c=1 α c . to fit the dirichlet distribution to the output layer of our network, we use a loss function consisting of the prediction error l p i and the evidence adjustment where λ is the annealing coefficient to balance the two terms. l p i can be crossentropy, negative log-likelihood, or mean square error , while l e i (θ) is kl divergence to the uniform dirichlet distribution "
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Ex-vivo Evaluation:,"the performance of the proposed network is compared with 3 baseline models including gtn, graph convolution network  clinical relevance: hormone receptor status plays an important role in determining breast cancer prognosis and tailoring treatment plans for patients "
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,4.0,Conclusion,"intra-operative deployment of deep learning solutions requires a measure of interpretability as well as predictive confidence. these two factors are particularly importance to deal with heterogeneity of tissues which represented as mixed or unseen labels for the retrospective models. in this paper, we propose an evidential graph transformer for margin detection in breast cancer surgery using mass spectrometry with these benefits in mind. this structure combines the attention mechanisms of graph transformer with predictive uncertainty. we demonstrate the significance of this model in different experiments. it has been shown that the proposed architecture can provide additional insight and consequently clearer interpretation of surgical margin characterization and clinical features like status of hormone receptors. in the future, we plan to work on other uncertainty estimation approaches and further investigate the graph conversion technique to be more targeted on the metabolic pathways, rather than regular conversion."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,1.0,Introduction,"breast cancer is the most common cancer and the leading cause of cancer death in women  with the development of computer technology, artificial intelligence-based methods have shown potential in image generation and have received extensive attention. some studies have shown that some generative models can effectively perform mutual synthesis between mr, ct, and pet  diffusion-weighted imaging (dwi) is emerging as a key imaging technique to complement breast ce-mri  i from the perspective of method, we innovatively proposed a multi-sequence fusion model, designed for combining t1-weighted imaging and multi-b-value dwi to synthesize ce-mri for the first time. ii we invented hierarchical fusion module, weighted difference module and multi-sequence attention module to enhance the fusion at different scale, to control the contribution of different sequence and maximising the usage of the information within and across sequences. iii from the perspective of clinical application, our proposed model can be used to synthesize ce-mri, which is expected to reduce the use of gbca."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.1,Patient Collection and Pre-processing,"this study was approved by institutional review board of our cancer institute with a waiver of informed consent. we retrospectively collected 765 patients with breast cancer presenting at our cancer institute from january 2015 to november 2020, all patients had biopsy-proven breast cancers (all cancers included in this study were invasive breast cancers, and ductal carcinoma in situ had been excluded). the mris were acquired with philips ingenia all mris were resampled to 1 mm isotropic voxels and uniformly sized, resulting in volumes of 352 × 352 pixel images with 176 slices per mri, and subsequent registration was performed based on advanced normalization tools (ants) "
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,4.0,Conclusion,"we have developed a multi-sequence fusion network based on multi-b-value dwi to synthesize ce-mri, using source data including dwis and t1-weighted fatsuppressed mri. compared to existing methods, we avoid the challenges of using full-sequence mri and aim to be selective on valuable source data dwi. hierarchical fusion generation module, weighted difference module, and multisequence attention module have all been shown to improve the performance of synthesizing target images by addressing the problems of synthesis at different scales, leveraging differentiable information within and across sequences. given that current research on synthetic ce-mri is relatively sparse and challenging, our study provides a novel approach that may be instructive for future research based on dwis. our further work will be to conduct reader studies to verify the clinical value of our research in downstream applications, such as helping radiologists on detecting tumors. in addition, synthesizing dynamic contrastenhanced mri at multiple time points will also be our future research direction. our proposed model can potentially be used to synthesize ce-mri, which is expected to reduce or avoid the use of gbca, thereby optimizing logistics and minimizing potential risks to patients."
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Image,"(ii) clarity of tumor-to-normal tissue interface. the clarity of tumor-normal tissue interface is critical for tumor delineation, which directly affects the delineation outcomes. oncologists were asked to use a 5-point likert scale ranging from 1 (poor) to 5 (excellent) to evaluate the clarity of tumor-to-normal tissue interface. paired two-tailed t-test (with a significance level of p = 0.05) was applied to analyses if the scores obtained from real patients and synthetic patients are significantly different. (iii) veracity of contrast enhancement in tumor invasion risk areas. in addition to the critical tumor-normal tissue interface, the areas surrounding the npc tumor will also be considered during delineation. to better evaluate the veracity of contrast enhancement in vce-mri, we selected 25 tumor invasion risk areas according to  the jaccard index (ji)  where r ce and r vce represents the set of risk areas that recorded from ce-mri and corresponding vce-mri, respectively. ji measures similarity of two datasets, which ranges from 0% to 100%. higher ji indicates more similar of the two sets. (iv) efficacy in primary tumor staging. a critical rt-related application of ce-mri is tumor staging, which plays a critical role in treatment planning and prognosis prediction  primary gtv delineation. gtv delineation is the foremost prerequisite for a successful rt treatment of npc tumor, which demands excellent precision  to mimic the real clinical setting, contrast-free t1w, t2w mri and corresponding ct of each patient were imported into the eclipse system since sometimes t1w and t2w mri will also be referenced during tumor delineation. due to both real patients and synthetic patients were involved in delineation, to erase the delineation memory of the same patient, we separated the patients to two datasets, each with the same number of patients, both two datasets with mixed real patients and synthetic patients without overlaps (i.e., the ce-mri and vce-mri from the same patient are not in the same dataset).when finished the first dataset delineation, there was a one-month interval before the delineation of the second dataset. after the delineation of all patients, the dice similarity coefficient (dsc)  dice similarity coefficient (dsc). dsc is a broadly used metric to compare the agreement between two segmentations  where c ce and c vce represent the contours delineated from real patients and synthetic patients, respectively."
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Hausdorff Distance (HD).,"even though dsc is a well-accepted segmentation comparison metric, it is easily influenced by the size of contours. small contours typically receive lower dsc than larger contours  where d(x, c vce ) and d(y, c ce ) represent the distance from point x in contour c ce to contour c vce and the distance from point y in contour c vce to contour c ce . (i) distinguishability between ce-mri and vce-mri. the overall judgement accuracy for the mri volumes was 53.33%, which is close to a random guess accuracy (i.e., 50%). for institution-1, 2 real patients were judged as synthetic and 1 synthetic patient was considered as real. for institution-2, 2 real patients were determined as synthetic and 4 synthetic patients were determined as real. for institution-3, 2 real patients were judged as synthetic and 3 synthetic patients were considered as real. in total, 6 real patients were judged as synthetic and 8 synthetic patients were judged as real. (ii) clarity of tumor-to-normal tissue interface. the overall clarity scores of tumorto-normal tissue interface for real and synthetic patients were 3.67 with a median of 4 and 3.47 with a median of 4, respectively. no significant difference was observed between these two scores (p = 0.38). the average scores for real and synthetic patients were 3.6 and 3, 3.6 and 3.8, 3.8 and 3.6 for institution-1, institution-2, and institution-3, respectively. 5 real patients got a higher score than synthetic patients and 3 synthetic patients obtained a higher score than real patients. the scores of the other 7 patient pairs were the same. (iii) veracity of contrast enhancement in tumor invasion risk areas. the overall ji score between the recorded tumor invasion risk areas from ce-mri and vce-mri was 74.06%. the average ji obtained from institution-1, institution-2, and institution-3 dataset were similar with a result of 71.54%, 74.78% and 75.85%, respectively. in total, 126 risk areas were recorded from the ce-mri for all of the evaluation patients, while 10 (7.94%) false positive high risk invasion areas and 9 (7.14%) false negative high risk invasion areas were recorded from vce-mri. (iv) efficacy in primary tumor staging. a t-staging accuracy of 86.67% was obtained using vce-mri. 13 patient pairs obtained the same staging results. for the institution-2 data, all synthetic patients observed the same stages as real patients."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,1.0,Introduction,"lung cancer is the leading cause of cancer death in the united states, and early detection is key to improving survival rates. ct lung cancer screening is a lowdose ct (ldct) scan of the chest that can detect lung cancer at an early stage, when it is most treatable. however, the current workflow for performing ct lung scans still requires an experienced technician to manually perform pre-scanning steps, which greatly decreases the throughput of this high volume procedure. while recent advances in human body modeling  since ldct scans are obtained in a single breath-hold and do not require any contrast medium to be injected, the scout scan consumes a significant portion of the scanning workflow time. it is further increased by the fact that tube rotation has to be adjusted between the scout and actual ct scan. furthermore, any patient movement during the time between the two scans may cause misalignment and incorrect dose profile, which could ultimately result in a repeat of the entire process. finally, while minimal, the radiation dose administered to the patient is further increased by a scout scan. we introduce a novel method for estimating patient scanning parameters from non-ionizing 3d camera images to eliminate the need for scout scans during pre-scanning. for ldct lung cancer screening, our framework automatically estimates the patient's lung position (which serves as a reference point to start the scan), the patient's isocenter (which is used to determine the table height for scanning), and an estimate of patient's water equivalent diameter (wed) profiles along the craniocaudal direction which is a well established method for defining size specific dose estimate (ssde) in ct imaging  -a novel workflow for automated ct lung cancer screening without the need for scout scan -a clinically relevant method meeting iec 62985:2019 requirements on wed estimation. -a generative model of patient wed trained on over 60, 000 patients. -a novel method for real-time refinement of wed, which can be used for dose modulation"
Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.2,Patient Preparation,patient positioning is the first step in lung cancer screening workflow. we first need to estimate the table position and the starting point of the scan. we propose to estimate the table position by regressing the patient isocenter and the starting point of the scan by estimating the location of the patient's lung top. starting position. we define the starting position of the scan as the location of the patient's lung top. we trained a denseunet  isocenter. the patient isocenter is defined as the centerline of the patient's body. we trained a densenet 
Automated CT Lung Cancer Screening Workflow Using 3D Camera,4.0,Conclusion,"we presented a novel 3d camera based approach for automating ct lung cancer screening workflow without the need for a scout scan. our approach effectively estimates start of scan, isocenter and water equivalent diameter from depth images and meets the iec acceptance criteria of relative wed error. while this approach can be used for other thorax scan protocols, it may not be applicable to trauma (e.g. with large lung resections) and inpatient settings, as the deviation in predicted and actual wed would likely be much higher. in future, we plan to establish the feasibility as well as the utility of this approach for other scan protocols and body regions."
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,1.0,Introduction,"diffuse glioma is the most common and aggressive primary brain tumors in adults, accounting for more deaths than any other type  recently, deep learning has achieved success in diagnosing various tumors  this paper proposes a deep learning model (deepmo-glioma) for glioma classification based on wsis, aiming to reflect the molecular pathology paradigm. previous methods are proposed to integrate histology and genomics for tumour diagnosis  moreover, multiple molecular markers are needed for classifying cancers, due to complex tumor biology. to reflect real-world clinical scenarios, we formulate predicting multiple molecular markers as a multi-label classification (mlc) task. previous mlc methods have successfully modeled the correlation among labels  lastly, we focus on modeling the interaction between molecular markers and histology. specifically, we devise a novel inter-omic interaction strategy to model the interaction between the predictions of molecular markers and histology, e.g., idh mutation and nmp, both of which are relevant in diagnosing glioblastoma. particularly, we design a dynamical confidence constraint (dcc) loss that constrains the model to focus on similar areas of wsis for both tasks. to the best of our knowledge, this is the first attempt to classify diffuse gliomas via modeling the interaction of histology and molecular markers. our main contributions are: "
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,1.0,Introduction,"esophageal cancer is a significant contributor to cancer-related deaths globally  in the clinical setting, patients may undergo a second round of rt treatment to achieve complete tumor control when initial treatment fails to completely eradicate cancer  recently, advances in deep learning  in this paper, we present a comprehensive study on accurate gtv delineation for the second course rt. we proposed a novel prior anatomy and rt information enhanced second-course esophageal gtv segmentation network (artseg). a region-preserving attention module (ram) is designed to effectively capture the long-range prior knowledge in the esophageal structure, while preserving regional tumor patterns. to the best of our knowledge, we are the first to reveal the domain gap between the first and second courses for gtv segmentation, and explicitly leverage prior information from the first course to improve gtv segmentation performance in the second course. the medical images are labeled sparsely, which are isolated by different tasks  nature of gtv on esophageal locations. to achieve this, we efficiently exploit knowledge from multi-center datasets that are not tailored for second-course gtv segmentation. our training strategy does not specific to any tasks but challenges the network to retrieve information from another encoder with augmented inputs, which enables the network to learn from the above three aspects. extensive quantitative and qualitative experiments validate our designs."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,2.0,Network Architecture,"in the first course of rt, a ct image denoted as i 1 is utilized to manually delineate the esophageal gtv, g 1 . during the second course of rt, a ct image i 2 of the same patient is acquired. however, i 2 is not aligned with i 1 due to soft tissue movement and changes in tumor volume that occurred during the first course of treatment. both images i 1/2 have the spatial shape of h × w × d. our objective is to predict the esophageal gtv g 2 of the second course. it would be advantageous to leverage insights from the first course, as it comprises comprehensive information pertaining to the tumor in its preceding phase. therefore, the input to encoder e 1 consists of the concatenation of i 1 and g 1 to encode the prior information (features f d 1 ) from the first course, while encoder e 2 embeds both low-and high-level features f d 2 of the local pattern of i 2 (fig.  where the spatial shape of , with 2 d+4 channels. region-preserving attention module. to effectively learn the prior knowledge in the elongated esophagus, we design a region-preserving attention module (ram), as shown in fig.  since mha perturbs the positional information, we preserve the tumor local patterns by concatenating original features to the attentive features at the channel dimension, followed by a 1 × 1 × 1 bottleneck convolution ξ 1×1×1 to squeeze the channel features (named as ram), as shown in the following equations, where the lower-level features from both encoders are fused by concatenation. the decoder d generates a probabilistic prediction ) with skip connections (fig. "
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3.0,Training Strategy,"the network should learn from three aspects: 1) tumor volume variation: the structural changes of the tumor from the first to the second course; 2) cancer cell proliferation: the tumor in esophageal cancer tends to infiltrate into the adjacent tissue; 3) reliance of gtv on esophageal anatomy: the anatomical dependency between esophageal gtv and the position of the esophagus. medical images are sparsely labeled which are isolated by different tasks  in order to fully leverage both public and private datasets, the training objective should not be specific to any tasks. here, we denote g 1 /g 2 as prior/target annotations respectively, which are not limited only to the gtv areas. as shown in fig. "
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3.1,Tumor Volume Variation,"the differences in tumor volume between the first and second courses following an rt treatment can have a negative impact on the state-of-the-art (sota) learning-based techniques, which will be discussed in sect. 4.2. to adequately monitor changes in tumor volume and integrate information from the initial course into the subsequent course, a paired first-second courses dataset s p = {i 1  p , i 2 p , g 1 p ; g 2 p } is necessary for training. in s p , i 1 p and i 2 p are the first and second course ct images, while g 1 p and g 2 p are the corresponding gtv annotations."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3.2,Cancer Cell Proliferation,"the paired dataset s p for the first and second courses is limited, whereas an unpaired gtv dataset s v = {i v ; g v } can be easily obtained in a standard clinical workflow with a substantial amount. s v lacks its counterpart for the second course, in which i v /g v are the ct image and the corresponding annotation for gtv. to address this, we apply two distinct randomized augmentations, p 1 , p 2 , to mimic the unregistered issue of the first and second course ct. the transformed data is feed into the encoders e 1/2 as shown in the following equations: , p 1 (g e ), p 2 (i e ), p 2 (g e ), when i e , g e ∈ s e . (4) the esophageal tumor can proliferate with varying morphologies into the surrounding tissues. although not paired, s v contains valuable information about the tumor. challenging the network to query information within gtv will enhance the capacity to retrieve pertinent information for the tumor positions."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.1,Experimental Setup,"datasets. the paired first-second course dataset, s p , is collected from sun yat-sen university cancer center (ethics approval number: b2023-107-01), comprising paired ct scans of 69 distinct patients from south china. we collected the gtv dataset s v from medmind technology co., ltd., which has ct scans from 179 patients. for both s p and s v , physicians annotated the esophageal cancer gtv in each ct. the gtv volume statistics (cm 3 , mean ± std.) in s v is 40.60 ± 29.75, and is 83.70 ± 55.97/71.66 ± 49.36 for the first/second course rt in s p respectively. additionally, we collect s e from segthor  performance metrics. dice score (dsc), averaged surface distance (asd) and hausdorff distance (hsd) are used as metrics for evaluation. the wilcoxon signed-rank test is used to compare the performance of different methods."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.2,Domain Gap Between the First and Second Course,"as previously mentioned, the volume of the tumors changes after the first course of rt. to demonstrate the presence of a domain gap between the first and second courses, we train sota methods with datasets s train p and s v , by feeding the data sequentially into the network. we then evaluate the models on s test p . the results presented in table  figure "
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.3,Evaluations of Second-Course GTV Segmentation Performance,"combination of various datasets. table  by subsequently incorporating s e for structural esophagus prior knowledge, the dsc improved to 69.42%. meanwhile, the esophageal tumor comprises two primary regions, the original part located in the esophagus and the extended part that has invaded the surrounding tissue. as shown in fig.  when s v is incorporated for learning tumor proliferation, the dsc improved to 72.64%. we can observe from case 2 in fig.  region-preserving attention module. although introducing the esophageal structural prior knowledge using s e can improve the performance in dsc and asd (table  however, there is no performance gain with mha as shown in table  to tackle the aforementioned problem, we propose ram which involves the concatenation of the original features with attention outputs, allowing for the preservation of convolution-generated regional tumor patterns while effectively comprehending long-range prior knowledge specific to the esophagus. finally, our proposed artseg with ram achieves the best dsc/hsd of 75.26%/19.75 mm, and outperforms its ablations as well as other baselines, as shown in table  limitations. for the method's generalizability, analysis of diverse imaging protocols and segmentation backbones are inadequate. besides, artseg requires more computational resources due to its dual-encoder and attention design."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,5.0,Conclusion,"in this paper, we reveal the domain gap between the first and second courses of rt for esophageal gtv segmentation. to improve the accuracy of gtv declination in the second course, we explicitly incorporated the naturally existing prior information from the first course. besides, to efficiently leverage prior knowledge contained in various medical ct datasets, we train the network in an information-querying manner. we proposed ram to capture long-range prior knowledge in the esophageal structure, while preserving the regional tumor patterns. our proposed artseg incorporates prior knowledge of the tumor volume variation, cancer cell proliferation, and reliance of gtv on esophageal anatomy, which enhances the gtv segmentation accuracy in the second course rt. our future research includes accurate delineation for multiple targets in the second course and knowledge transferring through the time series of multiple courses."
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,1.0,Introduction,"lung cancer is one of the most fatal diseases worldwide, and early diagnosis of the pulmonary nodule has been identified as an effective measure to prevent lung cancer. deep learning-based methods for lung nodule classification have been widely studied in recent years  however, the aforementioned methods still face challenges in distinguishing visually similar samples with adjacent rank labels. for example, in fig.  to integrate text annotations into the image-domain learning process, an effective text encoder providing accurate textual features is required. fortunately, recent advances in vision-language models, such as contrastive languageimage pre-training (clip)  the contributions of this paper are summarized as follows. 1) we propose clip-lung for lung nodule malignancy prediction, which leverages clinical textual knowledge to enhance the image encoder and classifier. 2) we design a channel-wise conditional prompt module to establish consistent relationships among the correlated text tokens and feature maps. 3) we simultaneously align the image features with class and attribute features through contrastive learning while generating more explainable attention maps. "
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,1.0,Introduction,"skin cancer is a serious and widespread form of cancer that requires early detection for successful treatment. computer-aided diagnosis systems (cad) using deep learning models have shown promise in accurate and efficient skin lesion diagnosis. however, recent research has revealed that the success of these models may be a result of overly relying on ""spurious cues"" in dermoscopic images, such as rulers, gel bubbles, dark corners, and hairs  to alleviate the artifact bias and enhance the model's generalization ability, we rethink the problem from the domain generalization (dg) perspective, where a model trained within multiple different but related domains are expected to perform well in unseen test domains. as illustrated in fig.  previous dg algorithms learning domain-invariant features from source domains have succeeded in natural image tasks  to overcome the above problems, we propose an environment-aware prompt vision transformer (epvt) for domain generalization of skin lesion recognition. on the one hand, inspired by the emerging prompt learning techniques that embed prompts into a model for adaptation to diverse downstream tasks  our contributions can be summarized as:  (3) a domain mixup strategy is devised to reduce the co-artifacts specific to dermoscopic images; (4) extensive experiments on four out-of-distribution skin datasets and six biased isic datasets demonstrate the outperforming generalization ability and robustness of epvt under heterogeneous distribution shifts."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.1,Dataset and Evaluation,"real-world pnens dataset. we validated our method on a real-world pnens dataset from two centers. all patients with arterial phase computed tomography (ct) images were included. the dataset contained 264 and 28 patients in center 1 and center 2, and a senior radiologist annotated the bounding boxes for all 408 and 28 lesions. we extracted 37 labels from clinical reports, including survival, immunohistochemical (ihc), ct findings, etc. among them, 1)recist drug response (rs), 2)tumor shrink (ts), 3)durable clinical benefit (dcb), 4)progression-free survival (pfs), 5)overall survival (os), 6)grade (gd), 7)somatostatin receptor subtype 2(sstr2), 8)vascular endothelial growth factor receptor 2 (vefgr2), 9)o6-methylguanine methyltransferase (mgmt), 10)metastatic foci (mtf), and 11)surgical recurrence (rt) are main tasks, and the remaining are auxiliary tasks. 143 and 28 lesions were segmented by radiologists, and the radiomics features of them were extracted, of which 162 features were selected and binarized as auxiliary tasks because of its statistically significant correlation with the main labels. the label distribution and the overlap ratio (jaccard index) of lesions between pairs of labels are shown in fig. "
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,1.0,Introduction,"breast cancer is a serious health problem with high incidence and wide prevalence for women throughout the world  therefore, there is a high demand for automatic and robust methods to achieve accurate breast tumor segmentation. however, due to speckle noise and shadows in ultrasound images, breast tumor boundaries tend to be blurry and are difficult to be distinguished from background. furthermore, the boundary and size of breast tumors are always variable and irregular  various approaches based on deep learning have been developed for tumor segmentation with promising results  to address these challenges, we present, to the best of our knowledge, the first work to adopt multi-scale features collected from large set of clinical ultrasound images for breast tumor segmentation. the main contributions of our work are as follows: "
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,3.1,Dataset and Implementation Details,"we collected 10927 cases for this research from yunnan cancer hospital. each scan is with resolution of 1 × 1 mm 2 and size of 512 × 480. the breast tumors of each case are delineated by three experienced experts. five-fold cross validation is performed on the dataset in all experiments to verify our proposed network. for external validation, we further test our model on two independent publicly-available datasets collected by stu-hospital (dataset 1) "
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,3.2,Comparison with State-of-the-Art Methods,"to verify the advantages of our proposed model for breast tumor segmentation in ultrasound images, we compare our deep-supervised convolutional network with the state-ofthe-art tumor segmentation methods, including deepres  representative segmentation results using different methods are provided in fig. "
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,4.0,Conclusion,"in this paper, we have developed a large pre-trained model for breast tumor segmentation from ultrasound images. in particular, two constraints are proposed to exploit both image similarity and space correlation information for refining the prediction maps. moreover, our proposed deep supervision strategy is used for quality control at each decoding stage, optimizing prediction maps layer-by-layer for overall performance improvement. using a large clinical dataset, our proposed model demonstrates not only state-of-the-art segmentation performance, but also the outstanding generalizability to new ultrasound data from different sites. besides, our large pre-trained model is general and robust in handling various tumor types and shadow noises in our acquired clinical ultrasound images. this also shows the potential of directly applying our model in real clinical applications."
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,1,Introduction,"mitochondria are membrane-bound organelles that generate the primary energy required to power the cell activities, thereby crucial for metabolism. mitochondrial dysfunction, which occurs when mitochondria are not functioning properly has been witnessed as a major factor in numerous diseases, including noncommunicable chronic diseases (e.g, cardiovascular and cancer), metabolic (e.g, obesity) and neurodegenerative (e.g, alzheimer and parkinson) disorders  earlier works on mitochondria segmentation employ standard image processing and machine learning methods  when designing a attention-based framework for 3d mitochondria instance segmentation, a straightforward way is to compute joint spatio-temporal selfattention where all pairwise interactions are modelled between all spatiotemporal tokens. however, such a joint spatio-temporal attention computation is computation and memory intensive as the number of tokens increases linearly with the number of input slices in the volume. in this work, we look into an alternative way to compute spatio-temporal attention that captures long-range global contextual relationships without significantly increasing the computational complexity. our contributions are as follows: -we propose a hybrid cnn-transformers based encoder-decoder framework, named stt-unet. the focus of our design is the introduction of a split spatio-temporal attention (sst) module that captures long-range dependencies within the cubic volume of human and rat mitochondria samples. the sst module independently computes spatial and temporal self-attentions in parallel, which are then later fused through a deformable convolution. -to accurately delineate the region of mitochondria instances from the cluttered background, we further introduce a semantic foreground-background (fg-bg) adversarial loss during the training that aids in learning improved instance-level features. -we conduct experiments on three commonly used benchmarks: lucchi  mitoem-r  our stt-unet approach achieves superior segmentation performance by accurately segmenting 16% more cell instances in these examples, compared to res-unet-r. segmentation performance on all three datasets. on lucchi test set, our stt-unet outperforms the recent  figure "
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,2,Methods,"data measurements. cest imaging was performed in seven subjects, including two glioblastoma patients, after written informed consent was obtained to investigate the dependence of cest effects on b 1 in brain tissue. the local ethics committee approved the study. all volunteers were measured at three b 1 field strengths 0.72 μt, 1.0 μt, and 1.5 μt. a method as described by mennecke et al.  conditional autoencoder. we developed a conditional autoencoder (cae) to solve the b 1 inhomogeneity problem, which is essential for the generation of metabolic cest contrast maps at 7t. the left part of fig.  physics-informed autoencoder. the lorentzian model and its b 1dispersion can be derived from the underlying spin physics described by the bloch-mcconnell equation system  where l denotes the lorentz function. the direct saturation pool (water) was defined as ( the remaining other four pools were defined as ) 2 , i ∈ amide, amine, rn oe, ssm t . ( the right part of fig.  bound loss. the peak positions δ i and widths τ i of the pools had to be within certain bounds so that certain neurons in the latent space layer of piae would not be exchanged and provide the same pool parameters for all samples. we developed a simple cost function along the lines of the hinge loss  the bound loss increases linearly as the output of the latent space neurons of piae exceeds or recede from the boundaries. the lower and upper limits for positions and widths are given in table  training and evaluation. four healthy volunteers formed the training and validation sets. the test set consisted of the two tumor patients and one healthy subject. to ensure that the outcomes were exclusively based on the cest-spectrum and not influenced by spatial position, the training was carried out voxel-by-voxel. consequently, there were approximately one million cestspectra for the training process. cae was first trained with mse loss. in this step, the cae encoder was fed with the cest-spectrum of a specific b 1 saturation amplitude, and it generated two cest-spectra, one for the input b 1 saturation level and the other for the b 1 level injected into the latent space (cf. fig.  piae, on the other hand, was trained with a combination of mse loss and bound loss. the piae loss was described as follows for evaluation we input the uncorrected cest-spectrum acquired at 1μt and generated corrected cest-spectra at b 1 0.5, 0.72, 1.0, 1.3, 1.5 μt. piae encoder yielded the amplitudes of 5-pool for b 1 corrected cest-spectrum. its decoder reconstructed the b 1 b 0 fitted cest-spectrum. the b 0 correction simply refers to the shift of the position of the water peak to 0 ppm. cest quantification. the multi-b 1 cest-spectra allow quantification of cest effects (amide, rnoe, amine)  where f i , k i , and r 2i express the concentrations, exchange rates, and relaxation rates of the pools. z ref defines the sum of all 5 distributions at the resonance frequency of the specific pool in b 1 b 0 corrected cest-spectrum and w 1 is the frequency of the oscillating field. the amplitudes of cest contrasts in the lorentzian function have the b 1 dispersion function given by the labeling efficiency α (eq. 7). the exchange rate occurs here separately from the concentration, which allows their quantification via the b 1 dispersion. concentration and exchange rate were fitted as a product and denoted as z 1 (quantified maps), and k(k+r 2 ) was also fitted with the single term z 2 using trust-region reflective least squares "
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,3.2,Application to a Digital Pathology Dataset,"her2 dataset. human epidermal growth factor receptor 2 (her2 or her2/neu) is a protein involved in normal cell growth, which plays an important role in the diagnosis and treatment of breast cancer  deep clustering models applied to the her2 dataset. we evaluate the performance and behavior of the dec, vade, and cdvade models on the her2 dataset. we investigate whether the models will learn to distinguish the her2 class labels, the scanner labels, or other potentially meaningful data subgroups in a fully unsupervised fashion. to investigate the clustering abilities of cdvade on the her2 dataset, we inject the her2 class labels into the latent embedding space. we hypothesize that this will disincentivize the encoder network from including information related to the her2 class labels in the latent representations z. thus, with cdvade we aim to guide the clustering towards identifying subgroup structures that are not associated with the her2 classes, and potentially were not previously recognized. the dimensionality of the latent embedding space was set to d = 500 for all three models.  as illustrated by the bar graphs in fig. "
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,1,Introduction,"brachial plexopathy is a form of peripheral neuropathy  radiation fibrosis, primary and metastatic lung cancer, and metastatic breast cancer account for almost three-fourths of causes  magnetic resonance imaging (mri) and ultrasound of the brachial plexus have become two reliable diagnostic tools for brachial plexopathy  many radiomics studies have experimentally demonstrated that image texture has great potential for differentiation of different tissue types and pathologies  with the goal of classifying normal from abnormal bp, we explored the approach of deep texture learning. this paper constructed a bp dataset with the most commonly used bp mris in our clinical practice. considering the shortcoming of traditional patterns, triple point pattern (tpp) is proposed for the quantitative representation of the heterogeneity of abnormal bp's. in contrast to glcm-cnn, tppnet is designed to train models by feeding tpp matrices as the input with a huge number of channels. finally, we analyze the model's performance in the experimental section. the major contributions of this study include 1) directed triangle construction idea for tpp, 2) huge number of tpp matrices as the heterogeneity representations of bp, 3) tppnet with 15 layers and huge number of channels, 4) the bp dataset containing mr images and their corresponding roi masks."
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,2.1,Dataset Preparation and Preprocessing,"following irb approval for this study, we search for patients with metastatic breast cancer who had a breast cancer mri performed between 2010 and 2020 and had morphologically positive bp on the mri report from our electronic medical records (emr) in * hospital. totally, we collect approximate 807 series which include 274 t2, 254 t1 and 279 post-gadolinium. since some scans are seriously degraded due to motion artifacts. therefore, each case underwent several essential image adjustments such as multi-series splitting, two-series merging, slice swapping, artifact checking and boundary corrections. to yield the roi, firstly, we randomly sampled -40% of the sequences including both normal and abnormal ones that were manually segmented with itk-snap by two skilled trainees "
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,4,Conclusions,"in this paper, we develop an approach to carry out the pioneer study of differentiating abnormal bp from normal ones relevant to breast cancer. in particular, tpp is proposed to extract texture features as the representation of bp's heterogeneity from mris. moreover, a tppnet with huge number of initial channels is designed to train the model. to testify our proposed tppnet, a bp dataset is constructed with 452 series including three most commonly used mr sequences in clinical practice, i.e. t2, t1 and post-gadolinium. the best result is yielded when the gray level is 12, intensity rescaling method adopts arc tangent approach. experimental outcomes also demonstrate that the proposed tppnet not only exhibit more stable performances but also outperform six famous state-of-the-art approaches over three most commonly used bp mr sequences."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,1,Introduction,"nuclei detection is a highly challenging task and plays an important role in many biological applications such as cancer diagnosis and drug discovery. rectangle object detection approaches that use cnn have made great progress in the    last decade  recently, detr  in this paper, we introduce circleformer, a transformer-based circular object detection for medical image analysis. inspired by dab-detr, we propose to use an anchor circle (x, y, r) as the query for circular object detection, where (x, y) is the center of the circle and r is the radius. we propose a novel circle cross attention module which enables us to apply circle center (x, y) to extract image features around a circle and make use of circle radius to modulate the cross attention map. in addition, a circle matching loss is adopted in the set-to-set prediction part to process circular predictions. in this way, our design of circle-former lends itself to circular object detection. we evaluate our circleformer on the public monuseg dataset for nuclei detection in whole slide images. experimental results show that our method outperforms both cnn-based methods for box detection and circular object detection. it also achieves superior results compared with recently transformer-based box detection approaches. meanwhile, we carry out ablation studies to demonstrate the effectiveness of each proposed component. to further study the generalization ability of our approach, we add a simple segmentation branch to circleformer following the recent query based instance segmentation models "
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,3.1,Datasets,to demonstrate the utility of tap for a diverse set of specimen and microscopy modalities we use the following four different datasets: hela. human cervical cancer cells expressing histone 2b-gfp imaged by fluorescence microscopy every 30 min  mdck. madin-darby canine kidney epithelial cells expressing histone 2b-gfp (cf. fig.  flywing. drosphila melanogaster pupal wing expressing ecad::gfp (cf. fig.  we use δt = 1. yeast. s. cerevisiae cells (cf. fig.  for each dataset we heuristically choose δt to roughly correspond to the time scale of observable biological processes (i.e. larger δt for higher frame rates). 
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,3.3,Results,"we chose overall accuracy and area under receiver operating characteristic curve (auroc) as the evaluation metrics. evaluation of prompt tuning performance: we compared the proposed prompt-mil with two baselines: 1) a conventional mil model with a frozen feature extractor  the computationally intensive full fine-tuning method under-performed conventional mil and prompt-mil. compared to the full fine-tuning method, our method achieved a relative improvement of 1.29% to 13.61% in accuracy and 3.22% to 27.18% in auroc on the three datasets. due to the relatively small amount of slide-level labels (few hundred to a few thousands) fully fine tuning 5m parameters in the feature model might suffer from overfitting. in contrast, our method contained less than 1.3% of parameters compared to full fine-tuning, leading to robust training.  evaluation of time and gpu memory efficiency: prompt-mil is an efficient method requiring less gpu memory to train and running much faster than full fine-tuning methods. we evaluated the training speed and memory consumption of our method and compared to the full fine-tuning baseline on four different sized wsis in the bright dataset. as shown in table  evaluation on the pathological foundation models: we demonstrated our prompt-mil also had a better performance when used with the pathological foundation model. foundational models refer to those trained on large-scale pathology datasets (e.g. the entire tcga pan-cancer dataset "
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,1,Introduction,"nucleus classification is to identify the cell types from digital pathology image, assisting pathologists in cancer diagnosis and prognosis  a number of methods  based on these observations, we develop a learnable grouping transformer based classifier (gtc) that leverages the similarity between nuclei and their cluster representations to infer their types. specifically, we define a number of nucleus clusters with learnable initial embeddings, and assign nucleus instances to their most correlated clusters by computing the correlations between clusters and nuclei. next, the cluster embeddings are updated with their affiliated instances, and are further grouped into the categorical representations. then, the cell types can be well estimated using the correlations between the nuclei and the categorical embeddings. we propose a novel fully transformer-based framework for nuclei detection and classification, by integrating a backbone, a centroid detector, and the grouping-based classifier. however, the transformer framework has a relatively large number of parameters, which could cause high costs in fine-tuning the whole model on large datasets. on the other hand, there exist domain gaps in the pathological images of different organs, staining, and institutions, which makes it necessary to fine-tune models to new applications. thus, it is of great significance to tune our proposed transformer framework efficiently. inspired by the prompt tuning methods "
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,3.1,Datasets and Implementation Details,"consep 1 [10] is a colorectal nuclear dataset with three types, consisting of 41 h&e stained image tiles from 16 colorectal adenocarcinoma whole-slide images (wsis). the wsis are at 20× magnification and the size of the slides is 500 × 500. we split them following the official partition  is a breast cancer dataset with three types and consists of 120 image tiles from 113 patients. the wsis are at 20× magnification and the size of the slides ranges from 465 × 465 to 504 × 504. we follow the work  lizard 3 "
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,1,Introduction,"recent advances in diffusion mri (dmri) and diffusion signal modeling equip brain researchers with an in vivo probe into microscopic tissue compositions  multi-compartment models are typically used to characterize signals from, for example, intra-and extra-neurite compartments  here, we propose a unified strategy to estimate using mte diffusion data (i) compartment specific t 2 relaxation times; (ii) non-t 2 -weighted (non-t 2 w) parameters of multi-scale microstructure; and (iii) non-t 2 w multi-scale fodfs. our method, called relaxation-diffusion spectrum imaging (rdsi), allows for the direct estimation of non-t 2 w volume fractions and t 2 relaxation times of tissue compartments. we evaluate rdsi using both ex vivo monkey and in vivo human brain mte data, acquired with fixed diffusion times across multiple b-values. using rdsi, we demonstrate the te dependence of t 2 w fodfs. furthermore, we show the diagnostic potential of rdsi in differentiating tumors and normal tissues."
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3.1,Data,"the proposed technique was tested on a healthy-subject dataset and on a dataset containing tumor cases. the first comprises 21 subjects of the human connectome project (hcp) that were used for testing the automated methods tractseg and classifyber  to test the proposed method on pathological data, we used an in-house dataset containing ten presurgical scans of patients with brain tumors. tractography was performed using probabilistic streamline tractography in mitk diffusion. to reduce computational costs, we retained one million streamlines that passed through a manually inserted roi located in an area traversed by the or "
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3.2,Experimental Setup,"to evaluate the proposed method, we conducted two types of experiments. manual segmentation experiments using an interactive prototype of attractive were initiated on the tumor data (holistic evaluation). additionally, reproducible simulations on the freely available hcp and the internal tumor dataset were created (algorithmic evaluation). in order to mimic expert annotation during algorithmic evaluation, class labels were assigned to streamlines using previously generated references. the quality of the predictions was measured by calculating the dice score of the binary mask. the code used for these experiments is publicly available for the algorithmic evaluation, the initial training dataset was created with 20 randomly selected streamlines from the whole-brain tractogram, which have been shown to be a decent number to start training. since some tracts contain only a fraction of streamlines from the entire tractogram, it might be unlikely that the training dataset will contain any streamline belonging to the target tract. therefore, two streamlines of the specific tract were further added to the training dataset, and class weights were used to compensate for the class unbalance. according to fig.  the holistic evaluation was conducted with equal settings, except that the workflow was terminated when the prediction matched the expectation of the expert. to ensure that the initial dataset s rand contained streamlines from the target tract, the expert initiated the active learning workflow by defining a small roi that included fibers of the tract. s rand was created by randomly sampling only those streamlines that pass through this roi. to allow comparison between the proposed and traditional roi-based techniques, the or of subjects from the tumor dataset were segmented using both approaches by an expert familiar with the respective tool, and the time required was reported to measure efficiency. note, in all experiments, the classifier is trained from scratch every iteration, prototypes are generated for each subject individually, and the classifier predicts on data from the same subject it is trained with, as it performs subject-individual tract segmentation and is not used as a fully automated method. to ensure a stable active learning setup that generalizes across different datasets, the whole method was developed on the hcp and applied with fixed settings to the tumor data "
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,4,Discussion,"active learning-based white matter tract segmentation enables the identification of arbitrary pathways and can be applied to cases where fully automated methods are unfeasible. in this work, algorithmic evaluation as well as the implementation of the technique into the gui-based tool attractive including further holistic manual experiments were conducted. the algorithmic evaluation yielded consistent results from the fifth to the tenth iterations on both the hcp and tumor datasets. as expected, outcomes obtained from the tumor dataset were not quite as good as those of the hcp dataset. this trend is generally observed in clinical datasets, which tend to exhibit lower performance levels compared to high-quality datasets, which could be responsible for the decline in the results. preliminary manual experiments with attractive indicated active learning to have shorter segmentation times compared to traditional roi-based techniques. these experiments are in line with the simulations as the generated tracts matched the expectations of the expert after around five to seven iterations, meaning that less than a hundred out of million annotated streamlines are required to train the model. enhancements to the usability of the prototype are expected to further improve efficiency. a current limitation of attractive is the selection of the initial subset, based on randomly sampling streamlines passing through a manually inserted roi. this approach does not guarantee that streamlines of the target tract are included in the subset. in that case, the roi has to be replaced or s rand needs to be regenerated. future analyses, evaluating the inter-and intra-rater variability compared to other interactive approaches, will be conducted on further tracts. for selected scenarios, the ability of the classifier to generalize by learning from previously annotated subjects will be investigated, which may even allow to train a fully automatic classifier for new tracts once enough data is annotated. to further optimize the method, the feature representation or sampling procedure could be improved. uncertainty sampling may select redundant streamlines due to similar high entropy values. instead, annotating samples with high entropy values being highly diverse or correcting false classifications could convey more information. by introducing active learning into tract segmentation, we provide an efficient and intuitive alternative compared to traditional roi-based approaches. attractive has the potential to interactively assist researchers in identifying arbitrary white matter tracts not captured by existing automated approaches."
B-Cos Aligned Transformers Learn Human-Interpretable Features,4,Implementation and Evaluation Details,"task-based evaluation: cancer classification and segmentation is an important first step for many downstream tasks such as grading or staging. therefore, we choose this problem as our target. we classify image patches from the public colorectal cancer dataset nct-crc-he-100k  domain-expert evaluation: our primary objective is to develop an extension of the vision transformer that is more transparent and trusted by medical professionals. to assess this, we propose a blinded study with four steps: (i) randomly selecting images from the test set of tcga-coad-20x (32 samples) and munich-aml-morphology (56 samples), (ii) plotting the last-layer attention and transformer attributions for each image, (iii) anonymizing and randomly shuffling the outputs, (iv) submitting them to two domain experts in histology and cytology for evaluation. most importantly, we show them all the available saliency maps without pre-selecting them to get their unbiased opinion. implementation details: in our experiments, we compare different variants of the b-cos vision transformer and the vision transformer. specifically, we implement two versions of vit: vit-t/8 and vit-s/8. they only differ in parameter size (5m for t models and 22m for s models) and use the same patch size of 8. all bvt models (bvt-t/8 and bvt-s/8) are derivatives of the corresponding vit models. the b-cos transform used in the bvt models has an exponent of b = 2. we use adamw with a cosine learning rate scheduler for optimization and a separate validation set for hyperparameter selection. following the findings of "
B-Cos Aligned Transformers Learn Human-Interpretable Features,,Domain-Expert Evaluation:,"the results show that bvts are significantly more trustworthy than vits (p < 0.05). this indicates that bvt consistently attends to biomedically relevant features such as cancer cells, nuclei, cytoplasm, or membrane "
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,1,Introduction,"glioblastomas (gbms, known as grade iv gliomas) are the most common primary malignant brain tumors with high spatial heterogeneity and varying degrees of aggressiveness  to do so, magnetic resonance imaging (mri) and its derived radiomics have been widely used to study gbm preoperative prognosis over the last few decades. for example, anand et al.  although both mri and its derived radiomics features have been demonstrated to have predictive power for survival analysis in the aforementioned literature, they do not account for brain's functional alternations caused by tumors, which are clinically significant as biologically-interpretable biomarkers of recovery and therapy. these alternations can be reflected by changes in resting-state functional mri (fmri)-derived functional connectivities/connections (fcs) between the blood oxygenation level-dependence (bold) time series of paired brain regions. therefore, the use of fcs to predict overall survival time for gbm has recently attracted increasing attention  nevertheless, current fc-based survival prediction still suffers from two main deficiencies when applied to gbm prognosis. first, due to mass effect and physical infiltration of gbm in the brain, fcs estimated directly from gbm patients' resting-state fmri might be inaccurate, especially when the tumors are near or in the regions of interest. second, resting-state fmri data are not routinely collected for gbm clinical practices, which restricts the size of annotated datasets such that it is infeasible to train a reliable prediction model based on deep learning for survival prediction. in order to circumvent these issues, in this paper we introduce a novel neuroimaging feature family, namely functional lesion network (fln) maps that are generated by our augmented lesion network mapping (a-lnm), for overall survival time prediction of gbm patients. our a-lnm is motivated by lesion network mapping (lnm)  the details of our workflow are described as follows. 1) we first manually segment the whole tumor (regarded as lesion in this paper) on structural mri for all gbm patients, and the resulting lesion masks are mapped onto a reference brain template, e.g., the mni152 2mm 3 template. 2) the proposed a-lnm is next used to generate fln maps for each gbm patient by using resting-state fmri from a large cohort of healthy subjects. specifically, for each patient, we correlate the mean bold time series of all voxels within the lesion with the bold time series of every voxel in the whole brain for all n subjects in the normative cohort, producing n functional disconnection (fdc) maps of voxel-wise correlation values (transformed to zscores). these resulting n fdc maps are partitioned into m disjoint subsets of equal size, and m fln maps are separately obtained by averaging the fdc maps in each of the m subsets. similar to data augmentation schemes, we can artificially boost data volume (i.e., fln maps) up to m times through producing m fln maps for each patient in the a-lnm, which helps to mitigate the risk of over-fitting and improve the performance of overall survival time prediction when learning a deep neural network from a small sized dataset. for this reason, we propose the name ""augmented lnm (a-lnm)"", compared to the traditional lnm where only one fln map is generated per patient by averaging all the n fdc maps. 3) finally, these augmented fln maps are fed to a 3d resnet-based backbone network followed by the average pooling operation and fully-connected layers for gbm survival prediction. to our knowledge, this paper is the first to demonstrate a successful extension of lnm for survival prediction in gbm. to evaluate the predictive power of the fln maps generated by our a-lnm, we conduct extensive experiments on 235 gbm patients in the training dataset of brats 2020 "
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2,Materials and Methods,"2.1 materials gsp1000 processed connectome. it publicly released preprocessed restingstate fmri data of 1000 healthy right-handed subjects with an average age 21.5 ± 2.9 years and approximately equal numbers of males and females from the brain genomics superstruct project (gsp)  brats 2020. it provided an open-access pre-operative imaging training dataset to segment brain tumors of glioblastoma (gbm, belonging to high grade glioma) and low grade glioma (lgg) patients, as well as to predict overall survival time of gbm patients  the union of all the three tumor sub-regions was considered as the whole tumor, which is regarded as the lesion in this paper."
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2.2,Methods,"in this paper, we propose to investigate the feasibility of the novel neuroimaging features, i.e., fln maps, for overall survival time prediction of gbm patients in the training dataset of the brats 2020, in which one patient alive was excluded, and the remaining 235 patients consisted of 89 short-term survivors (less than 10 months), 59 mid-term survivors (between 10 and 15 months), and 87 long-term survivors (more than 15 months). to this end, our framework for the three-class survival classification is shown in fig.  lesion mapping procedures. as stated above, the whole tumor is referred to as a lesion for each gbm patient. from the manual expert segmentation labels of lesions in the 235 gbm patients of the brats 2020, we co-register the lesion masks to the mni152 2mm 3 template by employing a symmetric normalization algorithm in antspy "
Intraoperative CT Augmentation for Needle-Based Liver Interventions,1.0,Introduction,"needle-based liver tumor ablation techniques (e.g., radiofrequency, microwave, laser, cryoablation) have a great potential for local curative tumor control  in standard clinical settings, the insertion of each needle requires multiple check points during its progression, fine-tune maneuvers, and eventual repositioning. this leads to multiple ct acquisitions to control the progression of the needle with respect to the vessels, the target, and other sensible structures  to address this challenge, two main strategies could be considered: image fusion and image processing techniques. image fusion typically relies on the estimation of rigid or non-rigid transformations between 2 images, to bring into the intraoperative image structures of interest only visible in the preoperative data. this process is often described as an optimization problem  on the other hand, deep learning techniques have proven to be very efficient at solving image processing challenges  in this paper we propose an alternative approach, where a neural network learns local image features in a ncct image by leveraging the known preoperative vessel tree geometry and topology extracted from a matching (undeformed) cct. then, the augmented ct is generated by fusing the deformed vascular tree with the non-contrasted intraoperative ct. section 2 presents the method and its integration in the medical workflow. section 3 presents and discusses the results, and finally we conclude in sect. 4 and highlight some perspectives."
Intraoperative CT Augmentation for Needle-Based Liver Interventions,4.0,Conclusion,"in this paper, we proposed a method for augmenting intra-operative ncct images as a means to improve needle ct-guided techniques while reducing the need for contrast agent injection during tumor ablation procedures, or other needle-based procedures. our method uses a u-net architecture to learn local vessel tree image features in the ncct by leveraging the known vessel tree geometry and topology extracted from a matching cct image. the augmented ct is generated by fusing the predicted vessel tree with the ncct. our method is validated on several porcine images, achieving an average dice score of 0.81 on the predicted vessel tree location. in addition, it demonstrates robustness even in the presence of large deformations between the preoperative and intraoperative images. our future steps will essentially involve applying this method to patient data and perform a small user study to evaluate the usefulness and limitations of our approach. aknowledgments. this work was partially supported by french state funds managed by the anr under reference anr-10-iahu-02 (ihu strasbourg). the authors would like to thank paul baksic and robin enjalbert for proofreading the manuscript."
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,1.0,Introduction,"healthy and cancerous soft tissue display different elastic properties, e.g. for breast  taking prostate cancer as an example, biomechanical characterization could guide needle placement for improved cancer detection rates while reducing complications associated with increased core counts, e.g. pain and erectile dysfunction  to perform oce in deep tissue structures, we propose a novel bevel tip oce needle design for the biomechanical characterization during needle insertions. we consider a dual-fiber setup with temporal multiplexing for the combined load and compression sensing at the needle tip. we design an experimental setup that can simulate friction forces and bulk displacement occurring during needle biopsy (fig. "
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,2.3,Experimental Setup,"we build an experimental setup for surface and deep tissue indentations with simulated force and bulk displacement (fig.  for surface measurements, we position the tissue phantoms separately without additional springs or tissue around the needle shaft. we use a motorized linear stage (zfs25b, thorlabs gmbh, ger) to drive the needle while simultaneously logging motor positions. an external force sensor (kd24s 20n, me-meßsysteme gmbh, ger) measures combined axial forces. we consider two gelatin gels as tissue mimicking materials for healthy and cancerous tissue. the two materials (mat. a and mat. b) display a young's modulus of 53.4 kpa and 112.3 kpa, respectively. reference elasticity is determined by unconfined compression experiments of three cylindrical samples for each material according to eq. 1, using force and position sensor data (see supplementary material). the young's modulus is obtained by linear regression for the combined measurements of each material. we calibrate tip force estimation (fiber 2) by indentation of silicone samples with higher tear resistance to ensure that no partial rupture has taken place. we then determine a linear fit according to eq. 5 and obtain a f = 174.4 mn mm -1 from external force sensor and motor position measurements (see supplementary material)."
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,4.0,Discussion and Conclusion,we demonstrate our approach on two tissue mimicking materials that have similar elastic properties as healthy and cancerous prostate tissue 
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,1.0,Introduction,"pelvic fracture is a severe type of high-energy injury, with a fatality rate greater than 50%, ranking the first among all complex fractures  several studies have been proposed to provide more efficient tools for operators. a semi-automatic graph-cut method based on continuous max-flow has been proposed for pelvic fracture segmentation, but it still requires the manual selection of seed points and trail-and-error  fracture segmentation is still a challenging task for the learning-based method because (1) compared to the more common organ/tumor segmentation tasks where the model can implicitly learn the shape prior of an object, it is difficult to learn the shape information of a bone fragment due to the large variations in fracture types and shapes  this paper proposes a deep learning-based method to segment pelvic fracture fragments from preoperative ct images automatically. our major contribution includes three aspects. "
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,1.0,Introduction,"gliomas are the most common central nervous system (cns) tumors in adults, accounting for 80% of primary malignant brain tumors  as the true underlying deformation from brain shift is impossible to obtain and the differences of image features between mri and us are large, quantitative validation of automatic mri-us registration algorithms often rely on homologous anatomical landmarks that are manually labeled between corresponding mri and intra-operative us scans  previously, many groups have proposed algorithms to label landmarks in anatomical scans "
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,3.1,Data and Landmark Annotation,we employed the publicly available easy-resect (retrospective evaluation of cerebral tumors) dataset 
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,1.1,Background,"traditional optical imaging samples the visual spectrum in three diffuse spectral bands (rgb), while hyperspectral imaging (hsi) provides much more detailed spectral information. this information is potentially valuable for making intraoperative decisions, particularly in cases where tissue differentiation is critical but challenging to perform using traditional visualisation techniques. in the case of brain tumour excision, fluorescence-guided resection is commonly used to minimize damage to healthy tissue  while hsi has been integrated into surgical microscope systems  the issue of reduced focal depth in real-time hsi systems could be mitigated by the introduction of a video autofocus system. autofocus methods are divided into active methods, which use transmission to probe the scene, and passive methods, which rely only on incoming light. passive methods are further split into phase-based, which require specialised hardware, and contrast-based, which compare images captured at different focal powers. our investigation focuses on contrast-based methods, which require minimal hardware development. "
Surgical Video Captioning with Mutual-Modal Concept Alignment,3.1,Dataset and Implementation Details,"neurosurgery video captioning dataset. to evaluate the effectiveness of surgical video captioning, we collect a large-scale dataset with 41 surgical videos of endonasal skull base neurosurgery. these surgical videos are recorded at the prince of wales hospital, chinese university of hong kong, where surgeons remove pituitary tumors through the endonasal corridor to the skull base. after necessary data cleaning, we divide these surgical videos with resolution of 1, 920× 1, 080 into 11, 004 thirty-second video clips with clear surgical purposes. these video clips are annotated under tool-tissue interaction (tti) principle  endovis image captioning dataset. we further compare our method with state-of-the-arts on the public endovis-2018 image captioning dataset  implementation details. we implement our sca-net and state-of-the-art captioning methods  evaluation metrics. to evaluate the captioning performance, we adopt standard metrics, including bleu@4 "
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,3.0,Experiments and Results,"in this section, two experiments are conducted. the first explores sensitivity to regularized kelvinlet function hyperparameters k grab , k twist , ε grab , and ε twist and establishes optimal hyperparameters in a training dataset of 11 breast deformations. the second validates the registration method in a breast cancer patient and compares registration accuracy and computation time to previously proposed methods."
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,3.2,Registration Methods Comparison,"this dataset consists of supine breast mr images simulating surgical deformations from one breast cancer patient. a 71-year-old patient with invasive mammary carcinoma in the left breast was enrolled in a study approved by the institutional review board at vanderbilt university. skin fiducial placement, image acquisition, arm placement, and preprocessing steps followed the same protocol detailed in sect. 3.1. the tumor was segmented in both images by a subject matter expert, and a 3d tumor model was created to evaluate tumor overlap metrics after registration. regularized kelvinlet function registration was compared to 3 other registration methods: rigid registration, an fem-based image-to-physical registration method, and an image-to-image registration method. a point-based rigid registration using the skin fiducials provided a baseline comparator for accuracy without deformable correction. the fem-based image-to-physical registration method, detailed in  registration results for the 4 methods are shown in table "
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,1.0,Introduction,"we address the important problem of intraoperative patient-to-image registration in a new way by relying on preoperative data to synthesize plausible transformations and appearances that are expected to be found intraoperatively. in particular, we tackle intraoperative 3d/2d registration during neurosurgery, where preoperative mri scans need to be registered with intraoperative surgical views of the brain surface to guide neurosurgeons towards achieving a maximal safe tumor resection  most existing techniques perform patient-to-image registration using intraoperative mri  the main limitation of existing intraoperative registration methods is that they rely heavily on processing intraoperative images to extract image features (eg., 3d surfaces, vessels centerlines, contours, or other landmarks) to drive registration, making them subject to noise and low-resolution images that can occur in the operating room "
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,4.0,Discussion and Conclusion,"clinical feasibility. we have shown that our method is clinically viable. our experiments using clinical data showed that our method provides accurate registration without manual intervention, that it is computationally efficient, and it is invariant to the visual appearance of the cortex. our method does not require intraoperative 3d imaging such as intraoperative mri or ultrasound, which require expensive equipment and are disruptive during surgery. training patient-specific models from preoperative imaging transfers computational tasks to the preoperative stage so that patient-to-image registration can be performed in near real-time from live images acquired from a surgical microscope. limitations. the method presented in this paper is limited to 6-dof pose estimation and does not account for deformation of the brain due to changes in head position, fluid loss, or tumor resection and assumes a known focal length. in the future, we will expand our method to model non-rigid deformations of the 3d mesh and to accommodate expected changes in zoom and focal depth during surgery. we will also explore how texture variability can be controlled and adapted to the observed image to improve model accuracy."
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,1.0,Introduction,"radiotherapy (rt) has proven effective and efficient in treating cancer patients. however, its application depends on treatment planning involving target lesion and radiosensitive organs-at-risk (oar) segmentation. this is performed to guide radiation to the target and to spare oar from inappropriate irradiation. hence, this manual segmentation step is very time-consuming and must be performed accurately and, more importantly, must be patient-safe. studies have shown that the manual segmentation task accounts for over 40% of the treatment planning duration  nowadays, training of dl segmentation models is predominantly based on loss functions defined by geometry-based (e.g., softdice loss  in this paper, we propose an end-to-end training loss function for dl-based segmentation models that considers dosimetric effects as a clinically-driven learning objective. our contributions are: (i) a dosimetry-aware training loss function for dl segmentation models, which (ii) yields improved model robustness, and (iii) leads to improved and safer dosimetry maps. we present results on a clinical dataset comprising fifty post-operative glioblastoma (gbm) patients. in addition, we report results comparing the proposed loss function, called dose-segmentation loss (doselo), with models trained with a combination of binary cross-entropy (bce) and softdice loss functions."
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,4.0,Discussion and Conclusion,"the ultimate goal of dl-based segmentation for rt planning is to provide reliable and patient-safe segmentations for dosimetric planning and optimally targeting tumor lesions and sparing of healthy tissues. however, current loss functions used to train models for rt purposes rely solely on geometric considerations that have been shown to correlate poorly with dosimetric objectives "
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,1.0,Introduction,"residual tumor in the cavity after head and neck cancer (hnc) surgery is a significant concern as it increases the risk of cancer recurrence and can negatively impact the patient's prognosis  during transoral robotic surgery (tors), surgeons may assess the surgical margin via visual inspection, palpation of the excised specimen and intraoperative frozen sections analysis (ifsa)  label-free mesoscopic fluorescence lifetime imaging (flim) has been demonstrated as an intraoperative imaging guidance technique with high classification performance (auc = 0.94) in identifying in vivo tumor margins at the epithelial surface prior to tumor excision  however, ability of label-free flim to identify residual tumors in vivo in the surgical cavity (deep margins) has not been reported. one significant challenge in developing a flim-based classifier to detect tumor in the surgical cavity is the presence of highly imbalanced labels. surgeons aim to perform an en bloc resection, removing the entire tumor and a margin of healthy tissue around it to ensure complete excision. therefore, in most cases, only healthy tissue in left in the cavity. to address the technical challenge of highly imbalanced label distribution and the need for intraoperative real-time cavity imaging, we developed an intraoperative flim guidance model to identify residual tumors by classifying residual cancer as anomalies. our proposed approach identified all patients with psm. in contrast, the ifsa reporting a sensitivity of 0.5 "
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.2,Patient Cohort and FLIm Data Labeling,"the research was performed under the approval of the uc davis institutional review board (irb) and with the patient's informed consent. all patients were anesthetized, intubated, and prepared for surgery as part of the standard of care. n = 22 patients are represented in this study, comprising hnc in the palatine tonsil (n = 15) and the base of the tongue (n = 7). for each patient, the operating surgeon conducted an en bloc surgical tumor resection procedure (achieved by tors-electrocautery instruments), and the resulting excised specimen was sent to a surgical pathology room for grossing. the tissue specimen was serially sectioned to generate tissue slices, which were then formalin-fixed, paraffin-embedded, sectioned, and stained to create hematoxylin & eosin (h&e) slides for pathologist interpretation (see fig.  after the surgical excision of the tumor, an in vivo flim scan of approximately 90 s was conducted within the patient's surgical cavity, where the tumor was excised. to validate optical measurements to pathology labels (e.g., benign tissue vs. residual tumor), pathology labels from the excision margins were digitally annotated by a pathologist on each h&e section. the aggregate of h&e sections was correspondingly labeled on the ex vivo specimen at the cut lines where the tissue specimen was serially sectioned. thereafter, the labels were spatially registered in vivo within the surgical cavity. this process enables the direct validation of flim measurements to the pathology status of the electrocauterized surgical margins (see table "
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.5,Classifier Training and Evaluation,"the novelty detection model used for detecting residual cancer is evaluated at the pointmeasurement level to assess the diagnostic capability of the method over an entire tissue surface. the evaluation followed a leave-one-patient-out cross-validation approach. the study further compared gods with two other novelty detection models: robust covariance and, one-class support vector machine (oc-svm)  results of a binary classification model using svm are also shown in the supplementary section table "
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,3.0,Results,"table  the gods uses two separating hyperplanes to minimize the distance between the two classifiers by learning a low-dimensional subspace containing flim data properties of healthy labels. residual tumor labels are detected by calculating the distance between the projected data points and the learned subspace. points that are far from the subspace are classified as residual tumors. we observed that the gods with the flim decay curves in the cdt space achieve the best classification performance compared to other novelty detection models with a mean accuracy of 0.76 ± 0.02. this is mainly due to the robustness of the model, the ability to handle high-dimensional data, and the contrast in the flim decay curves. the contrast in the flim decay curves was further improved in the cdt space by transforming the flim decay curves to a normalized scale and improving linear separability."
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,4.0,Discussion,"curent study demonstrates that label-free flim parameters-based classification model, using a novelty detection aproach, enables identification of residual tumors in the surgical cavity. the proposed model can resolve residual tumor at the point-measurement level over a tissue surface. the model reported low point-level false negatives and positives. moreover, the current approach correctly identified all patients with psms (see fig.  in context to the standard of care, the proposed residual tumor detection model exhibits high patient-level sensitivity (sensitivity = 1) in detecting patients with psms. in contrast, defect-driven ifsa reports a patient-level sensitivity of 0.5  the false positive predictions from the classification model presented two trends: false positives in an isolated region and false positives spreading across a larger region. isolated false positives are often caused by the noise of the flim system and are accounted for by the interpolation approach used for the classifier augmentation (refer to supplementary section fig.  the novelty detection model generalizes to the healthy labels and considers data falling off the healthy distribution as residual cancer. the flim properties associated with the healthy labels in the cavity are heterogeneous due to the electrocautery effects. electrocautery effects are mainly thermal and can be observed by the levels of charring in the tissue. refining the training labels based on the levels of charring could lead to a more homogeneous representation of the training set and result in an improved classification model with better generalization."
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,5.0,Conclusion,this study demonstrates a novel flim-based classification method to identify residual cancer in the surgical cavity of the oropharynx. the preliminary results underscore the significance of the proposed method in detecting psms. the model will be validated on a larger patient cohort in future work and address the limitations of the point-level false positive and negative predictions. this work may enhance surgical precision for tors procedures as an adjunctive technique in combination with ifsa.
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,1.0,Introduction,"resection of early-stage brain tumors can greatly reduce the mortality rate of patients. during the surgery, brain tissue deformation (called brain shift) can occur due to various causes, such as gravity, drug administration, and pressure change after craniotomy. while modern magnetic resonance imaging (mri) techniques can provide rich anatomical and physiological information with various contrasts (e.g., fmri) for more elaborate pre-surgical planning, intra-operative mri that can track brain shift requires a complex setup and is costly. in contrast, intra-operative ultrasound (ius) has gained popularity for real-time imaging during surgery to monitor tissue deformation and surgical tools because of its lower cost, portability, and flexibility  recently, automatic quality assessment for medical image registration has attracted increasing attention "
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.1,Dataset and Preprocessing,"for methodological development and assessment, we used the resect (retro-spective evaluation of cerebral tumors) dataset "
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,1.0,Introduction,"cancer remains a significant public health challenge worldwide, with a new diagnosis occurring every two minutes in the uk (cancer research uk  it is crucial to accurately determine the sensing area, with positive signal potentially indicating cancer or affected lymph nodes. geometrically, the sensing area is defined as the intersection point between the gamma probe axis and the tissue surface in 3d space, but projected onto the 2d laparoscopic image. however, it is not trivial to determine this using traditional methods due to poor textural definition of tissues and lack of per-pixel ground truth depth data. similarly, it is also challenging to acquire the probe pose during the surgery. problem redefinition. in this study, in order to provide sensing area visualization ground truth, we modified a non-functional 'sensei' probe by adding a miniaturized laser module to clearly optically indicate the sensing area on the laparoscopic images -i.e. the 'probe axis-surface intersection'. our system consists of four main components: a customized stereo laparoscope system for capturing stereo images, a rotation stage for automatic phantom movement, a shutter for illumination control, and a daq-controlled switchable laser module (see fig. "
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,3.0,Dataset,"to validate our proposed solution for the newly formulated problem, we acquired and publicly released two new datasets. in this section, we introduce the hardware and software design that was used to achieve our final goal, while fig.  all data acquisition and devices were controlled by python and labview programs, and complete data sets of the above images were collected on visually realistic phantoms for multiple probe and laparoscope positions. this provided 10 tissue surface profiles for a specific camera-probe pose, repeated for 120 different camera-probe poses, mimicking how the probe may be used in practice. therefore, our first newly acquired dataset, named jerry, contains 1200 sets of images. since it is important to report errors in 3d and in millimeters, we recorded another dataset similar to jerry but also including ground truth depth map for all frames by using structured-lighting system  these datasets have multiple uses such as: -intersection point detection: detecting intersection points is an important problem that can bring accurate surgical cancer visualization. we believe this is an under-investigated problem in surgical vision. -depth estimation: corresponding ground truth will be released. -tool segmentation: corresponding ground truth will be released."
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,6.0,Conclusion,"in this work, a new framework for using a laparoscopic drop-in gamma detector in manual or robotic-assisted minimally invasive cancer surgery was presented, where a laser module mock probe was utilized to provide training guidance and the problem of detecting the probe axis-tissue intersection point was transformed to laser point position inference. both the hardware and software design of the proposed solution were illustrated and two newly acquired datasets were publicly released. extensive experiments were conducted on various backbones and the best results were achieved using a simple network design, enabling real time inference of the sensing area. we believe that our problem reformulation and dataset release, together with the initial experimental results, will establish a new benchmark for the surgical vision community."
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,1.0,Introduction,"flexible ureteroscopy (furs) is a routinely performed surgical procedure for renal lithotripsy. this procedure inserts a flexible ureteroscope through the blad-der and ureters to get inside the kidneys for diagnosis and treatment of stones and tumors. unfortunately, such an examination and treatment depends on skills and experiences of surgeons. on the other hand, surgeons may miss stones and tumors and unsuccessfully orientate the ureteroscope inside the kidneys due to limited field of views, just 2d images without depth information, and the complex anatomical structure of the kidneys. to this end, ureteroscope tracking and navigation is increasingly developed as a promising tool to solve these issues. many researchers have developed various methods to boost endoscopic navigation. these methods generally consist of vision-and sensor-based tracking. han et al.  although these methods mentioned above work well, ureteroscopic navigation is still a challenging problem. compared to other endoscopes such as colonoscope and bronchoscope, the diameter of the ureteroscope is smaller, resulting in more limited lighting source and field of view. particularly, ureteroscopy involves much solids (impurities) and fluids (liquids), making ureteroscopic video images low-quality, as well as these solids and fluids inside the kidneys cannot be regularly observed in computed tomography (ct) images. on the other hand, the complex internal structures such as calyx, papilla, and pyramids of the kidneys are difficult to be observed in ct images. these issues introduce a difficulty in directly aligning ureteroscopic video sequences to ct images, leading to a challenge of image-based continuous ureteroscopic navigation. this work aims to explore an accurate and robust vision-based navigation method for furs procedures without using any external positional sensors. based on ureteroscopic video images and preoperative computed tomography urogram (ctu) images, we propose a novel video-ctu registration method to precisely locate the flexible ureteroscope in the ctu space. several highlights of this work are clarified as follows. to the best of our knowledge, this work shows the first study to continuously track the flexible ureteroscope in preoperative data using a vision-based method. technically, we propose a novel 2d-3d (video-ctu) registration method that introduces a structural point similarity measure without using image pixel intensity information to characterize the difference between the structural regions in real video images and ctu-driven virtual image depth maps. additionally, our proposed method can successfully deal with solid and fluid ureteroscopic video images and attains higher navigation accuracy than intensity-based 2d-3d registration methods."
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,1.0,Introduction,"colorectal cancer (crc) is the third most commonly diagnosed cancer but ranks second in terms of mortality worldwide  many researchers employ u-shaped network  unfortunately, limited field of view and illumination variations usually result in insufficient boundary contrast between intestinal lesions and their surrounding tissues. on the other hand, various polyps and adenomas with different pathological features have similar visual characteristics to intestinal folds. to address these issues mentioned above, we explore a new deep learning architecture called cascade transformer encoded boundary-aware multibranch fusion (ctbmf) networks with cascade transformers and multibranch fusion for polyp and adenoma segmentation in colonoscopic white-light and narrow-band video images. several technical highlights of this work are summarized as follows. first, we construct cascade transformers that can extract global semantic and subtle boundary features at different resolutions and establish weighted links between global semantic cues and local spatial ones for intermediate reasoning, providing long-range dependencies and a global receptive field for pixel-level segmentation. next, a hybrid spatial-frequency loss function is defined to compensate for loss features in the spatial domain but available in the frequency domain. additionally, we built a new colonoscopic lesion image database and will make it publicly available, while this work also conducts a thorough evaluation and comparison on our new database and four publicly available ones (fig. "
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,1.0,Introduction,"medical imaging is essential during diagnosis, surgical planning, surgical guidance, and follow-up for treating brain pathology. images from multiple modalities are typically acquired to distinguish clinical targets from surrounding tissues. for example, intra-operative ultrasound (ius) imaging and magnetic resonance imaging (mri) capture complementary characteristics of brain tissues that can be used to guide brain tumor resection. however, as noted in  medical image synthesis aims to predict missing images given available images. deep-learning based methods have reached the highest level of performance  to tackle this challenge, unified approaches have been proposed. these approaches are designed to have the flexibility to handle incomplete image sets as input, improving practicality as only one network is used for generating missing images. to handle partial inputs, some studies proposed to fill missing images with arbitrary values  have not yet been extended to multi-modal settings to synthesize missing images. in this work, we introduce multi-modal hierarchical latent representation vae (mhvae), the first multi-modal vae approach with a hierarchical latent representation for unified medical image synthesis. our contribution is four-fold. first, we integrate a hierarchical latent representation into the multi-modal variational setting to improve the expressiveness of the model. second, we propose a principled fusion operation derived from a probabilistic formulation to support missing modalities, thereby enabling image synthesis. third, adversarial learning is employed to generate realistic image synthesis. finally, experiments on the challenging problem of ius and mr synthesis demonstrate the effectiveness of the proposed approach, enabling the synthesis of high-quality images while establishing a mathematically grounded formulation for unified image synthesis and outperforming non-unified gan-based approaches and the state-of-the-art method for unified multi-modal medical image synthesis."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,3.0,Experiments,"dataset and preprocessing. our clinical dataset consists of 108 patients for whom were acquired both a pre-operative h&n ct scan and 4 to 11 wsis after laryngectomy (with a total amount of 849 wsis). the theoretical spacing between each slice is 5 mm, and the typical pixel size before downsampling is 100k × 100k. two expert radiation oncologists on ct delineated both the thyroid and cricoid cartilages for structure awareness and the gross tumor volume (gtv) for clinical validation, while two expert pathologists did the same on wsis. they then meet and agreed to place 6 landmarks for each slice at important locations (not used for training). we ended up with images of size 256 × 256 (×64 for 3d ct) of 1 mm isotropic grid space. we split the dataset patient-wise into three groups for training (64), validation  hyperparameters. we drew our code from cyclegan and voxelmorph implementations with modifications explained above, and we thank the authors of msv-regsynnet for making their code and data available to us  evaluation. we benchmarked our method against three baselines: first, to assess the benefit of modality translation over the multimodal loss, we re-used the original 3d voxelmorph model with mind as a multimodal metric for optimization. we also modified this approach by masking the loss function to account for the 2d-3d setting. next, we implemented the modality translation-based msv-regsyn-net and modified it for our application to measure the importance of joint structure-aware initialization and regularization. finally, to differentiate the latter contributions, we tested two ablation studies: without the cascaded rigid mapping or without the distance field control. according to the mr/ct application in rt, we compared our model against the state-of-the-art results of msv-regsynnet which were computed on the same dataset."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,4.2,Registration,"we present visual results in fig.  our method outperforms all baselines, proving the necessity of a singular approach to handle the specific case of histology. the popular voxelmorph framework fails, and the 2d-3d adaptation demonstrates the value of the masked loss function. the superior performance of msv-regsynnet advocates for a modality translation-based method compared to a direct multimodal similarity criterion. in addition, the ablation studies prove the benefit of the distance field regularization and more importantly the cascaded initialization. concerning the gpu runtime, with a 3-step cascade for initialization, the inference remains in a similar time scale to baseline methods and performs mapping in less than 3s. we also compared against msv-regsynnet on its own validation dataset for generalization assessment: we yielded comparable results for the first cohort and significantly better ones for the second, which proves that structuregnet behaves well on other modalities and that the structure awareness is an essential asset for better registration, as pelvis is a location where organs are moving. visuals of registration results are displayed in the supplementary material. eventually, an important clinical endpoint of our study is to compare the gtv delineated on ct with gold-standard tumor extent after co-registration to highlight systematic errors and better understand the biological environment from the radiologic signals. we show in (f) that the gtv delineated on ct overestimates the true tumor extent of around 31%, but does not always encompass the tumor with a proportion of histological tumor contained within the ct contour of 0.86. the typical error cases are the inclusion of cartilage or edema, which highlights the limitations and variability of radiology-based examinations, leading to increased toxicity or untreated areas in rt."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,5.0,Discussion and Conclusion,"we introduced a novel framework for 2d/3d multimodal registration. struc-turegnet leverages the structure of tissues to guide the registration through both initial plane selection and deformable regularization; it combines adversarial training for modality translation with a 2d-3d mapping setting and does not require any protocol for 3d reconstruction. it is worth noticing that even if the annotation of cartilage was manual, automating this process is not a bottleneck as the difference in contrast between soft tissue and stiff areas is clear enough to leverage any image processing tool for this task. finally, it is entirely versatile as we designed our experiments for ct-wsi but any 3d radiological images are suitable. we achieve superior results than state-of-the-art methods in dl-based registration in a similar time scale, allowing precise mapping of both modalities and a better understanding of the tumor microenvironment. the main limitation lies in the handling of organs without any rigid areas like the prostate. future work also includes a study with biomarkers from immunohistochemistry mapped onto radiology to go beyond binary tumor masks and move toward virtual biopsy."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,1.0,Introduction,"tomographic imaging estimates body density using hundreds of x-ray projections, but it's slow and harmful to patients. acquisition time may be too high for certain applications, and each projection adds dose to the patient. a quick, low-cost 3d estimation of internal structures using only bi-planar x-rays can revolutionize radiology, benefiting dental imaging, orthopedics, neurology, and more. this can improve image-guided therapies and preoperative planning, especially for radiotherapy, which requires precise patient positioning with minimal radiation exposure. however, this task is an ill-posed inverse problem: x-ray measurements are the result of attenuation integration across the body, which makes them very fig.  ambiguous. traditional reconstruction methods require hundreds of projections to get sufficient constraints on the internal structures. with very few projections, it is very difficult to disentangle the structures for even coarse 3d estimation. in other words, many 3d volumes may have generated such projections a priori. classical analytical and iterative methods  as illustrated in fig.  compared to other 3d gans, it is proven to provide the best disentanglement of the feature space related to semantic features  by contrast with feed-forward methods, our approach does not require paired projections-reconstructions, which are very tedious to acquire, and it can be used with different numbers of projections and different projection geometries without retraining. compared to nerf-based methods, our method exploits prior knowledge from many patients to require only two projections. we evaluate our method on reconstructing cancer patients' head-and-neck cts, which involves intricate and complicated structures. we perform several experiments to compare our method with a feed-forward-based method  we show that our method allows to retrieve results with the finest reconstructions and better matching structures, for a variety of number of projections. to summarize, our contributions are two-fold: (i) a new paradigm for 3d reconstruction with biplanar x-rays: instead of learning to invert the measurements, we leverage a 3d style-based generative model to learn deep image priors of anatomic structures and optimize over the latent space to match the input projections; (ii) a novel unsupervised method, fast and robust to sampling ratio, source energy, angles and geometry of projections, all of which making it general for downstream applications and imaging systems."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.1,Dataset and Preprocessing,"manifold learning. we trained our model with a large dataset of 3500 cts of patients with head-and-neck cancer, more exactly 2297 patients from the publicly available the cancer imaging archive (tcia)  3d reconstruction. to evaluate our approach, we used an external private cohort of 80 patients who had undergone radiotherapy for head-and-neck cancer, with their consent. planning ct scans were obtained for dose preparation, and cbct scans were obtained at each treatment fraction for positioning with full gantry acquisition. as can be seen in fig. "
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,1.0,Introduction,"magnetic resonance imaging (mri) and computed tomography (ct) are two commonly used cross-sectional medical imaging techniques. mri and ct produce different tissue contrast and are often used in tandem to provide complementary information. while mri is useful for visualizing soft tissues (e.g. muscle,   fat), ct is superior for visualizing bony structures. some medical procedures, such as radiotherapy for brain tumors, craniosynostosis, and spinal surgery, typically require both mri and ct for planning. unfortunately, ct imaging exposes patients to ionizing radiation, which can damage dna and increase cancer risk  most synthesis methods adopt supervised learning paradigms and train generative models to synthesize ct  recent unsupervised methods impose structural constraints on the synthesized ct through pixel-wise or shape-wise consistency. pixel-wise consistency methods "
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,1.0,Introduction,"x-ray computed tomography (ct) is an established diagnostic tool in clinical practice; however, there is growing concern regarding the increased risk of cancer induction associated with x-ray radiation exposure  in recent years, the success of deep learning has attracted much attention in the field of sparse-view ct reconstruction. existing learning-based approaches mainly include image-domain methods  motivation. we view the sparse-view ct image reconstruction as a two-step task: artifact removal and detail recovery. for the former, few work has investigated the fact that the artifacts exhibit similar pattern across different sparseview scenarios, which is evident in fourier domain as shown in fig.  while fourier domain band-pass maps help capture the pattern of the artifacts, restoring the image detail contaminated by strong artifacts may still be difficult due to the entanglement of artifacts and details in the residues. consequently, we propose a self-guided artifact refinement network (seednet) that provides supervision signals to aid freenet in refining the image details contaminated by the artifacts. with these novel designs, we introduce a simple yet effective model termed frequency-band-aware and self-guided network (freeseed), which enhances the reconstruction by modeling the pattern of artifacts from a frequency perspective and utilizing the artifact to restore the details. freeseed achieves promising results with only image data and can be further enhanced once the sinogram is available. our contributions can be summarized as follows: 1) a novel frequency-bandaware network is introduced to efficiently capture the pattern of global artifacts in the fourier domain among different sparse-view scenarios; 2) to promote the restoration of heavily corrupted image detail, we propose a self-guided artifact refinement network that ensures targeted refinement of the reconstructed image and consistently improves the model performance across different scenarios; and 3) quantitative and qualitative results demonstrate the superiority of freeseed over the state-of-the-art sparse-view ct reconstruction methods. "
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,,3D Pseudo Brain MRI.,"to evaluate the performance of atlas-based registration, it is essential to have the correct mapping of pathological regions to healthy brain regions. to create such a mapping, we created a pseudo dataset by utilizing images from the oasis-1 and brats2020. from the resulting t1 sequences, a pseudo dataset of 300 images was randomly selected for further analysis. appendix b provides a detailed process for creating the pseudo dataset. real data with landmarks. brats-reg 2022  atlas-based registration. after creating the pseudo dataset, we warped brain mr images without tumors to the atlas and used the resulting deformation field as the gold standard for evaluation. we then evaluated the mean deformation error (mde)  longitudinal registration. to perform the longitudinal registration task, we registered each pre-operative scan to the corresponding follow-up scan of the same patient and measured the mean target registration error (tre) of the paired landmarks using the resulting deformation field. for this purpose, we leveraged segnet, trained on brats2020, to segment the tumor of brat-sreg2022 and separated the landmarks into two regions: near tumor and far from tumor. figure  to quantitatively evaluate the segmentation capability of our proposed framework, we compared its performance with other unsupervised segmentation techniques methods, including unsupervised clustering toolbox aucseg  ablation study. we compared the performance of the inpnet trained with histogram matching (hm) and the segnet trained with ground truth masks (supervised). the results, shown in table "
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,3.1,Materials,"the data used in our experiments are collected from the cancer image archive (tcia)  each sample contains co-registered (acquired with pet-ct scans) ct, pet, and nac-pet whole-body scans. in our experiments, we re-sampled all of them to a voxel spacing of 2×2×2 and re-scaled the intensities of nac-pet/ac-pet images to a range of [0, 1], of ct images by multiplying 0.001. the input and output of our aseg framework are cropped patches with the size of 192 × 192 × 128 voxels. to achieve full-fov output, the consecutive outputs of each sample are composed into a single volume where the overlapped regions are averaged."
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,3.1,Dataset and Evaluation Metrics,"we use brain mri images of 1,251 subjects from brain tumor segmentation 2021 (brats2021) "
Geometric Ultrasound Localization Microscopy,1.0,Introduction,"ultrasound localization microscopy (ulm) has revolutionized medical imaging by enabling sub-wavelength resolution from images acquired by piezo-electric transducers and computational beamforming. however, the necessity of beamforming for ulm remains questionable. our work challenges the conventional assumption that beamforming is the ideal processing step for ulm and presents an alternative approach based on geometric reconstruction from time-of-arrival (toa) information. the discovery of ulm has recently surpassed the diffraction-limited spatial resolution and enabled highly detailed visualization of the vascularity  while contrast-enhanced ultra-sound (ceus) is used in the identification of musculoskeletal soft tissue tumours  for ulm to investigate its potential to refine mb localization  to this end, we propose an alternative approach for ulm, outlined in fig. "
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,1.0,Introduction,"near-infrared (nir) fluorescence imaging can allow the detection of fluorophores up to 4 cm depth in tissue  a majority of fluorescence imaging applications including fluorescence guided surgery (fgs) rely upon visible 2d surface imaging  the prime motivation of our work is to enable an efficient 3d tumor shape reconstruction for fgs in an operating room environment, where we do not have full control of the ambient light and we cannot rely on sophisticated time or frequency domain imaging instrumentation and setup. in these situations, one has to use clinical cameras producing rapid continuous wave (cw) fluorescence boundary measurements  we propose an incremental fluorescent target reconstruction (iftr) scheme, based on the recent advances in quadratic and conic convex optimization and sparse regularization, which can recover a relatively large 3d target in tissuelike media. in our experiments, iftr scheme demonstrates accurate reconstruction of 3d targets from reflectance mode cw-measurements collected at the top surface of the domain. to our best knowledge, this is the first report where the 3d shape of tumor-like target has been recovered from reflectance mode steady-state cw measurements. previously such results were reported in fdot literature only for time-consuming frequency-domain or time-domain measurements "
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.1,Affine Registration of Brain US-MR,"in this experiment, we evaluate the performance of different methods for estimating affine registration of the retrospective evaluation of cerebral tumors (resect) miccai challenge dataset "
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.3,Deformable Registration of Abdominal US-CT and US-MR,"as the most challenging experiment, we finally use our method to achieve deformable registration of abdominal 3d freehand us to a ct or mr volume. we are using a heterogeneous dataset of 27 cases, comprising liver cancer patients and healthy volunteers, different ultrasound machines, as well as optical vs. electro-magnetic external tracking, and sub-costal vs. inter-costal scanning of the liver. all 3d ultrasound data sets are accurately calibrated, with overall system errors in the range of commercial ultrasound fusion options. between 4 and 9 landmark pairs (vessel bifurcations, liver gland borders, gall bladder, kidney) were manually annotated by an expert. in order to measure the capture range, we start the registration from 50 random rigid poses around the ground truth and calculate the fiducial registration error (fre) after optimization. for local optimization, lc 2 is used in conjunction with bobyqa  from the results shown in table  note that this registration problem is much more challenging than the prior two due to difficult ultrasonic visibility in the abdomen, strong deformations, and ambiguous matches of liver vasculature. therefore, to the best of our knowledge, these results present a significant leap towards reliable and fully automatic fusion, doing away with cumbersome manual landmark placements."
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,4.0,Experiment,"datasets. first, our proposed approaches are evaluated on the ""mayo-clinic low-dose ct grand challenge"" (mayo-clinic) dataset of lung ct images  the dataset contains 2250 two dimensional slices from 9 patients for training, and the remaining 128 slices from 1 patient are reserved for testing. the lowdose measurements are simulated by parallel-beam x-ray with 200 (or 150) uniform views, i.e., n v = 200 (or n v = 150), and 400 (or 300) detectors, i.e., n d = 400 (or n d = 300). in order to further verify the denoising ability of our approaches, we add the gaussian noise with standard deviation σ = 2.0 to the sinograms after x-ray projection in 50% of the experiments. to evaluate the generalization of our model, we also consider another dataset rider with nonsmall cell lung cancer under two ct scans  baselines and evaluation metrics. we consider several existing popular algorithms for comparison. (  we set λ pix = 1.0, λ adv = 0.01 and λ per = 1.0 for the optimization objective in eq. (  results. a similar increasing trend with our approach across different settings but has worse reconstruction quality. to evaluate the stability and generalization of our model and the baselines trained on mayo-clinic dataset, we also test them on the rider dataset. the results are shown in table  to illustrate the reconstruction performances more clearly, we also show the reconstruction results for testing images in fig. "
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,1.0,Introduction,"radiotherapy (rt) is one of the cornerstones of cancer patients. it utilizes ionizing radiation to eradicate all cells of a tumor. the total radiation dose is typically divided over 3-30 daily fractions to optimize its effect. as the surrounding normal tissue is also sensitive to radiation, highly accurate delivery is vital. image guided rt (igrt) is a technique to capture the anatomy of the day using in room imaging in order to align the treatment beam with the tumor location  a major challenge especially for cbct imaging of the thorax and upperabdomen is the respiratory motion that introduces blurring of the anatomy, reducing the localization accuracy and the sharpness of the image. a technique used to alleviate motion artifacts is respiratory correlated cbct (4dcbct)  several traditional methods based on iterative reconstruction algorithms and motion compensation techniques are used to reduce view-aliasing in 4dcbcts  deep learning has been proposed as a way to address view-aliasing with accelerated reconstruction  a different method, called noise2inverse, uses an unsupervised approach to reduce measurement noise in the traditional ct setting  we propose noise2aliasing to address these limitations. the method can be used to provably train models to reduce both view-aliasing artifacts and stochastic noise from 4dcbcts in an unsupervised way. training deep learning models for medical applications often needs new data. this was not the case for noise2aliasing, and historical clinical data sufficed for training. we validated our method on publicly available data "
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,4.0,Experiments,"first, we used the spare varian dataset to study whether noise2aliasing can match the performance of the supervised baseline and if it can outperform it when adding noise to the projections. then, we use the internal dataset to explore the requirements for the method to be applied to an existing clinical dataset. these required around 64 gpu days on nvidia a100 gpus. training of the model is done on 2d slices. the projections obtained during a scan are sub-sampled according to the pseudo-average subset selection method described in  the datasets used in this study are two: 1. the spare varian dataset was used to provide performance results on publicly available patient data. to more closely resemble normal respiratory motion per projection image, the 8 min scan has been used from each patient (five such scans are available in the dataset). training is performed over 4 patients while 1 patient is used as a test set. the hyperparameters are optimized over the training dataset. 2. an internal dataset (irb approved) of 30 lung cancer patients' 4dcbcts from 2020 to 2022, originally used for igrt, with 25 patients for training and 5 patients for testing. the scans are 4 min 205 • scans with 120kev source and 512 × 512 sized detector, using elekta linacs. the data were anonymized prior to analysis. projection noise was added using the poisson distribution to the spare varian dataset to evaluate the ability of the unsupervised method to reduce it. given a projected value of p and a photon count π (chosen to be 2500), the rate of the poisson distribution is defined as πe -p and given a sample q from this distribution, then the new projected value is p =log q π . the architecture used in this work is the mixed scale dense cnn (msd)  the baselines we compare against are two. the first is the traditional fdk obtained using rtk  the metrics used in this work are the root mean squared error (rmse), peak signal-to-noise ratio (psnr), and structural similarity index measure (ssim) "
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,1.0,Introduction,"liver cancer is the most prevalent indication for liver surgery, and although there have been notable advancements in oncologic therapies, surgery remains as the only curative approach overall  liver laparoscopic resection has demonstrated fewer complications compared to open surgery  performing ious during laparoscopic liver surgery poses significant challenges, as laparoscopy has poor ergonomics and narrow fields of view, and on the other hand, ious demands skills to manipulate the probe and analyze images. at the end, and despite its real-time capabilities, ious images are intermittent and asynchronous to the surgery, requiring multiple iterations and repetitive steps (probe-in -→ instruments-out -→ probe-out -→ instruments-in). therefore, any method enabling a continuous and synchronous us assessment throughout the surgery, with minimal iterations required would significantly improve the surgical workflow, as well as its efficiency and safety. to overcome these limitations, the use of intravascular ultrasound (ivus) images has been proposed, enabling continuous and synchronous inside-out imaging during liver surgery  several approaches have been proposed to address this limitation by proposing a trackerless ultrasound volume reconstruction. physics-based methods have exploited speckle correlation models between different adjacent frames  our method improves upon previous solutions in terms of robustness and accuracy, particularly in the presence of rotational motion. such motion is predominant in the context highlighted above and is the source of additional nonlinearity in the pose estimation problem. to the best of our knowledge, this is the first work that provides a clinically sound and efficient 3d us volume reconstruction during minimally invasive procedures. the paper is organized as follows: sect. 2 details the method and its novelty, sect. 3 presents our current results on ex vivo porcine data, and finally, we conclude in sect. 4 and discuss future work."
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,1.0,Introduction,"magnetic resonance imaging (mri) is critical to the diagnosis, treatment, and follow-up of brain tumour patients  management  despite the success, gan-based models are challenged by the limited capability of adversarial learning in modelling complex multi-modal data distributions  diffusion model (dm) has achieved state-of-the-art performance in synthesizing natural images, promising to improve mri synthesis models. it shows superiority in model training  however, current dm-based methods focus on one-to-one mri translation, promising to be improved by many-to-one methods, which requires dedicated design to balance the multiple conditions introduced by multi-modal mri. moreover, as most dms operate in original image domain, all markov states are kept in memory  we propose a dm-based multi-modal mri synthesis model, cola-diff, which facilitates many-to-one mri translation in latent space, and preserve anatomical structure with accelerated sampling. our main contributions include: -present a denoising diffusion probabilistic model based on multi-modal mri. as far as we know, this is the first dm-based many-to-one mri synthesis model. -design a bespoke architecture, e.g., similar cooperative filtering, to better facilitate diffusion operations in the latent space, reducing the risks of excessive information compression and high-dimensional noise. -introduce structural guidance of brain regions in each step of the diffusion process, preserving anatomical structure and enhancing synthesis quality. -propose an auto-weight adaptation to balance multi-conditions and maximise the chance of leveraging relevant multi-modal information."
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,1.0,Introduction,"colorectal cancer (crc) is the third most commonly diagnosed cancer and is the second most common cause of cancer death  to overcome these difficulties, we rely on two properties of endoscopic images: -endoluminal cavities such as the gastrointestinal tract, and in particular the human colon, are watertight surfaces. to account for this, we represent its surface in terms of a signed distance function (sdf), which by its very nature presents continuous watertight surfaces. -in endoscopy the light source is co-located with the camera. it illuminates a dark scene and is always close to the surface. as a result, the irradiance decreases rapidly with distance t from camera to surface; more specifically it is a function of 1/t 2 . in other words, there is a strong correlation between light and depth, which remains unexploited to date. to take advantage of these specificities, we build on the success of neural implicit surfaces (neus)  neus training selects a pixel from an image and samples points along its projecting ray. however, the network is agnostic to the sampling distance. in lightneus, we explicitly feed to the renderer the distance of each one of these sampled points to the light source, as shown in fig.  our results show that exploiting the illumination is key to unlocking implicit neural surface reconstruction in endoscopy. it delivers accuracies in the range of 3 mm, whereas an unmodified neus is either 5 times less accurate or even fails to reconstruct any surface at all. earlier methods "
