<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prompt-Based Grouping Transformer for Nucleus Detection and Classification</title>
				<funder ref="#_dzVEUEa #_2Thhu55">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_qZ62fwK #_4Y4ERPC">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Guangdong Provincial</orgName>
				</funder>
				<funder ref="#_Xk5EpAj">
					<orgName type="full">Chinese Key-Area Research and Development Program of Guangdong Province</orgName>
				</funder>
				<funder ref="#_yVqVMUN #_Vyvvu9P">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junjia</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Research Institute of Sun Yat-sen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haofeng</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weijun</forename><surname>Sun</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<email>liguanbin@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Research Institute of Sun Yat-sen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">Key Laboratory of Big Data Computing</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Prompt-Based Grouping Transformer for Nucleus Detection and Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="569" to="579"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">8FAD3F079763DAAFA056351E143F3E51</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_55</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Nuclei classification</term>
					<term>Prompt tuning</term>
					<term>Clustering</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic nuclei detection and classification can produce effective information for disease diagnosis. Most existing methods classify nuclei independently or do not make full use of the semantic similarity between nuclei and their grouping features. In this paper, we propose a novel end-to-end nuclei detection and classification framework based on a grouping transformer-based classifier. The nuclei classifier learns and updates the representations of nuclei groups and categories via hierarchically grouping the nucleus embeddings. Then the cell types are predicted with the pairwise correlations between categorical embeddings and nucleus features. For the efficiency of the fully transformer-based framework, we take the nucleus group embeddings as the input prompts of backbone, which helps harvest grouping guided features by tuning only the prompts instead of the whole backbone. Experimental results show that the proposed method significantly outperforms the existing models on three datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nucleus classification is to identify the cell types from digital pathology image, assisting pathologists in cancer diagnosis and prognosis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">30]</ref>. For example, the involvement of tumor-infiltrating lymphocytes (TILs) is a critical prognostic variable for the evaluation of breast/lung cancer <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">29]</ref>. It is a challenge to infer the nucleus types due to the diversity and unbalanced distribution of nuclei. Thus, we aim to automatically classify cell nuclei in pathological images.</p><p>A number of methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b35">34]</ref> have been proposed for automatic nuclei segmentation and classification. Most of them use a U-shape model <ref type="bibr" target="#b29">[28]</ref> for training to produce dense predictions with expensive pixel-level labels. In this paper, we aim to obtain the location and category of cells, which only needs affordable labels of centroids or bounding boxes. The task can be solved by generic object detector <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27]</ref>, but they are usually built for everyday objects whose positions and combinations are quite random. Differently, in pathological images, experts often identify nuclear communities via their relationships and spatial distribution. Some recent methods resort to the spatial contexts among nuclei. Abousamra et al. <ref type="bibr" target="#b0">[1]</ref> adopt a spatial statistical function to model the local density of cells. Hassan et al. <ref type="bibr" target="#b10">[11]</ref> build a location-based graph for nuclei classification. However, the semantics similarity and dissimilarity between nucleus instances as well as the category representations have not been fully exploited.</p><p>Based on these observations, we develop a learnable Grouping Transformer based Classifier (GTC) that leverages the similarity between nuclei and their cluster representations to infer their types. Specifically, we define a number of nucleus clusters with learnable initial embeddings, and assign nucleus instances to their most correlated clusters by computing the correlations between clusters and nuclei. Next, the cluster embeddings are updated with their affiliated instances, and are further grouped into the categorical representations. Then, the cell types can be well estimated using the correlations between the nuclei and the categorical embeddings. We propose a novel fully transformer-based framework for nuclei detection and classification, by integrating a backbone, a centroid detector, and the grouping-based classifier. However, the transformer framework has a relatively large number of parameters, which could cause high costs in fine-tuning the whole model on large datasets. On the other hand, there exist domain gaps in the pathological images of different organs, staining, and institutions, which makes it necessary to fine-tune models to new applications. Thus, it is of great significance to tune our proposed transformer framework efficiently.</p><p>Inspired by the prompt tuning methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b20">20]</ref> which train continuous prompts with frozen pretrained models for natural language processing tasks, we propose a grouping prompt based learning strategy for efficient tuning. We prepend the embeddings of nucleus clusters to the input space and freeze the entire pre-trained transformer backbone so that these group embeddings act as prompt information to help the backbone extract grouping-aware features. Our contributions are: (1) a prompt-based grouping transformer framework for end-to-end detection and classification of nuclei; (2) a novel grouping prompt learning mechanism that exploits nucleus clusters to guide feature learning with low tuning costs; (3) Experimental results show that our method achieves the state-of-the-art on three public benchmarks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, We propose a novel framework, Prompt-based Grouping Transformer (PGT), which directly outputs the coordinates of nuclei centroids and leverages grouping prompts for cell-type prediction. In the architecture, the detection and classification parts are interdependent and can be trained together. The proposed framework consists of a transformer-based nucleus detector, a grouping transformer-based classifier, and a grouping prompt learning strategy, which are presented in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformer-Based Centroid Detector</head><p>Backbone. We adopt Swin Transformer <ref type="bibr" target="#b21">[21]</ref> as the backbone to learn deep features. The pixel-level feature maps output from Stage 2 to Stage 4 of the backbone are extracted. Then the Stage-4 feature map is downsampled with a 3 × 3 convolution of stride 2 to yield another lower-resolution feature map. We obtain four feature maps in total. The channel number of each feature map is aligned via a 1 × 1 convolution layer and a group normalization operator.</p><p>Encoder and Decoder. The encoder and decoder have 3 deformable attention layers <ref type="bibr" target="#b36">[35]</ref>, respectively. The multi-scale feature maps output by the backbone are fed into the encoder in which the pixel-level feature vectors in all these feature maps are updated via deformable self-attention. After the attention layers, we send each feature vector into 2 fully connected (FC) layers separately to obtain the fine-grained categorical scores of each pixel. Only the Q feature vectors with the highest confidence are preserved as object embeddings and their position coordinates are recorded as reference points. Each decoder layer utilizes crossattention to enhance the object embeddings by taking them as queries/values and the updated feature maps as keys. The enhanced query embeddings are fed into 2 FC layers to regress position offsets which are added to and refine the reference points. The reference points output by the last decoder layer are the finally detected nucleus centroids. The last query embeddings from the decoder are sent to the proposed classifier for cell type prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Grouping Transformer Based Classifier</head><p>In Fig. <ref type="figure" target="#fig_1">2</ref>, we develop a Grouping Transformer based Classifier (GTC) that takes grouping prompts g ∈ R G×D and query embeddings q ∈ R Q×D as inputs, and yields categorical scores for each nucleus query. To divide the queries into primary groups, The similarity matrix S ∈ R G×Q between the query embeddings and the grouping prompts is built via inner product and Gumbel-Softmax <ref type="bibr" target="#b11">[12]</ref> operation as Eq. ( <ref type="formula" target="#formula_0">1</ref>):</p><formula xml:id="formula_0">S = softmax(W 1 q g • (W 1 k q) T + γ/τ ),<label>(1)</label></formula><p>where W 1 q and W 1 k are the weights of learnable linear projections, γ ∈ R G×Q are i.i.d random samples drawn from the distribution Gumbel(0, 1) and τ denotes the Softmax temperature. Then we utilize the hard assignment strategy <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b33">32]</ref> and assign the query embedding to different groups as Eq. ( <ref type="formula" target="#formula_1">2</ref>):</p><formula xml:id="formula_1">Ŝ = one-hot(argmax(S)) + S -sg(S),<label>(2)</label></formula><p>where argmax(S) returns a 1 × Q vector, and one-hot(•) converts the vector to a binary G × Q matrix. sg is the stop gradient operator for better training of the one-hot function <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b33">32]</ref>. Then we merge the embeddings belonging to the same group into a primary group via Eq. (3):</p><formula xml:id="formula_2">g p = g + W 1 o Ŝ • W 1 v q G i=1 Ŝi<label>(3)</label></formula><p>where g p denotes the embeddings of primary groups, W 1 v and W 1 o are learnable linear weights. To separate the primary groups into the cell categories, we measure the similar matrix between the primary groups g p and learnable class embeddings c e ∈ R C×D to yield advanced class embeddings c a ∈ R C×D , in the same way as Eq.( <ref type="formula" target="#formula_0">1</ref>)-(3). To classify each centroid query, we measure the similarity between each query embedding and the advanced class embeddings. The category whose advanced embedding is most similar to a query, is assigned to the centroid query. The classification results c ∈ R C×Q are computed as:</p><formula xml:id="formula_3">c = c a • q T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Loss Function</head><p>The proposed method outputs a set of centroid proposals {(x q , y q )|q ∈ {1, • • • , Q}} with a decoder layer, and their corresponding cell-type scores {c q |q ∈ {1, • • • , Q}} with our proposed classifier. To compute the loss with detected centroids, we use the Hungarian algorithm <ref type="bibr" target="#b15">[15]</ref> to assign K target centroids (ground truth) to proposal centroids and get P positive (matched) samples and Q -P negative (unmatched) samples. The overall loss is defined as Eq. ( <ref type="formula" target="#formula_4">4</ref>):</p><formula xml:id="formula_4">L(y, ŷ) = 1 P P i=1 ω 1 ||(x i , y i ) -(x i , ŷi )|| 2 2 + ω 2 FL(c i , ĉi ) + ω 3 Q j=P +1 FL(c j , ĉj ),<label>(4)</label></formula><p>where ω 1 , ω 2 , ω 3 are weight terms, (x i , y i ) is the i th matched centroid coordinates, (x i , ŷi ) is the target coordinates. c i and c j denote the categorical scores of matched and unmatched samples, respectively. As the target of unmatched samples, ĉj is set to an empty category. FL(•) is the Focal Loss <ref type="bibr" target="#b18">[18]</ref> for training the proposed classifier. We adopt the deep supervision strategy <ref type="bibr" target="#b36">[35]</ref>. In the training, each decoder layer produces the side outputs of centroids and query embeddings that are fed into a GTC for classifying nuclei. For the 3 decoder layers, they yield 3 sets of detection and classification results for the loss in Eq. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Grouping Prompts Based Tuning</head><p>To avoid the inefficient fine-tuning of the backbone, we propose a new and simple learning strategy based on grouping prompts, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We inject a set of prompt embeddings as extra input of the Swin-Transformer <ref type="bibr" target="#b21">[21]</ref>, and only tune the prompts instead of the backbone. To learn group-aware representations, we further propose to share the embeddings of prompts with those of initial groups in the proposed GTC. Such prompt embeddings are define as Grouping Prompts.</p><p>For a typical Swin-Transformer backbone, an input pathological image I ∈ R H×W ×3 is divided into HW E 2 image patches of size E × E. We first embed each image patch into a D-dimensional latent space via a linear projection. Then we randomly initialize the grouping prompts g ∈ R G×D as learnable parameters, and concatenate them with the patch embeddings as input. Note that in the backbone, input patch embeddings are separated into different local windows and the grouping prompts are also inserted into each window, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Our proposed grouping prompt based learning consists of two phases, pre-tuning and prompt-tuning. In the pre-tuning phase, we adopt the Swin-b backbone pre-trained on ImageNet, replace the GTC head in our model (Fig. <ref type="figure" target="#fig_0">1</ref>) with 2 FC layers, and train the overall framework without prompts and GTC. In the prompt-tuning phase, grouping prompts are added to the input of the backbone and GTC, while the backbone parameters are frozen.</p><p>3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Implementation Details</head><p>CoNSeP 1 [10] is a colorectal nuclear dataset with three types, consisting of 41 H&amp;E stained image tiles from 16 colorectal adenocarcinoma whole-slide images (WSIs). The WSIs are at 20× magnification and the size of the slides is 500 × 500. We split them following the official partition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>.</p><formula xml:id="formula_5">BRCA-M2C 2 [1]</formula><p>is a breast cancer dataset with three types and consists of 120 image tiles from 113 patients. The WSIs are at 20× magnification and the size of the slides ranges from 465 × 465 to 504 × 504. We follow the work <ref type="bibr" target="#b0">[1]</ref> to apply the SLIC <ref type="bibr" target="#b1">[2]</ref> algorithm to generate superpixels as instances and split them into 80/10/30 slides for training/validation/testing.</p><p>Lizard 3 <ref type="bibr" target="#b8">[9]</ref> has 291 histology images of colon tissue from six datasets, containing nearly half a million labeled nuclei in H&amp;E stained colon tissue. The WSIs are at 20× magnification with an average size of 1,016 × 917 pixels. Our implementation and the setting of hyper-parameters are based on MMDetection <ref type="bibr" target="#b4">[5]</ref>. The number of grouping prompts G is 64. Random crop, flipping, and scaling are used for data augmentation. Our method is trained with PyTorch on a 48 GB GPU (NVIDIA A100) for 12-24 h (depending on the dataset size). More details are listed in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with the State-of-the-Art</head><p>The proposed method is compared with the state-of-the-art models: the existing methods for detecting and classifying cells in pathological images, i.e., Hover-Net <ref type="bibr" target="#b9">[10]</ref>, MCSpatNet <ref type="bibr" target="#b0">[1]</ref>, SONNET <ref type="bibr" target="#b6">[7]</ref>, and the sate-of-the-art methods for object detection in natural images, i.e., DDOD <ref type="bibr" target="#b5">[6]</ref>, TOOD <ref type="bibr" target="#b7">[8]</ref>, DAB-DETR <ref type="bibr" target="#b19">[19]</ref> and Uper-Net with ConvNeXt backbone <ref type="bibr" target="#b22">[22]</ref>. As shown in Table <ref type="table" target="#tab_0">1</ref>, our method exceeds all 1 https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/. 2 https://github.com/TopoXLab/Dataset-BRCA-M2C/. the other methods on three benchmarks with both detection and classification metrics. Specifically, on the CoNSeP dataset, our approach achieves 1.6% higher F-score on the detection (F d ) and 1.8% higher F-score on the classification (F c ) than the second best methods MCSpatNet <ref type="bibr" target="#b0">[1]</ref> and UperNet <ref type="bibr" target="#b22">[22]</ref>. On BRCA-M2C dataset, our method has 0.5% higher F d and 3.9% higher F c , compared with the second best models MCSpatNet <ref type="bibr" target="#b0">[1]</ref> and DAB-DETR <ref type="bibr" target="#b19">[19]</ref>. Besides, on Lizard dataset, our method outperforms UperNet <ref type="bibr" target="#b22">[22]</ref> by more than 1.5% and 6.4% on F d and F c , respectively. Meanwhile, we conduct t-test on CoNSeP dataset for statistical significance test. The details are listed in the supplementary material. The visual comparisons are shown in Fig. <ref type="figure" target="#fig_3">4</ref>. With the context information from surrounding nuclei, our method effectively reduces the misclassification rate of the lymphocytes and neutrophil categories (Blue and Red).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Analysis</head><p>The strengths of the grouping transformer based classifier and the grouping prompts are verified on CoNSeP dataset, as shown in Table <ref type="table" target="#tab_1">2</ref>.</p><p>Prompt-based Grouping Transformer (PGT) is our proposed detection and classification architecture with grouping prompts and the GTC (in Fig. <ref type="figure" target="#fig_0">1</ref>), while the  'Baseline' has no these two settings. PT means using naive prompt tuning. GTC means classifying nuclei with the grouping transformer. Our method achieves comparable results to the fully fine-tuning PGT with tuning only 15% parameters. Compared to the Baseline, our method yields 2.4% higher F d and 2.3% higher F c , respectively, which shows the effective combination of the grouping classifier and prompts. 'detached GTC &amp; PT' means that group features and prompts are independent. Our method surpasses the detached setting by 2.4% in F d and 3.1% in F c , which suggests that sharing embeddings of groups and prompts is effective. With a frozen backbone, the performances of 'w/o PT' and 'w/o GTC' are both dropping, which verifies the strength of the prompt tuning and the GTC module, respectively. Table <ref type="table" target="#tab_2">3</ref> shows the effect of different numbers of grouping prompts on CoNSeP dataset. When the number of groups is small, the classification result is inferior. When the group number is large than 64, the groups may contain too few nuclei to capture their common patterns. It is suggested to set the group number to a moderate value such as 64. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a new prompt-based grouping transformer framework that is fully transformer-based, and can achieve end-to-end nuclei detection and classification. In our framework, a grouping-based classifier groups nucleus features into cluster and category embeddings whose correlations with nuclei are used for identifying cell types. We further propose a novel learning scheme, which shares group embeddings with prompt tokens and extracts features guided by nuclei groups with less tuning costs. The results not only suggest that our method can obtain competitive performance on nuclei classification, but also indicate that the proposed prompt learning strategy can enhance the tuning efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of Prompt-based Grouping Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The Grouping Transformer based Classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The inputs with grouping prompts of the Shift-Window transformer backbone.</figDesc><graphic coords="5,80,46,54,47,247,48,91,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The visualization results on CoNSeP dataset. (Color figure online)</figDesc><graphic coords="8,44,79,54,65,334,72,136,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>3 https://warwick.ac.uk/fac/cross_fac/tia/data/lizard/. Comparison with existing methods on CoNSeP, BRCA-M2C and Lizard. For each dataset, we report the F-score of each class (F k c ), the mean F-score over all classes (Fc) and the detection F-score (F d ). F Inf l.denote the F-score for the inflammatory, epithelial, stromal, neutrophils, lymphocytes, plasma, Eosinophil and connective tissue cells, respectively. For each row, the best result is in bold and the second best is underlined.</figDesc><table><row><cell>c</cell><cell>, F Epi. c</cell><cell>, F Stro. c</cell><cell>, F Neu. c</cell><cell>, F Lym. c</cell><cell>,</cell></row></table><note><p>c</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on CoNSeP. PGT is the overall detection-classification framework. PT denotes training the network with Prompt Tuning. GTC means using the Grouping Transformer-based Classifier. * means freezing the weights of the backbone.</figDesc><table><row><cell>Methods</cell><cell>F Inf l.</cell><cell>Fc</cell><cell>F d</cell><cell>Tuned Params (M)</cell></row><row><cell>PGT (Full)</cell><cell cols="4">0.631 0.641 0.572 0.615 0.735 102.2</cell></row><row><cell cols="5">w/o GTC &amp; PT (Baseline) 0.599 0.600 0.570 0.590 0.714 95.767</cell></row><row><cell>w/o PT*</cell><cell cols="4">0.602 0.604 0.558 0.588 0.713 15.321</cell></row><row><cell>w/o GTC*</cell><cell cols="4">0.615 0.604 0.564 0.594 0.724 8.895</cell></row><row><cell cols="5">w/ detached GTC &amp; PT* 0.577 0.623 0.545 0.582 0.714 15.429</cell></row><row><cell>PGT* (Ours)</cell><cell cols="4">0.623 0.639 0.577 0.613 0.738 15.379</cell></row></table><note><p>c F Epi. c F Stro. c</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The effects of the number of grouping prompts G on CoNSeP.</figDesc><table><row><cell cols="2">F-score↑ 8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>128</cell></row><row><cell>F d</cell><cell cols="5">0.727 0.724 0.726 0.738 0.723</cell></row><row><cell>Fc</cell><cell cols="5">0.600 0.599 0.604 0.613 0.583</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>F d denotes the mean of detection F-scores of all testing images. * means p-value ≤0.05. ** means p-value ≤0.01. The Statistical Tests. As shown in Table 4, We calculate F d of each testing image as sample data and conduct t-test to obtain p-values on the CoNSeP dataset. The p-values are computed between our method and the others.</figDesc><table><row><cell cols="2">F-score↑ Hovernet</cell><cell>DDOD</cell><cell>TOOD</cell><cell>MCSpatNet</cell><cell>SONNET</cell><cell>DAT-DETR</cell><cell>ConvNeXt</cell><cell>PGT*</cell></row><row><cell></cell><cell>[10]</cell><cell>[6]</cell><cell>[8]</cell><cell>[1]</cell><cell>[7]</cell><cell>[19]</cell><cell>-UperNet</cell><cell>(Ours)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[22]</cell><cell></cell></row><row><cell>Fd</cell><cell>0.615</cell><cell>0.545</cell><cell>0.625</cell><cell>0.706</cell><cell>0.582</cell><cell>0.615</cell><cell>0.698</cell><cell>0.728</cell></row><row><cell cols="2">p-value 0.001*</cell><cell cols="3">0.000** 0.000* 0.027*</cell><cell>0.000**</cell><cell>0.000*</cell><cell>0.012*</cell><cell>-</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported in part by the <rs type="funder">Chinese Key-Area Research and Development Program of Guangdong Province</rs> (<rs type="grantNumber">2020B0101350001</rs>), in part by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62102267</rs>, NO. <rs type="grantNumber">61976250</rs>), in part by the <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (<rs type="grantNumber">2023A1515011464</rs>, <rs type="grantNumber">2020B1515020048</rs>), in part by the <rs type="funder">Shenzhen Science and Technology Program</rs> (<rs type="grantNumber">JCYJ20220818103001002</rs>, <rs type="grantNumber">JCYJ20220530141211024</rs>), and the <rs type="funder">Guangdong Provincial</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Xk5EpAj">
					<idno type="grant-number">2020B0101350001</idno>
				</org>
				<org type="funding" xml:id="_qZ62fwK">
					<idno type="grant-number">62102267</idno>
				</org>
				<org type="funding" xml:id="_4Y4ERPC">
					<idno type="grant-number">61976250</idno>
				</org>
				<org type="funding" xml:id="_dzVEUEa">
					<idno type="grant-number">2023A1515011464</idno>
				</org>
				<org type="funding" xml:id="_2Thhu55">
					<idno type="grant-number">2020B1515020048</idno>
				</org>
				<org type="funding" xml:id="_yVqVMUN">
					<idno type="grant-number">JCYJ20220818103001002</idno>
				</org>
				<org type="funding" xml:id="_Vyvvu9P">
					<idno type="grant-number">JCYJ20220530141211024</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_55.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multi-class cell detection using spatial context representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abousamra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4005" to="4014" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Introduction to digital image analysis in whole-slide imaging: a white paper from the digital pathology association</title>
		<author>
			<persName><forename type="first">F</forename><surname>Aeffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Pathol. Inform</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The role of tumor-infiltrating lymphocytes in development, progression, and prognosis of non-small cell lung cancer</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bremnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Thorac. Oncol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="789" to="800" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: open MMLab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Disentangle your dense object detector</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ACM MM</publisher>
			<biblScope unit="page" from="4939" to="4948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SONNET: a self-guided ordinal regression neural network for segmentation and classification of nuclei in large-scale multi-tissue histology images</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JBHI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3218" to="3228" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">TOOD: task-aligned onestage object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3490" to="3499" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lizard: a large-scale dataset for colonic nuclear instance segmentation and classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="684" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">HoVer-Net: simultaneous segmentation and classification of nuclei in multi-tissue histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101563</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nucleus classification in histology images using message passing network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Werghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102480</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual prompt tuning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13693</biblScope>
			<biblScope unit="page" from="709" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19827-4_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19827-4_41" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DenseRes-Unet: segmentation of overlapped/clustered nuclei from multi organ histopathology images</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ijaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page">105267</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Res. Logist. Q</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="3045" to="3059" />
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computational Linguistics</publisher>
			<pubPlace>Punta Cana; Dominican Republic</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detection and classification of cervical exfoliated cells based on faster R-CNN</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Advanced Infocomm Technology (ICAIT)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="52" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DAB-DETR: dynamic anchor boxes are better queries for DETR</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">P-tuning: prompt tuning can be comparable to fine-tuning across scales and tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A convnet for the 2020s</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">NAS-SCAM: neural architecture search-based spatial and channel joint attention module for nuclei semantic segmentation and classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-8_26" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Which pixel to annotate: a label-efficient nuclei segmentation framework</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="947" to="958" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-stream cell segmentation with low-level cues for multimodality images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Competitions in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mitotic nuclei detection in breast histopathology images using YOLOv4</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sugathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Gireesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Computing Communication and Networking Technologies</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">NucDETR: end-to-end transformer for nucleus detection in histopathology images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Obeid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mahbub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Werghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CMMCA 2022</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Qin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Zaki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13574</biblScope>
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-17266-3_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-17266-3_5" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The evaluation of tumor-infiltrating lymphocytes (TILs) in breast cancer: recommendations by an International TILs Working Group</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Oncol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="259" to="271" />
			<date type="published" when="2014">2014. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E A</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Cree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1196" to="1206" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">GroupViT: semantic segmentation emerges from text supervision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18134" to="18144" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">RIC-Unet: an improved neural network based on Unet for nuclei segmentation in histology images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="21420" to="21428" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SSMD: semi-supervised medical image detection with adaptive consistency and heterogeneous perturbation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">102117</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deformable DETR: deformable transformers for end-to-end object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
