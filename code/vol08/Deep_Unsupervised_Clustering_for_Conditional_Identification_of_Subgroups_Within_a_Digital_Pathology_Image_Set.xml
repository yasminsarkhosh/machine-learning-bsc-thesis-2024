<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set</title>
				<funder ref="#_w34VQUC">
					<orgName type="full">Oak Ridge Institute for Science and Education through an interagency agreement between the U.S. Department of Energy</orgName>
				</funder>
				<funder>
					<orgName type="full">U.S. Food and Drug Administration</orgName>
				</funder>
				<funder>
					<orgName type="full">Hightech Agenda Bayern</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mariia</forename><surname>Sidulova</surname></persName>
							<idno type="ORCID">0000-0001-9381-9506</idno>
							<affiliation key="aff0">
								<orgName type="department">Center for Devices and Radiological Health</orgName>
								<orgName type="institution">U.S. Food and Drug Administration</orgName>
								<address>
									<settlement>Silver Spring</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">George Washington University</orgName>
								<address>
									<settlement>Washington</settlement>
									<region>D.C</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xudong</forename><surname>Sun</surname></persName>
							<idno type="ORCID">0000-0001-9234-4932</idno>
							<affiliation key="aff1">
								<orgName type="department">Institute of AI for Health</orgName>
								<orgName type="institution">Helmholtz Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Alexej</forename><surname>Gossmann</surname></persName>
							<email>alexej.gossmann@fda.hhs.gov</email>
							<idno type="ORCID">0000-0001-9068-3877</idno>
							<affiliation key="aff0">
								<orgName type="department">Center for Devices and Radiological Health</orgName>
								<orgName type="institution">U.S. Food and Drug Administration</orgName>
								<address>
									<settlement>Silver Spring</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="666" to="675"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">050000B663F447E7906F9AE2BF5A7E2E</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_64</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Domain Identification</term>
					<term>Deep Clustering</term>
					<term>Subgroup Identification</term>
					<term>Variational Autoencoder</term>
					<term>Generative Model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Consideration of subgroups or domains within medical image datasets is crucial for the development and evaluation of robust and generalizable machine learning systems. To tackle the domain identification problem, we examine deep unsupervised generative clustering approaches for representation learning and clustering. The Variational Deep Embedding (VaDE) model is trained to learn lower-dimensional representations of images based on a Mixture-of-Gaussians latent space prior distribution while optimizing cluster assignments. We propose the Conditionally Decoded Variational Deep Embedding (CDVaDE) model which incorporates additional variables of choice, such as the class labels, as conditioning factors to guide the clustering towards subgroup structures in the data which have not been known or recognized previously. We analyze the behavior of CDVaDE on multiple datasets and compare it to other deep clustering algorithms. Our experimental results demonstrate that the considered models are capable of separating digital pathology images into meaningful subgroups. We provide a general-purpose implementation of all considered deep clustering methods as part of the open source Python package DomId (https://github.com/DIDSR/DomId).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning (ML), specifically deep learning (DL), algorithms have shown exceptional performance on numerous medical image analysis tasks <ref type="bibr" target="#b1">[2]</ref>. Never-theless, comprehensive reviews highlight major issues of generalizability, robustness, and reproducibility in medical imaging AI/ML <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>. For a generalizability assessment, reporting only aggregate performance measures is not sufficient. Due to model complexity and limited training data, ML performance often varies across data subgroups or domains, such as different patient subpopulations or varied data acquisition scenarios. Aggregate performance measures (e.g., sensitivity, specificity, ROC AUC) can be dominated by the larger subgroups, masking the poor ML model performance on smaller but clinically important subgroups <ref type="bibr" target="#b10">[11]</ref>. Thus, achieving (through training) and demonstrating (as part of testing) satisfactory ML model performance across relevant subgroups is crucial before the real-world clinical deployment of a medical ML system <ref type="bibr" target="#b12">[13]</ref>.</p><p>However, a challenging situation arises when relevant subgroups are unrecognized. One solution to this issue is to apply a clustering algorithm to the data, with the goal of identifying the unannotated subgroups. The main objective of unsupervised clustering is to group data points into distinct classes of similar traits. However, due to the complexity and high dimensionality of the medical imaging data and the resulting difficulty in establishing a concrete notion of similarity, extracting low-dimensional characteristics becomes the key to establishing the best criteria for grouping. Unsupervised generative clustering aims to simultaneously address both domain identification and dimensionality reduction. Deep unsupervised clustering algorithms could map the medical imaging data back to their causal factors or underlying domains, such as image acquisition equipment, patient subpopulations, or other meaningful data subgroups. However, there is a practical need to be able to guide the deep clustering model towards the identification of grouping structures in a given dataset that have not been already annotated. To that end, we propose a mechanism that is intended to constrain the model towards identifying clusters in the data that are not associated with given variables of choice (already known class labels or subgroup structures). The resulting algorithmic cluster assignments could then be used to improve ML algorithm training, or for generalizability and robustness evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We provide a PyTorch-based implementation of all deep clustering algorithms described below (VaDE, CDVaDE, and DEC) in the open source Python package DomId that is publicly available under https://github.com/DIDSR/DomId.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Variational Deep Embedding (VaDE)</head><p>Variational Deep Embedding (VaDE) <ref type="bibr" target="#b5">[6]</ref> is an unsupervised generative clustering approach based on Variational Autoencoders <ref type="bibr" target="#b9">[10]</ref>. In our study, VaDE is deployed as a deep clustering model using Convolutional Neural Network (CNN) architectures for the encoder g(x; φ) and the decoder f (z; θ). The encoder learns to compress the high-dimensional input images x into lower-dimensional latent representations z. Using a Mixture-of-Gaussians (MOG) prior distribution for the latent representations z, we examine subgroups or domains within the dataset, revealed by the individual Gaussians within the learned latent space, and how z affects the generation of x. The model can be used to perform inference, where observed images x are mapped to corresponding latent variables z and their cluster/domain assignments c. We denote the latent space dimensionality by d (i.e., z ∈ R d ), and the number of clusters by D (i.e., c ∈ {1, 2, . . . , D}). The trained decoder CNN can also be used to generate synthetic images from the algorithmically identified subgroups.</p><p>VaDE is optimized using Stochastic Gradient Variational Bayes <ref type="bibr" target="#b9">[10]</ref> to maximize a statistical measure called the Evidence Lower Bound (ELBO). We denote the true data distribution by p(z, x, c) and the variational posterior distribution by q(z, c|x). The ELBO of VaDE can be written as</p><formula xml:id="formula_0">L ELBO (x) = E q(z,c|x) log p(z, x, c) q(z, c|x) = E q(z,c|x) [log p(x|z) + log p(z|c) + log p(c) -log q(z|x) -log q(c|x)],<label>(1)</label></formula><p>where p(x|z) is modeled by the decoder CNN, and q(z|x) is modeled by the encoder CNN g(x; φ) as</p><formula xml:id="formula_1">q(z|x) = N z; μ, diag σ2 , μ, log σ2 = g(x; φ).</formula><p>Finally, the cluster assignments can be determined via</p><formula xml:id="formula_2">q(c|x) ≈ p(c|z) = p(c)p(z|c) D c =1 p(c )p(z|c ) , (<label>2</label></formula><formula xml:id="formula_3">)</formula><formula xml:id="formula_4">p(c) = Cat(π), p(z|c) = N z; μ c , diag σ 2 c ,<label>(3)</label></formula><p>where the probability distributions p(c) and p(z|c) come from the MOG prior of the latent space, with the respective distributional parameters π, μ c , σ 2 c (for c ∈ {1, 2, . . . , D}) optimized by maximizing the ELBO of Eq. ( <ref type="formula" target="#formula_0">1</ref>). Note that Eq. ( <ref type="formula" target="#formula_2">2</ref>) follows from the observation that in order to maximize the ELBO in Eq. 1, the KL Divergence between q(c|x) and p(c|z) needs to be equal to 0. We refer to <ref type="bibr" target="#b5">[6]</ref> for details.</p><p>In all our experiments, we apply VaDE with CNN architectures for the encoder and decoder. The CNN encoder consists of convolution layers with 32, 64, 128 filters, respectively, followed by a fully-connected layer. Respectively, the CNN decoder consists of a fully-connected layer followed by transposed convolution layers with the number of input/output channels decreasing as 128, 64, 32, 3. Batch normalization and the leaky ReLU activation functions are used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conditionally Decoded Variational Deep Embedding (CDVaDE)</head><p>We propose the Conditionally Decoded Variational Deep Embedding (CDVaDE) model as an extension to VaDE as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The generative process of CDVaDE differs from VaDE in that it concatenates additional variables y to the latent representation z. For example, y may contain the available class labels or already known subgroup structures, which do not need to be discovered. It is assumed that these additional variables y are available at training and test time. Specifically, the generative process of CDVaDE takes the form</p><formula xml:id="formula_5">p(c) = Cat(π)<label>(4)</label></formula><formula xml:id="formula_6">p(z|c) = N z; μ c , diag σ 2 c , (<label>5</label></formula><formula xml:id="formula_7">)</formula><formula xml:id="formula_8">μ xy , log σ 2 xy = f (z, y; φ),<label>(6)</label></formula><formula xml:id="formula_9">p(x|z, y) = N x; μ xy , diag σ 2 xy (7)</formula><p>Since our goal is to find clusters c that are unassociated with the available variables y of choice and to learn latent representations z that do not contain information about y, the generative process of CDVaDE assumes that z, c are jointly independent of y.</p><p>The changes compared to the generative process of VaDE can also be regarded as imposing a structure on the model, where the encoder learns hidden representations of the image x conditioned to the additional variables y (i.e., q(z|x, y)), but acts as an identity function with respect to y (i.e., y can be regarded as being simply concatenated to the latent space representations z). The decoder then translates this data representation in the form of (z, y) to the input space (i.e., p(x|z, y)). Given that the underlying VAE architecture seeks to efficiently compress the input data x into a learned representation, this incentivizes the model to exclude information about y from the learned variables z and c.</p><p>The ELBO of CDVaDE can be derived as follows,</p><formula xml:id="formula_10">L ELBO (x|y) = E q(z,c|x) log p(z, x, c|y) q(z, c|x) = E q(z,c|x) [log p(x|z, y) + log p(z|c) + log p(c) -log q(z|x) -log q(c|x)],<label>(8)</label></formula><p>where we use the fact that by the generative process of CDVaDE it holds that p(x, z, c|y) = p(x|z, y)p(z|c, y)p(c|y) = p(x|z, y)p(z|c)p(c),</p><p>and we adopt from VaDE the assumption that q(z, c|x) = q(z|x)q(c|x) holds. Hence, once the base VaDE decoder CNN is replaced by its modified version f (z, y; θ) in CDVaDE, there are no further differences between the ELBO loss function of Eq. ( <ref type="formula" target="#formula_10">8</ref>) compared to Eq. ( <ref type="formula" target="#formula_0">1</ref>).</p><p>While in this work we present our conditioning mechanism as an extension to VaDE, it can be combined with any deep clustering algorithm that follows an encoder-decoder architecture. In all our experiments, we use the same CNN architectures for the encoder and decoder as in VaDE (see Sect. 2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Embedding Clustering (DEC)</head><p>Deep Embedding Clustering (DEC) <ref type="bibr" target="#b13">[14]</ref> is a popular state-of-the-art clustering approach that combines a deep embedding model with k-means clustering. In this study, we include comparisons of VaDE and the proposed CDVaDE to DEC, because it is a model that belongs to a different family of deep clustering algorithms which are not based on variational inference. In our DEC experiments, we use the same autoencoder architecture and the same initialization as for the VaDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Related Works in Medical Imaging</head><p>A number of studies have been conducted with several approaches of deep clustering for medical imaging data. Typically, clustering is performed on top of features extracted with the use of an encoder neural network, and the cluster assignments are determined by using conventional clustering algorithms, such as k-means, on top of the learned latent representations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12]</ref>. In contrast, this work investigates models which enforce a clustering structure in the latent space through the use of a MOG prior distribution, as well as guidance of the clustering model via the proposed conditioning mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Colored MNIST</head><p>The Colored MNIST is an extension to the classic MNIST dataset <ref type="bibr" target="#b2">[3]</ref>, which contains binary images of handwritten digits. The Colored MNIST includes colored images of the same digits, where each number and background have a color assignment. We present results of the experiments with five distinct colors and five digits of MNIST (0-4). To enhance computational efficiency and expedite experiments, we utilized only 1% of the MNIST images, which were sampled at random. This simple dataset can be used to investigate whether a given clustering algorithm will categorize the images by color or by the digit label and whether the proposed conditioning mechanism of CDVaDE can successfully guide the clustering away from the categorization we want to avoid (e.g., condition the model to avoid clustering by color, in order to distinguish the digits in an unsupervised fashion). We compare CDVaDE to the deep clustering models VaDE and DEC that do not incorporate such conditioning. We use latent space dimensionality d = 20 for all models. In Fig. <ref type="figure" target="#fig_1">2</ref> a summary of the results for the experiments on the colored MNIST dataset is presented. The results demonstrate that by allowing for the incorporation of additional information, particularly color labels, the proposed CDVaDE model is more sensitized to learning other underlying features, which allows for distinguishing between the different digits in this particular example. Notably, both VaDE and DEC end up clustering the data by color, as it is the most striking distinguishing characteristic of these images. On the other hand, the predicted domains of CDVaDE have no association with color, and the data are separated by the shapes in the images, distinguishing some of the digit labels (albeit imperfectly). This example serves as a proof of concept for the proposed conditioning mechanism of CDVaDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Application to a Digital Pathology Dataset</head><p>HER2 Dataset. Human epidermal growth factor receptor 2 (HER2 or HER2/neu) is a protein involved in normal cell growth, which plays an important role in the diagnosis and treatment of breast cancer <ref type="bibr" target="#b7">[8]</ref>. The dataset consists of 241 patches extracted from 64 digitized slides of breast cancer tissue which were stained with HER2 antibody. Each tissue slide has been digitized at three different sites using three different whole slide imaging systems, evaluated by 7 pathologists on a 0-100 scale, and following clinical practice labeled as HER2 Class 1, 2, or 3 (based on mean pathologists' scores with cut-points at 33 and 66). We use a subset of this dataset consisting of 672 images (the remainder is held out for future research). Because the intended purpose is finding subgroups in the given dataset only, a separate test set is not used. The dimensions of the images vary from 600 to 826 pixels, and we scale all data to a uniform size of 128 × 128 pixels before further processing. We refer to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref> for more details about this dataset. This retrospective human subject dataset has been made available to us by the authors of the prior studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, who are not associated with this paper. Appropriate ethical approval for the use of this material in research has been obtained.</p><p>Deep Clustering Models Applied to the HER2 Dataset. We evaluate the performance and behavior of the DEC, VaDE, and CDVaDE models on the HER2 dataset. We investigate whether the models will learn to distinguish the HER2 class labels, the scanner labels, or other potentially meaningful data subgroups in a fully unsupervised fashion. To investigate the clustering abilities of CDVaDE on the HER2 dataset, we inject the HER2 class labels into the latent embedding space. We hypothesize that this will disincentivize the encoder network from including information related to the HER2 class labels in the latent representations z. Thus, with CDVaDE we aim to guide the clustering towards identifying subgroup structures that are not associated with the HER2 classes, and potentially were not previously recognized. The dimensionality of the latent embedding space was set to d = 500 for all three models.  As illustrated by the bar graphs in Fig. <ref type="figure" target="#fig_2">3</ref>, there is an association between HER2 class 2 and predicted domain 2, as well as between HER2 class 3 and predicted domain 3. Similarly to the VaDE model, the DEC model has also shown the ability to separate between HER2 class 2 and HER2 class 3. To investigate these observations further, we look at the distribution of the ground truth HER2/neu scores within each of the predicted domains. The boxplots in Fig. <ref type="figure" target="#fig_4">4</ref> show that both the VaDE and DEC models tend to separate high HER2/neu scores from the lower ones. The Pearson's correlation coefficient between the clustering assignments c of VaDE and the HER2/neu scores is 0.46. The correlation coefficient between the DEC clusters and the HER2/neu scores is 0.71. However, neither VaDE nor DEC clusters are associated to the scanner labels. We investigate the proposed CDVaDE model with the goal of identifying meaningful data subgroups which are not associated with the already known HER2 class labels. As visualized in Fig. <ref type="figure" target="#fig_2">3</ref>, the predicted domains are again clearly visually disparate. However, as intended, there is a weaker association with the HER2 class labels and a stronger association with the scanner labels, compared to the results of VaDE and DEC. In Fig. <ref type="figure" target="#fig_4">4</ref>, HER2/neu median scores of the three clusters move closer together, illustrating the decrease of association with HER2 class labels, as intended by the formulation of the CDVaDE model. The correlation coefficient between the CDVaDE cluster assignments and the HER2/neu scores is 0.39. While the CDVaDE model does not achieve full independence between the identified clusters and the HER2 labels, it decreases this association compared to VaDE and DEC. Moreover, the clusters identified by CDVaDE are distinctly different from those of VaDE, with a 0.43 proportion of agreement between the two algorithms (after matching the two sets of cluster assignments using the Hungarian algorithm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We investigated deep clustering models for the identification of meaningful subgroups within medical image datasets. The proposed CDVaDE model incorporates a conditioning mechanism that is capable of guiding the clustering model away from subgroup structures that have already been annotated and towards the identification of yet unrecognized image subgroups/domains. Our experimental findings on the HER2 digital pathology dataset surmise that VaDE and DEC are capable of finding, in an unsupervised fashion, image subgroups related to the HER2 class labels, while CDVaDE (conditioned on the HER2 labels) identifies visually distinct subgroups that have a weaker association to the HER2 labels. Because the CDVaDE clusters do not clearly correspond to the scanner labels either, future work involves a review by a pathologist to see whether these subgroups capture meaningful but unannotated characteristics in the images. While CDVaDE can be used as an exploratory tool to unveil unknown subgroups in a given dataset, developing specialized quantitative evaluation metrics for this unsupervised task is inherently difficult and will also be a focus in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Diagram of CDVaDE: z distribution is driven by Gaussian mean and covariance parameters μ k and σ 2 k , prior cluster probabilities π k , and conditioning variables y.</figDesc><graphic coords="4,151,98,53,84,148,93,67,87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Both VaDE and DEC cluster the Colored MNIST digits by the color, while CDVaDE clusters are associated with the digit label. Bar graphs labeled "Color"each color represents a specific color of Colored MNIST digits. In the "Digit" plots, colors correspond to digits labels.</figDesc><graphic coords="6,86,97,146,51,278,80,58,36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results summary for VaDE, CDVaDE, and DEC, all with D = 3. Example images from the identified clusters are visualized for each method. Distributions of HER2 class and scanner labels are shown per cluster (i.e., predicted domain).</figDesc><graphic coords="7,47,31,285,71,329,14,125,83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure3demonstrates that even without scrutinizing, one can observe a strong visual separation between the algorithmically identified image domains for both VaDE and DEC experiments. For example, in the first predicted domain by VaDE in Fig.3images tend to have slightly visible boundaries but a comparatively uniform light appearance overall. In the second predicted domain, images have less visible boundaries and more pail staining. In the third predicted domain, images have more visible staining and sharper edges compared to the other domains.As illustrated by the bar graphs in Fig.3, there is an association between HER2 class 2 and predicted domain 2, as well as between HER2 class 3 and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Boxplots of HER2/neu scores per predicted domain for all experiments.</figDesc><graphic coords="8,127,47,183,74,197,80,68,20" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Conditionally Decoded Variational Deep Embedding (CDVaDE)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. The authors would like to thank <rs type="person">Dr. Marios Gavrielides</rs> for providing access to the HER2 dataset and for helpful discussion. This project was supported in part by an appointment to the <rs type="programName">Research Participation Program</rs> at the <rs type="institution">U.S. Food and Drug Administration</rs> administered by the <rs type="funder">Oak Ridge Institute for Science and Education through an interagency agreement between the U.S. Department of Energy</rs> and the <rs type="funder">U.S. Food and Drug Administration</rs>. XS acknowledges support from the <rs type="funder">Hightech Agenda Bayern</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_w34VQUC">
					<orgName type="program" subtype="full">Research Participation Program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 64.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning with k-means and an ensemble of deep convolutional neural networks for medical image classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03359</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Artificial intelligence and machine learning for medical imaging: a technology review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barragán-Montero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica Med</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="242" to="256" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The MNIST database of handwritten digit images for machine learning research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Observer variability in the interpretation of HER2/neu immunohistochemical expression with unaided and computer-aided digital microscopy</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gavrielides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Gallas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Badano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hewitt</surname></persName>
		</author>
		<idno type="DOI">10.5858/135.2.233</idno>
		<ptr target="https://doi.org/10.5858/135.2.233" />
	</analytic>
	<monogr>
		<title level="j">Arch. Pathol. Lab. Med</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="242" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Performance deterioration of deep neural networks for lesion classification in mammography due to distribution shift: an analysis based on artificially created distribution shift</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2551346</idno>
		<ptr target="https://doi.org/10.1117/12.2551346" />
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Diagnosis</title>
		<imprint>
			<biblScope unit="volume">11314</biblScope>
			<biblScope unit="page">1131404</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>SPIE</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variational deep embedding: an unsupervised and generative approach to clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DeepMCAT: large-scale deep clustering for medical image categorization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-88210-5_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-88210-526" />
	</analytic>
	<monogr>
		<title level="m">DGM4MICCAI/DALI -2021</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Engelhardt</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13003</biblScope>
			<biblScope unit="page" from="259" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reproducibility in the automated quantitative assessment of HER2/neu for breast cancer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Keay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>O'flaherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gavrielides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Pathol. Inform</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Design characteristics of studies reporting the performance of artificial intelligence algorithms for diagnostic analysis of medical images: results from recently published papers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.3348/kjr.2019.0025</idno>
		<ptr target="https://doi.org/10.3348/kjr.2019.0025" />
	</analytic>
	<monogr>
		<title level="j">Korean J. Radiol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="405" to="410" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>arxiv.org/abs/1312.6114v10</idno>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hidden stratification causes clinically meaningful failures in machine learning for medical imaging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Oakden-Rayner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<idno type="DOI">10.1145/3368555.3384468</idno>
		<ptr target="https://doi.org/10.1145/3368555.3384468" />
	</analytic>
	<monogr>
		<title level="m">CHIL 2020</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised deep clustering for predictive texture pattern discovery in medical images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Perkonigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sobotka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ba-Ssalamah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03721</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mitigating bias in machine learning for medicine</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Vokinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feuerriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Kesselheim</surname></persName>
		</author>
		<idno type="DOI">10.1038/s43856-021-00028-w</idno>
		<ptr target="https://doi.org/10.1038/s43856-021-00028-w" />
	</analytic>
	<monogr>
		<title level="j">Commun. Med. 1(1)</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v48/xieb16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the 33rd International Conference on Machine Learning. Machine Learning Research<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">External validation of deep learning algorithms for radiologic diagnosis: a systematic review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mohajer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eng</surname></persName>
		</author>
		<idno type="DOI">10.1148/ryai.210064</idno>
		<ptr target="https://doi.org/10.1148/ryai.210064" />
	</analytic>
	<monogr>
		<title level="j">Radiol. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">210064</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
