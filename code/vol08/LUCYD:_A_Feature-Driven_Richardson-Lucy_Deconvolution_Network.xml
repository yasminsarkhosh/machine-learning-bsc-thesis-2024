<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network</title>
				<funder ref="#_633ZR7w">
					<orgName type="full">Helmholtz Association</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tomáš</forename><surname>Chobola</surname></persName>
							<idno type="ORCID">0009-0000-3272-9996</idno>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Helmholtz AI</orgName>
								<orgName type="institution" key="instit2">Helmholtz Munich -German Research Center for Environmental Health</orgName>
								<address>
									<settlement>Neuherberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gesine</forename><surname>Müller</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Georg-August-University Göttingen</orgName>
								<address>
									<settlement>Göttingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Veit</forename><surname>Dausmann</surname></persName>
							<idno type="ORCID">0000-0003-3281-9208</idno>
							<affiliation key="aff2">
								<orgName type="institution">GEOMAR Helmholtz Centre for Ocean Research Kiel</orgName>
								<address>
									<settlement>Kiel</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anton</forename><surname>Theileis</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">GEOMAR Helmholtz Centre for Ocean Research Kiel</orgName>
								<address>
									<settlement>Kiel</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Taucher</surname></persName>
							<idno type="ORCID">0000-0001-9944-0775</idno>
							<affiliation key="aff2">
								<orgName type="institution">GEOMAR Helmholtz Centre for Ocean Research Kiel</orgName>
								<address>
									<settlement>Kiel</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Huisken</surname></persName>
							<idno type="ORCID">0000-0001-7250-3756</idno>
							<affiliation key="aff1">
								<orgName type="institution">Georg-August-University Göttingen</orgName>
								<address>
									<settlement>Göttingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tingying</forename><surname>Peng</surname></persName>
							<email>tingying.peng@helmholtz-muenchen.de</email>
							<idno type="ORCID">0000-0002-7881-1749</idno>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Helmholtz AI</orgName>
								<orgName type="institution" key="instit2">Helmholtz Munich -German Research Center for Environmental Health</orgName>
								<address>
									<settlement>Neuherberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="656" to="665"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B5FFF8FDAAE680691E72EA571DAB6081</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_63</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deconvolution</term>
					<term>Deblurring</term>
					<term>Denoising</term>
					<term>Microscopy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The process of acquiring microscopic images in life sciences often results in image degradation and corruption, characterised by the presence of noise and blur, which poses significant challenges in accurately analysing and interpreting the obtained data. This paper proposes LUCYD, a novel method for the restoration of volumetric microscopy images that combines the Richardson-Lucy deconvolution formula and the fusion of deep features obtained by a fully convolutional network. By integrating the image formation process into a featuredriven restoration model, the proposed approach aims to enhance the quality of the restored images whilst reducing computational costs and maintaining a high degree of interpretability. Our results demonstrate that LUCYD outperforms the state-of-the-art methods in both synthetic and real microscopy images, achieving superior performance in terms of image quality and generalisability. We show that the model can handle various microscopy modalities and different imaging conditions by evaluating it on two different microscopy datasets, including volumetric widefield and light-sheet microscopy. Our experiments indicate that LUCYD can significantly improve resolution, contrast, and overall quality of microscopy images. Therefore, it can be a valuable tool for microscopy image restoration and can facilitate further research in various microscopy applications. We made the source code for the model accessible under https://github.com/ctom2/lucyd-deconvolution/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Microscopy is one of the most widely used imaging techniques that allows life scientists to analyse cells, tissues and subcellular structures with a high level of detail. However, microscopy images often suffer from degradation such as blur, noise and other artefacts, which may result in an inaccurate quantification and hinder downstream analysis. Therefore, deconvolution techniques are necessary to restore the images to improve their quality, thus increasing the accuracy of downstream tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref>. Image deconvolution is a well-studied task in computer vision and imaging sciences that aims to recover a sharp and clear object out of a degraded input. The mathematical representation of image corruption can be expressed as:</p><formula xml:id="formula_0">y = x * K + n,<label>(1)</label></formula><p>where * represents convolution, y denotes the resulting image of an object x, which has been blurred with a point spread function (PSF) K, and degraded by noise n. Two classic image deconvolution methods widely used in microscopy and medical imaging are Wiener filter <ref type="bibr" target="#b17">[18]</ref> and Richardson-Lucy algorithm (RL) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>. The Wiener filter is a linear filter that is applied to the frequency domain representation of the blurred image. It assumes the Gaussian noise distribution and thus minimises the mean squared error between the restored image and the original one. The RL method, on the other hand, is an iterative algorithm that works in the spatial domain, usually leading to better reconstruction than Wiener filter. It assumes Poisson noise distribution and seeks to estimate the corresponding sharp image x in a fixed number of iterations or until a convergence criterion is met <ref type="bibr" target="#b4">[5]</ref>. While being simple and effective, the main limitations of both methods are their susceptibility to noise amplification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref> and the assumption that the accurate PSF is known. In practice, however, PSF is challenging to obtain and is often unknown or varies across the image, which leads to inaccurate reconstructions of the sharp image. Moreover, as an iterative method, RL is computationally costly for three-dimensional (3D) data <ref type="bibr" target="#b3">[4]</ref>.</p><p>In the computer vision field, numerous deep learning models have been trained on large datasets with the objective to learn a direct mapping between input and output domains <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. Some of these models have also been adapted for use in microscopy, such as the U-Net-based content-aware image restoration networks (CARE) <ref type="bibr" target="#b16">[17]</ref>. These methods have exhibited exceptional performance in tasks such as super-resolution and denoising. However, the interpretability of these methods is limited, and given their data-driven nature, the quantity and quality of training data can be a restricting factor, particularly in biomedical applications where data pairs are often scarce or not available.</p><p>Inspired by the RL algorithm, the Richardson-Lucy Network (RLN) <ref type="bibr" target="#b7">[8]</ref> was recently designed to overcome the problem of data-driven models by embedding the RL formula for iterative image restoration into a neural network and substituting convolutions of the measured PSF kernel with learnable convolutional layers. Although being more compact than U-Net, the low capacity of RLN makes it insufficiently robust to different blur intensities and noise levels, requiring the network to be re-trained whenever there is a shift in the input image domain. This reduces the efficacy of the method.</p><p>To address the limitations of existing methods, we propose a novel lightweight model called LUCYD, which integrates the RL deconvolution formula and a Ushaped network. The main contributions of this paper can be summarised as:</p><p>1. LUCYD is a lightweight deconvolution method that embeds the RL deconvolution formula into a deep convolutional network that leverages the features extracted by a U-shaped module while maintaining low computational costs for processing 3D microscopy images and a high level of interpretability. 2. The proposed method outperforms existing deconvolution methods on both real and synthetic datasets, based on qualitative assessment and quantitative evaluation metrics, respectively. 3. We show that LUCYD has strong resistance to noise and can generalise well to new and unseen data. This ability makes it a valuable tool for practical applications in microscopy imaging fields where image quality is critical for downstream tasks yet training data are often scarce or unavailable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The overall architecture of the proposed model is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref> and it comprises three main components: a correction module, an update module, and a bottleneck that is shared between the two modules. The data flow in the model is based on the following iterative RL formula:</p><formula xml:id="formula_1">z (k) = z (k-1)</formula><p>x estimate</p><formula xml:id="formula_2">• y z (k-1) * K * K update term ,<label>(2)</label></formula><p>which aims to recover x in k steps. We bypass the requirement of k-1 preceding iterations with the correction module that generates a mask M to form an intermediate sharp image estimation through a single forward pass, allowing to rapidly process 3D data, as follows:</p><formula xml:id="formula_3">z = y + M. (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>Next, inspired by Li et al. <ref type="bibr" target="#b7">[8]</ref>, we adopt the three-step approach to decompose the RL update term from Eq. 2 in the update module:</p><formula xml:id="formula_5">(a) FP = y * f, (b) DV = y/FP, (c) u = DV * b. (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Specifically, we replace convolutions with a known PSF in steps (a) and (c) with forward projector f and backward projector b, which consist of sets of learnable convolutional layers. The produced update term u allows us to recondition the estimate z from the correction module into a sharp image through multiplication, i.e. the last step of image formation in the RL formula: x = z • u. The whole network can then be expressed as follows,</p><formula xml:id="formula_7">x = z • y y * f * b . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>By adhering to the image formation steps as prescribed by the RL formula, we maintain a high degree of interpretability, critical for real-world scenarios, where the accuracy and reliability of the generated results are of utmost importance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Correction Module and Bottleneck</head><p>The proposed correction module and bottleneck architectures consist of encoder blocks (EBs), decoder blocks (DBs), and multi-scale feature fusion blocks to facilitate efficient information exchange across different scales within the model.</p><p>Feature Encoding. The features of the volumetric input image y ∈ R C×D×H×W are obtained through the first encoder block EB 1 in the correction module, and then encoded by a convolutional layer with a stride 2. Subsequently, the downsampled features are concatenated with the encoded features of the forward projection f from the update module and then fed to the bottleneck encoder EB 2 to integrate the information from both modules.</p><p>Feature Fusion Block. Similarly to Cho et al. <ref type="bibr" target="#b1">[2]</ref>, we enhance the connections between encoders and decoders and allow information flow from different scales within the network through Feature Fusion Blocks (FFBlocks). The features from EB 1 and EB 2 are refined as follows,</p><formula xml:id="formula_9">FFBlock out 1 = FFBlock 1 EB out 1 , (EB out 2 ) ↑ , (<label>6</label></formula><formula xml:id="formula_10">)</formula><formula xml:id="formula_11">FFBlock out 2 = FFBlock 2 (EB out 1 ) ↓ , EB out 2 , (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where up-sampling (↑) and down-sampling (↓) is applied to allow for feature concatenation. The multi-scale features are then combined and processed by 1 × 1 and 3 × 3 convolutional layers, respectively, to allow the decoder blocks DB 1 and DB 2 to utilise information obtained on different scales. The structure of the blocks is shown in Fig. <ref type="figure" target="#fig_1">2a</ref>.</p><p>Feature Decoding. Initially, the refined features are decoded in the bottleneck using a convolutional layer and residual block within the DB 2 . Next, these features are expanded with a convolutional layer to match the dimensions in both the correction and update modules. The resulting features are then concatenated with the output of FFBlock 1 and subsequently fed into decoder DB 2 within the correction module. The features are then mapped to the image dimensions resulting in mask M , which is summed with y to form z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Update Module</head><p>Inspired by the forward and backward projector functions <ref type="bibr" target="#b7">[8]</ref>, we substitute the PSF convolution operations from Richardson-Lucy algorithm with learnable convolutional layers and residual blocks.</p><p>During forward projection (FP), shallow features are initially extracted by a single convolutional layer and then refined by a residual block. The output of f is then passed to Richardson-Lucy Division Block (RLDiv) which embeds the division of the raw image y by the channel-wise mean of the refined FP features. Next, we project the division result to a feature map to extract more information about the image. The visualisation of the process is in Fig. <ref type="figure" target="#fig_1">2b</ref>. These features are then concatenated with the features extracted by the bottleneck and combined by a convolutional layer which initiates the backward projection with b. The output is then summed with the output of RLDiv, forming a skipconnection, and passed through a residual block. The features are then refined by a convolutional layer and their channel-wise mean is taken to be the "update term" u, which is used to obtain the final model output x through multiplication with z (denoted as RLMul).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Loss Function</head><p>The entire model is trained end-to-end with a single loss function that combines the Mean Squared Error (MSE) and the Structural Similarity Index Measure (SSIM) as follows:</p><formula xml:id="formula_13">L(x , x) = MSE(x , x) -ln 1 + SSIM(x , x) 2 , (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>where x is the ground truth sharp image and x is the model estimation of x (Table <ref type="table" target="#tab_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>Datasets. We assess the performance of LUCYD on both simulated phantom objects and real microscopy images. To achieve this, we use five sets of 3D grayscale volumes generated by Li et al. <ref type="bibr" target="#b7">[8]</ref>, consisting of dots, solid spheres, and ellipsoidal surfaces, which are provided along with their sharp ground truth volumes of dimensions 128 × 128 × 128 (one exemplary image shown in Fig. <ref type="figure" target="#fig_2">3e</ref>).</p><p>To test the generalization capabilities of our method, we also include two blurry and noisy versions of the dataset, D nuc and D act , which utilize different image degradation processes for embryonic nuclei and membrane data. Additionally, we generate a mixed dataset by applying permutations of three Gaussian blur intensities (σ b = [1.0, 1.2, 1.5]) and three levels of additive Gaussian noise (σ n = [0, 15, 30]) to the ground truth volumes, and then test the ability of the model to generalize to volumes blurred with Gaussian kernels (σ b = [0.5, 2.0]) and additive Gaussian noise (σ n = [20, 50, 70, 100]) levels outside of the training dataset. The model is trained on patches of dimensions 32 × 64 × 64 that are randomly sampled from the training datasets. Moreover, we evaluate the model trained using synthetic phantom shapes on a real 3D light-sheet image of a starfish (private data) and widefield microscopy image of U2OS cell (from the dataset of <ref type="bibr" target="#b7">[8]</ref>), to explore the generalisation capabilities. Baseline and Metrics. We employ one classic U-Net-based fluorescence image restoration model CARE <ref type="bibr" target="#b16">[17]</ref> and one RL-based convolutional model RLN <ref type="bibr" target="#b7">[8]</ref> as baselines. We quantitatively evaluate the deconvolution performance on simulated data using two metrics: Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR).  ). LUCYD exhibits superior performance in recovering fine details and structures as compared to CARE and RLN, while simultaneously maintaining low levels of noise and haze surrounding the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>In Table <ref type="table" target="#tab_1">2</ref>, we present the quantitative results of all three methods on simulated phantom objects degraded with blur and noise levels that were not present in the training dataset. LUCYD achieves the best performance even in cases where the amount of additive noise exceeds the maximum level included in the training dataset. This is in contrast to CARE and RLN, which did not demonstrate such exceptional generalisation capabilities and noise resistance. We further examine LUCYD's performance on datasets simulating widefield microscopy imaging of embryo nuclei and membrane data. As shown in Table <ref type="table" target="#tab_2">3</ref>, LUCYD outperforms CARE and RLN in both in-domain and cross-domain assessments, further supporting the model's capabilities in cross-domain applications. We finally apply LUCYD on two real microscopy test samples, as illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>. On the 3D light-sheet image of starfish, LUCYD recovers more details and structures than RLN while maintaining low levels of noise and haze surrounding the object in both lateral and axial projections. On the other test sample of a fixed U2OS cell acquired by widefield microscopy, LUCYD also suppresses noise and haze to a higher degree compared to RLN and CARE and retrieves finer and sharper details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we introduce LUCYD, an innovative technique for deconvolving volumetric microscopy images that combines a classic image deconvolution formula with a U-shaped network. LUCYD takes advantages of both approaches, resulting in a highly efficient method capable of processing 3D data with high efficacy. We have demonstrated through experiments on both synthetic and real microscopy datasets that LUCYD exhibits strong generalization capabilities, as well as robustness to noise. These qualities make it an excellent tool for crossdomain applications in various domains, such as biology and medical imaging. Additionally, the lightweight nature of LUCYD makes it computationally feasible for real-time applications, which can be crucial in various settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of LUCYD consists of a correction module, an update module and a bottleneck that is shared between the two modules.</figDesc><graphic coords="4,70,98,54,29,310,48,195,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The architecture of submodules: (a) Feature Fusion Block (FFBlock), (b) Richardson-Lucy Division Block (RLDiv).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of the deconvolution process given an input (a). The outputs of the correction module and the update module are shown in (b) and (c), respectively, and the final output obtained through their multiplication is in (d).</figDesc><graphic coords="5,61,29,486,29,301,60,55,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Quantitative comparison of RLN and LUCYD on lateral and axial maximumintensity projections of a starfish acquired by 3D light-sheet microscopy is shown in (a).Additional analysis of the deconvolution results of CARE, RLN and LUCYD trained on synthetic phantom objects in (b) shows patches of four-colour lateral maximumintensity projections of a fixed U2OS cell acquired by widefield microscopy (from the dataset of<ref type="bibr" target="#b7">[8]</ref>). LUCYD exhibits superior performance in recovering fine details and structures as compared to CARE and RLN, while simultaneously maintaining low levels of noise and haze surrounding the objects.</figDesc><graphic coords="8,87,96,60,32,276,76,210,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Number of learnable parameters, comparing CARE, RLN and LUCYD.</figDesc><table><row><cell cols="2">CARE [17] RLN [8] LUCYD (ours)</cell></row><row><cell>1 M</cell><cell>15,900 24,964</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance on synthetic datasets (SSIM/PSNR (dB)) degraded with blur and noise levels not present in the training dataset. The models are trained on phantom objects blurred with σ b = [1.0, 1.2, 1.5] and corrupted with Gaussian noise intensities σn = [0, 15, 30].</figDesc><table><row><cell cols="3">Blur intensity σ b Noise level σn CARE [17]</cell><cell>RLN [8]</cell><cell>LUCYD (ours)</cell></row><row><cell>0.5</cell><cell>20</cell><cell cols="2">0.9166/21.62 0.9571/25.60 0.9725/26.85</cell></row><row><cell>0.5</cell><cell>50</cell><cell cols="2">0.7589/15.96 0.8519/21.67 0.9463/24.35</cell></row><row><cell>0.5</cell><cell>70</cell><cell cols="2">0.6828/14.32 0.7235/18.52 0.9040/21.78</cell></row><row><cell>0.5</cell><cell>100</cell><cell cols="2">0.5856/12.56 0.5644/15.91 0.7233/17.47</cell></row><row><cell>2.0</cell><cell>20</cell><cell cols="2">0.8582/20.36 0.9040/22.34 0.9271/23.49</cell></row><row><cell>2.0</cell><cell>50</cell><cell cols="2">0.7057/16.35 0.7443/18.85 0.8575/21.00</cell></row><row><cell>2.0</cell><cell>70</cell><cell cols="2">0.6259/15.06 0.6051/16.69 0.7995/19.38</cell></row><row><cell>2.0</cell><cell>100</cell><cell cols="2">0.5154/13.08 0.4495/14.86 0.6311/16.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance on synthetic datasets (SSIM/PSNR (dB)) given varying training data.</figDesc><table><row><cell></cell><cell cols="3">Train dataset Test dataset CARE [17]</cell><cell>RLN [8]</cell><cell>LUCYD (ours)</cell></row><row><cell>In-domain</cell><cell>Dnuc</cell><cell>Dnuc</cell><cell cols="2">0.7895/18.00 0.9247/26.43 0.9525/28.57</cell></row><row><cell></cell><cell>Dact</cell><cell>Dact</cell><cell cols="2">0.7666/17.44 0.8966/26.10 0.9450/27.83</cell></row><row><cell cols="2">Cross-domain Dnuc</cell><cell>Dact</cell><cell cols="2">0.7623/17.68 0.8841/24.33 0.9024/24.82</cell></row><row><cell></cell><cell>Dact</cell><cell>Dnuc</cell><cell cols="2">0.7584/17.00 0.9081/27.23 0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>.9336/27.63</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. <rs type="person">Tomáš Chobola</rs> is supported by the <rs type="funder">Helmholtz Association</rs> under the joint research school "<rs type="programName">Munich School for Data Science -MUDS</rs>".</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_633ZR7w">
					<orgName type="program" subtype="full">Munich School for Data Science -MUDS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Three-dimensional residual channel attention networks denoise and sharpen fluorescence microscopy image volumes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-021-01155-x</idno>
		<ptr target="https://doi.org/10.1038/s41592-021-01155-x" />
	</analytic>
	<monogr>
		<title level="j">Nat. Meth</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="678" to="687" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rethinking coarse-to-fine approach in single image deblurring</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4641" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A modified damped Richardson-Lucy algorithm to reduce isotropic background effects in spherical deconvolution</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dell'acqua</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2009.09.033</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2009.09.033" />
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1446" to="1458" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Richardson-Lucy algorithm with total variation regularization for 3D confocal microscope deconvolution</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dey</surname></persName>
		</author>
		<idno type="DOI">10.1002/jemt.20294</idno>
		<ptr target="https://doi.org/10.1002/jemt.20294" />
	</analytic>
	<monogr>
		<title level="j">Microsc. Res. Tech</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="260" to="266" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Comparison of the Richardson-Lucy method and a classical approach for spectrometer bandpass correction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eichstädt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metrologia</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rapid image deconvolution and multiview fusion for optical microscopy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41587-020-0560-x</idno>
		<ptr target="https://doi.org/10.1038/s41587-020-0560-x" />
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1337" to="1346" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Smart Nanoscopy: a review of computational approaches to achieve super-resolved optical microscopy</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kaderuppan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Woo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.3040319</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2020.3040319" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="214801" to="214831" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating the image formation process into deep learning improves network performance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Meth</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1427" to="1437" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An iterative technique for the rectification of observed distributions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Lucy</surname></persName>
		</author>
		<idno type="DOI">10.1086/111605</idno>
		<ptr target="https://doi.org/10.1086/111605" />
	</analytic>
	<monogr>
		<title level="j">Astron. J</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">745</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluation and development of deep neural networks for image super-resolution in optical microscopy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-020-01048-5</idno>
		<ptr target="https://doi.org/10.1038/s41592-020-01048-5" />
	</analytic>
	<monogr>
		<title level="j">Nat. Meth</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="194" to="202" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayesian-based iterative method of image restoration *</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="59" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DeconvolutionLab2: an open-source software for deconvolution microscopy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sage</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ymeth.2016.12.015</idno>
		<ptr target="https://doi.org/10.1016/j.ymeth.2016.12.015" />
	</analytic>
	<monogr>
		<title level="j">Methods</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="28" to="41" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Penalized maximum likelihood angular super-resolution method for scanning radar forward-looking imaging</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.3390/s18030912</idno>
		<ptr target="https://doi.org/10.3390/s18030912" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">912</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to reconstruct confocal microscopy stacks from single light field images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Vizcaíno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Saltarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belyaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lyck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCI.2021.3097611</idno>
		<ptr target="https://doi.org/10.1109/TCI.2021.3097611" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Imaging</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="775" to="788" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning-enhanced light-field imaging with continuous validation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-021-01136-0</idno>
		<idno>41592-021- 01136-0</idno>
		<ptr target="https://doi.org/10.1038/s" />
	</analytic>
	<monogr>
		<title level="j">Nat. Meth</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="557" to="563" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A workingperson&apos;s guide to deconvolution in light microscopy</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Swedlow</surname></persName>
		</author>
		<idno type="DOI">10.2144/01315bi01</idno>
		<ptr target="https://doi.org/10.2144/01315bi01" />
	</analytic>
	<monogr>
		<title level="j">Biotechniques</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1076" to="1097" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Content-aware image restoration: pushing the limits of fluorescence microscopy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weigert</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-018-0216-7</idno>
		<idno>41592-018-0216-7</idno>
		<ptr target="https://doi.org/10.1038/s" />
	</analytic>
	<monogr>
		<title level="j">Nat. Meth</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1090" to="1097" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Extrapolation, Interpolation, and Smoothing of Stationary Time Series</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wiener</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/2946.001.0001</idno>
		<ptr target="https://doi.org/10.7551/mitpress/2946.001.0001" />
		<imprint>
			<date type="published" when="1949">1949</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
