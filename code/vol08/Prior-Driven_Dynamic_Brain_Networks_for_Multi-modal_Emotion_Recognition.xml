<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition</title>
				<funder ref="#_wytRwDb">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_skYyR4Z #_hWhaXTg #_WjsSdJj">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_gmNykRD">
					<orgName type="full">Key Research and Development Plan of Jiangsu Province</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chuhang</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">College of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Brain-Machine Intelligence Technology</orgName>
								<orgName type="department" key="dep3">Ministry of Education</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>211106</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">College of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Brain-Machine Intelligence Technology</orgName>
								<orgName type="department" key="dep3">Ministry of Education</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>211106</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daoqiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">College of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Brain-Machine Intelligence Technology</orgName>
								<orgName type="department" key="dep3">Ministry of Education</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>211106</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">College of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Brain-Machine Intelligence Technology</orgName>
								<orgName type="department" key="dep3">Ministry of Education</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>211106</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="389" to="398"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">90C5F142A2E0E0126FF454181AF0AC76</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_38</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multi-modal emotion recognition</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotions are closely related to many mental and cognitive diseases, such as depression, mania, Parkinson's Disease, etc, and the recognition of emotion plays an important role in diagnosis of these diseases, which is mostly limited to the patient's self-description. Because emotion is always unstable, the objective quantitative methods are urgently needed for more accurate recognition of emotion, which can help improve the diagnosis performance for emotion related brain disease. Existing studies have shown that EEG and facial expressions are highly correlated, and combining EEG with facial expressions can better depict emotion-related information. However, most of the existing multimodal emotion recognition studies cannot combine multiple modalities properly, and ignore the temporal variability of channel connectivity in EEG. In this paper, we propose a spatial-temporal feature extraction framework for multi-modal emotion recognition by constructing priordriven Dynamic Functional Connectivity Networks (DFCNs). First, we consider each electrode as a node to construct the original dynamic brain networks. Second, we calculate the correlation between EEG and facial expression through cross attention, as a prior knowledge of dynamic brain networks, and embedded to obtain the final DFCNs representation with prior knowledge. Then, we design a spatial-temporal feature extraction network by stacking multiple residual blocks based on 3D convolutions, and non-local attention is introduced to capture the global information at the temporal level. Finally, we adopt the features from fully connected layer for classification. Experimental results on the DEAP dataset demonstrate the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In healthcare, affective computing can help measure the psychological state of patients automatically, especially for those with cognitive deficits. For example, the emotional state of hospitalized patients contributes to the early diagnosis of Parkinson's Disease (PD) <ref type="bibr" target="#b10">[11]</ref>. In addition, for patients with neurological diseases, since neurological diseases are degenerative in nature, resulting in unstable cognitive function. The patients may not notice the symptoms of their disease, such as changes in their mood. Recent clinical diagnosis standards rely on the patients' self reports of their feelings to emotional disorders, but it may not be very accurate and stable. Therefore, we need to develop data-driven emotion identification method to improve the diagnosis of these disorders.</p><p>As EEG signals are directly related to high-level cognitive processes, EEGbased emotion recognition draws increasing attention in recent years <ref type="bibr" target="#b0">[1]</ref>. Song et al. <ref type="bibr" target="#b14">[15]</ref> proposed a dynamic graph convolutional network, which trained neural networks to dynamically learn the internal relationships between different EEG channels and extract more discriminative features. Zhang et al. <ref type="bibr" target="#b22">[23]</ref> proposed a self-attention network to jointly model both local and global temporal information of EEG to reduce the effect of noise at the temporal level. These efforts do not take advantage of the complementary information between the modalities, which limits the performance of the model. Recently, a lot of works shown multi-modal data can provide complementary information to improve emotion recognition performance. Wang et al. <ref type="bibr" target="#b19">[20]</ref> combined transformer encoders with attention based fusion to integrate EEG and eye movement data for emotion recognition. Ma et al. <ref type="bibr" target="#b9">[10]</ref> designed a multi-modal residual long short-term memory network (MMResLSTM) to learn the correlation between EEG and peripheral physiological on multi-modal emotion recognition. However, the above work ignores correlations between EEG channels and fails to provide interpretable fusion model. Brain network analysis has been widely used in the field of disease diagnosis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>, which can describe the complex spatial relationships between brain regions of the brain. In recent years, researchers have migrated brain networks into emotion recognition. Wang et al. <ref type="bibr" target="#b20">[21]</ref> implemented PageRank algorithm to rank the importance of brain network nodes, and screened important channels in emotion recognition according to the weight of channels. Huang et al. <ref type="bibr" target="#b5">[6]</ref> proposed a novel neural decoding framework, which builds a bridge between emotions and brain regions, and captures their relationships by performing embedding propagation. However, the methods mentioned above regard the structure of brain network as static, ignoring that the variability of electrode channel connectivity over time. Since the multi-modal data is obtained from the synchronous stimulus in the same time period, this temporal level dynamic is particularly important in the multi-modal emotion recognition. In addition, integrating the heterogeneous data of EEG and facial expression also poses challenge to multi-modal emotion classification.</p><p>To overcome the above limitations, we design a spatial-temporal feature extraction framework based on prior-driven dynamic brain networks and apply it to emotion recognition. Specifically, we treat each electrode of EEG as a node of brain network, and then the dynamic functional connectivity networks (DFCNs) is constructed by Pearson correlation coefficient under non-overlapping time window. Besides, we calculate the correlation between EEG and facial expression across modal channels by cross attention mechanism, as the prior knowledge of DFCNs, and then embed it to above model obtain the final DFCNs representation. Finally, we implemented residual blocks and non-local attention to construct STFENet, so as to extract complex spatial-temporal feature and preserve the long-range dependencies in the time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the framework of our approach, including four parts, i.e., the construction of dynamic brain networks, the representation and learning of crossmodal correlation between EEG and facial expression, the embedding of correlation into DFCNs as prior knowledge, and the extraction of spatial-temporal features of DFCNs for emotion recognition based on 3D convolutions.</p><p>Dynamic Brain Networks Construction: Functional Connectivity Networks (FCNs) ignore the temporal changes of brain connectivity. In this paper, we construct Dynamic Functional Connectivity Networks (DFCNs) to solve the above problem. First, each subject's EEG data can be represented as X E ∈ R P ×T ×D , where P represents the number of channels, T is the number of time windows, and D represents the feature dimension. The t-th subsequence feature of P channels can be represented as a matrix</p><formula xml:id="formula_0">x(t) = [x 1 (t), x 2 (t), • • • , x p (t)] ∈ R P ×D ,</formula><p>where x i (t) ∈ R D represents the t-th subsequence feature extracted from the EEG time series of the i-th channel. According to the divided nonoverlapping sliding time window, we build a functional connectivity network (i.e., matrix) by computing Pearson correlation coefficient between EEG from a pair of channels within the t-th time window:</p><formula xml:id="formula_1">C ij (t) = cov (x i (t), x j (t)) σ xi(t) σ xj (t)<label>(1)</label></formula><p>where cov denotes the covariance between two vectors, σ xi(t) denotes the standard deviation of vector x i (t), x i (t) and x j (t) represent the EEG of a pair of channels i and j within the t-th time window, respectively. Thus, the original DFCNs of each subject</p><formula xml:id="formula_2">DF CN s original = [C(1), C(2), • • • , C(T )] T ∈ R T ×P ×P consists of T transient FCNs.</formula><p>Prior Knowledge Embedding: Most of the existing multi-modal emotion recognition works aim to extract the features of different modalities respectively for fusion, which always lost the correlation between modalities. Existing studies have found there is high correlation between EEG and facial expression <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, but it is still challenging to find an appropriate way to fuse them. Therefore, we calculate the correlation of different modality data as prior knowledge to embed the previously constructed DFCN. Specifically, for each subject, </p><formula xml:id="formula_3">x E ∈ R T ×PE ×D , x F ∈ R T ×PF</formula><formula xml:id="formula_4">Q E = X E W E Q , K E = X E W E K (2) Q F = X F W F Q , K F = X F W F K (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where W Q and W K are the parameter matrices used to generate query and key, which are updated through network back-propagation during model training.</p><p>We determined correlation scores across channel-dimension between modalities based on cross attention, by treating one modality as query and the other as key:</p><formula xml:id="formula_6">Cor(E, F ) = Q E K T F √ d 1 , Cor(F, E) = Q F K T E √ d 2<label>(4)</label></formula><p>where, Cor(E, F ) and Cor(F, E) represents the correlation score between the cross-modality channels, d 1 , d 2 are normalized parameters equal to the dimension of K. It is worth noting that softmax is applied to the scoring weight of the equation Eq. 4. However, softmax proved to be overconfident in its results, which would result in the correlation scores of certain time windows being too high or too low, affecting the reliability of the prior knowledge. Therefore, we improve softmax to softplus to solve this problem while ensuring that the correlation matrix is non-negative. The calculated correlation matrix is as follows:</p><formula xml:id="formula_7">Cor = softplus(Cor(E, F ) + Cor(F, E)) (5)</formula><p>At this point, we obtain the correlation between the cross-modal channels, and use it as the prior knowledge of DFCNs construction. We embed the modified prior knowledge into the previously constructed DFCNs by element-wise product:</p><formula xml:id="formula_8">DF CN s = DF CN s original Cor (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where represents element-wise product. By the embedding of prior knowledge, we obtain the discriminative DFCNs representations with prior knowledge.</p><p>Spatial-Temporal Feature Extraction: Different from static brain networks, DFCNs can not only describe brain connectivity, but also contain the temporallevel volatility of brain connectivity. Most of the existing methods focus on extracting the temporal and spatial features of EEG separately, and concat them for feature fusion, which ignores the dynamic variations of electrode connectivity in the temporal dimension. 2D convolution has been widely used in the field of computer vision, but it is challenging to capture information at the temporal level. Previous studies has shown that 3D convolution operations can better model spatial information in continuous sequences <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>. So, we introduce 3D convolution to extract spatial-temporal feature of DFCNs simultaneously. Considering the DFCNs representation X ∈ R C×T ×P ×P of each subject, where C is the number of channels, T represents the number of time windows, and P represents the number of electrode channels, then the m-th feature representation of the location (T, P, P ) calculated by 3D convolution in space can be represented as:</p><formula xml:id="formula_10">v T,P,P m = ∂ b m + σ P -1 ε=0 P -1 ρ=0 T -1 ϕ=0 w T ,P ,P c ,m v T +T ,P +P ,P +P c (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where σ is the assigned activation function, b m is the deviation, w T ,P ,P c ,m represents the weight of the convolution kernel connected by the c -th stacking channel to the feature representation of the position (T, P, P ), and v T +T ,P +P ,P +P c represents the characteristic value of the c -th stacking channel at the position (T, P, P ). To better capture the spatial-temporal topological structure in DFCNs, inspired by ResNet's remarkable success <ref type="bibr" target="#b3">[4]</ref>, we build a deeper network by stacking multiple residual blocks. A spatial-temporal feature extraction network (STFENet) is designed to extract spatial-temporal features of the DFCNs. The construction of STFENet is shown in the second half of Fig. <ref type="figure" target="#fig_0">1</ref>. A residual block is used as the basic block, which includes two 3D convolutions, two activation functions and a residual connection. 3D Maxpooling is adopted between the multiple stacked residual block.</p><p>Since the operation of convolution will eventually focus on local areas, longrange dependencies which describe luxuriant emotion-related information will be lost to some extent. To solve this problem, we further introduce non-local block <ref type="bibr" target="#b17">[18]</ref> to preserve information after the maxpooling layer. For a given input, non-local attention performs two different transformations:</p><formula xml:id="formula_12">θ (x i ) = W θ x i , φ (x i ) = W φ x i (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>where W θ and W φ is the weight to be learned, which is realized by 3D convolution in this paper. Then, non-local attention uses the self-attention term <ref type="bibr" target="#b16">[17]</ref> to calculate the final features with the help of softmax:</p><formula xml:id="formula_14">y = softmax x T W T θ W φ x g(x) (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>where g is implemented by 1×1×1 convolution in this paper. Then, the non-local block can be defined as:</p><formula xml:id="formula_16">z = W z y + x (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>where "+x" denotes the residual connection, and W z represents the weight matrix. By the STFENet, we finally effectively extract the spatial-temporal emotion-related information in prior-driven DFCNs for the identification of emotions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment Results</head><p>Emotional Database: The DEAP dataset <ref type="bibr" target="#b8">[9]</ref> collected EEG data from 32 healthy participants. The volunteers were asked to watch 40 one-minute videos and collect EEG signals from the subjects. The facial states of the first 22 subjects were recorded simultaneously. All participants rated each video on a 1-9 scale with the indicators, i.e. arousal, valence, dominance. We choose 18 subjects with both EEG signals and a complete facial video for the experiment. Same as many state-of-the-art studies <ref type="bibr" target="#b1">[2]</ref>, we turn the identification task into binary classification problem by setting the evaluation threshold of 5.</p><p>Data Pre-processing: For the EEG data, The 32-channel EEG signal with a duration of 63 s is down-sampled to 128HZ, and remove the first 3 s pre-trial baseline. Power spectral density (PSD) features is extracted from 3 s time windows with non-overlap through the Welch method in EEG, and 5 frequency bands are adopted, i.e. theta (4-8 Hz), slow alpha (8-10 Hz), alpha (8-12 Hz), beta (12-30 Hz), and gamma (30+ Hz) <ref type="bibr" target="#b8">[9]</ref>. For the facial video data, referring to <ref type="bibr" target="#b11">[12]</ref>, we utilize OPENFACE to extract expression features from facial videos, including 3 face positions relative to the camera, 3 head position, 6 eye gaze directions and 17 facial action units. Similar to EEG, the face sequences are divided according to the 3-second non-overlapping sliding time window and the average value of each feature is taken.</p><p>Experiment Settings: In our experiment, we adopt the leave one subject out (LOSO) cross-validation strategy to verify the effectiveness of our method. Specifically, the samples are divided into 18 non-overlapping parts according to the subjects. The samples of one subject are selected as the test set while the remaining subjects are selected as the training set for each cross-validation. This process is repeated 18 times, and the average performance of the crossvalidation is taken as the final result. Identification performance is measured by accuracy (ACC) and F1-Score. The proposed method is based on the Pytorch implementation, and the model mentioned in this study is trained on a single GPU (NVIDIA GeForce RTX3080). Adam algorithm is used to optimize this method, and the learning rate and batch size are set to 0.001 and 40, respectively.</p><p>Results and Discussion: We evaluate the performance of our method by calculating ACC and F1 on both valence and arousal. We also compare our method with many comparison methods, which can be divided into two categories: EEG based methods and multi-modal based methods. More specifically, EEG based methods are Support vector machine (SVM), GraphSleepNet (GSN) <ref type="bibr" target="#b6">[7]</ref>, Dynamical Graph-CNN (DGCNN) <ref type="bibr" target="#b14">[15]</ref>. Multi-modal based methods include Multi-kernel learning (MKL), Deep-CCA (DCCA), MMResLSTM <ref type="bibr" target="#b9">[10]</ref>. Emotion transformer fusion (ETF) <ref type="bibr" target="#b19">[20]</ref>. For quantitative results in Table <ref type="table" target="#tab_1">1</ref>, firstly, most of the multi-modal based methods achieve higher performance than EEGbased methods, which shows the advantage of complementary information from multiple modalities. Secondly, our proposed method achieve the best emotion recognition performance. On valence and arousal, the average ACC and F1 of our method reached 67.36%, 69.17% and 68.47% and 74.68% respectively. The main reason for the superiority of our method is that we can not only use multimodal data as prior knowledge to guide the construction of DFCNs, but also extract discriminate spatial-temporal features. To evaluate the effectiveness of the different modules of our framework, we conduct several ablation experiments on DEAP dataset. Our method mainly contains two modules, Prior knowledge embedding (PKE) module and STFENet. Besides, we also evaluate the contribution of non-local block in STFENet. As can be seen in Table <ref type="table" target="#tab_2">2</ref>, every module used in our framework greatly improve the performance compared with baseline model, with an increase of 6.25%, 4.36% and 7.56%, 9.58% for ACC and F1 on valence and arousal, respectively. It can be seen that both non-local block and STFENet demonstrate the better performance of our proposed method. The reason lies in that STFENet is able to extract complex spatial-temporal feature, and non-local block of STFENet helps it preserve the long-range dependencies in the time series. Moreover, when we remove PKE module from our method, there comes a performance degradation. It suggested that the prior knowledge has vital guiding significance for the construction of DFCNs, so that it can better express emotion-related information. In addition, to further verify the feasibility of the prior knowledge embedded in our method, we visualize the facial expression of subject over several time windows and its channel correlation with EEG, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Firstly, from the channel correlation topographic map, the deeper the red of the brain area, the higher the correlation between EEG and facial expression. Conversely, the deeper the blue, the lower the correlation. On valence and arousal, high correlation areas focus on the binaural and prefrontal regions, which is in line with existing medical cognition <ref type="bibr" target="#b15">[16]</ref>. As the stimulation method adopted by DEAP dataset is musical stimulation, the binaural region is highly activated. The prefrontal lobe plays a crucial role in emotional mobilization <ref type="bibr" target="#b1">[2]</ref>. The experimental results show that our method can mine electrode channels that are highly correlated with emotion to provide prior knowledge to guide the construction of dynamic brain networks. Combined with the experimental results after removing PKE of our method in Table <ref type="table" target="#tab_2">2</ref>, it can be seen that embedding prior knowledge can achieve better emotion recognition performance. Therefore, the prior knowledge can better describe emotion-related information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we develop a spatial-temporal feature extraction framework based on prior-driven DFCNs for multi-modal emotion recognition. In our approach, not only the connectivity between EEG channels but also the dynamics of connectivity over time are jointly learned. Besides, we also calculate the correlation across modalities via cross attention to guide the construction of DFCNs. In addition, we build STFENet based on 3D convolution to model the spatial-temporal features contained in DFCNs to extract emotion-related spatial-temporal information and preserve the long-range dependencies in the time series. Experimental results show that our method outperforms the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework of our proposed multi-modal emotion recognition method.</figDesc><graphic coords="2,41,79,361,34,340,21,108,97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of facial expression and its channel correlation with EEG in different time windows.</figDesc><graphic coords="8,44,31,196,61,335,59,149,26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>×D represents the EEG and facial expression modality respectively, where T represents the number of time Windows, P E and P F represents the number of channels, and D represents the feature dimension. We perform different transformations by linear mapping pairs x E and x F :</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of different methods on DEAP dataset (Mean/Std%).</figDesc><table><row><cell></cell><cell>method</cell><cell>Valence</cell><cell></cell><cell>Arousal</cell></row><row><cell></cell><cell></cell><cell>ACC</cell><cell>F1</cell><cell>ACC</cell><cell>F1</cell></row><row><cell>EEG based methods</cell><cell>SVM</cell><cell cols="3">58.88/11.47 67.81/13.28 58.05/15.26 66.71/20.54</cell></row><row><cell></cell><cell>GSN</cell><cell cols="3">63.77/07.25 66.19/12.20 63.51/11.36 68.03/15.40</cell></row><row><cell></cell><cell>DGCNN</cell><cell cols="3">63.28/08.15 65.17/10.22 62.61/12.93 69.22/15.17</cell></row><row><cell cols="2">Multi-modal based methods MKL</cell><cell cols="3">59.02/12.94 67.37/16.97 58.96/13.89 64.78/18.69</cell></row><row><cell></cell><cell>DCCA</cell><cell cols="3">61.04/11.73 65.46/12.29 59.58/11.89 65.78/16.98</cell></row><row><cell></cell><cell cols="4">MMResLSTM 64.67/10.57 68.36/11.50 63.25/12.38 67.32/15.92</cell></row><row><cell></cell><cell>ETF</cell><cell cols="3">64.63/09.35 66.35/13.41 65.64/10.29 66.63/16.97</cell></row><row><cell></cell><cell>Ours</cell><cell cols="3">67.36/05.58 69.17/09.01 68.47/08.45 74.68/13.36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of our proposed method (Mean/Std%).</figDesc><table><row><cell>method</cell><cell>Valence</cell><cell></cell><cell>Arousal</cell></row><row><cell></cell><cell>ACC</cell><cell>F1</cell><cell>ACC</cell><cell>F1</cell></row><row><cell>Baseline</cell><cell cols="4">61.11/11.35 64.81/13.40 60.91/12.45 65.10/18.99</cell></row><row><cell>w/o PKE</cell><cell cols="4">64.15/09.73 67.19/12.48 65.58/10.18 68.12/16.10</cell></row><row><cell cols="5">w/o NL block 66.12/08.60 66.92/10.36 66.24/09.28 72.31/14.63</cell></row><row><cell cols="5">w/o STFENet 65.51/08.19 67.18/11.29 67.10/10.15 70.83/15.84</cell></row><row><cell>Ours</cell><cell cols="4">67.36/05.58 69.17/09.01 68.47/08.45 74.68/13.36</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (Nos. <rs type="grantNumber">62076129</rs>, <rs type="grantNumber">62136004</rs>, and <rs type="grantNumber">62276130</rs>), the <rs type="funder">Key Research and Development Plan of Jiangsu Province</rs> (No. <rs type="grantNumber">BE2022842</rs>), and the <rs type="funder">Fundamental Research Funds for the Central Universities</rs> (No. <rs type="grantNumber">NS2023051</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_skYyR4Z">
					<idno type="grant-number">62076129</idno>
				</org>
				<org type="funding" xml:id="_hWhaXTg">
					<idno type="grant-number">62136004</idno>
				</org>
				<org type="funding" xml:id="_WjsSdJj">
					<idno type="grant-number">62276130</idno>
				</org>
				<org type="funding" xml:id="_gmNykRD">
					<idno type="grant-number">BE2022842</idno>
				</org>
				<org type="funding" xml:id="_wytRwDb">
					<idno type="grant-number">NS2023051</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">EEG-based emotion recognition using multiple kernel learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="472" to="484" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An efficient LSTM network for emotion recognition from multichannel EEG signals</title>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1528" to="1540" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep spatial-temporal 3D convolutional neural networks for traffic data forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3913" to="3926" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Why ResNet works? Residuals generalize</title>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5349" to="5362" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-modal emotion analysis from facial expressions and electroencephalogram</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="114" to="124" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph emotion decoding from visually evoked neural responses</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-1_38" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="396" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">GraphSleepNet: adaptive spatial-temporal graph convolutional networks for sleep stage classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1324" to="1330" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Brain connectivity hyper-network for MCI classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10470-6_90</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10470-6_90" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2014</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Hata</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Barillot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Howe</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8674</biblScope>
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DEAP: a database for emotion analysis; using physiological signals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emotion recognition using multimodal residual LSTM network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="176" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Specialized staff for the care of people with Parkinson&apos;s disease in Germany: an overview</title>
		<author>
			<persName><forename type="first">T</forename><surname>Prell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Med</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2581</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal gated information fusion for emotion recognition from EEG signals and facial behaviors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rayatdoost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rudrauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimodal Interaction</title>
		<meeting>the 2020 International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="655" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Impact of affective multimedia content on the electroencephalogram and facial expressions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16295</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition in response to videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="223" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">EEG emotion recognition using dynamical graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="532" to="541" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal affective state assessment using fNIRS+ EEG and spontaneous facial expression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ayaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Akansu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d auto-context-based locality adaptive multi-modality GANs for pet synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1328" to="1339" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Emotion transformer fusion: complementary representation properties of EEG and eye movements on recognizing anger and surprise</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1575" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">EEG emotion recognition using multichannel weighted multiscale permutation entropy</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="12064" to="12076" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unified brain network with functional and structural data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59728-3_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59728-3_12" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12267</biblScope>
			<biblScope unit="page" from="114" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">EEG-based emotion recognition with emotion localization via hierarchical self-attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
