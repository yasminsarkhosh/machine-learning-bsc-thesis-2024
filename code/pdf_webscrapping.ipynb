{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasminsarkhosh/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# pdf webscrapping and text extraction\n",
    "\n",
    "# get the pdf files or url from the web\n",
    "import requests\n",
    "\n",
    "# for tree traversal scraping in webpage\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# input output operations\n",
    "import io\n",
    "\n",
    "# getting information from the pdfs\n",
    "#from PyPDF2 import PdfReader\n",
    "\n",
    "#!pip install xhtml2pdf requests\n",
    "#!pip install lxml\n",
    "\n",
    "# for converting html to pdf\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from urllib import request as urllib2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Extraction: by url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Front page with 730 links, where each link directs you to another page with a specific research paper.\n",
    "Code below extracts text from the second webpage. However, it is not able to extract text from the pdf that are linked in the second page. There are 2 different links for downloading the same pdf. \n",
    "- DOI and \n",
    "- ShareIt\n",
    "\n",
    "\n",
    "```<p>\n",
    "DOI: \n",
    "<a href=\"https://doi.org/10.1007/978-3-031-43990-2_43\">https://doi.org/10.1007/978-3-031-43990-2_43</a>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p>\n",
    "SharedIt: \n",
    "<a href=\"https://rdcu.be/dnwLY\">https://rdcu.be/dnwLY</a>\n",
    "<br><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import regex as re\n",
    "from urllib.request import urlopen\n",
    "from urllib import request as urllib2\n",
    "\n",
    "# specify the URL of the archive here \n",
    "archive_url = \"http://conferences.miccai.org/2023/papers/\"\n",
    "url = \"http://conferences.miccai.org\"\n",
    "\n",
    "\n",
    "list_of_pdfs = []\n",
    "# create response object \n",
    "r = requests.get(archive_url) \n",
    "\n",
    "# create beautiful-soup object \n",
    "soup = BeautifulSoup(r.content,'html')\n",
    "\n",
    "# find all links on web-page \n",
    "links = soup.findAll('a') \n",
    "\n",
    "# filter links ending with .html \n",
    "list_of_urls = [url + link['href'] for link in links if link['href'].endswith('html')] \n",
    "# print(len(paper_links)) gives us 730 papers in total\n",
    "\n",
    "\n",
    "# now that we have all the links to the papers, we can go through each paper and find the pdf link\n",
    "for link in list_of_urls:\n",
    "\t# create response object per link from the total list of links\n",
    "\tr = requests.get(link) \n",
    "\tsoup = BeautifulSoup(r.content,'html')\n",
    "\n",
    "\t# find all urls with 'rdcu.be' which contains the specific paper\n",
    "\tlinks = soup.findAll(attrs={'href': re.compile(\"rdcu.be\")})\n",
    "\tpdfs = [link['href'] for link in links]\t\n",
    "\tlist_of_pdfs.extend(pdfs)\n",
    "\n",
    "print(len(list_of_pdfs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in list_of_pdfs:\n",
    "\t# create response object per link from the total list of links\n",
    "\tr = requests.get(link) \n",
    "\tsoup = BeautifulSoup(r.content,'html')\n",
    "\t# find all urls with 'rdcu.be' which contains the specific paper\n",
    "\tlinks = soup.findAll(attrs={'href': re.compile(\"/content/pdf\")})\n",
    "\tfinal_list = [link['content'] for link in links]\t\n",
    "\n",
    "print(len(final_list))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf in list_of_pdfs:\n",
    "    filename = pdf.split(\"/\")[-1]\n",
    "    res = requests.get(pdf)\n",
    "    pdf = open(\"pdfs/\" + filename, 'wb')\n",
    "    pdf.write(res.content)\n",
    "    pdf.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting papers into dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "part1 = open('/Users/yasminsarkhosh/Documents/micca2023part2.doc', encoding = \"ISO-8859-1\")\n",
    "soup = BeautifulSoup(part1, 'html.parser')\n",
    "#parsing the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_doi(href):\n",
    "    return href and re.compile(\"chapter/\").search(href)\n",
    "\n",
    "list_of_doi = soup.find_all(href=has_doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the titles and the doi's from above function\n",
    "\n",
    "titles = []\n",
    "doi_str = []\n",
    "\n",
    "for element in list_of_doi:\n",
    "    titles.append(element.get_text())\n",
    "    string = str(element)\n",
    "    first_substring = '/chapter'\n",
    "    second_substring ='\">'\n",
    "    doi_str.append(string[(string.find(first_substring)):string.find(second_substring)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now want to find authors - YES!\n",
    "authors = []\n",
    "\n",
    "authors = soup.find_all(\"li\", class_=\"c-author-list__item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping only the author names\n",
    "authors_str = []\n",
    "for element in authors:\n",
    "    string = str(element)\n",
    "    first_substring = 'item\">'\n",
    "    second_substring ='</li>'\n",
    "    authors_str.append(string[(string.find(first_substring)+6):string.find(second_substring)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next up is page numbers\n",
    "#line to find:\n",
    "#<div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 1-9</span>\n",
    "\n",
    "page_numbers = []\n",
    "\n",
    "page_numbers= soup.find_all('div', class_ = \"c-meta\")\n",
    "#page_numbers = page_numbers[3:] #removing the first two pages that are not relevant/chapters\n",
    "page_numbers = page_numbers[3:] #removing the first two pages that are not relevant/chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(page_numbers))\n",
    "print(len(authors_str))\n",
    "print(len(titles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <h3 class=\"c-card__title\" data-test=\"back-matter\">Back Matter</h3>\n",
    "            \n",
    "           # <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 623-626</span>\n",
    "back_matter = []\n",
    "back_matter = soup.find_all(\">Back Matter<\")\n",
    "\n",
    "back_matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strips it back to simply the pages\n",
    "page_numbers_str = []\n",
    "\n",
    "for element in page_numbers:\n",
    "    string = str(element)\n",
    "    first_substring = 'Pages'\n",
    "    second_substring ='</span>'\n",
    "    string = string[(string.find(first_substring)):string.find(second_substring)]\n",
    "    if len(string) > 0: #ensures I don't add an empty string to the list\n",
    "        page_numbers_str.append(string)\n",
    "\n",
    "page_numbers_str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a check that I've found the same amount of authors, titles, dois and page numbers\n",
    "if len(page_numbers_str) == len(authors_str) == len(titles) == len(doi_str):\n",
    "    print(\"true\")\n",
    "else:\n",
    "    print(\"You have an error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now want to combine everything in a pandas dataframe\n",
    "\n",
    "import pandas as pd\n",
    "#need to add the year (probably a better way to do this, but should be fine)\n",
    "year_of_pub = []\n",
    "for element in titles:\n",
    "    year_of_pub.append(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Title': titles,\n",
    "        'Authors': authors_str,\n",
    "        'Page numbers' : page_numbers_str,\n",
    "        'DOI': doi_str,\n",
    "        'Year of publication' : year_of_pub\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv('miccai_2023_part2.csv')\n",
    "#not sure why it seems to put \"\" around only the authors in the csv file, but good enough to start! \n",
    "#Now want to combine this into a more pretty script that I can then feed all the pages into it, will need to do some \n",
    "#combining at the end, adding everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main mining function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to find lines in html document with DOI and titles of articles\n",
    "def has_doi(href):\n",
    "    return href and re.compile(\"chapter/\").search(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main mining function returning the initial dataframe \n",
    "\n",
    "def mining(html_doc, year, current_page, all_pages, part):\n",
    "    #opening the html document (copy pasted and saved as a .doc file)\n",
    "    doc = open(html_doc, \"r\", encoding = \"ISO-8859-1\") \n",
    "    soup = BeautifulSoup(doc, 'html.parser' )\n",
    "\n",
    "    list_of_doi = soup.find_all(href=has_doi)\n",
    "        \n",
    "    #getting the titles and the doi's from list generated helper function\n",
    "\n",
    "    titles = []\n",
    "    doi_str = []\n",
    "\n",
    "    for element in list_of_doi:\n",
    "        titles.append(element.get_text()) #returns the titles as the only text in the list\n",
    "        string = str(element)\n",
    "        first_substring = '/chapter'\n",
    "        second_substring ='\">'\n",
    "        #separates out the DOIS (added the +9 to remove /chapter/ from the beginning of all DOIS)\n",
    "        doi_str.append(string[(string.find(first_substring)+9):string.find(second_substring)]) \n",
    "            \n",
    "    ## now the lines containing author are found\n",
    "    authors = soup.find_all(\"li\", class_=\"c-author-list__item\")\n",
    "        \n",
    "    #keeping only the author names\n",
    "    authors_str = []\n",
    "    for element in authors:\n",
    "        string = str(element)\n",
    "        first_substring = 'item\">'\n",
    "        second_substring ='</li>'\n",
    "        authors_str.append(string[(string.find(first_substring)+6):string.find(second_substring)])\n",
    "            \n",
    "    #now the lines containing page numbers are found\n",
    "    page_numbers= soup.find_all('div', class_ = \"c-meta\")\n",
    "        \n",
    "    #keeping only the page numbers\n",
    "    page_numbers_str = []\n",
    "\n",
    "    for element in page_numbers:\n",
    "        string = element.get_text()[6:-1] #removes the white spaces and \"Pages \"\n",
    "        both = string.split(\"-\")\n",
    "        if 'x' not in string: #filtering out front matters\n",
    "            try: \n",
    "                if int(both[1])-int(both[0]) > 1: \n",
    "                    page_numbers_str.append(string)\n",
    "            except:\n",
    "                if \"C\" in string or \"E\" in string: #including corrections and erratum, are removed later\n",
    "                    page_numbers_str.append(string)\n",
    "    #filtering out back matters, only an issue in 2021 \n",
    "    if year == 2023 and int(current_page) == int(all_pages):\n",
    "        page_numbers_str = page_numbers_str[:-1]\n",
    "            \n",
    "    #need to create a list of the year of publication to add to dataframe \n",
    "    year_of_pub = []\n",
    "    for element in titles:\n",
    "        year_of_pub.append(year)\n",
    "    \n",
    "    #will add the part of the publication to the dataframe as well\n",
    "    part_of_pub = []\n",
    "    for element in titles:\n",
    "        part_of_pub.append(part)\n",
    "        \n",
    "    \n",
    "    #creating the column names and content for the dataframe        \n",
    "    data = {'Title': titles,\n",
    "        'Authors': authors_str,\n",
    "        'Page numbers' : page_numbers_str,\n",
    "        'DOI': doi_str,\n",
    "        'Year of publication' : year_of_pub,\n",
    "        'Part of publication' : part_of_pub       }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to combine all df together\n",
    "def data_together(data, year):\n",
    "    combined_frame = pd.concat(data, ignore_index = True, sort = False)\n",
    "    if year == 2023:\n",
    "        combined_frame.drop(combined_frame[combined_frame[\"Title\"].str.contains(\"Correction to\")].index, inplace=True)\n",
    "    combined_frame.to_csv('database_miccai_'+ str(year) +'.csv')\n",
    "   \n",
    "    return combined_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 1\n",
      "2 4 1\n",
      "3 4 1\n",
      "4 4 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m miccai:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(element[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m], element[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m], element[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m17\u001b[39m])  \n\u001b[0;32m---> 15\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmining\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m data_together(data, \u001b[38;5;241m2023\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 73\u001b[0m, in \u001b[0;36mmining\u001b[0;34m(html_doc, year, current_page, all_pages, part)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#creating the column names and content for the dataframe        \u001b[39;00m\n\u001b[1;32m     66\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m: titles,\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthors\u001b[39m\u001b[38;5;124m'\u001b[39m: authors_str,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPage numbers\u001b[39m\u001b[38;5;124m'\u001b[39m : page_numbers_str,\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOI\u001b[39m\u001b[38;5;124m'\u001b[39m: doi_str,\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear of publication\u001b[39m\u001b[38;5;124m'\u001b[39m : year_of_pub,\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPart of publication\u001b[39m\u001b[38;5;124m'\u001b[39m : part_of_pub       }\n\u001b[0;32m---> 73\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    727\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    728\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    729\u001b[0m     )\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "#saving all 2012 together as one with the helper function above\n",
    "miccai =['/Users/yasminsarkhosh/Documents/miccai2023 papers/miccai2023part1 page 1 of 4.doc',\n",
    "         '/Users/yasminsarkhosh/Documents/miccai2023 papers/miccai2023part1 page 2 of 4.doc',\n",
    "         '/Users/yasminsarkhosh/Documents/miccai2023 papers/miccai2023part1 page 3 of 4.doc',\n",
    "         '/Users/yasminsarkhosh/Documents/miccai2023 papers/miccai2023part1 page 4 of 4.doc',\n",
    "\n",
    "         '/Users/yasminsarkhosh/Documents/miccai2023 papers/miccai2023part2 page 1 of 4.doc',\n",
    "         '/Users/yasminsarkhosh/Documents/miccai2023 papers/miccai2023part2 page 2 of 4.doc',\n",
    "         '/Users/yasminsarkhosh/Documents/miccai2023 papers/miccai2023part2 page 3 of 4.doc']\n",
    "\n",
    "\n",
    "data = []\n",
    "for element in miccai:\n",
    "    print(element[-10], element[-5], element[-17])  \n",
    "    data.append(mining(element, 2023, element[-10], element[-5], element[-17]))\n",
    "\n",
    "data_together(data, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mining() missing 2 required positional arguments: 'all_pages' and 'part'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m miccai:\n\u001b[0;32m----> 5\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmining\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m data_together(data, \u001b[38;5;241m2023\u001b[39m) \n",
      "\u001b[0;31mTypeError\u001b[0m: mining() missing 2 required positional arguments: 'all_pages' and 'part'"
     ]
    }
   ],
   "source": [
    "miccai =['/Users/yasminsarkhosh/Documents/micca2023part2.doc']\n",
    "\n",
    "data = []\n",
    "for element in miccai:\n",
    "    data.append(mining(element, 2023, element[-5]))\n",
    "\n",
    "\n",
    "data_together(data, 2023) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  '/Users/yasminsarkhosh/Documents/micca2023part3.doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
