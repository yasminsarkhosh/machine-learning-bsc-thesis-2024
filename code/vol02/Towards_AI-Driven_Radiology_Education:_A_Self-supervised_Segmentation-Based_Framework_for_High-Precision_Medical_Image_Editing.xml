<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Kazuma</forename><surname>Kobayashi</surname></persName>
							<email>kazumkob@ncc.go.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Cancer Center Research Institute</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><surname>Gu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryuichiro</forename><surname>Hataya</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">RIKEN Information R&amp;D and Strategy Headquarters</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mototaka</forename><surname>Miyake</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">National Cancer Center Hospital</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yasuyuki</forename><surname>Takamizawa</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">National Cancer Center Hospital</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sono</forename><surname>Ito</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">National Cancer Center Hospital</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hirokazu</forename><surname>Watanabe</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">National Cancer Center Hospital</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yukihiro</forename><surname>Yoshida</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">National Cancer Center Hospital</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hiroki</forename><surname>Yoshimura</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Hiroshima University School of Medicine</orgName>
								<address>
									<settlement>Hiroshima</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryuji</forename><surname>Hamamoto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Cancer Center Research Institute</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="403" to="413"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">FAF108F07E5683A5D6CE503ABB4B6D0D</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_38</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image editing</term>
					<term>Self-supervised segmentation</term>
					<term>Education</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical education is essential for providing the best patient care in medicine, but creating educational materials using real-world data poses many challenges. For example, the diagnosis and treatment of a disease can be affected by small but significant differences in medical images; however, collecting images to highlight such differences is often costly. Therefore, medical image editing, which allows users to create their intended disease characteristics, can be useful for education. However, existing image-editing methods typically require manually annotated labels, which are labor-intensive and often challenging to represent fine-grained anatomical elements precisely. Herein, we present a novel algorithm for editing anatomical elements using segmentation labels acquired through self-supervised learning. Our self-supervised segmentation achieves pixel-wise clustering under the constraint of invariance to photometric and geometric transformations, which are assumed not to change the clinical interpretation of anatomical elements. The user then edits the segmentation map to produce a medical image with the intended detailed findings. Evaluation by five expert physicians demonstrated that the edited images appeared natural as medical images and that the disease characteristics were accurately reproduced.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite the success of artificial intelligence (AI) in aiding diagnosis, its application to medical education remains limited. Trainee physicians require several years of experience with a diverse range of clinical cases to develop sufficient skills and expertise. However, designing educational materials solely based on real-world data poses several challenges. For example, although small but significant disease characteristics (e.g., depth of cancer invasion) can sometimes alter diagnosis and treatment, collecting pairs with and without these characteristics is cumbersome. Another major challenge is longitudinal tracking of pathological progression over time (e.g., from the early stage of cancer to the advanced stage), which is difficult to understand because medical images are often snapshots. Privacy is also a concern since images of educational materials are widely distributed. Therefore, medical image editing that allows users to generate their intended disease characteristics is useful for precise medical education <ref type="bibr">[3]</ref>.</p><p>Image editing can synthesize low-or high-level image contents <ref type="bibr">[11]</ref>. Our goal is to develop high-precision medical image editing according to the fine-grained characteristics of individual diseases, rather than at the level of disease categories. For example, even if two diseases belong to the same disease category of "lung tumor," the impression of benign or malignant will differ depending on fine-grained characteristics, such as whether the margins are "smooth" or "spiculated." In this case, our approach is to edit the tumor margins to be smooth or spiculated. These fine-grained characteristics consist of low-to mid-level image features to distinguish the substructures of organs and diseases, which we call anatomical elements.</p><p>Several types of image editing techniques for medical imaging have been introduced, mainly using generative adversarial networks [5] and, more recently, diffusion models <ref type="bibr" target="#b1">[2]</ref>. Nevertheless, editing specific anatomical elements remains a challenge <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">11]</ref>. Latent space manipulation generates images by controlling latent feature axes [4,14], but the editable attributes are often global rather than fine-grained. Conditional generation can precisely edit image content by using class or segmentation labels. However, it requires manually provided labels [15] or virtual models <ref type="bibr">[18]</ref>, which are labor-intensive. Additionally, accurately modeling certain fine-grained characteristics, such as the textual variations of disease, can be a daunting task. Image interpolation [17] requires actual images with targeted content, which limits its applicability.</p><p>Here, we propose a novel framework for image editing called U3-Net that allows the generation of anatomical elements with precise conditions. The core technique is self-supervised segmentation, which aims to achieve pixel-wise clustering without manually annotated labels <ref type="bibr">[6,</ref><ref type="bibr">7]</ref>. As shown in Fig. <ref type="figure" target="#fig_0">1a,</ref><ref type="figure" target="#fig_3">U3</ref>-Net converts an input image into a segmentation map corresponding to the anatomical elements. Once the user has completed editing, U3-Net synthesizes an image in which the targeted anatomical element has been modified. As a result, our synthesized medical images can highlight hypothetical pathological changes and significant clinical differences in a single image. For example, Fig. <ref type="figure" target="#fig_0">1b</ref> shows that whether or not rectal cancer invades the muscularis propria (i.e., b-2 vs. b-3) affects cancer staging (i.e., T1 vs. T2) as well as treatment strategy (i.e., endoscopic resection vs. surgery). The distinction between mucinous and nonmucinous rectal cancers (see Fig. <ref type="figure" target="#fig_0">1c</ref>) is also important to estimate the better or worse prognosis of the disease. These synthetic images can help trainees intuitively comprehend clinically significant findings and alleviate privacy concerns. Five expert physicians evaluated the edited images from a clinical perspective using two datasets: a pelvic MRI dataset and chest CT dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions: Our contributions are as follows:</head><p>-We propose a novel image-editing algorithm, U3-Net, to synthesize images for medical education via self-supervised segmentation. -U3-Net can faithfully synthesize intended anatomical elements according to the editing operation on the segmentation labels. -Evaluation by five expert physicians showed that the edited images were natural as medical images with the intended features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>U3-Net consists of three neural networks: encoder, decoder, and discriminator (see Fig. <ref type="figure" target="#fig_1">2</ref>). The encoder achieves self-supervised segmentation with a feature extraction (FE) module and a pixel-wise clustering (CL) module. We perform pixel-wise clustering under the constraint of invariance to photometric and geometric transformations [6], with the assumption that these transformations should not change the clinical interpretation of the anatomical elements. Given a pair of differently transformed images, the FE module produces embedding maps corresponding to the input images. The CL module then performs Kmeans clustering on the embedding maps to produce two interchangeable outputs: segmentation maps and corresponding quantized embedding maps. These outputs are trained to be consistent between the two views. The decoder then estimates the corresponding images from the quantized embedding maps, while the discriminator forces the decoder to produce more realistic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">First Training Stage for Self-supervised Segmentation</head><p>The training process for U3-Net is two-stage. First, we train the encoder and decoder (excluding the discriminator) to conduct K-class self-supervised segmentation. To achieve pixel-wise clustering that is consistent between two transformed views of the input images, we introduce four constraints: intra-cluster pull force, inter-cluster push force, cross-view consistency, and reconstruction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Image Transformation:</head><p>We consider a sequence of image transformations [t 1 , . . . , t n ] specified by the type (e.g., image rotation) and magnitude (e.g., degree of rotation) of each transformation:  Cluster Assignment and Update: In the CL module, K-means clustering in the first iteration initializes K mean vectors µ k ∈ R D . Then, the embedding vector of the i-th pixel e i∈{1,...,H×W } ∈ R D in the embedding maps, E 1 and E 2 , is assigned to the cluster with the nearest mean vector as follows:</p><formula xml:id="formula_0">T = t n • t n-1 • • • • • t 1 .</formula><formula xml:id="formula_1">y i = argmin k∈{1,...,K} µ k -e i 2</formula><p>, where y i is the cluster index of the i-th pixel. By replacing embedding vectors with their respective mean vectors, quantized embedding maps, E q1 and E q2 , are generated g(E) = E q = [µ y1 , . . . , µ yH×W ] ∈ R D×H×W . The cluster indices form the segmentation maps S = [y 1 , . . . , y H×W ] ∈ R H×W , S 1 and S 2 . The mean vectors µ k are updated by using the exponential moving average [9].</p><p>Intra-cluster Pull Force: For transformation-invariant pixel-wise clustering, we define four loss terms. The first term, cluster loss, forces the embedding vectors to adhere to the associated mean vector (see Fig. <ref type="figure" target="#fig_3">3</ref>), as defined:</p><formula xml:id="formula_2">L cluster = i∈H×W µ yi -e i 2 .</formula><p>Inter-cluster Push Force: The second term, distance loss, pushes the distance between the mean vectors above a margin parameter m (see Fig. <ref type="figure" target="#fig_3">3</ref>), as defined:</p><formula xml:id="formula_3">L dist = 1 K(K-1) K kA=1 K kB =1,kB =kA [2m -µ kA -µ kB ] 2 + ,</formula><p>where k A and k B indicate two different cluster indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-view Consistency:</head><p>The segmentation maps from the different views, S 1 and S 2 , should overlap after re-transforming to align the coordinates. Such a re-transform is composed of inverse and forward geometric transformations:</p><formula xml:id="formula_4">T 2 (T † 1 (S 1 )) = S 1 and T 1 (T † 2 (S 2 )) = S 2 .</formula><p>The inverse transformations of the photometric transformations are not considered. Using the re-transformed segmentation maps, we impose a third term, cross-view consistency loss, which forces the embedding vectors of one view to match the mean vector of the other (see Fig. <ref type="figure" target="#fig_3">3</ref>), as defined:</p><formula xml:id="formula_5">L cross = i∈H×W µ yi2 -e i1 2 + i∈H×W µ yi1 -e i2 2 .</formula><p>Reconstruction Loss: Without user editing, the decoder reconstructs the input images from quantized embedding maps h(E q ) = R ∈ R C×H×W . We thus employ reconstruction loss, which minimizes the mean squared error between the reconstructed and input images.</p><p>Learning Objective: The weighted sum of the loss functions is set to be minimized:</p><formula xml:id="formula_6">L total = w cluster L cluster + w dist L dist + w cross L cross + w recon L recon .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Second Training Stage for Faithful Image Synthesis</head><p>In the second stage, we train the decoder and discriminator (excluding the encoder) to produce naturally appearing images from the quantized embedding maps. Learning Objective: We impose generator loss L gen for the decoder to produce more faithful images by deceiving the discriminator, and discriminator loss L dis for the discriminator to judge the real or fake of the images as the perpixel feedback <ref type="bibr">[16]</ref>. We also add cutmix augmentation L cutmix and consistency regularization L cons to the latter <ref type="bibr">[16]</ref>. In this stage, the decoder and discriminator are trained by alternately minimizing the following competing objectives:</p><formula xml:id="formula_7">L Dec = L app + w gen L gen and L Dis = w dis L dis + w cutmix L cutmix + w cons L cons .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference Stage for Medical Image Editing</head><p>After training, the encoder can output a segmentation map from a testing image.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1a</ref>, when a user edits the segmentation map S → S by changing the cluster indices y i → y i , the quantized embedding map is subsequently updated E q → E q by reassigning the mean vectors according to the edited indices µ yi → µ y i . Finally, the decoder converts the quantized embedding map into a synthetic image with the intended disease characteristics h(E q ) = R ∈ R C×H×W . SSIM, and PSNR were 1.41 × 10 -2 ± 1.04 × 10 -2 , 7.40 × 10 -1 ± 0.57 × 10 -1 , and 22.5 ± 2.7 in the pelvic MRI testing dataset and 5.03 × 10 -4 ± 3.03 × 10 -4 , 9.08 × 10 -1 ± 0.34 × 10 -1 , and 38.6 ± 1.7 in the chest CT testing dataset. Subsequently, segmentation maps from the testing images were edited to generate images with the intended characteristics (see Fig. <ref type="figure" target="#fig_4">4cd</ref>). Five expert physicians (two diagnostic radiologists, two colorectal surgeons, and one thoracic surgeon) assessed them from a clinical perspective. First, we tested whether the evaluators could identify real or synthesized images from 20 images, which include ten real images and ten synthesized images. The accuracies (i.e., the ratio of images correctly identified as real or synthetic) were 0.69 ± 0.11 and 0.65 ± 0.11, for the pelvic MRI and chest CT testing datasets, respectively. Note that when the synthetic images cannot be distinguished at all, the accuracy should be 0.5. Second, we presented image captions explaining the radiological features, which also represented the editing intention for the synthetic images. We asked the evaluators to rate each presented image from A to C. A: The image is natural as a medical image, and the caption is consistent with the image. B: The image is natural as a medical image, but the caption is NOT consistent with the image. C : The image is NOT natural as a medical image. This test was conducted after informing the evaluators of the assumption that all 20 images could be synthetic, without indicating which image was real or synthetic. As a result, the ratio of synthetic images (vs. that of real images) categorized as A, B, and C were 0.80 ± 0.15 (vs. 0.78 ± 0.20), 0.02 ± 0.04 (vs. 0.08 ± 0.07), and 0.18 ± 0.11 (vs. 0.14 ± 0.13) for the pelvic MRI testing dataset, and 0.74 ± 0.28 (vs. 0.76 ± 0.30), 0.08 ± 0.09 (vs. 0.12 ± 0.15), and 0.18 ± 0.21 (vs. 0.12 ± 0.14) for the chest CT testing dataset. There were no significant differences between real and synthetic images (t-test: p &gt; 0.05). Consequently, the majority of the edited images were natural-looking medical images with accurately reproduced disease features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we propose a medical image-editing framework to edit fine-grained anatomical elements. The self-supervised segmentation extracted low-to midlevel content of medical images, which corresponded well to the clinically meaningful substructures of organs and diseases. The majority of the edited images with intended characteristics were perceived as natural medical images by several expert physicians. Our medical image editing method can be applied to medical education, which has been overlooked as an application of AI. Future challenges include improving scalability with fewer manual operations, validating segmentation maps from a more objective perspective, and comparing our proposed algorithm with existing methods, such as those based on superpixels <ref type="bibr">[10]</ref>.</p><p>Data use declaration and acknowledgment: The pelvic MRI and chest CT datasets were collected from the National Cancer Center Hospital. The study, data use, and data protection procedures were approved by the Ethics Committee of the National Cancer Center, Tokyo, Japan (protocol number 2016-496).</p><p>Our implementation and all synthesized images will be available here: https:// github.com/Kaz-K/medical-image-editing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Editing of anatomical elements. (a) Users can edit the segmentation map obtained from an input image to express intended fine-grained disease characteristics. A spiculated lung nodule was generated. (b) Synthetic disease progression showing a normal-appearing rectum (b-1), a rectal tumor extending into the submucosal layer (b-2), and the tumor extending into the muscularis propria (b-3). (c) A synthetic rectal tumor (c-1) and the contrasting tumor with T2 hyperintensity of extracellular mucin suspicious for mucinous adenocarcinoma (c-2).</figDesc><graphic coords="2,73,80,99,17,276,70,131,14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Overall architecture of U3-Net. We apply two random transformations to the input image to produce images in different views, I1 and I2. The encoder converts the transformed images into quantized embedding as well as segmentation maps consisting of cluster indices, S1 and S2. Pixel-wise clustering, which should be consistent between views, is performed for the self-supervised segmentation. The decoder generates reconstructed images, R1 and R2, from the quantized embedding maps. The discriminator adversarially enhances the natural appearance by judging whether the images are real or fake on a pixel-by-pixel basis.</figDesc><graphic coords="4,56,31,54,08,311,41,111,61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Two random transformation sequences are applied to an input image I ∈ R C×H×W to produce two transformed images, T 1 (I) = I 1 and T 2 (I) = I 2 . The FE module of the encoder produces two embedding maps f (I) = E ∈ R D×H×W , E 1 and E 2 , which are then fed into the CL module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Transformation-invariant pixel-wise clustering. Suppose that the majority of pixels inside the black box in S1 are assigned to the kA-th cluster. The intracluster pull force causes the embedding vectors ea1, . . . e f 1 to adhere to the mean vector µ k A . From the other viewpoint, some of the same pixels, ea2, e b2 , and e f 2 , are assigned to the kB-th cluster, which can be assessed by re-transforming S2 into the coordination of S1. Cross-view consistency loss forces the embedding vectors of one view, ea2, . . . e f 2 , to match the mean vector of the other view µ k A . The inter-cluster push force maintains the distance between the mean vectors.</figDesc><graphic coords="5,70,47,53,84,311,17,105,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results of the image segmentation and editing. The segmentation maps were well aligned with the anatomical elements in both (a) the pelvic MRI and (b) the chest CT testing datasets. (c) A synthetic image generated by editing the testing image with the caption, "Axial T2-weighted MR image shows a tumor approximately 4 cm in size on the dorsal wall of the rectum. The deepest structure of the rectal wall was intact, indicating no infiltration beyond the muscularis propria." (d) A synthetic image with the caption, "Axial CT image showing a pulmonary nodule with a length of 2-3 cm and a cavity on the dorsal side of the right upper lobe of the lung."</figDesc><graphic coords="8,56,31,54,62,311,26,278,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The decoder, initially optimized in the first training stage, undergoes further training to enhance its image generation capabilities. We impose adversarial learning with an extended reconstruction loss term, called appearance loss. The training is performed only in a single view.</figDesc><table /><note><p>Appearance Loss: Appearance loss combines mean squared loss L mse , focal frequency loss L ffl [8], perceptual loss L lpips [19], and intermediate loss L int , as follows: L app = w mse L mse + w ffl L ffl + w lpips L lpips + w int L int , where intermediate loss L int refers to the L2 distance of the intermediate features of the discriminator between the reconstructed and input images.</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_38.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Implementation and Datasets: All neural networks were implemented in Python 3.8 using the PyTorch library 1.10.0 [12] on an NVIDIA Tesla A100 GPU running CUDA 10.2. The encoder, decoder, and discriminator were implemented based on U-Net <ref type="bibr">[13]</ref> (see Supplementary Information for details). The pelvic MRI dataset with rectal cancer contained 289 image series for training and 100 image series for testing. For each image series, the min-max normalization converted the pixel values to [-1, 1]. The chest CT dataset with lung cancer contained 500 image series for training and 100 image series for testing. The CT values in the range <ref type="bibr">[-2048, 2048]</ref> were normalized to [-1, 1]. Both were in-house datasets collected from a single hospital. Every image series comprises two-dimensional (2D) consecutive slices, and we applied our algorithm on a per 2D slice basis.</p><p>Self-supervised Medical Image Segmentation: We began by optimizing the hyperparameters to achieve self-supervised segmentation. Appropriate transformations were selected from six candidate functions: t 1 , Random HorizontalFlip, t 2 , RandomAffine, t 3 , ColorJitter, t 4 , RandomGaussianBlur, t 5 , RandomPosterize, t 6 , RandomGaussianNoise. Because anatomical elements, including the substructures of organs and diseases, are too detailed for human annotators to segment, it was difficult to create ground-truth labels. Therefore, the training configuration was selected based on the consensus of two expert radiologists with domain knowledge. By comparing different settings on the pelvic MRI training dataset (see Supplementary Information), the number of segmentation classes of 10, the combination of t 1 , t 2 , and t 3 with moderate magnitude, the weakly imposed reconstruction loss, and a certain value of the margin parameter were considered suitable for self-supervised segmentation. In particular, we found that reconstruction loss is essential for obtaining segmentation maps corresponding to anatomical elements, although such a loss term was not included in previous studies <ref type="bibr">[6,</ref><ref type="bibr">7]</ref>. A similar configuration was applied to the chest CT training dataset. The resultant segmentation maps are shown in Fig. <ref type="figure">4ab</ref>. The anatomical substructures, including the histological structure of the colorectal wall and subregions within the lung, corresponded well with the segmentation maps in both the pelvic MRI and chest CT testing datasets. Because our self-supervised segmentation extracts low-to mid-level image content, a semantic object (e.g., rectum or lung cancer) typically consists of multiple segmentation classes shared with other objects (see the magnified images in Fig. <ref type="figure">4ab</ref>). These anatomical elements may be too detailed for humans to annotate, demonstrating the necessity of self-supervised segmentation for highprecision medical-image editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of the Synthesized Images:</head><p>We measured the quality of image reconstruction using mean square error (MSE), structural similarity (SSIM), and peak signal-to-noise ratio (PSNR). The mean ± standard deviations of MSE,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial networks in medical image augmentation: a review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiomed.2022.105382</idno>
		<ptr target="https://doi.org/10.1016/j.compbiomed.2022.105382" />
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page">105382</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion Models Beat GANs on Image Synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
