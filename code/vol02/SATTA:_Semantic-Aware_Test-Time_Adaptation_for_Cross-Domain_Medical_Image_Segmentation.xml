<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation</title>
				<funder ref="#_gymtYe7">
					<orgName type="full">Hong Kong Innovation and Technology Fund</orgName>
				</funder>
				<funder ref="#_KSZktfe">
					<orgName type="full">Shenzhen Portion of Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone</orgName>
				</funder>
				<funder ref="#_4gTfSSJ">
					<orgName type="full">Research Grants Council of the Hong Kong Special Administrative Region, China</orgName>
				</funder>
				<funder ref="#_R9HEMNj #_dr65nUQ">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
							<email>zhangyuh@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Medical Intelligence and XR</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Research Institute</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Huang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Center for Advanced Medical Computing and Analysis</orgName>
								<orgName type="institution">Harvard Medical School and Massachusetts General Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Medical Intelligence and XR</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="148" to="158"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">FEA7F200B0C2B18B0C59B260DDECDFB6</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_14</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>test-time adaptation</term>
					<term>domain shift</term>
					<term>medical image segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-domain distribution shift is a common problem for medical image analysis because medical images from different devices usually own varied domain distributions. Test-time adaptation (TTA) is a promising solution by efficiently adapting source-domain distributions to target-domain distributions at test time with unsupervised manners, which has increasingly attracted important attention. Previous TTA methods applied to medical image segmentation tasks usually carry out a global domain adaptation for all semantic categories, but global domain adaptation would be sub-optimal as the influence of domain shift on different semantic categories may be different. To obtain improved domain adaptation results for different semantic categories, we propose Semantic-Aware Test-Time Adaptation (SATTA), which can individually update the model parameters to adapt to target-domain distributions for each semantic category. Specifically, SATTA deploys an uncertainty estimation module to measure the discrepancies of semantic categories in domain shift effectively. Then, a semantic adaptive learning rate is developed based on the estimated discrepancies to achieve a personalized degree of adaptation for each semantic category. Lastly, semantic proxy contrastive learning is proposed to individually adjust the model parameters with the semantic adaptive learning rate. Our SATTA is extensively validated on retinal fluid segmentation based on SD-OCT images. The experimental results demonstrate that SATTA consistently</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has achieved remarkable success in medical image segmentation when the training and test data are independent and identically distributed (i.i.d) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref>. However, in many practical situations, training and test data are collected by different medical imaging devices, leading to the presence of distribution shifts. Therefore, the models trained on source-domain data perform poorly on target-domain data. An effective solution for this issue is to fine-tune the models with labeled target-domain data to adapt to the target-domain distributions <ref type="bibr" target="#b8">[9]</ref>, but it is impractical to label the target-domain data considering the high annotation cost. Existing unsupervised domain adaptation (UDA) methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref> make full use of the labeled source-domain data and the unlabeled target-domain data in the model training, but even the target-domain data may not be available in the model training due to various practical problems.</p><p>Domain generalization (DG) methods exploit the diversity of source domains to improve the model generalization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref> when target-domain data is not available in the model training. However, it is also difficult and cost-consuming to collect multiple source-domain datasets with different domain distributions for DG. Another promising solution is test-time adaptation (TTA), which aims to gradually update the model parameters to adapt to target-domain distributions by learning from test data at test time. TTA shows greater flexibility than DG as the models could be pre-trained on single source-domain data. In TTA, a mainstream strategy is to adjust the affine parameters in BN layers for domain adaptation at test time by unsupervised loss, such as PTBN <ref type="bibr" target="#b11">[12]</ref> and TENT <ref type="bibr" target="#b18">[19]</ref>. Besides, auxiliary self-supervised tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref> and contrastive learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref> are also considerable for TTA.</p><p>TTA methods have also been recently applied in medical image applications. Ma et al. <ref type="bibr" target="#b10">[11]</ref> innovated distribution calibration by dynamically aggregating multiple representative classifiers via TTA to deal with arbitrary label shifts. Hu et al. <ref type="bibr" target="#b5">[6]</ref> designed regional nuclear-norm loss and contour regularization loss for TTA on medical image segmentation tasks. Bateson et al. <ref type="bibr" target="#b0">[1]</ref> performed inference by minimizing the entropy of predictions and a class-ratio prior, and integrated shape priors through penalty constraints for guide adaptation. Varsavsky et al. <ref type="bibr" target="#b17">[18]</ref> introduced domain adversarial learning and consistency training in TTA for sclerosis lesion segmentation. These TTA methods have a common limitation of using a fixed learning rate for all test samples. Since test samples arrive sequentially and the scale of domain shift would change frequently, a fixed learning rate would be sub-optimal for TTA. DLTTA <ref type="bibr" target="#b22">[23]</ref> proposed a memory bank-based discrepancy measurement for dynamic learning rate adjustment of TTA to effectively adapt the model to the varying domain shift. However, we find that the influence of domain shift on different semantic categories may also be different, DLTTA performed global domain adaptation for all semantic categories.</p><p>In this paper, we present Semantic-Aware Test-Time Adaptation (SATTA) for cross-domain medical image segmentation, aiming to perform individual domain adaptation for each semantic category at test time. SATTA first utilizes an uncertainty estimation module to effectively measure the discrepancies of different semantic categories in domain shift. Based on the estimated discrepancies, a semantic adaptive learning rate is then developed to achieve a personalized degree of adaptation for each semantic category. Lastly, a semantic proxy contrastive loss is proposed to individually adjust the model parameters with the semantic adaptive learning rate. Our SATTA is evaluated on retinal fluid segmentation based on spectral-domain optical coherence tomography (SD-OCT) images, and the experimental results show superior performance than other state-of-the-art TTA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Test-Time Adaptation Review</head><p>Given a labeled source-domain dataset S = {(x s n , y s n )} N s n=1 , model parameters θ are pre-trained on S by supervised risk minimization:</p><formula xml:id="formula_0">θ s = arg min θ 1 N s N s n=1 L sup (F θ (x s n ), y s n )<label>(1)</label></formula><p>where L sup is the supervised loss for model optimization, such as the crossentropy loss. However, for an unlabeled target-domain dataset</p><formula xml:id="formula_1">T = {(x t n )} N t n=1</formula><p>that has different domain distributions with S, the model F θ s may have an obvious performance degeneration. To make the model F θ s adapt to the targetdomain distributions, an unsupervised TTA loss L tta (such as rotation prediction loss <ref type="bibr" target="#b16">[17]</ref>, entropy minimization loss <ref type="bibr" target="#b18">[19]</ref>, contrastive loss <ref type="bibr" target="#b2">[3]</ref>, etc.) is designed to fine-tune model based on target-domain samples at test time:</p><formula xml:id="formula_2">θ t n ← θ t n-1 -η( L tta (F θ t n-1 (x t n ))), n ∈ [1, N t ] (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where η is learning rate, θ t 0 is initialized with θ s . The final prediction on x t n can be given by y</p><formula xml:id="formula_4">t n ∼ ŷt n = F θ t n (x t n ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic Adaptive Learning Rate</head><p>Pseudo-labeling for Semantic Aggregation. </p><formula xml:id="formula_5">W ∈ R d×C = [w 1 , w 2 , • • • , w C ] consisted of C category proxies,</formula><p>where each category proxy w c can be regarded as the high-dimensional representative of the category. Category predictor measures the similarities between pixel embeddings and all category proxies. We represent the classification process of pixel p i as:</p><formula xml:id="formula_6">y i ∼ ŷi ∈ R C = W • (f (p i )), i ∈ [1, hw]<label>(3)</label></formula><p>where f (•) is the feature extractor, C is the category number, h and w are the height and width of images. Since pixel-wise labels are not available for targetdomain samples at test time, we are hard to obtain the semantic information of all pixel embeddings directly. To address this problem, we assign pseudo labels to all pixel embeddings by passing them through the category predictor and then aggregate all pixel embeddings into C semantic clusters as</p><formula xml:id="formula_7">[Ω 1 , Ω 2 , • • • , Ω C ]</formula><p>according to their pseudo labels.</p><p>Semantic Uncertainty Estimation. After performing semantic aggregation by pseudo-labeling, we need to estimate the varying discrepancies of domain shift on categories. Here, we employ Monte Carlo Dropout <ref type="bibr" target="#b4">[5]</ref> for semantic uncertainty estimation. We enable dropout at test time and perform L stochastic forward passes through the model to obtain a set of predictive outputs for pixel embedding e i :</p><formula xml:id="formula_8">u l i = M(e i ), l ∈ [1, L], i ∈ [1, hw]<label>(4)</label></formula><p>where M(•) is a mapping network that maps the pixel embedding e i into an additional probability output. Then we estimate the standard deviation of the L outputs as the uncertainty score of e i :</p><formula xml:id="formula_9">s i = std(u 1 i , u 2 i , • • • , u L i ), i ∈ [1, hw]<label>(5)</label></formula><p>Here we take a single pixel as an example to show the uncertainty score computation, it should be noted that all pixels are performed parallel computation in the semantic segmentation model. Therefore, the computation cost does not increase with the number of pixels. For category c, its semantic uncertainty score can be calculated by:</p><formula xml:id="formula_10">U c = 1 N Ωc ei∈Ωc s i , c ∈ [1, C], i ∈ [1, hw]<label>(6)</label></formula><p>where N Ωc is the number of pixel embeddings in Ω c . U c captures the unique semantic domain discrepancy over category c. Later, semantic adaptive learning rate η c of category c for TTA is obtained directly based on the semantic domain discrepancy U c :</p><formula xml:id="formula_11">η c = α • U c (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where α is a scale factor. In this work, α could be set as the learning rate used for the model pre-training with the source-domain dataset. Each semantic category has its own individual learning rate in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semantic Proxy Contrastive Learning</head><p>General contrastive losses focus on exploring rich sample-to-sample relations, but they are hard to learn specific semantic information from samples. Proxy contrastive loss can model semantic relations by category proxies, as category proxies are more robust intuitively to noise samples <ref type="bibr" target="#b23">[24]</ref>. Therefore, a proxy contrastive loss is more suitable for unsupervised TTA optimization with our proposed semantic adaptive learning rate.</p><p>Projection Heads. We regard each category proxy as the anchor and consider all proxy-to-sample relations. Since proxy-based methods converge very easily, we consider applying projection heads to map both pixel embeddings and category proxies to a new feature space where proxy contrastive loss is applied.</p><formula xml:id="formula_13">Given semantic clusters [Ω 1 , Ω 2 , • • • , Ω C ] and category proxy weights W = [w 1 , w 2 , • • • , w C ],</formula><p>We use a three-layer MLP H 1 (•) for projecting pixel embeddings and one-layer MLP H 2 (•) for projecting category proxy weights. The new pixel embedding and category proxy weight can be given by z i = H 1 (e i ) and v c = H 2 (w c ).</p><p>Top-K Selection. Pixel embeddings with high uncertainty scores contribute little to semantic proxy contrastive learning. Besides, the computation cost is huge for all pixel embeddings. To address this problem, we select K pixel embeddings with the highest confidence from each semantic cluster Ω c . Specifically, for each semantic cluster Ω c , we first order all pixel embeddings in it from smallest to largest according to their uncertainty scores. Then we select the first K pixel embeddings as the new semantic cluster Ω c for the next proxy contrastive loss by Ω c = T opK(Order(Ω c )).</p><p>Semantic Proxy Contrastive Loss. For an anchor category cluster Ω c , we associate all pixel embeddings in it with category proxy weight v c to form the positive pairs. We ignore the sample-to-sample positive pairs and only consider the sample-to-sample negative pairs. The semantic proxy contrastive loss for category c can be given by:</p><formula xml:id="formula_14">L spc (x, W , c) = - 1 K z i ∈Ω c log exp(v c z i • τ ) Z s.t. Z = exp(v c z i • τ ) + C-1 r0=1 exp(v r0 z i • τ ) + 1 C C r1=1 z j ∈Ω r 1 exp(z i z j • τ )<label>(8)</label></formula><p>where {z i } K i=1 are obtained by x and C is the number of categories appearing in samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training and Adaptation Procedure</head><p>The overview of our SATTA is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Given the source-domain dataset</p><formula xml:id="formula_15">S = {(x s n , y s n )} N s n=1</formula><p>, the model parameters θ are pre-trained by the combination of supervised cross-entropy loss and semantic proxy contrastive loss:</p><formula xml:id="formula_16">L total = L ce (x s n , y s n ) + λ • 1 C C c=1 L spc (x s n , W , c)<label>(9)</label></formula><p>At test time, for a target-domain sample at time step n, we perform a forward pass to obtain semantic clusters and uncertainty scores and calculate the semantic adaptive learning rate of each category to serve for semantic proxy contrastive loss. For category c, the model parameters are updated to achieve desired adaptation by:</p><formula xml:id="formula_17">θ c n ← θ c n-1 -η c ( L spc (f θ c n-1 (x t n ))), n ∈ [1, N t ]<label>(10)</label></formula><p>The updated model parameters are stored in a memory bank and will be loaded for the next domain adaptation of category c. If category c does not appear in the test sample by pseudo-labeling, we ignore the update of model parameters θ c n-1 . We only update the parameters of the feature extractor and freeze the parameters of the category predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Materials</head><p>Our SATTA was evaluated on retinal fluid segmentation based on RETOUCH challenge <ref type="bibr" target="#b1">[2]</ref>, which is a representative benchmark for segmenting all of the three fluid types in SD-OCT images, including intraretinal fluid (IRF), subretinal fluid  <ref type="bibr" target="#b14">[15]</ref> w/o CD 0.716 0.794 0.802 0.738 0.904 0.786 0.711 0.664 0.768 U-Net <ref type="bibr" target="#b14">[15]</ref> w/ CD 0.637 0.751 0.667 0.587 0.733 0.624 0.536 0.512 0.565 TENT <ref type="bibr" target="#b18">[19]</ref> 0.648 0.772 0.733 0.605 0.828 0.703 0.589 0.603 0.637 FTTA <ref type="bibr" target="#b5">[6]</ref> 0.663 0.769 0.746 0.632 0.831 0.729 0.611 0.618 0.669 TTAS <ref type="bibr" target="#b0">[1]</ref> 0.672 0.774 0.762 0.674 0.865 0.762 0.663 0.641 0.724 CoTTA <ref type="bibr" target="#b19">[20]</ref> 0 (SRF) and pigment epithelial detachment (PED). SD-OCT images were acquired by three different vendors: Cirrus, Spectralis, and Topcon. The training set consists of 3072 (Cirrus), 1176 (Spectralis), and 3072 (Topcon) SD-OCT images, and the test set consists of 1792 (Cirrus), 686 (Spectralis) and 1792 (Topcon) SD-OCT images. We regard the SD-OCT images from three different vendors as three different domains, namely Domain-1 (Cirrus), Domain-2 (Spectralis), and Domain-3 (Topcon). We employ the dice similarity coefficient (DSC) as the quantitative segmentation metric and a higher DSC indicates a better segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with State-of-the-Arts</head><p>We compare our SATTA with four state-of-the-art TTA methods, including TENT <ref type="bibr" target="#b18">[19]</ref>, FTTA <ref type="bibr" target="#b5">[6]</ref>, TTAS <ref type="bibr" target="#b0">[1]</ref> and CoTTA <ref type="bibr" target="#b19">[20]</ref>. The four comparative methods have been reviewed in Sect. 1. To verify the TTA performance on cross-domain retinal fluid segmentation based on the RETOUCH challenge with three different domains, we train the segmentation models on two source domains and run TTA methods on the remaining target domain at test time. We carry out three times until all of three domains are tested as unseen target domains. For fair comparisons, all of TTA methods use U-Net <ref type="bibr" target="#b14">[15]</ref> as a feature extractor and share the same experimental setting, such as initial learning rate, batch size, etc. The quantitative comparison results are presented in Table <ref type="table" target="#tab_1">1</ref>. We also include "U-Net w/ CD" as the lower bound and "U-Net w/o CD" as the upper bound, where "CD" denotes cross domain. "U-Net w/o CD" denotes that the segmentation model is trained and tested on the samples from the same domain. "U-Net w/ CD" denotes that the segmentation model is trained and tested on the samples from different domains, without any domain adaptation. We observe that different TTA methods consistently improve the segmentation performance over "U-Net w/ CD". Our SATTA achieves the highest DSC than other methods. The qualitative comparison results are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. These visual results confirm  that a segmentation model trained only on source-domain distributions performs poorly on target-domain distributions without domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>We conduct ablation studies to analyze the key factors regarding our SATTA. We first explore the effect of K value in the Top-K selection strategy. The Top-K selection strategy aims to select pixel embeddings with high confidence scores to improve the semantic proxy contrastive learning and reduce the computation cost significantly. Figure <ref type="figure" target="#fig_2">3(a)</ref> shows the effect of different K values on three domains for IRF, SRF, and PED. The DSC values consistently increase when rising the K value from 10 to 20, generally, peak when the K value is between 20 and 25, and consistently decrease when further rising K value. This affirms that the pixel embeddings with high confidence scores are conducive to semantic proxy contrastive learning while the pixel embeddings with low confidence scores weaken semantic proxy contrastive learning.</p><p>We also investigate the effect of the initial learning rate. We select different initial learning rates from the set {5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5} for TTA, and Fig. <ref type="figure" target="#fig_2">3</ref>(b) shows the total average DSC values of all TTA methods. Our SATTA consistently performs better than other state-of-the-art TTA methods. We also find that different initial learning rates actually affect the domain adaptation ability. Therefore, a proper initial learning rate is essential for TTA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present the SATTA method for cross-domain medical image segmentation. Aiming at the problem that the domain shift has different effects on the semantic categories, our SATTA provides a semantic adaptive parameter optimization scheme at test time. Although our SATTA shows superior crossdomain segmentation performance than other state-of-the-art methods, it still has a limitation. Since SATTA adjusts the model for each semantic category, it is not quite suitable for the samples with too many semantic categories due to high computation costs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of SATTA for cross-domain medical image segmentation. Semantic adaptive learning rates are obtained by the uncertainty estimation module, and a semantic proxy contrastive loss is designed for individual semantic domain adaption.</figDesc><graphic coords="3,63,81,54,38,296,11,162,34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative comparison of cross-domain segmentation of different TTA methods on RETOUCH challenge. The first row shows the IRF segmentation on Spectralis SD-OCT images, the second row shows the SRF segmentation on Topcon SD-OCT images, and the third row shows the PED segmentation on Cirrus SD-OCT images.</figDesc><graphic coords="8,92,46,244,40,267,13,93,79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Ablation study with different K values in SATTA. (b) Ablation study with different initial learning rates in all TTA methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison of different TTA methods on RETOUCH challenge using DSC metric. (Note: CD denotes cross domain.)</figDesc><table><row><cell>Methods</cell><cell>Domain-1</cell><cell>Domain-2</cell><cell>Domain-3</cell></row><row><cell></cell><cell cols="3">IRF SRF PED IRF SRF PED IRF SRF PED</cell></row><row><cell>U-Net</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by the <rs type="funder">Shenzhen Portion of Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone</rs> under <rs type="grantNumber">HZQB-KCZYB-20200089</rs>, and partially supported by a grant from the <rs type="funder">Research Grants Council of the Hong Kong Special Administrative Region, China</rs> (Project Number: <rs type="grantNumber">T45-401/22-N</rs>) and by a grant from the <rs type="funder">Hong Kong Innovation and Technology Fund</rs> (Project Number: <rs type="grantNumber">GHP/080/20SZ</rs>). This work was also supported by the <rs type="funder">National Natural Science Foundation of China</rs> under Grants (<rs type="grantNumber">62202408</rs>, <rs type="grantNumber">62172223</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KSZktfe">
					<idno type="grant-number">HZQB-KCZYB-20200089</idno>
				</org>
				<org type="funding" xml:id="_4gTfSSJ">
					<idno type="grant-number">T45-401/22-N</idno>
				</org>
				<org type="funding" xml:id="_gymtYe7">
					<idno type="grant-number">GHP/080/20SZ</idno>
				</org>
				<org type="funding" xml:id="_R9HEMNj">
					<idno type="grant-number">62202408</idno>
				</org>
				<org type="funding" xml:id="_dr65nUQ">
					<idno type="grant-number">62172223</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_14.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Test-time adaptation with shape moments for image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bateson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_70</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_70" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="736" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Retouch: the retinal oct fluid detection and segmentation benchmark and challenge</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bogunović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1858" to="1874" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contrastive test-time adaptation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="295" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Y-net: a spatiospectral dualencoder network for medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yeganeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehlbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-7_56" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="582" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fully Test-Time Adaptation for Image Segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-4_24" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="251" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain specific convolution and high frequency reconstruction based unsupervised domain adaptation for medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_62</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_62" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="650" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wildnet: learning domain generalized semantic segmentation from the wild</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9936" to="9946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A domain adaptation model for early gear pitting fault diagnosis based on deep transfer learning network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Instit. Mech. Eng. Part O: J. Risk Reliabi</title>
		<imprint>
			<biblScope unit="volume">234</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="168" to="182" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ttt++: when does self-supervised test-time training fail or thrive?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Delft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bellot-Gurlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21808" to="21820" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Test-time adaptation with calibration of medical image classification nets for label distribution shift</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_30" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Evaluating prediction-time batch normalization for robustness under covariate shift</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Padhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>D'amour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10963</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A robust volumetric transformer for accurate 3d tumor segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_16" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="162" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic-aware domain generalized segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2594" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention-enhanced disentangled representation learning for unsupervised domain adaptation in cardiac segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_71</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_71" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="745" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Test-time training with self-supervision for generalization under distribution shifts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9229" to="9248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Test-time unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Varsavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orbes-Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-8_42" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="428" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10726</idno>
		<title level="m">Tent: fully test-time adaptation by entropy minimization</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Continual test-time domain adaptation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7201" to="7211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nestedformer: nested modality-aware transformer for brain tumor segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_14" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="140" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Denoising for relaxing: unsupervised domain adaptive fundus image segmentation without source data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_21" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dltta: Dynamic learning rate for test-time adaptation on crossdomain medical images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.13723</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pcl: proxy-based contrastive learning for domain generalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7097" to="7107" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
