<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models</title>
				<funder ref="#_xPXxVp4">
					<orgName type="full">Shenzhen General Program</orgName>
				</funder>
				<funder ref="#_rTJPmhE">
					<orgName type="full">Shenzhen-Hong Kong Joint Funding</orgName>
				</funder>
				<funder>
					<orgName type="full">Guangdong Provincial Key Laboratory of Big Data Computing</orgName>
				</funder>
				<funder ref="#_GgDbQjS">
					<orgName type="full">Chinese Key-Area Research and Development Program of Guangdong Province</orgName>
				</funder>
				<funder ref="#_vqbmE9E">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_zmEjATV #_p6WqJgB">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuhao</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SSE</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuncheng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SSE</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">FNii</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuangyi</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SSE</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xusheng</forename><surname>Wu</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Shenzhen Health Development Research and Data Management Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Dou</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<email>lizhen@cuhk.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SSE</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">FNii</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<email>liguanbin@mail.sysu.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Research Institute of Sun Yat-sen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SSE</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="339" to="349"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2E58641BF674ED6E205B2C0CEB8FAE2B</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_32</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Diffusion models</term>
					<term>Colonoscopy</term>
					<term>Polyp segmentation</term>
					<term>Polyp detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Colonoscopy analysis, particularly automatic polyp segmentation and detection, is essential for assisting clinical diagnosis and treatment. However, as medical image annotation is labour-and resourceintensive, the scarcity of annotated data limits the effectiveness and generalization of existing methods. Although recent research has focused on data generation and augmentation to address this issue, the quality of the generated data remains a challenge, which limits the contribution to the performance of subsequent tasks. Inspired by the superiority of diffusion models in fitting data distributions and generating high-quality data, in this paper, we propose an Adaptive Refinement Semantic Diffusion Model (ArSDM) to generate colonoscopy images that benefit the downstream tasks. Specifically, ArSDM utilizes the ground-truth segmentation mask as a prior condition during training and adjusts the diffusion loss for each input according to the polyp/background size ratio. Furthermore, ArSDM incorporates a pre-trained segmentation model to refine the training process by reducing the difference between the ground-truth mask and the prediction mask. Extensive experiments on segmentation and detection tasks demonstrate the generated data by ArSDM could significantly boost the performance of baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Colonoscopy is a critical tool for identifying adenomatous polyps and reducing rectal cancer mortality. Deep learning methods have shown powerful abilities in automatic colonoscopy analysis, including polyp segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b30">29]</ref> and polyp detection <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b24">24]</ref>. However, the scarcity of annotated data due to high manual annotation costs results in poorly trained and low generalizable models. Previous methods have relied on generative adversarial networks (GANs) <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b25">25]</ref> or data augmentation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b29">28]</ref> to enhance learning features, but these methods yielded limited improvements in downstream tasks. Recently, diffusion models <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b15">15]</ref> have emerged as promising solutions to this problem, demonstrating remarkable progress in generating multiple modalities of medical data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b21">21]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT Masks Original Images</head><note type="other">Synthesis</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion Sampler</head><p>Fig. <ref type="figure">1</ref>. Overview of the pipeline of our proposed approach, where details of ArSDM are described in Sect. 2.</p><p>Despite recent progress in these methods for medical image analysis, existing models face two major challenges when applied to colonoscopy image analysis. Firstly, the foreground (polyp) of colonoscopy images contains rich pathological information yet is often tiny compared with the background (intestine wall) and can be easily overwhelmed during training. Thus, naive generative models may generate realistic colonoscopy images but those images seldom contain polyp regions. In addition, in order to generate high-quality annotated samples, it is crucial to maintain the consistency between the polyp morphologies in synthesized images and the original masks, which current generative models struggle to achieve.</p><p>To tackle these issues and inspired by the remarkable success achieved by diffusion models in generating high-quality CT or MRI data <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b23">23]</ref>, we creatively propose an effective adaptive refinement semantic diffusion model (ArSDM) to generate polyp-contained colonoscopy images while preserving the original annotations. The pipeline of the data generation and downstream task training is shown in Fig. <ref type="figure">1</ref>. Specifically, we use the original segmentation masks as conditions to train a conditional diffusion model, which makes the generated samples share the same masks with the input images. Moreover, during diffusion model training, we employ an adaptive loss re-weighting method to assign loss weights for each input according to the size ratio of polyps and background, which addresses the overfitting problem for the large background. In addition, we fine-tune the diffusion model by minimizing the distance between the original ground truth masks and the prediction masks from synthesis images via a pretrained segmentation network. Thus the refined model could generate samples better aligned with the original masks.</p><p>In summary, our contributions are three-fold: (1) Adaptive Refinement SDM: Based on the standard semantic diffusion model <ref type="bibr" target="#b21">[21]</ref>, we propose a novel ArSDM with the adaptive loss re-weighting and the prediction-guided sample refinement mechanisms, which is capable of generating realistic polyp-contained colonoscopy images while preserving the original annotations. To the best of our knowledge, this is the first work for adapting diffusion models to colonoscopy image synthesis. (2) Large-Scale Colonoscopy Generation: The proposed approach can be used to generate large-scale datasets with no/arbitrary annotations, which significantly benefits the medical image society, laying the foundation for large-scale pre-training models in automatic colonoscopy analysis. (3) Qualitative and Quantitative Evaluation: We conduct extensive experiments to evaluate our method on five public benchmarks for polyp segmentation and detection. The results demonstrate that our approach could help deep learning methods achieve better performances. The source code is available at https:// github.com/DuYooho/ArSDM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Background. Denoising diffusion probabilistic models (DDPMs) <ref type="bibr" target="#b6">[6]</ref> are classes of deep generative models, which have forward and reverse processes. The forward process is a Markov Chain that gradually adds Gaussian noise to the original data. This process can be formulated as the joint distribution q (x 1:T | x 0 ):</p><formula xml:id="formula_0">q (x 1:T | x 0 ) := T t=1 q (x t | x t-1 ) , q (x t | x t-1 ) := N x t ; 1 -β t x t-1 , β t I , (1)</formula><p>where q (x 0 ) is the original data distribution with x 0 ∼ q (x 0 ), x 1:T are latents with the same dimension of x 0 and β t is a variance schedule.</p><p>The reverse process is aiming to learn a model to reverse the forward process that reconstructs the original input data, which is defined as:</p><formula xml:id="formula_1">p θ (x 0:T ) := p (x T ) T t=1 p θ (x t-1 | x t ) , p θ (x t-1 | x t ) := N x t-1 ; μ θ (x t , t) , σ 2 t I ,</formula><p>(2) where p (x T ) is the noised Gaussian transition from the forward process at timestep T . In this case, we only need to use deep-learning models to represent μ θ with θ as the model parameters. According to the original paper <ref type="bibr" target="#b6">[6]</ref>, the loss function can be simplified as:</p><formula xml:id="formula_2">L simple := E t,xt, ∼N (0,I) -θ (x t , t) 2 . (<label>3</label></formula><formula xml:id="formula_3">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion Process Sampler</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>U-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re-Weighting Module</head><p>Diffusion Loss ℒ Thus, instead of training the model μ θ to predict μt , we can train the model θ to predict ˜ , which is easier for parameterization and learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refinement Loss ℒ</head><note type="other">Weights</note><p>In this paper, we propose an adaptive refinement semantic diffusion model, a variant of DDPM, which has three key parts, i.e., mask conditioning, adaptive loss re-weighting, and prediction-guided sample refinement. The overall illustration of our framework is shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mask Conditioning</head><p>Unlike the previous generative methods, our work aims to generate a synthetic image with an identical segmentation mask to the original annotation. To accomplish this, we adapt the widely used conditional U-Net architecture <ref type="bibr" target="#b21">[21]</ref> in the reverse process, where the mask is fed as a condition. Specifically, for an input image x 0 ∈ R H×W ×C , x t can be sampled at any timestep t with the closed form:</p><formula xml:id="formula_4">x t = √ ᾱt x 0 + √ 1 -ᾱt , (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where ∼ N (0, I) , α t := 1β t and ᾱt := t s=1 α s . It will be fed into the encoder E of the U-Net, and its corresponding mask annotation c 0 ∈ R H×W will be injected into the decoder D. The model output can be formulated as:</p><formula xml:id="formula_6">θ (x t , t, c 0 ) = D (E (x t ) , c 0 ) . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>Thus, the U-Net model θ in Eq. 3 becomes θ (x t , t, c 0 ), and the loss function in Eq. 3 is changed to:</p><formula xml:id="formula_8">L condition = E t,xt,c0, ∼N (0,I) -θ (x t , t, c 0 ) 2 . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>Algorithm 1: One training iteration of ArSDM</p><formula xml:id="formula_10">Input: t ∼ Uniform({1, ..., T }), x0 ∼ q(x0), c0, ∼ N (0, I) Output: ˜ , c0 1 xt = √ ᾱtx0 + √ 1 -ᾱt ; xt = √ ᾱtx0 + √ 1 -ᾱt θ (xt, t, c0) 2 for i = t, ..., 1 do 3 z ∼ N (0, I) if i &gt; 1, else z = 0; xi-1 = 1 √ α i xi -1-α i √ 1-ᾱi θ (xi, i, c0) + σiz 4 end for 5 c0 = P(x0)</formula><p>6 Take gradient descent step on ∇ θ L total</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptive Loss Re-weighting</head><p>The polyp regions in the colonoscopy images differ from the background regions, which contain more pathological information and should be adequately treated to learn a better model. However, training the diffusion models using the original loss function ignores the difference between different regions, where each pixel shares the same weights when calculating the loss. This would lead to the model generating more background-like polyps since the large background region will easily overwhelm the small foreground polyp regions during training. A simple way to alleviate this problem is to apply a weighted loss function that assigns the polyp and background regions with different weights. However, most polyps vary a lot in size and shape. Thus assigning constant weights for all polyps exacerbated the imbalance problem. In this case, to tackle this problem, we propose an adaptive loss function that vests different weights according to the size ratio of the polyp over the background. Specifically, we define a pixel-wise weights matrix W λ ∈ R H×W with each entry w λ i,j to be:</p><formula xml:id="formula_11">w λ i,j = 1 -r , p = 1 r , p = 0 , r= #(p = 1) H × W , (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where p = 1 means the pixel p at (h, w) belongs to the polyp region and p = 0 means it belongs to the background region. Thus, the loss function becomes:</p><formula xml:id="formula_13">L adaptive = E t,xt,c0, ∼N (0,I) W λ • -θ (x t , t, c 0 ) 2 . (<label>8</label></formula><formula xml:id="formula_14">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Prediction-Guided Sample Refinement</head><p>The downstream tasks of polyp segmentation and detection require rich semantic information on polyp regions to train a good model. Through extensive experiments, we found inaccurate sample images with coarse polyp boundary that is not aligned properly with the original masks may introduce large biases and noises to the datasets. The model can be confused by several conflicting training images with the same annotation. To this end, we design a refinement strategy that uses the prediction of a pre-trained segmentation model on the sampled images to guide the training process and restore the proper polyp boundary information. Specifically, at each iteration of training, the output ˜ = θ (x t , t, c 0 ) will go into the sampler to generate sample image x0 . Then, we take the sample image as the input of the segmentation model to predict the pseudo masks c0 . We propose the following refinement loss based on IoU loss and binary cross entropy (BCE) loss between c0 and c 0 . The refinement loss is:</p><formula xml:id="formula_15">L refine = L(c, cg ) + i=5 i=3 L ( ci ) , c0 = { c3 , c4 , c5 , cg } = P (S (˜ )) , (<label>9</label></formula><formula xml:id="formula_16">)</formula><p>where L = L IoU + L BCE is the sum of the IoU loss and BCE loss, c0 is the collection of the three side-outputs ( c3 , c4 , c5 ) and the global map cg as described in <ref type="bibr" target="#b4">[5]</ref>. P(•) represents the PraNet model and S(•) is the DDIM <ref type="bibr" target="#b16">[16]</ref> sampler. The detailed procedure of one training iteration is shown in Algorithm 1 and the overall loss function is defined as:</p><formula xml:id="formula_17">L total = L adaptive + L refine . (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ArSDM Experimental Settings</head><p>We conducted our experiments on five public polyp segmentation datasets: EndoScene <ref type="bibr" target="#b20">[20]</ref>, CVC-ClincDB/CVC-612 <ref type="bibr" target="#b0">[1]</ref>, CVC-ColonDB <ref type="bibr" target="#b18">[18]</ref>, ETIS <ref type="bibr" target="#b14">[14]</ref> and Kvasir <ref type="bibr" target="#b7">[7]</ref>. Following the standard of PraNet, 1,450 image-mask pairs from Kvasir and CVC-ClinicDB are taken as the training set. The evaluations are conducted on the five datasets separately to verify the learning and generalization capability. The training image-mask pairs are padded to have the same height and width and then resized to the size of 384 × 384. Experiments with predictionguided sample refinement are trained with around one-half NVIDIA A100 days, while others are trained with approximately one day for convergence. We use the DDIM sampler with a maximum timestep of 200 for sampling images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Downstream Experimental Settings</head><p>We conduct the evaluation of our methods and the state-of-the-art counterparts on polyp segmentation and detection tasks. We generated the same number of samples as the diffusion training set using the original masks, and then combined them to create a new downstream training set. We employed PraNet <ref type="bibr" target="#b4">[5]</ref>, SANet <ref type="bibr" target="#b22">[22]</ref>, and Polyp-PVT <ref type="bibr" target="#b1">[2]</ref> as baseline segmentation models with default settings, and evaluated them using mean Intersection over Union (IoU) and mean Dice metrics. For detection, we selected CenterNet <ref type="bibr" target="#b31">[30]</ref>, Sparse-RCNN <ref type="bibr" target="#b17">[17]</ref>, and Deformable-DETR <ref type="bibr" target="#b32">[31]</ref> as baseline models with the same settings as the original papers, and evaluated them using Average Precision (AP) and F1-scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Quantitative Comparisons</head><p>The experimental results presented in Table <ref type="table" target="#tab_0">1</ref> and 2 demonstrate the effectiveness of our proposed method in training better downstream models to achieve superior performance. Specifically, data generated by our approach assists the  significant improvements for each model in mDice and mIoU, with increases of 6.0% and 5.7% over PraNet, 2.1% and 2.7% over SANet, and 0.7% and 0.7% over Polyp-PVT. We also observe superior AP and F1-scores compared to CenterNet, Sparse-RCNN, and Deformable-DETR trained with original data, with gains of 9.1% and 5.3%, 2.7% and 5.8%, and 3.4% and 6.1%, respectively. Moreover, we conducted a comprehensive comparison with SOTA models, noting that these models were not specifically designed for colonoscopy images and may generate data that hinder the training process or lack the ability for effective improvement. Nevertheless, our experimental results confirm the superiority of our proposed method.</p><p>Ablation Study. We conducted an ablation study to assess the importance of each proposed component. Table <ref type="table" target="#tab_2">3</ref> and 4 report the overall accuracies on the test set. The results demonstrate both components contribute to the accuracy improvement of baseline models, indicating their essential roles in achieving the best final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Qualitative Analyses</head><p>To further investigate the generative performance of our approach, we present visualization results in Fig. <ref type="figure">3</ref>, which displays the generated samples and their corresponding masks, alongside the original images for reference. The generated samples demonstrate differences from the original images in both the polyp regions and the backgrounds while maintaining alignment with the masks. Additionally, we sought evaluations from medical professionals to assess the authenticity of the generated samples, and non-medical professionals to locate polyps in the images, which yielded positive feedback on the quality of the generated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Automatic generation of annotated data is essential for colonoscopy image analysis, where the scale of existing datasets is limited by the expertise and time required for manual annotation. In this paper, we propose an adaptive refinement semantic diffusion model (ArSDM) for generating colonoscopy images while preserving annotations by introducing innovative adaptive loss re-weighting and prediction-guided sample refinement mechanisms. To evaluate our approach comprehensively, we conduct polyp segmentation and detection experiments on five widely used datasets, where experimental results demonstrate the effectiveness of our approach, in which model performances are greatly enhanced with little synthesized data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overall architecture of ArSDM.</figDesc><graphic coords="4,48,75,53,99,333,31,179,14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of different settings applied on three polyp segmentation baselines.</figDesc><table><row><cell cols="3">Methods EndoScene</cell><cell cols="2">ClinicDB</cell><cell>Kvasir</cell><cell cols="2">ColonDB</cell><cell>ETIS</cell><cell>Overall</cell></row><row><cell></cell><cell cols="7">mDice mIoU mDice mIoU mDice mIoU mDice mIoU mDice mIoU mDice mIoU</cell></row><row><cell cols="2">PraNet 87.1</cell><cell cols="2">79.7 89.9</cell><cell cols="2">84.9 89.8</cell><cell>84.0 70.9</cell><cell>64.0 62.8</cell><cell>56.7 74.0</cell><cell>67.5</cell></row><row><cell cols="2">+LDM 83.7</cell><cell cols="2">76.9 88.2</cell><cell cols="2">83.5 88.4</cell><cell>83.0 62.6</cell><cell>56.0 56.2</cell><cell>50.3 67.8</cell><cell>61.7</cell></row><row><cell>+SDM</cell><cell cols="3">89.9 83.2 89.2</cell><cell cols="2">83.7 88.4</cell><cell>82.6 74.2</cell><cell>66.5 66.4</cell><cell>60.3 76.4</cell><cell>69.6</cell></row><row><cell cols="2">+Ours 89.7</cell><cell cols="6">82.7 93.3 88.5 89.9 84.5 76.1 68.9 75.5 68.1 80.0 73.2</cell></row><row><cell>SANet</cell><cell>88.8</cell><cell cols="4">81.5 91.6 85.9 90.4</cell><cell>84.7 75.3</cell><cell>67.0 75.0</cell><cell>65.4 79.4</cell><cell>71.4</cell></row><row><cell cols="2">+LDM 72.7</cell><cell cols="2">60.5 88.8</cell><cell cols="2">82.8 88.7</cell><cell>82.7 64.3</cell><cell>55.4 58.0</cell><cell>49.2 68.3</cell><cell>59.8</cell></row><row><cell>+SDM</cell><cell cols="3">90.2 83.0 89.9</cell><cell cols="2">84.1 90.9</cell><cell>85.4 77.6</cell><cell>69.3 74.7</cell><cell>66.8 80.4</cell><cell>72.9</cell></row><row><cell cols="4">+Ours 90.2 83.2 91.4</cell><cell cols="4">86.1 91.1 85.6 77.7 70.0 78.0 69.5 81.5 74.1</cell></row><row><cell>PVT</cell><cell cols="3">90.0 83.3 93.7</cell><cell cols="3">88.9 91.7 86.4 80.8</cell><cell>72.7 78.7</cell><cell>70.6 83.3</cell><cell>76.0</cell></row><row><cell cols="2">+LDM 88.2</cell><cell cols="2">81.2 92.3</cell><cell cols="2">87.1 91.2</cell><cell>85.7 78.7</cell><cell>70.4 78.0</cell><cell>69.6 81.9</cell><cell>74.2</cell></row><row><cell>+SDM</cell><cell>88.8</cell><cell cols="4">81.7 93.9 89.2 91.2</cell><cell>86.1 81.3</cell><cell>73.5 78.7</cell><cell>71.1 83.4</cell><cell>76.3</cell></row><row><cell cols="2">+Ours 88.2</cell><cell cols="2">81.2 92.2</cell><cell cols="2">87.5 91.5</cell><cell cols="2">86.3 81.7 73.8 80.6 72.9 84.0 76.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparisons of different settings applied on three polyp detection baselines. Deform 98.1 94.4 89.7 89.9 80.2 74.4 82.2 75.5 65.3 54.7 64.5 71.8 +LDM 94.6 90.5 91.6 89.5 79.3 73.4 78.0 73.2 69.0 64.0 63.4 73.3 +SDM 96.0 90.6 90.3 91.2 82.2 78.9 80.1 75.1 67.5 66.7 65.1 75.8 +Ours 94.7 94.3 92.3 92.0 80.0 80.3 81.4 77.3 74.1 69.3 67.9 77.9</figDesc><table><row><cell cols="4">Methods EndoScene ClinicDB Kvasir</cell><cell cols="2">ColonDB ETIS</cell><cell>Overall</cell></row><row><cell></cell><cell>AP F1</cell><cell>AP F1</cell><cell>AP F1</cell><cell>AP F1</cell><cell>AP F1</cell><cell>AP F1</cell></row><row><cell>Center</cell><cell cols="6">86.9 91.4 84.7 89.2 75.6 81.4 62.2 72.3 62.7 70.1 56.6 76.0</cell></row><row><cell cols="7">+LDM 84.1 84.4 90.4 89.9 81.3 81.8 73.4 74.5 65.2 71.7 62.0 76.9</cell></row><row><cell>+SDM</cell><cell cols="6">87.8 86.9 88.7 91.0 77.0 82.8 71.8 78.1 68.2 72.6 61.8 79.1</cell></row><row><cell cols="7">+Ours 85.0 89.1 86.1 90.8 77.3 84.7 74.2 80.2 68.7 75.6 65.7 81.3</cell></row><row><cell>Sparse</cell><cell cols="6">89.9 87.8 81.4 86.4 75.6 80.2 78.2 73.2 63.8 62.4 63.7 73.2</cell></row><row><cell cols="7">+LDM 87.4 76.3 95.0 93.5 81.5 58.8 80.0 71.0 64.4 54.3 65.3 66.3</cell></row><row><cell>+SDM</cell><cell cols="6">94.5 90.5 88.7 86.5 79.0 80.5 81.4 76.8 67.8 67.1 65.2 76.7</cell></row><row><cell cols="7">+Ours 92.8 86.2 92.2 90.6 81.6 82.3 80.1 79.8 72.4 70.4 66.4 79.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of different components on polyp segmentation tasks.</figDesc><table><row><cell cols="2">Methods</cell><cell>PraNet</cell><cell>SANet</cell><cell></cell></row><row><cell cols="5">Ada. Ref. mDice mIoU mDice mIoU</cell></row><row><cell>✗</cell><cell>✗</cell><cell>76.4</cell><cell>69.6 80.4</cell><cell>72.9</cell></row><row><cell>✓</cell><cell>✗</cell><cell>79.1</cell><cell>72.4 80.5</cell><cell>72.8</cell></row><row><cell>✗</cell><cell>✓</cell><cell>78.5</cell><cell>71.5 81.1</cell><cell>73.2</cell></row><row><cell>✓</cell><cell>✓</cell><cell>80.0</cell><cell>73.2 81.5</cell><cell>74.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of different components on polyp detection tasks.</figDesc><table><row><cell cols="2">Methods</cell><cell cols="2">CenterNet Sparse.</cell></row><row><cell cols="3">Ada. Ref. AP F1</cell><cell>AP F1</cell></row><row><cell>✗</cell><cell>✗</cell><cell cols="2">61.8 79.1 65.2 76.7</cell></row><row><cell>✓</cell><cell>✗</cell><cell cols="2">62.2 80.1 65.8 77.2</cell></row><row><cell>✗</cell><cell>✓</cell><cell cols="2">64.0 80.4 66.0 77.6</cell></row><row><cell>✓</cell><cell>✓</cell><cell cols="2">65.7 81.3 66.4 79.0</cell></row></table><note><p><p><p><p>Samples</p>Fig.</p>3</p>. Illustration of generated samples with the corresponding masks and original images for comparison reference.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported in part by the <rs type="funder">Chinese Key-Area Research and Development Program of Guangdong Province</rs> (<rs type="grantNumber">2020B0101350001</rs>), in part by the <rs type="funder">Shenzhen General Program</rs> (No. <rs type="grantNumber">JCYJ20220530143600001</rs>), in part by the <rs type="funder">National Natural Science Foundation of China</rs> (NO. <rs type="grantNumber">61976250</rs>), in part by the <rs type="funder">Shenzhen-Hong Kong Joint Funding</rs> (No. <rs type="grantNumber">SGDX20211123112401002</rs>), in part by the <rs type="funder">Shenzhen Science and Technology Program</rs> (NO. <rs type="grantNumber">JCYJ20220818103001002</rs>, NO. <rs type="grantNumber">JCYJ20220530141211024</rs>), and in part by the <rs type="funder">Guangdong Provincial Key Laboratory of Big Data Computing</rs>, <rs type="affiliation">The Chinese University of Hong Kong, Shenzhen</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GgDbQjS">
					<idno type="grant-number">2020B0101350001</idno>
				</org>
				<org type="funding" xml:id="_xPXxVp4">
					<idno type="grant-number">JCYJ20220530143600001</idno>
				</org>
				<org type="funding" xml:id="_vqbmE9E">
					<idno type="grant-number">61976250</idno>
				</org>
				<org type="funding" xml:id="_rTJPmhE">
					<idno type="grant-number">SGDX20211123112401002</idno>
				</org>
				<org type="funding" xml:id="_zmEjATV">
					<idno type="grant-number">JCYJ20220818103001002</idno>
				</org>
				<org type="funding" xml:id="_p6WqJgB">
					<idno type="grant-number">JCYJ20220530141211024</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_32.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fernández-Esparrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wenhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deng-Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jinpeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huazhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ling</surname></persName>
		</author>
		<title level="m">Polyp-PVT: Polyp segmentation with pyramidvision transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised task-driven data augmentation for medical image segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">101934</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PraNet: parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-2_26" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kvasir-SEG: a segmented polyp dataset</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-37734-2_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-37734-2_37" />
	</analytic>
	<monogr>
		<title level="m">MMM 2020</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11962</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diffusion deformable model for 4D temporal medical image generation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_51</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-6_51" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention. MICCAI 2022. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cycle structure and illumination constrained GAN for medical image enhancement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59713-9_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59713-9_64" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12262</biblScope>
			<biblScope unit="page" from="667" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast unsupervised brain anomaly detection and segmentation with diffusion models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Pinaya</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-1_67" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data augmentation using generative adversarial networks (cycleGAN) to improve generalizability in CT segmentation tasks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sandfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Pickhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16884</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in WCE images for early diagnosis of colorectal cancer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Granado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision And Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14454" to="14463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automated polyp detection in colonoscopy videos using shape and context information</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="630" to="644" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automated polyp detection in colonoscopy videos using shape and context information</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="630" to="644" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A benchmark for endoluminal scene segmentation of colonoscopy images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Healthcare Eng</title>
		<imprint>
			<biblScope unit="page">4031790</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.00050</idno>
		<title level="m">Semantic image synthesis via diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shallow attention network for polyp segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_66</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_66" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="699" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">diffusion models for medical anomaly detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-1_4" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-frame collaboration for effective endoscopic video polyp detection via spatial-temporal feature transformation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_29" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="302" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">OfGAN: realistic rendition of synthetic colonoscopy videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_70</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0_70" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="732" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lesion-Aware Dynamic Kernel for Polyp Segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="99" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_10" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive context selection for polyp segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-2_25" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Data augmentation using learned transformations for one-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8543" to="8553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised spatial temporal attention network for video polyp segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="456" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<title level="m">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
