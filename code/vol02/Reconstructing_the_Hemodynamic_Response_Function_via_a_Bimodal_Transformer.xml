<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reconstructing the Hemodynamic Response Function via a Bimodal Transformer</title>
				<funder ref="#_x5vM3Ek">
					<orgName type="full">Israel Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Tel Aviv University Center for AI and Data Science</orgName>
					<orgName type="abbreviated">TAD</orgName>
				</funder>
				<funder ref="#_4gzKJHK">
					<orgName type="full">ISRAEL SCIENCE FOUNDA-TION</orgName>
				</funder>
				<funder ref="#_3ZkNGfR">
					<orgName type="full">European Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yoni</forename><surname>Choukroun</surname></persName>
							<email>choukroun.yoni@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">The School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<settlement>Tel Aviv-Yafo</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lior</forename><surname>Golgher</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The Edmond and Lily Safra Center for Brain Sciences</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pablo</forename><surname>Blinder</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Neurobiology, Biochemistry and Biophysics School</orgName>
								<orgName type="department" key="dep2">Wise Life Science Faculty</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<settlement>Tel Aviv-Yafo</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">The Sagol School for Neuroscience</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<settlement>Tel Aviv-Yafo</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<settlement>Tel Aviv-Yafo</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reconstructing the Hemodynamic Response Function via a Bimodal Transformer</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="371" to="381"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">27977FE9E5F931DA0F46D174729E0B6E</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Hemodynamic Response Function • Bimodal transformers</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The relationship between blood flow and neuronal activity is widely recognized, with blood flow frequently serving as a surrogate for neuronal activity in fMRI studies. At the microscopic level, neuronal activity has been shown to influence blood flow in nearby blood vessels. This study introduces the first predictive model that addresses this issue directly at the explicit neuronal population level. Using in vivo recordings in awake mice, we employ a novel spatiotemporal bimodal transformer architecture to infer current blood flow based on both historical blood flow and ongoing spontaneous neuronal activity. Our findings indicate that incorporating neuronal activity significantly enhances the model's ability to predict blood flow values. Through analysis of the model's behavior, we propose hypotheses regarding the largely unexplored nature of the hemodynamic response to neuronal activity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The brain consumes copious amounts of energy to sustain its activity, resulting in a skewed energetic budget per mass compared to the rest of the body (about 25% utilized by about 3%, see <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref> for an elaborate review of energy utilization). Given this disproportionate need, resources are allocated on a need-basis: active areas signal to the nearby blood vessel to dilate and increase blood flow, bringing a surplus of resources, to that area. This fundamental physiological process is called neurovascular coupling. It is non-trivial to model and different types of neuronal activity have been shown to elicit opposite vascular responses.</p><p>Neurovascular coupling is a cornerstone of proper brain function and also underpins the ability to observe and study the human brain in action. Imaging methods based on blood oxygenated level dependent (BOLD) approaches rely on it <ref type="bibr" target="#b12">[13]</ref>, as do methods that are based on rheological properties, such as blood volume and flow speed. Since these methods do not directly measure neuronal activity per-se, but a physiological proxy, i.e. the resulting change in vascular dynamics and oxygen levels, it is of utmost importance to know the precise transform function linking neuronal activity to the observed vascular dynamics. Given the differential response to neuronal activity (see <ref type="bibr" target="#b4">[5]</ref> for a timely review), obtaining a cellular and population level hemodynamic response function (HRF) remains an unmet need in this field, that would finally unlock the ability to infer neuronal activity directly from blood flow dynamics <ref type="bibr" target="#b19">[20]</ref>.</p><p>The initial characterization of the hemodynamic response function (HRF) was performed at the system level, where system refers to large cortical regions encompassing tens of thousands of neurons of different types, without taking into account the fine details of different vascular compartments (see <ref type="bibr" target="#b28">[29]</ref> for a succinct review on the original works). At this level, a canonical response function was derived from extensive work on sensory-evoked somatosensory responses. This HRF has become widely accepted and used in the interpretation of BOLD signals. This function consists of three components: an initial dip (its existence and physiological origin are much debated), a prolonged and very pronounced overshoot, followed by a shallower and much shorter undershoot. The initial dip occurs within one second of the sensory stimulus, the overshoot peaks around five seconds later, overshoot and return to baseline level occurs within 15-20 s post stimuli. It should be noted that vascular reactivity is much faster than the collective behavior described by the canonical HRF, with reports showing sensoryevoked vascular responses observed after just 300ms. Recently, more advanced imaging and analysis methods have pushed the formulation of an HRF at the single cell to single blood vessel (capillary) level, pointing to a rather narrow family of possible functions. Importantly, this work also established that the HRF derived at the microscopic level can be partially translated to macroscopic imaging approaches. Nevertheless, single neuron to single vessel responses fail to capture the more complex and varied neuronal population level responses that could be integrated across the extensive vascular network that surrounds them. Here, we exploit a unique dataset, in which neuronal and vascular responses (changes in diameter) were recorded in a volumetric fashion and with relevant temporal resolution, allowing us to establish a novel pipeline to uncover/formulate a many-to-many HRF.</p><p>Our model needs to combine neuron firing and blood vessel data and employs a multi-modal transformer. There are three types of multi-modal transformers: (i) a multi-modal Transformer where the two modalities are concatenated and separated by the [SEP] token <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, and self-attention is used, (ii) coattention-based model modules that contextualize each modality with the other modality <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref>, and (iii) generative models containing an encoder that uses self-attention on the input and a decoder that uses both the encoded data and data from the decoder's domain as inputs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b33">34]</ref>. Our model is of the third type and presents two distinctive properties: pulling from multiple time points and an attention mechanism that is modulated based on distance.</p><p>Our results show that the new transformer model can predict the state of blood vessels better than the baseline models. The utility of neuronal data in the prediction is demonstrated by an ablation study. By analyzing the learned model, we obtain insights into the link between neuronal and vascular activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>All procedures were approved by the Ethics Committee of Tel Aviv University for Animal Use and Welfare and followed pertinent Institutional Animal Care and Use Committee (IACUC) and local guidelines. Neuronal activity was monitored in female C57BL/6J transgenic mice expressing Thy1-GCaMP6s. Vascular dynamics were tracked using a Texas Red fluorescent dye, which was conjugated to a large polysaccharide moiety (2 mega Dalton dextran) and retro-orbitally bolus injected under brief isoflurane sedation at the beginning of the imaging day.</p><p>425 quasi-linear vascular segments and 50 putative neuronal cell bodies were manually labeled within a volume of 490×500×300μm 3 , which was continuously imaged across two consecutive 1850-second long sessions at an imaging rate of 30.03 volumes per second <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. For neuronal activity estimation, we selected a cuboid volume of interest around each neuronal cell body and summed the fluorescence within it following an axial intensity normalization corresponding to an uneven duty cycle of our varifocal lens.</p><p>For vascular diameter estimation we used the Radon transform, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref> as its resilience to rotation and poor contrast are particularly useful for our application. Specifically, Gao and Drew have formerly found that thresholding the vascular intensity profile in Radon space is more resilient to noise than other thresholding methods <ref type="bibr" target="#b7">[8]</ref>. Based on their observation, we used the timecollapsed imagery to determine a threshold in Radon space, which was then applied separately for each frame in time.</p><p>This unique ability to rapidly track neuronal and vascular interactions across a continuous brain volume bears several important advantages <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. In particular, a greater proportion of the vascular ensemble that reacts to a given neuronal metabolic demand can be accounted for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The HRF learning problem explored in this work is defined as the prediction of current blood flow rates at different vessel segments, given the previous neuronal spikes as well as previous blood flow rates. We propose to design a parameterized deep neural network f θ for scalar regression of blood flow rates at different vessel segments, such that at a given time t we have</p><formula xml:id="formula_0">f θ (S t , F t , X S , X F ) → R m (1)</formula><p>where the matrix S t ∈ R ts×n denotes the n neurons' spikes at the t s previous samples, while the matrix F t ∈ R tv×m denotes the blood flow of the m vessel segment at the previous t v time samples. X S ∈ R n×3 and X F ∈ R m×3 are the three-dimensional positions of the neurons and vessel segments, respectively. HRF predictions should satisfy fundamental symmetries and invariance of physiological priors and of experimental bias, such as invariance to rigid spatial transformation (rotation and translation). Therefore, a positional input X u is transformed to inter-elements Euclidean distances</p><formula xml:id="formula_1">D u = {d u ij } i,j where d u ij = (X u ) i -(X u ) j 2 for rigid transform invariance.</formula><p>Thus, the learning problem is refined as</p><formula xml:id="formula_2">f θ : {S t , F t , D S , D F , D SF } → R m</formula><p>, where D S , D F , D SF represents the Euclidean distance matrix between neurons, vessel segments, and neurons to vessel segments, respectively. We do not include any further auxiliary features or prior in the input.</p><p>We model f θ using a new variant of the Transformer family. The proposed model consists of an encoder and a decoder. The encoder embeds the neurons at both spatial and temporal levels. The decoder predicts vessel segment flow by utilizing both the past flow values and the spatial information of the vessel segments, along with the neuronal activity via the cross-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformers.</head><p>The self-attention mechanism introduced by Transformers <ref type="bibr" target="#b29">[30]</ref> is based on a trainable associative memory with (key, value) vector pairs, where a query vector q ∈ R d is matched against a set of k key vectors using scaled inner products, as follows</p><formula xml:id="formula_3">A(Q, K, V ) = Softmax QK T √ d V,<label>(2)</label></formula><p>where Q ∈ R N ×d , K ∈ R k×d and V ∈ R k×d represent the packed N queries, k keys and values tensors respectively. Keys, queries and values are obtained using linear transformations of the sequence's elements. A multi-head self-attention layer is defined by extending the self-attention mechanism using h attention heads, i.e. h self-attention functions applied to the input, reprojected to values via a dh × D linear layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neuronal Encoding.</head><p>To obtain the initial Spatio-Temporal Encoding, for the prediction at time t, we project each neuron to a high d dimensional embedding φ s t ∈ R ts×n×d by modulating it with its spike value such that</p><formula xml:id="formula_4">φ s t = S t (1 d W T )</formula><p>, where W ∈ R d denotes the neuronal encoding. The embedding is modulated by the magnitude of the spike, such that higher neuronal activities are projected farther in the embedding space.</p><p>The temporal encoding is defined using sinusoidal encoding <ref type="bibr" target="#b29">[30]</ref> applied on φ and augmented with a learnable embedding such that φ s t ← φ s t + p t • p where p t and p represent the sinusoidal time encoding and the learned vector, respectively. We emphasize the fact that, contrary to traditional transformers, the embedding tensor φ t has an additional spatial dimension such that the tensor is threedimensional, enabling both spatial and temporal attention.</p><p>In order to incorporate the spatial information of the neurons, we propose to insert spatial encoding by importing the pairwise information directly into the self-attention layer. For this, we multiply the distance relation by the similarity tensor as follows</p><formula xml:id="formula_5">A S (Q, K, D S ) = Softmax QK T √ d ψ S (D S ),<label>(3)</label></formula><p>with denoting the Hadamard product, and ψ S (D S ) : R + → R + an elementwise learnable parameterized similarity function. This way, the similarity function scales the self-attention map according to the distance between the elements (in our case the neurons).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vascular Decoding.</head><p>The spatio-temporal encoding of the vascular data is similar to the embedding performed by the encoder. The information on each vascular segment is embedded in a high-dimensional vector φ F t ∈ R tv×m×d to be further projected by the temporal encoding. The spatial geometric information is incorporated via the pairwise vascular segments' distance matrix D F via the decoder's self-attention module A F .</p><p>The most important element of the decoder is the cross-attention module, which incorporates neuronal information for vascular prediction. Given the final neuronal embeddings φ s t , the cross-attention module performs cross-analysis of the neuronal embeddings such that</p><formula xml:id="formula_6">A SF (Q F , K S , D S ) = Softmax Q F K T S √ d ψ SF (D SF ),<label>(4)</label></formula><p>where Q F and K S represent the affine transform of φ F t and φ s t , respectively. Here also, the (non-square) cross-attention map is modulated by the neuronvessel distance matrix D SF .</p><p>The spatio-temporal map is of dimensions A SF ∈ R tv×ts×h×m×n where h denotes the number of attention heads. Thus, we perform aggregation by averaging over the neuronal time dimension, in order to remain invariant to the temporal neuronal embedding and to gather all past neuronal influence on blood flow rates. This way, one can observe that the proposed method is not limited to any spatial or time constraint. The model can be deployed in different spatiotemporal settings at test time, thanks to both the geometric spatial encoding and the Transformer's sequential processing ability. Finally, the output module reprojects the last time vessel embedding into the prediction space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture and Training.</head><p>The initial encoding defines the model embedding dimension d = 64. The encoder and the decoder are defined as the concatenation of L = 3 layers, each composed of self-attention and feed-forward layers interleaved with normalization layers. The decoder also contains N additional cross-attention modules. The output layer is defined by a fully connected layer that projects the last vascular time embedding into the objective dimension m. An illustration of the model is given in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>The dimension of the feed-forward network is four times that of the embedding <ref type="bibr" target="#b29">[30]</ref>. It is composed of GEGLU layers <ref type="bibr" target="#b24">[25]</ref>, with layer normalization set to the pre-layer norm setting, as in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref>. We use an eight-head self-attention module in all experiments. The geometric filtering first augments the distance using Fourier features <ref type="bibr" target="#b26">[27]</ref> and the module is a fully connected neural network with two 50-dimensional hidden layers and GELU non-linearities, expanded to all the heads of the self-attention module. We provide the module with the element-wise inverse of the distance matrix instead of the regular Euclidean matrix, both in order to reduce the dynamic range and since closer elements may have a higher impact.</p><p>The training objective is the Mean Squared Error loss</p><formula xml:id="formula_7">L = E t m j f θ (S t , F t , D S , D F , D SF ) -F t+1 2<label>(5)</label></formula><p>The Adam optimizer <ref type="bibr" target="#b13">[14]</ref> is used with 32 samples per minibatch, for 300 epochs. We initialized the learning rate to 5 • 10 -5 coupled with a cosine decay scheduler down to 1 • 10 -6 at the end of the training. The dataset of the first data collection session has been split by 85%, 7.5% and 7.5% for the training, validation, and testing set, respectively. Training time is approximately 20 h for time windows t s = t v = 10, on an NVIDIA RTX A600. Testing time is approximately 0.25 ms per sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare the proposed method, dubbed Hemodynamic Response Function Transformer (HRFT), with several popular statistical and machine-learning models: (i) naive persistence model, which predicts the previous time step's vascular input, (ii) linear regression, which concatenates all the input (blood flow and neuronal data) from all times stamps before performing the regression, and (iii) a Recurrent Neural Network composed of two stacked GRU <ref type="bibr" target="#b3">[4]</ref> layers. All the methods are experimented with using the same inputs and the models have similar capacity (∼0.5 M parameters). In order to understand the impact of neuronal information, we also compare our method with the HRFT encoder only applied to the vascular input, referred to as HRFT-S. The only difference between this model and the full HRFT is the cross-attention module. If the neuronal input is irrelevant or the link is too weak to improve the prediction, HRFT is expected not to outperform HRFT-S, which makes the comparison pertinent.</p><p>We present both MSE and Normalized Root MSE. Because of computational constraints, we randomly subsample 55 vessel segments among the 425. We trained the model with temporal windows of size t s = t v = 10, equivalent to 300 ms according to the original data acquisition's 30.03 Hz sampling rate.</p><p>In addition to the original sampling rate, we also present results for prediction based on lower frequencies, in order to check the ability of the models to capture longer-range dependencies. We note that the error in these cases is expected to be larger, since the time gap between the last measurement and the required prediction is larger.</p><p>In order to check the generalization abilities of the methods, we test the trained models on a second dataset obtained 30 min after the sampling of the original dataset (that includes training, validation, and the first test set). The results are presented in Table <ref type="table" target="#tab_0">1</ref>. As can be seen, the HRFT method outperforms all baselines, including the HRFT-S variant, for 6 Hz and 15 Hz. At the original sampling rate, the performance of HRFT and HRFT-S is similar and better than the baselines. This is expected since at this frame rate the history of t v = 10 we employ spans only 300 ms, which is at the limit of the shortest known neurovascular response reported in the literature <ref type="bibr" target="#b28">[29]</ref>. It is reassuring that error levels for HRFT remain similar for samples taken 30min after the training set (and the first test set) were collected.</p><p>To gain insights into the HRF, we examine the HRFT model. The learned distance function ψ SF of the 1st cross attention layer is depicted in Fig. <ref type="figure" target="#fig_1">2(a)</ref> (other layers are similar). The plot shows the learned function in blue and the actual samples in red. Evidently, this prior on the attention is monotonically decreasing with the distance between the neuron and the blood vessel. Panel (b) shows the cross-attention in the same layer. We note that some neurons have little influence, and the rest of the attention is scattered relatively uniformly. Panel (c) considers the derivative of the prediction vector F t+1 by each of the neuron data, summed over all test samples of the 2nd session at 6 Hz, and all neurons and vessels. There are two negative peaks (contractions) that occur at 333 ms and 1333 ms, which is remarkably consistent with current knowledge <ref type="bibr" target="#b28">[29]</ref>. There is also a dilation effect at 666 ms. The 15 Hz data with t v = 10 captures shifts of 0-700 ms and the 300 ms peak is clearly visible in that model as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We present the first local HRF model. While for the baseline methods, the performance is at the same level with and without neuronal data (omitted from the tables), the transformer we present supports an improved prediction capability using neuronal firing rates (ablation) and also gives rise to interesting insights regarding the behavior of the hemodynamic response function.</p><p>Limitations. Our main goal is to verify the ability to model HRF by showing that using neuronal data helps predict blood flow beyond the history of the latter. The next challenge is to scale the model in order to be able to model more vessels (without subsampling) and longer historical sequences (larger t v , t s ). With transformers being used for very long sequences, this is a limitation of our resources and not of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the proposed HRF Transformer architecture. The main differences from the traditional Transformers are the Geometric self-attention modules and the unified spatiotemporal analysis induced by the time aggregation module.</figDesc><graphic coords="6,110,31,54,56,203,05,214,15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) The learned function ψSF for the 1st cross-attention layer of the transformer. (b) The magnitude of the self-attention map between every neuron and every vessel at this layer. (c) The impact of the neurons on the vessels (saturated at 90%) for each shift in time as obtained by marginalizing over all 2nd session test samples in the 30.02 Hz dataset. More visualizations of the datasets and the learned features are provided in the Appendix.</figDesc><graphic coords="8,41,79,54,05,340,33,83,77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The prediction errors of the various methods on two test sets. The first is obtained in the same session for which the training samples were collected (top half of the table, and the second in a separate session 30 min later (bottom half).</figDesc><table><row><cell>Method</cell><cell>6 Hz</cell><cell>15 Hz</cell><cell>30.03 Hz</cell></row><row><cell></cell><cell cols="3">MSE NRMSE MSE NRMSE MSE NRMSE</cell></row><row><cell cols="2">Persistence 24.38 0.220</cell><cell>12.93 0.160</cell><cell>6.923 0.115</cell></row><row><cell>Linear</cell><cell>13.65 0.166</cell><cell>9.660 0.139</cell><cell>5.911 0.107</cell></row><row><cell>RNN</cell><cell>13.78 0.168</cell><cell>11.38 0.157</cell><cell>10.31 0.147</cell></row><row><cell>HRFT-S</cell><cell>13.34 0.165</cell><cell>9.426 0.138</cell><cell>5.782 0.110</cell></row><row><cell>HRFT</cell><cell>13.00 0.162</cell><cell>9.370 0.137</cell><cell>5.783 0.106</cell></row><row><cell cols="2">Persistence 23.11 0.221</cell><cell>11.97 0.160</cell><cell>7.662 0.125</cell></row><row><cell>Linear</cell><cell>17.01 0.192</cell><cell>10.26 0.147</cell><cell>6.002 0.111</cell></row><row><cell>RNN</cell><cell>15.193 0.182</cell><cell>13.04 0.172</cell><cell>11.87 0.162</cell></row><row><cell>HRFT-S</cell><cell>14.63 0.176</cell><cell>9.820 0.147</cell><cell>5.914 0.110</cell></row><row><cell>HRFT</cell><cell>14.34 0.173</cell><cell>9.191 0.143</cell><cell>5.908 0.110</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. The authors thank <rs type="person">David Kain</rs> for conducting the mouse surgery. This project has received funding from the <rs type="funder">ISRAEL SCIENCE FOUNDA-TION</rs> (grant No. <rs type="grantNumber">2923/20</rs>) within the <rs type="programName">Israel Precision Medicine Partnership program</rs>. It was also supported by a grant from the <rs type="funder">Tel Aviv University Center for AI and Data Science (TAD)</rs>. It was also supported by the <rs type="funder">European Research Council</rs>, grant No <rs type="grantNumber">639416</rs>, and the <rs type="funder">Israel Science Foundation</rs>, grant No <rs type="grantNumber">2342/21</rs>. The contribution of the first author is part of a PhD thesis research conducted at <rs type="institution">Tel Aviv University</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4gzKJHK">
					<idno type="grant-number">2923/20</idno>
					<orgName type="program" subtype="full">Israel Precision Medicine Partnership program</orgName>
				</org>
				<org type="funding" xml:id="_3ZkNGfR">
					<idno type="grant-number">639416</idno>
				</org>
				<org type="funding" xml:id="_x5vM3Ek">
					<idno type="grant-number">2342/21</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 35.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tracking and diameter estimation of retinal vessels using gaussian process and radon transform</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Koohbanani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gooya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">34006</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Thermodynamic limitations on brain oxygen metabolism: physiological implications</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Buxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2024" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<title level="m">Endto-end object detection with transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vascular and neural basis of the bold signal</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Drew</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.CONB.2019.06.004</idno>
		<ptr target="https://doi.org/10.1016/J.CONB.2019.06.004" />
	</analytic>
	<monogr>
		<title level="j">Current Opin. Neurobiol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="61" to="69" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rapid determination of particle velocity from space-time images using the radon transform</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cauwenberghs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kleinfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Neurosci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="11" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient machine learning framework for computer-aided detection of cerebral microbleeds using the radon transform</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fazlollahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="113" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Determination of vessel cross-sectional area by thresholding in radon space</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cereb. Blood Flow Metab</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1180" to="1187" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rapid volumetric imaging of numerous neuro-vascular interactions in awake mammalian brain</title>
		<author>
			<persName><forename type="first">L</forename><surname>Golgher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>Sagol School of Neuroscience, Tel Aviv University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microvascular dynamics from 4d microscopy using temporal segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golgher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blinder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Symposium on Biocomputing 2020</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="331" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pysight: plug and play photon counting for fast continuous volumetric intravital microscopy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Har-Gil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optica</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1104" to="1112" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Versatile software and hardware combo enabling photon counting acquisition and real-time display for multiplexing, 2d and continuous 3d two-photon imaging applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Har-Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golgher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blinder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurophotonics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">31920</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Biophysical and physiological origins of blood oxygenation level-dependent FMRI signals</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ogawa</surname></persName>
		</author>
		<idno type="DOI">10.1038/jcbfm.2012.23</idno>
		<ptr target="https://doi.org/10.1038/jcbfm.2012.23" />
	</analytic>
	<monogr>
		<title level="j">J. Cereb. Blood Flow Metab. Off. J. Int. Soc. Cereb. Blood Flow Metab</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1206" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open-source toolkit for neural machine translation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Communication consumes 35 times more energy than computation in the human cortex, but both costs are needed to predict synapse number</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Calvert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">18</biblScope>
			<date type="published" when="2021">2008173118. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">VisualBERT: a simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Oscar: object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58577-8_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58577-88" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12375</biblScope>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What we can do and what we cannot do with FMRI</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Logothetis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature06976</idno>
		<ptr target="https://doi.org/10.1038/nature06976" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="page" from="869" to="878" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">VilBERT: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A review of machine learning methods for retinal blood vessel segmentation and artery/vein classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R K</forename><surname>Mookiah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">101905</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Local memory attention for fast video semantic segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01715</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A radon transform based approach for extraction of blood vessels in conjunctival images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Banaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Kakhki</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-88636-5_89</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-88636-589" />
	</analytic>
	<monogr>
		<title level="m">MICAI 2008. LNCS (LNAI)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Morales</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5317</biblScope>
			<biblScope unit="page" from="948" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">GLU variants improve transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">LXMERT: learning cross-modality encoder representations from transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7537" to="7547" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Radon transform technique for linear structures detection: application to vessel detection in fluorescein angiography fundus images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehdizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Banaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Toosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Nuclear Science Symposium Conference Record</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="3051" to="3056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linking brain vascular physiology to hemodynamic response in ultra-high field MRI</title>
		<author>
			<persName><forename type="first">K</forename><surname>Uludag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blinder</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2017.02.063</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2017.02.063" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="279" to="295" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00759</idno>
		<title level="m">Max-deeplab: end-to-end panoptic segmentation with mask transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04745</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deformable DETR: deformable transformers for end-to-end object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
