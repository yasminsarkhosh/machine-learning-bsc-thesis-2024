<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation</title>
				<funder ref="#_ZU7AwAN">
					<orgName type="full">National Key R&amp;D program of China</orgName>
				</funder>
				<funder ref="#_ZB7Vbhb">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_r9qDKPX">
					<orgName type="full">Hong Kong Health and Medical Research Fund</orgName>
					<orgName type="abbreviated">HMRF</orgName>
				</funder>
				<funder ref="#_Vd84NdQ">
					<orgName type="full">Hong Kong RGC CRF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fan</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyu</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinyu</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoli</forename><surname>Yin</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Shengjing Hospital of China Medical University</orgName>
								<address>
									<postCode>110004</postCode>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Shi</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Shengjing Hospital of China Medical University</orgName>
								<address>
									<postCode>110004</postCode>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Le Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">-H</forename><surname>Meng</surname></persName>
							<email>mengqh@sustech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E92D04517AD757E61A2B096647CC5168</idno>
					<idno type="DOI">10.1007/978-3-031-43895-02.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Active Learning</term>
					<term>Prompt Tuning</term>
					<term>Segmentation Supplementary Information The online version contains supplementary material</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical image analysis using deep learning is often challenged by limited labeled data and high annotation costs. Fine-tuning the entire network in label-limited scenarios can lead to overfitting and suboptimal performance. Recently, prompt tuning has emerged as a more promising technique that introduces a few additional tunable parameters as prompts to a task-agnostic pre-trained model, and updates only these parameters using supervision from limited labeled data while keeping the pre-trained model unchanged. However, previous work has overlooked the importance of selective labeling in downstream tasks, which aims to select the most valuable downstream samples for annotation to achieve the best performance with minimum annotation cost. To address this, we propose a framework that combines selective labeling with prompt tuning (SLPT) to boost performance in limited labels. Specifically, we introduce a feature-aware prompt updater to guide prompt tuning and a TandEm Selective LAbeling (TESLA) strategy. TESLA includes unsupervised diversity selection and supervised selection using prompt-based uncertainty. In addition, we propose a diversified visual prompt tuning strategy to provide multi-promptbased discrepant predictions for TESLA. We evaluate our method on liver tumor segmentation and achieve state-of-the-art performance, outperforming traditional fine-tuning with only 6% of tunable parameters, also achieving 94% of full-data performance by labeling only 5% of the data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has achieved promising performance in computer-aided diagnosis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b24">24]</ref>, but it relies on large-scale labeled data to train, which is challenging in medical imaging due to label scarcity and high annotation cost <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">25]</ref>. Specifically, expert annotations are required for medical data, which can be costly and time-consuming, especially in tasks such as 3D image segmentation.</p><p>Transferring pre-trained models to downstream tasks is an effective solution for addressing the label-limited problem <ref type="bibr" target="#b7">[8]</ref>, but fine-tuning the full network with small downstream data is prone to overfitting <ref type="bibr" target="#b16">[16]</ref>. Recently, prompt tuning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">18]</ref> is emerging from natural language processing (NLP), which introduces additional tunable prompt parameters to the pre-trained model and updates only prompt parameters using supervision signals obtained from a few downstream training samples while keeping the entire pre-trained unchanged. By tuning only a few parameters, prompt tuning makes better use of pre-trained knowledge. It avoids driving the entire model with few downstream data, which enables it to outperform traditional fine-tuning in limited labeled data. Building on the recent success of prompt tuning in NLP <ref type="bibr" target="#b4">[5]</ref>, instead of designing text prompts and Transformer models, we explore visual prompts on Convolutional Neural Networks (CNNs) and the potential to address data limitations in medical imaging.</p><p>However, previous prompt tuning research <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b28">28]</ref>, whether on language or visual models, has focused solely on the model-centric approach. For instance, CoOp <ref type="bibr" target="#b29">[29]</ref> models a prompt's context using a set of learnable vectors and optimizes it on a few downstream data, without discussing what kind of samples are more suitable for learning prompts. VPT <ref type="bibr" target="#b12">[13]</ref> explores prompt tuning with a vision Transformer, and SPM <ref type="bibr" target="#b17">[17]</ref> attempts to handle downstream segmentation tasks through prompt tuning on CNNs, which are also model-centric. However, in downstream tasks with limited labeled data, selective labeling as a data-centric method is crucial for determining which samples are valuable for learning, similar to Active Learning (AL) <ref type="bibr" target="#b23">[23]</ref>. In AL, given the initial labeled data, the model actively selects a subset of valuable samples for labeling and improves performance with minimum annotation effort. Nevertheless, directly combining prompt tuning with AL presents several problems. First, unlike the task-specific models trained with initial data in AL, the task-agnostic pre-trained model (e.g., trained by related but not identical supervised or self-supervised task) is employed for data selection with prompt tuning. Second, in prompt tuning, the pre-trained model is frozen, which may render some AL methods inapplicable, such as those previously based on backbone gradient <ref type="bibr" target="#b8">[9]</ref> and feature <ref type="bibr" target="#b19">[19]</ref>. Third, merging prompt tuning with AL takes work. Their interplay must be considered. However, previous AL methods <ref type="bibr" target="#b27">[27]</ref> did not consider the existence of prompts or use prompts to estimate sample value.</p><p>Therefore, this paper proposes the first framework for selective labeling and prompt tuning (SLPT), combining model-centric and data-centric methods to improve performance in medical label-limited scenarios. We make three main contributions: <ref type="bibr" target="#b0">(1)</ref> We design a novel feature-aware prompt updater embedded in the pre-trained model to guide prompt tuning in deep layers. <ref type="bibr" target="#b1">(2)</ref> We propose a diversified visual prompt tuning mechanism that provides multi-prompt-based discrepant predictions for selective labeling. <ref type="bibr" target="#b2">(3)</ref> We introduce the TESLA strategy which includes both unsupervised diversity selection via task-agnostic features and supervised selection considering prompt-based uncertainty. The results show that SLPT outperforms fine-tuning with just 6% of tunable parameters and achieves 94% of full-data performance by selecting only 5% of labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Given a task-agnostic pre-trained model and unlabeled data for an initial medical task, we propose SLPT to improve model performance. SLPT consists of three components, as illustrated in Fig. <ref type="figure">1:</ref> (a) a prompt-based visual model, (b) diversified visual prompt tuning, and (c) tandem selective labeling. Specifically, with SLPT, we can select valuable data to label and tune the model via prompts, which helps the model overcome label-limited medical scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Prompt-Based Visual Model</head><p>The pre-trained model, learned by supervised or unsupervised training, is a powerful tool for improving performance on label-limited downstream tasks. Finetuning a large pre-trained model with limited data may be suboptimal and prone to overfitting <ref type="bibr" target="#b16">[16]</ref>. To overcome this issue, we draw inspiration from NLP <ref type="bibr" target="#b18">[18]</ref> and explore prompt tuning on visual models. In order to facilitate prompt tuning on the model's deep layers, we introduce the Feature-aware Prompt Updater (FPU). FPUs are inserted into the network to update deep prompts and features. In Fig. <ref type="figure">1</ref>(a), an FPU receives two inputs, feature map F out i-1 and prompt P i-1 , of the same shape, and updates to F i and P i through two parallel branches. In the feature branch, F out i-1 and P i-1 are concatenated and fed into a 1x1 convolution and fusion module. The fusion module utilizes ASPP <ref type="bibr" target="#b6">[7]</ref> to extract multi-scale contexts. Then a SE <ref type="bibr" target="#b10">[11]</ref> module for channel attention enhances context by channel. Finally, the attention output and F out i-1 are element-wise multiplied and added to obtain the updated feature F i . In the prompt branch, the updated feature F i is concatenated with the previous prompt P i-1 , and a parameter-efficient depth-separable convolution is employed to generate the updated prompt P i .</p><p>To incorporate FPU into a pre-trained model, we consider the model comprising N modular M i (i = 1, ..., N ) and a head output layer. After each M i , we insert an F P U i . Given the input F in i-1 and prompt P i-1 , we have the output feature F i , updated prompt P i and prediction Y as follows:</p><formula xml:id="formula_0">F out i-1 = M i (F in i-1 ), F i , P i = FPU i (F out i-1 , P i-1 ), Y = Head(F N )<label>(1)</label></formula><p>where input X = F 0 , FPU and Head are tuned while M i is not tunable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Diversified Visual Prompt Tuning</head><p>Inspired by multi-prompt learning <ref type="bibr" target="#b18">[18]</ref> in NLP, we investigate using multiple visual prompts to evaluate prompt-based uncertainty. However, initializing and optimizing K prompts directly can significantly increase parameters and may not ensure prompt diversity. To address these challenges, we propose a diversified visual prompt tuning approach. As shown in Fig. <ref type="figure">1</ref>(b), our method generates K prompts P k ∈ R 1×D×H×W from a meta prompt</p><formula xml:id="formula_1">P M ∈ R 1× D 2 × H 2 × W 2</formula><p>through K different upsampling and convolution operations UpConv k . P M is initialized from the statistical probability map of the foreground category, similar to <ref type="bibr" target="#b17">[17]</ref>. Specifically, we set the foreground to 1 and the background to 0 in the groundtruth mask, and then average all masks and downsample to 1</p><formula xml:id="formula_2">× D 2 × H 2 × W 2 .</formula><p>To enhance prompt diversity, we introduce a prompt diversity loss L div that regularizes the cosine similarity between the generated prompts and maximizes their diversity. This loss is formulated as follows:</p><formula xml:id="formula_3">L div = K-1 k1=1 K k2=k1+1 P k1 • P k2 ||P k1 || 2 • ||P k 2 || 2<label>(2)</label></formula><p>where P k1 and P k2 represent the k 1 -th and k 2 -th generated prompts, respectively, and || • || 2 denotes the L2 norm. By incorporating the prompt diversity loss, we aim to generate a set of diverse prompts for our visual model. In NLP, using multiple prompts can produce discrepant predictions <ref type="bibr" target="#b1">[2]</ref> that help estimate prompt-based uncertainty. Drawing inspiration, we propose a visual prompt tuning approach that associates diverse prompts with discrepant predictions. To achieve this, we design K different data augmentation, heads, and losses based on corresponding K prompts. By varying hyperparameters, we can achieve different data augmentation strengths, increasing the model's diversity and generalization. Different predictions Y k are generated by K heads, each </p><formula xml:id="formula_4">P k = UpConv k (P M ), X k = DA k (X), Y k = Head k (M F P U (X k , P k )) (3) L = K k=1 (λ 1 * T L k (Y k , Y ) + λ 2 * CE(Y k , Y )) + λ 3 * L div (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where k = 1, ..., K, M F P U is the pre-trained model with FPU, CE is the crossentropy loss, and λ 1 = λ 2 = λ 3 = 1 weight each loss component. Y represents the ground truth and L is the total loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tandem Selective Labeling</head><p>Previous studies overlook the critical issue of data selection for downstream tasks, especially when available labels are limited. To address this challenge, we propose a novel strategy called TESLA. TESLA consists of two tandem steps: unsupervised diversity selection and supervised uncertainty selection. The first step aims to maximize the diversity of the selected data, while the second step aims to select the most uncertain samples based on diverse prompts.</p><p>Step 0: Unsupervised Diversity Selection. Since we do not have any labels in the initial and our pre-trained model is task-agnostic, we select diverse samples to cover the entire dataset. To achieve this, we leverage the pre-trained model to obtain feature representations for all unlabeled data. Although these features are task-independent, they capture the underlying relationships, with similar samples having closer feature distances. We apply the k-center method from Coreset <ref type="bibr" target="#b22">[22]</ref>, which identifies the B samples that best represent the diversity of the data based on these features. These selected samples are then annotated and serve as the initial dataset for downstream tasks.</p><p>Step 1: Supervised Uncertainty Selection. After prompt tuning with the initial dataset, we obtain a task-specific model that can be used to evaluate data value under supervised training. Since only prompt-related parameters can be tuned while others are frozen, we assess prompt-based uncertainty via diverse prompts, considering inter-prompts uncertainty and intra-prompts uncertainty.</p><p>In the former, we compute the multi-prompt-based divergence map D, given K probability predictions Y k through K diverse prompts P k , as follows:</p><formula xml:id="formula_6">D = K k=1 KL(Y k ||Y mean ), Y mean = 1 K K k=1 Y k (5)</formula><p>where KL refers to the KL divergence <ref type="bibr" target="#b15">[15]</ref>. Then, we have the divergence score S d = Mean(D), which reflects inter-prompts uncertainty.</p><p>In the latter, we evaluate intra-prompts uncertainty by computing the mean prediction of the prompts and propose to estimate prompt-based gradients as the model's performance depends on the update of prompt parameters θ p . However, for these unlabeled samples, computing their supervised loss and gradient directly is not feasible. Therefore, we use the entropy of the model's predictions as a proxy for loss. Specifically, we calculate the entropy-based prompt gradient score S g for each unlabeled sample as follows:</p><formula xml:id="formula_7">S g = θp ||∇ θp (- Y mean * log Y mean )|| 2<label>(6)</label></formula><p>To avoid manual weight adjustment, we employ multiplication instead of addition. We calculate our uncertainty score S as follows:</p><formula xml:id="formula_8">S = S d max(S d ) × S g max(S g )<label>(7)</label></formula><p>where max(•) finds the maximum value. We sort the unlabeled data by their corresponding S values in ascending order and select the top B data to annotate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Datasets and Pre-trained Model. We conducted experiments on automating liver tumor segmentation in contrast-enhanced CT scans, a crucial task in liver cancer diagnosis and surgical planning <ref type="bibr" target="#b0">[1]</ref>. Although there are publicly available liver tumor datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">24]</ref>, they only contain major tumor types and differ in image characteristics and label distribution from our hospital's data. Deploying a model trained from public data to our hospital directly will be problematic.</p><p>Collecting large-scale data from our hospital and training a new model will be expensive. Therefore, we can use the model trained from them as a starting point and use SLPT to adapt it to our hospital with minimum cost. We collected a dataset from our in-house hospital comprising 941 CT scans with eight categories: hepatocellular carcinoma, cholangioma, metastasis, hepatoblastoma, hemangioma, focal nodular hyperplasia, cyst, and others. It covers both major and rare tumor types. Our objective is to segment all types of lesions accurately. We utilized a pre-trained model for liver segmentation using supervised learning on two public datasets <ref type="bibr" target="#b24">[24]</ref> with no data overlap with our downstream task. The nnUNet <ref type="bibr" target="#b11">[12]</ref> was used to preprocess and sample the data into 24 × 256 × 256 patches for training. To evaluate the performance, we employed a 5-fold crossvalidation (752 for selection, 189 for test).</p><p>Metrics. We evaluated lesion segmentation performance using pixel-wise and lesion-wise metrics. For pixel-wise evaluation, we used the Dice per case, a commonly used metric <ref type="bibr" target="#b0">[1]</ref>. For lesion-wise evaluation, we first do connected component analysis to predicted and ground truth masks to extract lesion instances, and then compute precision and recall per case <ref type="bibr" target="#b20">[20]</ref>. A predicted lesion is regarded as a TP if its overlap with ground truth is higher than 0.2 in Dice. Competing Approaches. In the prompt tuning experiment, we compared our method with three types of tuning: full parameter update (Fine-tuning, Learn-from-Scratch), partial parameter update (Head-tuning, Encoder-tuning, Decoder-tuning), and prompt update (SPM <ref type="bibr" target="#b17">[17]</ref>). In the unsupervised diversity selection experiment, we compared our method with random sampling. In the supervised uncertainty selection experiment, we compared our method with random sampling, diversity sampling (Coreset <ref type="bibr" target="#b22">[22]</ref>, CoreCGN <ref type="bibr" target="#b5">[6]</ref>), and uncertainty sampling (Entropy, MC Dropout <ref type="bibr" target="#b9">[10]</ref>, Ensemble <ref type="bibr" target="#b3">[4]</ref>, UncertainGCN <ref type="bibr" target="#b5">[6]</ref>, Ent-gn <ref type="bibr" target="#b26">[26]</ref>). Unlike Ensemble, our method was on multi-prompt-based heads. Furthermore, unlike Ent-gn, which computed the entropy-based gradient from a single prediction, we calculated a stable entropy from the muti-prompt-based mean predictions and solely considered the prompt gradient.</p><p>Training Setup. We conducted the experiments using the Pytorch framework on a single NVIDIA Tesla V100 GPU. The nnUNet <ref type="bibr" target="#b11">[12]</ref> framework was used for 3D lesion segmentation with training 500 epochs at an initial learning rate of 0.01. We integrated 13 FPUs behind each upsampling or downsampling of nnUNet, adding only 2.7M parameters. During training, we set k = 3 and employed diverse data augmentation techniques such as scale, elastic, rotation, and mirror. Three sets of TL parameters is (α 1,2,3 = 0.5,0.7,0.3, β 1,2,3 = 0.5,0.3,0.7). To ensure fairness and eliminate model ensemble effects, we only used the model's prediction with k = 1 during testing. We used fixed random seeds and 5-fold cross-validation for all segmentation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Evaluation of Prompt Tuning. Since we aim to evaluate the efficacy of prompt tuning on limited labeled data in Table <ref type="table" target="#tab_0">1</ref>, we create a sub-dataset of approximately 5% (40/752) from the original dataset. Specifically, we calculate the class probability distribution vector for each sample based on the pixel class in the mask and use CoreSet with these vectors to select 40 class-balanced samples. Using this sub-dataset, we evaluated various tuning methods for limited  <ref type="table" target="#tab_0">1</ref>. Fine-tuning all parameters served as the strongest baseline, but our method, which utilizes only 6% tunable parameters, outperformed it by 5.4%. Although SPM also outperforms fine-tuning, our methods outperform SPM by 1.18% and save 0.44M tunable parameters with more efficient FPU. In cases of limited data, fine-tuning tends to overfit on a larger number of parameters, while prompt tuning does not. The pre-trained model is crucial for downstream tasks with limited data, as it improves performance by 9.52% compared to Learn-from-Scratch. Among the three partial tuning methods, the number of tuning parameters positively correlates with the model's performance, but they are challenging to surpass fine-tuning.</p><p>Evaluation of Selective Labeling. We conducted steps 0 (unsupervised selection) and 1 (supervised selection) from the unlabeled 752 data and compared our approach with other competing methods, as shown in Table <ref type="table" target="#tab_1">2</ref>. In step 0, without any labeled data, our diversity selection outperformed the random baseline by 1.86%. Building upon the 20 data points selected by our method in step 0, we proceeded to step 1, where we compared our method with eight other data selection strategies in supervised mode. As a result, our approach outperformed other methods because of prompt-based uncertainty, such as Ent-gn and Ensemble, by 2.05% and 1.46%, respectively. Our approach outperformed Coreset by 6.05% and CoreGCN by 5.43%. We also outperformed UncertainGCN by 1.93%. MC Dropout and Entropy underperformed in our prompt tuning, likely due to the difficulty of learning such uncertain data with only a few prompt parameters. Notably, our method outperformed random sampling by 10.28%. These results demonstrate the effectiveness of our data selection approach in practical tasks.</p><p>Ablation Studies. We conducted ablation studies on S d and S g in TESLA. As shown in Table <ref type="table" target="#tab_1">2</ref>, the complete TESLA achieved the best performance, outperforming the version without S d by 1.84% and the version without S g by 1.98%. It shows that each component plays a critical role in improving performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We proposed a pipeline called SLPT that enhances model performance in labellimited scenarios. With only 6% of tunable prompt parameters, SLPT outperforms fine-tuning due to the feature-aware prompt updater. Moreover, we presented a diversified visual prompt tuning and a TESLA strategy that combines unsupervised and supervised selection to build annotated datasets for downstream tasks. SLPT pipeline is a promising solution for practical medical tasks with limited data, providing good performance, few tunable parameters, and low labeling costs. Future work can explore the potential of SLPT in other domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 . 3 )</head><label>13</label><figDesc>Fig. 1. Workflow of SLPT: (1) Create an initial label set via the pre-trained model for unsupervised diversity selection (subplot c step 0). (2) Insert a feature-aware prompt updater (subplot a) into the pre-trained model for prompt tuning with initial labels. (3) Use diversified visual prompt tuning (subplot b) to obtain prompt-based discrepant predictions. (4) Select valuable data by prompt-based uncertainty (subplot c step 1) and update the prompt-based model accordingly. Note: The orange modules are tunable for prompt tuning, while the gray ones are frozen. Please zoom in for details.</figDesc><graphic coords="3,44,79,54,41,334,57,137,29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>F</head><label></label><figDesc>.Bai et al.    supervised with a Tversky loss<ref type="bibr" target="#b21">[21]</ref> T L k = T P T P +α k F P +β k F N , where TP, FP, and FN represent true positive, false positive, and false negative, respectively. To obtain diverse predictions with false positives and negatives, we use different α k and β k values in T L k . The process is formulated as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of different tunings on the lesion segmentation with limited data (40 class-balanced patients). Prec. and Rec. denote precision and recall.</figDesc><table><row><cell>Method</cell><cell>Tuning</cell><cell>Trainable</cell><cell>Pixel-wise</cell><cell>Lesion-wise</cell><cell>Mean</cell></row><row><cell></cell><cell>Type</cell><cell>Parameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dice Prec Rec</cell><cell>Prec Rec</cell><cell></cell></row><row><cell>Fine-tuning</cell><cell>All</cell><cell>44.81M</cell><cell cols="3">64.43 87.69 59.86 50.84 54.14 63.39</cell></row><row><cell>Learn-from-Scratch</cell><cell></cell><cell>44.81M</cell><cell cols="3">54.15 73.33 50.25 45.84 45.78 53.87</cell></row><row><cell>Encoder-tuning</cell><cell>Part</cell><cell>19.48M</cell><cell cols="3">65.61 82.00 61.96 29.36 41.10 56.00</cell></row><row><cell>Decoder-tuning</cell><cell></cell><cell>23.64M</cell><cell cols="3">67.87 77.96 70.56 30.82 35.92 56.63</cell></row><row><cell>Head-tuning</cell><cell></cell><cell>0.10M</cell><cell cols="3">56.73 74.45 55.57 23.29 29.74 47.96</cell></row><row><cell>SPM [17]</cell><cell>Prompt</cell><cell>3.15M</cell><cell cols="3">68.60 83.07 69.02 62.15 55.19 67.61</cell></row><row><cell>Ours</cell><cell></cell><cell>2.71M</cell><cell cols="3">68.76 79.63 69.76 64.63 61.18 68.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of data selection methods for label-limited lesion segmentation.Step 0: unsupervised diversity selection. Step 1: supervised uncertainty selection. The labeling budget for each step is 20 patients. Step +∞ refers to fully labeled 752 data.</figDesc><table><row><cell cols="2">Step Method</cell><cell>Pixel-wise</cell><cell>Lesion-wise</cell><cell>Mean</cell></row><row><cell></cell><cell></cell><cell>Dice Prec Rec</cell><cell>Prec Rec</cell></row><row><cell>0</cell><cell>Random</cell><cell cols="3">65.58 80.00 65.21 23.46 39.94 54.84</cell></row><row><cell></cell><cell>Ours</cell><cell cols="3">68.20 78.97 69.15 32.51 34.67 56.70</cell></row><row><cell>1</cell><cell>Random</cell><cell cols="3">66.67 79.95 70.67 41.45 39.45 59.64</cell></row><row><cell></cell><cell>Entropy</cell><cell cols="3">66.39 80.85 66.96 37.40 39.47 58.21</cell></row><row><cell></cell><cell>MC Dropout [10]</cell><cell cols="3">69.23 79.61 69.48 30.43 36.29 57.01</cell></row><row><cell></cell><cell>Ensemble [4]</cell><cell cols="3">69.79 80.25 69.54 64.38 58.34 68.46</cell></row><row><cell></cell><cell>CoreSet [22]</cell><cell cols="3">70.72 79.34 72.03 46.03 51.24 63.87</cell></row><row><cell></cell><cell>CoreGCN [6]</cell><cell cols="3">70.91 77.56 72.37 51.73 49.88 64.49</cell></row><row><cell></cell><cell>UncertainGCN [6]</cell><cell cols="3">71.44 75.07 75.62 72.83 44.99 67.99</cell></row><row><cell></cell><cell>Ent-gn [26]</cell><cell cols="3">70.54 79.91 71.42 61.12 56.37 67.87</cell></row><row><cell></cell><cell>Ours (w/o S d )</cell><cell cols="3">69.54 81.97 68.59 60.47 59.82 68.08</cell></row><row><cell></cell><cell>Ours (w/o S g )</cell><cell cols="3">71.01 80.68 69.83 59.42 58.78 67.94</cell></row><row><cell></cell><cell>Ours</cell><cell cols="3">72.07 82.07 72.37 61.21 61.90 69.92</cell></row><row><cell cols="2">+∞ Fine-tuning with</cell><cell cols="3">77.44 85.44 77.15 62.78 68.56 74.27</cell></row><row><cell></cell><cell>Full Labeled Data</cell><cell></cell><cell></cell></row><row><cell cols="4">medical lesion diagnosis data. The results are summarized in Table</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The work was supported by <rs type="programName">Alibaba Research Intern Program</rs>. <rs type="person">Fan Bai</rs> and <rs type="person">Max Q.-H. Meng</rs> were supported by <rs type="funder">National Key R&amp;D program of China</rs> with Grant No. <rs type="grantNumber">2019YFB1312400</rs>, <rs type="funder">Hong Kong RGC CRF</rs> grant <rs type="grantNumber">C4063-18G</rs>, and <rs type="funder">Hong Kong Health and Medical Research Fund (HMRF)</rs> under Grant <rs type="grantNumber">06171066</rs>. <rs type="person">Xiaoli Yin</rs> and <rs type="person">Yu Shi</rs> were supported by <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">82071885</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZU7AwAN">
					<idno type="grant-number">2019YFB1312400</idno>
					<orgName type="program" subtype="full">Alibaba Research Intern Program</orgName>
				</org>
				<org type="funding" xml:id="_Vd84NdQ">
					<idno type="grant-number">C4063-18G</idno>
				</org>
				<org type="funding" xml:id="_r9qDKPX">
					<idno type="grant-number">06171066</idno>
				</org>
				<org type="funding" xml:id="_ZB7Vbhb">
					<idno type="grant-number">82071885</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The liver tumor segmentation benchmark (LiTS)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">102680</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A simple zero-shot prompt weighting technique to improve prompt ensembling in text-image models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">U</forename><surname>Allingham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06235</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discrepancy-based active learning for weakly supervised bleeding segmentation in wireless capsule endoscopy images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q H</forename><surname>Meng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-13" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The power of ensembles for active learning in image classification</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Beluch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nürnberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Köhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9368" to="9377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1876" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sequential graph convolutional network for active learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caramalau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhattarai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9583" to="9592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Not-so-supervised: a survey of semisupervised, multi-instance, and transfer learning in medical image analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="280" to="296" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Suggestive annotation of brain tumour images with gradient-guided sampling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-116" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual prompt tuning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13693</biblScope>
			<biblScope unit="page" from="709" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19827-4_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19827-441" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning in medical imaging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurospine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">657</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10054</idno>
		<title level="m">Fine-tuning can distort pretrained features and underperform out-of-distribution</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.10159</idno>
		<title level="m">Prompt-matched semantic segmentation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Active learning by feature mixing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parvaneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12237" to="12246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Powers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16061</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tversky loss function for image segmentation using 3D fully convolutional deep networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-67389-9_44</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-67389-944" />
	</analytic>
	<monogr>
		<title level="m">MLMI 2017</title>
		<editor>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10541</biblScope>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Active learning for convolutional neural networks: a core-set approach</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00489</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A large annotated medical image dataset for the development and evaluation of segmentation algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Simpson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09063</idno>
		<editor>F. Bai et al.</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Embracing imperfect datasets: a review of deep learning solutions for medical image segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jeyaseelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101693</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Boosting active learning via improving test performance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="8566" to="8574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13450</idno>
		<title level="m">A comparative survey of deep active learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Prompt design for text classification with transformer-based models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2709" to="2722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2337" to="2348" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
