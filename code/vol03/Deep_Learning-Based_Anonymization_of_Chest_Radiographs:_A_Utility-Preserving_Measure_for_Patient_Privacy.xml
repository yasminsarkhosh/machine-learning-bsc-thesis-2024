<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy</title>
				<funder>
					<orgName type="full">German Research Foundation (DFG)</orgName>
				</funder>
				<funder ref="#_58xtwMK">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
					<orgName type="abbreviated">FAU</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Kai</forename><surname>Packhäuser</surname></persName>
							<email>kai.packhaeuser@fau.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Gündel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florian</forename><surname>Thamm</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felix</forename><surname>Denzinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Maier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="262" to="272"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">4493EAE4686B825B0753993626F265C3</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_26</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Anonymization</term>
					<term>Patient Privacy</term>
					<term>Data Utility</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robust and reliable anonymization of chest radiographs constitutes an essential step before publishing large datasets of such for research purposes. The conventional anonymization process is carried out by obscuring personal information in the images with black boxes and removing or replacing meta-information. However, such simple measures retain biometric information in the chest radiographs, allowing patients to be re-identified by a linkage attack. Therefore, there is an urgent need to obfuscate the biometric information appearing in the images. We propose the first deep learning-based approach (PriCheXy-Net) to targetedly anonymize chest radiographs while maintaining data utility for diagnostic and machine learning purposes. Our model architecture is a composition of three independent neural networks that, when collectively used, allow for learning a deformation field that is able to impede patient re-identification. Quantitative results on the ChestX-ray14 dataset show a reduction of patient re-identification from 81.8% to 57.7% (AUC) after re-training with little impact on the abnormality classification performance. This indicates the ability to preserve underlying abnormality patterns while increasing patient privacy. Lastly, we compare our proposed anonymization approach with two other obfuscation-based methods (Privacy-Net, DP-Pix) and demonstrate the superiority of our method towards resolving the privacyutility trade-off for chest radiographs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning (DL) <ref type="bibr" target="#b18">[19]</ref> has positively contributed to the development of diagnostic algorithms for the detection and classification of lung diseases in recent years <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>. This progress can largely be attributed to the availability of public chest X-ray datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>. However, as chest radiographs inherently contain biometric information (similar to a fingerprint), the public release of such data bears the risk of automated patient re-identification by DL-based linkage attacks <ref type="bibr" target="#b21">[22]</ref>. This would allow patient-related information, e. g., age, gender, or disease findings to be revealed. Therefore, there is an urgent need for stronger privacy mechanisms for chest X-ray data to alleviate the risk of linkage attacks.</p><p>Perturbation-based anonymization approaches <ref type="bibr" target="#b15">[16]</ref> -such as differential privacy (DP) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> -have become the gold standard to obfuscate biometric identifiers from sensitive data. Such approaches are based on the postulate that the global statistical distribution of a dataset is retained while personal information is reduced. This can be realized by applying randomized modifications, i. e. noise, to either the inputs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>, the computational results, or to algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>. Although originally proposed for statistical data, DP has been extended to image data with the differentially private pixelization method (DP-Pix) <ref type="bibr">[9,</ref><ref type="bibr" target="#b9">10]</ref>. This method involves pixelizing an image by averaging the pixel values of each b × b grid cell, followed by adding Laplace noise with 0 mean and</p><formula xml:id="formula_0">255m b 2</formula><p>scale. Parameter is used to determine the privacy budget (smaller values indicate greater privacy), while the m-neighborhood represents a sensitivity factor. However, one major drawback of perturbation-based anonymization is the potential degradation of image quality and an associated reduction in data utility.</p><p>In recent years, DL has emerged as a prominent tool for anonymizing medical images. In this context, synthetic image generation with privacy guarantees is currently actively explored, aimed at creating fully anonymous medical image datasets <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>. Furthermore, adversarial approaches -such as Privacy-Net <ref type="bibr" target="#b16">[17]</ref> -have been proposed, which focus on concealing biometric information while maintaining data utility. Privacy-Net is composed of a U-Net encoder that predicts an anonymized image, an identity discriminator, and a task-specific segmentation network. Through the dual process of deceiving the discriminator and optimizing the segmentation network, the encoder acquires the ability to obfuscate patient-specific patterns, while preserving those necessary for the downstream task. However, while originally designed for utility preservation on MRI segmentation tasks, the direct applicability and transferability of Privacy-Net to other image modalities and downstream tasks (e. g. chest X-ray classification) is limited. As we will experimentally demonstrate in our study, more sophisticated constraints are required for the utility-preserving anonymization of chest radiographs in order to successfully maintain fine-grained abnormality details that are crucial for reliable classification tasks.</p><p>In this work, we aim to resolve the privacy-utility trade-off by proposing theto the best of our knowledge -first adversarial image anonymization approach for chest radiography data. Our proposed model architecture (PriCheXy-Net) is a composition of three independent neural networks that collectively allow for the learning of targeted image deformations to deceive a well-trained patient verification model. We apply our method to the publicly available ChestX-ray14 dataset <ref type="bibr" target="#b26">[27]</ref> and evaluate the impact of different deformation degrees on anonymization capability and utility preservation. To evaluate the effectiveness of image anonymization, we perform linkage attacks on anonymized data and analyze the respective success rates. Furthermore, to quantify the extent of data utility preservation despite the induced image deformations, we compare the performance of a trained thoracic abnormality classification system on anonymized images versus the performance on real data. Throughout our study, we utilize Privacy-Net <ref type="bibr" target="#b16">[17]</ref> and DP-Pix [9,10] as baseline obfuscation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>We use X-ray data from the ChestX-ray14 dataset <ref type="bibr" target="#b26">[27]</ref>, a large-scale collection of 112,120 frontal-view chest radiographs from 30,805 unique patients. The 8bit gray-scale images are provided with a resolution of 1024×1024 pixels. We resize the images to a size of 256×256 pixels for further processing. On average, the dataset contains ≈ 3.6 images per patient. The corresponding 14 abnormality labels include Atelectasis, Cardiomegaly, Consolidation, Edema, Effusion, Emphysema, Fibrosis, Hernia, Infiltration, Mass, Nodule, Pleural Thickening, Pneumonia, and Pneumothorax. A patient-wise splitting strategy is applied to roughly divide the data into a training, validation, and test set by ratio 70:10:20, respectively. Based on this split, we utilize available follow-up scans to randomly construct positive image pairs (two unique images from the same patient) and negative image pairs (two unique images from two different patients) -10,000 for training, 2,000 for validation, and 5,000 for testing. The resulting subsets are balanced with respect to the number of positive and negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">PriCheXy-Net: Adversarial Image Anonymization</head><p>The proposed adversarial image anonymization approach is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. It is composed of three trainable components: (1) A U-Net generator G that predicts a flow field F used to deform the original image x 1 of abnormality class y c , (2) an auxiliary classifier that takes the modified image F (x 1 ) resulting in corresponding class predictions ŷc , and (3) a siamese neural network (SNN) that receives the deformed image F (x 1 ) as well as another real image x 2 of either the same or a different patient and yields the similarity score ŷv for patient verification. The U-Net serves as an anonymization tool aiming to obfuscate biometric information through targeted image deformations, while the auxiliary classifier and the patient verification model contribute as guidance to optimize the flow field generator. <ref type="bibr" target="#b25">[26]</ref> architecture is implemented according to Buda et al. <ref type="bibr" target="#b2">[3]</ref>. Its last sigmoid activation function is replaced by a hyperbolic tangent activation function to predict a 2-channel flow field F , bounded by [-1, 1]. During training, especially in early stages, the raw output of the U-Net may lead to random deformations that destroy the content of the original images, thus revoking the diagnostic utility. To circumvent this issue, the following constraints are imposed for F . First, to ensure that the learned flow field F does not substantially deviate from the identity F id , it is weighted with factor μ and subsequently subtracted from the identity according to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>U-Net. The U-Net</head><formula xml:id="formula_1">F = F id -μF.</formula><p>(</p><p>Factor μ controls the degree of deformation, with larger values allowing for more deformation. Note that the exclusive use of F id would result in the original image, i. e., x = F (x), assuming the deformation factor is being set to μ = 0. The resulting flow field F is Gaussian filtered (kernel size 9, σ = 2) to ensure smooth deformations in the final image. The corresponding parameters were selected manually in preliminary experiments.</p><p>Auxiliary Classifier. To ensure the preservation of underlying abnormality patterns and image utility during deformation, PriCheXy-Net integrates an auxiliary classifier using CheXNet <ref type="bibr" target="#b24">[25]</ref>, a densely connected convolutional network (DenseNet) <ref type="bibr" target="#b12">[13]</ref> consisting of 121 layers. It outputs a 14-dimensional probability vector ŷc indicating the presence or absence of each abnormality appearing in the ChestX-ray14 dataset. Its parameters θ aux are initialized using a pre-trained model that achieves a mean AUC of 80.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patient Verification Network.</head><p>The incorporated patient verification model is represented by the SNN architecture presented by Packhäuser et al. <ref type="bibr" target="#b21">[22]</ref>, consisting of two ResNet-50 <ref type="bibr" target="#b11">[12]</ref> branches that are merged using the absolute difference of their resulting 128-dimensional feature vectors. A fully-connected layer with a sigmoid activation function produces the final verification score ŷv in the value range of [0, 1], indicating the probability of whether or not the two input images belong to the same patient. For initializing the network parameters θ ver , we employ a pre-trained network that has been created according to <ref type="bibr" target="#b21">[22]</ref> yielding an AUC value of 99.4% for a patient verification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Objective Functions</head><p>Similar to most adversarial models, our system undergoes training through the use of dual loss functions that guide the model towards opposing directions. To enforce the U-Net not to eliminate important class information while deforming a chest radiograph, we introduce the auxiliary classifier loss L aux (θ G , θ aux ) realized by the class-wise binary cross entropy (BCE) loss according to Eq. 2</p><formula xml:id="formula_3">L aux (θ G , θ aux ) = - 14 i=1 [y c,i log(ŷ c,i ) + (1 -y c,i ) log(1 -ŷc,i )],<label>(2)</label></formula><p>where i represents one out of 14 abnormality classes. Conversely, to guide the U-Net with deceiving the incorporated patient verification model, we utilize its output as an additional verification loss term L ver (θ G , θ ver ) (see Eq. 3):</p><formula xml:id="formula_4">L ver (θ G , θ ver ) = -log(1 -ŷv ) , with ŷv = SNN F (x 1 ), x 2<label>(3)</label></formula><p>The total loss to be minimized (see Eq. 4) results from the sum of the two partial losses L aux (θ G , θ aux ) and L ver (θ G , θ ver ):</p><formula xml:id="formula_5">arg min θG L(θ G , θ aux , θ ver ) = L aux (θ G , θ aux ) + L ver (θ G , θ ver )<label>(4)</label></formula><p>Lastly, both the auxiliary classifier and the verification model are updated by minimizing the loss terms in Eq. 5 and Eq. 6, respectively. Note that the similarity labels for positive and negative pairs are encoded using y v = 1 and y v = 0. 3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>For all experiments, we used PyTorch (1.10.2) <ref type="bibr" target="#b22">[23]</ref> and Python (3.9.5). We followed a multi-part experimental setup consisting of the following steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training of the Flow Field</head><p>Generator. The incorporated U-Net architecture was pre-trained on an autoencoder-like reconstruction task for 200 epochs using the mean squared error (MSE) loss, Adam <ref type="bibr" target="#b17">[18]</ref>, a batch size of 64 and a learning rate of 10 -4 to enable faster convergence. The model that performed best on the validation set was then used for weight initialization in step 2.</p><p>Training of PriCheXy-Net. After pre-training, PriCheXy-Net was trained in an end-to-end fashion for 250 epochs using the Adam optimizer <ref type="bibr" target="#b17">[18]</ref>, a batch size of 64 and a learning rate of 10 -4 using the objective functions presented above. To evaluate the effect of the deformation degree μ on anonymization capability and image utility, we performed multiple training runs with various values μ ∈ {0.001, 0.005, 0.01}. For each configuration, the U-Net that performed best on the validation set was then used for further evaluations in steps 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re-training and Evaluation of the Verification Model.</head><p>To assess the anonymization capability of PriCheXy-Net and to determine if the anonymized images can reliably deceive the verification model, we re-trained the incorporated SNN for each model configuration by using deformed images only. We then simulated multiple linkage attacks by comparing deformed images with real ones.</p><p>Training was conducted until early stopping (patience p = 5) using the BCE loss, the Adam optimizer <ref type="bibr" target="#b17">[18]</ref>, a batch size of 32 and a learning rate of 10 -4 . For each model configuration, we performed 10 independent training and testing runs. We report the means and standard deviations of the resulting AUC values.</p><p>Evaluation of the Classification Model on Anonymized Data. To assess the extent to which underlying abnormalities, and thus data utility, were preserved during the anonymization process, each individually trained anonymization network was used to perturb the images of our test set. Then, the pre-trained auxiliary classifier was evaluated using the resulting images. We report the mean of the 14 class-wise AUC values. To quantify the uncertainty, the 95% confidence intervals (CIs) from 1,000 bootstrap runs were computed.</p><p>Comparison with Other Obfuscation-Based Methods. To compare our proposed system with other obfuscation-based methods, we additionally analyzed the anonymization capability and utility preservation of Privacy-Net <ref type="bibr" target="#b16">[17]</ref> and DP-Pix [9,10]. Since Privacy-Net was originally proposed for segmentation tasks, we replaced its segmentation component with the auxiliary classifier. Then, the network was trained and evaluated in the exact same setting as PriCheXy-Net. For DP-Pix, we investigated the effect of different cell sizes b ∈ {1, 2, 4, 8} at a common privacy budget = 0.1. The m-neighborhood was set to the smallest possible value (m = 1) to prevent the added Laplace noise (mean: 0; scale: 255m b 2 ) from destroying the complete content of the images.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Baseline and Comparison Methods. The results of all conducted experiments are shown in Table <ref type="table" target="#tab_0">1</ref>. Compared to real (non-anonymized) data, which enables a successful re-identification with an AUC of 81.8%, the patient verification performance desirably decreases after applying DP-Pix (50.0%-52.5%) and Privacy-Net (49.8%). However, while the classification performance on real data indicates a high data utility with a mean AUC of 80.5%, we observe a sharp drop for images that have been modified with DP-Pix (50.0%-52.9%) and Privacy-Net (57.5%). This suggests that relevant class information and specific abnormality patterns are destroyed during the obfuscation process. Resulting example images for both comparison methods are provided in Suppl. Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PriCheXy-Net.</head><p>The results of our proposed PriCheXy-Net (see Table <ref type="table" target="#tab_0">1</ref>) show more promising behavior. As can be seen, increasing deformation degrees μ lead to a successive decline in patient verification performance. Compared to the baseline, the AUC decreases to 74.7% (μ = 0.001), to 64.3% (μ = 0.005), and to 57.7% (μ = 0.01), indicating a positive effect on patient privacy and the obfuscation of biometric information. In addition, PriCheXy-Net hardly results in a loss of data utility, as characterized by the constantly high classification performance with a mean AUC of 80.4% (μ = 0.001), 79.3% (μ = 0.005), and 76.2% (μ = 0.01). These findings are further visualized in the privacy-utility trade-off plot in Fig. <ref type="figure" target="#fig_3">2</ref>. In contrast to the examined comparison methods, the data point corresponding to our best experiment with PriCheXy-Net (green) lies near the top right corner, highlighting the capability to closely satisfy both objectives in the privacy-utility trade-off. Examples of deformed chest radiographs resulting from a trained model of PriCheXy-Net are shown in Fig. <ref type="figure" target="#fig_4">3</ref>. More examples are given in Suppl. Fig. <ref type="figure" target="#fig_0">1</ref>. Difference maps are provided in Suppl. Fig. <ref type="figure" target="#fig_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>To the best of our knowledge, we presented the first adversarial approach to anonymize thoracic images while preserving data utility for diagnostic purposes. Our proposed anonymization approach -PriCheXy-Net -is a composition of three independent neural networks consisting of (1) a flow field generator, (2) an auxiliary classifier, and (3) a patient verification network. In this work, we were able to show that collective utilization of these three components enables learning of a flow field that targetedly deforms chest radiographs and thus reliably deceives a patient verification model, even after re-training was performed. For the best hyper-parameter configuration of PriCheXy-Net, the re-identification performance drops from 81.8% to 57.7% in AUC for a simulated linkage attack, whereas the abnormality classification performance only decreases from 80.5% to 76.2%, which indicates the effectiveness of the proposed approach. We strongly hypothesize that the promising performance of PriCheXy-Net can be largely attributed to the constraints imposed on the learned flow field F . The limited deviation of the flow field from the identity (cf. Eq. 1) ensures a realistic appearance of the resulting deformed image to a considerable extent, thereby avoiding its content from being completely destroyed. This idea has a positive impact on preserving relevant abnormality patterns in chest radiographs, while allowing adequate scope to obfuscate biometric information. Such domain-specific constraints are not integrated in examined comparison methods such as Privacy-Net (which directly predicts an anonymized image without ensuring realism) and DP-Pix (which does not contain any mechanism to maintain data utility). This is, as we hypothesize, the primary reason for their limited ability to preserve data utility and the overall superiority of our proposed system. Interestingly, PriCheXy-Net's deformation fields primarily focus on anatomical structures, including lungs and ribs, as demonstrated in Fig. <ref type="figure" target="#fig_4">3</ref>, Suppl. 1, and Suppl. Fig. <ref type="figure" target="#fig_3">2</ref>. This observation aligns with Packhäuser et al.'s previous findings <ref type="bibr" target="#b21">[22]</ref>, which revealed that these structures contain the principal biometric information in chest radiographs.</p><p>In future work, we aim to further improve the performance of PriCheXy-Net by incorporating additional components into its current architecture. For instance, we plan to integrate a discriminator loss into the model, which may positively contribute to achieving perceptual realism. Furthermore, we also consider implementing a region of interest segmentation step into the pipeline to ensure not to perturb diagnostically relevant image areas. Lastly, we hypothesize that our method is robust to variations in image size or compression rate, and posit its applicability beyond chest X-rays to other imaging modalities as well. However, confirmation of these hypotheses requires further exploration to be conducted in forthcoming studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. PriCheXy-Net: Proposed adversarial image anonymization architecture.</figDesc><graphic coords="3,41,79,79,70,221,95,101,38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>v log ŷv -(1 -y v ) log(1ŷv )] (6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual illustration of the results and the associated privacy-utility trade-off. The patient verification performance (y-axis) measures the amount of privacy, whereas the abnormality classification performance (x-axis) represents the level of data utility.</figDesc><graphic coords="7,41,79,254,51,340,21,76,57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Chest radiographs resulting from PriCheXy-Net when using different deformation degrees µ. Images were cropped to better highlight the diagnostically relevant area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison of all examined methods. The baseline performance results from leveraging non-anonymized real data. Verification: AUC (mean ± std) over 10 independent training and testing runs; Classification: mean AUC + 95% CIs. Performance scores of 50% indicate random decisions.</figDesc><table><row><cell></cell><cell>Baseline</cell><cell cols="3">Privacy-Net PriCheXy-Net (Ours)</cell></row><row><cell>Task</cell><cell cols="2">(real data) [17]</cell><cell cols="3">µ = 0.001 µ = 0.005 µ = 0.01</cell></row><row><cell>Ver. ↓</cell><cell cols="2">81.8 ± 0.6 49.8 ± 2.2</cell><cell cols="3">74.7 ± 2.6 64.3 ± 5.8 57.7 ± 4.0</cell></row><row><cell cols="2">Class. ↑ 80.5 80.9 80.1</cell><cell>57.5 58.1 56.9</cell><cell>80.4 80.8 79.9</cell><cell>79.3 79.7 78.9</cell><cell>76.2 76.6 75.8</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. The research leading to these results has received funding from the <rs type="funder">European Research Council (ERC)</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation program</rs> (<rs type="grantName">ERC Grant</rs> no. <rs type="grantNumber">810316</rs>). The authors gratefully acknowledge the scientific support and HPC resources provided by the <rs type="funder">Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)</rs>. The hardware is funded by the <rs type="funder">German Research Foundation (DFG)</rs>. The authors declare that they have no conflicts of interest.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_58xtwMK">
					<idno type="grant-number">810316</idno>
					<orgName type="grant-name">ERC Grant</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation program</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Use</head><p>Declaration. This research study was conducted retrospectively using human subject data made available in open access by the National Institutes of Health (NIH) Clinical Center <ref type="bibr" target="#b26">[27]</ref>. Ethical approval was not required as confirmed by the license attached with the open-access data.</p><p>Code Availability. The source code of this study has been made available at https://github.com/kaipackhaeuser/PriCheXy-Net.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 26.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning with differential privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="308" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning with gaussian differential privacy</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harv. Data Sci. Rev</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">23</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Association of genomic subtypes of lowergrade gliomas with shape features automatically extracted by a deep learning algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PadChest: a large chest x-ray image dataset with multi-label annotated reports</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bustos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pertusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De La Iglesia-Vayá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">101797</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">COVID-19 image data collection: prospective predictions are the future</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11988</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02383</idno>
		<title level="m">Gaussian differential privacy</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Differential privacy: a survey of results</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-79228-4_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-79228-41" />
	</analytic>
	<monogr>
		<title level="m">TAMC 2008</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">4978</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Calibrating noise to sensitivity in private data analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1007/11681878_14</idno>
		<ptr target="https://doi.org/10.1007/1168187814" />
	</analytic>
	<monogr>
		<title level="m">TCC 2006</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Halevi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Rabin</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3876</biblScope>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image pixelization with differential privacy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-95729-6_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-95729-610" />
	</analytic>
	<monogr>
		<title level="m">DBSec 2018</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Kerschbaum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Paraboschi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10980</biblScope>
			<biblScope unit="page" from="148" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Differential privacy for image publication</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory and Practice of Differential Privacy (TPDP) Workshop</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to recognize abnormalities in chest x-rays with location-aware dense networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gündel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grbic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-13469-3_88</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-13469-388" />
	</analytic>
	<monogr>
		<title level="m">CIARP 2018</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Vera-Rodriguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Morales</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11401</biblScope>
			<biblScope unit="page" from="757" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CheXpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Secure, privacypreserving and federated machine learning in medical imaging</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Makowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Braren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="305" to="311" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Privacy-net: an adversarial approach for identity-obfuscated segmentation of medical images</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1737" to="1749" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A gentle introduction to deep learning in medical image processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Syben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Z. Med. Phys</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="86" to="101" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Generation of anonymous chest radiographs using latent diffusion models for training thoracic abnormality classification systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Packhäuser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Folle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Thamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01323</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning-based patient re-identification is able to exploit the biometric nature of medical chest X-ray data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Packhäuser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gündel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Münster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Syben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Christlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Brain imaging generation with latent diffusion models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Pinaya</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-18576-2_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-18576-212" />
	</analytic>
	<monogr>
		<title level="m">DGM4MICCAI 2022</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Oksuz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13609</biblScope>
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">CheXNet: radiologist-level pneumonia detection on chest xrays with deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ChestX-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Ziller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Usynin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Braren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Makowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaissis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical imaging deep learning with differential privacy. Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
