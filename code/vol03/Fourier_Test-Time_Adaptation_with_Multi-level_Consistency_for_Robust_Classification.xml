<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification</title>
				<funder ref="#_qaD3tQw">
					<orgName type="full">Shenzhen-Hong Kong Joint Research Program</orgName>
				</funder>
				<funder ref="#_bp8Hysx">
					<orgName type="full">Shenzhen Science and Technology Innovations Committee</orgName>
				</funder>
				<funder ref="#_t9Pg2Gt #_zbbYCkh">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuhao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoqiong</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinrui</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haozhe</forename><surname>Chi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoran</forename><surname>Dou</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Centre for Computational Imaging and Simulation Technologies in Biomedicine (CISTIB)</orgName>
								<orgName type="institution">University of Leeds</orgName>
								<address>
									<settlement>Leeds</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xindi</forename><surname>Hu</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Shenzhen RayShape Medical Technology Co. Ltd</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Wang</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">School of Biomedical Engineering and Informatics</orgName>
								<orgName type="institution">Nanjing Medical University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuedong</forename><surname>Deng</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">The Affiliated Suzhou Hospital of Nanjing Medical University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dong</forename><surname>Ni</surname></persName>
							<email>nidong@szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="221" to="231"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">92BA413ADDD4C87B8F9587DA175B0219</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_22</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Classifier robustness</term>
					<term>Testing-time adaptation</term>
					<term>Consistency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep classifiers may encounter significant performance degradation when processing unseen testing data from varying centers, vendors, and protocols. Ensuring the robustness of deep models against these domain shifts is crucial for their widespread clinical application. In this study, we propose a novel approach called Fourier Test-time Adaptation (FTTA), which employs a dual-adaptation design to integrate input and model tuning, thereby jointly improving the model robustness. The main idea of FTTA is to build a reliable multi-level consistency measurement of paired inputs for achieving self-correction of prediction. Our contribution is two-fold. First, we encourage consistency in global features and local attention maps between the two transformed images of the same input. Here, the transformation refers to Fourier -based input adaptation, which can transfer one unseen image into source style to reduce the domain gap. Furthermore, we leverage style-interpolated images to enhance the global and local features with learnable parameters, which can smooth the consistency measurement and accelerate convergence.</p><p>Y. Huang and X. Yang-Contribute equally to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Domain shift (see Fig. <ref type="figure" target="#fig_0">1</ref>) may cause deep classifiers to struggle in making plausible predictions during testing <ref type="bibr" target="#b14">[15]</ref>. This risk seriously limits the reliable deployment of these deep models in real-world scenarios, especially for clinical analysis. Collecting data from the target domain to retrain from scratch or fine-tune the trained model is the potential solution to handle the domain shift risks. However, obtaining adequate testing images with manual annotations is laborious and impracticable in clinical practice. Thus, different solutions have been proposed to conquer the problem and improve the model robustness. From left to right: 1) four-chamber views of heart from Vendor A&amp;B, 2) abdomen planes from Vendor C&amp;D, 3) fundus images with diabetic retinopathy of grade 3 from Center E-G. Appearance and distribution differences can be seen in each group.</p><p>Unsupervised Domain Adaptation (UDA) refers to training the model with labeled source data and adapting it with target data without annotation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>. Recently, Fourier domain adaptation was proposed in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, with the core idea of achieving domain transfer by replacing the low-frequency spectrum of source data with that of the target one. Although effective, they require obtaining sufficient target data in advance, which is challenging for clinical practice. Domain Generalization (DG) aims to generalize models to the unseen domain not presented during training. Adversarial learning-based DG is one of the most popular choices that require multi-domain information for learning domain-invariant representations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Recently, Liu et al. <ref type="bibr" target="#b13">[14]</ref> proposed to construct a continuous frequency space to enhance the connection between different domains. Atwany et al. <ref type="bibr" target="#b0">[1]</ref> imposed a regularization to reduce gradient variance from different domains for diabetic retinopathy classification. One drawback is that they require multiple types of source data for extracting rich features. Other alternatives proposed using only one source domain to perform DG <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>. However, they still heavily rely on simulating new domains via various data augmentations, which can be challenging to control.</p><p>Test-Time Adaptation (TTA) adapts the target data or pre-trained models during testing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Test-time Training (TTT) <ref type="bibr" target="#b20">[21]</ref> and TTT++ <ref type="bibr" target="#b14">[15]</ref> proposed to minimize a self-supervised auxiliary loss. Wang et al. <ref type="bibr" target="#b22">[23]</ref> proposed the TENT framework that focused on minimizing the entropy of its predictions by modulating features via normalization statistics and transformation parameters estimation. Instead of batch input like the above-mentioned methods, Single Image TTA (SITA) <ref type="bibr" target="#b9">[10]</ref> was proposed with the definition that having access to only one given test image once. Recently, different mechanisms were developed to optimize the TTA including distribution calibration <ref type="bibr" target="#b15">[16]</ref>, dynamic learning rate <ref type="bibr" target="#b23">[24]</ref>, and normalizing flow <ref type="bibr" target="#b16">[17]</ref>. Most recently, Gao et al. <ref type="bibr" target="#b4">[5]</ref> proposed projecting the test image back to the source via the source-trained diffusion models. Although effective, these methods often suffer from the problems of unstable parameter estimation, inaccurate proxy tasks/pseudo labels, difficult training, etc. Thus, a simple yet flexible approach is highly desired to fully mine and combine information from test data for online adaptation.</p><p>In this study, we propose a novel framework called Fourier TTA (FTTA) to enhance the model robustness. We believe that this is the first exploration of dual-adaptation design in TTA that jointly updates input and model for online refinement. Here, one assumption is that a well-adapted model will get consistent outputs for different transformations of the same image. Our contribution is twofold. First, we align the high-level features and attention regions of transformed paired images for complementary consistency at global and local dimensions. We adopt the Fourier -based input adaptation as the transformation strategy, which can reduce the distances between unseen testing images and the source domain, thus facilitating the model learning. We further propose to smooth the hard consistency via the weighted integration of features, thus reducing the adaptation difficulties of the model. Second, we employ self-consistency of frequency-based style interpolation to regularize the output logits. It can provide direct and effective hints to improve model robustness. Validated on three classification datasets, we demonstrate that FTTA is general in improving classification robustness, and achieves state-of-the-art results compared to other strong TTA methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Figure <ref type="figure" target="#fig_2">3</ref> shows the pipeline of FTTA. Given a trained classifier G, FTTA first conducts F ourier-based input adaptation to transfer each unseen testing image x t into two source-like images (x t1 and x t2 ). Then, using linear style interpolation, two groups of images will be obtained for subsequent smooth consistency measurement at global features (L f ) and local visual attention (L c ). Furthermore, regularization in the logit space can be computed following the style interpolation consistency in the frequency space (L s ). Finally, FTTA updates once based on the multi-consistency losses to output the final average prediction.</p><p>Fourier-Based Input Adaptation for Domain Transfer. Transferring unseen images to the known domain plays an important role in handling domain shift risks. In this study, instead of learning on multiple domains, we only have access to one single domain of data during training. Therefore, we need to utilize the limited information and find an effective way to realize the fast transfer from the unseen domain to the source domain. Inspired by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, we adopt the Fast Fourier Transform (FFT) based strategy to transfer the domain information and achieve input adaptation during testing. Specifically, we transfer the domain information from one image to another by low-frequency amplitude (A) swapping while keeping the phase components (see Fig. <ref type="figure" target="#fig_1">2</ref>). This is because in Fourier space, the low-frequency A encodes the style information, and semantic contents are preserved in P <ref type="bibr" target="#b24">[25]</ref>. Domain transfer via amplitude swapping between image x s to x t can be defined as:</p><formula xml:id="formula_0">A x t = ((1 -λ)A xt + λA xs ) • M + A xt • (1 -M), (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where M is the circular low-pass filtering with radius r to obtain the radialsymmetrical amplitude <ref type="bibr" target="#b25">[26]</ref>. λ aims to control the degree of style interpolation <ref type="bibr" target="#b13">[14]</ref>, and it can make the transfer process continues (see Fig. <ref type="figure" target="#fig_1">2</ref>). After inverse FFT (IFFT, F -1 ), we can obtain an image x t by F -1 (A x t , P xt ).</p><p>Since one low-level amplitude represents one style, we have n style choices. n is the number of training data. The chosen styles for input adaptation should be representative of the source domain while having significant differences from each other. Hence, we use the validation set to select the styles by first turning the whole validation data into the n styles and calculating n accuracy. Then, styles for achieving top-k performance are considered representative, and L2 distances between the C 2 K pairs are computed to reflect the differences. Smooth Consistency for Global and Local Constraints. Building a reliable consistency measurement of paired inputs is the key to achieving TTA. In this study, we propose global and local alignments to provide a comprehensive consistency signal for tuning the model toward robustness. For global consistency, we compare the similarity between high-level features of paired inputs. These features encode rich semantic information and are therefore well-suited for assessing global consistency. Specifically, we utilize hard and soft feature alignments via pixel-level L2 loss and distribution-level cosine similarity loss, to accurately compute the global feature loss L f . To ensure local consistency, we compute the distances between the classification activation maps (CAMs) of the paired inputs. It is because CAMs (e.g., Grad-CAM <ref type="bibr" target="#b18">[19]</ref>) can reflect the local region the model focuses on when making predictions. Forcing CAMs of paired inputs to be close can guide the model to optimize the attention maps and predict using the correct local region for refining the prediction and improving model robustness (see Fig. <ref type="figure" target="#fig_2">3</ref>, c t1 is encouraged to be closer with c t2 for local visual consistency). Finally, the distances between two CAMs can be computed by the combination of L2 and JS-divergence losses.</p><p>Despite global and local consistency using single paired images can provide effective self-supervised signals for TTA in most cases, they may be difficult or even fail in aligning the features with a serious gap during testing. This is because the representation ability of single-paired images is limited, and the hard consistency between them may cause learning and convergence difficulties. For example, the left-upper CAMs of c1 and c2 in Fig. <ref type="figure" target="#fig_2">3</ref> are with no overlap. Measuring the local consistency between them is meaningless since JS divergence will always output a constant in that case. Thus, we first generate two groups of images, each with four samples, by style interpolation using different λ. Then, we fed them into the model for obtaining two groups of features. Last, we propose learnable integration with parameters u and v to linearly integrate the global and local features. This can enhance the feature representation ability, thus smoothing the consistency evaluation to accelerate the adaptation convergence.</p><p>Style Consistency for Regularization on Logit Space. As described in the first half of Eq. 1, two low-level amplitudes (i.e., styles) can be linearly combined into a new one. We propose to use this frequency-based style consistency to regularize the model outputs in logit space, which is defined as the layer before softmax. Thus, it is directly related to the model prediction. A total of 8 logit pairs can be obtained (see Fig. <ref type="figure" target="#fig_2">3</ref>), and the loss can be defined as:</p><formula xml:id="formula_2">L s = 2 i=1 4 j=1 ||(1 -λ j ) * y log (x t ) + λ j * y log (x ti ) -y log (x ij )|| 2 /8, (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where x t and x ti,i∈1,2 are the testing image and two transformed images after input adaptation. x ij represents style-interpolated images controlled by λ j . y log (•) outputs the logits of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>Materials and Implementations. We validated the FTTA framework on three classification tasks, including one private dataset and two public datasets (see Fig. <ref type="figure" target="#fig_0">1</ref>). Approved by the local IRB, the in-house Fetal-17 US dataset containing 8727 standard planes with gestational age (GA) ranging from 20 to 24 +6 weeks was collected. It contains 17 categories of planes with different parts, including limbs (4), heart (4), brain (3), abdomen (3), face (2), and spine (1). Four 10-year experienced sonographers annotated one classification tag for each image using the Pair annotation software package <ref type="bibr" target="#b12">[13]</ref>. Fetal-17 consists of two vendors (A&amp;B) and we conducted bidirectional experiments (A2B and B2A) for method evaluation. The Maternal-fetal US dataset named Fetal-8 (GA: 18-40 weeks) <ref type="bibr" target="#b1">[2]</ref> <ref type="foot" target="#foot_0">1</ref> contains 8 types of anatomical planes including brain (3), abdomen (1), femur (1), thorax (1), maternal cervix (1), and others (1). Specifically, 10850 images from vendors ALOKA and Voluson (C&amp;D) were used for bidirectional validation (C2D and D2C). Another public dataset is a fundus dataset named Messidor, which contains 1200 images from 0-3 stage of diabetic retinopathy [3]<ref type="foot" target="#foot_1">2</ref> . It was collected from three ophthalmologic centers (E, F&amp;G) with each of them can treated as a source domain, allowing us to conduct three groups of experiments (E2FG, F2EG and G2EF). Dataset split information is listed in Table <ref type="table" target="#tab_0">1</ref>. We implemented FTTA in Pytorch, using an NVIDIA A40 GPU. All images were resized to 256 × 256, and normalized before input to the model. For the fetal datasets, we used a 1-channel input, whereas, for the fundus dataset, 3-channel input was utilized. During training, we augmented the data using common strategies including rotation, flipping and contrast transformation. We selected ImageNet-pretrained ResNet-18 <ref type="bibr" target="#b6">[7]</ref> as our classifier backbone and optimized it using the AdamW optimizer in 100 epochs. For offline training, with batch size = 196, the learning rate (lr) is initialized to 1e-3 and multiplied by 0.1 per 30 epochs. Cross-entropy loss is the basic loss for training. We selected models with the best performance on validation sets to work with FTTA. For online testing, we set the lr equal to 5e-3, and λ j,j=1,2,3,4 for style interpolation was set as 0.2, 0.4, 0.6, and 0.8, respectively. We only updated the network parameters and learnable weights once based on the multi-level consistency losses function before obtaining the final predictions. Quantitative and Qualitative Analysis. We evaluated the classification performance using four metrics including Accuracy (Acc, %), Precision (Pre, %), Recall (Rec, %), and F1-score (F1, %). Table <ref type="table" target="#tab_1">2</ref> compares the FTTA (Ours) with seven competitors including the Baseline without any adaptation and six stateof-the-art TTA methods. Upper-bound represents the performance when training and testing on the target domain. It can be seen from Upper-bound and Baseline that all the metrics have serious drops due to the domain shift. Ours achieves significant improvements on Baseline, and outperforms all the strong competitors in terms of all the evaluation metrics, except for the Pre in Group B2A. It is also noted that the results of Ours are approaching the Upper-bound, with only 5.31% and 4.40% gaps in Acc.</p><p>We also perform ablation studies on the Fetal-17 dataset in the last 7 rows of Table <ref type="table" target="#tab_1">2</ref>. Table <ref type="table" target="#tab_2">3</ref> reports the results of FTTA on two public datasets. We only perform methods including Upper-bound, Baseline, and Ours with evaluation metrics Acc and F1. Huge domain gaps can be observed by comparing Upper-bound and Baseline. All five experimental groups prove that our proposed FTTA can boost the classification performance over baseline, and significantly narrow the gaps between upper-bound. Note that MESSDIOR is a challenging dataset, with all the groups having low Upper-bounds. Even for the multi-source DG method, Messidor only achieves 66.70% accuracy <ref type="bibr" target="#b0">[1]</ref>. For the worst group (F2EG), Acc drops 35.13% in the testing sets. However, the proposed FTTA can perform a good adaptation and improve 26.30% and 5.96% in Acc and F1.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the CAM results obtained by Ours. The red boxes denote the key regions, like the eyes in (a), which were annotated by sonographers and indicate the region-of-interest (ROI) with discriminant information. We consider that if one model can focus on the region having a high overlap with the ROI box, it has a high possibility to be predicted correctly. The second columns visualize the misclassified results before adaptation. It can be observed via the CAMs that the focus of the model is inaccurate. Specifically, they spread dispersed on the whole image, overlap little with the ROI, or with low prediction confidence. After TTA, the CAMs can be refined and close to the ROI, with prediction corrected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we proposed a novel and general FTTA framework to improve classification robustness. Based on Fourier-based input adaptation, FTTA is driven by the proposed multi-level consistency, including smooth global and local constraints, and also the self-consistency on logit space. Extensive experiments on three large datasets validate that FTTA is effective and efficient, achieving stateof-the-art results over strong TTA competitors. In the future, we will extend the FTTA to segmentation or object detection tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. From left to right: 1) four-chamber views of heart from Vendor A&amp;B, 2) abdomen planes from Vendor C&amp;D, 3) fundus images with diabetic retinopathy of grade 3 from Center E-G. Appearance and distribution differences can be seen in each group.</figDesc><graphic coords="2,57,30,315,26,309,28,51,22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the amplitude swapping between two images with different styles. Pseudo-color images shown in the right-down corner indicate the differences between images before and after amplitude swapping.</figDesc><graphic coords="3,57,48,53,81,337,96,91,90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Pipeline of our proposed FTTA framework.</figDesc><graphic coords="4,41,79,54,11,340,21,150,58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Four typical cases in Fetal-17 dataset: (a) Axial orbit and lenses, (b) Humerus plane, (c) Sagittal plane of the spine, and (d) Left ventricular outflow tract view. (Color figure online)</figDesc><graphic coords="8,56,79,54,47,310,60,105,22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>FTTA-IA denotes that without model updating, only input adaptation is conducted. Four experiments are performed to analyze the contribution of three consistency measurements (-C1, -C2, and -C3 for global features, local CAM, and style regularization, respectively), and also the combination of them (-C ). They are all equipped with the input adaptation for fair comparisons. FTTA-C * indicates replacing the Fourier-based input adaptation with 90 • rotation to augment the test image for consistency evaluation. Different from FTTA-C, Ours integrates learnable weight groups to smooth consistency measurement. Experiences show that the naive Fourier input adaptation in FTTA-IA can boost the performance of Baseline. The three consistency variants improve the classification performance respectively, and combining them together can further enhance the model robustness. Then, the comparison between FTTA-C and Ours validates the effectiveness of the consistency smooth strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Datasets split of each experimental group.</figDesc><table><row><cell></cell><cell cols="4">Groups Training Validation Testing</cell></row><row><cell cols="2">Fetal-17 A2B</cell><cell>2622</cell><cell>1135</cell><cell>4970</cell></row><row><cell></cell><cell>B2A</cell><cell>3472</cell><cell>1498</cell><cell>3757</cell></row><row><cell>Fetal-8</cell><cell>C2D</cell><cell>3551</cell><cell>1529</cell><cell>5770</cell></row><row><cell></cell><cell>D2C</cell><cell>4035</cell><cell>1735</cell><cell>5080</cell></row><row><cell cols="3">Messidor E2FG 279</cell><cell>121</cell><cell>800</cell></row><row><cell></cell><cell cols="2">F2EG 278</cell><cell>122</cell><cell>800</cell></row><row><cell></cell><cell cols="2">G2EF 279</cell><cell>121</cell><cell>800</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparisons on Fetal-17 dataset. The best results are shown in bold.</figDesc><table><row><cell>Methods</cell><cell cols="2">Fetal-17: A2B</cell><cell></cell><cell></cell><cell cols="2">Fetal-17: B2A</cell><cell></cell></row><row><cell></cell><cell>Acc</cell><cell>Pre</cell><cell>Rec</cell><cell>F1</cell><cell>Acc</cell><cell>Pre</cell><cell>Rec</cell><cell>F1</cell></row><row><cell cols="9">Upper-bound 96.33 95.79 94.54 94.61 91.81 89.28 88.89 88.67</cell></row><row><cell>Baseline [7]</cell><cell cols="8">61.25 64.57 59.46 57.83 63.51 60.60 61.60 57.50</cell></row><row><cell>TTT [21]</cell><cell cols="8">71.91 65.62 67.33 63.28 72.77 57.34 58.11 55.51</cell></row><row><cell cols="9">TTT++ [15] 78.65 79.08 75.21 73.20 78.60 76.15 71.36 71.02</cell></row><row><cell>TENT [23]</cell><cell cols="8">76.48 74.54 71.41 69.40 75.83 73.38 70.75 67.01</cell></row><row><cell>DLTTA [24]</cell><cell cols="8">85.51 88.30 83.87 83.83 83.20 81.39 76.77 76.94</cell></row><row><cell>DTTA [5]</cell><cell cols="8">87.00 86.93 84.48 83.87 83.39 82.17 78.73 79.09</cell></row><row><cell cols="9">TTTFlow [17] 86.66 85.98 85.38 84.96 84.08 85.41 76.99 75.70</cell></row><row><cell>FTTA-IA</cell><cell cols="8">73.56 72.46 67.49 61.47 69.07 64.17 52.81 50.64</cell></row><row><cell>FTTA-C1</cell><cell cols="8">82.27 81.66 76.72 75.32 81.55 78.56 68.43 67.17</cell></row><row><cell>FTTA-C2</cell><cell cols="8">83.06 82.42 81.81 79.92 81.95 69.03 67.51 65.78</cell></row><row><cell>FTTA-C3</cell><cell cols="8">84.77 82.14 77.04 76.30 82.09 78.57 80.24 77.15</cell></row><row><cell>FTTA-C  *</cell><cell cols="8">80.70 82.52 77.24 78.18 79.24 80.64 74.42 74.00</cell></row><row><cell>FTTA-C</cell><cell cols="8">88.93 89.43 82.40 82.96 84.91 80.91 75.64 76.52</cell></row><row><cell>Ours</cell><cell cols="8">91.02 89.62 89.74 89.37 87.41 82.46 81.91 81.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparisons on Fetal-8 and MESSDIOR datasets.</figDesc><table><row><cell>Methods</cell><cell>C2D</cell><cell>D2C</cell><cell>E2FG</cell><cell>F2EG</cell><cell>G2EF</cell></row><row><cell></cell><cell>Acc F1</cell><cell>Acc F1</cell><cell>Acc F1</cell><cell>Acc F1</cell><cell>Acc F1</cell></row><row><cell cols="6">Upper-bound 87.49 85.14 94.05 85.24 60.91 53.46 66.26 57.63 62.14 53.34</cell></row><row><cell>Baseline</cell><cell cols="5">67.68 65.93 79.92 72.09 47.62 37.09 31.13 31.30 41.25 39.41</cell></row><row><cell>Ours</cell><cell cols="5">82.13 76.89 91.87 73.19 59.26 43.98 57.43 37.26 58.02 45.46</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://zenodo.org/record/3904280#.YqIQvKhBy3A.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.adcis.net/en/third-party/messidor/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the grant from <rs type="funder">National Natural Science Foundation of China</rs> (Nos. <rs type="grantNumber">62171290</rs>, <rs type="grantNumber">62101343</rs>), <rs type="funder">Shenzhen-Hong Kong Joint Research Program</rs> (No. <rs type="grantNumber">SGDX20201103095613036</rs>), and <rs type="funder">Shenzhen Science and Technology Innovations Committee</rs> (No. <rs type="grantNumber">20200812143441001</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_t9Pg2Gt">
					<idno type="grant-number">62171290</idno>
				</org>
				<org type="funding" xml:id="_zbbYCkh">
					<idno type="grant-number">62101343</idno>
				</org>
				<org type="funding" xml:id="_qaD3tQw">
					<idno type="grant-number">SGDX20201103095613036</idno>
				</org>
				<org type="funding" xml:id="_bp8Hysx">
					<idno type="grant-number">20200812143441001</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_22.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DRGen: domain generalization in diabetic retinopathy classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Atwany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yaqub</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_61</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-7_61" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of deep convolutional neural networks for automatic classification of common maternal fetal ultrasound planes</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feedback on a publicly distributed image database: the messidor database</title>
		<author>
			<persName><forename type="first">E</forename><surname>Decencière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cazuguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cochener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Trone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Anal. Stereol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="234" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adversarially adaptive normalization for single domain generalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF CVPR</title>
		<meeting>the IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8208" to="8217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Back to the source: diffusion-driven adaptation to test-time corruption</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF CVPR</title>
		<meeting>the IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="11786" to="11796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DAML: domain adaptation metric learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2980" to="2989" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR</title>
		<meeting>the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Test-time bi-directional adaptation between image and model for robust segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">233</biblScope>
			<biblScope unit="page">107477</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online reflective learning for robust medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_62</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-1_62" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="652" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aggarwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02355</idno>
		<title level="m">SITA: single image testtime adaptation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR</title>
		<meeting>the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01267-0_38</idno>
		<idno>978-3-030-01267-0_38</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11219</biblScope>
			<biblScope unit="page" from="647" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102461</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FedDG: federated domain generalization on medical image segmentation via episodic learning in continuous frequency space</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF CVPR</title>
		<meeting>the IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1013" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TTT++: when does self-supervised test-time training fail or thrive?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Delft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bellot-Gurlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21808" to="21820" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Test-time adaptation with calibration of medical image classification nets for label distribution shift</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_30" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TTTFlow: unsupervised test-time training with normalizing flow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Osowiechi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A V</forename><surname>Hakim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Noori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheraghalikhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2126" to="2134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation for classification of prostate histopathology whole-slide images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Foran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00934-2_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00934-2_23" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11071</biblScope>
			<biblScope unit="page" from="201" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradcam: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE ICCV</title>
		<meeting>the IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ultrasound domain adaptation using frequency domain analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Tehrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Benali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rivaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">2021 IEEE IUS</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2021">2021</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Test-time training with self-supervision for generalization under distribution shifts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9229" to="9248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR</title>
		<meeting>the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TENT: fully test-time adaptation by entropy minimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DLTTA: dynamic learning rate for test-time adaptation on crossdomain medical images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3575" to="3586" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FDA: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF CVPR</title>
		<meeting>the IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feather-light Fourier domain adaptation in magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">I</forename><surname>Zakazov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shaposhnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bespalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Dylov</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16852-9_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16852-9_9" />
	</analytic>
	<monogr>
		<title level="m">DART 2022</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13542</biblScope>
			<biblScope unit="page" from="88" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maximum-entropy adversarial data augmentation for improved generalization and robustness</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14435" to="14447" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
