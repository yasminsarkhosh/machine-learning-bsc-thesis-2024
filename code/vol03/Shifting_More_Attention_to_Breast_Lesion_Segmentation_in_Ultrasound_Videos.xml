<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos</title>
				<funder ref="#_4VcpsJn">
					<orgName type="full">A*STAR Central Research Fund</orgName>
				</funder>
				<funder ref="#_FqjRHQ3">
					<orgName type="full">Regional Joint Fund of Guangdong (Guangdong-Hong Kong-Macao Research Team Project)</orgName>
				</funder>
				<funder ref="#_WfJU7t3">
					<orgName type="full">National Research Foundation, Singapore</orgName>
				</funder>
				<funder ref="#_ChWZsmW">
					<orgName type="full">Guangzhou Municipal Science and Technology Project</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junhao</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
							<email>leizhu@ust.hk</email>
							<affiliation key="aff1">
								<orgName type="laboratory">ROAS Thrust</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology (Guangzhou)</orgName>
								<address>
									<addrLine>System Hub</addrLine>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Agency for Science, Technology and Research</orgName>
								<orgName type="institution">Institute of High Performance Computing</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiong</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weibin</forename><surname>Li</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">School of Medicine</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenhao</forename><surname>Rao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyang</forename><surname>Huang</surname></persName>
							<email>xyhuang@xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liansheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="497" to="507"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">43E1E4E69E8E6C91A0BA1BA0A6138F86</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_48</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ultrasound Video</term>
					<term>Breast lesion</term>
					<term>Segmentation Supplementary Information The online version contains supplementary material</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Breast lesion segmentation in ultrasound (US) videos is essential for diagnosing and treating axillary lymph node metastasis. However, the lack of a well-established and large-scale ultrasound video dataset with high-quality annotations has posed a persistent challenge for the research community. To overcome this issue, we meticulously curated a US video breast lesion segmentation dataset comprising 572 videos and 34,300 annotated frames, covering a wide range of realistic clinical scenarios. Furthermore, we propose a novel frequency and localization feature aggregation network (FLA-Net) that learns temporal features from the frequency domain and predicts additional lesion location positions to assist with breast lesion segmentation. We also devise a localization-based contrastive loss to reduce the lesion location distance between neighboring video frames within the same video and enlarge the location distances between frames from different ultrasound videos. Our experiments on our annotated dataset and two public video polyp segmentation datasets demonstrate that our proposed FLA-Net achieves state-of-the-art performance in breast lesion segmentation in US videos and video polyp segmentation while significantly reducing time and space complexity. Our model and dataset are available at https://github.com/jhl-Det/FLA-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Axillary lymph node (ALN) metastasis is a severe complication of cancer that can have devastating consequences, including significant morbidity and mortality. Early detection and timely treatment are crucial for improving outcomes and reducing the risk of recurrence. In breast cancer diagnosis, accurately segmenting breast lesions in ultrasound (US) videos is an essential step for computer-aided diagnosis systems, as well as breast cancer diagnosis and treatment. However, this task is challenging due to several factors, including blurry lesion boundaries, inhomogeneous distributions, diverse motion patterns, and dynamic changes in lesion sizes over time <ref type="bibr" target="#b12">[12]</ref>. The work presented in <ref type="bibr" target="#b10">[10]</ref> proposed the first pixel-wise annotated benchmark dataset for breast lesion segmentation in US videos, but it has some limitations. Although their efforts were commendable, this dataset is private and contains only 63 videos with 4,619 annotated frames. The small dataset size increases the risk of overfitting and limits the generalizability capability. In this work, we collected a larger-scale US video breast lesion segmentation dataset with 572 videos and 34,300 annotated frames, of which 222 videos contain ALN metastasis, covering a wide range of realistic clinical scenarios. Please refer to Table <ref type="table" target="#tab_0">1</ref> for a detailed comparison between our dataset and existing datasets.</p><p>Although the existing benchmark method DPSTT <ref type="bibr" target="#b10">[10]</ref> has shown promising results for breast lesion segmentation in US videos, it only uses the ultrasound image to read memory for learning temporal features. However, ultrasound images suffer from speckle noise, weak boundaries, and low image quality. Thus, there is still considerable room for improvement in ultrasound video breast lesion segmentation. To address this, we propose a novel network called Frequency and Localization Feature Aggregation Network (FLA-Net) to improve breast lesion segmentation in ultrasound videos. Our FLA-Net learns frequency-based temporal features and then uses them to predict auxiliary breast lesion location maps to assist the segmentation of breast lesions in video frames. Additionally, we devise a contrastive loss to enhance the breast lesion location similarity of video frames within the same ultrasound video and to prohibit location similarity of different ultrasound videos. The experimental results unequivocally showcase that our network surpasses state-of-the-art techniques in the realm of both breast lesion segmentation in US videos and two video polyp segmentation benchmark datasets (Fig. <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ultrasound Video Breast Lesion Segmentation Dataset</head><p>To support advancements in breast lesion segmentation and ALN metastasis prediction, we collected a dataset containing 572 breast lesion ultrasound videos with 34,300 annotated frames. Table <ref type="table" target="#tab_0">1</ref> summarizes the statistics of existing breast lesion US video datasets. Among 572 videos, 222 videos with ALN metastasis. Nine experienced pathologists were invited to manually annotate breast lesions at each video frame. Unlike previous datasets <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b12">12]</ref>, our dataset has a reserved validation set to avoid model overfitting. The entire dataset is partitioned into training, validation, and test sets in a proportion of 4:2:4, yielding a total of 230 training videos, 112 validation videos, and 230 test videos for comprehensive benchmarking purposes. Moreover, apart from the segmentation annotation, our dataset also includes lesion bounding box labels, which enables benchmarking breast lesion detection in ultrasound videos. More dataset statistics are available in the Supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Figure <ref type="figure" target="#fig_1">2</ref> provides a detailed illustration of the proposed frequency and localization feature aggregation network (FLA-Net). When presented with an ultrasound frame denoted as I t along with its two adjacent video frames (I t-1 and I t-2 ), our initial step involves feeding them through an Encoder, specifically the Res2Net50 architecture <ref type="bibr" target="#b6">[6]</ref>, to acquire three distinct features labeled as f t , f t-1 , and f t-2 . Then, we devise a frequency-based feature aggregation (FFA) module to integrate frequency features of each video frame. After that, we pass the output features o t of the FFA module into two decoder branches (similar to the UNet decoder <ref type="bibr" target="#b14">[14]</ref>): one is the localization branch to predict the localization map of the breast lesions, while another segmentation branch integrates the features of the localization branch to fuse localization feature for segmenting breast lesions. Moreover, we devise a location-based contrastive loss to regularize the breast lesion locations of inter-video frames and intra-video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Frequency-Based Feature Aggregation (FFA) Module</head><p>According to the spectral convolution theorem in Fourier theory, any modification made to a single value in the spectral domain has a global impact on all the original input features <ref type="bibr" target="#b0">[1]</ref>. This theorem guides the design of FFA module, which has a global receptive field to refine features in the spectral domain. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our FFA block takes three features (f t ∈ R c×h×w , f t-1 ∈ R c×h×w , and f t-2 ∈ R c×h×w ) as input. To integrate the three input features and extract relevant information while suppressing irrelevant information, our FFA block first employs a Fast Fourier Transform (FFT) to transform the three input features into the spectral domain, resulting in three corresponding spectral domain features ( ft ∈ C c×h×w , ft-1 ∈ C c×h×w , and ft-2 ∈ C c×h×w ), which capture the frequency information of the input features. Note that the current spectral features ( ft , ft-1 , and ft-2 ) are complex numbers and incompatible with the neural layers. Therefore we concatenate the real and imaginary parts of these complex numbers along the channel dimension respectively and thus obtain three new tensors (x t ∈ R 2c×h×w , x t-1 ∈ R 2c×h×w , and x t-2 ∈ R 2c×h×w ) with double channels. Afterward, we take the current frame spectral-domain features x t as the core and fuse the spatial-temporal information from the two auxiliary spectral-domain features (x t-1 and x t-2 ), respectively. Specifically, we first group three features into two groups ({x t , x t-1 } and {x t , x t-2 }) and develop a channel attention function CA(•) to obtain two attention maps. The CA(•) passes an input feature map to a feature normalization, two 1×1 convolution layers Conv(•), a ReLU activation function δ(•), and a sigmoid function σ(•) to compute an attention map. Then, we element-wise multiply the obtained attention map from each group with the input features, and the multiplication results (see y 1 and y 2 ) are then transformed into complex numbers by splitting them into real and imaginary parts along the channel dimension. After that, inverse FFT (iFFT) operation is employed to transfer the spectral features back to the spatial domain, and then two obtained features at the spatial domain are denoted as z 1 and z 2 . Finally, we further element-wisely add z 1 and z 2 and then pass it into a "BConv" layer to obtain the output feature o t of our FFA module. Mathematically, o t is computed by o t = BConv(z 1 + z 2 ), where "BConv" contains a 3 × 3 convolution layer, a group normalization, and a ReLU activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Two-Branch Decoder</head><p>After obtaining the frequency features, we introduce a two-branch decoder consisting of a segmentation branch and a localization branch to incorporate temporal features from nearby frames into the current frame. Each branch is built based on the UNet decoder <ref type="bibr" target="#b14">[14]</ref> with four convolutional layers. Let d 1 s and d 2 s denote the features at the last two layers of the segmentation decoder branch, and d 1 l and d 2 l denote the features at the last two layers of the localization decoder branch. Then, we pass d 1 l at the localization decoder branch to predict a breast lesion localization map. Then, we element-wisely add d 1 l and d 1 s , and elementwisely add d 2 l and d 2 s , and pass the addition result into a "BConv" convolution layer to predict the segmentation map S t of the input video frame I t .</p><p>Location Ground Truth. Instead of formulating it as a regression problem, we adopt a likelihood heatmap-based approach to encode the location of breast lesions, since it is more robust to occlusion and motion blur. To do so, we compute a bounding box of the annotated breast lesion segmentation result, and then take the center coordinates of the bounding box. After that, we apply a Gaussian kernel with a standard deviation of 5 on the center coordinates to generate a heatmap, which is taken as the ground truth of the breast lesion localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Location-Based Contrastive Loss</head><p>Note that the breast lesion locations of neighboring ultrasound video frames are close, while the breast lesion location distance is large for different ultrasound videos, which are often obtained from different patients. Motivated by this, we further devise a location-based contrastive loss to make the breast lesion locations at the same video to be close, while pushing the lesion locations of frames from different videos away. By doing so, we can enhance the breast lesion location prediction in the localization branch. Hence, we devise a location-based contrastive loss based on a triplet loss <ref type="bibr" target="#b15">[15]</ref>, and the definition is given by:</p><formula xml:id="formula_0">L contrastive = max(MSE(H t , H t-1 ) -MSE(H t , N t ) + α, 0), (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where α is a margin that is enforced between positive and negative pairs. H t and H t-1 are predicted heatmaps of neighboring frames from the same video. N t denotes the heatmap of the breast lesion from a frame from another ultrasound video. Hence, the total loss L total of our network is computed by:</p><formula xml:id="formula_2">L total = L contrastive + λ 1 L MSE (H t , G H t ) + λ 2 L BCE (S t , G S t ) + λ 3 L IoU (S t , G S t ),<label>(2)</label></formula><p>where G H t and G S t denote the ground truth of the breast lesion segmentation and the breast lesion localization. We empirically set weights</p><formula xml:id="formula_3">λ 1 = λ 2 = λ 3 = 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Implementation Details. To initialize the backbone of our network, we pretrained Res2Net-50 <ref type="bibr" target="#b6">[6]</ref> on the ImageNet dataset, while the remaining components of our network were trained from scratch. Prior to inputting the training video frames into the network, we resize them to 352 × 352 dimensions. Our network is implemented in PyTorch and employs the Adam optimizer with a learning rate of 5 × 10 -5 , trained over 100 epochs, and a batch size of 24. Training is conducted on four GeForce RTX 2080 Ti GPUs. For quantitative comparisons, we utilize various metrics, including the Dice similarity coefficient (Dice), Jaccard similarity coefficient (Jaccard), F1-score, and mean absolute error (MAE).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparisons with State-of-the-Arts</head><p>We conduct a comparative analysis between our network and nine state-of-theart methods, comprising four image-based methods and five video-based methods. Four image-based methods are UNet <ref type="bibr" target="#b14">[14]</ref>, UNet++ <ref type="bibr" target="#b19">[19]</ref>, TransUNet <ref type="bibr" target="#b3">[4]</ref>, and SETR <ref type="bibr" target="#b18">[18]</ref>, while five video-based methods are STM <ref type="bibr" target="#b13">[13]</ref>, AFB-URR <ref type="bibr" target="#b11">[11]</ref>, PNS+ <ref type="bibr" target="#b9">[9]</ref>, DPSTT <ref type="bibr" target="#b10">[10]</ref>, and DCFNet <ref type="bibr" target="#b16">[16]</ref>. To ensure a fair and equitable comparison, we acquire the segmentation results of all nine compared methods by utilizing either their publicly available implementations or by implementing them ourselves. Additionally, we retrain these networks on our dataset and fine-tune their network parameters to attain their optimal segmentation performance, enabling accurate and meaningful comparisons.</p><p>Quantitative Comparisons. The quantitative results of our network and the nine compared breast lesion segmentation methods are summarized in Table <ref type="table" target="#tab_1">2</ref>. Analysis of the results reveals that, in terms of quantitative metrics, video-based methods generally outperform image-based methods. Among nine compared methods, DCFNet <ref type="bibr" target="#b16">[16]</ref> achieves the largest Dice, Jaccard, and F1-score results, while PNS+ <ref type="bibr" target="#b9">[9]</ref> and DPSTT <ref type="bibr" target="#b10">[10]</ref> have the smallest MAE score. More importantly, our FLA-Net further outperforms DCFNet <ref type="bibr" target="#b16">[16]</ref> in terms of Dice, Jaccard, and F1-score metrics, and has a superior MAE performance over PNS+ <ref type="bibr" target="#b9">[9]</ref> and DPSTT <ref type="bibr" target="#b10">[10]</ref>. Specifically, our FLA-Net improves the Dice score from 0.762 to 0.789, the Jaccard score from 0.659 to 0.687, the F1-score result from 0.799 to 0.815, and the MAE score from 0.036 to 0.033. Metrics UNet <ref type="bibr" target="#b14">[14]</ref> UNet++ <ref type="bibr" target="#b19">[19]</ref> ResUNet <ref type="bibr" target="#b7">[7]</ref> ACSNet <ref type="bibr" target="#b17">[17]</ref> PraNet <ref type="bibr" target="#b4">[5]</ref> PNSNet <ref type="bibr" target="#b8">[8]</ref> Ours Qualitative Comparisons. Figure <ref type="figure" target="#fig_2">3</ref> visually presents a comparison of breast lesion segmentation results obtained from our network and three other methods across various input video frames. Apparently, our method accurately segments breast lesions of the input ultrasound video frames, although these target breast lesions have varied sizes and diverse shapes in the input video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>To evaluate the effectiveness of the major components in our network, we constructed three baseline networks. The first one (denoted as "Basic") removed the localization encoder branch and replaced our FLA modules with a simple feature concatenation and a 1 × 1 convolutional layer. The second and third baseline networks (named "Basic+FLA" and "Basic+LB") incorporate the FLA module and the localization branch into the basic network, respectively. Table <ref type="table" target="#tab_2">3</ref> reports the quantitative results of our method and three baseline networks. The superior metric performance of "Basic+FLA" and "Basic+LB" compared to "Basic" clearly indicates that our FLA module and the localization encoder branch effectively enhance the breast lesion segmentation performance in ultrasound videos. Then, the superior performance of "Basic+FLA+LB" over "Basic+FLA" and "Basic+LB" demonstrate that combining our FLA module and the localization encoder branch can incur a more accurate segmentation result. Moreover, our method has larger Dice, Jaccard, F1-score results and a smaller MAE result than "Basic+FLA+LB", which shows that our location-based contrastive loss has its contribution to the success of our video breast lesion segmentation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generalizability of Our Network</head><p>To further evaluate the effectiveness of our FLA-Net, we extend its application to the task of video polyp segmentation. Following the experimental protocol employed in a recent study on video polyp segmentation <ref type="bibr" target="#b8">[8]</ref>, we retrain our network and present quantitative results on two benchmark datasets, namely CVC-300-TV <ref type="bibr" target="#b1">[2]</ref> and CVC-612-V <ref type="bibr" target="#b2">[3]</ref>. Table <ref type="table" target="#tab_3">4</ref> showcases the Dice, IoU, S α , E φ , and MAE results achieved by our network in comparison to state-of-the-art methods on these two datasets. Our method demonstrates clear superiority over state-ofthe-art methods in terms of Dice, IoU, E φ , and MAE on both the CVC-300-TV and CVC-612-V datasets. Specifically, our method enhances the Dice score from 0.840 to 0.874, the IoU score from 0.745 to 0.789, the E φ score from 0.921 to 0.969, and reduces the MAE score from 0.013 to 0.010 for the CVC-300-TV dataset. Similarly, for the CVC-612-V dataset, our method achieves improvements of 0.012, 0.014, 0.019, and 0 in Dice, IoU, E φ , and MAE scores, respectively. Although our S α results (0.907 on CVC-300-TV and 0.920 on CVC-612-V) take the 2nd rank, they are very close to the best S α results, which are 0.909 on CVC-300-TV and 0.923 on CVC-612-V. Hence, the superior metric results obtained by our network clearly demonstrate its ability to accurately segment polyp regions more effectively than state-of-the-art video polyp segmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this study, we introduce a novel approach for segmenting breast lesions in ultrasound videos, leveraging a larger dataset consisting of 572 videos containing a total of 34,300 annotated frames. We introduce a frequency and location feature aggregation network that incorporates frequency-based temporal feature learning, an auxiliary prediction of breast lesion location, and a location-based contrastive loss. Our proposed method surpasses existing state-of-the-art techniques in terms of performance on our annotated dataset as well as two publicly available video polyp segmentation datasets. These outcomes serve as compelling evidence for the effectiveness of our approach in achieving accurate breast lesion segmentation in ultrasound videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of our ultrasound video dataset for breast lesion segmentation.</figDesc><graphic coords="3,56,97,53,72,338,32,111,97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our FLA-Net. Our network takes an ultrasound frame It and its adjacent two frames (It-1 and It-2) as input. Three frames are first passed through an encoder to learn three CNN features (ft, ft-1, and ft-2). Then Frequency-based Feature Aggregation Module is then used to aggregate these features and the aggregated feature map is then passed into our two-branch decoder to predict the breast lesion segmentation mask of It, and a lesion localization heatmap. Moreover, we devise a location-aware contrastive loss (see Lcontrastive) to reduce location distance of frames from the same video and enlarge the location distance of different video frames.</figDesc><graphic coords="4,56,31,54,62,311,32,176,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual comparisons of breast lesion segmentation results produced by our network and state-of-the-art methods. "GT" denotes the ground truth. For more visualization results, please refer to the supplementary material.</figDesc><graphic coords="7,70,47,54,62,311,20,95,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of existing breast lesion US videos datasets and the proposed dataset. #videos: numbers of videos. #AD: number of annotated frames. BBox: whether provide bounding box annotation. BBox: whether provide segmentation mask annotation. BM: whether provide lesion classification label (Benign or Malignant). PA: whether provide axillary lymph node (ALN) metastasis label (Presence or Absence).</figDesc><table><row><cell>Dataset</cell><cell cols="4">Year # videos # AF BBox Mask BM PA</cell></row><row><cell cols="2">Li et al. [10] 2022 63</cell><cell>4,619</cell><cell>×</cell><cell>×</cell></row><row><cell cols="2">Lin et al. [12] 2022 188</cell><cell>25,272</cell><cell>×</cell><cell>×</cell></row><row><cell>Ours</cell><cell>2023 572</cell><cell>34,300</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparisons between our FLA-Net and the state-of-the-art methods on our test set in terms of breast lesion segmentation in ultrasound videos.</figDesc><table><row><cell>Method</cell><cell cols="4">image/video Dice ↑ Jaccard ↑ F1-score ↑ MAE ↓</cell></row><row><cell>UNet [14]</cell><cell>image</cell><cell>0.745 0.636</cell><cell>0.777</cell><cell>0.043</cell></row><row><cell>UNet++ [19]</cell><cell>image</cell><cell>0.749 0.633</cell><cell>0.780</cell><cell>0.039</cell></row><row><cell cols="2">TransUNet [4] image</cell><cell>0.733 0.637</cell><cell>0.784</cell><cell>0.042</cell></row><row><cell>SETR [18]</cell><cell>image</cell><cell>0.709 0.588</cell><cell>0.748</cell><cell>0.045</cell></row><row><cell>STM [13]</cell><cell>video</cell><cell>0.741 0.634</cell><cell>0.782</cell><cell>0.041</cell></row><row><cell cols="2">AFB-URR [11] video</cell><cell>0.750 0.635</cell><cell>0.781</cell><cell>0.038</cell></row><row><cell>PNS+ [9]</cell><cell>video</cell><cell>0.754 0.648</cell><cell>0.783</cell><cell>0.036</cell></row><row><cell>DPSTT [10]</cell><cell>video</cell><cell>0.755 0.649</cell><cell>0.785</cell><cell>0.036</cell></row><row><cell>DCFNet [16]</cell><cell>video</cell><cell>0.762 0.659</cell><cell>0.799</cell><cell>0.037</cell></row><row><cell cols="2">Our FLA-Net video</cell><cell>0.789 0.687</cell><cell>0.815</cell><cell>0.033</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparison results of ablation study experiments. FLA Loc-Branch Contrastive loss Dice ↑ Jaccard ↑ F1-score ↑ MAE ↓</figDesc><table><row><cell>Basic</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>0.747 0.641</cell><cell>0.777</cell><cell>0.037</cell></row><row><cell>Basic+FLA</cell><cell></cell><cell>×</cell><cell>×</cell><cell>0.777 0.669</cell><cell>0.806</cell><cell>0.035</cell></row><row><cell>Basic+LB</cell><cell>×</cell><cell></cell><cell>×</cell><cell>0.751 0.646</cell><cell>0.781</cell><cell>0.037</cell></row><row><cell>Basic+FLA+LB</cell><cell></cell><cell></cell><cell>×</cell><cell>0.780 0.675</cell><cell>0.809</cell><cell>0.034</cell></row><row><cell>Our method</cell><cell></cell><cell></cell><cell></cell><cell>0.789 0.687</cell><cell>0.815</cell><cell>0.033</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Quantitative comparison results on different video polyp segmentation datasets. For more quantitative results please refer to the supplementary material.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research is supported by <rs type="funder">Guangzhou Municipal Science and Technology Project</rs> (Grant No. <rs type="grantNumber">2023A03J0671</rs>), the <rs type="funder">Regional Joint Fund of Guangdong (Guangdong-Hong Kong-Macao Research Team Project)</rs> under Grant <rs type="grantNumber">2021B1515130003</rs>, the <rs type="funder">National Research Foundation, Singapore</rs> under its <rs type="programName">AI Singapore Programme (AISG Award</rs> No: <rs type="grantNumber">AISG2-TC-2021-003</rs>), A*<rs type="programName">STAR AME Programmatic Funding Scheme Under Project A20H4b0141</rs>, and <rs type="funder">A*STAR Central Research Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ChWZsmW">
					<idno type="grant-number">2023A03J0671</idno>
				</org>
				<org type="funding" xml:id="_FqjRHQ3">
					<idno type="grant-number">2021B1515130003</idno>
				</org>
				<org type="funding" xml:id="_WfJU7t3">
					<idno type="grant-number">AISG2-TC-2021-003</idno>
					<orgName type="program" subtype="full">AI Singapore Programme (AISG Award</orgName>
				</org>
				<org type="funding" xml:id="_4VcpsJn">
					<orgName type="program" subtype="full">STAR AME Programmatic Funding Scheme Under Project A20H4b0141</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A guided tour of the fast Fourier transform</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Bergland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectr</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="41" to="52" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards automatic polyp detection with a polyp appearance model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Best Papers of Iberian Conference on Pattern Recognition and Image Analysis</title>
		<imprint>
			<date type="published" when="2011">2012. IbPRIA 2011</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="3166" to="3182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">WM-DOVA maps for accurate polyp highlighting in colonoscopy: validation vs. saliency maps from physicians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fernández-Esparrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">TransUNet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PraNet: parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-2_26" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Res2Net: a new multi-scale backbone architecture</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="652" to="662" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ResUNet++: an advanced architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Progressively normalized self-attention network for video polyp segmentation</title>
		<author>
			<persName><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_14" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="142" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video polyp segmentation: a deep learning perspective</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="531" to="549" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking breast lesion segmentation in ultrasound: a new video dataset and a baseline network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_38" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="391" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video object segmentation with adaptive feature bank and uncertain-region refinement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3430" to="3441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new dataset and a baseline model for breast lesion detection in ultrasound videos</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_59</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_59" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="614" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10">October 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FaceNet: a unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic context-sensitive filtering network for video salient object detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1553" to="1563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive context selection for polyp segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-2_25" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">UNet++: redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
