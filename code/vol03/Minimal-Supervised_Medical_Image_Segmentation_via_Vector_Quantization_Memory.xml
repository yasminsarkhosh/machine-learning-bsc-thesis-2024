<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory</title>
				<funder ref="#_Hyp6JSH">
					<orgName type="full">National Research Foundation, Singapore</orgName>
				</funder>
				<funder ref="#_c43PuM9">
					<orgName type="full">Agency for Science, Technology and Research (A*STAR)</orgName>
				</funder>
				<funder>
					<orgName type="full">A*STAR Central Research Fund &quot;A Secure and Privacy Preserving AI Platform for Digital Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
							<email>xuyanyu@ihpc.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way, #16-16</addrLine>
									<postCode>138632</postCode>
									<settlement>Connexis</settlement>
									<country>Singapore, Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Menghan</forename><surname>Zhou</surname></persName>
							<email>zhoumenghan@ihpc.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way, #16-16</addrLine>
									<postCode>138632</postCode>
									<settlement>Connexis</settlement>
									<country>Singapore, Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yangqin</forename><surname>Feng</surname></persName>
							<email>fengyangqin@ihpc.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way, #16-16</addrLine>
									<postCode>138632</postCode>
									<settlement>Connexis</settlement>
									<country>Singapore, Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinxing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way, #16-16</addrLine>
									<postCode>138632</postCode>
									<settlement>Connexis</settlement>
									<country>Singapore, Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<email>fuhuazhu@ihpc.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way, #16-16</addrLine>
									<postCode>138632</postCode>
									<settlement>Connexis</settlement>
									<country>Singapore, Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rick</forename><forename type="middle">Siow Mong</forename><surname>Goh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<email>liuyong@ihpc.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way, #16-16</addrLine>
									<postCode>138632</postCode>
									<settlement>Connexis</settlement>
									<country>Singapore, Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="625" to="636"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">BF185863AD184BEC9B6042ADECCC289E</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_60</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Annotation-efficient Learning â€¢ Vector Quantization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical imaging segmentation is a critical key task for computer-assisted diagnosis and disease monitoring. However, collecting a large-scale medical dataset with well-annotation is time-consuming and requires domain knowledge. Reducing the number of annotations poses two challenges: obtaining sufficient supervision and generating high-quality pseudo labels. To address these, we propose a universal framework for annotation-efficient medical segmentation, which is capable of handling both scribble-supervised and point-supervised segmentation. Our approach includes an auxiliary reconstruction branch that provides more supervision and backwards sufficient gradients for learning visual representations. Besides, a novel pseudo label generation branch utilizes the Vector Quantization (VQ) bank to store texture-oriented and global features for generating pseudo labels. To boost the model training, we generate the high-quality pseudo labels by mixing the segmentation prediction and pseudo labels from the VQ bank. The experimental results on the ACDC MRI segmentation dataset demonstrate effectiveness of our designed method. We obtain a comparable performance (0.86 vs. 0.87 DSC score) with a few points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The medical imaging segmentation plays a crucial role in the computer-assisted diagnosis and monitoring of diseases. In recent years, deep neural networks have demonstrated remarkable results in automatic medical segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22]</ref>. However, the process of collecting large-scale and sufficiently annotated medical datasets remains expensive and tedious, requiring domain knowledge and clinical experience. To mitigate the annotation cost, various techniques have been developed to train models using as few annotations as possible, including semi-supervised learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>, and weakly supervised learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref>. In this study, we focus on annotation-efficient learning and propose a universal framework for training segmentation models with scribble and point annotation.</p><p>Reducing the number of annotations from dense annotations to scribbles or even points poses two challenges in segmentation: <ref type="bibr" target="#b0">(1)</ref> how to obtain sufficient supervision to train a network and (2) how to generate high-quality pseudo labels. In the context of scribble-supervised segmentation, various segmentation methods have been explored, including machine learning or other algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>, as well as deep learning networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>. However, these methods only use scribble annotations, excluding point annotations, and their performance remains inferior to training with dense annotations, limiting their practical use in clinical settings. Pseudo labeling <ref type="bibr" target="#b11">[12]</ref> is widely used to generate supervision signals for unlabeled images/pixels from imperfect annotations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>. Recently, some works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> have demonstrated that semi-supervised learning can benefit from high-quality pseudo labels. In this study, we propose generating pseudo labels by randomly mixing prediction and texture-oriented pseudo label, which can address the inherent weakness of the previous methods.</p><p>This study aims to address the challenges of obtaining sufficient supervisions and generating high-quality pseudo labels. Previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> have demonstrated repetitive patterns in texture and feature spaces under single-task learning in both natural and medical images. This observation raises the question of whether similar feature patterns exist in multi-task branches. To investigate this question, we conducted experiments and validated our findings in Fig. <ref type="figure" target="#fig_0">1</ref>. We first trained a network with segmentation and reconstruction branches and computed the feature distance distributions between one point and the rest of the regions for each class on both the segmentation and reconstruction feature maps. The features are extracted from the last conv layers in their branches.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> (c) shows there are similar patterns between reconstruction and segmentation feature distance maps for each class at the global level. Black color indicates smaller distances. The feature distances in segmentation maps appear to be cleaner than those in the recon maps. It suggests that segmentation features possess task-specific information, while recon features can be seen as a broader set with segmentation information.</p><p>Taking inspiration from the similar feature patterns observed in segmentation and reconstruction features, we propose a novel framework that utilizes a memory bank to generate pseudo labels. Our framework consists of an encoder that extracts visual features, as well as two decoders: one for segmenting target objects using scribble or point annotations, and another for reconstructing the input image. To address the challenge of seeking sufficient supervision, we employ the reconstruction branch as an auxiliary task to provide additional supervision and enable the network to learn visual representations. To tackle the challenge of generating high-quality pseudo labels, we use a VQ memory bank to store texture-oriented and global features, which we use to generate the pseudo labels. We then combine information from the global dataset and local image to generate improved, confident pseudo labels.</p><p>The contributions of this work can be summarized as follows. Firstly, a universal framework for annotation-efficient medical segmentation is proposed, which is capable of handling both scribble-supervised and point-supervised segmentation. Secondly, an auxiliary reconstruction branch is employed to provide more supervision and backwards sufficient gradients to learn visual representations. Thirdly, a novel pseudo label generation method from memory bank is proposed, which utilizes the VQ memory bank to store texture-oriented and global features to generate high-quality pseudo labels. To boost the model training, we generate high-quality pseudo labels by mixing the segmentation prediction and pseudo labels from the VQ bank. Finally, experimental results on public MRI segmentation datasets demonstrate the effectiveness of the proposed method. Specifically, our method outperforms existing scribble-supervised segmentation approaches on the ACDC dataset and also achieves better performance than several semi-supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this study, we focus on the problem of annotation-efficient medical image segmentation and propose a universal and adaptable framework for both scribblesupervised and point-supervised learning, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. These annotations involve only a subset of pixels in the image and present two challenges: seeking sufficient supervisions to train the network and generating high-quality pseudo labels. To overcome these challenges, we draw inspiration from the recent success of self-supervised learning and propose a framework that includes a reconstruction branch as an auxiliary task and a novel pseudo label generation method using VQ bank memory. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview:</head><p>The proposed framework for annotation-efficient medical image segmentation is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, which consists of a segmentation task and an auxiliary reconstruction task. Firstly, visual features are extracted using one encoder f , and then fed into one decoder g seg to learn from scribble or point annotations to segment target objects, as well as one decoder g recon to reconstruct the input image. The memory bank in the reconstruction branch is utilized to generate the pseudo labels, which are then used to assist in the training of the segmentation branch. The entire network is trained in an end-to-end manner.</p><p>Feature Extraction: In this work, we employ a U-Net <ref type="bibr" target="#b21">[22]</ref> as the encoder to extract features from the input image x. The size of the input patch is H Ã— W , and the resulting feature map F has the same size as the input patch. It is worth noting that the U-Net backbone used in our work can be replaced with other state-of-the-art structures. Our focus is on designing a universal framework for annotation-efficient medical segmentation, rather than on optimizing the network architecture for a specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation Branch:</head><p>The segmentation branch g seg takes the feature map F as input and produces the final segmentation masks based on the available scribble or point annotations. Following recent works such as <ref type="bibr" target="#b12">[13]</ref> [24] and <ref type="bibr" target="#b18">[19]</ref>, we utilize the partial cross-entropy loss to train the decoder L pCE (y, s) = c iâˆˆÏ‰s log y c i , where s denotes the annotation set with reduced annotation efficiency, and y c i is the predicted probability of pixel i belonging to class c. The set of labeled pixels in s is denoted by Ï‰ s . To note that the number of pixels s in point annotations is much less than that in scribble annotations, with around s &lt; 10 for each class. Reconstruction Branch: To address the first challenge of seeking sufficient supervisions to train a network with reduced annotations, we propose an auxiliary reconstruction branch. This branch is designed to add more supervision and provide sufficient gradients for learning visual representations. The reconstruction branch has the same decoder structure as the segmentation branch, except for the final prediction layer. We employ the mean squared error loss for the reconstruction task, given by L recon = |x -y recon | 2 F , where x is the input image and y recon is the predicted image.</p><p>VQ Memory Bank: Motivated by the similar feature patterns observed in medical images, we utilize the Vector Quantization (VQ) memory bank to store texture-oriented and global features, which are then employed for pseudo label generation. The pseudo label generation process involves three stages, as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Bank Definition.</head><p>In accordance with the VQVAE framework <ref type="bibr" target="#b26">[27]</ref> [34], we use a memory bank E to encode and store the visual features on reconstruction branch of the entire dataset. The memory bank E is defined as a dictionary of latent vectors E := e 1 , e 2 , ..., e n , where e i âˆˆ R 1Ã—64 represents the stored feature in the dictionary and n = 512 is the total size of the memory.</p><p>Memory Update Stage. The feature map F recon is obtained from the last layer in the reconstruction branch and is utilized to update the VQ memory bank and retrieve an augmented feature Frecon . For each spatial location f j âˆˆ R 1Ã—64 in F recon âˆˆ R 64Ã—256Ã—256 , we use L2 is used to compute the distance between f j and e k and find the nearest feature e i âˆˆ R 1Ã—64 in the VQ memory bank, as follows: fj = e i , i = arg min k |f j -e k | 2 2 . Following <ref type="bibr" target="#b26">[27]</ref>, we use the VQ loss to update the memory bank and encoder,</p><formula xml:id="formula_0">L V Q = |sg[f ] -e| 2 2 + |f -sg[e]| 2 2</formula><p>, where sg denotes the stop-gradient operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo Label Table Update</head><p>Stage. The second stage mainly updates a pseudo label table, using the labelled regions on the reconstruction features and assigning pseudo labels on memory vectors. In particular, it uses the labelled pixels and their corresponding reconstruction features and finds the nearest vectors in the memory bank. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, for the features extracted from the first class regions F 1 , its nearest memory vector is e 2 . Then, we need to assign probability [1, 0, 0] on e 2 and [0.7, 0.1, 0.2] is stored probabilities after doing Exponentially Moving Average (EMA) and delay is 0.9 in our implementation. We do the same thing for the rest labelled pixels. The pseudo label table is updated on each iteration and records the average values for each vectors.</p><p>Pseudo Label Generation Stage. The third stage utilizes the pseudo label table to generate the pseudo labels. It takes the feature map F recon as inputs, then finds their nearest memory vectors, and retrieve the pseudo label according to the vector indices. The generated pseudo label is generated by the repetitive texture patterns on the reconstruction branch, which would include the segmenation information as well as other things.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo Label Generation:</head><p>The generation of pseudo labels from the reconstruction branch is based on a texture-oriented and global view, as the memory bank stores the features extracted from the entire dataset. However, relying solely on it may not be sufficient, and it is necessary to incorporate more segmentation-specific information from the segmentation branch. Therefore, we leverage both approaches to enhance the model training.</p><p>To incorporate both the segmentation-specific information and the textureoriented and global information, we dynamically mix the predictions y 1 from the segmentation branch and the pseudo labels y 2 from the VQ memory bank to generate the final pseudo labels y * <ref type="bibr" target="#b35">[36]</ref>  <ref type="bibr" target="#b18">[19]</ref>. Specifically, we use the following equation:</p><formula xml:id="formula_1">y * = argmax[Î± Ã— y 1 +(1-Î±) Ã— y 2 ]</formula><p>, where Î± is uniformly sampled from [0, 1]. The argmax function is used to generate hard pseudo labels. We then use the generated y * to supervise y 1 and assist in the network training. The pseudo label loss is defined as L pl (P L, y 1 ) = 0.5 Ã— L dice (y * , y 1 ), where L dice is the dice loss, which can be substituted with other segmentation loss functions such as cross-entropy loss.</p><p>Loss Function: Finally, our loss function is calculated as</p><formula xml:id="formula_2">L = L pCE + L recon + L P LS (P L, y 1 ) + Î» V Q L V Q , (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where Î» V Q is hyper weights with Î» V Q = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setting</head><p>We use the PyTorch <ref type="bibr" target="#b20">[21]</ref> platform to implement our model with the following parameter settings: mini-batch size <ref type="bibr" target="#b31">(32)</ref>, learning rate (3.0e-2), and the number of iterations (60000). We employ the default initialization of PyTorch (1.8.0) to initialize the model. We evaluated our proposed universal framework on scribble and point annotations using the ACDC dataset <ref type="bibr" target="#b1">[2]</ref>. The dataset comprises 200 short-axis cine-MRI scans collected from 100 patients, with each patient having two annotated end-diastolic (ED) and end-systolic (ES) phases scans. Each scan has three structures with dense annotation, namely, the right ventricle (RV), myocardium (Myo), and left ventricle (LV). Following previous studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref> and consistent with the dataset's convention, we performed 2D slice segmentation instead of 3D volume segmentation. Scribble annotations are simulated by ITK-SNAP. To simulate point annotations, we randomly generated five points for each class. During testing, we predicted the segmentation slice by slice and combined them to form a 3D volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Comparisons</head><p>We conducted an evaluation of our model on the ACDC dataset, utilizing the 3D Dice Coefficient (DSC) and the 95% Hausdorff Distance (95) as the metrics. In this study, we compare our proposed model with various state-ofthe-art methods and designed baselines. These include: (1) scribble-supervised segmentation methods such as pCE only <ref type="bibr" target="#b14">[15]</ref> (lower bound), the model using pseudo labels generated by Random Walker (RW) <ref type="bibr" target="#b6">[7]</ref>, Uncertainty-aware Selfensembling and Transformation-consistent Model (USTM) <ref type="bibr" target="#b15">[16]</ref>, Scribble2Label (S2L) <ref type="bibr" target="#b12">[13]</ref>, Mumford-shah Loss (MLoss) <ref type="bibr" target="#b10">[11]</ref>, Entropy Minimization (EM) <ref type="bibr" target="#b7">[8]</ref> and Regularized Loss (RLoss) <ref type="bibr" target="#b23">[24]</ref>; (2) widely-used semi-supervised segmentation methods, including Deep Adversarial Network (DAN) <ref type="bibr" target="#b36">[37]</ref>, Adversarial Entropy Minimization (AdvEnt) <ref type="bibr" target="#b28">[29]</ref>, Mean Teacher (MT) <ref type="bibr" target="#b24">[25]</ref>, and Uncertainty Aware Mean Teacher (UAMT) <ref type="bibr" target="#b34">[35]</ref>. Additionally, we also conduct partially supervised (PS) learning, where only 10% labeled data is used to train the networks.  Table <ref type="table" target="#tab_0">1</ref> presents the performance comparisons of our proposed method with state-of-the-art methods on the ACDC dataset. Our proposed method employing scribble annotations achieves superior performance over existing semi-supervised and weakly-supervised methods. Furthermore, our proposed method utilizing a few point annotations demonstrates comparable performance with weaklysupervised methods and outperforms semi-supervised segmentation methods by a significant margin. Despite achieving slightly lower performance than fully supervised methods, our proposed method requires much lower annotation costs. We present the qualitative comparison results in Fig. <ref type="figure" target="#fig_3">4</ref>. The visual analysis of the results indicates that our proposed methods using scribble even point annotation perform well in terms of visual similarity with the ground truth. These results demonstrate the effectiveness of using scribble or point annotations as a potential way to reduce the annotation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>We investigate the effect of our proposed method on the ACDC datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the Auxiliary Task:</head><p>We designed a baseline by removing the reconstruction branch and VQ memory. The results in Table <ref type="table" target="#tab_1">2</ref> show a significant performance gap, indicating the importance of the reconstruction branch in stabilizing the training process. We also use local pixel-wise contrastive learning <ref type="bibr" target="#b8">[9]</ref> to replace reconstruction and keep the rest same. Results are 0.85 for point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Pseudo Labels:</head><p>We also designed a baseline using only the predictions as pseudo labels. In Table <ref type="table" target="#tab_1">2</ref>, the performance drop highlights the effectiveness of the texture-orient and global information in the VQ memory bank. The size and dimension of embedding of VQ bank are 512 and 64 the default setting in VQVAE. Model results (sizes of bank 64, 256, 512) are 0.865, 0.866, 0.866. We find 20-24 vectors are commonly used as clusters of 95% features and can set 64 as bank size to save memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Different Levels of Aannotations:</head><p>We also evaluated the impact of using different levels of annotations in point-supervised learning, ranging from more annotations, e.g. 10 points, to fewer annotations, e.g. 2 points. The results in Table <ref type="table" target="#tab_1">2</ref> indicate that clicking points is a promising data annotation approach to reduce annotation costs. Overall, our findings suggest that the proposed universal framework could effectively leverage different types of annotations and provide high-quality segmentation results with less annotation costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we introduce a universal framework for annotation-efficient medical segmentation. Our framework leverages an auxiliary reconstruction branch to provide additional supervision to learn visual representations and a novel pseudo label generation method from memory bank, which utilizes the VQ memory bank to store global features to generate high-quality pseudo labels. We evaluate the proposed method on a publicly available MRI segmentation dataset, and the experimental results demonstrate its effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustrations: (a) Examples of point and scribble annotations; (b) Performance comparison; (c) Investigation of similar feature patterns in multi-task branches. The reconstruction and segmentation Î” feature maps are feature distance between annotated points and the rest of the regions for each class on both the segmentation and reconstruction feature maps. The blue, red, and green dots is annotated points. BG, Myo, LV, RV, and UA are background, myocardium, left ventricle, right ventricle, and unannotated pixels.</figDesc><graphic coords="2,58,80,54,38,306,28,125,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed method. It consists of a segmentation task and an auxiliary reconstruction task. One encoder f is used to extract visual features, and one decoder gseg learns from scribble or point annotations to segment target objects, as well as one decoder grecon reconstructs the input image. The memory bank in the reconstruction branch is utilized to generate the pseudo labels, which are then used to assist in the training of the segmentation branch.</figDesc><graphic coords="4,61,29,54,59,301,84,118,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of the VQ Memory Bank.</figDesc><graphic coords="5,74,46,53,90,303,40,86,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Qualitative comparison of our method using different level of annotations.</figDesc><graphic coords="8,47,31,184,46,329,20,119,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance Comparisons on the ACDC dataset. All results are based on the 5-fold cross-validation with same backbone (UNet). Mean and standard variance values of 3D DSC and HD95 (mm) are presented in this table</figDesc><table><row><cell>Type Method</cell><cell>RV</cell><cell></cell><cell>Myo</cell><cell></cell><cell>LV</cell><cell></cell><cell>Mean</cell><cell></cell></row><row><cell></cell><cell>DSC</cell><cell>HD</cell><cell>DSC</cell><cell>HD</cell><cell>DSC</cell><cell>HD</cell><cell>DSC</cell><cell>HD</cell></row><row><cell>SSL PS</cell><cell>0.659(0.261)</cell><cell cols="2">26.8(30.4) 0.724(0.176)</cell><cell cols="2">16.0(21.6) 0.790(0.205)</cell><cell cols="2">24.5(30.4) 0.724(0.214)</cell><cell>22.5(27.5)</cell></row><row><cell>DAN</cell><cell>0.639(0.26)</cell><cell cols="2">20.6(21.4) 0.764(0.144)</cell><cell>9.4(12.4)</cell><cell>0.825(0.186)</cell><cell cols="2">15.9(20.8) 0.743(0.197)</cell><cell>15.3(18.2)</cell></row><row><cell>AdvEnt</cell><cell>0.615(0.296)</cell><cell cols="2">20.2(19.4) 0.760(0.151)</cell><cell>8.5(8.3)</cell><cell>0.848(0.159)</cell><cell cols="2">11.7(18.1) 0.741(0.202)</cell><cell>13.5(15.3)</cell></row><row><cell>MT</cell><cell>0.653(0.271)</cell><cell cols="2">18.6(22.0) 0.785(0.118)</cell><cell cols="2">11.4(17.0) 0.846(0.153)</cell><cell cols="2">19.0(26.7) 0.761(0.180)</cell><cell>16.3(21.9)</cell></row><row><cell>UAMT</cell><cell>0.660(0.267)</cell><cell cols="2">22.3(22.9) 0.773(0.129)</cell><cell cols="2">10.3(14.8) 0.847(0.157)</cell><cell cols="2">17.1(23.9) 0.760(0.185)</cell><cell>16.6(20.5)</cell></row><row><cell>WSL pCE</cell><cell>0.625(0.16)</cell><cell cols="2">187.2(35.2) 0.668(0.095)</cell><cell cols="2">165.1(34.4) 0.766(0.156)</cell><cell cols="2">167.7(55.0) 0.686(0.137)</cell><cell>173.3(41.5)</cell></row><row><cell>RW</cell><cell>0.813(0.113)</cell><cell cols="2">11.1(17.3) 0.708(0.066)</cell><cell>9.8(8.9)</cell><cell>0.844(0.091)</cell><cell>9.2(13.0)</cell><cell>0.788(0.09)</cell><cell>10.0(13.1)</cell></row><row><cell>USTM</cell><cell>0.815(0.115)</cell><cell cols="2">54.7(65.7) 0.756(0.081)</cell><cell cols="2">112.2(54.1) 0.785(0.162)</cell><cell cols="2">139.6(57.7) 0.786(0.119)</cell><cell>102.2(59.2)</cell></row><row><cell>S2L</cell><cell>0.833(0.103)</cell><cell cols="2">14.6(30.9) 0.806(0.069)</cell><cell cols="2">37.1(49.4) 0.856(0.121)</cell><cell cols="2">65.2(65.1) 0.832(0.098)</cell><cell>38.9(48.5)</cell></row><row><cell>MLoss</cell><cell>0.809(0.093)</cell><cell cols="2">17.1(30.8) 0.832(0.055)</cell><cell cols="2">28.2(43.2) 0.876(0.093)</cell><cell cols="2">37.9(59.6) 0.839(0.080)</cell><cell>27.7(44.5)</cell></row><row><cell>EM</cell><cell>0.839(0.108)</cell><cell cols="2">25.7(44.5) 0.812(0.062)</cell><cell cols="2">47.4(50.6) 0.887(0.099)</cell><cell cols="2">43.8(57.6) 0.846(0.089)</cell><cell>39.0(50.9)</cell></row><row><cell>RLoss</cell><cell>0.856(0.101)</cell><cell>7.9(12.6)</cell><cell>0.817(0.054)</cell><cell>6.0(6.9)</cell><cell>0.896(0.086)</cell><cell>7.0(13.5)</cell><cell>0.856(0.080)</cell><cell>6.9(11.0)</cell></row><row><cell>WSL4MI</cell><cell>0.861(0.096)</cell><cell>7.9(12.5)</cell><cell>0.842(0.054)</cell><cell>9.7(23.2)</cell><cell>0.913(0.082)</cell><cell cols="2">12.1(27.2) 0.872(0.077)</cell><cell>9.9(21.0)</cell></row><row><cell>Ours-points</cell><cell>0.843(0.002)</cell><cell>4.7(8.8)</cell><cell>0.842(0.001)</cell><cell>9.0(30.8)</cell><cell>0.916(0.001)</cell><cell>9.7(27.7)</cell><cell>0.866(0.001)</cell><cell>5.1(8.2)</cell></row><row><cell cols="3">Ours-scribbles 0.858(0.001) 3.4(4.9)</cell><cell cols="2">0.857(0.001) 3.7(3.2)</cell><cell cols="2">0.919(0.001) 4.3(4.0)</cell><cell cols="2">0.881(0.001) 3.8(2.7)</cell></row><row><cell>FSL FullSup</cell><cell>0.882(0.095)</cell><cell>6.9(10.8)</cell><cell>0.883(0.042)</cell><cell>5.9(15.2)</cell><cell>0.930(0.074)</cell><cell>8.1(20.9)</cell><cell>0.898(0.070)</cell><cell>7.0(15.6)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This research/project is supported by the <rs type="funder">National Research Foundation, Singapore</rs> under its <rs type="programName">AI Singapore Programme (AISG Award</rs> No: <rs type="grantNumber">AISG2-TC-2021-003</rs>) This work was supported by the <rs type="funder">Agency for Science, Technology and Research (A*STAR)</rs> through its <rs type="programName">AME Programmatic Funding Scheme Under Project A20H4b0141</rs>. This work was partially supported by <rs type="funder">A*STAR Central Research Fund "A Secure and Privacy Preserving AI Platform for Digital Health</rs>"</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Hyp6JSH">
					<idno type="grant-number">AISG2-TC-2021-003</idno>
					<orgName type="program" subtype="full">AI Singapore Programme (AISG Award</orgName>
				</org>
				<org type="funding" xml:id="_c43PuM9">
					<orgName type="program" subtype="full">AME Programmatic Funding Scheme Under Project A20H4b0141</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for network-based cardiac MR image segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66185-8_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66185-829" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Duchesne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10434</biblScope>
			<biblScope unit="page" from="253" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is the problem solved?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lalande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Transunet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2613" to="2622" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Teach me to segment with mixed supervision: confident students become masters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-78191-0_40</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-78191-040" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sommer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12729</biblScope>
			<biblScope unit="page" from="517" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inter extreme points geodesics for end-to-end weakly supervised image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dorent</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_57</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-357" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="615" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised contrastive learning for labelefficient medical image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_45</idno>
		<idno>978-3-030-87196-3 45</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">nnu-net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mumford-shah loss functional for image segmentation with deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1856" to="1866" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pseudo-label: the simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scribble2Label: scribble-supervised cell segmentation via self-generating pseudo-labels with consistency</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Jeong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_2</idno>
		<idno>978-3-030-59710-8 2</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="14" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Few-shot domain adaptation with polymorphic transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-331" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="330" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scribblesup: scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Weakly supervised segmentation of covid19 infection with scribble annotation on ct images. Pattern Recogn</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page">108341</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation via strong-weak dualbranch network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58558-7_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58558-7" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12350</biblScope>
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image segmentation through dual-task consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8801" to="8809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scribble-supervised medical image segmentation via dualbranch network and dynamically mixed pseudo labels supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02106</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient semi-supervised gross target volume of nasopharyngeal carcinoma segmentation via uncertainty rectified pyramid consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-330" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pytorch: an imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">grabcut&quot; interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On regularized losses for weakly-supervised cnn segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="507" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1195" to="1204" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to segment from scribbles using multi-scale adversarial attention gates</title>
		<author>
			<persName><forename type="first">G</forename><surname>Valvano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1990">1990-2001 (2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Growcut: interactive multi-label nd image segmentation by cellular automata</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Konouchine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Graphicon</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="150" to="156" />
			<date type="published" when="2005">2005</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Advent: adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>PÃ©rez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-tuning for data-efficient deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10738" to="10748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised left atrium segmentation with mutual consistency training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-328" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Facing annotation redundancy: Oct layer segmentation with only 10 annotated pixels per layer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S M</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16876-5_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16876-513" />
	</analytic>
	<monogr>
		<title level="m">REMIA 2022</title>
		<editor>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Petitjean</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="126" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Partially-supervised learning for vessel segmentation in ocular images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-226" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Crowd counting with partial annotations in an image</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15570" to="15579" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Uncertainty-aware self-ensembling model for semi-supervised 3D left atrium segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-867" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep adversarial networks for biomedical image segmentation utilizing unannotated images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredericksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66179-7_47</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66179-747" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Duchesne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10435</biblScope>
			<biblScope unit="page" from="408" to="416" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
