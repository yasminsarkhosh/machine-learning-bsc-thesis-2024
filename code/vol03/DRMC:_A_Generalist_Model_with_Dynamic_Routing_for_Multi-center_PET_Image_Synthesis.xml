<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiwen</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biological Science and Medical Engineering</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education</orgName>
								<orgName type="department" key="dep3">Advanced Innovation Center for Biomedical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">I3 is Department of Nuclear Medicine</orgName>
								<orgName type="department" key="dep2">I4 is Department of Nuclear Medicine</orgName>
								<orgName type="laboratory" key="lab1">I1 and I5 are Peking</orgName>
								<orgName type="laboratory" key="lab2">I6 is Beijing Friendship Hospital</orgName>
								<orgName type="institution" key="instit1">Union Medical College Hospital</orgName>
								<orgName type="institution" key="instit2">I2 is Beijing Hospital</orgName>
								<orgName type="institution" key="instit3">Ruijin Hospital</orgName>
								<orgName type="institution" key="instit4">Shanghai Jiao Tong University School of Medicine</orgName>
								<orgName type="institution" key="instit5">University of Bern</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biological Science and Medical Engineering</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education</orgName>
								<orgName type="department" key="dep3">Advanced Innovation Center for Biomedical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">I3 is Department of Nuclear Medicine</orgName>
								<orgName type="department" key="dep2">I4 is Department of Nuclear Medicine</orgName>
								<orgName type="laboratory" key="lab1">I1 and I5 are Peking</orgName>
								<orgName type="laboratory" key="lab2">I6 is Beijing Friendship Hospital</orgName>
								<orgName type="institution" key="instit1">Union Medical College Hospital</orgName>
								<orgName type="institution" key="instit2">I2 is Beijing Hospital</orgName>
								<orgName type="institution" key="instit3">Ruijin Hospital</orgName>
								<orgName type="institution" key="instit4">Shanghai Jiao Tong University School of Medicine</orgName>
								<orgName type="institution" key="instit5">University of Bern</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bingzheng</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Xiaomi Corporation</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yubo</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biological Science and Medical Engineering</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education</orgName>
								<orgName type="department" key="dep3">Advanced Innovation Center for Biomedical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">I3 is Department of Nuclear Medicine</orgName>
								<orgName type="department" key="dep2">I4 is Department of Nuclear Medicine</orgName>
								<orgName type="laboratory" key="lab1">I1 and I5 are Peking</orgName>
								<orgName type="laboratory" key="lab2">I6 is Beijing Friendship Hospital</orgName>
								<orgName type="institution" key="instit1">Union Medical College Hospital</orgName>
								<orgName type="institution" key="instit2">I2 is Beijing Hospital</orgName>
								<orgName type="institution" key="instit3">Ruijin Hospital</orgName>
								<orgName type="institution" key="instit4">Shanghai Jiao Tong University School of Medicine</orgName>
								<orgName type="institution" key="instit5">University of Bern</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biological Science and Medical Engineering</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education</orgName>
								<orgName type="department" key="dep3">Advanced Innovation Center for Biomedical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">I3 is Department of Nuclear Medicine</orgName>
								<orgName type="department" key="dep2">I4 is Department of Nuclear Medicine</orgName>
								<orgName type="laboratory" key="lab1">I1 and I5 are Peking</orgName>
								<orgName type="laboratory" key="lab2">I6 is Beijing Friendship Hospital</orgName>
								<orgName type="institution" key="instit1">Union Medical College Hospital</orgName>
								<orgName type="institution" key="instit2">I2 is Beijing Hospital</orgName>
								<orgName type="institution" key="instit3">Ruijin Hospital</orgName>
								<orgName type="institution" key="instit4">Shanghai Jiao Tong University School of Medicine</orgName>
								<orgName type="institution" key="instit5">University of Bern</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">https://github.com/Yaziwel/Multi-Center-PET-Image-Synthesis</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="36" to="46"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6FF84577EC996095AB47D043E0C51053</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-Center</term>
					<term>Positron Emission Tomography</term>
					<term>Synthesis</term>
					<term>Generalist Model</term>
					<term>Dynamic Routing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-center positron emission tomography (PET) image synthesis aims at recovering low-dose PET images from multiple different centers. The generalizability of existing methods can still be suboptimal for a multi-center study due to domain shifts, which result from non-identical data distribution among centers with different imaging systems/protocols. While some approaches address domain shifts by training specialized models for each center, they are parameter inefficient and do not well exploit the shared knowledge across centers. To address this, we develop a generalist model that shares architecture and parameters across centers to utilize the shared knowledge. However, the generalist model can suffer from the center interference issue, i.e. the gradient directions of different centers can be inconsistent or even opposite owing to the non-identical data distribution. To mitigate such interference, we introduce a novel dynamic routing strategy with cross-layer connections that routes data from different centers to different experts. Experiments show that our generalist model with dynamic routing (DRMC) exhibits excellent generalizability across centers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Positron emission tomography (PET) image synthesis <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> aims at recovering high-quality full-dose PET images from low-dose ones. Despite great success, most algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> are specialized for PET data from a single center with a fixed imaging system/protocol. This poses a significant problem for practical applications, which are not usually restricted to any one of the centers. Towards filling this gap, in this paper, we focus on multi-center PET image synthesis, aiming at processing data from multiple different centers.</p><p>However, the generalizability of existing models can still be suboptimal for a multi-center study due to domain shift, which results from non-identical data distribution among centers with different imaging systems/protocols (see Fig. <ref type="figure" target="#fig_1">1</ref> (a)). Though some studies have shown that a specialized model (i.e. a convolutional neural network (CNN) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> or Transformer <ref type="bibr" target="#b8">[9]</ref> trained on a single center) exhibits certain robustness to different tracer types <ref type="bibr" target="#b8">[9]</ref>, different tracer doses <ref type="bibr" target="#b2">[3]</ref>, or even different centers <ref type="bibr" target="#b5">[6]</ref>, such generalizability of a center-specific knowledge is only applicable to small domain shifts. It will suffer a severe performance drop when exposed to new centers with large domain shifts <ref type="bibr" target="#b10">[11]</ref>. There are also some federated learning (FL) based <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> medical image synthesis methods that improve generalizability by collaboratively learning a shared global model across centers. Especially, federated transfer learning (FTL) <ref type="bibr" target="#b6">[7]</ref> first successfully applies FL to PET image synthesis in a multiple-dose setting. Since the resultant shared model of the basic FL method <ref type="bibr" target="#b11">[12]</ref> ignores center specificity and thus cannot handle centers with large domain shifts, FTL addresses this by finetuning the shared model for each center/dose. However, FTL only focuses on different doses and does not really address the multi-center problem. Furthermore, it still requires a specialized model for each center/dose, which ignores potentially transferable shared knowledge across centers and scales up the overall model size.</p><p>A recent trend, known as generalist models, is to request that a single unified model works for multiple tasks/domains, and even express generalizability to novel tasks/domains. By sharing architecture and parameters, generalist models can better utilize shared transferable knowledge across tasks/domains. Some pioneers <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> have realized competitive performance on various high-level vision tasks like classification <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>, object detection <ref type="bibr" target="#b13">[14]</ref>, etc.</p><p>Nonetheless, recent studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref> report that conventional generalist <ref type="bibr" target="#b14">[15]</ref> models may suffer from the interference issue, i.e. different tasks with shared parameters potentially conflict with each other in the update directions of the gradient. Specific to PET image synthesis, due to the non-identical data distribution across centers, we also observe the center interference issue that the gradient directions of different centers may be inconsistent or even opposite (see Fig. <ref type="figure" target="#fig_1">1</ref>). This will lead to an uncertain update direction that deviates from the optimal, resulting in sub-optimal performance of the model. To address the interference issue, recent generalist models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref> have introduced dynamic routing <ref type="bibr" target="#b18">[19]</ref> which learns to activate experts (i.e. sub-networks) dynamically. The input feature will be routed to different selected experts accordingly so as to avoid interference. Meanwhile, different inputs can share some experts, thus maintaining collaboration across domains. In the inference time, the model can reasonably generalize to different domains, even unknown domains, by utilizing the knowledge of existing experts. In spite of great success, the study of generalist models rarely targets the problem of multi-center PET image synthesis.</p><p>In this paper, inspired by the aforementioned studies, we innovatively propose a generalist model with Dynamic Routing for Multi-Center PET image synthesis, termed DRMC. To mitigate the center interference issue, we propose a novel dynamic routing strategy to route data from different centers to different experts. Compared with existing routing strategies, our strategy makes an improvement by building cross-layer connections for more accurate expert decisions. Extensive experiments show that DRMC achieves the best generalizability on both known and unknown centers. Our contribution can be summarized as:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Center Interference Issue</head><p>Due to the non-identical data distribution across centers, different centers with shared parameters may conflict with each other in the optimization process.</p><p>To verify this hypothesis, we train a baseline Transformer with 15 base blocks (Fig. <ref type="figure" target="#fig_2">2 (b</ref>)) over four centers. Following the paper <ref type="bibr" target="#b15">[16]</ref>, we calculate the gradient direction interference metric I i,j of the j-th center C j on the i-th center C i . As shown in Fig. <ref type="figure" target="#fig_1">1</ref> (b), interference is observed between different centers at different layers. This will lead to inconsistent optimization and inevitably degrade the model performance. Details of I i,j <ref type="bibr" target="#b15">[16]</ref> are shown in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Architecture</head><p>The overall architecture of our DRMC is shown in Fig. <ref type="figure" target="#fig_2">2</ref> (a). DRMC firstly applies a 3×3×3 convolutional layer for shallow feature extraction. Next, the shallow feature is fed into N blocks with dynamic routing (DRBs), which are expected to handle the interference between centers and adaptively extract the deep feature with high-frequency information. The deep feature then passes through another 3×3×3 convolutional layer for final image synthesis. In order to alleviate the burden of feature learning and stabilize training, DRMC adopts global residual learning as suggested in the paper <ref type="bibr" target="#b19">[20]</ref> to estimate the image residual from different centers. In the subsequent subsection, we will expatiate the dynamic routing strategy as well as the design of the DRB. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dynamic Routing Strategy</head><p>We aim at alleviating the center interference issue in deep feature extraction.</p><p>Inspired by prior generalist models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>, we specifically propose a novel dynamic routing strategy for multi-center PET image synthesis. The proposed dynamic routing strategy can be flexibly adapted to various network architectures, such as CNN and Transformer. To utilize the recent advance in capturing global contexts using Transformers <ref type="bibr" target="#b8">[9]</ref>, without loss of generality, we explore the application of the dynamic routing strategy to a Transformer block, termed dynamic routing block (DRB, see Fig. <ref type="figure" target="#fig_2">2 (c)</ref>). We will introduce our dynamic routing strategy in detail from four parts: base expert foundation, expert number scaling, expert dynamic routing, and expert sparse fusion.</p><p>Base Expert Foundation. As shown in Fig. <ref type="figure" target="#fig_2">2</ref> (b), we first introduce an efficient base Transformer block (base block) consisting of an attention expert and a feedforward network (FFN) expert. Both experts are for basic feature extraction and transformation. To reduce the complexity burden of the attention expert, we follow the paper <ref type="bibr" target="#b8">[9]</ref> to perform global channel attention with linear complexity instead of spatial attention <ref type="bibr" target="#b20">[21]</ref>. Notably, as the global channel attention may ignore the local spatial information, we introduce depth-wise convolutions to emphasize the local context after applying attention. As for the FFN expert, we make no modifications to it compared with the standard Transformer block <ref type="bibr" target="#b20">[21]</ref>.</p><p>It consists of a 2-layer MLP with GELU activation in between.</p><p>Expert Number Scaling. Center interference is observed on both attention experts and FFN experts at different layers (see Fig. <ref type="figure" target="#fig_1">1 (b)</ref>). This indicates that a single expert can not be simply shared by all centers. Thus, we increase the number of experts in the base block to M to serve as expert candidates for different centers. Specifically, each Transformer block has an attention expert bank</p><formula xml:id="formula_0">E AT T = [E 1 AT T , E 2 AT T , ..., E M AT T ] and an FFN expert bank E F F N = [E 1 F F N , E 2 F F N , ..., E M F F N ]</formula><p>, both of which have M base experts. However, it does not mean that we prepare specific experts for each center. Although using center-specific experts can address the interference problem, it is hard for the model to exploit the shared knowledge across centers, and it is also difficult to generalize to new centers that did not emerge in the training stage <ref type="bibr" target="#b15">[16]</ref>. To address this, we turn to different combinations of experts.</p><p>Expert Dynamic Routing. Given a bank of experts, we route data from different centers to different experts so as to avoid interference. Prior generalist models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16</ref>] in high-level vision tasks have introduced various routing strategies to weigh and select experts. Most of them are independently conditioned on the information of the current layer feature, failing to take into account the connectivity of neighboring layers. Nevertheless, PET image synthesis is a dense prediction task that requires a tight connection of adjacent layers for accurate voxel-wise intensity regression. To mitigate the potential discontinuity <ref type="bibr" target="#b12">[13]</ref>, we propose a dynamic routing module (DRM, see Fig. <ref type="figure" target="#fig_2">2 (c</ref>)) that builds cross-layer connection for expert decisions. The mechanism can be formulated as:</p><formula xml:id="formula_1">W = ReLU(MLP([GAP(X), H])),<label>(1)</label></formula><p>where X denotes the input; GAP(•) represents the global average pooling operation to aggregate global context information of the current layer; H is the hidden representation of the previous MLP layer. ReLU activation generates sparsity by setting the negative weight to zero. W is a sparse weight used to assign weights to different experts.</p><p>In short, DRM sparsely activates the model and selectively routes the input to different subsets of experts. This process maximizes collaboration and meanwhile mitigates the interference problem. On the one hand, the interference across centers can be alleviated by sparsely routing X to different experts (with positive weights). The combinations of selected experts can be thoroughly different across centers if violent conflicts appear. On the other hand, experts in the same bank still cooperate with each other, allowing the network to best utilize the shared knowledge across centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expert Sparse Fusion.</head><p>The final output is a weighted sum of each expert's knowledge using the sparse weight W = [W 1 , W 2 , ..., W M ] generated by DRM. Given an input feature X, the output X of an expert bank can be obtained as:</p><formula xml:id="formula_2">X = M m=1 W m • E m (X),<label>(2)</label></formula><p>where E m (•) represents an operator of E m AT T (•) or E m F F N (•). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Loss Function</head><p>We utilize the Charbonnier loss <ref type="bibr" target="#b22">[23]</ref> with hyper-parameter as 10 -3 to penalize pixel-wise differences between the full-dose (Y ) and estimated ( Ŷ ) PET images:</p><formula xml:id="formula_3">L = Y -Ŷ 2 + 2 . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation</head><p>Full-dose PET images are collected from 6 different centers (C 1 -C 6 ) at 6 different institutions 1 . The data of C 3 and C 4 <ref type="bibr" target="#b21">[22]</ref> are borrowed from the Ultra-low Dose PET Imaging Challenge 2 , while the data from other centers were privately collected. The key information of the whole dataset is shown in Table <ref type="table" target="#tab_0">1</ref>. Note that C 1 -C 4 are for both training and testing. We denote them as C kn as these centers are known to the generalist model. C 5 and C 6 are unknown centers (denote as C ukn ) that are only for testing the model generalizability. The low-dose PET data is generated by randomly selecting a portion of the raw scans based on the dose reduction factor (DRF), such as 25% when DRF=4. Then we reconstruct low-dose PET images using the standard OSEM method <ref type="bibr" target="#b23">[24]</ref>. Since the voxel size differs across centers, we uniformly resample the images of different centers so that their voxel size becomes 2×2×2 mm 3 . In the training phase, we unfold images into small patches (uniformly sampling 1024 patches from 20 patients per center) with a shape of 64×64×64. In the testing phase, the whole estimated PET image is acquired by merging patches together.</p><p>To evaluate the model performance, we choose the PSNR metric for image quantitative evaluation. For clinical evaluation, to address the accuracy of the standard uptake value (SUV) that most radiologists care about, we follow the paper <ref type="bibr" target="#b2">[3]</ref> to calculate the bias of SU V mean and SU V max (denoted as B mean and B max , respectively) between low-dose and full-dose images in lesion regions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation</head><p>Unless specified otherwise, the intermediate channel number, expert number in a bank, and Transformer block number are 64, 3, and 5, respectively. We employ Adam optimizer with a learning rate of 10 -4 . We implement our method with Pytorch using a workstation with 4 NVIDIA A100 GPUs with 40GB memory (1 GPU per center). In each training iteration, each GPU independently samples data from a single center. After the loss calculation and the gradient backpropagation, the gradients of different GPUs are then synchronized. We train our model for 200 epochs in total as no significant improvement afterward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparative Experiments</head><p>We compare our method with five methods of two types. (i) 3D-cGAN <ref type="bibr" target="#b0">[1]</ref> and 3D CVT-GAN <ref type="bibr" target="#b9">[10]</ref> are two state-of-the-art methods for single center PET image synthesis. (ii) FedAVG <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, FL-MRCM <ref type="bibr" target="#b10">[11]</ref>, and FTL <ref type="bibr" target="#b6">[7]</ref> are three federated learning methods for privacy-preserving multi-center medical image synthesis.</p><p>All methods are trained using data from C kn and tested over both C kn and C ukn .</p><p>For methods in (i), we regard C kn as a single center and mix all data together for training. For federated learning methods in (ii), we follow the "Mix" mode (upper bound of FL-based methods) in the paper <ref type="bibr" target="#b10">[11]</ref> to remove the privacy constraint and keep the problem setting consistent with our multi-center study.</p><p>Comparison Results for Known Centers. As can be seen in Table <ref type="table" target="#tab_1">2</ref>, in comparison with the second-best results, DRMC boosts the performance by 0.77 dB PSNR, 0.0078 B mean , and 0.0135 B max . This is because our DRMC not only leverages shared knowledge by sharing some experts but also preserves center-specific information with the help of the sparse routing strategy. Further evaluation can be found in the supplement.</p><p>Comparison Results for Unknown Centers. We also test the model generalization ability to unknown centers C 5 and C 6 . C 5 consists of normal brain data (without lesion) that is challenging for generalization. As the brain region only occupies a small portion of the whole-body data in the training dataset but has more sophisticated structure information. C 6 is a similar center to C 1 but has different working locations and imaging preferences. The quantitative results are shown in Table <ref type="table" target="#tab_2">3</ref> and the visual results are shown in Fig. <ref type="figure" target="#fig_1">1 (a)</ref>. DRMC achieves the best results by dynamically utilizing existing experts' knowledge for generalization. On the contrary, most comparison methods process data in a static pattern and unavoidably produce mishandling of out-of-distribution data. Furthermore, we investigate model's robustness to various DRF data, and the results are available in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>Specialized Model vs. Generalist Model. As can be seen in Table <ref type="table" target="#tab_4">5</ref>, the baseline model (using 15 base blocks) individually trained for each center acquires good performance on its source center. But it suffers performance drop on other centers. The baseline model trained over multiple centers greatly enhances the overall results. But due to the center interference issue, its performance on a specific center is still far from the corresponding specialized model. DRMC mitigates the interference with dynamic routing and achieves comparable performance to the specialized model of each center. Ablation Study of Routing Strategy. To investigate the roles of major components in our routing strategy, we conduct ablation studies through (i) removing the condition of hidden representation H that builds cross-layer connection, and replacing ReLU activation with (ii) softmax activation <ref type="bibr" target="#b13">[14]</ref> and (iii) top-2 gating <ref type="bibr" target="#b12">[13]</ref>. The results are shown in Table <ref type="table" target="#tab_3">4</ref>. We also analyze the interpretability of the routing by showing the distribution of different layers' top-1 weighted experts using the testing data. As shown in Fig. <ref type="figure" target="#fig_3">3</ref> (b), different centers show similarities and differences in the expert distribution. For example, C 6 shows the same distribution with C 1 as their data show many similarities, while C 5 presents a very unique way since brain data differs a lot from whole-body data.</p><p>Ablation Study of Hyperparameters. In Fig. <ref type="figure" target="#fig_3">3</ref> (c) and (d), we show ablation results on expert number (M ) and block number (N ). We set M =3 and N =5, as it has realized good performance with acceptable computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we innovatively propose a generalist model with dynamic routing (DRMC) for multi-center PET image synthesis. To address the center interference issue, DRMC sparsely routes data from different centers to different experts. Experiments show that DRMC achieves excellent generalizability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>-</head><label></label><figDesc>A generalist model called DRMC is proposed, which enables multi-center PET image synthesis with a single unified model. -A novel dynamic routing strategy with cross-layer connection is proposed to address the center interference issue. It is realized by dynamically routing data from different centers to different experts. -Extensive experiments show that DRMC exhibits excellent generalizability over multiple different centers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Examples of PET images at different Centers. There are domain shifts between centers. (b) The interference metric Ii,j [16] of the center Cj on the center Ci at the 1-st/4-th blocks as examples. The red value indicates that Cj has a negative impact on Ci, and the green value indicates that Cj has a positive impact on Ci.</figDesc><graphic coords="3,47,79,274,16,124,03,75,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The framework of our proposed DRMC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Figures of different experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Multi-Center PET Dataset Information</figDesc><table><row><cell>Center</cell><cell cols="2">Institution Type</cell><cell cols="2">Lesion System</cell><cell>Tracer</cell><cell>Dose</cell><cell cols="2">DRF Spacing (mm 3 ) Shape</cell><cell>Train Test</cell></row><row><cell>Ckn C1</cell><cell>I1</cell><cell cols="2">Whole Body Yes</cell><cell>PolarStar m660</cell><cell cols="3">18 F-FDG 293MBq 12</cell><cell>3.15×3.15×1.87 192×192×slices 20</cell><cell>10</cell></row><row><cell>C2</cell><cell>I2</cell><cell cols="2">Whole Body Yes</cell><cell>PolarStar Flight</cell><cell cols="3">18 F-FDG 293MBq 4</cell><cell>3.12×3.12×1.75 192×192×slices 20</cell><cell>10</cell></row><row><cell cols="2">C3 [22] I3</cell><cell cols="2">Whole Body Yes</cell><cell>United Imaging uEXPLORER</cell><cell cols="3">18 F-FDG 296MBq 10</cell><cell>1.67×1.67×2.89 256×256×slices 20</cell><cell>10</cell></row><row><cell cols="2">C4 [22] I4</cell><cell cols="2">Whole Body Yes</cell><cell cols="4">Siemens Biograph Vision Quadra 18 F-FDG 296MBq 10</cell><cell>1.65×1.65×1.65 256×256×slices 20</cell><cell>10</cell></row><row><cell>Cukn C5</cell><cell>I5</cell><cell>Brain</cell><cell>No</cell><cell>PolarStar m660</cell><cell cols="3">18 F-FDG 293MBq 4</cell><cell>1.18×1.18×1.87 256×256×slices -</cell><cell>10</cell></row><row><cell>C6</cell><cell>I6</cell><cell cols="2">Whole Body Yes</cell><cell>PolarStar m660</cell><cell cols="3">18 F-FDG 293MBq 12</cell><cell>3.15×3.15×1.87 192×192×slices -</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on C kn . The Best and the Second-Best Results are Highlighted. *: Significant Difference at p &lt; 0.05 between Comparison Method and Our Method.</figDesc><table><row><cell>Methods</cell><cell>PSNR↑</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Bmean↓</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Bmax↓</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell><cell>Avg</cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell><cell>Avg</cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell><cell>Avg</cell></row><row><cell>(i) 3D-cGAN</cell><cell cols="15">47.30* 44.97* 45.15* 43.08* 45.13* 0.0968* 0.0832 0.0795* 0.1681* 0.1069* 0.1358* 0.1696* 0.1726* 0.2804* 0.1896*</cell></row><row><cell cols="16">3D CVT-GAN 47.46* 45.17* 45.94* 44.04* 45.65* 0.0879 0.0972* 0.0594* 0.1413* 0.0965* 0.1178* 0.1591* 0.1652* 0.2224* 0.1661*</cell></row><row><cell>(ii) FedAVG</cell><cell cols="15">47.43* 44.62* 45.61* 43.75* 45.35* 0.0985* 0.0996* 0.1006* 0.2202* 0.1122* 0.1459* 0.1546* 0.2011* 0.2663* 0.1920*</cell></row><row><cell>FL-MRCM</cell><cell cols="15">47.81* 45.56* 46.10* 44.31* 45.95* 0.0939* 0.0929* 0.0631* 0.1344* 0.0961* 0.1571* 0.1607* 0.1307* 0.1518* 0.1501*</cell></row><row><cell>FTL</cell><cell cols="15">48.05* 45.62* 46.01* 44.75* 46.11* 0.0892 0.0945* 0.0587* 0.0895 0.0830* 0.1243* 0.1588* 0.0893 0.1436 0.1290*</cell></row><row><cell>DRMC</cell><cell cols="15">49.48 46.32 46.71 45.01 46.88 0.0844 0.0792 0.0491 0.0880 0.0752 0.1037 0.1313 0.0837 0.1431 0.1155</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on C ukn .</figDesc><table><row><cell>Methods</cell><cell>PSNR↑</cell><cell></cell><cell>Bmean↓</cell><cell>Bmax↓</cell></row><row><cell></cell><cell>C5</cell><cell>C6</cell><cell>C5 C6</cell><cell>C5 C6</cell></row><row><cell>(i) 3D-cGAN</cell><cell cols="4">26.53* 46.07* -0.1956* -0.1642*</cell></row><row><cell cols="5">3D CVT-GAN 27.11* 46.03* -0.1828 -0.1686*</cell></row><row><cell>(ii) FedAVG</cell><cell cols="4">27.09* 46.48* -0.1943* -0.2291*</cell></row><row><cell>FL-MRCM</cell><cell cols="4">25.38* 47.08* -0.1998* -0.1762*</cell></row><row><cell>FTL</cell><cell cols="4">27.38* 48.05* -0.1898* -0.1556*</cell></row><row><cell>DRMC</cell><cell>28.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>54 48.26 -0.1814 -0.1483Table 4 .</head><label>4</label><figDesc>Routing Ablation Results.</figDesc><table><row><cell>Methods</cell><cell>Ckn</cell><cell>Cukn</cell></row><row><cell></cell><cell cols="2">PSNR↑ Bmean↓ Bmax↓ PSNR↑ Bmean↓ Bmax↓</cell></row><row><cell>w/o H</cell><cell cols="2">46.64* 0.0907* 0.1436* 38.23* 0.1826 0.1548*</cell></row><row><cell>Softmax</cell><cell cols="2">46.70* 0.0849* 0.1277* 38.33 0.1864* 0.1524*</cell></row><row><cell cols="3">Top-2 Gating 46.61* 0.0896* 0.1295* 38.38 0.1867* 0.1564*</cell></row><row><cell>DRMC</cell><cell>46.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>88 0.0752 0.1155 38.40 0.1814 0.1483Table 5 .</head><label>5</label><figDesc>Comparison results for Specialized Models and Generalist Models.</figDesc><table><row><cell>Methods</cell><cell cols="2">Train Centers PNSR↑</cell><cell></cell><cell></cell><cell></cell><cell>Bmean↓</cell><cell></cell><cell></cell><cell></cell><cell>Bmax↓</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Test Centers</cell><cell></cell><cell></cell><cell cols="2">Avg. Test Centers</cell><cell></cell><cell>Avg.</cell><cell cols="2">Test Centers</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell></row><row><cell cols="2">Specialized Model Baseline C1</cell><cell cols="11">48.89* 45.06* 43.94* 41.55* 44.86* 0.0849 0.0949* 0.1490* 0.2805* 0.1523* 0.1207* 0.1498* 0.3574* 0.4713* 0.2748*</cell></row><row><cell></cell><cell>C2</cell><cell cols="11">47.05* 46.08* 43.82* 41.53* 44.62* 0.0933* 0.0557* 0.1915* 0.2247* 0.1413* 0.1326* 0.1243* 0.3275* 0.4399* 0.2561*</cell></row><row><cell></cell><cell>C3</cell><cell cols="11">44.04* 41.00* 46.52* 44.07* 44.11* 0.2366* 0.2111* 0.0446 0.1364* 0.1572* 0.4351* 0.5567* 0.0729* 0.1868* 0.3129*</cell></row><row><cell></cell><cell>C4</cell><cell cols="11">44.41* 41.39* 46.01* 44.95 44.29* 0.2462* 0.2063* 0.0897* 0.0966* 0.1597* 0.4887* 0.5882* 0.1222* 0.1562* 0.3388*</cell></row><row><cell>Generalist</cell><cell cols="12">Baseline C1, C2, C3, C4 47.59* 44.73* 46.02* 44.20* 45.64* 0.0924* 0.0839* 0.0844* 0.1798* 0.1101* 0.1424* 0.1424* 0.1579* 0.2531* 0.1740*</cell></row><row><cell>Model</cell><cell cols="7">DRMC C1, C2, C3, C4 49.48 46.32 46.71 45.01 46.88 0.0844 0.0792</cell><cell cols="4">0.0491 0.0880 0.0752 0.1037 0.1313</cell><cell>0.0837</cell><cell>0.1431 0.1155</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div> <ref type="bibr" target="#b1">2</ref> <p>Challenge site: https://ultra-low-dose-pet.grand-challenge.org/. The investigators of the challenge contributed to the design and implementation of DATA, but did not participate in analysis or writing of this paper. A complete listing of investigators can be found at:https://ultra-low-dose-pet.grand-challenge.org/Description/.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 4.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d conditional generative adversarial networks for high-quality pet image estimation at low dose</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="550" to="562" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep auto-context convolutional neural networks for standarddose pet image estimation from low-dose pet/MRI</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="406" to="416" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised learning with cyclegan for low-dose FDG pet image denoising</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schaefferkoetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101770</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d segmentation guided style-based generative adversarial networks for pet synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2092" to="2104" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive rectification based adversarial network with spectrum constraint for high-quality pet image synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">102335</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Low-count whole-body pet with deep learning in a multicenter and externally validated study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digit. Med</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">127</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Federated transfer learning for low-dose pet denoising: a pilot study with simulated heterogeneous data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Radiat. Plasma Med. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="284" to="295" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D transformer-GAN for high-quality PET reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-127" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="276" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Spach transformer: spatial and channel-wise transformer based on local and global self-attentions for pet image denoising</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Jang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-09">September 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">3D CVT-GAN: a 3d convolutional vision transformer-GAN for PET reconstruction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-09">September 2022</date>
			<biblScope unit="page" from="516" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-institutional collaborations for improving deep learning-based magnetic resonance image reconstruction using federated learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="2423" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05629</idno>
		<title level="m">Communicationefficient learning of deep networks from decentralized data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: the sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-01">January 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards universal object detection by domain attention</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7289" to="7298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Uni-perceiver: pre-training unified architecture for generic perception for zero-shot and few-shot tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01522</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Uni-perceiver-MOE: learning sparse generalist models with conditional MOEs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR abs/2202.03052</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gradient surgery for multi-task learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06782</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Dynamic neural networks: a survey</title>
		<imprint>
			<date type="published" when="2021-02">February 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: residual learning of deep CNN for image denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A cross-scanner and cross-tracer deep learning method for the recovery of standard-dose imaging quality from low-dose pet</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Nucl. Med. Mol. Imaging</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1619" to="7089" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two deterministic half-quadratic regularization algorithms for computed imaging</title>
		<author>
			<persName><forename type="first">P</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blanc-Feraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st International Conference on Image Processing</title>
		<meeting>1st International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Accelerated image reconstruction using ordered subsets of projection data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Larkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="601" to="609" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
