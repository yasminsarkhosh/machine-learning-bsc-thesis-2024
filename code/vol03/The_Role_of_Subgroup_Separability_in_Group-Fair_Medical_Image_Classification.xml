<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Role of Subgroup Separability in Group-Fair Medical Image Classification</title>
				<funder>
					<orgName type="full">Imperial College London President</orgName>
				</funder>
				<funder>
					<orgName type="full">Royal Academy of Engineering</orgName>
				</funder>
				<funder>
					<orgName type="full">Microsoft Research</orgName>
				</funder>
				<funder ref="#_7pjZRSu">
					<orgName type="full">EPSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Charles</forename><surname>Jones</surname></persName>
							<email>charles.jones17@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mélanie</forename><surname>Roschewitz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Glocker</surname></persName>
							<email>b.glocker@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Role of Subgroup Separability in Group-Fair Medical Image Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="179" to="188"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B3CB93F3843199D691DB7F9C89931590</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_18</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate performance disparities in deep classifiers. We find that the ability of classifiers to separate individuals into subgroups varies substantially across medical imaging modalities and protected characteristics; crucially, we show that this property is predictive of algorithmic bias. Through theoretical analysis and extensive empirical evaluation (Code is available at https://github.com/biomedia-mira/subgroupseparability), we find a relationship between subgroup separability, subgroup disparities, and performance degradation when models are trained on data with systematic bias such as underdiagnosis. Our findings shed new light on the question of how models become biased, providing important insights for the development of fair medical imaging AI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image computing has seen great progress with the development of deep image classifiers, which can be trained to perform diagnostic tasks to the level of skilled professionals <ref type="bibr" target="#b18">[19]</ref>. Recently, it was shown that these models might rely on sensitive information when making their predictions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and that they exhibit performance disparities across protected population subgroups <ref type="bibr" target="#b19">[20]</ref>. Although many methods exist for mitigating bias in image classifiers, they often fail unexpectedly and may even be harmful in some situations <ref type="bibr" target="#b25">[26]</ref>. Today, no bias mitigation methods consistently outperform the baseline approach of empirical risk minimisation (ERM) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>, and none are suitable for real-world deployment. If we wish to deploy appropriate and fair automated systems, we must first understand the underlying mechanisms causing ERM models to become biased.</p><p>An often overlooked aspect of this problem is subgroup separability: the ease with which individuals can be identified as subgroup members. Some medical images encode sensitive information that models may leverage to classify individuals into subgroups <ref type="bibr" target="#b6">[7]</ref>. However, this property is unlikely to hold for all modalities and protected characteristics. A more realistic premise is that subgroup separability varies across characteristics and modalities. We may expect groups with intrinsic physiological differences to be highly separable for deep image classifiers (e.g. biological sex from chest X-ray can be predicted with &gt; 0.98 AUC). In contrast, groups with more subtle differences (e.g. due to 'social constructs') may be harder for a model to classify. This is especially relevant in medical imaging, where attributes such as age, biological sex, self-reported race, socioeconomic status, and geographic location are often considered sensitive for various clinical, ethical, and societal reasons.</p><p>We highlight how the separability of protected groups interacts in non-trivial ways with the training of deep neural networks. We show that the ability of models to detect which group an individual belongs to varies across modalities and groups in medical imaging and that this property has profound consequences for the performance and fairness of deep classifiers. To the best of our knowledge, ours is the first work which analyses group-fair image classification through the lens of subgroup separability. Our contributions are threefold:</p><p>-We demonstrate empirically that subgroup separability varies across realworld modalities and protected characteristics. -We show theoretically that such differences in subgroup separability affect model bias in learned classifiers and that group fairness metrics may be inappropriate for datasets with low subgroup separability. -We corroborate our analysis with extensive testing on real-world medical datasets, finding that performance degradation and subgroup disparities are functions of subgroup separability when data is biased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Group-fair image analysis seeks to mitigate performance disparities caused by models exploiting sensitive information. In medical imaging, Seyyed-Kalantari et al. <ref type="bibr" target="#b19">[20]</ref> highlighted that classification models trained through ERM underdiagnose historically underserved population subgroups. Follow-up work has additionally shown that these models may use sensitive information to bias their predictions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Unfortunately, standard bias mitigation methods from computer vision, such as adversarial training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> and domain-independent training <ref type="bibr" target="#b23">[24]</ref>, are unlikely to be suitable solutions. Indeed, recent benchmarking on the MEDFAIR suite <ref type="bibr" target="#b26">[27]</ref> found that no method consistently outperforms ERM. On natural images, Zietlow et al. <ref type="bibr" target="#b25">[26]</ref> showed that bias mitigation methods worsen performance for all groups compared to ERM, giving a stark warning that blindly applying methods and metrics leads to a dangerous 'levelling down' effect <ref type="bibr" target="#b15">[16]</ref>.</p><p>One step towards overcoming these challenges and developing fair and performant methods is understanding the circumstances under which deep classifiers learn to exploit sensitive information inappropriately. Today, our understanding of this topic is limited. Closely related to our work is Oakden-Rayner et al., who consider how 'hidden stratification' may affect learned classifiers <ref type="bibr" target="#b17">[18]</ref>; similarly, Jabbour et al. use preprocessing filters to inject spurious correlations into chest X-ray data, finding that ERM-trained models are more biased when the correlations are easier to learn <ref type="bibr" target="#b11">[12]</ref>. Outside of fairness, our work may have broader impact in the fields of distribution shift and shortcut learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>, where many examples exist of models learning to exploit inappropriate spurious correlations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17]</ref>, yet tools for detecting and mitigating the problem remain immature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Role of Subgroup Separability</head><p>Consider a binary disease classification problem where, for each image x ∈ X, we wish to predict a class label y ∈ Y : {y + , y -}. We denote P : [Y |X] → [0, 1] the underlying mapping between images and class labels. Suppose we have access to a (biased) training dataset, where P tr is the conditional distribution between training images and training labels; we say that such a dataset is biased if P tr = P . We focus on group fairness, where each individual belongs to a subgroup a ∈ A and aim to learn a fair model that maximises performance for all groups when deployed on an unbiased test dataset drawn from P . We assume that the groups are consistent across both datasets. The bias we consider in this work is underdiagnosis, a form of label noise <ref type="bibr" target="#b3">[4]</ref> where some truly positive individuals x + are mislabeled as negative. We are particularly concerned with cases where underdiagnosis manifests in specific subgroups due to historic disparities in healthcare provision or discriminatory diagnosis policy. Formally, group A = a * is said to be underdiagnosed if it satisfies Eq. ( <ref type="formula">1</ref>):</p><formula xml:id="formula_0">P tr (y|x + , a * ) ≤ P (y|x + , a * ) and ∀a = a * , P tr (y|x + , a) = P (y|x + , a) (1)</formula><p>We may now use the law of total probability to express the overall mapping from image to label in terms of the subgroup-wise mappings in Eq. ( <ref type="formula">2</ref>). Together with Eq. ( <ref type="formula">1</ref>), this implies Eq. ( <ref type="formula" target="#formula_1">3</ref>) -the probability of a truly positive individual being assigned a positive label is lower in the biased training dataset than for the unbiased test set.</p><p>P tr (y|x) = a∈A P tr (y|x, a)P tr (a|x)</p><p>(2)</p><formula xml:id="formula_1">P tr (y|x + ) ≤ P (y|x + )<label>(3)</label></formula><p>At training time, supervised learning with empirical risk minimisation aims to obtain a model p, mapping images to predicted labels ŷ = argmax y∈Y p(y|x) such that p(y|x) ≈ P tr (y|x), ∀(x, y). Since this model approximates the biased training distribution, we may expect underdiagnosis from the training data to be reflected by the learned model when evaluated on the unbiased test set. However, the distribution of errors from the learned model depends on subgroup separability. Revisiting Eq. ( <ref type="formula">2</ref>), notice that the prediction for any individual is a linear combination of the mappings for each subgroup, weighted by the probability the individual belongs to each group. When subgroup separability is high due to the presence of sensitive information, the model will learn a different mapping for each subgroup, shown in Eq. ( <ref type="formula" target="#formula_2">4</ref>) and Eq. ( <ref type="formula" target="#formula_3">5</ref>). This model underdiagnoses group A = a * whilst recovering the unbiased mapping for other groups. p(y|x + , a * ) ≈ P tr (y|x + , a * ) ≤ P (y|x + , a * ) (</p><p>and ∀a = a * , p(y|x + , a) ≈ P tr (y|x + , a) = P (y|x + , a) (</p><p>Equation ( <ref type="formula" target="#formula_2">4</ref>) and Eq. <ref type="bibr" target="#b4">(5)</ref> show that, at test-time, our model will demonstrate worse performance for the underdiagnosed subgroup than the other subgroups. Indeed, consider True Positive Rate (TPR) as a performance metric. The groupwise TPR of an unbiased model, TPR (u)  a , is expressed in Eq. <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_4">TPR (u) a = |p(y|x + , a) &gt; 0.5| N +,a ≈ |P (y|x + , a) &gt; 0.5| N +,a<label>(6)</label></formula><p>Here, N +,a denotes the number of positive samples belonging to group a in the test set. Remember, in practice, we must train our model on the biased training distribution P tr . We thus derive test-time TPR for such a model, TPR (b)  a , from Eq. ( <ref type="formula" target="#formula_2">4</ref>) and Eq. ( <ref type="formula" target="#formula_3">5</ref>), giving Eq. ( <ref type="formula">7</ref>) and Eq. <ref type="bibr" target="#b7">(8)</ref>.</p><formula xml:id="formula_5">TPR (b) a * ≈ |P tr (y|x + , a * ) &gt; 0.5| N +,a * ≤ |P (y|x + , a * ) &gt; 0.5| N +,a * ≈ TPR (u) a * (7)</formula><p>and</p><formula xml:id="formula_6">∀a = a * , TPR (b) a ≈ |P tr (y|x + , a) &gt; 0.5| N +,a ≈ TPR (u) a (<label>8</label></formula><formula xml:id="formula_7">)</formula><p>In the case of high subgroup separability, Eq. ( <ref type="formula">7</ref>) and Eq. ( <ref type="formula" target="#formula_6">8</ref>) demonstrate that TPR of the underdiagnosed group is directly affected by bias from the training set while other groups are mainly unaffected. Given this difference across groups, an appropriately selected group fairness metric may be able to identify the bias, in some cases even without access to an unbiased test set <ref type="bibr" target="#b22">[23]</ref>. On the other hand, when subgroup separability is low, this property does not hold. With nonseparable groups (i.e. P (a|x) ≈ 1  |A| , ∀a ∈ A), a trained model will be unable to learn separate subgroup mappings, shown in Eq. <ref type="bibr" target="#b8">(9)</ref>. p(y|x + , a) ≈ P tr (y|x + ), ∀a ∈ A (9)</p><p>Equations ( <ref type="formula" target="#formula_1">3</ref>) and ( <ref type="formula">9</ref>) imply that the performance of the trained model degrades for all groups. Returning to the example of TPR, Eq. ( <ref type="formula">10</ref>) represents performance degradation for all groups when separability is poor. In such situations, we expect performance degradation to be uniform across groups and thus not be detected by group fairness metrics. The severity of the degradation depends on both the proportion of corrupted labels in the underdiagnosed subgroup and the size of the underdiagnosed subgroup in the dataset.</p><formula xml:id="formula_8">TPR (b) a ≈ |P tr (y|x + , a) &gt; 0.5| N +,a ≤ |P (y|x + , a) &gt; 0.5| N +,a ≈ TPR (u) a , ∀a ∈ A (10)</formula><p>We have derived the effect of underdiagnosis bias on classifier performance for the two extreme cases of high and low subgroup separability. In practice, subgroup separability for real-world datasets may vary continuously between these extremes. In Sect. 4, we empirically investigate (i) how subgroup separability varies in the wild, (ii) how separability impacts performance for each group when underdiagnosis bias is added to the datasets, (iii) how models encode sensitive information in their representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We support our analysis with experiments on five datasets adapted from a subset of the MEDFAIR benchmark <ref type="bibr" target="#b26">[27]</ref>. We treat each dataset as a binary classification task (no-disease vs disease) with a binary subgroup label. For datasets with multiple sensitive attributes available, we investigate each individually, giving eleven dataset-attribute combinations. The datasets cover the modalities of skin dermatology <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21]</ref>, fundus images <ref type="bibr" target="#b14">[15]</ref>, and chest X-ray <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>. We record summary statistics for the datasets used in the supplementary material (Table <ref type="table" target="#tab_0">A1</ref>), where we also provide access links (Table <ref type="table">A2</ref>). Our architecture and hyperparameters are listed in Table <ref type="table">A3</ref>, adapted from the experiments in MEDFAIR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subgroup Separability in the Real World</head><p>We begin by testing the premise of this article: subgroup separability varies across medical imaging settings. To measure subgroup separability, we train binary subgroup classifiers for each dataset-attribute combination. We use testset area under receiver operating characteristic curve (AUC) as a proxy for separability, reporting results over ten random seeds in Table <ref type="table" target="#tab_0">1</ref>. Some patterns are immediately noticeable from Table <ref type="table" target="#tab_0">1</ref>. All attributes can be predicted from chest X-ray scans with &gt; 0.9 AUC, implying that the modality encodes substantial information about patient identity. Age is consistently well predicted across all modalities, whereas separability of biological sex varies, with prediction of sex from fundus images being especially weak. Importantly, the wide range of AUC results [0.642 → 0.986] across the dataset-attribute combinations confirms our premise that subgroup separability varies substantially across medical imaging applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Degradation Under Label Bias</head><p>We now test our theoretical finding: models are affected by underdiagnosis differently depending on subgroup separability. We inject underdiagnosis bias into each training dataset by randomly mislabelling 25% of positive individuals in Group 1 (see Table <ref type="table" target="#tab_0">1</ref>) as negative. For each dataset-attribute combination, we train ten disease classification models with the biased training data and ten models with the original clean labels; we test all models on clean data. We assess how the test-time performance of the models trained on biased data degrades relative to models trained on clean data. We illustrate the mean percentage point accuracy degradation for each group in Fig. <ref type="figure" target="#fig_0">1</ref> and use the Mann-Whitney U test (with the Holm-Bonferroni adjustment for multiple hypothesis testing) to determine if the performance degradation is statistically significant at p critical = 0.05. We include an ablation experiment over varying label noise intensity in Fig. <ref type="figure" target="#fig_0">A1</ref>. Our results in Fig. <ref type="figure" target="#fig_0">1</ref> are consistent with our analysis in Sect. 3. We report no statistically significant performance degradation for dataset-attribute combinations with low subgroup separability (&lt;0.9 AUC). In these experiments, the proportion of mislabelled images is small relative to the total population; thus, the underdiagnosed subgroups mostly recover from label bias by sharing the correct mapping with the uncorrupted group. While we see surprising improvements in performance for PAPILA, note that this is the smallest dataset, and these improvements are not significant at p critical = 0.05. As subgroup separability increases, performance degrades more for the underdiagnosed group (Group 1), whilst performance for the uncorrupted group (Group 0) remains somewhat unharmed. We see a statistically significant performance drop for Group 0 in the MIMIC-Sex experiment -we believe this is because the model learns separate group-wise mappings, shrinking the effective size of the dataset for Group 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Use of Sensitive Information in Biased Models</head><p>Finally, we investigate how biased models use sensitive information. We apply the post hoc Supervised Prediction Layer Information Test (SPLIT) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> to all models trained for the previous experiment, involving freezing the trained backbone and re-training the final layer to predict the sensitive attribute. We report test-set SPLIT AUC in Fig. <ref type="figure" target="#fig_1">2</ref>, plotting it against subgroup separability AUC from Table <ref type="table" target="#tab_0">1</ref> and using Kendall's τ statistic to test for a monotonic association between the results (p critical = 0.05). We find that models trained on biased data learn to encode sensitive information in their representations and see a statistically significant association between the amount of information available and the amount encoded in the representations. Models trained on unbiased data have no significant association, so do not appear to exploit sensitive information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We investigated how subgroup separability affects the performance of deep neural networks for disease classification. We discuss four takeaways from our study: Subgroup Separability Varies Substantially in Medical Imaging. In fairness literature, data is often assumed to contain sufficient information to identify individuals as subgroup members. But what if this information is only partially encoded in the data? By testing eleven dataset-attribute combinations across three medical modalities, we found that the ability of classifiers to predict sensitive attributes varies substantially. Our results are not exhaustive -there are many modalities and sensitive attributes we did not consider -however, by demonstrating a wide range of separability results across different attributes and modalities, we highlight a rarely considered property of medical image datasets.</p><p>Performance Degradation is a Function of Subgroup Separability. We showed, theoretically and empirically, that the performance and fairness of models trained on biased data depends on subgroup separability. When separability is high, models learn to exploit the sensitive information and the bias is reflected by stark subgroup differences. When separability is low, models cannot exploit sensitive information, so they perform similarly for all groups. This indicates that group fairness metrics may be insufficient for detecting bias when separability is low. Our analysis centred on bias in classifiers trained with the standard approach of empirical risk minimisation -future work may wish to investigate whether subgroup separability is a factor in the failure of bias mitigation methods and whether it remains relevant in further image analysis tasks (e.g. segmentation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sources of Bias Matter.</head><p>In our experiments, we injected underdiagnosis bias into the training set and treated the uncorrupted test set as an unbiased ground truth. However, this is not an endorsement of the quality of the data. At least some of the datasets may already contain an unknown amount of underdiagnosis bias (among other sources of bias) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>. This pre-existing bias will likely have a smaller effect size than our artificial bias, so it should not play a significant role in our results. Still, the unmeasured bias may explain some variation in results across datasets. Future work should investigate how subgroup separability interacts with other sources of bias. We renew the call for future datasets to be released with patient metadata and multiple annotations to enable analysis of different sources and causes of bias.</p><p>Reproducibility and Impact. This work tackles social and technical problems in machine learning for medical imaging and is of interest to researchers and practitioners seeking to develop and deploy medical AI. Given the sensitive nature of this topic, and its potential impact, we have made considerable efforts to ensure full reproducibility of our results. All datasets used in this study are publicly available, with access links in Table <ref type="table">A2</ref>. We provide a complete implementation of our preprocessing, experimentation, and analysis of results at https://github. com/biomedia-mira/subgroup-separability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Percentage-point degradation in accuracy for disease classifiers trained on biased data, compared to training on clean data. Lower values indicate worse performance for the biased model when tested on a clean dataset. Results are reported over ten random seeds, and bars marked with * represent statistically significant results. Dataset-attribute combinations are sorted by ascending subgroup separability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. AUC of the SPLIT test for sensitive information encoded in learned representations, plotted against subgroup separability. Along the maximum sensitive information line, models trained for predicting the disease encode as much sensitive information in their representations as the images do themselves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Separability of protected subgroups in real-world datasets, measured by testset AUC of classifiers trained to predict the groups. Mean and standard deviation are reported over ten random seeds, with results sorted by ascending mean AUC.</figDesc><table><row><cell>Dataset-Attribute</cell><cell>Modality</cell><cell cols="2">Subgroups</cell><cell>AUC</cell></row><row><cell></cell><cell></cell><cell cols="2">Group 0 Group 1</cell><cell>μ</cell><cell>σ</cell></row><row><cell>PAPILA-Sex</cell><cell>Fundus Image</cell><cell>Male</cell><cell>Female</cell><cell cols="2">0.642 0.057</cell></row><row><cell>HAM10000-Sex</cell><cell cols="2">Skin Dermatology Male</cell><cell>Female</cell><cell cols="2">0.723 0.015</cell></row><row><cell>HAM10000-Age</cell><cell cols="2">Skin Dermatology &lt;60</cell><cell>≥60</cell><cell cols="2">0.803 0.020</cell></row><row><cell>PAPILA-Age</cell><cell>Fundus Image</cell><cell>&lt;60</cell><cell>≥60</cell><cell cols="2">0.812 0.046</cell></row><row><cell cols="3">Fitzpatrick17k-Skin Skin Dermatology I-III</cell><cell>IV-VI</cell><cell cols="2">0.891 0.010</cell></row><row><cell>CheXpert-Age</cell><cell>Chest X-ray</cell><cell>&lt;60</cell><cell>≥60</cell><cell cols="2">0.920 0.003</cell></row><row><cell>MIMIC-Age</cell><cell>Chest X-ray</cell><cell>&lt;60</cell><cell>≥60</cell><cell cols="2">0.930 0.002</cell></row><row><cell>CheXpert-Race</cell><cell>Chest X-ray</cell><cell>White</cell><cell cols="3">Non-White 0.936 0.005</cell></row><row><cell>MIMIC-Race</cell><cell>Chest X-ray</cell><cell>White</cell><cell cols="3">Non-White 0.951 0.004</cell></row><row><cell>CheXpert-Sex</cell><cell>Chest X-ray</cell><cell>Male</cell><cell>Female</cell><cell cols="2">0.980 0.020</cell></row><row><cell>MIMIC-Sex</cell><cell>Chest X-ray</cell><cell>Male</cell><cell>Female</cell><cell cols="2">0.986 0.008</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. C.J. is supported by <rs type="funder">Microsoft Research</rs> and <rs type="funder">EPSRC</rs> through the <rs type="programName">Microsoft PhD Scholarship Programme</rs>. M.R. is funded through an <rs type="funder">Imperial College London President</rs>'s PhD Scholarship. B.G. received support from the <rs type="funder">Royal Academy of Engineering</rs> as part of his Kheiron/RAEng Research Chair.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7pjZRSu">
					<orgName type="program" subtype="full">Microsoft PhD Scholarship Programme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 18.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Turning a blind eye: explicit removal of biases and variation from deep neural network embeddings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nellåker</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11009-3_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-11009-334" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11129</biblScope>
			<biblScope unit="page" from="556" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bernhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-022-01846-8</idno>
		<ptr target="https://doi.org/10.1038/s41591-022-01846-8" />
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1157" to="1158" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freyberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<title level="m">Detecting and Preventing Shortcut Learning for Fair Medical AI using Shortcut Testing (ShorT)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Causality matters in medical imaging</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-020-17478-w</idno>
		<ptr target="https://doi.org/10.1038/s41467-020-17478-w" />
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">AI for radiographic COVID-19 detection selects shortcuts over signal</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Janizek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-021-00338-7</idno>
		<idno>42256-021-00338-7</idno>
		<ptr target="https://doi.org/10.1038/s" />
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="610" to="619" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-020-00257-z</idno>
		<ptr target="https://doi.org/10.1038/s42256-020-00257-z" />
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AI recognition of patient race in medical imaging: a modelling study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Gichoya</surname></persName>
		</author>
		<idno type="DOI">10.1016/S2589-7500(22)00063-2</idno>
		<ptr target="https://doi.org/10.1016/S2589-7500" />
	</analytic>
	<monogr>
		<title level="j">Lancet Digit. Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="63" to="65" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Algorithmic encoding of protected characteristics in chest X-ray disease detection models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winzeck</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ebiom.2023.104467</idno>
		<ptr target="https://doi.org/10.1016/j.ebiom.2023.104467" />
	</analytic>
	<monogr>
		<title level="j">eBioMedicine</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards transparency in dermatology image datasets with skin tone annotations by experts, crowds, and an algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Daneshjou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Badri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koochek</surname></persName>
		</author>
		<idno type="DOI">10.1145/3555634</idno>
		<ptr target="https://doi.org/10.1145/3555634" />
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Hum.-Comput. Interact</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating deep neural networks trained on clinical images in dermatology with the fitzpatrick 17k dataset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Groh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1820" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CheXpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.3301590</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.3301590" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning applied to chest x-rays: exploiting and preventing shortcuts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jabbour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kazerooni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Sjoding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiens</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Machine Learning for Healthcare Conference</title>
		<meeting>the Machine Learning for Healthcare Conference</meeting>
		<imprint>
			<date type="published" when="2020-09">Sep 2020</date>
			<biblScope unit="page" from="750" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-019-0322-0</idno>
		<idno>41597-019-0322-0</idno>
		<ptr target="https://doi.org/10.1038/s" />
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">317</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning not to learn: training deep neural networks with biased data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9012" to="9020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PAPILA: dataset with fundus images and clinical data of both eyes of the same patient for glaucoma assessment</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kovalyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morales-Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verdú-Monedero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sellés-Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Palazón-Cabanes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Sancho-Gómez</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-022-01388-1</idno>
		<ptr target="https://doi.org/10.1038/s41597-022-01388-1" />
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">291</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The unfairness of fair machine learning: levelling down and strict egalitarianism by default</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-01">January 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uncovering and correcting shortcut learning in machine learning models for skin cancer diagnosis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nauta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
		<idno type="DOI">10.3390/diagnostics12010040</idno>
		<ptr target="https://doi.org/10.3390/diagnostics12010040" />
	</analytic>
	<monogr>
		<title level="j">Diagnostics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hidden stratification causes clinically meaningful failures in machine learning for medical imaging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Oakden-Rayner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="DOI">10.1145/3368555.3384468</idno>
		<ptr target="https://doi.org/10.1145/3368555.3384468" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Health, Inference, and Learning</title>
		<meeting>the ACM Conference on Health, Inference, and Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">CheXNet: radiologist-level pneumonia detection on chest xrays with deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-11">November 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seyyed-Kalantari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-021-01595-0</idno>
		<ptr target="https://doi.org/10.1038/s41591-021-01595-0" />
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2176" to="2182" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2018.161</idno>
		<ptr target="https://doi.org/10.1038/sdata.2018.161" />
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">180161</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An overview of statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.788640</idno>
		<ptr target="https://doi.org/10.1109/72.788640" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="988" to="999" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bias preservation in machine learning: the legality of fairness metrics under EU non-discrimination law. West Virginia Law Rev</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards fairness in visual recognition: effective strategies for bias mitigation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A fine-grained analysis on distribution shift</title>
		<author>
			<persName><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022-01">January 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Leveling down in computer vision: pareto inefficiencies in fair deep classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zietlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10410" to="10421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MEDFAIR: benchmarking fairness for medical imaging</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023-02">February 2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
