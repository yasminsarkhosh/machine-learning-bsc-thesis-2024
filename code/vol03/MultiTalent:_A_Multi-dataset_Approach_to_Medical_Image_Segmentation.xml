<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MultiTalent: A Multi-dataset Approach to Medical Image Segmentation</title>
				<funder>
					<orgName type="full">Helmholtz Imaging</orgName>
					<orgName type="abbreviated">HI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Constantin</forename><surname>Ulrich</surname></persName>
							<email>constantin.ulrich@dkfz-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Medical Image Computing</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">National Center for Tumor Diseases (NCT)</orgName>
								<orgName type="institution" key="instit1">NCT Heidelberg</orgName>
								<orgName type="institution" key="instit2">DKFZ and University Medical Center Heidelberg</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Medical Faculty Heidelberg</orgName>
								<orgName type="institution">University of Heidelberg</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabian</forename><surname>Isensee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Medical Image Computing</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Helmholtz Imaging</orgName>
								<orgName type="department" key="dep2">DKFZ</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tassilo</forename><surname>Wald</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Medical Image Computing</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Helmholtz Imaging</orgName>
								<orgName type="department" key="dep2">DKFZ</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maximilian</forename><surname>Zenk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Medical Image Computing</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Medical Faculty Heidelberg</orgName>
								<orgName type="institution">University of Heidelberg</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Baumgartner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Medical Image Computing</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Helmholtz Imaging</orgName>
								<orgName type="department" key="dep2">DKFZ</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Klaus</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Medical Image Computing</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiation Oncology</orgName>
								<orgName type="laboratory">Pattern Analysis and Learning Group</orgName>
								<orgName type="institution">Heidelberg University Hospital</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MultiTalent: A Multi-dataset Approach to Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="648" to="658"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">698B95F589D04A11AA476BC2EAD7FEEA</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_62</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical image segmentation</term>
					<term>multitask learning</term>
					<term>transfer learning</term>
					<term>foundation model</term>
					<term>partially labeled datasets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The medical imaging community generates a wealth of datasets, many of which are openly accessible and annotated for specific diseases and tasks such as multi-organ or lesion segmentation. Current practices continue to limit model training and supervised pre-training to one or a few similar datasets, neglecting the synergistic potential of other available annotated data. We propose MultiTalent, a method that leverages multiple CT datasets with diverse and conflicting class definitions to train a single model for a comprehensive structure segmentation. Our results demonstrate improved segmentation performance compared to previous related approaches, systematically, also compared to single-dataset training using state-of-the-art methods, especially for lesion segmentation and other challenging structures. We show that Mul-tiTalent also represents a powerful foundation model that offers a superior pre-training for various segmentation tasks compared to commonly used supervised or unsupervised pre-training baselines. Our findings offer a new direction for the medical imaging community to effectively utilize the wealth of available data for improved segmentation performance. The code and model weights will be published here: https://github.com/ MIC-DKFZ/MultiTalent.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of deep neural networks heavily relies on the availability of large and diverse annotated datasets across a range of computer vision tasks. To learn a strong data representation for robust and performant medical image segmentation, huge datasets with either many thousands of annotated data structures or less specific self-supervised pretraining objectives with unlabeled data are needed <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b33">33]</ref>. The annotation of 3D medical images is a difficult and laborious task. Thus, depending on the task, only a bare minimum of images and target structures is usually annotated. This results in a situation where a zoo of partially labeled datasets is available to the community. Recent efforts have resulted in a large dataset of &gt;1000 CT images with &gt;100 annotated classes each, thus providing more than 100,000 manual annotations which can be used for pre-training <ref type="bibr" target="#b30">[30]</ref>. Focusing on such a dataset prevents leveraging the potentially precious additional information of the above mentioned other datasets that are only partially annotated. Integrating information across different datasets potentially yields a higher variety in image acquisition protocols, more anatomical target structures or details about them as well as information on different kinds of pathologies. Consequently, recent advances in the field allowed utilizing partially labeled datasets to train one integrated model <ref type="bibr" target="#b21">[21]</ref>. Early approaches handled annotations that are present in one dataset but missing in another by considering them as background <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">27]</ref> and penalizing overlapping predictions by taking advantage of the fact that organs are mutually exclusive <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">28]</ref>. Some other methods only predicted one structure of interest for each forward pass by incorporating the class information at different stages of the network <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b31">31]</ref>. Chen et al. trained one network with a shared encoder and separate decoders for each dataset to generate a generalized encoder for transfer learning <ref type="bibr" target="#b1">[2]</ref>. However, most approaches are primarily geared towards multi-organ segmentation as they do not support overlapping target structures, like vessels or cancer classes within an organ <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b23">23]</ref>. So far, all previous methods do not convincingly leverage cross-dataset synergies. As Liu et al. pointed out, one common caveat is that many methods force the resulting model to average between distinct annotation protocol characteristics <ref type="bibr" target="#b22">[22]</ref> by combining labels from different datasets for the same target structure (visualized in Fig. <ref type="figure" target="#fig_0">1 b</ref>)). Hence, they all fail to reach segmentation performance on par with cutting-edge single dataset segmentation methods. To this end, we introduce MultiTalent (MULTI daTAset LEarNing and pre-Training), a new, flexible, multi-dataset training method: 1) MultiTalent can handle classes that are absent in one dataset but annotated in another during training. 2) It retains different annotation protocol characteristics for the same target structure and 3) allows for overlapping target structures with different level of detail such as liver, liver vessel and liver tumor. Overall, Mul-tiTalent can include all kinds of new datasets irrespective of their annotated target structures.</p><p>MultiTalent can be used in two scenarios: First, in a combined multi-dataset (MD) training to generate one foundation segmentation model that is able to predict all classes that are present in any of the utilized partially annotated datasets, and second, for pre-training to leverage the learned representation of this foundation model for a new task. In experiments with a large collection of abdominal CT datasets, the proposed model outperformed state-of-the-art segmentation networks that were trained on each dataset individually as well as all previous methods that incorporated multiple datasets for training. Interestingly, the benefits of MultiTalent are particularly notable for more difficult classes and pathologies. In comparison to an ensemble of single dataset solutions, MultiTalent comes with shorter training and inference times. Additionally, at the example of three challenging datasets, we demonstrate that fine-tuning MultiTalent yields higher segmentation performance than training from scratch or initializing the model parameters using unsupervised pretraining strategies <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b33">33]</ref>. It also surpasses supervised pretrained and fine-tuned state-of-the art models on most tasks, despite requiring orders of magnitude less annotations during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We introduce MultiTalent, a multi dataset learning and pre-training method, to train a foundation medical image segmentation model. It comes with a novel dataset and class adaptive loss function. The proposed network architecture enables the preservation of all label properties, learning overlapping classes and the simultaneous prediction of all classes. Furthermore, we introduce a training schedule and dataset preprocessing which balances varying dataset size and class characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>We begin with a dataset collection of K datasets</p><formula xml:id="formula_0">D (k) , k ∈ [1, K], with N (k) image and label pairs D (k) = {(x, y) (k) 1 , ..., (x, y) (k) N (k) }. In these datasets, every image voxel x (k) i , i ∈ [1, I], is assigned to one class c ∈ C (k)</formula><p>, where C (k) ⊆ C is the label set associated to dataset D (k) . Even if classes from different datasets refer to the same target structure we consider them as unique, since the exact annotation protocols and labeling characteristics of the annotations are unknown and can vary between datasets: C (k) ∩ C (j) = ∅, ∀k = j. This implies that the network must be capable of predicting multiple classes for one voxel to account for the inconsistent class definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MultiTalent</head><p>Network Modifications. We employ three different network architectures, which are further described below, to demonstrate that our approach is applicable to any network topology. To solve the label contradiction problem we decouple the segmentation outputs for each class by applying a Sigmoid activation function instead of the commonly used Softmax activation function across the dataset. The network shares the same backbone parameters Θ but it has independent segmentation head parameters Θ c for each class. The Sigmoid probabilities for each class are defined as ŷc = f (x, Θ, Θ c ). This modification allows the network to assign multiple classes to one pixel and thus enables overlapping classes and the conservation of all label properties from each dataset. Consequently, the segmentation of each class can be thought of as a binary segmentation task. </p><formula xml:id="formula_1">L c = 1 I b,i BCE(ŷ (k) i,b,c , y (k) i,b,c ) - 2 b,i ŷ(k) i,b,c y (k) i,b,c b,i ŷ(k) i,b,c + b,i y (k) i,b,c<label>(1)</label></formula><p>While the regular dice loss is calculated for each image within a batch, we calculate the dice loss jointly for all images of the input batch. This regularizes the loss if only a few voxels of one class are present in one image and a larger area is present in another image of the same batch. Thus, an inaccurate prediction of a few pixels in the first image has a limited effect on the loss. In the following, we unite the sum over the image voxels i and the batch b to z . We modify the loss function to be calculated only for classes that were annotated in the corresponding partially labeled dataset <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">27]</ref>, in the following indicated by 1</p><formula xml:id="formula_2">(k) c , where 1 (k) c = 1 if c ∈ C (k)</formula><p>and 0 otherwise. Instead of averaging, we add up the loss over the classes. Hence, the loss signal for each class prediction does not depend on the number of other classes within the batch. This compensates for the varying number of annotated classes in each dataset. Otherwise, the magnitude of the loss e.g. for the liver head from D1 (2 classes) would be much larger as for D7 <ref type="bibr">(13 classes)</ref>. Gradient clipping captures any potential instability that might arise from a higher loss magnitude:</p><formula xml:id="formula_3">L = c 1 (k) c 1 I z BCE(ŷ (k) z,c , y (k) z,c ) - 2 z 1 (k) c ŷ(k) z,c y (k) z,c z 1 (k) c ŷ(k) z,c + z 1 (k) c y (k) z,c<label>(2)</label></formula><p>Network Architectures. To demonstrate the general applicability of this approach, we applied it to three segmentation networks. We employed a 3D U-Net <ref type="bibr" target="#b24">[24]</ref>, an extension with additional residual blocks in the encoder (Resenc U-Net), that demonstrated highly competitive results in previous medical image segmentation challenges <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref> and a recently proposed transformer based architecture (SwinUNETR <ref type="bibr" target="#b29">[29]</ref>). We implemented our approach in the nnU-Net framework <ref type="bibr" target="#b13">[13]</ref>. However, the automatic pipeline configuration from nnU-net was not used in favor of a manually defined configuration that aims to reflect the peculiarities of each of the datasets, irrespective of the number of training cases they contain. We manually selected a patch size of <ref type="bibr">[96,</ref><ref type="bibr">192,</ref><ref type="bibr">192]</ref> and image spacing of 1mm in plane and 1.5mm for the axial slice thickness, which nnU-Net used to automatically create the two CNN network topologies. For the SwinUNETR, we adopted the default network topology.</p><p>Multi-dataset Training Setup. We trained MultiTalent with 13 public abdominal CT datasets with a total of 1477 3D images, including 47 classes (Multi-dataset (MD) collection) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref>. Detailed information about the datasets, can be found in the appendix in Table <ref type="table">3</ref> and Fig. <ref type="figure">3</ref>, including the corresponding annotated classes. We increased the batch size to 4 and the number of training epochs to 2000 to account for the high number of training images. To compensate for the varying number of training images in each dataset, we choose a sampling probability per case that is inversely proportional to √ n, where n is the number of training cases in the corresponding source dataset. Apart from that, we have adopted all established design choices from nnU-Net to ensure reproducibility and comparability.</p><p>Transfer Learning Setup. We used the BTCV (small multi organ dataset <ref type="bibr" target="#b19">[19]</ref>), AMOS (large multi organ dataset <ref type="bibr" target="#b16">[16]</ref>) and KiTS19 (pathology dataset <ref type="bibr" target="#b11">[11]</ref>) datasets to evaluate the generalizability of the MultiTalent features in a pre-training and fine tuning setting. Naturally, the target datasets were excluded from the respective pre-training. Fine tuning was performed with identical configuration as the source training, except for the batch size which was set to 2. We followed the fine-tuning schedule proposed by Kumar et al. <ref type="bibr" target="#b17">[17]</ref>. First, the segmentation heads were warmed up over 10 epochs with linearly increasing learning rate, followed by a whole-network warm-up over 50 epochs. Finally, we continued with the standard nnU-Net training schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Baselines</head><p>As a baseline for the MultiTalent, we applied the 3D U-Net generated by the nnU-Net without manual intervention to each dataset individually. Furthermore, we trained a 3D U-Net, a Resenc U-Net and a SwinUNETR with the same network topology, patch and batch size as our MultiTalent for each dataset. All baseline networks were also implemented within the nnU-Net framework and follow the default training procedure. Additionally, we compare MultiTalent with related work on the public BTCV leaderboard in Table <ref type="table" target="#tab_0">1</ref>. Furthermore, the utility of features generated by MultiTalent is compared to supervised and unsupervised pre-training baselines. As supervised baseline, we used the weights resulting from training the three model architectures on the TotalSegmentator dataset, which consists of 1204 images and 104 classes <ref type="bibr" target="#b30">[30]</ref>, resulting in more than 10 5 annotated target structures. In contrast, Mul-tiTalent is only trained with about 3600 annotations. We used the same patch size, image spacing, batch size and number of epochs as for the MultiTalent training. As unsupervised baseline for the CNNs, we pre-trained the networks on the Multi-dataset collection based on the work of Zhou et al. (Model Genesis <ref type="bibr" target="#b33">[33]</ref>). Finally, for the SwinUNETR architecture, we compared the utility of the weights from our MultiTalent with the ones provided by Tan et al. who performed self-supervised pre-training on 5050 CT images. This necessitated the use of the original (org.) implementation of SwinUNETR because the recommended settings for fine tuning were used. This should serve as additional external validation of our model. To ensure fair comparability, we did not scale up any models. Despite using gradient checkpointing, the SwinUNETR models requires roughly 30 GB of GPU memory, compared to less than 17 GB for the CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Multi-dataset training results are presented in Fig. <ref type="figure" target="#fig_2">2</ref>. In general, the convolutional architectures clearly outperform the transformer-inspired SwinUNETR. MultiTalent improves the performance of the purely convolutional architectures (U-Net and Resenc U-Net) and outperforms the corresponding baseline models that were trained on each dataset individually. Since a simple average over all classes would introduce a biased perception due to the highly varying numbers of images and classes, we additionally report an average over all datasets. For example, dataset 7 consists of only 30 training images but has 13 classes, whereas   <ref type="table">4</ref> in the appendix provides all results for all classes. Averaged over all datasets, the MultiTalent gains 1.26 Dice points for the Resenc U-Net architecture and 1.05 Dice points for the U-Net architecture. Compared to the default nnU-Net, configured without manual intervention for each dataset, the improvements are 1.56 and 0.84 Dice points. Additionally, in Fig. <ref type="figure" target="#fig_2">2</ref> we analyzed two subgroups of classes. The first group includes all "difficult" classes for which the default nnU-Net has a Dice smaller than 75 (labeled by a "d" in Table <ref type="table">4</ref> in the appendix). The second group includes all cancer classes because of their clinical relevance. Both class groups, but especially the cancer classes, experience notable performance improvements from MultiTalent. For the official BTCV test set in Table <ref type="table" target="#tab_0">1</ref>, MultiTalent outperforms all related work that have also incorporated multiple datasets during training, proving that MultiTalent is substantially superior to related approaches. The advantages of MultiTalent include not only better segmentation results, but also considerable time savings for training and inference due to the simultaneous prediction of all classes. The training is 6.5 times faster and the inference is around 13 times faster than an ensemble of models trained on 13 datasets.</p><p>Transfer learning results are found in Table <ref type="table" target="#tab_1">2</ref>, which compares the finetuned 5-fold cross-validation results of different pre-training strategies for three different models on three datasets. The MultiTalent pre-training is highly beneficial for the convolutional models and outperforms all unsupervised baselines. Although MultiTalent was trained with a substantially lower amount of manually annotated structures ( ˜3600 vs. ˜10 5 annotations), it also exceeds the supervised pre-training baseline. Especially for the small multi-organ dataset, which only has 30 training images (BTCV), and for the kidney tumor (KiTs19), the Multi-Talent pre-training boosts the segmentation results. In general, the results show that supervised pre-training can be beneficial for the SwinUNETR as well, but pre-training on the large TotalSegmentator dataset works better than the MD pre-training. For the AMOS dataset, no pre-training scheme has a substantial impact on the performance. We suspect that it is a result of the dataset being saturated due to its large number of training cases. The Resenc U-Net pretrained with MultiTalent, sets a new state-of-the-art on the BTCV leaderboard<ref type="foot" target="#foot_0">1</ref> (Table <ref type="table" target="#tab_0">1</ref>).  allows including any publicly available datasets (e.g. AMOS and TotalSegmentator). This paves the way towards holistic whole body segmentation model that is even capable of handling pathologies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Usually only a few classes are annotated in publicly available datasets. b) Different groundtruth label properties can generate contradicting class predictions. For example, the heart annotation of dataset 11 differs from the heart annotation of dataset 10, which causes the aorta of dataset 11 to overlap with the heart of dataset 10. In contrast to dataset 11, in dataset 7 the aorta is also annotated in the lower abdomen. c) Instead of training one network for each dataset, we introduce a method to train one network with all datasets, while retaining dataset-specific annotation protocols.</figDesc><graphic coords="2,56,46,303,56,339,40,203,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Dataset and Class Adaptive Loss Function. Based on the well established combination of a Cross-entropy and Dice loss for single dataset medical image segmentation, we employ the binary Binary Cross-entropy loss (BCE) and a modified Dice loss for each class over all B, b ∈ [1, B], images in a batch:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Dice scores for all datasets, all classes, and classes of special interest. It should be noted that individual points within a boxplot corresponds to a different task. Difficult classes are those for which the default nnU-Net has a Dice below 75. The same color indicates the same architecture and the pattern implies training with multiple datasets using MultiTalent. The mean Dices are written on the Figure.</figDesc><graphic coords="7,43,80,53,66,336,52,186,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>dataset 6</head><label>6</label><figDesc>has 126 training images but only 1 class. Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Official BTCV test set leaderboard results, Dice and 95% Hausdorff Distance. * indicates usage of multiple datasets. We submitted both a 5-fold cv ensemble and a single model to improve comparability to related methods. MultiTalent takes less time for training and inference, saving resources compared to training many single dataset models. In the transfer learning setting, the feature representations learned by MultiTalent boost segmentation performance and set a new state-of-the-art on the BTCV leaderboard. The nature of MultiTalent imposes no restrictions on additional datasets, which</figDesc><table><row><cell>Method</cell><cell># models</cell><cell cols="2">Avg. Dice Avg. HD95</cell></row><row><cell>nnU-Net (cascaded &amp; fullres) [13]</cell><cell cols="2">2 models, each 88.10</cell><cell>17.26</cell></row><row><cell></cell><cell>5-fold ensemble</cell><cell></cell><cell></cell></row><row><cell>UNETR [10, 22]</cell><cell>single model</cell><cell>81.43</cell><cell>-</cell></row><row><cell>SwinUNETR [22, 29]</cell><cell>single model</cell><cell>82.06</cell><cell>-</cell></row><row><cell>Universal Model* [22]</cell><cell>single model</cell><cell>86.13</cell><cell>-</cell></row><row><cell>DoDNet (pretrained*) [31]</cell><cell>single model</cell><cell>86.44</cell><cell>15.62</cell></row><row><cell>PaNN* [32]</cell><cell>single model</cell><cell>84.97</cell><cell>18.47</cell></row><row><cell>MultiTalent Resenc U-Net*</cell><cell>single model</cell><cell>88.82</cell><cell>16.35</cell></row><row><cell>MultiTalent Resenc U-Net*</cell><cell cols="2">5-fold ensemble 88.91</cell><cell>14.68</cell></row><row><cell cols="3">Resenc U-Net (pre-trained MultiTalent*) 5-fold ensemble 89.07</cell><cell>15.01</cell></row><row><cell>4 Discussion</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">MultiTalent demonstrates the remarkable potential of utilizing multiple pub-</cell></row><row><cell cols="4">licly available partially labeled datasets to train a foundation medical segmen-</cell></row><row><cell cols="4">tation network, that is highly beneficial for pre-training and finetuning various</cell></row><row><cell cols="4">segmentation tasks. MultiTalent surpasses state-of-the-art single-dataset models</cell></row><row><cell cols="4">and outperforms related work for multi dataset training, while retaining conflict-</cell></row><row><cell cols="4">ing annotation protocol properties from each dataset and allowing overlapping</cell></row><row><cell>classes. Furthermore,</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc><ref type="bibr" target="#b4">5</ref>-fold cross validation results of different architectures and pretraining schemes. We used the original (org.) SwinUNETR implementation and the provided self-supervised weights as additional baseline<ref type="bibr" target="#b29">[29]</ref>. We applied a one sided paired t-test for each pretraining scheme compared to training from scratch.</figDesc><table><row><cell>Architecture</cell><cell>Pretraining scheme</cell><cell>BTCV</cell><cell></cell><cell>AMOS</cell><cell></cell><cell>KiTs19</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Dice avg. p</cell><cell cols="2">Dice avg. p</cell><cell cols="2">Kidney Dice p</cell><cell>Tumor Dice p</cell></row><row><cell>SwinUNETR</cell><cell>from scratch</cell><cell>74.27</cell><cell></cell><cell>86.04</cell><cell></cell><cell>87.69</cell><cell></cell><cell>46.56</cell></row><row><cell cols="2">org. implement self-supervised [29]</cell><cell>74.71</cell><cell>0.30</cell><cell>86.11</cell><cell>0.20</cell><cell>87.62</cell><cell cols="2">0.55 43.64</cell><cell>0.97</cell></row><row><cell>SwinUNETR</cell><cell>from scratch</cell><cell>81.44</cell><cell></cell><cell>87.59</cell><cell></cell><cell>95.97</cell><cell></cell><cell>76.52</cell></row><row><cell></cell><cell cols="2">supervised ( ˜10 5 annot.) [30] 83.08</cell><cell cols="2">&lt;0.01 88.63</cell><cell cols="2">&lt;0.01 96.36</cell><cell cols="2">0.01 80.30</cell><cell>&lt;0.01</cell></row><row><cell></cell><cell cols="2">MultiTalent( ˜3600 annot.) 82.14</cell><cell>0.02</cell><cell>87.32</cell><cell>1.00</cell><cell>96.08</cell><cell cols="2">0.28 76.56</cell><cell>0.48</cell></row><row><cell>U-Net</cell><cell>from scratch</cell><cell>83.76</cell><cell></cell><cell>89.40</cell><cell></cell><cell>96.56</cell><cell></cell><cell>80.69</cell></row><row><cell></cell><cell>self-supervised [33]</cell><cell>84.01</cell><cell>0.11</cell><cell>89.30</cell><cell>0.92</cell><cell>96.59</cell><cell cols="2">0.35 80.91</cell><cell>0.39</cell></row><row><cell></cell><cell cols="2">supervised ( ˜10 5 annot.) [30] 84.22</cell><cell>0.01</cell><cell>89.66</cell><cell cols="2">&lt;0.01 96.72</cell><cell cols="2">0.09 82.48</cell><cell>0.02</cell></row><row><cell></cell><cell cols="2">MultiTalent ( ˜3600 annot.) 84.41</cell><cell cols="2">&lt;0.01 89.60</cell><cell cols="2">&lt;0.01 96.81</cell><cell cols="2">0.04 83.03</cell><cell>&lt;0.01</cell></row><row><cell cols="2">Resenc U-Net from scratch</cell><cell>84.38</cell><cell></cell><cell>89.71</cell><cell></cell><cell>96.83</cell><cell></cell><cell>83.22</cell></row><row><cell></cell><cell>self-supervised [33]</cell><cell>84.27</cell><cell>0.72</cell><cell>89.70</cell><cell>0.64</cell><cell>96.82</cell><cell cols="2">0.56 83.53</cell><cell>0.35</cell></row><row><cell></cell><cell cols="2">supervised ( ˜10 5 annot.) [30] 84.79</cell><cell>0.04</cell><cell>89.91</cell><cell cols="2">&lt;0.01 96.85</cell><cell cols="2">0.31 83.73</cell><cell>0.23</cell></row><row><cell></cell><cell cols="2">MultiTalent ( ˜3600 annot.) 84.92</cell><cell>0.03</cell><cell>89.81</cell><cell>0.16</cell><cell>96.89</cell><cell cols="2">0.04 84.01</cell><cell>0.12</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Assuming that no additional private data from the same data domain has been used.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. Part of this work was funded by <rs type="funder">Helmholtz Imaging (HI)</rs>, a platform of the Helmholtz Incubator on Information and Data Science.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_62.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The medical segmentation decathlon</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">4128</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00625</idno>
		<title level="m">Med3D: transfer learning for 3D medical image analysis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Cancer Imaging Archive (TCIA): maintaining and operating a public information repository</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1045" to="1057" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning multi-class segmentations from single-class datasets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dmitriev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-organ segmentation over partially labeled datasets with multi-scale feature abstraction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3619" to="3629" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">MS-KD: multi-organ segmentation with multiple binary-labeled datasets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02559</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Label-set loss functions for partial supervision: application to fetal brain 3D MRI parcellation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fidon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_60</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-3_60" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning from partially overlapping labels: image segmentation under annotation shift</title>
		<author>
			<persName><forename type="first">G</forename><surname>Filbrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DART/FAIR -2021</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12968</biblScope>
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87722-4_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87722-4_12" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic multi-organ segmentation on abdominal CT with dense V-networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1822" to="1834" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">UNETR: transformers for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2022-01">January 2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The KiTS19 challenge data: 300 kidney tumor cases with clinical context, CT semantic segmentations, and surgical outcomes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Heller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00445</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-organ segmentation via cotraining weight-averaged models from few-organ datasets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07149</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02182</idno>
		<title level="m">An attempt at beating the 3D U-Net</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.10791</idno>
		<title level="m">Extending nnU-Net is all you need</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">AMOS: a large-scale abdominal multi-organ benchmark for versatile medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08023</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10054</idno>
		<title level="m">Fine-tuning can distort pretrained features and underperform out-of-distribution</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dubray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05950</idno>
		<title level="m">SegTHOR: segmentation of thoracic organs at risk in CT images</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">MIC-CAI multi-atlas labeling beyond the cranial vault-workshop and challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Igelsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Langerak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<ptr target="https://www.synapse.org/#!Synapse:syn3193805/wiki/217760" />
		<imprint>
			<date type="published" when="2015-02-25">2015. 25 Feb 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automatic structure segmentation for radiotherapy planning challenge</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://structseg2019.grand-challenge.org/" />
		<imprint>
			<date type="published" when="2019-02-25">2019. 25 Feb 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-organ segmentation: a progressive exploration of learning paradigms under scarce annotation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00785</idno>
		<title level="m">Clip-driven universal model for organ segmentation and tumor detection</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context-aware voxel-wise contrastive learning for label efficient multi-organ segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_62</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_62" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">DeepOrgan: multi-level deep convolutional networks for automated pancreas segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06448</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DeepOrgan: multi-level deep convolutional networks for automated pancreas segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24553-9_68</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24553-9_68" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9349</biblScope>
			<biblScope unit="page" from="556" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint learning of brain lesion and anatomy segmentation from heterogeneous datasets</title>
		<author>
			<persName><forename type="first">N</forename><surname>Roulet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Slezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2nd International Conference on Medical Imaging with Deep Learning</title>
		<meeting>The 2nd International Conference on Medical Imaging with Deep Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Marginal loss and exclusion loss for partially supervised multi-organ segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">101979</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-supervised pre-training of Swin transformers for 3D medical image analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wasserthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Breit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cyriac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Segeroth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.05868</idno>
		<title level="m">TotalSegmentator: robust segmentation of 104 anatomical structures in CT images</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DoDNet: learning to segment multi-organ and tumors from multiple partially labeled datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Prior-aware neural network for partially-supervised multi-organ segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Models genesis. Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101840</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
