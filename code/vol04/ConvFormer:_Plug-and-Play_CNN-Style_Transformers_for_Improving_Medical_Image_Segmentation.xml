<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation</title>
				<funder ref="#_gy8XwUc #_qT58UZG">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">HPC Platform of HUST</orgName>
				</funder>
				<funder ref="#_EeBMkb5">
					<orgName type="full">Natural Science Foundation of Hubei Province of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xian</forename><surname>Lin</surname></persName>
							<email>xianlin@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zengqiang</forename><surname>Yan</surname></persName>
							<email>z_yan@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianbo</forename><surname>Deng</surname></persName>
							<email>dengxianbo@hotmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution" key="instit1">Union Hospital</orgName>
								<orgName type="institution" key="instit2">Tongji Medical College</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuansheng</forename><surname>Zheng</surname></persName>
							<email>cszheng@hust.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution" key="instit1">Union Hospital</orgName>
								<orgName type="institution" key="instit2">Tongji Medical College</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="642" to="651"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">26C95E33407B60382A223BF755569259</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CNN-Style Transformers</term>
					<term>Attention Collapse</term>
					<term>Adaptive Self-Attention</term>
					<term>Medical Image Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers have been extensively studied in medical image segmentation to build pairwise long-range dependence. Yet, relatively limited well-annotated medical image data makes transformers struggle to extract diverse global features, resulting in attention collapse where attention maps become similar or even identical. Comparatively, convolutional neural networks (CNNs) have better convergence properties on small-scale training data but suffer from limited receptive fields. Existing works are dedicated to exploring the combinations of CNN and transformers while ignoring attention collapse, leaving the potential of transformers under-explored. In this paper, we propose to build CNN-style Transformers (ConvFormer) to promote better attention convergence and thus better segmentation performance. Specifically, Con-vFormer consists of pooling, CNN-style self-attention (CSA), and convolutional feed-forward network (CFFN) corresponding to tokenization, self-attention, and feed-forward network in vanilla vision transformers. In contrast to positional embedding and tokenization, ConvFormer adopts 2D convolution and max-pooling for both position information preservation and feature size reduction. In this way, CSA takes 2D feature maps as inputs and establishes long-range dependency by constructing selfattention matrices as convolution kernels with adaptive sizes. Following CSA, 2D convolution is utilized for feature refinement through CFFN. Experimental results on multiple datasets demonstrate the effectiveness of ConvFormer working as a plug-and-play module for consistent performance improvement of transformer-based frameworks. Code is available at https://github.com/xianlin7/ConvFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Benefiting from the prominent ability to model long-range dependency, transformers have become the de-facto standard for natural language processing <ref type="bibr" target="#b0">[1]</ref>. Compared with convolutional neural networks (CNNs), which encourage locality, weight sharing, and translation equivariance, transformers build global dependency through self-attention layers, bringing more possibilities for feature exaction and breaking the performance ceiling of CNNs in return <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>.</p><p>Inspired by this, transformers are introduced into medical image segmentation and arouse wide concerns <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref>. In vision transformers, each medical image is first split into a series of patches and then projected into a 1D sequence of patch embeddings <ref type="bibr" target="#b3">[4]</ref>. Through building pairwise interaction among patches/tokens, transformers are supposed to aggregate global information for robust feature exaction. However, learning well-convergence global dependency in transformers is highly data-intensive, making transformers less effective given relatively limited medical imaging data. To figure out how transformers work in medical image segmentation, we trained four state-of-the-art transformer-based models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">[12]</ref><ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref> on the ACDC dataset and visualized the learned self-attention matrices across different layers as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. For all approaches, the attention matrices tend to become uniform among patches (i.e., attention collapse <ref type="bibr" target="#b15">[15]</ref>), especially in deeper layers. Attention collapse is more noticeable, especially in CNN-Transformer hybrid approaches (i.e., TransUNet, TransFuse, and FAT-Net). On the one hand, insufficient training data would make transformers learn sub-optimal long-range dependency. On the other hand, directly combining CNNs with transformers would make the network biased to the learning of CNNs, as the convergence of CNNs is more achievable compared to transformers, especially on small-scale training data. Therefore, how to address attention collapse and improve the convergence of transformers is crucial for performance improvement.</p><p>In this work, we propose a plug-and-play module named ConvFormer to address attention collapse by constructing a kernel-scalable CNN-style transformer. In ConvFormer, 2D images can directly build sufficient long-range dependency without being split into 1D sequences. Specifically, corresponding to tok-enization, self-attention, and feed-forward network in vanilla vision transformers, ConvFormer consists of pooling, CNN-style self-attention (CSA), and convolutional feed-forward network (CFFN) respectively. For an input image/feature map, its resolution is first reduced by applying convolution and max-pooling alternately. Then, CSA builds appropriate dependency for each pixel by adaptively generating a scalable convolutional, being smaller to include locality or being larger for long-range global interaction. Finally, CFFN refines the features of each pixel by applying continuous convolutions. Extensive experiments on three datasets across five state-of-the-art transformer-based methods validate the effectiveness of ConvFormer, outperforming existing solutions to attention collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent transformer-based approaches for medical image analysis mainly focus on introducing transformers for robust features exaction in the encoder, crossscale feature interactive in skip connection, and multifarious feature fusion in the decoder <ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref>. The study about addressing attention collapse for transformers in medical imaging is under-explored. Even in natural image processing, attention collapse, usually existing in the very deep layers of deep transformerbased models, has not been fully studied. Specifically, Zhou et al. <ref type="bibr" target="#b15">[15]</ref> developed Re-attention to re-generate self-attention matrices aiming at increasing their diversity on different layers. Zhou et al. <ref type="bibr" target="#b20">[20]</ref> projected self-attention matrices into a high-dimensional space and applied convolutions to promote the locality and diversity of self-attention matrices. Touvron et al. <ref type="bibr" target="#b21">[21]</ref> proposed to re-weight the channels of the outputs from the self-attention module and the feed-forward module to facilitate the convergence of transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The comparison between the vision transformer (ViT) and ConvFormer is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. The greatest difference is that our ConvFormer is conducted on 2D inputs while ViT is applied to 1D sequences. Specifically, the pooling module is utilized to replace tokenization in ViT, which well preserves locality and positional information without extra positional embeddings. The CNN-style selfattention (CSA) module, i.e. the core of ConvFormer, is developed to replace the self-attention (SA) module in ViT to build long-range dependency by constructing self-attention matrices in a similar way like convolutions with adaptive and scalable kernels. The convolutional feed-forward network (CFFN) is developed to refine the features for each pixel corresponding to the feed-forward network (FFN) in ViT. No upsampling procedure is adopted to resize the output of Con-vFormer back to the input size as the pooling module can match the output size by adjusting the maxpooling times. It should be noticed that ConvFormer is realized based on convolutions, which eliminates the training tension between CNNs and transformers as analyzed in Sect. 1. Each module of ConvFormer is described in the following. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pooling vs. Tokenization</head><p>The pooling module is developed to realize the functions of tokenization (i.e., making the input suitable to transformers in the channel dimension and shaping and reducing the input size when needed) while without losing details in the grid lines in tokenization. For an input X in ∈ R c×H×W , convolution with a kernel size of 3 × 3 followed by batch normalization and Relu, is first applied to capture local features. Then, corresponding to each patch size S in ViT, total d = log 2 S downsampling operations are applied in the pooling module to produce the same resolutions. Here, each downsampling operation consists of a max-pooling with a kernel size of 2 × 2 and a combination of 3 × 3 convolution, batch normalization, and Relu. Finally,</p><formula xml:id="formula_0">X in becomes X 1 ∈ R c m × H 2 d × W 2 d</formula><p>through the pooling module where c m is corresponding to the embedding dimension in ViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CNN-Style vs. Sequenced Self-attention</head><p>The building of long-range dependency in ConvFormer is relying on CNN-style self-attention, which creates an adaptive receptive field for each pixel by constructing a customized convolution kernel. Specifically, for each pixel x i, j of X 1 , the convolution kernel A i, j is constructed based on two intermediate variables:</p><formula xml:id="formula_1">Q i, j = 1 l=-1 1 g=-1 E q 2+l,2+g x i+l, j+g ,<label>(1)</label></formula><formula xml:id="formula_2">K i, j = 1 l=-1 1 g=-1 E k 2+l,2+g x i+l, j+g ,<label>(2)</label></formula><p>where E q and E k ∈ R c q ×c m ×3×3 are the learnable projection matrices and c q is corresponding to the embedding dimension of Q, K, and V in ViT, which incorporates the features of adjacent pixels in 3 × 3 neighborhood into x i, j . Then, the initial customized convolutional kernel</p><formula xml:id="formula_3">I i, j ∈ R H 2 d × W</formula><p>2 d for x i, j is calculated by computing the cosine similarity:</p><formula xml:id="formula_4">I i, j m,n = c q l=0 Q i, j K m,n c q l=0 Q 2 i, j c q l=0 K 2 m,n .<label>(3)</label></formula><p>Here,</p><formula xml:id="formula_5">I i, j m,n ∈ [-1, 1] and seldom occurs I i, j m,n = 0. I i, j</formula><p>m,n corresponds to attention score calculation in ViT (constrained to be positive while I i, j m,n can be either positive or negative). Then, we dynamically determine the size of the customized convolution kernel for x i, j by introducing a learnable Gaussian distance map M:</p><formula xml:id="formula_6">M i, j m,n = e -(i-m) 2 (2 d /H ) 2 +( j-n) 2 (2 d /W ) 2 2(θ ×α) 2 , (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where θ ∈ (0, 1) is a learnable network parameter to control the receptive field of A and α is a hyper-parameter to control the tendency of the receptive field. θ is proportional to the receptive field. For instance, under the typical setting H = W = 256, d = 3, and α = 1, when θ = 0.003, the receptive field only covers five adjacent pixels, when θ &gt; 0.2, the receptive field is global. The larger α is, the more likely A tends to have a global receptive field. Based on I i, j and M i, j , A i, j is calculated by A i, j = I i, j × M i, j . In this way, every pixel x i, j has a customized size-scalable convolution kernel A i, j . By multiplying A with V, CSA can build adaptive long-range dependency, where V can be formulated similarly according to Eq. ( <ref type="formula" target="#formula_1">1</ref>). Finally, the combination of 1 × 1 convolution, batch normalization, and Relu is utilized to integrate features learned from long-range dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Convolution Vs. Vanilla Feed-Forward Network</head><p>The convolution feed-forward network (CFFN) is to refine the features produced by CSA, just consisting of two combinations of Approaches outperforming the state-of-the-art 2D approaches on the publiclyavailable ACDC (i.e., FAT-Net <ref type="bibr" target="#b14">[14]</ref>: 91.46% in Avg. DSC) and ISIC (i.e., Ms Red <ref type="bibr" target="#b28">[28]</ref>: 90.25% in Avg. DSC) datasets respectively. More comprehensive quantitative comparison results can be found in the supplemental materials. myocardium (MYO), and right ventricle (RV) are available <ref type="bibr" target="#b22">[22]</ref>. Following <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18]</ref>, 70, 10, and 20 cases are used for training, validation, and testing respectively. ISIC 2018<ref type="foot" target="#foot_1">2</ref> . A publicly-available dataset for skin lesion segmentation. Totally 2594 dermoscopic lesion images with pixel-level annotations are available <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref>. Following <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref>, the dataset is randomly divided into 2076 images for training and 520 images for testing.</p><p>ICH. A locally-collected dataset for hematoma segmentation. Totally 99 CT scans consisting of 2648 slices were collected and annotated by three radiologists. The dataset is randomly divided into the training, validation, and testing sets according to a ratio of 7:1:2. Implementation Details. For a fair comparison, all the selected state-of-theart transformer-based baselines were trained with or without ConvFormer under the same settings. All models were trained by an Adam optimizer with a learning rate of 0.0001 and a batch size of 4 for 400 rounds. Data augmentation includes random rotation, scaling, contrast augmentation, and gamma augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>ConvFormer can work as a plug-and-play module and replace the vanilla transformer blocks in transformer-based baselines. To evaluate the effectiveness of ConvFormer, five state-of-the-art transformer-based approaches are selected as backbones, including SETR <ref type="bibr" target="#b4">[5]</ref>, TransUNet <ref type="bibr" target="#b12">[12]</ref>, TransFuse <ref type="bibr" target="#b13">[13]</ref>, FAT-Net <ref type="bibr" target="#b14">[14]</ref>, and Patcher <ref type="bibr" target="#b27">[27]</ref>. SETR and Patcher utilize pure-transformer encoders, while TransUNet, TransFuse, and FAT-Net adopt CNN-Transformer hybrid encoders. In addition, three state-of-the-art methods for addressing attention collapse, including Re-attention <ref type="bibr" target="#b15">[15]</ref>, LayerScale <ref type="bibr" target="#b21">[21]</ref>, and Refiner <ref type="bibr" target="#b20">[20]</ref>, are equipped with the above transformer-based baselines for comparison. Quantitative Results. Quantitative results of ConvFormer embedded into various transformer-based baselines on the three datasets are summarized in Table <ref type="table" target="#tab_0">1</ref>. ConvFormer achieves consistent performance improvements on all five backbones. Compared to CNN-Transformer hybrid approaches (i.e., TransUNet, TransFuse, and FAT-Net), ConvFormer is more beneficial on pure-transformer approaches (i.e., SETR and Patcher). Specifically, with ConvFormer, SETR achieves an average increase of 3.86%, 1.38%, and 1.39% in Dice on the ACDC, ISIC, and ICH datasets respectively, while the corresponding performance improvements of Patcher are 0.66%, 1.07%, and 1.15% respectively. Comparatively, in CNN-Transformer hybrid approaches, as analyzed above, CNNs would be more dominating against transformers during training. Despite this, re-balancing CNNs and Transformers through ConvFormer can build better longrange dependency for consistent performance improvement. Comparison with SOTA Approaches. Quantitative results compared with the state-of-the-art approaches to addressing attention collapse are summarized in Table <ref type="table" target="#tab_0">1</ref>. In general, given relatively limited training data, existing approaches designed for natural image processing are unsuitable for medical image segmentation, resulting in unstable performance across different backbones and datasets. Comparatively, ConvFormer consistently outperforms these approaches and brings stable performance improvements to various backbones across datasets, demonstrating the excellent generalizability of ConvFormer as a plug-and-play module. Visualization of Self-Attention Matrices. To qualitatively evaluate the effectiveness of ConvFormer in addressing attention collapse and building efficient long-range dependency, we visualize the self-attention matrices with and  without ConvFormer as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. By introducing ConvFormer, attention collapse is effectively alleviated. Compare to the self-attention matrices of baselines, the matrices learned by ConvFormer are more diverse. Specifically, the interactive range for each pixel is scalable, being small for locality preserving or being large for global receptive fields. Besides, dependency is no longer constrained to be positive like ViT, which is more consistent with convolution kernels. Qualitative segmentation results of different approaches on the three datasets can be found in the supplemental materials. Ablation Study As described in Sec. 3.2, α is to control the receptive field tendency in ConvFormer, The larger the α, the more likely ConvFormer contains larger receptive fields. To validate this, we conduct an ablation study on α as summarized in Table <ref type="table" target="#tab_1">2</ref>. In general, using a large α does not necessarily lead to more performance improvements, which is consistent with our observation that not every pixel needs global information for segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we construct the transformer as a kernel-scalable convolution to address the attention collapse and build diverse long-range dependencies for efficient medical image segmentation. Specifically, it consists of pooling, CNNstyle self-attention (CSA), and convolution feed-forward network (CFFN). The pooling module is first applied to extract the locality details while reducing the computational costs of the following CSA module by downsampling the inputs. Then, CSA is developed to build adaptive long-range dependency by constructing CSA as a kernel-scalable convolution, Finally, CFFN is used to refine the features of each pixel. Experimental results on five state-of-the-art baselines across three datasets demonstrate the prominent performance of ConvFormer, stably exceeding the baselines and comparison methods across three datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Visualization of attention maps from the selected layers of the first head in different transformer frameworks. The darker the color, the closer the dependency.</figDesc><graphic coords="2,55,98,262,76,340,27,88,87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison between vanilla vision transformer and ConvFormer. CBR is short for the combination of convolution, batch normalization, and Relu. Multiple heads are omitted for simplicity.</figDesc><graphic coords="4,55,98,54,47,340,15,218,23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of self-attention matrices by baselines w/ and w/o ConvFormer.</figDesc><graphic coords="8,57,48,53,78,337,36,212,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>1 × 1 convolution, batch normalization, and Relu. By replacing linear projection and layer normalization in ViT, CFFN makes ConvFormer completely CNN-based, avoiding the combat between CNN and Transformer during training like CNN-Transformer hybrid approaches. Quantitative results in Dice (DSC) and Hausdorff Distance (HD).</figDesc><table><row><cell>4 Experiments</cell></row><row><cell>4.1 Datasets and Implementation Details</cell></row><row><cell>ACDC</cell></row></table><note><p><p>1 </p>. A publicly-available dataset for the automated cardiac diagnosis challenge. Totally 100 scans with pixel-wise annotations of left ventricle (LV),</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of hyper-parameter α on the ACDC dataset.</figDesc><table><row><cell>α</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell></row></table><note><p>Dice (%) 90.71 91.00 90.76 90.66 90.45</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.creatis.insa-lyon.fr/Challenge/acdc/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://challenge.isic-archive.com/data/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62271220</rs> and Grant <rs type="grantNumber">62202179</rs>, and in part by the <rs type="funder">Natural Science Foundation of Hubei Province of China</rs> under Grant <rs type="grantNumber">2022CFB585</rs>. The computation is supported by the <rs type="funder">HPC Platform of HUST</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gy8XwUc">
					<idno type="grant-number">62271220</idno>
				</org>
				<org type="funding" xml:id="_qT58UZG">
					<idno type="grant-number">62202179</idno>
				</org>
				<org type="funding" xml:id="_EeBMkb5">
					<idno type="grant-number">2022CFB585</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_61.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention gated networks: learning to leverage salient regions in medical images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Class-aware generative adversarial transformers for medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10737</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolution-free medical image segmentation using transformers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Vasylechko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="78" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">mmFormer: multimodal medical transformer for incomplete multimodal learning of brain tumor segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_11" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SMESwin unet: merging CNN and transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_50" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2022</title>
		<editor>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SATr: slice attention with transformer for universal lesion detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_16" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="163" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Transunet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transfuse: fusing transformers and CNNs for medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_2" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FAT-Net: feature adaptive transformers for automated skin lesion segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">102327</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">DeepViT: towards deeper vision transformer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Missformer: an effective medical image segmentation transformer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07162</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Swin-unet: Unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05537</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Levit-unet: make faster encoders with transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08623</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phtrans: parallelly aggregating global and local representations for medical image segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_23" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Refiner: refining self-attention for vision transformers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03714</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic MRI cardiac multistructures segmentation and diagnosis: is the problem solved?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection 2018: a challenge hosted by the international skin imaging collaboration (ISIC)</title>
		<author>
			<persName><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03368</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ds-transunet: dual swin transformer u-net for medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Transattunet: multi-level attention-guided u-net with transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W K</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05274</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Patcher: patch transformers with mixture of experts for precise medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_46" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="475" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ms RED: a novel multi-scale residual encoding and decoding network for skin lesion segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">102293</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
