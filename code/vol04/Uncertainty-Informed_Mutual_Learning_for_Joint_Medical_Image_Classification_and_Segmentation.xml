<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation</title>
				<funder ref="#_vdwBnVZ">
					<orgName type="full">A*STAR Central Research Fund</orgName>
				</funder>
				<funder ref="#_cv5vtnV">
					<orgName type="full">China Scholarship Council</orgName>
				</funder>
				<funder ref="#_4KHVfHJ">
					<orgName type="full">Science and Technology Department of Sichuan Province</orgName>
				</funder>
				<funder ref="#_GheuXFu">
					<orgName type="full">National Research Foundation, Singapore</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kai</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Key Laboratory of Fundamental Science on Synthetic Vision</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu, Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu, Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Key Laboratory of Fundamental Science on Synthetic Vision</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu, Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu, Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianjie</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu, Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yidi</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution" key="instit1">West China Hospital</orgName>
								<orgName type="institution" key="instit2">Sichuan University</orgName>
								<address>
									<settlement>Chengdu, Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuedong</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Key Laboratory of Fundamental Science on Synthetic Vision</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu, Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu, Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaojing</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Key Laboratory of Fundamental Science on Synthetic Vision</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu, Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">College of Mathematics</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu, Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Agency for Science, Technology and Research</orgName>
								<orgName type="institution">Institute of High Performance Computing</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<email>hzfu@ieee.org</email>
							<affiliation key="aff4">
								<orgName type="department">Agency for Science, Technology and Research</orgName>
								<orgName type="institution">Institute of High Performance Computing</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="35" to="45"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">053892163DD626A1C06F6E45460C3314</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Mutual learning</term>
					<term>Medical image classification and segmentation</term>
					<term>Uncertainty estimation K. Ren and K. Zou-Denotes equal contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classification and segmentation are crucial in medical image analysis as they enable accurate diagnosis and disease monitoring. However, current methods often prioritize the mutual learning features and shared model parameters, while neglecting the reliability of features and performances. In this paper, we propose a novel Uncertainty-informed Mutual Learning (UML) framework for reliable and interpretable medical image analysis. Our UML introduces reliability to joint classification and segmentation tasks, leveraging mutual learning with uncertainty to improve performance. To achieve this, we first use evidential deep learning to provide image-level and pixel-wise confidences. Then, an uncertainty navigator is constructed for better using mutual features and generating segmentation results. Besides, an uncertainty instructor is proposed to screen reliable masks for classification. Overall, UML could produce confidence estimation in features and performance for each link (classification and segmentation). The experiments on the public datasets demonstrate that our UML outperforms existing methods in terms of both accuracy and robustness. Our UML has the potential to explore the development of more reliable and explainable medical image analysis models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate and robust classification and segmentation of the medical image are powerful tools to inform diagnostic schemes. In clinical practice, the image-level classification and pixel-wise segmentation tasks are not independent <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>. Joint classification and segmentation can not only provide clinicians with results for both tasks simultaneously, but also extract valuable information and improve performance. However, improving the reliability and interpretability of medical image analysis is still reaching.</p><p>Considering the close correlation between the classification and segmentation, many researchers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> proposed to collaboratively analyze the two tasks with the help of sharing model parameters or task interacting. Most of the methods are based on sharing model parameters, which improves the performance by fully utilizing the supervision from multiple tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>. For example, Thomas et al. <ref type="bibr" target="#b19">[20]</ref> combined whole image classification and segmentation of skin cancer using a shared encoder. Task interacting is also a widely used method <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref> as it can introduce the high-level features and results produced by one task to benignly guide another. However, there has been relatively little research on introducing reliability into joint classification and segmentation. The reliability and interpretability of the model are particularly important for clinical tasks, a single result of the most likely hypothesis without any clues about how to make the decision might lead to misdiagnoses and sub-optimal treatment <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>. One potential way of improving reliability is to introduce uncertainty for the medical image analysis model.</p><p>The current uncertainty estimation method can roughly include the Dropoutbased <ref type="bibr" target="#b10">[11]</ref>, ensemble-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, deterministic-based methods <ref type="bibr" target="#b20">[21]</ref> and evidential deep learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. All of these methods are widely utilized in classification and segmentation applications for medical image analysis. Abdar et al. <ref type="bibr" target="#b0">[1]</ref> employed three uncertainty quantification methods (Monte Carlo dropout, Ensemble MC dropout, and Deep Ensemble) simultaneously to deal with uncertainty estimation during skin cancer image classification. Zou et al. <ref type="bibr" target="#b30">[31]</ref> proposed TBraTS based on evidential deep learning to generate robust segmentation results for brain tumor and reliable uncertainty estimations. Unlike the aforementioned methods, which only focus on uncertainty in either medical image classification or segmentation. Furthermore, none of the existing methods have considered how pixel-wise and image-level uncertainty can help improve performance and reliability in mutual learning.</p><p>Based on the analysis presented above, we design a novel Uncertaintyinformed Mutual Learning (UML) network for medical image analysis in this study. Our UML not only enhances the image-level and pixel-wise reliability of medical image classification and segmentation, but also leverages mutual learning under uncertainty to improve performance. Specifically, we adopt evidential deep learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31]</ref> to simultaneously estimate the uncertainty of both to estimate image-level and pixel-wise uncertainty. We introduce an Uncertainty Navigator for segmentation (UN) to generate preliminary segmentation results, taking into account the uncertainty of mutual learning features. We also propose an Uncer- tainty Instructor for classification (UI) to screen reliable masks for classification based on the preliminary segmentation results. Our UML represents pioneering work in introducing reliability and interpretability to joint classification and segmentation, which has the potential to the development of more trusted medical analysis tools<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The overall architecture of the proposed UML, which leverages mutual learning under uncertainty, is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Firstly, Uncertainty Estimation for Classification and Segmentation adapts evidential deep learning to provide image-level and pixel-wise uncertainty. Then, Trusted Mutual Learning not only utilizes the proposed UN to fully exploit pixel-wise uncertainty as the guidance for segmentation but also introduces the UI to filter the feature flow between task interaction.</p><p>Given an input medical image I, I ∈ R H,W , where H, W are the height and width of the image, separately. To maximize the extraction of specific information required for two different tasks while adequately mingling the common feature which is helpful for both classification and segmentation, I is firstly fed into the dual backbone network that outputs the classification feature maps f c i , i ∈ 1, ..., 4 and segmentation feature maps f s i , i ∈ 1, ..., 4, where i denotes the i th layer of the backbone. Then following <ref type="bibr" target="#b28">[29]</ref>, we construct the Feature Mixer using Pairwise Channel Map Interaction to mix the original feature and get the mutual feature maps f m i , i ∈ 1, ..., 4. Finally, we combine the last layer of mutual feature with the original feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Uncertainty Estimation for Classification and Segmentation</head><p>Classification Uncertainty Estimation. For the K classification problems, we utilize Subjective Logic <ref type="bibr" target="#b6">[7]</ref> to produce the belief mass of each class and the uncertainty mass of the whole image based on evidence. Accordingly, given a classification result, its K + 1 mass values are all non-negative and their sum is one:</p><formula xml:id="formula_0">K k=1 b c k + U c = 1,<label>(1)</label></formula><p>where b c k ≥ 0 and U c ≥ 0 denote the probability belonging to the k th class and the overall uncertainty value, respectively. As shown in Fig. <ref type="figure" target="#fig_0">1</ref> </p><formula xml:id="formula_1">c k = e c k T c = α c k -1 T c , and U c = K T c ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">T c = K k=1 α c k = K k=1 (e c k + 1)</formula><p>represents the Dirichlet strength. Actually, Eq. 2 describes such a phenomenon that the higher the probability assigned to the k th class, the more evidence observed for k th category should be.</p><p>Segmentation Uncertainty Estimation. Essentially, segmentation is the classification for each pixel of a medical image. Given a pixel-wise segmentation result, following <ref type="bibr" target="#b30">[31]</ref> the seg Dirichlet distribution can be parameterized by α s(h,w) = [α s(h,w) 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, . . . , α s(h,w) Q</head><p>], (h, w) ∈ (H, W ). We can compute the belief mass and uncertainty mass of the input image by</p><formula xml:id="formula_3">b s(h,w) q = e s(h,w) q T s(h,w) = α s(h,w) q -1 T s(h,w) , and u s(h,w) = Q T s(h,w) , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where b s(h,w) q ≥ 0 and u s(h,w) ≥ 0 denote the probability of the pixel at coordinate (h, w) for the q th class and the overall uncertainty value respectively. We also define U s = {u s(h,w) , (h, w) ∈ (H, W )} as the pixel-wise uncertainty of the segmentation result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Uncertainty-Informed Mutual Learning</head><p>Uncertainty Navigator for Segmentation. Actually, we have already obtained an initial segmentation mask M = α s , M ∈ (Q, H, W ) through estimating segmentation uncertainty, and achieved lots of valuable features such as lesion location. In our method, appropriate uncertainty guided decoding on the feature list can obtain more reliable information and improve the performance of segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref>. So we introduce Uncertainty Navigator for Segmentation(UN) as a feature decoder, which incorporates the pixel-wise uncertainty in U s and lesion location information in M with the segmentation feature maps to generate the segmentation result and reliable features. Having a UNet-like architecture <ref type="bibr" target="#b14">[15]</ref>, UN computes segmentation s i , i ∈ 1, .., 4 at each layer, as well as introduces the uncertainty in the bottom and top layer by the same way. Take the top layer as an example, as shown in Fig. <ref type="figure" target="#fig_2">2</ref>(a), UN calculates the reliable mask M r by:</p><formula xml:id="formula_5">M r = (s 1 ⊕ M ) ⊗ e -U s , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Then, the reliable segmentation feature r s , which combines the trusted information in M r with the original features, is generated by:</p><formula xml:id="formula_7">r s = Cat(Conv(M r ), Cat(f s 1 , f b 2 )),<label>(5)</label></formula><p>where f s 1 derives from jump connecting and f b 2 is the feature of the s 2 with one up-sample operation. Conv(•) represents the convolutional operation, Cat(•, •) denotes the concatenation. Especially, the U s is also used to guide the bottom feature with the dot product. The r s is calculated from the segmentation result s 1 and contains uncertainty navigated information not found in s 1 .</p><p>Uncertainty Instructor for Classification. In order to mine the complementary knowledge of segmentation as the instruction for the classification and eliminate intrusive features, we devise an Uncertainty Instructor for classification (UI) following <ref type="bibr" target="#b21">[22]</ref>.  and the rich information (e.g., lesion location and boundary characteristic) in r s , which can be expressed by:</p><formula xml:id="formula_8">r c = f c 4 ⊕ (Conv(d 3 (r s )) ⊗ f c 4 ),<label>(6)</label></formula><p>where d n (•) denotes that the frequency of down-sampling operations is n. Then the produced features are transformed into a semantic feature vector by the global average pooling. The obtained vector is converted into the final result (belief values) of classification with uncertainty estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mutual Learning Process</head><p>In a word, to obtain the final results of classification and segmentation, we construct an end-to-end mutual learning process, which is supervised by a joint loss function. To obtain an initial segmentation result M and a pixel-wise uncertainty estimation U s , following <ref type="bibr" target="#b30">[31]</ref>, a mutual loss is used as:</p><formula xml:id="formula_9">L m (α s , y s ) = L ice (α s , y s ) + λ m 1 L KL (α s ) + λ m 2 L Dice (α s , y s ),<label>(7)</label></formula><p>where y s is the Ground Truth (GT) of the segmentation. The hyperparameters λ m 1 and λ m 2 play a crucial role in controlling the Kullback-Leibler divergence (KL) and Dice score, as supported by <ref type="bibr" target="#b30">[31]</ref>. Similarly, in order to estimate the imagelevel uncertainty and classification results. a classification loss is constructed following <ref type="bibr" target="#b4">[5]</ref>, as:</p><formula xml:id="formula_10">L c (α c , y c ) = L ace (α c , y c ) + λ c L KL (α c ), (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where y c is the true class of the input image. The hyperparameter λ c serves as a crucial hyperparameter governing the KL, aligning with previous work <ref type="bibr" target="#b4">[5]</ref>. To obtain reliable segmentation results, we also adopt deep supervision for the final segmentation result S = {s i , i = 1, ..., 4}, which can be denoted as:</p><formula xml:id="formula_12">L s = 4 i=1 L Dice (υ i-1 (s i ), y s ) 4 , (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where υ n indicates the number of up-sampling is 2 n . Thus, the overall loss function of our UML can be given as:</p><formula xml:id="formula_14">L UML (α s , α c , y s , y c ) = w m L m (α s , y s ) + w c L c (α c , y c ) + w s L s , (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>where w m , w c , w s denote the weights and are set 0.1, 0.5, 0.4, separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset and Implementation. We evaluate the our UML network on two datasets REFUGE <ref type="bibr" target="#b13">[14]</ref> and ISPY-1 <ref type="bibr" target="#b12">[13]</ref>. REFUGE contains two tasks, classification of glaucoma and segmentation of optic disc/cup in fundus images. The overall 1200 images were equally divided for training, validation, and testing. All images are uniformly adjusted to 256 × 256 px. The tasks of ISPY-1 are the pCR prediction and the breast tumor segmentation. A total of 157 patients who suffer the breast cancer are considered -43 achieve pCR and 114 non-pCR.</p><p>For each case, we cut out the slices in the 3D image and totally got 1,570 2D images, which are randomly divided into the train, validation, and test datasets with 1,230, 170, and 170 slices, respectively.  We implement the proposed method via PyTorch and train it on NVIDIA GeForce RTX 2080Ti. The Adam optimizer is adopted to update the overall parameters with an initial learning rate 0.0001 for 100 epochs. The scale of the regularizer is set as 1 × 10 -5 . We choose VGG-16 and Res2Net as the encoders for classification and segmentation, separately.</p><p>Compared Methods and Metrics. We compared our method with singletask methods and multi-task methods. (1) Single-task methods: (a) EC <ref type="bibr" target="#b16">[17]</ref>, (b) TBraTS <ref type="bibr" target="#b30">[31]</ref> and (c) TransUNet <ref type="bibr" target="#b1">[2]</ref>. Evidential deep learning for classification (EC) first proposed to parameterize classification probabilities as Dirichlet distributions to explain evidence. TBraTS then extended EC to medical image segmentation. Meriting both Transformers and U-Net, TransUNet is a strong model for medical image segmentation. (2) Multi-task methods: (d) BCS <ref type="bibr" target="#b24">[25]</ref> and (e) DSI <ref type="bibr" target="#b27">[28]</ref>. The baseline of the Joint Classification and Segmentation framework (BCS) is a simple but useful way to share model parameters, which utilize two different encoders and decoders for learning respectively. The Deep Synergistic Interaction Network (DSI) has demonstrated superior performance in joint task. We adopt overall Accuracy (ACC) and F1 score (F1) as the evaluation criteria for the classification task. Dice score (DI) and Average Symmetric Surface Distance (ASSD) are chosen for the segmentation task. Comparison Under Noisy Data. To further valid the reliability of our model, we introduce Gaussian noise with various levels of standard deviations (σ) to the input medical images. The comparison results are shown in Table <ref type="table" target="#tab_1">2</ref>. As can be observed that, the accuracy of classification and segmentation significantly decreases after adding noise to the raw data. However, benefiting from the uncertainty-informed guiding, our UML consistently deliver impressive results. In Fig. <ref type="figure" target="#fig_5">3</ref>, we show the output of our model under the noise. It is obvious that both the image-level uncertainty and the pixel-wise uncertainty respond reasonably well to noise. These experimental results can verify the reliability and interpre of the uncertainty guided interaction between the classification and segmentation in the proposed UML. The results of more qualitative comparisons can be found in the Supplementary Material.</p><p>Ablation Study. As illustrated in Table <ref type="table" target="#tab_2">3</ref>, both of the proposed UN and UI play important roles in trusted mutual learning. The baseline method is BCS.</p><p>MD represents the mutual feature decoder. It is clear that the performance of classification and segmentation is significantly improved when we introduce supervision of mutual features. As we thought, the introduction of UN and UI takes the reliability of the model to a higher level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel deep learning approach, UML, for joint classification and segmentation of medical images. Our approach is designed to improve the reliability and interpretability of medical image classification and segmentation, by enhancing image-level and pixel-wise reliability estimated by evidential deep learning, and by leveraging mutual learning with the proposed UN and UI modules. Our extensive experiments demonstrate that UML outperforms baselines and introduces significant improvements in both classification and segmentation. Overall, our results highlight the potential of UML for enhancing the performance and interpretability of medical image analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The framework of Uncertainty-informed Mutual Learning network.</figDesc><graphic coords="3,71,46,54,59,309,58,195,10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, the cls evidence e c = [e c 1 , . . . , e c K ] for the classification result is acquired by an activation function layer softplus and e c k ≥ 0. Then the cls Dirichlet distribution can be parameterize by α c = [α c 1 , . . . , α c K ], which associated with the cls evidence e c k , i.e. α c k = e c k + 1. In the end, the image-level belief mass and the uncertainty mass of the classification can be calculated by b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Details of (a) Uncertainty Navigator (UN) and (b) Uncertainty Instructor (UI).</figDesc><graphic coords="5,104,97,54,47,242,17,111,22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 (</head><label>2</label><figDesc>b) shows the architecture of UI. It firstly generates reliable classification features r c fusing the initial classification feature maps f c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The visual result of segmentation and classification in REFUGE and ISPY-1. Top is the original image, and bottom is the input with Gaussian noise (σ = 0.05). From left to right, input (with GT), the result of classification (belief and image-level uncertainty), the result of segmentation, pixel-wise uncertainty.</figDesc><graphic coords="7,66,84,195,74,329,92,82,18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of the classification and segmentation performance. The top-2 results are highlighted in bold and underlined (p ≤ 0.01).</figDesc><table><row><cell>Method</cell><cell cols="2">REFUGE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ISPY-1</cell></row><row><cell></cell><cell>CLS</cell><cell></cell><cell>SEG</cell><cell></cell><cell></cell><cell></cell><cell>CLS</cell><cell>SEG</cell></row><row><cell></cell><cell cols="2">ACC F 1</cell><cell cols="6">DIdisc ASSDdisc DIcup ASSDcup ACC F 1</cell><cell>DI</cell><cell>ASSD</cell></row><row><cell>Single-task EC</cell><cell cols="3">0.560 0.641 \</cell><cell>\</cell><cell>\</cell><cell>\</cell><cell cols="2">0.735 0.648 \</cell><cell>\</cell></row><row><cell>TBraTS</cell><cell>\</cell><cell>\</cell><cell cols="2">0.776 1.801</cell><cell cols="2">0.787 1.798</cell><cell>\</cell><cell>\</cell><cell>0.784 4.075</cell></row><row><cell cols="2">TransUNet \</cell><cell>\</cell><cell cols="2">0.633 2.807</cell><cell cols="2">0.628 2.638</cell><cell>\</cell><cell>\</cell><cell>0.692 5.904</cell></row><row><cell>Multi-task BCS</cell><cell cols="4">0.723 0.778 0.802 1.692</cell><cell cols="2">0.831 1.532</cell><cell cols="2">0.758 0.692 0.773 3.804</cell></row><row><cell>DSI</cell><cell cols="4">0.838 0.834 0.793 2.030</cell><cell cols="2">0.811 1.684</cell><cell cols="2">0.741 0.673 0.760 4.165</cell></row><row><cell>Ours</cell><cell cols="4">0.853 0.875 0.855 1.560</cell><cell cols="2">0.858 1.251</cell><cell cols="2">0.771 0.713 0.785 3.927</cell></row><row><cell>Non</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Glaucomatous</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The quantitative comparisons on the REFUGE dataset with vary NOISE levels.</figDesc><table><row><cell cols="2">REFUGE 0.030</cell><cell></cell><cell></cell><cell>0.050</cell></row><row><cell></cell><cell>CLS</cell><cell>SEG</cell><cell></cell><cell>CLS</cell><cell>SEG</cell></row><row><cell></cell><cell>ACC F 1</cell><cell cols="3">DIdisc ASSDdisc DIcup ASSDcup ACC F 1</cell><cell>DIdisc ASSDdisc DIcup ASSDcup</cell></row><row><cell>BCS</cell><cell cols="2">0.620 0.694 0.743 2.104</cell><cell>0.809 1.670</cell><cell cols="2">0.430 0.510 0.610 3.142</cell><cell>0.746 2.188</cell></row><row><cell>DSI</cell><cell cols="2">0.675 0.733 0.563 9.196</cell><cell>0.544 9.705</cell><cell cols="2">0.532 0.574 0.409 8.481</cell><cell>0.364 9.794</cell></row><row><cell>Ours</cell><cell cols="2">0.827 0.857 0.830 1.752</cell><cell>0.840 1.407</cell><cell cols="2">0.733 0.785 0.744 2.142</cell><cell>0.778 2.009</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The result of Ablation Study.</figDesc><table><row><cell>MD UN UI CLS</cell><cell>SEG</cell></row><row><cell>ACC F 1</cell><cell cols="2">DI disc ASSD disc DIcup ASSDcup</cell></row><row><cell cols="2">0.765 0.810 0.835 1.454</cell><cell>0.841 1.423</cell></row><row><cell cols="2">0.813 0.845 0.836 1.333</cell><cell>0.826 1.525</cell></row><row><cell cols="2">0.828 0.856 0.786 1.853</cell><cell>0.823 1.593</cell></row><row><cell cols="2">0.853 0.875 0.855 1.560</cell><cell>0.858 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>.251 Comparison with Single-and Multi-task Methods.</head><label></label><figDesc>As shown in Table1, we report the performance on the two datasets of the proposed UML and other methods. By comparison, we can observe the fact that the accuracy of the model results is low if either classification or segmentation is done in isolation, the ACC has only just broken 0.5 in EC. But joint classification and segmentation changes this situation, the performance of BCS and DSI improves considerably, especially the ACC and the Dice score of optic cup. Excitingly, our UML not only achieves the best classification performance in ACC (85.3%) and F1 (0.875) with significant increments of 1.8%, 4.9%, but also obtains the superior segmentation performance with increments of 6.6% in DI disc and 3.2% in DI cup . A similar improvement can be observed in the experimental results in ISPY-1.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our code has been released in https://github.com/KarryRen/UML.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. This work was supported by the <rs type="funder">National Research Foundation, Singapore</rs> under its <rs type="programName">AI Singapore Programme (AISG Award</rs> No: <rs type="grantNumber">AISG2-TC-2021-003</rs>), A*<rs type="programName">STAR AME Programmatic Funding Scheme Under Project A20H4b0141</rs>, <rs type="funder">A*STAR Central Research Fund</rs>, the <rs type="funder">Science and Technology Department of Sichuan Province</rs> (Grant No. <rs type="grantNumber">2022YFS0071 &amp; 2023YFG0273</rs>), and the <rs type="funder">China Scholarship Council</rs> (No. <rs type="grantNumber">202206240082</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GheuXFu">
					<idno type="grant-number">AISG2-TC-2021-003</idno>
					<orgName type="program" subtype="full">AI Singapore Programme (AISG Award</orgName>
				</org>
				<org type="funding" xml:id="_vdwBnVZ">
					<orgName type="program" subtype="full">STAR AME Programmatic Funding Scheme Under Project A20H4b0141</orgName>
				</org>
				<org type="funding" xml:id="_4KHVfHJ">
					<idno type="grant-number">2022YFS0071 &amp; 2023YFG0273</idno>
				</org>
				<org type="funding" xml:id="_cv5vtnV">
					<idno type="grant-number">202206240082</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 4.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uncertainty quantification in skin cancer classification using three-way decision-based Bayesian deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">TransUNet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Uncertainty-aware distillation for semisupervised few-shot class-incremental learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.09964</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02051</idno>
		<title level="m">Trusted multi-view classification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Universal multi-modal deep network for classification and segmentation of medical images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Harouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Negahdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beymer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="872" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Jsang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-42337-1</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-42337-1" />
		<title level="m">Subjective Logic: A Formalism for Reasoning Under Uncertainty</title>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Thyroid nodule segmentation and classification in ultrasound images through intra-and inter-task consistent learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102443</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">UACANet: uncertainty augmented context attention for polyp segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2167" to="2175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A probabilistic u-net for segmentation of ambiguous images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ynet: joint segmentation and classification for diagnosis of breast biopsy images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mercan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00934-2_99</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00934-299" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018, Part II</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11071</biblScope>
			<biblScope unit="page" from="893" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-center breast DCE-MRI data and segmentations from patients in the I-SPY 1/ACRIN 6657 trials</title>
		<author>
			<persName><forename type="first">D</forename><surname>Newitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hylton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Imaging Arch</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Refuge challenge: a unified framework for evaluating automated methods for glaucoma assessment from fundus photographs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Orlando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101570</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evidential deep learning to quantify classification uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sensoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evidential deep learning to quantify classification uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sensoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3183" to="3193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Understanding measures of uncertainty for adversarial example detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08533</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interpretable deep learning systems for multi-class segmentation and classification of non-melanoma skin cancer</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">101915</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Uncertainty estimation using a single deep deterministic neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9690" to="9700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Information bottleneck-based interpretable multitask network for breast cancer classification and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">102687</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Uncertainty-inspired open set learning for retinal anomaly identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03981</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint learning of 3D lesion segmentation and classification for explainable COVID-19 diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2463" to="2476" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Tey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01025</idno>
		<title level="m">A novel multi-task deep learning model for skin lesion segmentation and classification</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PreyNet: preying on camouflaged objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-task learning for segmentation and classification of tumors in 3d automated breast ultrasound images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">101918</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DSI-Net: deep synergistic interaction network for joint classification and segmentation with endoscope images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3315" to="3325" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An interactive dual-branch network for hard palate segmentation of the oral cavity from CBCT images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page">109549</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00349</idno>
		<title level="m">EvidenceCap: towards trustworthy medical image segmentation via evidential identity cap</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TBraTS: Trusted brain tumor segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-148" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VIII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="503" to="513" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
