<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network</title>
				<funder ref="#_QVWd8jh">
					<orgName type="full">National Institute Of General Medical Sciences of the National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bryar</forename><surname>Shareef</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Idaho</orgName>
								<address>
									<postCode>83402</postCode>
									<settlement>Idaho Falls</settlement>
									<region>ID</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Min</forename><surname>Xian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Idaho</orgName>
								<address>
									<postCode>83402</postCode>
									<settlement>Idaho Falls</settlement>
									<region>ID</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aleksandar</forename><surname>Vakanski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Idaho</orgName>
								<address>
									<postCode>83402</postCode>
									<settlement>Idaho Falls</settlement>
									<region>ID</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haotian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Idaho</orgName>
								<address>
									<postCode>83402</postCode>
									<settlement>Idaho Falls</settlement>
									<region>ID</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="344" to="353"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">3FF97C97B7B81A60378A8C3507685135</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_33</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Breast Ultrasound</term>
					<term>Classification</term>
					<term>Multitask Learning</term>
					<term>Hybrid CNN-Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Capturing global contextual information plays a critical role in breast ultrasound (BUS) image classification. Although convolutional neural networks (CNNs) have demonstrated reliable performance in tumor classification, they have inherent limitations for modeling global and long-range dependencies due to the localized nature of convolution operations. Vision Transformers have an improved capability of capturing global contextual information but may distort the local image patterns due to the tokenization operations. In this study, we proposed a hybrid multitask deep neural network called Hybrid-MT-ESTAN, designed to perform BUS tumor classification and segmentation using a hybrid architecture composed of CNNs and Swin Transformer components. The proposed approach was compared to nine BUS classification methods and evaluated using seven quantitative metrics on a dataset of 3,320 BUS images. The results indicate that Hybrid-MT-ESTAN achieved the highest accuracy, sensitivity, and F1 score of 82.7%, 86.4%, and 86.0%, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Breast cancer is the leading cause of cancer-related fatalities among women. Currently, it holds the highest incidence rate of cancer among women in the U.S., and in 2022 it accounted for 31% of all newly diagnosed cancer cases <ref type="bibr" target="#b0">[1]</ref>. Due to the high incidence rate, early breast cancer detection is essential for reducing mortality rates and expanding treatment options. BUS imaging is an effective screening option because it is cost-effective, nonradioactive, and noninvasive. However, BUS image analysis is also challenging due to the large variations in tumor shape and appearance, speckle noise, low contrast, weak boundaries, and occurrence of artifacts.</p><p>In the past decade, deep learning-based approaches achieved remarkable advancements in BUS tumor classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. The progress has been driven by the capability of CNN-based models to learn hierarchies of structured image representations as semantics. To extract deep context features, CNNs apply a series of convolutional and downsampling layers, frequently organized into blocks with residual connections. Nevertheless, one disadvantage of such architectural choice is that the feature representations in the deeper layers become increasingly abstract, leading to a loss of spatial and contextual information. The intrinsic locality of convolutional operations hinders the ability of CNNs to model longrange dependencies while preserving spatial information in images effectively.</p><p>Vision Transformer (ViT) <ref type="bibr" target="#b4">[5]</ref> and its variants recently demonstrated superior performance in image classification tasks. These models convert input images into smaller patches and utilize the self-attention mechanism to model the relationships between the patches. Self-attention enables ViTs to capture long-range dependencies and model complex relationships between different regions of the image. However, the effectiveness of ViT-based approaches heavily relies on access to large datasets for learning meaningful representations of input images. This is primarily because the architectural design of ViTs does not rely on the same inductive biases in feature extraction which allow CNNs to learn spatially invariant features.</p><p>Accordingly, numerous prior studies introduced modifications to the original ViT network specifically designed for BUS image classification <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>. In addition, several works proposed network architectures that combined Transformers and CNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. For instance, Mo et al. <ref type="bibr" target="#b14">[15]</ref> proposed a hybrid CNN-Transformer incorporating BUS anatomical priors. Qu et al. <ref type="bibr" target="#b15">[16]</ref> employed squeeze and excitation blocks to enhance the feature extraction capacity in a hybrid CNN-based VGG16 network and ViT. Similarly, Iqbal et al. <ref type="bibr" target="#b3">[4]</ref> designed two hybrid CNN-Transformer networks intended either for classification or segmentation of multi-modal breast cancer images. Despite the promising results of such hybrid approaches, effectively capturing the local patterns and global long-range dependencies in BUS images remains challenging <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Multitask learning leverages shared information across related tasks by jointly training the model. It constrains models to learn representations that are relevant to all tasks rather than learning task-specific details. Moreover, multitask learning acts as a regularizer by introducing inductive bias and prevents overfitting <ref type="bibr" target="#b24">[25]</ref> (particularly with ViTs), and with that, can mitigate the challenges posed by small BUS dataset sizes. In <ref type="bibr" target="#b2">[3]</ref>, the authors demonstrated that multitask learning outperforms single-task learning approaches for BUS classification.</p><p>In this study, we introduce a hybrid multitask approach, Hybrid-MT-ESTAN, which encompasses tumor classification as a primary task and tumor segmentation as a secondary task. Hybrid-MT-ESTAN combines the advantages of CNNs and Transformers in a framework incorporating anatomical tissue information in BUS images. Specifically, we designed a novel attention block named Anatomy-Aware Attention (AAA), which modifies the attention block of Swin Transformer by considering the breast anatomy. The anatomy of the human breast is categorized into four primary layers: the skin, premammary (subcutaneous fat), mammary, and retromammary layers, where each layer has a distinct texture and generates different echo patterns. The primary layers in BUS images are arranged in a vertical stack, with similar echo patterns appearing horizontally across the images. The kernels in the introduced AAA attention blocks are organized in rows and columns to capture the anatomical structure of the breast tissue. In the published literature, the closest approach to ours is the work by Iqbal et al. <ref type="bibr" target="#b3">[4]</ref>, in which the authors used hybrid single-task CNN-Transformer networks for either classification or segmentation of BUS images. Conversely, Hybrid-MT-ESTAN employs a multitask approach and introduces novel architectural design. The main contributions of this work are summarized as:   <ref type="bibr" target="#b16">[17]</ref>, which employs row-column-wise kernels to learn and fuse context information in BUS images at different context scales (see Fig. <ref type="figure" target="#fig_2">2</ref>). Specifically, each MT-ESTAN block is composed of two parallel branches consisting of four square convolutional kernels and two consecutive row-column-wise kernels. These specialized convolutional kernels effectively extract contextual information of small tumors in BUS images. Refer to <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>, and <ref type="bibr" target="#b2">[3]</ref> for the implementation details of ESTAN and MT-ESTAN. The source codes of these works are available at http://busbench.midalab.net. f l = W-MSA(LN(f l-1 )) + f l-1 (1)</p><formula xml:id="formula_0">f l = MLP(LN( f l )) + f l (2) f l+1 = SW-MSA(LN(f l )) + f l (3) f l+1 = MLP(LN( f l+1 )) + f l+1 (4)</formula><p>where f l and f l are the output features of the MLP module and the (S)W-MSA module for block l, respectively; in the proposed Anatomy-Aware Attention (AAA) block, we redesigned the Swin blocks to enhance their ability to model both global and local features by adding an attention block based on the breast anatomy (see Fig. <ref type="figure" target="#fig_3">3</ref>). The additional layers are defined by</p><formula xml:id="formula_1">y i = M (f l+1 ) (<label>5</label></formula><formula xml:id="formula_2">)</formula><formula xml:id="formula_3">B i = U (MAX-P(y i ) + AVG-P(y i ))<label>(6)</label></formula><formula xml:id="formula_4">O i = y i • (σ(A(B)))<label>(7)</label></formula><p>Concretely, we first reconstruct the i-th feature map (y i ) by merging (M ) all patches, and afterward, we applied average pooling (AVG-P) and max pooling (MAX-P) layers with size (2, 2). The outputs of (AVG-P) and (MAX-P) layers are concatenated and up-sampled (U ) with size (2, 2) and stride (2, 2). Rowcolumn-wise kernels (A) with size (9 , 1) and <ref type="bibr">(1 , 9)</ref> are then employed to adapt to the anatomy of the breast, and finally a sigmoid function (σ) is applied to the output of (A) multiplied by the input feature map (y i ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Segmentation and Classification Branches/Tasks</head><p>The segmentation branch in Fig. <ref type="figure" target="#fig_1">1</ref> outputs dense mask predictions of BUS tumors. It consists of four Up Blocks, each with three convolutional layers and one upsampling layer (with size (2, 2) and stride (2, 2)). The settings of the convolutional layers are adopted from <ref type="bibr" target="#b2">[3]</ref>. In addition, the blocks receive four skip connections from the MT-ESTAN encoder, i.e., there is a skip connection from each MT-ESTAN block 1 to 4. The classification branch consists of three dense layers, a dropout layer (50%), and the final dense layer that predicts the tumor class into benign or malignant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Loss Function</head><p>We applied a multitask loss function (L mt ) that aggregates two terms: a focal loss L F ocal for the classification task and dice loss L Dice for the segmentation task. Therefore, the composite loss function is L mt = w 1 • L F ocal + L Dice , where the weight coefficient w 1 is set to apply greater importance to the classification task as the primary task. Since in medical image diagnosis achieving high sensitivity places emphasis on the detection of malignant lesions, we employed the focal loss for the classification task to trade off between sensitivity and specificity. Because malignant tumors are more challenging to detect due to greater differences in margin, shape, and appearance in BUS images, focal loss forces the model to focus more on difficult predictions. Specifically, focal loss adds a factor (1 -p i ) γ to the cross-entropy loss where γ is a focusing parameter, resulting in</p><formula xml:id="formula_5">L F ocal = -1/N N i=1 [(α•t i •(1-p i ) γ •log(p i )+(1-α)•p i •log(1-p i )].</formula><p>In the formulation, α is a weighting coefficient, N denotes the number of image samples, t i is the target label of the i th training sample, and p i denotes the prediction. The segmentation loss is calculated using the commonly-employed Dice loss (L Dice ) function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We evaluated the performance of Hybrid-MT-ESTAN using four public datasets, HMSS <ref type="bibr" target="#b8">[9]</ref>, BUSI <ref type="bibr" target="#b9">[10]</ref>, BUSIS <ref type="bibr" target="#b19">[20]</ref>, and Dataset B <ref type="bibr" target="#b5">[6]</ref>. We combined all four datasets to build a large and diverse dataset with a total of 3,320 B-mode BUS images, of which 1,664 contain benign tumors and 1,656 have malignant tumors. Table <ref type="table" target="#tab_0">1</ref> shows the detailed information for each dataset. HMSS dataset does not provide the segmentation ground-truth masks, and for this study we arranged with a group of experienced radiologists to prepare the masks for HMSS. Refer to the original publications of the datasets for more details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>For performance evaluation of the classification task, we used the following metrics: accuracy (Acc), sensitivity (Sens), specificity (Spec), F1 score, Area Under the Curve of Receiver Operating Characteristic (AUC), false positive rate (FPR), and false negative rate (FNR). To evaluate the segmentation performance, we used dice similarity coefficient (DSC) and Jaccard index (JI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>The proposed approach was implemented with Keras and TensorFlow libraries.</p><p>All experiments were performed on a machine with NVIDIA Quadro RTX 8000 GPUs and two Intel Xeon Silver 4210R CPUs (2.40GHz) with 512 GB of RAM. All BUS images in the dataset were zero-padded and reshaped to form square images. To avoid data leakage and bias, we selected the train, test, and validation sets based on the cases, i.e., the images from one case (patient) were assigned to only one of the training, validation, and test sets. Furthermore, we employed horizontal flip, height shift (20%), width shift (20%), and rotation (20 • C) for data augmentation. The proposed approach utilizes the building blocks of ResNet50 and Swin-Transformer-V2, pretrained on ImageNet dataset. Namely, MT-ESTAN uses pretrained ResNet50 as a base model for the five encoder blocks (the implementation details of MT-ESTAN can be found in <ref type="bibr" target="#b2">[3]</ref>). The encoder with AAA blocks uses the SwinTransformer V2 Base 256 pretrained model as a backbone. For the composite loss function, we adopted a weight coefficient w 1 = 3, and in the focal loss α = 0.5 and γ = 2. For model training we utilized Adam optimizer with a learning rate of 10 -5 and mini batch size of 4 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Performance Evaluation and Comparative Analysis</head><p>We compared the performance of Hybrid-MT-ESTAN for BUS classification to nine deep learning approaches commonly used for medical image analysis. The compared models include CNN-based, ViT-based, and hybrid approaches. CNNbased networks are SHA-MTL <ref type="bibr" target="#b7">[8]</ref>, MobileNet <ref type="bibr" target="#b18">[19]</ref>, DenseNet121 <ref type="bibr" target="#b6">[7]</ref>, and EMT-Net <ref type="bibr" target="#b11">[12]</ref>. ViT-based approaches include the original ViT <ref type="bibr" target="#b4">[5]</ref>, Chowdery <ref type="bibr" target="#b9">[10]</ref>, and Swin Transformer <ref type="bibr" target="#b17">[18]</ref>. VGGA-ViT <ref type="bibr" target="#b15">[16]</ref> is a hybrid CNN-Transformer network. The values of the performance metrics are shown in Table <ref type="table" target="#tab_1">2</ref>, indicating that the proposed Hybrid-MT-ESTAN outperformed all nine approaches by achieving the best accuracy, sensitivity, F1 score, and AUC with 82.8%, 86.4%, 86.0%, and 82.8%, respectively. Although SHA-MTL <ref type="bibr" target="#b7">[8]</ref> obtained the highest specificity of 90.8% and FNR of 9.2%, the trade-off between sensitivity and specificity should be taken into consideration, as that approach had sensitivity of 48.1%. The preferred trade-off in medical image analysis typically is high sensitivity without significant degradation in specificity.</p><p>We evaluated the segmentation performance of Hybrid MT-ESTAN and compared the results to five multitask approaches, including SHA-MTL <ref type="bibr" target="#b7">[8]</ref>, EMT-Net <ref type="bibr" target="#b11">[12]</ref>, Chowdery <ref type="bibr" target="#b9">[10]</ref>, MT-ESTAN <ref type="bibr" target="#b2">[3]</ref>, and VGGA-ViT <ref type="bibr" target="#b15">[16]</ref>. As shown in Table <ref type="table" target="#tab_1">2</ref>,the proposed Hybrid MT-ESTAN achieved the highest performance and increased DSC and JI by 5.9% and 6.4%, respectively compared to MT-ESTAN. Note that results of single-task models in Table <ref type="table" target="#tab_1">2</ref> are not provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Effectiveness of the Anatomy-Aware Attention (AAA) Block</head><p>To verify the effectiveness of the Anatomy-Aware Attention (AAA) block, we conducted an ablation study that quantified the impact of the different components in Hybrid-MT-ESTAN on the classification and segmentation performance. Table <ref type="table" target="#tab_2">3</ref> presents the values of the performance metrics for MT-ESTAN (pure CNN-based approach), Swin Transformer (pure Transformer network), a hybrid architecture of MT-ESTAN and Swin Transformer, and our proposed Hybrid-MT-ESTAN with AAA block. According to the results in Table <ref type="table" target="#tab_2">3</ref>, MT-ESTAN achieved better sensitivity and F1 score than Swin Transformer, with 83.7% and 83%, respectively. The hybrid architectures of MT-ESTAN with Swin Transformer improved the classification performance and has higher accuracy, sensitivity, F1 score, and AUC with 80.3%, 84.2%, 83%, and 80.2%, compared to MT-ESTAN and Swin Transformer individually. The proposed approach, Hybrid-MT-ESTAN with AAA block, further improved accuracy, sensitivity, F1 score, and AUC by 2.5%, 2.2%, 3%, and 2.6%, respectively, relative to the hybrid model without the AAA block.</p><p>To evaluate the segmentation performance, we compared the proposed approach with and without the AAA block and Swin Transformer. As shown in Table <ref type="table" target="#tab_2">3</ref>, MT-ESTAN combined with Swin Transformer improved DSC and JI by 4.1% and 4.3%, respectively compared to MT-ESTAN. Employing the proposed AAA block further improved DSC and JI by 1.8% and 2.1%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we introduced the Hybrid-MT-ESTAN, a multitask learning approach for BUS image analysis that alleviates the lack of global contextual infor-mation in the low-level layers of CNN-based approaches. Hybrid-MT-ESTAN concurrently performs BUS tumor classification and segmentation, with a hybrid architecture that employs CNN-based and Swin Transformer layers. The proposed approach exploits multi-scale local patterns and global long-range dependencies provided by MT-ESTA and AAA Transformer blocks for learning feature representations, resulting in improved generalization. Experimental validation demonstrated significant performance improvement by Hybrid-MT-ESTAN in comparison to current state-of-the-art models for BUS classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>The proposed architecture effectively integrates the advantages of CNNs for extracting hierarchical and local patterns in BUS images and Swin Transformers for leveraging long-range dependencies.• The designed Anatomy-Aware Attention (AAA) block improves the learning of contextual information based on the anatomy of the breast. • The multitask learning approach leverages the shared representations across the classification and segmentation tasks to improve the model performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Hybrid-MT-ESTAN consists of MT-ESTAN and AAA encoders, a segmentation branch, and a classification branch.2 Proposed Method2.1 Hybrid-MT-ESTANThe architecture of Hybrid-MT-ESTAN is shown in Fig.1, and consists of: (1) the MT-ESTAN encoder<ref type="bibr" target="#b2">[3]</ref>, and a Swin Transformer-based encoder with Anatomy-Aware Attention (AAA) blocks, (2) a decoder branch for the segmentation task, and (3) a branch with fully-connected layers for the classification task. MT-ESTAN<ref type="bibr" target="#b2">[3]</ref> is a CNN-based multitask learning network that simultaneously performs BUS classification and segmentation. The encoder sub-network of MT-ESTAN is ESTAN<ref type="bibr" target="#b16">[17]</ref>, which employs row-column-wise kernels to learn and fuse context information in BUS images at different context scales (see Fig.2). Specifically, each MT-ESTAN block is composed of two parallel branches consisting of four square convolutional kernels and two consecutive row-column-wise kernels. These specialized convolutional kernels effectively extract contextual information of small tumors in BUS images. Refer to<ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>, and<ref type="bibr" target="#b2">[3]</ref> for the implementation details of ESTAN and MT-ESTAN. The source codes of these works are available at http://busbench.midalab.net.</figDesc><graphic coords="3,49,29,225,62,325,39,126,49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. MT-ESTAN blocks include parallel convolutional branches with different kernel size, followed by 1 × 1 convolution and a pooling layer.</figDesc><graphic coords="4,182,97,53,87,86,50,89,83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Anatomy-Aware Attention (AAA) block.</figDesc><graphic coords="5,51,30,137,60,321,52,121,06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Breast ultrasound (BUS) datasets. 'b' denotes benign tumor and 'm' is malignant tumor.</figDesc><table><row><cell cols="3">BUS dataset No. of images Distribution</cell><cell>Source</cell></row><row><cell>HMSS</cell><cell>1,948</cell><cell>b:812, m:1136</cell><cell>Netherlands</cell></row><row><cell>BUSI</cell><cell>647</cell><cell>b:437, m:210</cell><cell>Egypt</cell></row><row><cell>BUSIS</cell><cell>562</cell><cell>b:306, m:256</cell><cell>China</cell></row><row><cell>Dataset B</cell><cell>163</cell><cell>b:109, m:54</cell><cell>Spain</cell></row><row><cell>Total</cell><cell>3,320</cell><cell>b: 1,664, m: 1,656</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance metrics of the compared methods for BUS image classification and segmentation. Note: A dash '-' in the Segmentation column indicates that the model uses single-task learning.</figDesc><table><row><cell></cell><cell>Classification</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Segmentation</cell></row><row><cell>Methods</cell><cell cols="6">Acc↑ Sens.↑ Spec.↑ F1↑ Auc↑ FNR↓ FPR↓ DSC↑ JI↑</cell></row><row><cell>SHA-MTL [8]</cell><cell>69.6 48.1</cell><cell cols="2">90.8 0.58 69.5 51.9</cell><cell>9.2</cell><cell cols="2">72.2 60.7</cell></row><row><cell>MobileNet [19]</cell><cell>71.0 82.0</cell><cell>61.0</cell><cell>0.74 71.5 18.0</cell><cell>39.0</cell><cell>-</cell><cell>-</cell></row><row><cell>VGGA-ViT [16]</cell><cell>73.6 61.8</cell><cell>79.8</cell><cell>0.61 70.8 38.2</cell><cell>20.2</cell><cell cols="2">74.9 64.9</cell></row><row><cell cols="2">DenseNet121 [7] 73.0 74.0</cell><cell>71.0</cell><cell>0.73 72.5 26.0</cell><cell>29.0</cell><cell>-</cell><cell>-</cell></row><row><cell>EMT-Net [12]</cell><cell>74.1 79.4</cell><cell>69.1</cell><cell>0.75 74.3 20.6</cell><cell>30.9</cell><cell cols="2">76.7 67.0</cell></row><row><cell>ViT [5]</cell><cell>72.1 74.1</cell><cell>69.3</cell><cell>0.73 71.7 25.9</cell><cell>30.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Chowdery [10]</cell><cell>77.4 77.3</cell><cell>77.3</cell><cell>0.77 77.3 22.7</cell><cell>22.7</cell><cell cols="2">77.0 67.9</cell></row><row><cell cols="2">Swin Transformer 77.4 72.6</cell><cell>82.5</cell><cell>0.74 77.6 27.4</cell><cell>17.5</cell><cell>-</cell><cell>-</cell></row><row><cell>MT-ESTAN</cell><cell>78.6 83.7</cell><cell>72.6</cell><cell>0.83 78.2 16.3</cell><cell>27.4</cell><cell cols="2">78.2 69.3</cell></row><row><cell>Ours</cell><cell cols="2">82.8 86.4 79.2</cell><cell cols="2">0.86 82.8 13.6 20.8</cell><cell cols="2">84.1 75.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study for evaluating the components of Hybrid-MT-ESTAN.</figDesc><table><row><cell></cell><cell>Classification</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Segmentation</cell></row><row><cell>Methods</cell><cell cols="6">Acc↑ Sens.↑ Spec.↑ F1↑ Auc↑ FNR↓ FPR↓ DSC↑ JI↑</cell></row><row><cell>MT-ESTAN [10]</cell><cell>78.6 83.7</cell><cell>72.6</cell><cell>0.83 78.2 16.3</cell><cell>27.4</cell><cell cols="2">78.2 69.3</cell></row><row><cell>Swin Trans.</cell><cell>77.4 72.6</cell><cell cols="2">82.5 0.74 77.6 27.4</cell><cell>17.5</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">MT-ESTAN + Swin Trans. 80.3 84.2</cell><cell>76.3</cell><cell>0.83 80.2 15.8</cell><cell>23.7</cell><cell cols="2">82.3 73.6</cell></row><row><cell>Ours</cell><cell cols="2">82.8 86.4 79.2</cell><cell cols="2">0.86 82.8 13.6 20.8</cell><cell cols="2">84.1 75.7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. Research reported in this publication was supported by the <rs type="funder">National Institute Of General Medical Sciences of the National Institutes of Health</rs> under Award Number <rs type="grantNumber">P20GM104420</rs>. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QVWd8jh">
					<idno type="grant-number">P20GM104420</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://www.cancer.org" />
		<title level="m">Cancer Facts &amp; Figures</title>
		<imprint>
			<publisher>American Cancer Society</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Breast ultrasound tumor image classification using image decomposition and fusion based on adaptive multi-model spatial feature fusion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="page">106221</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A benchmark for breast ultrasound image classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shareef</surname></persName>
		</author>
		<ptr target="https://ssrn.com/abstract=4339660" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Available at SSRN</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BTS-ST: Swin transformer network for segmentation and classification of multimodality breast cancer images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">262</biblScope>
			<biblScope unit="page">110393</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated breast ultrasound lesions detection using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1218" to="1226" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mateen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SHA-MTL: soft and hard attention multi-task learning for automated breast cancer ultrasound image segmentation and classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-021-02445-7</idno>
		<ptr target="https://doi.org/10.1007/s11548-021-02445-7" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assisted Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1719" to="1725" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ultrasound cases</title>
		<author>
			<persName><forename type="first">T</forename><surname>Geertsma</surname></persName>
		</author>
		<author>
			<persName><surname>Fujifilm</surname></persName>
		</author>
		<ptr target="https://www.ultrasoundcases.info/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multi-task learning framework for automated segmentation and classification of breast tumors from ultrasound images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chowdary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yogarajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Guruviah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrason. Imaging</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="12" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluation of complexity measures for deep learning generalization in medical image analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vakanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 31st Int. Workshop on MLSP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">EMT-NET: efficient multitask network for computer-aided diagnosis of breast cancer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vakanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vision transformers for classification of breast ultrasound images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gheflati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rivaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 44th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="480" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting breast tumor malignancy using deep ConvNeXt radiomics and quality-based score pooling in ultrasound sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hassanien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdel-Nasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagnostics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1053</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hover-trans: anatomy-aware hover-transformer for ROI-Free breast cancer diagnosis in ultrasound images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1696" to="1706" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A VGG attention vision transformer network for benign and malignant classification of breast ultrasound images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5787" to="5798" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ESTAN: enhanced small tumoraware network for breast ultrasound image segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shareef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vakanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Freer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Healthcare</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2262</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BUSIS: a benchmark for breast ultrasound image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Healthcare</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">729</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Breast ultrasound lesions recognition: end-to-end deep learning approaches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yap</surname></persName>
		</author>
		<idno>011007. SPIE</idno>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stan: small tumor-aware network for breast ultrasound image segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shareef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vakanski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BUViTNet: breast ultrasound detection via vision transformers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choe</surname></persName>
		</author>
		<idno type="DOI">10.3390/diagnostics12112654</idno>
		<ptr target="https://doi.org/10.3390/diagnostics12112654" />
	</analytic>
	<monogr>
		<title level="j">Diagnostics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2654</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transformer-based multi-task learning for classification and segmentation of gastrointestinal tract endoscopic images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Bio. Med</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page">106723</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sebastian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
