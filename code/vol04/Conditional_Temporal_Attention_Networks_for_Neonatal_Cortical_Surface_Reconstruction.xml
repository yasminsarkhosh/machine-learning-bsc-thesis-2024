<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction</title>
				<funder ref="#_h7MNPMN #_YCtZJPU">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder>
					<orgName type="full">European Union Seventh Framework Programme</orgName>
				</funder>
				<funder>
					<orgName type="full">President</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution" key="instit1">BioMedIA</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution" key="instit1">BioMedIA</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vanessa</forename><surname>Kyriakopoulou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emma</forename><forename type="middle">C</forename><surname>Robinson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernhard</forename><surname>Kainz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution" key="instit1">BioMedIA</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">FAU Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution" key="instit1">BioMedIA</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Klinikum rechts der Isar</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="312" to="322"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">5B3DE0C3101B483C2FA41C2D96803FD4</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_30</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cortical surface reconstruction plays a fundamental role in modeling the rapid brain development during the perinatal period. In this work, we propose Conditional Temporal Attention Network (CoTAN), a fast end-to-end framework for diffeomorphic neonatal cortical surface reconstruction. CoTAN predicts multi-resolution stationary velocity fields (SVF) from neonatal brain magnetic resonance images (MRI). Instead of integrating multiple SVFs, CoTAN introduces attention mechanisms to learn a conditional time-varying velocity field (CTVF) by computing the weighted sum of all SVFs at each integration step. The importance of each SVF, which is estimated by learned attention maps, is conditioned on the age of the neonates and varies with the time step of integration. The proposed CTVF defines a diffeomorphic surface deformation, which reduces mesh self-intersection errors effectively. It only requires 0.21 s to deform an initial template mesh to cortical white matter and pial surfaces for each brain hemisphere. CoTAN is validated on the Developing Human Connectome Project (dHCP) dataset with 877 3D brain MR images acquired from preterm and term born neonates. Compared to state-of-the-art baselines, CoTAN achieves superior performance with only 0.12 ± 0.03 mm geometric error and 0.07 ± 0.03% selfintersecting faces. The visualization of our attention maps illustrates that CoTAN indeed learns coarse-to-fine surface deformations automatically without intermediate supervision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cortical surface reconstruction aims to extract 3D meshes of inner (white matter) and outer (pial) surfaces of the cerebral cortex from brain magnetic resonance images (MRI). These surfaces provide both 3D visualization and estimation of morphological features for the cortex <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. In addition to accurately representing the highly folded cortex, the cortical surfaces of each hemisphere are   <ref type="bibr" target="#b19">[20]</ref>. Right: neonatal brain MRI from the dHCP dataset <ref type="bibr" target="#b8">[9]</ref>.</p><p>required to be closed manifolds and topologically homeomorphic to a sphere <ref type="bibr" target="#b6">[7]</ref>. Traditional neuroimage analysis pipelines <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b29">29]</ref> such as FreeSurfer <ref type="bibr" target="#b9">[10]</ref> comprise a series of processing steps to extract cortical surfaces from brain MRI. These pipelines provide an invaluable service to the research community. However, the current implementations have limited accuracy and require several hours to process a single MRI scan.</p><p>With the recent advances of geometric deep leaning <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33]</ref>, a growing number of fast learning-based approaches have been proposed to learn either implicit surface representation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> or explicit mesh deformation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b28">28]</ref> for cortical surface reconstruction. These approaches enhance the accuracy and reduce the processing time to a few seconds for a single subject. Recent studies focus on generating manifold cortical meshes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">28]</ref> and preventing mesh self-intersections by learning diffeomorphic deformations, which have been widely adopted in medical image registration <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. The basic idea is to learn stationary velocity fields (SVF) to deform an initial mesh template to target surfaces. Since a single SVF has limited representation capacity, several approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b30">30]</ref> have proposed to train multiple neural networks to predict a sequence of SVFs for coarse-to-fine surface deformation. This improves the geometric accuracy but increases the computational burden for both training and inference.</p><p>Cortical surface reconstruction plays an essential role in modeling and quantifying the brain development in fetal and neonatal neuroimaging studies such as the Developing Human Connectome Project (dHCP) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">24]</ref>. However, most learning-based approaches so far rely on adult MRI as training data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">25]</ref>. Compared to adult data, neonatal brain MR images have lower resolution and contrast due to a smaller region of interest and the use of, e.g., fast imaging sequences with sparse acquisition to minimize head motion artifacts of unsedated infants <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>. Besides, the rapid growth and continuously increasing complexity of the cortex during the perinatal period lead to considerable variation in shape and scale between neonatal cortical surfaces at different post-menstrual ages (PMA) (see Fig. <ref type="figure" target="#fig_0">1</ref>). Moreover, since the neonatal head is still small, the cortical sulci of term-born neonates are much narrower than adults as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Hence, the neonatal pial surfaces are prone to be affected by partial volume effects and more likely to produce surface self-intersections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution.</head><p>In this work, we present Conditional Temporal Attention Network (CoTAN). CoTAN adopts attention mechanism <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">27]</ref> to learn a conditional time-varying velocity field (CTVF) for neonatal cortical surface reconstruction. Given an input brain MR image, CoTAN first predicts multiple SVFs at different resolutions. Rather than integrating all SVFs as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">28]</ref>, CoTAN learns conditional temporal attention maps to attend to specific SVFs for different time steps of integration and PMA of neonates. The CTVF is represented by the weighted sum of learned SVFs, and thus a single CoTAN model is sufficient to model the large deformation and variation of neonatal cortical surfaces. The evaluation on the dHCP neonatal dataset <ref type="bibr" target="#b8">[9]</ref> verifies that CoTAN performs better in geometric accuracy, mesh quality and computational efficiency than state-of-theart methods. The visualization of attention maps indicates that CoTAN learns coarse-to-fine deformations automatically without intermediate constraints. Our source code is released publicly at https://github.com/m-qiang/CoTAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Diffeomorphic Surface Deformation. We define the diffeomorphic surface deformation φ t : R 3 × R → R 3 as a flow ordinary differential equation (ODE) following previous work <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">28]</ref>:</p><formula xml:id="formula_0">∂ ∂t φ t = v t (φ t ), φ 0 = Id, t ∈ [0, T ],<label>(1)</label></formula><p>where v t is a time-varying velocity field (TVF) and Id is the identity mapping. Given an initial surface S 0 ⊂ R 3 with points x 0 ∈ S 0 , we define x t := φ t (x 0 ) as the trajectories of the points on the deformable surface S t = φ t (S 0 ) for t ∈ [0, T ].</p><p>Then the flow equation (1) can be rewritten as d dt x t = v t (x t ) with initial value x 0 . By the existence and uniqueness theorem for ODE solutions <ref type="bibr" target="#b15">[16]</ref>, if v t (x) is Lipschitz continuous with respect to x, the trajectories x t will not intersect with each other, so that the surface self-intersections can be prevented effectively. By integrating the ODE (1), we obtain a diffeomorphism φ T that deforms S 0 to a manifold surface S T , on which the points are</p><formula xml:id="formula_1">x T = φ T (x 0 ) = x 0 + T 0 v t (x t )dt.</formula><p>Conditional Temporal Attention Network (CoTAN). An overview of the CoTAN architecture is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. CoTAN first predicts multiple SVFs given a 3D brain MRI volume. A 3D U-Net <ref type="bibr" target="#b26">[26]</ref> is used to extract feature maps with R resolution levels, each of which scales the input size by the factor of 2 r-R for r = 1, ..., R. Then we upsample the multi-scale feature maps and learn M volumetric SVFs for each resolution. Let V denote all R × M discrete SVFs. The continuous multi-resolution SVFs v : R 3 → R R×M ×3 can be obtained by v(x) = Lerp(x, V), where Lerp(•) is the trilinear interpolation function. Each element v r,m : R 3 → R 3 is an SVF for r = 1, ..., R and m = 1, ..., M . Note that v(x) is Lipschitz continuous since Lerp(•) is continuous and piece-wise linear.</p><p>CoTAN adopts a channel-wise attention mechanism <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">27]</ref> to focus on specific SVFs since it is time-consuming to integrate all R × M SVFs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">28]</ref>. The attention is conditioned on both integration time t ∈ [0, T ] and information about the subject. To model the high variation between infant brains, we consider the post-menstrual ages (PMA) a ∈ R of the neonates at scan time as the conditioning variable in this work. Note that we do not use a self-attention module <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">31]</ref> to learn key and query pairs. Instead, we learn a probability attention map to measure the importance of each SVF. More precisely, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>, we use a fully connected network (FCN) to encode the input time t and PMA a into a (R • M ) × 1 feature vector. After reshaping and softmax activation, the FCN learns conditional temporal attention maps p(t, a) ∈ R R×M which satisfy R r=1 M m=1 p r,m (t, a) = 1 for any t and a. Then a conditional time-varying velocity field (CTVF) is predicted by computing the weighted sum of all SVFs:</p><formula xml:id="formula_2">v t (x; a) = R r=1 M m=1 p r,m (t, a) • v r,m (x). (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>The CTVF is adaptive to the integration time and the age of subjects, which can handle the large deformation and variation for neonatal cortical surfaces. Such an attention mechanism encourages CoTAN to learn coarse-to-fine surface deformation by attending to SVFs at different resolutions.</p><p>To deform the initial surface S 0 to the target surface, we integrate the flow ODE (1) with the CTVF through the forward Euler method. For k = 0, ..., K -1, the surface points are updated by x k+1 = x k + hv k (x k ; a), where K is the total integration steps and h = T /K is the step size with T = 1. For each step k, we only need to recompute the attention maps p(hk, a) and update the CTVF v k (x k ; a) accordingly by Eq. ( <ref type="formula" target="#formula_2">2</ref>). CoTAN only integrates a single CTVF which saves considerable runtime compared to integrating multiple SVFs directly as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">28]</ref>. Neonatal Cortical Surface Reconstruction. We train two CoTAN models on the dHCP neonatal dataset <ref type="bibr" target="#b8">[9]</ref> to 1) deform an initial surface into a white matter surface and to 2) expand the white matter surface into a pial surface as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. We use the same initial surface (leftmost in Fig. <ref type="figure" target="#fig_3">4</ref>) for all subjects, which is created by iteratively applying Laplacian smoothing to a Conte-69 surface atlas <ref type="bibr" target="#b13">[14]</ref>. We generate pseudo ground truth (GT) surfaces by the dHCP structural neonatal pipeline <ref type="bibr" target="#b24">[24]</ref>, which has been fully validated through quality control performed by clinical experts.</p><p>For white matter surface reconstruction, we consider loss functions that have been widely used in previous work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33]</ref>: the Chamfer distance loss L cd computes the distance between two point clouds, the mesh Laplacian loss L lap regularizes the smoothness of the mesh, and the normal consistency loss L nc constrains the cosine similarity between the normals of two adjacent faces. The final loss is weighted by L = L cd + λ lap L lap + λ nc L nc . We train CoTAN in two steps for white matter surface extraction. First, we pre-train the model using relatively large weights λ lap and λ nc for regularization. The Chamfer distance is computed between the vertices of the predicted and pseudo-GT surfaces. These ensure that the initial surface can be deformed robustly during training. Then, we fine-tune CoTAN using small weights to increase geometric accuracy. The distances are computed between 150k uniformly sampled points on the surfaces.</p><p>For pial surface reconstruction, we follow <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> and use the pseudo-GT white matter surfaces as the input for training. Then the MSE loss L = i xix * i 2 can be computed between the vertices of predicted and pseudo-GT pial meshes. No point matching is required since the pseudo-GT white matter and pial surfaces have the same mesh connectivity. Therefore, the MSE loss provides stronger supervision than the Chamfer distance, while the latter is prone to mismatching the points in narrow sulci, resulting in mesh self-intersections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Implementation Details. We evaluate CoTAN on the third release of dHCP neonatal dataset <ref type="bibr" target="#b8">[9]</ref> (https://biomedia.github.io/dHCP-release-notes/), which includes 877 T2-weighted (T2w) brain MRI scanned from newborn infants at PMA between 27 to 45 weeks. The MRI images are affinely aligned to the MNI152 space and clipped to the size of 112 × 224 × 160 for each brain hemisphere. The dataset is split into 60/10/30% for training/validation/testing.</p><p>For the CoTAN model, we set the resolution R=3 and the number of SVFs M =4 for each resolution. For integration, we set the total number of steps to  K=50 with step size h=0.02. We re-mesh the initial mesh to 140k vertices, of which the coordinates are normalized to [-1, 1]. We use the Adam optimizer for training. For the white matter surface, we first pre-train CoTAN for 100 epochs using a learning rate of γ=10 -4 and weights λ lap =0.5, λ nc =5×10 -4 for the loss function. Then we fine-tune for 100 epochs using smaller weights λ lap =0.1 and λ nc =10 -4 with γ=2×10 -5 . For the pial surface, we set the maximum channel size of CoTAN as 32 to avoid overfitting and train for 200 epochs with γ=10 -4 . We only consider left brain hemisphere in the experiments. All experiments are conducted on a Nvidia RTX3080 GPU with 12GB memory.</p><p>Comparative Results. We compare the performance of CoTAN with existing learning-based cortical surface extraction approaches including Cortex-ODE <ref type="bibr" target="#b21">[22]</ref>, CorticalFlow++ (CFPP) <ref type="bibr" target="#b28">[28]</ref>, CorticalFlow <ref type="bibr" target="#b20">[21]</ref>, Vox2Cortex <ref type="bibr" target="#b3">[4]</ref> and DeepCSR <ref type="bibr" target="#b5">[6]</ref>. We employ the fast topology correction <ref type="bibr" target="#b21">[22]</ref> for DeepCSR. CoTAN can guarantee spherical topology and the Euler number is 2 for all predicted surfaces.</p><p>-Geometric Accuracy: We measure the geometric accuracy by commonly used metrics <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22]</ref>: average symmetric surface distance (ASSD) and 90th percentile of Hausdorff distance (HD90). The distances are computed between uniformly sampled 100k points on the predicted and pseudo-GT surfaces. For fair com- parison, the predicted cortical meshes have around 140k vertices for all baseline approaches. Note that CoTAN and <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">28]</ref> can generalize on high-resolution meshes with up to 600k vertices (see Appendix). We conduct a paired t-test to examine the statistical significance. As reported in Table <ref type="table" target="#tab_0">1</ref>, CoTAN achieves significantly superior geometric accuracy (p-value&lt;0.05) compared to all stateof-the-art baselines, except for the white matter surfaces of CortexODE.</p><p>-Mesh Quality: We evaluate the mesh quality by the ratio of self-intersecting faces (SIF) as shown in Table <ref type="table" target="#tab_0">1</ref>. Note that due to the narrower sulcal gaps of neonatal cortex compared to adult <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">25]</ref> (see Fig. <ref type="figure" target="#fig_1">2</ref>), all baseline methods produce more SIFs than reported in their original papers. Except that DeepCSR produces no SIFs since it uses the Marching Cubes algorithm, CoTAN achieves the best mesh quality with only 0.001% SIFs in white matter surfaces and 0.071% SIFs in pial surfaces. These remaining SIFs are likely introduced by the discretization of triangular mesh representation and ODE integration. CoTAN produces fewer SIFs for three reasons. Firstly, CoTAN employs diffeomorphic deformation, while the non-diffeomorphic Vox2Cortex creates 14% SIFs. We further set the integration steps K=5 for CoTAN so that the deformation is no longer diffeomorphic. The SIFs of pial surfaces are increased to 2.99 ± 1.19%. Secondly, CoTAN reconstructs the pial surface by expanding the input white matter surface. It is difficult to avoid collisions during the deformation from a smooth template into deep sulci, e.g., in Vox2Cortex and CorticalFlow. Lastly, CoTAN uses MSE loss rather than Chamfer distance for pial surface extraction, which alleviates the mismatch between points in the narrow sulci.</p><p>-Computational Efficiency: We report the runtime and GPU memory cost for both training and testing, as well as the number of learnable parameters for CoTAN and all baseline approaches in Table <ref type="table" target="#tab_1">2</ref>. The runtime includes both model inference and post-processing. CoTAN only requires 0.21 s to extract cortical surfaces for each hemisphere, which is 2× faster than the best baseline. CoTAN can be trained end-to-end and reduces training time by 46% compared to CorticalFlow and CFPP, which have to train three U-Nets consecutively to parameterize three SVFs for a single surface. Although CoTAN uses relatively Ablation Study. We conduct ablation studies on CoTAN and evaluate the geometric accuracy on the white matter surface reconstruction. First, without fine-tuning, the geometric errors increase by 8% as reported in Table <ref type="table" target="#tab_2">3</ref>. Then we consider CoTAN with single resolution (R=1) or only predict a single SVF (M =1) for each resolution. The geometric distances increase in both cases.</p><p>Next, we examine the effectiveness of the CTVF v t (x; a) by fixing the input time t=0 or age a=0. We train CoTAN models to predict the TVF (a=0), CVF (t=0) and SVF (a=t=0), which are degraded from the CTVF. Table <ref type="table" target="#tab_2">3</ref> shows that the SVF increases geometric errors by 30% due to its limited representation capacity. The CVF increases accuracy slightly by learning conditional deformations adaptive to the age of neonates. The TVF exploits temporal information to model a wider range of deformations and effectively improves the performance.</p><p>Lastly, we train a U-Net to predict R×M SVFs and integrate them directly without attention. Since the gradients are backpropagated through all SVFs, it requires &gt;140 h training time which is 2.4× slower than our attention-based CoTAN. The model is also sensitive to small updates, which can affect all SVFs. This results in exploding gradients which we have observed in the training, whereas CoTAN can be trained robustly by integrating a single CTVF.</p><p>Attention Maps. We explore the attention maps p r,m (t, a) learned by CoTAN. We define p r = M m=1 p r,m to reflect the importance of the SVFs at each resolution level r=1, ..., R, where R = 3 and larger r means higher resolution. Figure <ref type="figure" target="#fig_5">6</ref> visualizes the attention maps p r (t, a) for white matter surface reconstruction for integration time t ∈ [0, 1] and age a ∈ {28, 35, 42}. It shows that the attention maps focus on low resolution (r=1) at the beginning of the integration, and then attends to high resolution (r=3) when t→1. Furthermore, Fig. <ref type="figure" target="#fig_5">6</ref> (Right) shows that an initial white matter surface deforms into a coarse shape for t≤0.3 and learns fine details for t≥0.6. This matches the attention maps and demonstrates that CoTAN learns coarse-to-fine deformations automatically without any supervision on the intermediate deformations. Additionally, Fig. <ref type="figure" target="#fig_5">6</ref> shows that CoTAN pays more attention to low resolution for younger subjects (28 week) whose brains have not fully developed yet. More deformations at higher resolutions (r≥2) are required for older neonates (≥35 week) with highly folded cortex.</p><p>Discussion. One limitation of our experiments is that we only train and evaluate CoTAN based on the pseudo-GT generated by the dHCP structural pipeline <ref type="bibr" target="#b24">[24]</ref>. Previous approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22]</ref> have been validated by the test-retest experiments. However, this is infeasible for neonates, whose brain develops rapidly even within a short period. To verify the superior anatomical accuracy of CoTAN, we provide qualitative comparison between the pseudo-GT and CoTAN as visualized in Fig. <ref type="figure" target="#fig_4">5</ref>. It shows that CoTAN can effectively mitigate corruptions introduced by the dHCP pipeline for neonatal subjects at different ages. In addition, the dHCP pipeline requires 4 h to process a single subject <ref type="bibr" target="#b24">[24]</ref>, while CoTAN extracts cortical surfaces in only 0.21 s for each brain hemisphere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we propose CoTAN for diffeomorphic neonatal cortical surface reconstruction. CoTAN employs an attention mechanism to combine multiple SVFs to a CTVF, which outperforms existing baselines in geometric accuracy and mesh quality. CoTAN can also be extended and applied to extract adult cortical surfaces conditioned on the age, gender or pathological information of the subjects. Our future work will integrate CoTAN into a learning-based pipeline for universal cortical surface analysis across all age groups.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Neonatal cortical surfaces at different post-menstrual ages.</figDesc><graphic coords="2,53,55,57,05,155,89,67,66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Left: adult brain MRI from the ADNI dataset [20]. Right: neonatal brain MRI from the dHCP dataset [9].</figDesc><graphic coords="2,236,85,54,50,155,62,70,21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The architecture of the proposed CoTAN framework. Given an input 3D brain MRI, CoTAN uses a U-Net to predict M SVFs for each resolution level R. An attention map is learned to focus on specific SVFs varying with the input integration time t and conditioned on the age a of the neonatal subjects. Further conditioning could be achieved with minimal effort, e.g., biological sex, diagnoses, etc. For each time step t, the CTVF vt is represented by the weighted sum of all R × M SVFs. By integrating the CTVF, CoTAN deforms an input initial surface to the predicted cortical surface.</figDesc><graphic coords="4,55,98,54,23,340,18,112,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Diffeomorphic deformation from an initial template mesh to cortical surfaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Visualization of neonatal cortical surfaces generated by the dHCP structural pipeline<ref type="bibr" target="#b24">[24]</ref> and CoTAN for different ages. CoTAN shows better anatomical accuracy as highlighted in both surface meshes and corresponding brain MRI images.</figDesc><graphic coords="7,41,79,54,20,340,21,101,50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Visualization of attention maps predicted by CoTAN for white matter surface reconstruction. Left: conditional temporal attention maps p r (t, a) at different time and ages. Right: coarse-to-fine white matter surface deformations learned by CoTAN.</figDesc><graphic coords="8,55,98,54,47,340,18,64,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparative results of neonatal cortical surface reconstruction on the dHCP dataset. The geometric accuracy (ASSD and HD90) and mesh quality (the ratio of SIFs) are reported for white matter and pial surfaces. Smaller values mean better results. *CoTAN (ours) shows significant improvement (p &lt;0.05) compared to baselines.</figDesc><table><row><cell></cell><cell cols="2">White Matter Surface</cell><cell></cell><cell>Pial Surface</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>ASSD (mm)</cell><cell>HD90 (mm)</cell><cell>SIF (%)</cell><cell>ASSD (mm)</cell><cell>HD90 (mm)</cell><cell>SIF (%)</cell></row><row><cell>CoTAN</cell><cell cols="6">0.107 ± 0.026 0.217 ± 0.076 0.001 ± 0.004 0.121 ± 0.029 0.259 ± 0.075 0.071 ± 0.034</cell></row><row><cell cols="7">CortexODE 0.109 ± 0.052 0.231 ± 0.326 0.001 ± 0.002 0.134 ± 0.052* 0.306 ± 0.358* 0.221 ± 0.114*</cell></row><row><cell>CFPP</cell><cell cols="6">0.118 ± 0.028* 0.241 ± 0.085* 0.075 ± 0.057* 0.124 ± 0.031* 0.273 ± 0.086* 2.457 ± 1.003*</cell></row><row><cell cols="7">CorticalFlow 0.122 ± 0.029* 0.247 ± 0.080* 0.048 ± 0.032* 0.157 ± 0.031* 0.331 ± 0.089* 9.798 ± 2.902*</cell></row><row><cell cols="7">Vox2Cortex 0.115 ± 0.035* 0.233 ± 0.110* 0.253 ± 0.169* 0.130 ± 0.039* 0.291 ± 0.141* 14.366 ± 2.262*</cell></row><row><cell>DeepCSR</cell><cell cols="3">0.129 ± 0.047* 0.276 ± 0.211* -</cell><cell cols="3">0.299 ± 0.070* 1.214 ± 0.337* -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparative results of runtime, GPU memory cost, and the number of model parameters for both training and testing.</figDesc><table><row><cell></cell><cell>Runtime</cell><cell></cell><cell>GPU (GB) Model</cell></row><row><cell>Method</cell><cell>Train</cell><cell>Test</cell><cell>Train Test #Param</cell></row><row><cell cols="4">CoTAN (Ours) 57.9h 0.21s 8.71 4.05 2.47 M</cell></row><row><cell>CortexODE</cell><cell cols="3">51.6h 1.88 s 9.24 4.61 1.99 M</cell></row><row><cell>CFPP</cell><cell cols="3">131.5h 0.57 s 9.86 3.56 1.03 M</cell></row><row><cell>CorticalFlow</cell><cell cols="3">105.7h 0.51 s 9.12 3.56 1.03 M</cell></row><row><cell>Vox2Cortex</cell><cell cols="3">63.7h 1.48 s 6.96 6.26 6.54 M</cell></row><row><cell>DeepCSR</cell><cell cols="3">15.1 h 10.69 s 5.26 2.33 4.65 M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The results of ablation experiments for CoTAN on white matter surface reconstruction.</figDesc><table><row><cell>Method</cell><cell>ASSD (mm) HD90 (mm)</cell></row><row><cell cols="2">CoTAN (Ours)0.107 ± 0.0260.217 ± 0.076</cell></row><row><cell>Pre-train</cell><cell>0.116 ± 0.029 0.238 ± 0.095</cell></row><row><cell>R=1</cell><cell>0.112 ± 0.027 0.232 ± 0.081</cell></row><row><cell>M =1</cell><cell>0.111 ± 0.030 0.231 ± 0.097</cell></row><row><cell cols="2">SVF (a=t=0) 0.138 ± 0.037 0.291 ± 0.109</cell></row><row><cell>CVF (t=0)</cell><cell>0.135 ± 0.036 0.285 ± 0.111</cell></row><row><cell>TVF (a=0)</cell><cell>0.108 ± 0.028 0.222 ± 0.090</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">President</rs>'s PhD Scholarship at <rs type="institution">Imperial College London</rs>. Support was also received from the <rs type="funder">ERC</rs> project <rs type="projectName">MIA-NORMAL 101083647</rs> and <rs type="funder">ERC</rs> project <rs type="grantNumber">Deep4MI 884622</rs>. Data were provided by the <rs type="programName">developing Human Connectome Project</rs>, <rs type="institution">KCL-Imperial-Oxford Consortium</rs> funded by the <rs type="funder">ERC</rs> under the <rs type="funder">European Union Seventh Framework Programme</rs> (FP/2007-2013) / <rs type="funder">ERC</rs> Grant Agreement no. [319456]. We are grateful to the families who generously supported this trial.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_h7MNPMN">
					<orgName type="project" subtype="full">MIA-NORMAL 101083647</orgName>
				</org>
				<org type="funding" xml:id="_YCtZJPU">
					<idno type="grant-number">Deep4MI 884622</idno>
					<orgName type="program" subtype="full">developing Human Connectome Project</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_30.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A fast diffeomorphic image registration algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ashburner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="113" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VoxelMorph: a learning framework for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1788" to="1800" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computing large deformation metric mappings via geodesic flows of diffeomorphisms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Beg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trouvé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="139" to="157" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vox2cortex: fast explicit reconstruction of cortical surfaces from 3D MRI scans with geometric deep neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bongratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rickmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pölsterl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20773" to="20783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Threedimensional motion corrected sensitivity encoding reconstruction for multi-shot multi-slice MRI: application to neonatal brain imaging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cordero-Grande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1365" to="1376" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DeepCSR: a 3D deep learning approach for cortical surface reconstruction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lebrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bourgeat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fripp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Salvado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="806" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cortical surface-based analysis: I. segmentation and surface reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Sereno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="194" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The developing human connectome project neonatal data release</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">886772</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FreeSurfer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="774" to="781" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Measuring the thickness of the human cerebral cortex from magnetic resonance images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="11050" to="11055" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cortical surface-based analysis: II: inflation, flattening, and a surface-based coordinate system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Sereno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="207" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The minimal preprocessing pipelines for the human connectome project</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Glasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Sotiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="105" to="124" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mapping human cortical areas in vivo based on myelin content as revealed by T1-and T2-weighted MRI</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Glasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">32</biblScope>
			<biblScope unit="page" from="11597" to="11616" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SegRecon: learning joint brain surface reconstruction and segmentation from images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_61</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-2_61" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12907</biblScope>
			<biblScope unit="page" from="650" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural mesh flow: 3D manifold mesh generation via diffeomorphic flows</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1747" to="1758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TopoFit: rapid reconstruction of topologically-correct cortical surfaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hoopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Greve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A dedicated neonatal brain imaging system</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winchman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Padormo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Teixeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="794" to="804" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Alzheimer&apos;s disease neuroimaging initiative (ADNI): MRI methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="685" to="691" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CorticalFlow: a diffeomorphic mesh transformer network for cortical surface reconstruction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lebrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="29491" to="29505" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cortex-ODE: learning cortical surface reconstruction by neural ODEs</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alansary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="430" to="443" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PialNN: a fast deep learning framework for cortical pial surface reconstruction</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alansary</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87586-2_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87586-2_8" />
	</analytic>
	<monogr>
		<title level="m">MLCN 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13001</biblScope>
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The developing human connectome project: a minimal processing pipeline for neonatal cortical surface reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="88" to="112" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Open access series of imaging studies (OASIS): cross-sectional MRI data in young, middle aged, nondemented, and demented older adults</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Csernansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Buckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1498" to="1507" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Concurrent spatial and channel &apos;squeeze &amp; excitation&apos; in fully convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00928-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00928-1_48" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11070</biblScope>
			<biblScope unit="page" from="421" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CorticalFlow++: boosting cortical surface reconstruction accuracy, regularity, and interoperability</title>
		<author>
			<persName><forename type="first">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_48" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BrainSuite: an automated cortical surface identification tool</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Shattuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Leahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="142" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Topology-preserving shape reconstruction and registration via neural diffeomorphic flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20845" to="20855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pixel2Mesh: generating 3D mesh models from single RGB images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="52" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Voxel2Mesh: 3D mesh model generation from volumetric data</title>
		<author>
			<persName><forename type="first">U</forename><surname>Wickramasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Knott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_30</idno>
		<idno>978-3-030-59719-1_30</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="299" to="308" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
