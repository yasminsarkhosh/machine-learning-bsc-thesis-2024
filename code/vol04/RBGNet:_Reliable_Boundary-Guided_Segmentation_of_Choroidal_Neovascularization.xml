<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization</title>
				<funder ref="#_gygNs7r #_TVd6x39 #_j4KnYAE">
					<orgName type="full">Zhejiang Provincial Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_zKFpKst #_Mk9RtsT">
					<orgName type="full">National Science Foundation Program of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Central Research Fund</orgName>
				</funder>
				<funder ref="#_KsDRQCa">
					<orgName type="full">Ningbo Natural Science Foundation</orgName>
				</funder>
				<funder ref="#_9sF6qRp">
					<orgName type="full">A*STAR AME Programmatic Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Technology and Engineering</orgName>
								<orgName type="institution" key="instit1">Ningbo Institute of Materials</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Cixi Biomedical Research Institute</orgName>
								<orgName type="institution" key="instit2">Wenzhou Medical University</orgName>
								<address>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yitian</forename><surname>Zhao</surname></persName>
							<email>yitian.zhao@nimte.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Technology and Engineering</orgName>
								<orgName type="institution" key="instit1">Ningbo Institute of Materials</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Mou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Technology and Engineering</orgName>
								<orgName type="institution" key="instit1">Ningbo Institute of Materials</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Cyber Science and Engineering</orgName>
								<orgName type="institution">Ningbo University of Technology</orgName>
								<address>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiayu</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Life Science and Technology</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengting</forename><surname>Liu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Institute of High Performance Computing, A*STAR</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
							<email>zhangjiong@nimte.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Technology and Engineering</orgName>
								<orgName type="institution" key="instit1">Ningbo Institute of Materials</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="163" to="172"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6D6DF10B2D9AD5E36B96CDFA6DDE193B</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_16</idno>
					<note type="submission">Raw Image CNV Region CNV Vessels Noise Interference Raw Image CNV Region CNV Vessels Artifact Interference Raw Image CNV Region CNV Vessels Noise Interference Raw Image CNV Region CNV Vessels Artifact Interference Raw Image CNV Region CNV Vessels Noise Interference Raw Image CNV Region CNV Vessels Artifact Interference Raw Image CNV Region CNV Vessels Noise Interference Raw Image CNV Region CNV Vessels</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CNV</term>
					<term>OCTA</term>
					<term>Transformer</term>
					<term>Uncertainty</term>
					<term>Multi-task</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Choroidal neovascularization (CNV) is a leading cause of visual impairment in retinal diseases. Optical coherence tomography angiography (OCTA) enables non-invasive CNV visualization with micrometerscale resolution, aiding precise extraction and analysis. Nevertheless, the irregular shape patterns, variable scales, and blurred lesion boundaries of CNVs present challenges for their precise segmentation in OCTA images. In this study, we propose a Reliable Boundary-Guided choroidal neovascularization segmentation Network (RBGNet) to address these issues. Specifically, our RBGNet comprises a dual-stream encoder and a multi-task decoder. The encoder consists of a convolutional neural network (CNN) stream and a transformer stream. The transformer captures global context and establishes long-range dependencies, compensating for the limitations of the CNN. The decoder is designed with multiple tasks to address specific challenges. Reliable boundary guidance is achieved by evaluating the uncertainty of each pixel label, By assigning it as a weight to regions with highly unstable boundaries, the network's ability to learn precise boundary locations can be improved, ultimately leading to more accurate segmentation results. The prediction results are also used to adaptively adjust the weighting factors between losses to guide the network's learning process. Our experimental results demonstrate that RBGNet outperforms existing methods, achieving a Dice score of 90.42% for CNV region segmentation and 90.25% for CNV vessel segmentation. https://github.com/iMED-Lab/RBGnet-Pytorch.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Age-related macular degeneration (AMD) is a leading cause of blindness worldwide, primarily attributable to choroidal neovascularization (CNV) <ref type="bibr" target="#b0">[1]</ref>. Optical coherence tomography angiography (OCTA), a non-invasive imaging technique, has gained popularity in recent years due to its ability to visualize blood flow in the retina and choroid with micrometer depth resolution <ref type="bibr" target="#b1">[2]</ref>. Thus, automated CNV segmentation based on OCTA images can facilitate quantitative analysis and enhance the diagnosis performance of AMD <ref type="bibr" target="#b2">[3]</ref>. However, the accurate segmentation of CNV from OCTA images poses a significant challenge due to the complex morphology of CNVs and the presence of imaging artifacts <ref type="bibr" target="#b3">[4]</ref>, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Hence, reliable CNV segmentation is promptly needed to assist ophthalmologists in making informed clinical decisions.</p><p>Several methods have been proposed to segment CNV regions from OCTA images, including handcraft feature descriptors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and deep learning-based techniques <ref type="bibr" target="#b6">[7]</ref>. For example, a saliency-based method for automated segmentation of CNV regions in OCTA images was proposed by Liu et al. <ref type="bibr" target="#b4">[5]</ref>, which capitalizes on distinguishing features of CNV regions with higher intensity compared to background artifacts and noise. In <ref type="bibr" target="#b5">[6]</ref>, an unsupervised algorithm for CNV segmentation was proposed, which utilizes a density cell-like P system. However, their accuracy is restricted by weak saliency and ambiguous boundaries. With the recent advancements of deep learning, several methods have also been proposed for CNV segmentation in OCT images. U-shaped multiscale information fusion networks are proposed in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref> for segmenting CNVs with multiscale scenarios. Wang et al. <ref type="bibr" target="#b6">[7]</ref> further proposed a two-stage CNN-based architecture based on OCTA images that is capable of extracting both CNV regions and vessel details. However, common issues including substantial scale variations of CNV regions and low-contrast microvascular boundaries were not fully deliberated in previous network designs. Thus, more dedicated modules with scale adaptivity and boundary refinement properties need to be explored to solve existing challenges.</p><p>Previously, Gal et al. <ref type="bibr" target="#b9">[10]</ref> proposed a theory to effectively model uncertainty with dropout NNs. Afterward, Bragman et al. <ref type="bibr" target="#b10">[11]</ref> applied this method to the field of medical image analysis. Nair et al. <ref type="bibr" target="#b11">[12]</ref> showed the success of using dropout for the detection of three-dimensional multiple sclerosis lesions. Motivated by these findings, we also consider taking advantage of the pixel-level uncertainty estimation and making it adaptive to the segmentation of ambiguous CNV boundaries.</p><p>In this work, we propose a reliable boundary-guided network (RBGNet) to simultaneously segment both the CNV regions and vessels. Our proposed method is composed of a dual-branch encoder and a boundary uncertainty-guided multitask decoder. The dual-branch encoder is designed to capture both of the global long-range dependencies and the local context of CNVs with significant scale variations, while the proposed uncertainty-guided multi-task decoder is designed to strengthen the model to segment ambiguous boundaries. The uncertainty is achieved by approximating a Bayesian network through Monte Carlo dropout.</p><p>The main contributions are summarized as follows:</p><p>(a) We propose a multi-task joint optimization method to interactively learn shape patterns and boundary contours for more accurate segmentation of CNV regions and vessels. (b) We design a dual-stream encoder structure to take advantages of the CNN and transformer, which promote the network to effectively learn both local and global information. (c) We propose an uncertainty estimation-guided weight optimization strategy to provide reliable guidance for multi-task network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>The proposed RBGNet comprises three primary components: a dual-stream encoder, a multi-task decoder, and an uncertainty-guided weight optimization strategy, as depicted in Fig. <ref type="figure" target="#fig_1">2</ref>. The OCTA image is firstly processed using a dual-stream encoder that combines Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) models <ref type="bibr" target="#b12">[13]</ref> to produce high-dimensional semantic features. This approach enhances the local context and global dependencies of the image, thereby improving the CNV feature representation. These features are then fed into a multi-task decoder, which integrates information from multiple tasks to achieve better CNV segmentation. To further optimize the model's performance, we introduce a pixel-level uncertainty estimation approach that enhances the model's capacity to handle ambiguous region boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dual-Stream Encoder</head><p>The CNVs in OCTA images are of various shapes and a wide range of scales, which pose challenges to the accurate segmentation of CNVs. To extract the long-range dependencies of cross-scale CNV information, we employ VIT <ref type="bibr" target="#b12">[13]</ref> as an independent stream in the feature encoder of the proposed method. Moreover, the proposed dual-stream encoder is also embedded with a CNN-based multi-scale encoder to obtain the local context of CNV regions. Specifically, the VIT stream utilizes a stack of twelve transformer layers to extract features from flattened uniform non-overlapping patches, which seamlessly integrates with the CNN encoder stream through skip connections, similar to <ref type="bibr" target="#b13">[14]</ref>. However, unlike <ref type="bibr" target="#b13">[14]</ref>, we divide the output representations of the transformer layers in VIT into four groups, each containing three feature representations. These groups correspond to the scales of the CNN stream, uniformly arranged from shallow to deep. Then, we perform element-wise summation for each group of representations, followed by reshaping them into non-overlapping patch sizes. The reshaped representation is further upsampled to the corresponding CNN feature resolution. The CNN branch is a U-shaped network <ref type="bibr" target="#b14">[15]</ref> that extracts multiscale features using ReSidual U-block (RSU) proposed by <ref type="bibr" target="#b15">[16]</ref> to preserve highresolution information locally. To integrate complementary information from the CNN and VIT features, we concatenate them in each of the first four feature extraction layers of the CNN branch. To enhance the features from the dualbranch encoder, we apply a bottleneck layer that consists of a feature extraction block, max-pooling, upsampling, and another feature extraction block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-task Decoder</head><p>The proposed multi-task decoder performs two main tasks: CNV region segmentation and vessel segmentation. The CNV region segmentation task contains two auxiliary subtasks including boundary prediction and shape regression. Each task is implemented at the end of the decoder using a 1 × 1 convolutional layer followed by a Sigmoid activation, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Region Segmentation: Region segmentation of CNV allows accurate assessment of lesion size. This task aims to accurately segment the entire CNV regions in OCTA images via boundary prediction and shape regression. Region segmentation is typically accomplished by categorizing individual pixels as either belonging to the CNV region or the background region. The purpose of region boundary prediction is to explicitly enhance the model's focus on ambiguous boundaries, allowing for more accurate region segmentation. The process of shape regression for region segmentation involves the transformation of boundary regression into a task of signed distance field regression. This is achieved by assigning a signed distance to each pixel, representing its distance from the boundary, with negative values inside the boundary, positive values outside the boundary, and zero values on the boundary. By converting the ground truth into a signed distance map (SDM), the network can learn CNV shape patterns from the rich shape pattern information contained in the SDMs.</p><p>Vessel Segmentation: To improve the vessel segmentation performance, we propose to guide the model to focus on low-contrast vessel details by estimating their pixel uncertainty. Simultaneously, the complementary information from the CNV region segmentation task is further utilized to eliminate the interference of vessel pixels outside the region, thus better refining the vessel segmentation results. The proposed multi-task decoder improves the segmentation accuracy of regions and vessels by explicitly or implicitly using the information between individual tasks and optimizing each task itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Uncertainty-Guided Multi-Task Optimization</head><p>Uncertainty Estimation: In contrast to traditional deep learning models that produce deterministic predictions, Bayesian neural networks <ref type="bibr" target="#b16">[17]</ref> can provide not only predictions but also uncertainty. It treats the network weights as random variables with a priori distribution and infers the posterior distribution of the weights. In this paper, we employ Monte Carlo dropout (MC-dropout) <ref type="bibr" target="#b9">[10]</ref> to approximate Bayesian networks and capture the uncertainty of the model. Bayesian inference offers a rigorous method for making decisions in the presence of uncertainty. However, the computational complexity of computing the posterior distribution often renders it infeasible. This issue is usually solved by finding the best approximation in a finite space.</p><p>To learn the weight distribution of the network, we minimize the Kullback-Leibler (KL) scatter between the true posterior distribution and its approximation. The probability distribution of each pixel is obtained based on Dropout to sample the posterior weight distribution M times. Then, the mean P i of each pixel is used to generate the prediction, while the variance V i is used to quantify the uncertainty of the pixel. This process can be described as follows.</p><formula xml:id="formula_0">P i = 1 M M m=1</formula><p>Pm , and</p><formula xml:id="formula_1">V i = 1 M M m=1 Pm -P i 2 . (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>Uncertainty-Weighted Loss: In CNV region segmentation, the importance of each pixel may vary, especially for ambiguous boundaries, while assigning equal weights to all samples may not be optimal. To address this issue, uncertainty maps are utilized to assign increased weights to pixels with higher levels of uncertainty. This, in turn, results in a more substantial impact on the update of the model parameters. Moreover, the incorporation of multiple tasks can generate different uncertainty weights for a single image, enabling a more comprehensive exploration of CNV boundary features via joint optimization. We employ a combination of loss functions, including binary cross-entropy (BCE) loss, mean squared error (MSE) loss, and Dice loss, to optimize the model parameters across all tasks. However, for the region shape regression task, we restricted the loss functions to only BCE and MSE. We incorporate uncertainty weights into the BCE loss by weighting each pixel to guide uncertainty on model training, i.e.,</p><formula xml:id="formula_3">L UBCE = - 1 N N i=1 (1 + V i ) • [y i log ( ŷi ) + (1 -y i ) log (1 -ŷi )] , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where y i and ŷi are respective ground truth and prediction for pixel i. The total loss function can be expressed as L = T t λ t L t , where L t denotes the loss function for t th task. λ t denotes the loss weight, obtained by averaging the uncertainty map of the corresponding task and normalizing them to sum to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset:</head><p>The proposed RBGNet was evaluated on a dataset consisting of 74 OCTA images obtained using the Heidelberg OCT2 system (Heidelberg, Germany). All images were from AMD patients with CNV progression, captured in a 3 × 3 mm 2 area centered at the fovea. The enface projected OCTA images of the avascular complex were used for our experiments. All the images were resized into a resolution of 384 × 384 for experiments. The CNV areas and vessels were manually annotated by one senior ophthalmologist, and then reviewed and refined by another senior ophthalmologist. All images were acquired with regulatory approvals and patient consents as appropriate, following the Declaration of Helsinki.</p><p>Implementation Details: Our method is implemented based on the PyTorch framework with NVIDIA GeForce GTX 1080Ti. We train the model using an Adam optimizer with an initial learning rate of 0.0001 and a batch size of 4 for 300 epochs, without implementing a learning rate decay strategy. During training, the model inputs were subject to standard data augmentation pipelines, including random horizontal, vertical flips, random rotation, and random cropping. A 5-fold cross-validation approach is adopted to evaluate the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with State-of-the-Arts:</head><p>To benchmark our model's performance, we compared it with several state-of-the-art methods in the medical image segmentation field, including U-Net <ref type="bibr" target="#b14">[15]</ref>, CE-Net <ref type="bibr" target="#b17">[18]</ref>, CS-Net <ref type="bibr" target="#b18">[19]</ref>, Tran-sUNet <ref type="bibr" target="#b19">[20]</ref>, and the backbone method U 2 Net <ref type="bibr" target="#b15">[16]</ref>. We use the Dice coefficient (Dice), intersection over union (IoU), false discovery rate (FDR), and area under the ROC curve (AUC) to evaluate the segmentation performance. The quantitative results are demonstrated in Table <ref type="table" target="#tab_0">1</ref>. Our results demonstrate that the  The proposed method exhibits superior performance in precisely segmenting ambiguous boundaries of CNV regions, as demonstrated in the first two rows of Fig. <ref type="figure" target="#fig_2">3</ref>. In contrast, existing state-of-the-art methods such as U-Net <ref type="bibr" target="#b14">[15]</ref>, and CS-Net <ref type="bibr" target="#b18">[19]</ref> exhibit limitations in accurately segmenting complex and variable structures, leading to the misidentification of background structures as CNV regions. The illustrated quantitative results and performance comparisons serve as evidence of the proposed method's ability to simultaneously segment CNV regions and vessels with state-of-the-art performance. Accurate segmentation of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conclusion</head><p>In summary, this study proposes a novel method to address the challenges of CNV segmentation in OCTA images. It incorporates a dual-branch encoder, multi-task optimization, and uncertainty-weighted loss to accurately segment CNV regions and vessels. The findings indicate that the utilization of crossscale information, multi-task optimization, and uncertainty maps improve CNV segmentations. The proposed method exhibits superior performance compared to state-of-the-art methods, which suggests potential clinical implications for the diagnosis of CNV-related diseases. Nevertheless, further research is needed to validate the effectiveness of the proposed approach in large-scale clinical studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Interference to CNV in OCTA images. Red arrows indicate artifacts and noise interference, respectively. (Color figure online)</figDesc><graphic coords="2,41,79,54,59,340,33,66,13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Schematic diagram of the proposed RBGNet, which contains a dual-stream encoder and a multi-task decoder.</figDesc><graphic coords="4,52,26,53,72,327,97,184,45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance comparisons of different methods. Under-segmentation is shown in red, and over-segmentation is shown in green. (Color figure online)</figDesc><graphic coords="7,55,98,54,17,340,15,164,53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Uncertainty maps during model training. #n represents the n th epoch.</figDesc><graphic coords="8,41,79,168,80,340,33,57,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparisons for CNV segmentation.</figDesc><table><row><cell>Methods</cell><cell cols="2">Leison segmentation</cell><cell></cell><cell cols="2">Vessel segmentation</cell></row><row><cell></cell><cell>DICE IoU</cell><cell>FDR</cell><cell>AUC</cell><cell>DICE IoU</cell><cell>FDR</cell><cell>AUC</cell></row><row><cell>MF-Net [8]</cell><cell cols="6">0.8166 0.7142 0.1574 0.9084 0.7582 0.7191 0.1734 0.9062</cell></row><row><cell>U-Net [15]</cell><cell cols="6">0.8228 0.7290 0.1694 0.9148 0.8193 0.7280 0.1685 0.9175</cell></row><row><cell>CE-Net [18]</cell><cell cols="6">0.8690 0.7806 0.1173 0.9331 0.8372 0.7308 0.1459 0.9156</cell></row><row><cell>CS-Net [19]</cell><cell cols="6">0.8567 0.7689 0.1423 0.9365 0.8518 0.7622 0.1519 0.9378</cell></row><row><cell cols="7">TransUNet [20] 0.8414 0.7438 0.1254 0.9197 0.8301 0.7283 0.1423 0.9171</cell></row><row><cell>U 2 Net [16]</cell><cell cols="6">0.8770 0.7930 0.1072 0.9383 0.8561 0.7662 0.0982 0.9214</cell></row><row><cell>Proposed</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>9042 0.8328 0.0797 0.9464 0.9025 0.8229 0.0688 0.9455 proposed</head><label></label><figDesc>method surpasses the existing state-of-the-art methods in both tasks. Specifically, our method achieved outstanding results on the test set for region segmentation, with a Dice of 90.42%, an IOU of 83.28%, and an AUC of 94.64%. The results in Table1indicate that superior vessel segmentation is positively associated with the model's ability to segment CNV regions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation results for CNV region segmentation and vessel segmentation.</figDesc><table><row><cell>Methods</cell><cell cols="3">Leison segmentation</cell><cell></cell><cell cols="2">Vessel segmentation</cell></row><row><cell></cell><cell cols="2">DICE IoU</cell><cell>FDR</cell><cell>AUC</cell><cell>DICE IoU</cell><cell>FDR</cell><cell>AUC</cell></row><row><cell>Backbone</cell><cell cols="6">0.8869 0.8051 0.1149 0.9473 0.8934 0.8160 0.0918 0.9401</cell></row><row><cell cols="7">Backbone + M1 0.8957 0.8194 0.1119 0.9534 0.8947 0.8177 0.0932 0.9530</cell></row><row><cell cols="7">Backbone + M2 0.9001 0.8260 0.0856 0.9448 0.8985 0.8183 0.0725 0.9322</cell></row><row><cell cols="7">Backbone + M3 0.9042 0.8328 0.0797 0.9464 0.9025 0.8229 0.0688 0.9455</cell></row><row><cell># 66</cell><cell># 108</cell><cell># 150</cell><cell></cell><cell># 192</cell><cell># 234</cell><cell># 273</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported in part by the <rs type="funder">National Science Foundation Program of China</rs> (<rs type="grantNumber">62103398</rs>, <rs type="grantNumber">62272444</rs>), <rs type="funder">Zhejiang Provincial Natural Science Foundation of China</rs> (<rs type="grantNumber">LZ23F010002</rs>, <rs type="grantNumber">LR22F020008</rs>, <rs type="grantNumber">LQ23F010002</rs>), in part by the <rs type="funder">Ningbo Natural Science Foundation</rs> (<rs type="grantNumber">2022J143</rs>), and <rs type="funder">A*STAR AME Programmatic Fund</rs> (<rs type="grantNumber">A20H4b0141</rs>) and <rs type="funder">Central Research Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zKFpKst">
					<idno type="grant-number">62103398</idno>
				</org>
				<org type="funding" xml:id="_Mk9RtsT">
					<idno type="grant-number">62272444</idno>
				</org>
				<org type="funding" xml:id="_gygNs7r">
					<idno type="grant-number">LZ23F010002</idno>
				</org>
				<org type="funding" xml:id="_TVd6x39">
					<idno type="grant-number">LR22F020008</idno>
				</org>
				<org type="funding" xml:id="_j4KnYAE">
					<idno type="grant-number">LQ23F010002</idno>
				</org>
				<org type="funding" xml:id="_KsDRQCa">
					<idno type="grant-number">2022J143</idno>
				</org>
				<org type="funding" xml:id="_9sF6qRp">
					<idno type="grant-number">A20H4b0141</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Prevalence of age-related macular degeneration in the United States</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="564" to="572" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Retinal vascular layers imaged by fluorescein angiography and optical coherence tomography angiography</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Spaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Klancnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="50" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quantitative optical coherence tomography angiography of choroidal neovascularization in age-related macular degeneration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1435" to="1444" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image artefacts in sweptsource optical coherence tomography angiography</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Falavarjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Sadda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="564" to="568" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated choroidal neovascularization detection algorithm for optical coherence tomography angiography</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3564" to="3576" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic quantification of choroidal neovascularization lesion area on OCT angiography based on density cell-like p systems with active membranes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Camino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3208" to="3219" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automated diagnosis and segmentation of choroidal neovascularization in OCT angiography using deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="927" to="944" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MF-Net: multi-scale information fusion network for CNV segmentation in retinal OCT images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">743769</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmentation of choroid neovascularization in OCT images based on convolutional neural network with differential amplification blocks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing</title>
		<imprint>
			<biblScope unit="volume">11313</biblScope>
			<biblScope unit="page" from="491" to="497" />
			<date type="published" when="2020">2020. 2020</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Uncertainty in multitask learning: joint representations for probabilistic MR-only radiotherapy planning</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J S</forename><surname>Bragman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-31" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring uncertainty measures in deep networks for multiple sclerosis lesion detection and segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101557</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An image is worth 16 × 16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">UNETR: transformers for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">U2-Net: going deeper with nested U-structure for salient object detection. Pattern Recogn</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaiane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">107404</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uncertainty guided semi-supervised segmentation of retinal layers in OCT images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sedai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_32</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-732" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="282" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CE-Net: context encoder network for 2D medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2281" to="2292" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CS-Net: channel and spatial attention network for curvilinear structure segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_80</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-780" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="721" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">TransUNet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
