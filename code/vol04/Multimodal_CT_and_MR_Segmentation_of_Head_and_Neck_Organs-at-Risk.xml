<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk</title>
				<funder ref="#_gr86AaK">
					<orgName type="full">Novo Nordisk Foundation</orgName>
				</funder>
				<funder ref="#_Cjmj2PJ #_bq4T6Uj #_PHfjgX5">
					<orgName type="full">Slovenian Research Agency (ARRS)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Gašper</forename><surname>Podobnik</surname></persName>
							<email>gasper.podobnik@fe.uni-lj.si</email>
							<idno type="ORCID">0000-0002-4142-3304</idno>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="institution">University of Ljubljana</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Primož</forename><surname>Strojan</surname></persName>
							<idno type="ORCID">0000-0002-0445-112X</idno>
							<affiliation key="aff1">
								<orgName type="department">Institute of Oncology Ljubljana</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Primož</forename><surname>Peterlin</surname></persName>
							<idno type="ORCID">0000-0002-0057-5635</idno>
							<affiliation key="aff1">
								<orgName type="department">Institute of Oncology Ljubljana</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bulat</forename><surname>Ibragimov</surname></persName>
							<idno type="ORCID">0000-0001-7739-7788</idno>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="institution">University of Ljubljana</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tomaž</forename><surname>Vrtovec</surname></persName>
							<idno type="ORCID">0000-0002-6625-0307</idno>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="institution">University of Ljubljana</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="745" to="755"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">025EEA07922A506659373EADF3C8DB30</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_71</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multimodal segmentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Radiotherapy (RT) is a standard treatment modality for head and neck (HaN) cancer that requires accurate segmentation of target volumes and nearby healthy organs-at-risk (OARs) to optimize radiation dose distribution. However, computed tomography (CT) imaging has low image contrast for soft tissues, making accurate segmentation of soft tissue OARs challenging. Therefore, magnetic resonance (MR) imaging has been recommended to enhance the segmentation of soft tissue OARs in the HaN region. Based on our two empirical observations that deformable registration of CT and MR images of the same patient is inherently imperfect and that concatenating such images at the input layer of a deep learning network cannot optimally exploit the information provided by the MR modality, we propose a novel modality fusion module (MFM) that learns to spatially align MR-based feature maps before fusing them with CT-based feature maps. The proposed MFM can be easily implemented into any existing multimodal backbone network. Our implementation within the nnU-Net framework shows promising results on a dataset of CT and MR image pairs from the same patients. Furthermore, the evaluation on a clinically realistic scenario with the missing MR modality shows that MFM outperforms other state-of-the-art multimodal approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Head and neck (HaN) cancer is a prevalent type of cancer <ref type="bibr" target="#b2">[3]</ref> with a yearly incidence of above 1 million cases and prevalence of above 4 million cases worldwide, accounting for around 5% of all cancer sites <ref type="bibr" target="#b17">[17]</ref>. Radiotherapy (RT) is a standard treatment modality for HaN cancer, which aims to deliver high doses of radiation to cancerous cells while sparing nearby healthy organs-at-risk (OARs) <ref type="bibr" target="#b21">[21]</ref>. To optimize radiation dose distribution, accurate three-dimensional (3D) segmentation of target volumes and OARs is required. Computed tomography (CT) is the primary imaging modality used for RT planning due to its ability to provide information about electron density, however, its low image contrast for soft tissues, including tumors, makes accurate segmentation of soft tissue OARs challenging. Therefore, the integration of complementary imaging modalities, such as magnetic resonance (MR), has been strongly recommended in clinical practice to enhance the segmentation of several soft tissue OARs in the HaN region <ref type="bibr" target="#b0">[1]</ref>. This naturally poses a question of whether automatic OAR segmentation can benefit from the MR image modality. Our study therefore aims to evaluate the impact of MR integration on the quality and robustness of automatic OAR segmentation in the HaN region, therefore contributing to the growing body of research on multimodal methods for medical image analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work.</head><p>A literature review by Zhang et al. <ref type="bibr" target="#b24">[24]</ref> divides deep learning (DL)-based multimodal segmentation methods into three fusion strategy groups: early, late and hybrid (also named layer ) fusion. The first two groups of methods are most commonly applied; early fusion comprises simple concatenation of modalities along the channel dimension before feeding them into the deep neural network. Additionally, concatenating feature maps (FMs) from separate modality encoders can also be considered as early fusion <ref type="bibr" target="#b6">[7]</ref>. Late fusion, on the other hand, employs separate branches for each input modality and then fuses the output features by either plain concatenation or by weighing the contributions of separate branches at the decision level. For example, Zhang et al. <ref type="bibr" target="#b23">[23]</ref> proposed an attention mechanism to fuse FMs from two separate U-Nets that accepted contrast-enhanced arterial and venous phase CT images. The third group, hybrid fusion, aims to combine the strengths of early and late fusion <ref type="bibr" target="#b24">[24]</ref> by employing two or more separate encoders (i.e. one for each modality) and a single decoder, where features from different resolution levels of the encoder are fused and fed into the decoder that produces the final full-resolution segmentation. Such hybrid or multi-level fusion along with the adaptive fusion method represents the current trend in computer vision <ref type="bibr" target="#b24">[24]</ref>, with the self-supervised model adaptation method as a prime example <ref type="bibr" target="#b18">[18]</ref>. One important aspect is also the missing modality scenario, meaning that the multimodal model should produce satisfactory results even if only one input modality is available. Nevertheless, the optimal fusion strategy remains an open question in need of further exploration. Similar conclusions were reached in a review of multimodal segmentation methods in the medical imaging community by Zhou et al. <ref type="bibr" target="#b25">[25]</ref>. Most methods implement either early or late fusion, however, the layer fusion strategy was identified as a better choice, since dense connections among layers can exploit more complex and complementary information to enhance training. The highlight is HyperDenseNet, a dual-path 3D network proposed by Dolz et al. <ref type="bibr" target="#b3">[4]</ref> that employs dense connections between two convolutional paths, and achieves improvements compared to other fusion strategies and single modality variants. However, other studies have shown that the best fusion strategy depends on the specific nature of the problem, e.g. Yan et al. <ref type="bibr" target="#b22">[22]</ref> demonstrated that the late fusion outperforms the other two approaches for the longitudinal detection of diabetic retinopathy. Relevant to the field of multimodal segmentation are also developments on unpaired multimodal segmentation, where cross-modality learning is employed to take advantage of different image modalities covering the same anatomy, but without the constraint to collect images from the same patients <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b19">19]</ref>. Although the methodologies comprising CycleGANs and/or multiple segmentation networks <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b19">19]</ref> seem promising, they can be excessively complex for the task of HaN OAR segmentation where both CT and MR image modalities from the same patient are often available. Consequently, our primary focus is the paired multimodal segmentation problem, including the missing modality scenario.</p><p>Motivation. When segmenting OARs in the HaN region for the purpose of RT planning, a multimodal segmentation model that can leverage the information from CT and MR images of the same patient might be beneficial compared to separate single-modal models. Firstly, as intuition suggests, such a model would rely on the CT image for bone structures and on the MR image for soft tissues, and therefore improve the overall segmentation quality by exploiting the complementary information from both modalities. Secondly, a multimodal model would facilitate cross-modality learning by extracting knowledge from one and applying that knowledge to the other modality, potentially improving the segmentation accuracy. Several studies indicated that such an approach is feasible, for example, for improving video classification by training a model on an auxiliary audio reconstruction task <ref type="bibr" target="#b12">[12]</ref>, or for audio-based detection by using the multimodal knowledge distillation concept, where teacher networks trained on RGB, depth and thermal images improve a student network trained only on audio data <ref type="bibr" target="#b20">[20]</ref>. Finally, from the DL infrastructure maintenance perspective, it is easier to maintain a single model that can handle both modalities than two separate models for each modality. However, clinical practice differs considerably from theory, meaning that a number of considerations must be taken into account. Firstly, although MR image acquisition is recommended, it is not always feasible due to time constraints, scanner occupancy and financial aspects. Consequently, automatic OAR multimodal segmentation is required to handle the missing modality scenario, and provide a similar segmentation quality as a single-modality system. Secondly, because CT and MR images are not acquired simultaneously and with the same acquisition parameters (e.g. resolution), there is an inherent misalignment between both modalities. This can be mitigated with image registration, but not completely, mainly due to different patient positioning that especially affects the deformation of soft tissues, and various modality-specific artifacts (e.g. motion, implants, partial volume effect, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>To tackle these considerations, we propose a mechanism named modality fusion module (MFM) that can generally be applied to any network architecture that learns features from multiple modalities, and shows promising performance also in the missing modality scenario. The advantages of the proposed MFM are the following: 1) it enables the spatial alignment of FMs from one with FMs from the other modality to further reduce errors that persist after deformable registration of input images, and enrich the FMs to improve the final OAR segmentation, 2) it significantly improves the performance of the missing modality scenario compared to other baseline fusion approaches, and 3) it performs well also on single modality out-of-distribution data, therefore facilitating cross-modality learning and contributing to better model generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Backbone Architecture. Our chosen backbone network is based on nnU-Net, a publicly available framework for DL-based segmentation <ref type="bibr" target="#b7">[8]</ref> that builds on the U-Net architecture <ref type="bibr" target="#b16">[16]</ref>, adds self-configurable pre-processing, augmentation and post-processing, and employs efficient training strategies. However, nnU-Net, which uses an early fusion strategy by concatenating input images or patches before feeding them to the first network layer, may not be the optimal strategy for multimodal segmentation. Recent studies have shown that this approach does not allow the network to learn meaningful high-level features from each modality before their fusion, resulting in only simple relationships between intensities from each input modality <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">23]</ref>. This is particularly problematic when fusing CT and MR images, which differ in several aspects, such as the type and location of artifacts, acquisition parameters, and visibility of soft tissues and bone structures. While MR images can help to improve the delineations of OARs that are poorly visible in CT images, the primary delineation is always performed on CT images with the help of registered MR images. An important repercussion is that image registration errors propagate into OAR delineations, which is particularly salient in the HaN region. To address these challenges, we propose an upgraded nnU-Net network with two separate encoders, one for each modality, and a common decoder that fuses FMs using the proposed MFM that learns to infer affine transformation parameters in a single forward pass. This approach efficiently pseudo-registers FMs from the MR encoder with those from the CT encoder, mitigating the effects of registration errors caused by non-rigid deformation of OARs and imaging artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modality Fusion Module.</head><p>The proposed MFM draws inspiration from the work of Jaderberg et al. <ref type="bibr" target="#b8">[9]</ref>, who introduced a spatial transformer network (STN) that learns to infer transformation parameters in a single forward pass, and then uses them to transform images and/or FMs. The fundamental idea is that STN can learn meaningful features that are spatially invariant to characteristics of the input data, without the need for extra supervision, thereby enhancing task performance. While it was demonstrated that complete spatial invariance cannot be achieved with STNs <ref type="bibr" target="#b5">[6]</ref>, the work of Jaderberg et al. is crucial in showing that STNs can be implemented as differentiable modules, enabling the loss to be propagated through the sampling (interpolation) mechanism. The same underlying principle of STNs has also been leveraged in optical flow and its derivative work semantic flow, where the flow alignment module was proposed to resample low-resolution FMs and align them with high-resolution FMs <ref type="bibr" target="#b1">[2]</ref>. We capitalize on the same principle to register FMs from MR images to those from CT images. Notably, MFM is different from semantic flow, as it takes two FMs of the same resolution but from different modalities, and aligns FMs from the auxiliary modality to FMs of the primary modality. We propose to use MFM at each resolution level of the nnU-Net backbone, which is schematically presented in Fig. <ref type="figure" target="#fig_0">1</ref>, and consists of three blocks: localization network, grid generator and sampler. The localization network is a regressor network that accepts concatenated FMs from both encoders and applies four blocks of strided convolutions followed by the ReLU activation to reduce their spatial dimensions. The final FMs are flattened and fed into a simple two-layer fully connected network, which outputs 12 affine 3D transformation parameters that are then passed to the grid generator. The generated sampling grid is used by the sampler to resample FMs from the second encoder, which are then concatenated with the untouched FMs from the first encoder and the decoder (right before the bottleneck, only the first two are concatenated, as there are no decoder FMs at that level). Both the grid generator and the sampler and readily implemented in the PyTorch library <ref type="bibr" target="#b8">[9]</ref>, and because they are both differentiable, no special optimization is needed for the localization network, allowing localization parameters to be optimized with the main (segmentation) loss function. Since there is no additional supervision that would assure perfect registration, we refer to this process as pseudo-registration. The purpose of this architecture is to align FMs from both modalities and improve their fusion, leading to better segmentation results.</p><p>Baseline Comparison. We evaluate the performance of the proposed MFM nnU-Net against three baseline networks: 1) a single modality nnU-Net trained only on CT images, 2) a nnU-Net trained on concatenated CT and MR image pairs, and 3) a model with separate encoders for both modalities, but with a simple concatenation along the channel axis instead of the proposed MFM. In addition, we compare our model with the state-of-the-art modality-aware mutual learning nnU-Net (MAML) that was presented at MICCAI 2021 <ref type="bibr" target="#b23">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Image Datasets. The proposed methodology was evaluated on two publicly available datasets: our recently released HaN-Seg dataset <ref type="bibr" target="#b14">[14]</ref> and the PDDCA dataset <ref type="bibr" target="#b15">[15]</ref>. The HaN-Seg dataset comprises CT and T1-weighted MR images of 56 patients, which were deformably registered with the SimpleElastix registration tool, and corresponding curated manual delineations of 30 OARs (for details, please refer to <ref type="bibr" target="#b14">[14]</ref>). Although only a subset of images is publicly available<ref type="foot" target="#foot_0">1</ref> due to the ongoing HaN-Seg challenge<ref type="foot" target="#foot_1">2</ref> , both the publicly available training as well as the privately withheld test images were used in our 4-fold cross-validation experiments. On the other hand, to evaluate the generalization ability of our method, we also conducted experiments on the CT-only PDDCA dataset (for details, please refer to <ref type="bibr" target="#b15">[15]</ref>), from which we collected 15 images from the offand on-site test sets of the corresponding challenge for our evaluation. As this dataset is widely used for evaluating the performance of automatic HaN OAR segmentation methods, it serves as a valuable benchmark for comparison with other state-of-the-art methods. Note that none of the images from the CT-only PDDCA dataset were used for training, and as our model expects two inputs, we substituted the missing MR modality with an empty matrix (i.e. zeros).</p><p>Implementation Details. All models were trained for all OARs using the 3d fullres configuration of nnU-Net, with the only modification that we reduced rotation around the axial axis and disabled image flipping along the sagittal plane, which eliminated segmentation errors that were previously observed for the paired (left and right) OARs. The same modification was also used with the MAML model. To ensure a fair model comparison, we set the number of filters in the encoder of the single modality baseline model to match the number of filters of the entry-level concatenation encoder. We also halved the number of filters in networks that have separate encoders so that the overall number of parameters in the proposed model and the baselines remains approximately the same (excluding the parameters in the localization part of MFM  block). Note that the MAML model, which is composed of two U-Nets, had a considerably higher number of parameters. To address the challenge of a relatively small dataset, we adopted a 4-fold cross-validation strategy without using any external training images. All models were trained until convergence, i.e. when the validation loss plateaued, and we selected the model with the best validation loss for inference.</p><p>Results. The quality of the obtained OAR segmentation masks was evaluated by computing the Dice similarity coefficient (DSC) and the 95 th -percentile Hausdorff distance (HD 95 ) against reference manual delineations, and the results for all OARs are presented in Figs. <ref type="figure" target="#fig_1">2</ref> and<ref type="figure" target="#fig_2">3</ref>, respectively. Since not all images contain all 30 OARs (due to a different field-of-view), we first calculated the mean metric for each OAR and then the overall mean across all OARs to ensure that the contributions were equally weighted. We also performed analysis of statistical significance by applying paired sample t-tests with the Bonferroni correction, presented with bars on top of the box plots (non-significant: ns (p &gt; 0.05), significant: * (0.01 &lt; p &lt; 0.05), * * (0.001 &lt; p &lt; 0.01), * * * (0.0001 &lt; p &lt; 0.001) and * * * * (p &lt; 0.0001)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>In this study, we evaluated the impact on the quality and robustness of automatic OAR segmentation in the HaN region caused by the incorporation of the MR modality into the segmentation framework. We devised a mechanism named MFM and combined it with nnU-Net as our backbone segmentation network. The choice of using nnU-Net as the backbone was based on the rationale that nnU-Net already incorporates numerous state-of-the-art DL innovations proposed in recent years, and therefore validation of the proposed MFM is more challenging in comparison to simply improving a vanilla U-Net architecture, and consequently also more valuable to the research community.</p><p>Segmentation Results. The obtained results demonstrate that our model performs best in terms of DSC (Fig. <ref type="figure" target="#fig_1">2</ref>). The resulting gains are significant compared to separate encoders and CT-only models, and were achieved with 4-fold crossvalidation, therefore reducing the chance of a favorable initialization. However, DSC has been identified not to be the most appropriate metric for evaluating the clinical adequacy of segmentations, especially when the results are close to the interrater variability <ref type="bibr" target="#b13">[13]</ref>, moreover, it is not appropriate for volumetrically small structures <ref type="bibr" target="#b11">[11]</ref>. On the other hand, distance-based metrics, such as HD 95 (Fig. <ref type="figure" target="#fig_2">3</ref>), are preferred as they better measure the shape consistency between the reference and predicted segmentations. Although MAML achieved the best results in terms of HD 95 , indicating that late fusion can efficiently merge the information from both modalities, it should be noted that MAML has a considerate advantage due to having two decoders and an additional attention fusion block compared to the baseline nnU-Net with separate encoders and a single decoder. On the other hand, our approach based on separate encoders with MFM is not far behind, with a mean HD 95 of 4.06 mm, which is more than 15% better than the early concatenation fusion. The comparison to the baseline nnU-Net with separate encoders offers the most direct evaluation of the proposed MFM. An approximate 10% improvement in HD 95 suggests that MFM allows the network to learn more informative FMs that lead to a better overall performance.</p><p>Missing Modality Scenario. The overall good performance on the HaN-Seg dataset suggests that all models are close to the maximal performance, which is bounded by the quality of reference segmentations. However, the performance on the PDDCA dataset that consists only of CT images allows us to test how the models handle the missing modality scenario and perform on an out-ofdistribution dataset, as images from this dataset were not used for training. As expected, the CT-only model performed best in its regular operating scenario, with a mean DSC of 74.7% (Fig. <ref type="figure" target="#fig_1">2</ref>) and HD 95 of 6.02 mm (Fig. <ref type="figure" target="#fig_2">3</ref>). However, significant differences can be observed between multimodal methods, where the proposed model outperformed MAML and other baselines by a large margin in both metrics. The MAML model with a mean DSC of less than 15% and HD 95 of more than 190 mm was not able to handle the missing modality scenario, whereas the MFM model performed almost as good as the CT-only model, with a mean DSC of 67.8% and HD 95 of 8.18 mm. It should be noted that we did not employ any training strategies to improve handling of missing modalities, such as swapping input images or intensity augmentations. A possible explanation is that the proposed MFM facilitates cross-modality learning, enabling nnU-Net to extract better FMs from CT images even in such extreme scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this study, we introduced MFM, a fusion module that aligns FMs from an auxiliary modality (e.g. MR) to FMs from the primary modality (e.g. CT). The proposed MFM is versatile, as it can be applied to any multimodal segmentation network. However, it has to be noted that it is not symmetrical, and therefore requires the user to specify the primary modality, which is typically the same as the primary modality used in manual delineation (i.e. in our case CT). We evaluated the performance of MFM combined with the nnU-Net backbone for segmentation of OARs in the HaN region, an important task in RT cancer treatment planning. The obtained results indicate that the performance of MFM is similar to other state-of-the-art methods, but it outperforms other multimodal methods in scenarios with one missing modality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The proposed backbone architecture is based on nnU-Net but with separate encoders for the computed tomography (CT) and magnetic resonance (MR) image, and with the proposed modality fusion module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The results in terms of the Dice similarity coefficient (DSC) for the HaN-Seg (left) and PDDCA (right) dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The results in terms of the 95 th -percentile Hausdorff distance (HD95) for the HaN-Seg (left) and PDDCA (right) dataset. (Note: Infinite values of HD95 were replaced with a maximal value over all data.)</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://doi.org/10.5281/zenodo.7442914.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://hanseg2023.grand-challenge.org.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported by the <rs type="funder">Slovenian Research Agency (ARRS)</rs> under grants <rs type="grantNumber">J2-1732</rs>, <rs type="grantNumber">P2-0232</rs> and <rs type="grantNumber">P3-0307</rs>, and partially by the <rs type="funder">Novo Nordisk Foundation</rs> under grant <rs type="grantNumber">NFF20OC0062056</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Cjmj2PJ">
					<idno type="grant-number">J2-1732</idno>
				</org>
				<org type="funding" xml:id="_bq4T6Uj">
					<idno type="grant-number">P2-0232</idno>
				</org>
				<org type="funding" xml:id="_PHfjgX5">
					<idno type="grant-number">P3-0307</idno>
				</org>
				<org type="funding" xml:id="_gr86AaK">
					<idno type="grant-number">NFF20OC0062056</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 71.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CT-based delineation of organs at risk in the head and neck region: DAHANCA, EORTC, GORTEC, HKN-PCSG, NCIC CTG, NCRI, NRG oncology and TROG consensus guidelines. Radiother</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Steenbakkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bourhis</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.radonc.2015.07.041</idno>
		<ptr target="https://doi.org/10.1016/j.radonc.2015.07.041" />
	</analytic>
	<monogr>
		<title level="j">Oncol</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="83" to="90" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">FAFNet: fully aligned fusion network for RGBD semantic segmentation based on hierarchical semantic flows</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1049/ipr2.12614</idno>
		<ptr target="https://doi.org/10.1049/ipr2.12614" />
	</analytic>
	<monogr>
		<title level="j">IET Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="32" to="41" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Head and neck cancer</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Q M</forename><surname>Chow</surname></persName>
		</author>
		<idno type="DOI">10.1056/NEJMRA1715715</idno>
		<ptr target="https://doi.org/10.1056/NEJMRA1715715" />
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="page" from="60" to="72" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HyperDense-Net: a hyper-densely connected CNN for multi-modal image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2018.2878669</idno>
		<ptr target="https://doi.org/10.1109/TMI.2018.2878669" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1116" to="1126" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unpaired multi-modal segmentation via knowledge distillation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2019.2963882</idno>
		<ptr target="https://doi.org/10.1109/TMI.2019.2963882" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2415" to="2425" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding when spatial transformer networks do not support invariance, and what to do about it</title>
		<author>
			<persName><forename type="first">L</forename><surname>Finnveden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPR48806.2021.9412997</idno>
		<ptr target="https://doi.org/10.1109/ICPR48806.2021.9412997" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition -ICPR 2020</title>
		<meeting><address><addrLine>Milan, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3427" to="3434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ACNet: attention based network to exploit complementary features for RGBD semantic segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2019.8803025</idno>
		<ptr target="https://doi.org/10.1109/ICIP.2019.8803025" />
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Image Processing -ICIP 2019</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1440" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-020-01008-z</idno>
		<ptr target="https://doi.org/10.1038/s41592-020-01008-z" />
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems -NIPS 2015</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Curran</forename><surname>Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Montréal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canada</forename><surname>Qc</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.1506.02025</idno>
		<idno>1506.02025</idno>
		<ptr target="https://doi.org/10.48550/arxiv" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unpaired cross-modality educed distillation (CMEDL) for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rimner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Deasy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2021.3132291</idno>
		<ptr target="https://doi.org/10.1109/TMI.2021.3132291" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1057" to="1068" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BIAS: transparent reporting of biomedical image analysis challenges</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kozubek</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2020.101796</idno>
		<ptr target="https://doi.org/10.1016/j.media.2020.101796" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">101796</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th International Conference on International Conference on Machine Learning -ICML 2011</title>
		<meeting><address><addrLine>Bellevue, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clinically applicable segmentation of head and neck anatomy for radiotherapy: deep learning algorithm development and validation study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zverovitch</surname></persName>
		</author>
		<idno type="DOI">10.2196/26151</idno>
		<ptr target="https://doi.org/10.2196/26151" />
	</analytic>
	<monogr>
		<title level="j">J. Med. Internet Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">26151</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HaN-Seg: the head and neck organ-at-risk CT and MR segmentation dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Podobnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Strojan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peterlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ibragimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vrtovec</surname></persName>
		</author>
		<idno type="DOI">10.1002/mp.16197</idno>
		<ptr target="https://doi.org/10.1002/mp.16197" />
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1917" to="1927" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluation of segmentation methods on head and neck CT: auto-segmentation challenge 2015</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Raudaschl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zaffino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Sharp</surname></persName>
		</author>
		<idno type="DOI">10.1002/mp.12197</idno>
		<ptr target="https://doi.org/10.1002/mp.12197" />
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2020" to="2036" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Global cancer statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<idno type="DOI">10.3322/caac.21660</idno>
		<ptr target="https://doi.org/10.3322/caac.21660" />
	</analytic>
	<monogr>
		<title level="j">CA Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="209" to="249" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised model adaptation for multimodal semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-019-01188-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-019-01188-y" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="1239" to="1285" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-modal learning from unpaired images: application to multi-organ segmentation in CT and MRI</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Valindria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV.2018.00066</idno>
		<ptr target="https://doi.org/10.1109/WACV.2018.00066" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision -WACV 2018</title>
		<meeting><address><addrLine>Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">There is more than meets the eye: selfsupervised multi-object detection and tracking with sound by distilling multimodal knowledge</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Valverde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR46437.2021.01144</idno>
		<ptr target="https://doi.org/10.1109/CVPR46437.2021.01144" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition -CVPR 2021</title>
		<meeting><address><addrLine>Nashville, TN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11607" to="11616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The evolution of care of cancers of the head and neck region: state of the science in 2020</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Knochelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Morgan</surname></persName>
		</author>
		<idno type="DOI">10.3390/cancers12061543</idno>
		<ptr target="https://doi.org/10.3390/cancers12061543" />
	</analytic>
	<monogr>
		<title level="j">Cancers</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1543</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Longitudinal detection of diabetic retinopathy early severity grade changes using deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87000-3_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87000-32" />
	</analytic>
	<monogr>
		<title level="m">OMIA 2021</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Garvin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Macgillivray</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12970</biblScope>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modality-aware mutual learning for multi-modal medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-256" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="589" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion for semantic image segmentation: a survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sidibé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mériaudeau</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.imavis.2020.104042</idno>
		<ptr target="https://doi.org/10.1016/j.imavis.2020.104042" />
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page">104042</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A review: deep learning for medical image segmentation using multi-modality fusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.array.2019.100004</idno>
		<ptr target="https://doi.org/10.1016/j.array" />
		<imprint>
			<date type="published" when="2019">Array 3-4, 100004 (2019. 2019</date>
			<biblScope unit="page">100004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
