<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Masoud</forename><surname>Mokhtari</surname></persName>
							<email>masoud@ece.ubc.ca</email>
							<idno type="ORCID">0000-0001-9471-5573</idno>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mobina</forename><surname>Mahdavi</surname></persName>
							<email>mobina@ece.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hooman</forename><surname>Vaseli</surname></persName>
							<email>hoomanv@ece.ubc.ca</email>
							<idno type="ORCID">0000-0002-8259-9488</idno>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christina</forename><surname>Luong</surname></persName>
							<email>christina.luong@ubc.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">Vancouver General Hospital</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Purang</forename><surname>Abolmaesumi</surname></persName>
							<email>purang@ece.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Teresa</forename><forename type="middle">S M</forename><surname>Tsang</surname></persName>
							<email>t.tsang@ubc.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">Vancouver General Hospital</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
							<email>rjliao@ece.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="227" to="237"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">AF27B484CB2E20FBE7116EFCE9A0B5A0</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_22</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Landmark Detection</term>
					<term>Ultrasound</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The functional assessment of the left ventricle chamber of the heart requires detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle. The key challenge of automating this task with machine learning is the sparsity of clinical labels, i.e., only a few landmark pixels in a high-dimensional image are annotated, leading many prior works to heavily rely on isotropic label smoothing. However, such a label smoothing strategy ignores the anatomical information of the image and induces some bias. To address this challenge, we introduce an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection (EchoGLAD). Our main contributions are: 1) a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs; 2) induced hierarchical supervision at different levels of granularity using a multi-level loss. We evaluate our model on a public and a private dataset under the in-distribution (ID) and out-of-distribution (OOD) settings. For the ID setting, we achieve the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on the two datasets. Our model also shows better OOD generalization than prior works with a testing MAE of 4.3 mm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Left Ventricular Hypertrophy (LVH), one of the leading predictors of adverse cardiovascular outcomes, is the condition where heart's mass abnormally increases secondary to anatomical changes in the Left Ventricle (LV) <ref type="bibr" target="#b9">[10]</ref>. These anatomical changes include an increase in the septal and LV wall thickness, and the enlargement of the LV chamber. More specifically, Inter-Ventricular Septal (IVS), LV Posterior Wall (LVPW) and LV Internal Diameter (LVID) are assessed to investigate LVH and the risk of heart failure <ref type="bibr" target="#b20">[21]</ref>. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), four landmarks on a parasternal long axis (PLAX) echo frame can characterize IVS, LVPW and LVID, and allow cardiac function assessment. To automate this, machine learning-based (ML) landmark detection methods have gained traction.</p><p>It is difficult for such ML models to achieve high accuracy due to the sparsity of positive training signals (four or six) pertaining to the correct pixel locations. In an attempt to address this, previous works use 2D Gaussian distributions to smooth the ground truth landmarks of the LV <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>. However, as shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, for LV landmark detection where landmarks are located at the wall boundaries (as illustrated by the dashed line), we argue that an isotropic Gaussian label smoothing approach confuses the model by being agnostic to the structural information of the echo frame and penalizing the model similarly whether the predictions are perpendicular or along the LV walls.</p><p>In this work, to address the challenge brought by sparse annotations and label smoothing, we propose a hierarchical framework based on Graph Neural Networks (GNNs) <ref type="bibr" target="#b24">[25]</ref> to detect LV landmarks in ultrasound images. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our framework learns useful representations on a hierarchical grid graph built from the input echo image and performs multi-level prediction tasks.</p><p>Our contributions are summarized below.</p><p>• We propose a novel GNN framework for LV landmark detection, performing message passing over hierarchical graphs constructed from an input echo; • We introduce a hierarchical supervision that is automatically induced from sparse annotations to alleviate the issue of label smoothing;</p><p>• We evaluate our model on two LV landmark datasets and show that it not only achieves state-of-the-art mean absolute errors (MAEs) (1.46 mm and 1.86 mm across three LV measurements) but also outperforms other methods in out-of-distribution (OOD) testing (achieving 4.3 mm). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Various convolution-based LV landmark detection works have been proposed. Sofka et al. <ref type="bibr" target="#b25">[26]</ref> use Fully Convolutional Networks to generate prediction heatmaps followed by a center of mass layer to produce the coordinates of the landmark locations. Another work <ref type="bibr" target="#b17">[18]</ref> uses a modified U-Net <ref type="bibr" target="#b23">[24]</ref> model to produce a segmentation map followed by a focal loss to penalize pixel predictions in close proximity of the ground truth landmark locations modulated by a Gaussian distribution. Jafari et al. <ref type="bibr" target="#b12">[13]</ref> use a similar U-Net model with Bayesian neural networks <ref type="bibr" target="#b7">[8]</ref> to estimate the uncertainty in model predictions and reject samples that exhibit high uncertainties. Gilbert et al. <ref type="bibr" target="#b5">[6]</ref> smooth ground truth labels by placing 2D Gaussian heatmaps around landmark locations at angles that are statistically obtained from training data. Lastly, Duffy et al. <ref type="bibr" target="#b3">[4]</ref> use atrous convolutions <ref type="bibr" target="#b0">[1]</ref> to make predictions for LVID, IVS and LVPW measurements.</p><p>Other related works focus on the detection of cephalometric landmarks from X-ray images. These works are highly transferable to the task of LV landmark detection as they must also detect a sparse number of landmarks. McCouat et al. <ref type="bibr" target="#b19">[20]</ref> is one of these works that abstains from using Gaussian label smoothing, but still relies on one-hot labels and treats landmark detection as a pixel-wise classification task. Chen et al. <ref type="bibr" target="#b1">[2]</ref> is another cephalometric landmark detection work that creates a feature pyramid from the intermediate layers of a ResNet <ref type="bibr" target="#b10">[11]</ref>.</p><p>Our approach is different from prior works in that it aims to avoid the issue shown in Fig. <ref type="figure" target="#fig_0">1(b</ref>) and the sparse annotations problem by the introduction of simpler auxiliary tasks to guide the main pixel-level task, so that the ML model learns the location of the landmarks without relying on Gaussian label smoothing. It further improves the representation learning via efficient messagepassing <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> of GNNs among pixels and patches at different levels without having as high a computational complexity as transformers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>. Lastly, while GNNs have never been applied to the task of LV landmark detection, they have been used for landmark detection in other domains. Li et al. <ref type="bibr" target="#b15">[16]</ref> and Lin et al. <ref type="bibr" target="#b16">[17]</ref> perform face landmark detection via modeling the landmarks with a graph and performing a cascaded regression of the locations. These methods, however, do not leverage hierarchical graphs and hierarchical supervision and instead rely on initial average landmark locations, which is not an applicable approach to echo, where the anatomy of the depicted heart can vary significantly. Additionally, Mokhtari et al. <ref type="bibr" target="#b21">[22]</ref> use GNNs for the task of EF prediction from echo cine series. However, their work focuses on regression tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setup</head><p>We consider the following supervised setting for LV wall landmark detection. We have a dataset D = {X, Y }, where |D| = n is the number of {x i , y i } pairs such that x i ∈ X, y i ∈ Y , and i ∈ <ref type="bibr">[1, n]</ref>. Each x i ∈ R H×W is an echo image of the heart, where H and W are height and width of the image, respectively, and each y i is the set of four point coordinates [(h i 1 , w i 1 ), (h i 2 , w i 2 ), (h i 3 , w i 3 ), (h i 4 , w i 4 )] indicating the landmark locations in x i . Our goal is to learn a function f : R H×W → R 4×2 that predicts the four landmark coordinates for each input image. A figure in the supp. material further clarifies how the model generates landmark location heatmaps on different scales (Fig. <ref type="figure" target="#fig_1">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Overview</head><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, each input echo frame is represented by a hierarchical grid graph where each sub-graph corresponds to the input echo frame at a different resolution. The model produces heatmaps over both the main pixel-level task as well as the coarse auxiliary tasks. While the pixel-level heatmap prediction is of main interest, we use a hierarchical multi-level loss approach where the model's prediction over auxiliary tasks is used during training to optimize the model through comparisons to coarser versions of the ground truth. The intuition behind such an approach is that the model learns nuances in the data by performing landmark detection on the easier auxiliary tasks and uses this established reasoning when performing the difficult pixel-level task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Graph Construction</head><p>To learn representations that better capture the dependencies among pixels and patches, we introduce a hierarchical grid graph along with multi-level prediction tasks. As an example, the simplest task consists of a grid graph with only four nodes, where each node corresponds to four equally-sized patches in the original echo image. In the main task (the one that is at the bottom in Fig. <ref type="figure" target="#fig_1">2</ref> and is the most difficult), the number of nodes is equal to the total number of pixels.</p><p>More formally, let us denote a graph as G = (V, E), where V is the set of nodes, and E is the set of edges in the graph such that if v i , v j ∈ V and there is an edge from v i to v j , then e i,j ∈ E. To build hierarchical task representations, for each image x ∈ X and the ground truth y ∈ Y , K different auxiliary graphs G k (V k , E k ) are constructed using the following steps for each k ∈ [1, K]:</p><formula xml:id="formula_0">1. 2 k ×2 k = 4 k nodes are added to V k to represent each patch in the image. Note</formula><p>that the larger values of k correspond to graphs of finer resolution, while the smaller values of k correspond to coarser graphs. 2. Grid-like, undirected edges are added such that e m-1,q , e m+1,q , e m,q-1 , e m,q+1 ∈ E k for each m, q ∈ [1 . . . 2 k ] if these neighbouring nodes exist in the graph (border nodes will not have four neighbouring nodes). 3. A patch feature embedding z k j , where j ∈ [1 . . . 4 k ] is generated and associated with that patch (node) v j ∈ V k . The patch feature construction technique is described in Sect. 3.4. 4. Binary node labels ŷk ∈ {0, 1} 4 k ×4 are generated such that ŷkj = 1 if at least one of the ground truth landmarks in y is contained in the patch associated with node v j ∈ V k . Note that for each auxiliary graph, four different one-hot labels are predicted, which correspond to each of the four landmarks required to characterize LV measurements.</p><p>The main graph, G main , has a grid structure and contains H × W nodes regardless of the value of K, where each node corresponds to a pixel in the image. Additionally, to allow the model to propagate information across levels, we add inter-graph edges such that each node in a graph is connected to four nodes in the corresponding region in the next finer graph as depicted in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Node Feature Construction</head><p>The graph representation described in Sect. 3.3 is not complete without proper node features, denoted by z ∈ R |V |×d , characterizing patches or pixels of the image. To achieve this, the grey-scale image is initially expanded in the channel dimension using a CNN. The features are then fed into a U-Net where the decoder part is used to obtain node features such that deeper layer embeddings correspond to the node features for the finer graphs. This means that the main pixel-level graph would have the features of the last layer of the network. A figure clarifying node feature construction is provided in the supp. material (Fig. <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Hierarchical Message Passing</head><p>We now introduce how we perform message passing on our constructed hierarchical graph using GNNs to learn node representations for predicting landmarks. The whole hierarchical graph created for each sample, i.e., the main graph, auxiliary graphs, and cross-level edges, are collectively denoted as G i , where i ∈ <ref type="bibr">[1, . . . , n]</ref>. Each G i is fed into GNN layers followed by an MLP:</p><formula xml:id="formula_1">h l+1 nodes = ReLU(GNN l (G i ), h l nodes ), l ∈ [0, . . . , L] (1) h out = σ(MLP(h nodes L+1 )), (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where σ is the Sigmoid function, h l nodes ∈ R |V G i |×d is the set of d-dimensional embeddings for all nodes in the graph at layer l, and h out ∈ [0, 1] |V G i |×4 is the four-channel prediction for each node with each channel corresponding to a heatmap for each of the pixel landmarks. The initial node features h 1 nodes are set to the features z described in Sects. 3.3 and 3.4. The coordinates (x p out , y p out ) for each landmark location p ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> are obtained by taking the expected value of individual heatmaps h p out along the x and y directions such that:</p><formula xml:id="formula_3">x p out = |V G i | s=1 softmax(h p out ) s * loc x (s),<label>(3)</label></formula><p>where similar operations are performed in the y direction for y p out . Here, we vectorize the 2D heatmap into a single vector and then feed it to the softmax. loc x and loc y return the x and y positions of a node in the image. It must be noted that unlike some prior works such as Duffy et al. <ref type="bibr" target="#b3">[4]</ref> that use postprocessing steps such as imposing thresholds on the heatmap values, our work directly uses the output heatmaps to find the final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training and Objective Functions</head><p>To train the network, we leverage two types of objective functions. 1) Weighted Binary Cross Entropy (BCE): Since the number of landmark locations is much smaller than non-landmark locations, we use a weighted BCE loss; 2) L2 regression of landmark coordinates: We add a regression objective which is the L2 loss between the predicted coordinates and the ground truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Internal Dataset: Our private dataset contains 29,867 PLAX echo frames, split in a patient-exclusive manner with 23824, 3004, and 3039 frames for training, validation, and testing, respectively. External Dataset: The public Unity Imaging Collaborative (UIC) <ref type="bibr" target="#b11">[12]</ref> LV landmark dataset consists of a combination of 3822 end-systolic and end-diastolic PLAX echo frames acquired from seven British echocardiography labs. The provided splits contain 1613, 298, and 1911 training, validation, and testing samples, respectively. For both datasets, we down-sample the frames to a fixed size of 224 × 224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Our model creates K = 7 auxiliary graphs. For the node features, the initial single-layer CNN uses a kernel size of 3 and zero-padding to output features with a dimension of 224 × 224 × 4 (C = 4). The U-Net's encoder contains 7 layers with 128 × 128, 64 × 64, 32 × 32, 16 × 16, 8 × 8, 4 × 4, and 2 × 2 spatial dimensions, and 8, 16, 32, 64, 128, 256, and 512 number of channels, respectively. Three Graph Convolutional Network (GCN) <ref type="bibr" target="#b14">[15]</ref> layers (L = 3) with a hidden node dimension of 128 are used. To optimize the model, we use the Adam optimizer <ref type="bibr" target="#b13">[14]</ref> with an initial learning rate of 0.001, β of (0.9, 0.999) and a weight decay of 0.0001, and for the weighted BCE loss, we use a weight of 9000. The model is implemented using PyTorch <ref type="bibr" target="#b22">[23]</ref> and Pytorch Geometric <ref type="bibr" target="#b4">[5]</ref> and is trained on two 32-GB Nvidia Titan GPUs. Our code-base is publicly available at https://github.com/ MasoudMo/echoglad.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We evaluate models using Mean Absolute Error (MAE) in mm, and Mean Percent Error (MPE) in percents, which is formulated as MPE = 100×</p><p>|L pred -Ltrue| Ltrue , where L pred and L true are the prediction and ground truth values for every measurement. We also report the Success Detection Rate (SDR) for LVID for 2 and 6 mm thresholds. This rate shows the percentage of samples where the absolute error between ground truth and LVID predictions is below the specific threshold. These thresholds are chosen based on the healthy ranges for IVS (0.6-1.1cm), LVID (2.0-5.6cm), and LVPW (0.6-0.1cm). Hence, the 2 mm threshold provides a stringent evaluation of the models, while the 6 mm threshold facilitates the assessment of out-of-distribution performance.</p><p>In-Distribution (ID) Quantitative Results. In Table <ref type="table" target="#tab_0">1</ref>, we compare the performance of our model with previous works in the ID setting where the training and test sets come from the same distribution (e.g., the same clinical setting), we separately train and test the models on the private and the public dataset. The results for the public dataset are provided in the supp. material (Table <ref type="table" target="#tab_0">1</ref>).</p><p>Out-of-Distribution (OOD) Quantitative Results. To investigate the generalization ability of our model compared to previous works, we train all models on the private dataset (which consists of a larger number of samples compared to UIC), and test the trained models on the public UIC dataset as shown in Table <ref type="table" target="#tab_1">2</ref>. Based on our visual assessment, the UIC dataset looks very different compared to the private dataset, thus serving as an OOD test-bed. Qualitative Results. Failure cases are shown in supp. material (Fig. <ref type="figure">3</ref>).</p><p>Ablation Studies. In Table <ref type="table" target="#tab_2">3</ref>, we show the benefits of a hierarchical graph representation with a multi-scale objective for the task of LV landmark detection.</p><p>We provide a qualitative view of the ablation study in supp. material (Fig. <ref type="figure">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this work, we introduce a novel hierarchical GNN for LV landmark detection. The model performs better than the state-of-the-art on most measurements without relying on label smoothing. We attribute this gain in performance to two main contributions. First, our choice of representing each frame with a hierarchical graph has facilitated direct interaction between pixels at differing scales. This approach is effective in capturing the nuanced dependencies amongst the landmarks, bolstering the model's performance. Secondly, the implementation of a multi-scale objective function as a supervisory mechanism has enabled the model to construct a superior inductive bias. This approach allows the model to leverage simpler tasks to optimize its performance in the more challenging pixel-level landmark detection task.</p><p>For future work, we believe that the scalability of the framework for higherresolution images must be studied. Additionally, extension of the model to video data can be considered since the concept of intra-scale and inter-scale edges connecting nodes could be extrapolated to include temporal edges linking similar spatial locations across frames. Such an approach could greatly enhance the model's performance in unlabeled frames, mainly through the enforcement of consistency in predictions from frame to frame.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) IVS, LVID and LVPW measurements visualized on a PLAX echo frame. (b) If the wall landmark labels are smoothed by an isotropic Gaussian distribution, points along the visualized wall and ones perpendicular are penalized equally. Ideally, points along the walls must be penalized less.</figDesc><graphic coords="2,89,79,54,56,244,12,120,58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our proposed model architecture. Hierarchical Feature Construction provides node features for the hierarchical graph representation of each echo frame where the nodes in the main graph correspond to pixels in the image, and nodes in the auxiliary graphs correspond to patches of different granularity in the image. Graph Neural Networks are used to process the hierarchical graph representation and produce node embeddings for the auxiliary graphs and the main graph. Multi-Layer Perceptrons (MLPs) are followed by a Sigmoid output function to map the node embeddings into landmark heatmaps of different granularity over the input echo frame.</figDesc><graphic coords="3,55,98,133,55,340,18,132,13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on the private test set for models trained on the private training set. We see that our model has the best average performance over the three measurements, which shows the superiority of our model in the in-distribution setting for high-data regime.</figDesc><table><row><cell>Model</cell><cell cols="2">MAE [mm] ↓</cell><cell cols="2">MPE [%] ↓</cell><cell cols="2">SDR[%] of LVID &lt;↑</cell></row><row><cell></cell><cell cols="6">LVID IVS LVPW LVID IVS LVPW 2.0 mm 6.0 mm</cell></row><row><cell>Gilbert et al. [6]</cell><cell>2.9</cell><cell>1.4 1.4</cell><cell>6.5</cell><cell>14.5 15.2</cell><cell>48.1</cell><cell>88.9</cell></row><row><cell>Lin et al. [18]</cell><cell>9.4</cell><cell>11.2 9.0</cell><cell cols="2">21.2 116.5 92.9</cell><cell>26.0</cell><cell>49.1</cell></row><row><cell cols="2">McCouat et al. [20] 2.2</cell><cell>1.3 1.4</cell><cell>4.8</cell><cell>13.5 15.1</cell><cell>58.3</cell><cell>93.9</cell></row><row><cell>Chen et al. [2]</cell><cell>2.3</cell><cell>1.2 1.2</cell><cell>5.2</cell><cell>12.6 13.8</cell><cell>60.4</cell><cell>92.6</cell></row><row><cell>Duffy et al. [4]</cell><cell>2.5</cell><cell>1.2 1.2</cell><cell>5.4</cell><cell>13.2 13.5</cell><cell>52.1</cell><cell>93.0</cell></row><row><cell>Ours</cell><cell>2.2</cell><cell>1.1 1.1</cell><cell>4.8</cell><cell>11.2 12.2</cell><cell>62.4</cell><cell>94.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results on the public UIC test set for models trained on the private training set. This table shows the out-of-distribution performance of the models when trained on a larger dataset and tested on a smaller external dataset. We can see that in this case, our model outperforms previous works by a large margin, which attests to the generalizability of our framework.</figDesc><table><row><cell>Model</cell><cell cols="2">MAE [mm] ↓</cell><cell>MPE [%] ↓</cell><cell cols="2">SDR[%] of LVID &lt; ↑</cell></row><row><cell></cell><cell cols="5">LVID IVS LVPW LVID IVS LVPW 2.0 mm 6.0 mm</cell></row><row><cell>Gilbert et al. [6]</cell><cell>9.5</cell><cell>4.8 4.1</cell><cell>23.5 32.3 26.8</cell><cell>22.5</cell><cell>52.2</cell></row><row><cell>Lin et al. [18]</cell><cell cols="2">51.5 51.7 41.3</cell><cell cols="2">121.0 375.8 298.0 11.3</cell><cell>24.6</cell></row><row><cell cols="2">McCouat et al. [20] 5.9</cell><cell>3.6 4.4</cell><cell>18.5 30.5 36.4</cell><cell>34.6</cell><cell>72.3</cell></row><row><cell>Chen et al. [2]</cell><cell>7.4</cell><cell>5.3 6.9</cell><cell>22.5 49.4 62.4</cell><cell>28.9</cell><cell>65.3</cell></row><row><cell>Duffy et al. [4]</cell><cell cols="2">13.7 4.1 5.5</cell><cell>36.8 36.4 45.4</cell><cell>6.2</cell><cell>20.6</cell></row><row><cell>Ours</cell><cell>5.8</cell><cell>2</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>.8 4.3 18.4 23.8 34.6 35.8 74.9Table 3 .</head><label>3</label><figDesc>Ablation results on the validation set of our private dataset. Vanilla U-Net uses a simple U-Net model, while U-Net Main Graph only uses the pixel-level graph (no aux. graphs). Main Model is our proposed approach. Lastly, Single-Scale Loss has the same framework as the Main Model but only computes the loss for the model's predictions on the main graph (no multi-scale loss).</figDesc><table><row><cell>Model</cell><cell>MPE [%]</cell></row><row><cell></cell><cell>LVID IVS</cell><cell>LVPW</cell></row><row><cell>Vanilla U-Net</cell><cell cols="2">5.31 13.17 13.47</cell></row><row><cell cols="3">U-Net Main Graph 4.98 11.67 12.78</cell></row><row><cell>Single-Scale Loss</cell><cell cols="2">5.41 12.37 12.8</cell></row><row><cell>Main Model</cell><cell cols="2">4.91 11.45 12.36</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 22.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cephalometric landmark detection by attentive feature pyramid fusion and regression-voting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32248-9_97</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32248-997" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11766</biblScope>
			<biblScope unit="page" from="873" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High-throughput precision phenotyping of left ventricular hypertrophy with cardiovascular deep learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Cardiol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="386" to="395" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated left ventricle dimension measurement in 2D cardiac ultrasound via an anatomically meaningful CNN approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eikvil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Aase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Samset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcleod</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32875-7_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32875-74" />
	</analytic>
	<monogr>
		<title level="m">PIPPI/SUSI -2019</title>
		<editor>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11798</biblScope>
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bayesian neural networks: an introduction and survey</title>
		<author>
			<persName><forename type="first">E</forename><surname>Goan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Mengersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pudlo</surname></persName>
		</author>
		<author>
			<persName><surname>Robert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-42553-1_3</idno>
		<idno>978-3-030-42553-1 3</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">Case Studies in Applied Bayesian Data Science</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">P</forename></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2259</biblScope>
			<biblScope unit="page" from="45" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An efficient deep landmark detection network for PLAX EF estimation using sparse annotations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A D</forename><surname>Goco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2022: Image-Guided Procedures, Robotic Interventions, and Modeling</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Linte</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Siewerdsen</surname></persName>
		</editor>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">12034</biblScope>
			<biblScope unit="page">120340</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From left ventricular hypertrophy to congestive heart failure: Management of hypertensive heart disease</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gradman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alfayoumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress Cardiovas. Dis</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="326" to="341" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
	<note>Update</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automated left ventricular dimension assessment using artificial intelligence developed and validated by a UK-wide collaborative</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circ. Cardiovasc. Imaging</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="11951" to="e11951" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-land: uncertainty-driven video landmark detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Jafari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="793" to="804" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structured landmark detection via topology-adapting deep graph learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58545-7_16</idno>
		<idno>978-3-030-58545-7 16</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12354</biblScope>
			<biblScope unit="page" from="266" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structure-coherent deep feature learning for robust face alignment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5313" to="5326" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reciprocal landmark detection and tracking with extremely few annotations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15165" to="15174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2021-10">oct 2021</date>
			<biblScope unit="page" from="9992" to="10002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Contour-hugging heatmaps for landmark detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mccouat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Voiculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20565" to="20573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Echocardiographic diagnosis of left ventricular hypertrophy</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Pickard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Echognn: explainable ejection fraction estimation with graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="360" to="369" />
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>MICCAI</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pytorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fully convolutional regression network for accurate detection of measurement points</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sofka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rothberg</surname></persName>
		</author>
		<editor>Cardoso, M.J., et al.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<idno type="DOI">10.1007/978-3-319-67558-9_30</idno>
		<idno>DLMIA/ML-CDS -2017</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-67558-930" />
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">10553</biblScope>
			<biblScope unit="page" from="258" to="266" />
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
