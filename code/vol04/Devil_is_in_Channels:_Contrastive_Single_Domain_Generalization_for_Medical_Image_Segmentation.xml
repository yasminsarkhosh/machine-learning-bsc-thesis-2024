<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation</title>
				<funder ref="#_WHp6M9U">
					<orgName type="full">Key Research and Development Program of Shaanxi Province, China</orgName>
				</funder>
				<funder ref="#_s5wUfMX">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Z6JPhJU">
					<orgName type="full">Key Technologies Research and Development Program</orgName>
				</funder>
				<funder ref="#_Mhwxtqs">
					<orgName type="full">Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shishuai</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zehui</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yong</forename><surname>Xia</surname></persName>
							<email>yxia@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="14" to="23"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6B1CDB64345586113A1B6CFA7B2B0809</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Single domain generalization</term>
					<term>Medical image segmentation</term>
					<term>Contrastive learning</term>
					<term>Feature disentanglement</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning-based medical image segmentation models suffer from performance degradation when deployed to a new healthcare center. To address this issue, unsupervised domain adaptation and multisource domain generalization methods have been proposed, which, however, are less favorable for clinical practice due to the cost of acquiring target-domain data and the privacy concerns associated with redistributing the data from multiple source domains. In this paper, we propose a Channel-level Contrastive Single Domain Generalization (C 2 SDG) model for medical image segmentation. In C 2 SDG, the shallower features of each image and its style-augmented counterpart are extracted and used for contrastive training, resulting in the disentangled style representations and structure representations. The segmentation is performed based solely on the structure representations. Our method is novel in the contrastive perspective that enables channel-wise feature disentanglement using a single source domain. We evaluated C 2 SDG against six SDG methods on a multi-domain joint optic cup and optic disc segmentation benchmark. Our results suggest the effectiveness of each module in C 2 SDG and also indicate that C 2 SDG outperforms the baseline and all competing methods with a large margin. The code is available at https://github.com/ShishuaiHu/CCSDG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It has been widely recognized that the success of supervised learning approaches, such as deep learning, relies on the i.i.d. assumption for both training and test samples <ref type="bibr" target="#b10">[11]</ref>. This assumption, however, is less likely to be held on medical image segmentation tasks due to the imaging distribution discrepancy caused by non-uniform characteristics of the imaging equipment, inconsistent skills of the operators, and even compromise with factors such as patient radiation exposure and imaging time <ref type="bibr" target="#b13">[14]</ref>. Therefore, the imaging distribution discrepancy across different healthcare centers renders a major hurdle that prevents deep learningbased medical image segmentation models from clinical deployment <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>To address this issue, unsupervised domain adaptation (UDA) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref> and multi-source domain generalization (MSDG) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref> have been studied. UDA needs access to the data from source domain(s) and unlabeled target domain, while MSDG needs access to the data from multiple source domains. In clinical practice, both settings are difficult to achieve, considering the cost of acquiring target-domain data and the privacy concerns associated with redistributing the data from multiple source domains <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>By contrast, single domain generalization (SDG) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> is a more practical setting, under which only the labeled data from one source domain are used to train the segmentation model, which is thereafter applied to the unseen target-domain data. The difficulty of SDG is that, due to the existence of imaging distribution discrepancy, the trained segmentation model is prone to overfit the source-domain data but generalizes poorly on target-domain data. An intuitive solution is to increase the diversity of training data by performing data augmentation at the image-level <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>. This solution has recently been demonstrated to be less effective than a more comprehensive one, i.e., conducting domain adaptation on both image-and feature-levels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref>. As a more comprehensive solution, Dual-Norm <ref type="bibr" target="#b22">[23]</ref> first augments source-domain images into 'source-similar' images with similar intensities and 'source-dissimilar' images with inverse intensities, and then processes these two sets of images using different batch normalization layers in the segmentation model. Although achieving promising performance in cross-modality CT and MR image segmentation, Dual-Norm may not perform well under the cross-center SDG setting, where the source-and target-domain data are acquired at different healthcare centers, instead of using different imaging modalities. In this case, the 'source-dissimilar' images with inverse intensities do not really exist, and it remains challenging to determine the way to generate both 'source-similar' and 'source-dissimilar' images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. To address this challenge, we suggest resolving 'similar' and 'dissimilar' from the perspective of contrastive learning. Given a source image and its style-augmented counterpart, only the structure representations between them are 'similar', whereas their style representations should be 'dissimilar'. Based on contrastive learning, we can disentangle and then discard the style representations, which are structure-irrelevant, using images from only a single domain.</p><p>Specifically, to disentangle the style representations, we train a segmentation model, i.e., the baseline, using single domain data and assess the impact of the features extracted by the first convolutional layer on the segmentation performance, since shallower features are believed to hold more style-sensitive information <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. A typical example was given in Fig. <ref type="figure" target="#fig_0">1(a)</ref> and<ref type="figure">(b)</ref>, where the green line is the average Dice score obtained on the target domain (the BASE2 dataset) versus the index of the feature channel that has been dropped. It reveals that, in most cases, removing a feature does not affect the model performance, indicating that the removed feature is redundant. For instance, the performance even increases slightly after removing the 24th channel. This observation is consistent with the conclusion that there exists a sub-network that can achieve comparable performance <ref type="bibr" target="#b5">[6]</ref>. On the contrary, it also shows that some features, such as the 36th channel, are extremely critical. Removing this feature results in a significant performance drop. We visualize the 24th and 36th channels obtained on three target-domain images in Fig. <ref type="figure" target="#fig_0">1(c</ref>) and (d), respectively. It shows that the 36th channel is relatively 'clean' and most structures are visible on it, whereas the 24th channel contains a lot of 'shadows'. The poor quality of the 24th channel can be attributed to the fact that the styles of source-and target-domain images are different and the style representation ability learned on source-domain images cannot generalize well on target-domain images. Therefore, we suggest that the 24th channel is more style-sensitive, whereas the 36th channel contains more structure information. This phenomenon demonstrates that 'the devil is in channels'. Fortunately, contrastive learning provides us a promising way to identify and expel those style-sensitive 'devil' channels from the extracted image features.</p><p>In this paper, we incorporate contrastive feature disentanglement into a segmentation backbone and thus propose a novel SDG method called Channel-level Contrastive Single Domain Generalization (C 2 SDG) for joint optic cup (OC) and optic disc (OD) segmentation on fundus images. In C 2 SDG, the shallower features of each image and its style-augmented counterpart are extracted and used for contrastive training, resulting in the disentangled style representations and structure representations. The segmentation is performed based solely on the structure representations. This method has been evaluated against other SDG methods on a public dataset and improved performance has been achieved. Our main contributions are three-fold: (1) we propose a novel contrastive perspective for SDG, enabling contrastive feature disentanglement using the data from only a single domain; (2) we disentangle the style representations and structure representations explicitly and channel-wisely, and then diminish the impact of style-sensitive 'devil' channels; and (3) our C 2 SDG outperforms the baseline and six state-of-the-art SDG methods on the joint OC/OD segmentation benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition and Method Overview</head><p>Let the source domain be denoted by D s = {x s i , y s i } Ns i=1 , where x s i is the i-th source domain image, and y s i is its segmentation mask. Our goal is to train a segmentation model F θ : x → y on D s , which can generalize well to an unseen target domain D t = {x t i } Nt i=1 . The proposed C 2 SDG mainly consists of a segmentation backbone, a style augmentation (StyleAug) module, and a contrastive feature disentanglement (CFD) module. For each image x s , the StyleAug module generates its style-augmented counterpart x a , which shares the same structure but different style to x s . Then a convolutional layer extracts high-dimensional representations f s and f a from x s and x a . After that, f s and f a are fed to the CFD module to perform contrastive training, resulting in the disentangled style representations f sty and structure representations f str . The segmentation backbone only takes f str as its input and generates the segmentation prediction y. Note that, although we take a U-shape network <ref type="bibr" target="#b4">[5]</ref> as the backbone for this study, both StyleAug and CFD modules are modularly designed and can be incorporated into most segmentation backbones. The diagram of our C 2 SDG is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. We now delve into its details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Style Augmentation</head><p>Given a batch of source domain data {x s n } NB n=1 , we adopt a series of stylerelated data augmentation approaches, i.e., gamma correction and noise addition in BigAug <ref type="bibr" target="#b20">[21]</ref>, and Bezier curve transformation in SLAug <ref type="bibr" target="#b14">[15]</ref>, to generate {x BA n } NB n=1 and {x SL n } NB n=1 . Additionally, to fully utilize the style diversity inside single domain data, we also adopt low-frequency components replacement <ref type="bibr" target="#b19">[20]</ref> within a batch of source domain images. Specifically, We reverse {x s n } NB n=1 to match x s n with x s r , where r = N B + 1 -n to ensure x s r provides a different reference style. Then we transform x s n and x s r to the frequency domain and exchange their low-frequency components Low(Amp(x s ); β) in the amplitude map, where β is the cut-off ratio between low and high-frequency components and is randomly selected from (0.05, 0.15]. After that, we recover all low-frequency exchanged images to generate</p><formula xml:id="formula_0">{x F R n } NB n=1 . The style-augmented images batch {x a n } NB n=1 is set to {x BA n } NB n=1 , {x SL n } NB n=1</formula><p>, and {x F R n } NB n=1 in turn to perform contrastive training and segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contrastive Feature Disentanglement</head><p>Given x s and x a , we use a convolutional layer with parameter θ c to generate their shallow features f s and f a , which are 64-channel feature maps for this study.</p><p>Then we use a channel mask prompt P ∈ R 2×64 to disentangle each shallow feature map f into style representation f sty and structure representation f str explicitly channel-wisely</p><formula xml:id="formula_1">f sty = f × P sty = f × SM ( P τ ) 1 f str = f × P str = f × SM ( P τ ) 2 , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where SM (•) is a softmax function, the subscript i denotes i-th channel, and τ = 0.1 is a temperature factor that encourages P sty and P str to be binaryelement vectors, i.e., approximately belonging to {0, 1} 64 . After channel-wise feature disentanglement, we have {f s sty , f s str } from x s and {f a sty , f a str } from x a . It is expected that (a) f s sty and f a sty are different since we want to identify them as the style-sensitive 'devil' channels, and (b) f s str and f a str are the same since we want to identify them as the style-irrelevant channels and x s and x a share the same structure. Therefore, we design two contrastive loss functions L sty and L str</p><formula xml:id="formula_3">L str = |P roj(f s str ) -P roj(f a str )| L sty = -|P roj(f s sty ) -P roj(f a sty )|,<label>(2)</label></formula><p>where the P roj(•) with parameters θ p reduces the dimension of f str and f sty . Only f s str and f a str are fed to the segmentation backbone with parameters θ seg to generate the segmentation predictions y s and y a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training and Inference</head><p>Training. For the segmentation task, we treat OC/OD segmentation as two binary segmentation tasks and adopt the binary cross-entropy loss as our objective</p><formula xml:id="formula_4">L ce (y, y) = -( y log y + (1 -y) log (1 -y)) (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where y represents the segmentation ground truth and y is the prediction. The total segmentation loss can be calculated as</p><formula xml:id="formula_6">L seg = L s seg + L a seg = L ce (y s , y s ) + L ce (y s , y a ).<label>(4)</label></formula><p>During training, we alternately minimize L seg to optimize {θ c , P, θ seg }, and minimize L str + L sty to optimize {P, θ p }.</p><p>Inference. Given a test image x t , its shallow feature map f t can be extracted by the first convolutional layer. Based on f t , the optimized channel mask prompt P can separate it into f t sty and f t str . Only f t str is fed to the segmentation backbone to generate the segmentation prediction y t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Materials and Evaluation Metrics. The multi-domain joint OC/OD segmentation dataset RIGA+ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref> was used for this study. It contains annotated fundus images from five domains, including 195 images from BinRushed, 95 images from Magrabia, 173 images from BASE1, 148 images from BASE2, and 133 images from BASE3. Each image was annotated by six raters, and only the first rater's annotations were used in our experiments. We chose BinRushed and Magrabia, respectively, as the source domain to train the segmentation model, and evaluated the model on the other three (target) domains. We adopted the Dice Similarity Coefficient (D, %) to measure the segmentation performance.</p><p>Implementation Details. The images were center-cropped and normalized by subtracting the mean and dividing by the standard deviation. The input batch contains eight images of size 512 × 512. The U-shape segmentation network, whose encoder is a modified ResNet-34, was adopted as the segmentation backbone of our C 2 SDG and all competing methods for a fair comparison. The Table <ref type="table">1</ref>. Average performance of three trials of our C 2 SDG and six competing methods in joint OC/OD segmentation using BinRushed (row 2-row 9) and Magrabia (row 10-row 17) as source domain, respectively. Their standard deviations are reported as subscripts. The performance of 'Intra-Domain' and 'w/o SDG' is displayed for reference. The best results except for 'Intra-Domain' are highlighted in blue. projector in our CFD module contains a convolutional layer followed by a batch normalization layer, a max pooling layer, and a fully connected layer to convert f sty and f str to 1024-dimensional vectors. The SGD algorithm with a momentum of 0.99 was adopted as the optimizer. The initial learning rate was set to lr 0 = 0.01 and decayed according to lr = lr 0 ×(1-e/E) 0.9 , where e is the current epoch and E = 100 is the maximum epoch. All experiments were implemented using the PyTorch framework and performed with one NVIDIA 2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Comparative Experiments. We compared our C 2 SDG with two baselines, including 'Intra-Domain' (i.e., training and testing on the data from the same target domain using 3-fold cross-validation) and 'w/o SDG' (i.e., training on the source domain and testing on the target domain), and six SDG methods, including BigAug <ref type="bibr" target="#b20">[21]</ref>, CISDG <ref type="bibr" target="#b12">[13]</ref>, ADS <ref type="bibr" target="#b18">[19]</ref>, MaxStyle <ref type="bibr" target="#b1">[2]</ref>, SLAug <ref type="bibr" target="#b14">[15]</ref>, and Dual-Norm <ref type="bibr" target="#b22">[23]</ref>. In each experiment, only one source domain is used for training, ensuring that only the data from a single source domain can be accessed during training. For a fair comparison, all competing methods are re-implemented using the same backbone as our C 2 SDG based on their published code and paper. The results of C 2 SDG and its competitors were given in Table <ref type="table">1</ref>. It shows that C 2 SDG improves the performance of 'w/o SDG' with a large margin and outperforms all competing SDG methods. We also visualize the segmentation predictions generated by our C 2 SDG and six competing methods in Fig. <ref type="figure" target="#fig_2">3</ref>. It reveals that our C 2 SDG can produce the most accurate segmentation map.</p><p>Ablation Analysis. To evaluate the effectiveness of low-frequency components replacement (FR) in StyleAug and CFD, we conducted ablation experiments using BinRushed and Magrabia as the source domain, respectively. The average performance is shown in Table <ref type="table" target="#tab_2">2</ref>. The performance of using both BigAug and SLAug is displayed as 'Baseline'. It reveals that both FR and CFD contribute to performance gains.</p><p>Analysis of CFD. Our CFD is modularly designed and can be incorporated into other SDG methods. We inserted our CFD to ADS <ref type="bibr" target="#b18">[19]</ref> and SLAug <ref type="bibr" target="#b14">[15]</ref>, respectively. The performance of these two approaches and their variants, denoted as C 2 -ADS and C 2 -SLAug, was shown in Table <ref type="table" target="#tab_3">3</ref>. It reveals that our CFD module can boost their ability to disentangle structure representations and improve the segmentation performance on the target domain effectively. We also adopted     dropout (see Table <ref type="table" target="#tab_4">4</ref>). It shows that the adversarial training strategy fails to perform channel-level feature disentanglement, due to the limited training data <ref type="bibr" target="#b2">[3]</ref> for SDG. Nonetheless, our channel-level contrastive learning strategy achieves the best performance compared to other strategies, further confirming the effectiveness of our CFD module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel SDG method called C 2 SDG for medical image segmentation. In C 2 SDG, the StyleAug module generates style-augmented counterpart of each source domain image and enables contrastive learning, the CFD module performs channel-level style and structure representations disentanglement via optimizing a channel prompt P, and the segmentation is performed based solely on structure representations. Our results on a multi-domain joint OC/OD segmentation benchmark indicate the effectiveness of StyleAug and CFD and also suggest that our C 2 SDG outperforms the baselines and six completing SDG methods with a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Average OD (a) and OC (b) segmentation performance (Dice%) obtained on unseen target domain (BASE2) versus removed channel of shallow features. The Dice scores obtained before and after dropping a channel are denoted by 'Baseline' and 'DASC', respectively. The 24th channel (c) and 36th channel (d) obtained on three target-domain images are visualized.</figDesc><graphic coords="2,57,78,154,07,336,91,53,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Diagram of our C 2 SDG. The rectangles in blue and green represent the convolutional layer and the segmentation backbone, respectively. The cubes represent different features. The projectors with parameters θ p in (b) are omitted for simplicity. (Color figure online)</figDesc><graphic coords="4,55,98,53,78,340,30,188,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of segmentation masks predicted by our C 2 SDG and six competing methods, together with ground truth.</figDesc><graphic coords="8,55,98,304,04,340,18,112,57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Average performance of our C 2 SDG and three variants.</figDesc><table><row><cell cols="2">Methods Average</cell></row><row><cell></cell><cell>D OD D OC</cell></row><row><cell cols="2">Baseline 94.13 81.62</cell></row><row><cell>w/o FR</cell><cell>95.07 84.90</cell></row><row><cell cols="2">w/o CFD 95.07 84.83</cell></row><row><cell>Ours</cell><cell>95.31 86.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Average performance of ADS, SLAug, and their two variants.</figDesc><table><row><cell>Methods</cell><cell>Average</cell></row><row><cell></cell><cell>D OD D OC</cell></row><row><cell>ADS [19]</cell><cell>92.24 79.87</cell></row><row><cell>C 2 -ADS</cell><cell>93.76 81.35</cell></row><row><cell cols="2">SLAug [15] 94.06 81.80</cell></row><row><cell cols="2">C 2 -SLAug 94.24 83.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Average performance of using contrastive and other strategies.</figDesc><table><row><cell>Methods</cell><cell>Average</cell></row><row><cell></cell><cell>D OD D OC</cell></row><row><cell>Baseline</cell><cell>95.07 84.83</cell></row><row><cell>Dropout</cell><cell>95.14 84.95</cell></row><row><cell cols="2">Adversarial 90.27 78.47</cell></row><row><cell>Ours</cell><cell>95.31 86.05</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62171377</rs>, in part by the <rs type="funder">Key Technologies Research and Development Program</rs> under Grant <rs type="grantNumber">2022YFC2009903/2022YFC2009900</rs>, in part by the <rs type="funder">Key Research and Development Program of Shaanxi Province, China</rs>, under Grant <rs type="grantNumber">2022GY-084</rs>, in part by the <rs type="funder">Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University</rs> under Grant <rs type="grantNumber">CX2023016</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_s5wUfMX">
					<idno type="grant-number">62171377</idno>
				</org>
				<org type="funding" xml:id="_Z6JPhJU">
					<idno type="grant-number">2022YFC2009903/2022YFC2009900</idno>
				</org>
				<org type="funding" xml:id="_WHp6M9U">
					<idno type="grant-number">2022GY-084</idno>
				</org>
				<org type="funding" xml:id="_Mhwxtqs">
					<idno type="grant-number">CX2023016</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_2.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Retinal fundus images for glaucoma analysis: the RIGA dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Almazroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Imaging Informatics for Healthcare, Research, and Applications</title>
		<imprint>
			<biblScope unit="volume">10579</biblScope>
			<biblScope unit="page">105790</biblScope>
			<date type="published" when="2018">2018. 2018</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MaxStyle: adversarial style composition for robust medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_15</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_15" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Why adversarial training can hurt robust accuracy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Clarysse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hörrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=-CA8yFkPc7O" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feedback on a publicly distributed image database: the Messidor database</title>
		<author>
			<persName><forename type="first">E</forename><surname>Decencière</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Anal. Stereol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="234" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">U-net: deep learning for cell counting, detection, and morphometry</title>
		<author>
			<persName><forename type="first">T</forename><surname>Falk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="70" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: finding sparse, trainable neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain adaptation for medical image analysis: a survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1173" to="1185" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain specific convolution and high frequency reconstruction based unsupervised domain adaptation for medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_62</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_62" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="650" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ProSFDA: prompt learning based source-free domain adaptation for medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.11514</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain and content adaptive convolution based multi-source domain generalization for medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="233" to="244" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2017.07.005</idno>
		<ptr target="https://doi.org/10.1016/j.media.2017.07.005" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">I2F: a unified image-to-feature approach for domain adaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Causality-inspired single-source domain generalization for medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1095" to="1106" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image characteristics and quality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sprawls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Principles of Medical Imaging</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="1993">1993</date>
			<pubPlace>Aspen Gaithersburg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking data augmentation for single-source domain generalization in medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains: a survey on domain generalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8052" to="8072" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey of unsupervised deep domain adaptation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey on incorporating domain knowledge into deep learning for medical image analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2021.101985</idno>
		<ptr target="https://doi.org/10.1016/j.media.2021.101985" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101985</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial consistency for single domain generalization in medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ragoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_64" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="671" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FDA: fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalizing deep learning for medical image segmentation to unseen domains via deep stacked transformation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2531" to="2540" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain generalization: a survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="4396" to="4415" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generalizable cross-modality medical image segmentation via style augmentation and dual normalization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20856" to="20865" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
