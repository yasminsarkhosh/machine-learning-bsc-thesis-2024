<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Medical Boundary Diffusion Model for Skin Lesion Segmentation</title>
				<funder ref="#_EyRKpvS">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiacheng</forename><surname>Wang</surname></persName>
							<email>jiachengw@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science at School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Manteia Technologies Co., Ltd</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science at School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qichao</forename><surname>Zhou</surname></persName>
							<email>zhouqc@manteiatech.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Manteia Technologies Co., Ltd</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liansheng</forename><surname>Wang</surname></persName>
							<email>lswang@xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science at School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Medical Boundary Diffusion Model for Skin Lesion Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="427" to="436"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">69A2FB43C157B0663B0B7ED0D71C8F98</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_41</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Skin lesion segmentation â€¢ Diffusion model</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skin lesion segmentation in dermoscopy images has seen recent success due to advancements in multi-scale boundary attention and feature-enhanced modules. However, existing methods that rely on end-to-end learning paradigms, which directly input images and output segmentation maps, often struggle with extremely hard boundaries, such as those found in lesions of particularly small or large sizes. This limitation arises because the receptive field and local context extraction capabilities of any finite model are inevitably limited, and the acquisition of additional expert-labeled data required for larger models is costly. Motivated by the impressive advances of diffusion models that regard image synthesis as a parameterized chain process, we introduce a novel approach that formulates skin lesion segmentation as a boundary evolution process to thoroughly investigate the boundary knowledge. Specifically, we propose the Medical Boundary Diffusion Model (MB-Diff), which starts with a randomly sampled Gaussian noise, and the boundary evolves within finite times to obtain a clear segmentation map. First, we propose an efficient multi-scale image guidance module to constrain the boundary evolution, which makes the evolution direction suit our desired lesions. Second, we propose an evolution uncertainty-based fusion strategy to refine the evolution results and yield more precise lesion boundaries. We evaluate the performance of our model on two popular skin lesion segmentation datasets and compare our model to the latest CNN and transformer models. Our results demonstrate that our model outperforms existing methods in all metrics and achieves superior performance on extremely challenging skin lesions. The proposed approach has the potential to significantly enhance the accuracy and reliability of skin lesion segmentation, providing critical information for diagnosis and treatment. All resources will be publicly available at https://github. com/jcwang123/MBDiff.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Segmentation of skin lesions from dermoscopy images is a critical task in disease diagnosis and treatment planning of skin cancers <ref type="bibr" target="#b16">[17]</ref>. Manual lesion segmentation is time-consuming and prone to inter-and intra-observer variability. To improve the efficiency and accuracy of clinical workflows, numerous automated skin lesion segmentation models have been developed over the years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>. These models have focused on enhancing feature representations using various techniques such as multi-scale feature fusion <ref type="bibr" target="#b9">[10]</ref>, attention mechanisms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, self-attention mechanisms <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, and boundary-aware attention <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, resulting in significant improvements in skin lesion segmentation performance. Despite these advances, the segmentation of skin lesions with ambiguous boundaries, particularly at extremely challenging scales, remains a bottleneck issue that needs to be addressed. In such cases, even state-of-the-art segmentation models struggle to achieve accurate and consistent results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boundary Evolution Image</head><p>Small Large Fig. <ref type="figure">1</ref>. The boundary evolution process. It could be seen that various lesions can be accurately segmented by splitting the segmentation into sequential timesteps (t), named as boundary evolution in this work.</p><p>Two representative boundaries are visualized in Fig. <ref type="figure">1</ref>, where one extremely small lesion and one particularly large lesion are presented. The small one covers 1.03% in the image space and the large one covers 72.96%. As studied prior, solving the segmentation problems of such two types of lesions have different strategies. <ref type="bibr" target="#b0">(1)</ref> For the small lesions, translating the features at a lower depth to the convolutional layers at a higher depth can avoid losing local contexts <ref type="bibr" target="#b9">[10]</ref>. <ref type="bibr" target="#b1">(2)</ref> For the large lesions, enlarging the receptive field by dilated convolution <ref type="bibr" target="#b0">[1]</ref>, and even global attention <ref type="bibr" target="#b17">[18]</ref> can capture the long-range dependencies to improve the boundary decision. Besides the challenge of how to yield stable representations for various scales, multi-scale lesions will cause training fluctuation, that is, small lesions usually lead to large Dice loss. Feeding more boundary-aware supervision can reduce these negative effects to some degree <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>. The latest transformer, Xbound-Former, comprehensively addresses the multi-scale boundary problem through cross-scale boundary learning and exactly reaches higher performance on whatever small or large lesions.</p><p>However, current models for skin lesion segmentation are still struggling with extremely challenging cases, which are often encountered in clinical practice. While some approaches aim to optimize the model architecture by incorporating local and global contexts and multi-task supervision, and others seek to improve performance by collecting more labeled data and building larger models, both strategies are costly and can be limited by the inherent complexity of skin lesion boundaries. Therefore, we propose a novel approach that shifts the focus from merely segmenting lesion boundaries to predicting their evolution. Our approach is inspired by recent advances in image synthesis achieved by diffusion probabilistic models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, which generate synthetic samples from a randomly sampled Gaussian distribution in a series of finite steps. We adapt this process to model the evolution of skin lesion boundaries as a parameterized chain process, starting from Gaussian noise and progressing through a series of denoising steps to yield a clear segmentation map with well-defined lesion boundaries. By predicting the next step in the chain process rather than the final segmentation map, our approach enables the more accurate segmentation of challenging lesions than previous models. We illustrate the process of boundary evolution in Fig. <ref type="figure">1</ref>, where each row corresponds to a different step in the evolution process, culminating in a clear segmentation map with well-defined boundaries.</p><p>In this paper, we propose a Medical Boundary Diff usion model (MB-Diff ) to improve the skin lesion segmentation, particularly in cases where the lesion boundaries are ambiguous and have extremely large or small sizes. The MB-Diff model follows the basic design of the plain diffusion model, using a sequential denoising process to generate the lesion mask. However, it also includes two key innovations: Firstly, we have developed an efficient multi-scale image guidance module, which uses a pretrained transformer encoder to extract multi-scale features from prior images. These features are then fused with the evolution features to constrain the direction of evolution. Secondly, we have implemented an evolution uncertainty-based fusion strategy, which takes into account the uncertainty of different initializations to refine the evolution results and obtain more precise lesion boundaries. We evaluate our model on two popular skin lesion segmentation datasets, ISIC-2016 and PH 2 datasets, and find that it performs significantly better than existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The key objective of MB-Diff is to improve the representation of ambiguous boundaries by learning boundary evolution through a cascaded series of steps, rather than a single step. In this section, we present the details of our cascaded boundary evolution learning process and the parameterized architecture of the evolution process. We also introduce our evolution-based uncertainty estimation and boundary ensemble techniques, which have significant potential for enhancing the precision and reliability of the evolved boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Boundary Evolution Process</head><p>We adopt a step-by-step denoising process to model boundary evolution in MB-Diff, drawing inspiration from recent diffusion probabilistic models (DPMs). Specifically, given the image and boundary mask distributions as (X , Y), assuming that the evolution consists of T steps in total, the boundary at T -th step (y T ) is the randomly initialized noise and the boundary at 0-th (y 0 ) step denotes the accurate result. We formulate the boundary evolution process as follows:</p><formula xml:id="formula_0">y 0 âˆ¼ p Î¸ (y 0 |x) := p Î¸ (y 0:T |x)dy 1:T := p(y T ) T t=1 p Î¸ (y t-1 |y t , x), (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where p(y T ) = N (y T ; 0, I) is the initialized Gaussian distribution and p Î¸ (y t-1 |y t ) is each learnable evolution step, formulated as the Gaussian transition, denoted as:</p><formula xml:id="formula_2">p Î¸ (y t-1 |y t ) := N (y t-1 ; Î¼ Î¸ (y t , x, t), Î¸ (y t , x, t)). (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>Note that the prediction function takes the input image as a condition, enabling the evolving boundary to fit the corresponding lesion accurately. By modeling boundary evolution as a step-by-step denoising process, MB-Diff can effectively capture the complex structures of skin lesions with ambiguous boundaries, leading to superior performance in lesion segmentation.</p><p>To optimize the model parameters Î¸, we use the evolution target as an approximation of the posterior at each evolution step. Given the segmentation label y as y 0 , the label is gradually added by a Gaussian noise as:</p><formula xml:id="formula_4">q(y 1:T |y 0 ) := T t=1 q(y t |y t-1 ) := T t=1 N (y t ; âˆš 1 -Î² t y t-1 , Î² t I),<label>(3)</label></formula><p>where {Î² t } T t=1 is a set of constants ranging from 0 to 1. After that, we compute the posterior q(y t-1 |y t , y 0 ) using Bayes' rule. The MSE loss function is utilized to measure the distance between the predicted mean and covariance of the Gaussian transition distribution and the evolution target q(y t-1 |y t , y 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Paramterized Architecture with Image Prior</head><p>The proposed model is a parameterized chain process that predicts the Î¼ * t-1 and * t-1 at each evolution step t under the prior conditions of the image x and the prior evolution y * t . To capture the deep semantics of these conditions and perform efficient fusion, we adopt a basic U-Net <ref type="bibr" target="#b15">[16]</ref> architecture inspired by the plain DPM and introduce novel designs for condition fusion, that is the efficient multi-scale image guidance module.</p><p>The architecture consists of a multi-level convolutional encoder and a symmetric decoder with short connection layers between them. To incorporate the variable t into the model, we first embed it into the latent space. Then, the prior evolution y * t is added to the latent t before each convolution. At the bottleneck layer, we fuse the evolution features with the image guidance to constrain the evolution and ensure that the final boundary suits the conditional image.</p><p>To achieve this, priors train a segmentation model concurrently with the evolution model and use an attention-based parser to translate the image features in the segmentation branch into the evolution branch <ref type="bibr" target="#b21">[22]</ref>. Since the segmentation model is trained much faster than the evolution model, we adopt a pretrained pyramid vision transformer (PVT) <ref type="bibr" target="#b19">[20]</ref> as the image feature extractor to obtain the multi-scale image features. Let {f l } 4 l=1 denote the extracted features at four levels, with a 2x, 4x, 8x, 16x, smaller size of the original input. Each feature at the three lower levels is resized to match the scale of f 4 using Adaptive Averaging Pooling layers. After that, the four features are concatenated and fed into a fullconnection layer to map the image feature space into the evolution space. We then perform a simple yet effective addition of the mapped image feature and the encoded prior evolution feature, similar to the fusion of time embeddings, to avoid redundant computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evolution Uncertainty</head><p>Similar to typical evolutionary algorithms, the final results of boundary evolution are heavily influenced by the initialized population. As a stochastic chain process, the boundary evolution process may result in different endpoints due to the random Gaussian samples at each evolution step. This difference is particularly evident when dealing with larger ambiguity in boundary regions. The reason is that the image features in such ambiguous regions may not provide discriminative guidance for the evolution, resulting in significant variations in different evolution times. Instead of reducing the differences, we surprisingly find that these differences can represent segmentation uncertainty. Based on the evolution-based uncertainty estimation, the segmentation results become more accurate and trustworthy in practice <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Uncertainty Estimation: To estimate uncertainty, the model parameters Î¸ are fixed, and the evolution starts with a randomly sampled Gaussian noise y * T âˆ¼ N (0, I). Let {y * ,i T } n i=1 denote a total of n initializations. Once the evolution is complete, the obtained {Î¼ * ,i } n i=1 , { * ,i } n i=1 are used to the sample final lesion maps as: y * ,i = Î¼ * ,i + exp( 1 2 * ,i )N (0, I). Unlike traditional segmentation models that typically scale the prediction into the range of 0 to 1, the evolved maps generated by MB-Diff have unfixed distributions due to random sampling. Since the final result is primarily determined by the mean value Î¼, and the predicted has a limited range <ref type="bibr" target="#b5">[6]</ref>, we calculate the uncertainty as:</p><formula xml:id="formula_5">Î´ = 1 n n i=1 (Î¼ * ,i -1 n n j=1 Î¼ * ,j ) 2 , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Evolution Ensemble: Instead of training multiple networks or parameters to make the ensemble, MB-Diff allows running the inference multiple times and fusing the obtained evolutions. However, simply averaging the predicted identities from multiple evolutions is not effective, as the used MSE loss without activation constrains the predicted identities to be around 0 or 1, unlike the Sigmoid function which would limit the identities to a range between 0 and 1. Therefore, we employ the max vote algorithm to obtain the final segmentation map. In this algorithm, each pixel is classified as a lesion only if its identity sum across all n evolutions is greater than a threshold value Ï„ . Finally, the segmentation map is generated as</p><formula xml:id="formula_7">y * = ( n i=1 y * ,i ) â‰¥ Ï„.</formula><p>3 Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation Metrics</head><p>Datasets: We use two publicly available skin lesion segmentation datasets from different institutions in our experiments: the ISIC-2016 dataset and the PH 2 dataset. The ISIC-2016 dataset <ref type="bibr" target="#b7">[8]</ref> is provided by the International Skin Imaging Collaboration (ISIC) archive and consists of 900 samples in the public training set and 379 samples in the public validation set. As the annotation for its public test set is not currently available, we additionally collect the PH 2 dataset <ref type="bibr" target="#b12">[13]</ref>, which contains 200 labeled samples and is used to evaluate the generalization performance of our methods.</p><p>Evaluation Metrics: To comprehensively compare the segmentation results, particularly the boundary delineations, we employ four commonly used metrics to quantitatively evaluate the performance of our segmentation methods. These metrics include the Dice score, the IoU score, Average Symmetric Surface Distance (ASSD), and Hausdorff Distance of boundaries (95-th percentile; HD95).</p><p>To ensure fair comparison, all labels and predictions are resized to (512Ã—512) before computing these scores, following the approach of a previous study <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>For the diffusion model hyper-parameters, we use the default settings of the plain diffusion model, which can be found in the supplementary materials. Regarding dataset. We highlight the small lesions using dotted boxes in the third row.</p><p>the training parameters, we resize all images to (256 Ã— 256) for efficient memory utilization and computation. We use a set of random augmentations, including vertical flipping, horizontal flipping, and random scale change (limited to 0.9 âˆ¼ 1.1), to augment the training data. We set the batch size to 4 and train our model for a total of 200,000 iterations. During training, we use the AdamW optimizer with an initial learning rate of 1e-4. For the inference, we set n = 4 and Ï„ = 2 considering the speeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with State-of-the-Arts</head><p>We majorly compare our method to the latest skin lesion segmentation models, including the CNN-based and transformer-based models, i.e., U-Net++ <ref type="bibr" target="#b23">[24]</ref>, CA-Net <ref type="bibr" target="#b6">[7]</ref>, TransFuse <ref type="bibr" target="#b22">[23]</ref>, TransUNet <ref type="bibr" target="#b2">[3]</ref>, and especially the boundary-enhanced method, X-BoundFormer <ref type="bibr" target="#b17">[18]</ref>. Additionally, we evaluate our method against MedSegDiff <ref type="bibr" target="#b21">[22]</ref>, a recently released diffusion-based model, which we re-trained for 200,000 steps to ensure a fair comparison. The quantitative results are shown in Table <ref type="table" target="#tab_0">1</ref>, which reports four evaluation scores for two datasets. Though the parameters of CNNs and Transformers are selected with the best performance on ISIC-2016 validation set and the parameters of our method are selected by completing the 200,000 iterations, MB-Diff still achieves the 1.18% IoU improvement and 0.7% Dice improvement. Additionally, our predicted boundaries are closer to the annotations, as evidenced by the ASSD and HD95 metrics, which reduce by 1.02 and 1.93 pixels, respectively. When compared to MedSegDiff, MB-Diff significantly outperforms it in all metrics. Moreover, our method shows a larger improvement in generalization performance on the PH 2 dataset, indicating its better ability to handle new data. We present a visual comparison of challenging samples in Fig. <ref type="figure" target="#fig_0">2</ref>, including three samples from the ISIC-2016 validation set and three from the PH 2 dataset. These samples represent edge cases that are currently being studied in the community, including size variation, boundary ambiguity, and neighbor confusion. Our visual comparison reveals several key findings: (1) MB-Diff consistently achieves better segmentation performance on small and large lesions due to its thorough learning of boundary evolution, as seen in rows 3, 5, and 6. (2) MB-Diff is able to produce correct boundaries even in cases where they are nearly indistinguishable in human perception, eliminating the need for further manual adjustments and demonstrating significant practical value. (3) MB-Diff generates fewer false positive segmentation, resulting in cleaner predictions that enhance the user experience.</p><p>Furthermore, we provide a visualization of evolution uncertainties in Fig. <ref type="figure" target="#fig_0">2</ref>, where deeper oranges indicate larger uncertainties. It is evident that most regions with high uncertainties correspond to false predictions. This information can be used to guide human refinement of the segmentation in practical applications, ultimately increasing the AI's trustworthiness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Detailed Analysis of the Evolution</head><p>In this subsection, we make a comprehensive analysis to investigate the performance of each component in our method and compare it to the diffusion-based model, MedSegDiff. The results of our ablation study are presented in Fig. <ref type="figure" target="#fig_1">3(a)</ref>, where "w/o Evo" refers to using image features to directly train a segmentation model with FPN <ref type="bibr" target="#b10">[11]</ref> architecture and "w/o Fusion" means no evolution fusion is used. To ensure a fair comparison, we average the scores of multiple evolutions to represent the performance of "w/o Fusion". The results demonstrate that our evolutionary approach can significantly improve performance, and the evolution uncertainty-based fusion strategy further enhances performance. Comparing our method to MedSegDiff, the training loss curve in Fig. <ref type="figure" target="#fig_1">3</ref>(b) shows that our method converges faster and achieves smaller losses, indicating that our multi-scale image guidance is more effective than that of MedSegDiff. Furthermore, we evaluate our method's performance using parameters saved at different iterations, as shown in Fig. <ref type="figure" target="#fig_1">3(c</ref>). Our results demonstrate that our method has competitive performance at 50k iterations versus MedSegDiff at 200k iterations and our method at 100k iterations has already outperformed well-trained MedSegDiff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we introduced the medical boundary diffusion (MB-Diff) model, which is a novel approach to segment skin lesions. Our proposed method formulates lesion segmentation as a boundary evolution process with finite timesteps, which allows for efficient and accurate segmentation of skin lesions. To guide the boundary evolution towards the lesions, we introduce an efficient multi-scale image guidance module. Additionally, we propose an evolution uncertainty-based fusion strategy to yield more accurate segmentation. Our method is evaluated on two well-known skin lesion segmentation datasets, and the results demonstrate superior performance and generalization ability in unseen domains. Through a detailed analysis of our training program, we find that our model has faster convergence and better performance compared to other diffusion-based models. Overall, our proposed MB-Diff model offers a promising solution to accurately segment skin lesions, and has the potential to be applied in a clinical setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Visual comparison of our method and the SOTAs. The first three rows are samples from the ISIC-2016 validation set and the last three rows are from the PH 2 dataset. We highlight the small lesions using dotted boxes in the third row.</figDesc><graphic coords="7,55,98,63,17,340,18,168,70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Detailed analysis of our method, including the ablation analysis (a) and the comparison to the other diffusion-based method (b, c).</figDesc><graphic coords="8,41,79,53,90,340,21,86,65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of skin lesion segmentation with different approaches on the ISIC-2016 and PH 2 datasets. The averaged scores of both sets are presented respectively.</figDesc><table><row><cell>Method</cell><cell>ISIC-2016 [8]</cell><cell>PH 2 [13]</cell><cell></cell></row><row><cell></cell><cell cols="3">IoUâ†‘ Diceâ†‘ ASSDâ†“ HD95â†“ IoUâ†‘ Diceâ†‘ ASSDâ†“ HD95â†“</cell></row><row><cell>U-Net++ [24]</cell><cell>81.84 88.93 15.01</cell><cell>44.83 81.26 88.99 15.97</cell><cell>46.66</cell></row><row><cell>CA-Net [7]</cell><cell>80.73 88.10 15.67</cell><cell>44.98 75.18 84.66 21.06</cell><cell>64.53</cell></row><row><cell>TransFuse [23]</cell><cell>86.19 92.03 10.04</cell><cell>30.33 82.32 89.75 15.00</cell><cell>39.98</cell></row><row><cell>TransUNet [3]</cell><cell>84.89 91.26 10.63</cell><cell>28.51 83.99 90.96 12.65</cell><cell>33.30</cell></row><row><cell cols="2">XBound-Former [18] 87.69 93.08 8.21</cell><cell>21.83 85.38 91.80 10.72</cell><cell>26.00</cell></row><row><cell>MedSegDiff [22]</cell><cell>83.39 89.85 12.38</cell><cell>31.23 82.21 89.73 13.53</cell><cell>36.59</cell></row><row><cell>MB-Diff (Ours)</cell><cell>88.87 93.78 7.19</cell><cell>18.90 87.12 92.85 9.16</cell><cell>22.95</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is supported by the <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2019YFE0113900</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EyRKpvS">
					<idno type="grant-number">2019YFE0113900</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention Deeplabv3+: multi-level context attention mechanism for skin lesion segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Asadi-Aghbolaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-66415-2_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-66415-2_16" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Fusiello</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12535</biblScope>
			<biblScope unit="page" from="251" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ICL-Net: global and local inter-pixel correlations learning network for skin lesion segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">TransUNet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Is segmentation uncertainty useful?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Czolbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arnavaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-78191-0_55</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-78191-0_55" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sommer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12729</biblScope>
			<biblScope unit="page" from="715" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00502</idno>
		<title level="m">Leveraging uncertainty estimates for predicting segmentation quality</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ca-net: Comprehensive attention convolutional neural networks for explainable medical image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="699" to="711" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01397</idno>
		<title level="m">Skin lesion analysis toward melanoma detection: A challenge at the international symposium on biomedical imaging (ISBI) 2016, hosted by the international skin imaging collaboration (ISIC)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dense deconvolutional network for skin lesion segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="527" to="537" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Confidence calibration and predictive uncertainty estimation for deep medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Tempany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kapur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3868" to="3878" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PH 2-A dermoscopic image database for research and benchmarking</title>
		<author>
			<persName><forename type="first">T</forename><surname>MendonÃ§a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Marcal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rozeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="5437" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cancer statistics</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jemal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="33" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>CA</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">XBound-former: toward cross-scale boundary modeling in transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1735" to="1745" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Boundary-aware transformers for skin lesion segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_20" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="206" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: a versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated skin lesion segmentation via an adaptive dual attention module</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.00611</idno>
		<title level="m">MedSegDiff: Medical image segmentation with diffusion probabilistic model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TransFuse: fusing transformers and CNNs for medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_2" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">UNet++: a nested u-net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<editor>Stoyanov, D., et al.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<idno>DLMIA/ML-CDS -2018</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-5_1" />
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
