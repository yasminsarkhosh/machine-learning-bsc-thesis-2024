<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss</title>
				<funder ref="#_dspM6mD #_nutJZSz #_bPk8yta #_BEY9AqV">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_zKZGydk">
					<orgName type="full">Funding of Xiamen Science and Technology Bureau</orgName>
				</funder>
				<funder ref="#_E7muQWp #_9fxG9jR #_ydZ2kpr">
					<orgName type="full">Foundation of Science and Technology Commission of Shanghai Municipality</orgName>
				</funder>
				<funder ref="#_BkMnyKX #_4x5zCPV">
					<orgName type="full">Shanghai Jiao Tong University Foundation on Medical and Technological Joint Science Research</orgName>
				</funder>
				<funder ref="#_KFkvTmy">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bolun</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Biomedical Manufacturing and Life Quality Engineering</orgName>
								<orgName type="department" key="dep2">School of Mechanical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuanyi</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Ultrasound in Medicine</orgName>
								<orgName type="department" key="dep2">Shanghai Sixth People&apos;s Hospital</orgName>
								<orgName type="institution">Shanghai Jiao Tong University School of Medicine</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ron</forename><surname>Kikinis</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Radiology</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<orgName type="laboratory">The Surgical Planning Laboratory</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiaojun</forename><surname>Chen</surname></persName>
							<email>xiaojunchen@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Biomedical Manufacturing and Life Quality Engineering</orgName>
								<orgName type="department" key="dep2">School of Mechanical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="395" to="404"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">1256516E0587D008D2FE2D477607031D</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_38</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hand bone segmentation</term>
					<term>Adaptive convolution</term>
					<term>Anatomical constraint</term>
					<term>3D Slicer</term>
					<term>Ultrasound images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ultrasound imaging is a promising tool for clinical hand examination due to its radiation-free and cost-effective nature. To mitigate the impact of ultrasonic imaging defects on accurate clinical diagnosis, automatic fine-grained hand bone segmentation is highly desired. However, existing ultrasound image segmentation methods face difficulties in performing this task due to the presence of numerous categories and insignificant inter-class differences. To address these challenges, we propose a novel Adaptive Multi-dimensional Convolutional Network (AMCNet) for finegrained hand bone segmentation. It is capable of dynamically adjusting the weights of 2D and 3D convolutional features at different levels via an adaptive multi-dimensional feature fusion mechanism. We also design an anatomy-constraint loss to encourage the model to learn anatomical relationships and effectively mine hard samples. Experiments demonstrate that our method outperforms other comparison methods and effectively addresses the task of fine-grained hand bone segmentation in ultrasound volume. We have developed a user-friendly and extensible module on the 3D Slicer platform based on the proposed method and will release it globally to promote greater value in clinical applications. The source code is available at https://github.com/BL-Zeng/AMCNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hand imaging examination is a standard clinical procedure commonly utilized for various medical purposes such as predicting biological bone age <ref type="bibr" target="#b15">[16]</ref> and diagnosing finger bone and joint diseases <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. Ultrasound (US) is a promising alternative imaging modality for clinical examinations due to its radiationfree and cost-effective nature, especially the three-dimensional (3D) US volume, which is increasingly preferred for its intuitive visualization and comprehensive clinical information. However, the current US imaging technology is limited by low signal-to-noise ratio and inherent imaging artifacts, making the examination of hand with complex and delicate anatomical structure highly dependent on high-level expertise and experience.</p><p>To address this challenge, deep learning-based US image segmentation methods have been explored. For instance, Liu et al. <ref type="bibr" target="#b9">[10]</ref> propose an attention-based network to segment seven key structures in the neonatal hip bone. Rahman et al. <ref type="bibr" target="#b13">[14]</ref> present a graph convolutional network with orientation-guided supervision to segment bone surfaces. Studies such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref> use the convolutional-based network for efficient bone surface segmentation. Additionally, some studies have focused on the automatic identification and segmentation of soft tissues, such as finger tendons and synovial sheaths <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. Although these methods are effective in segmenting specific objects, they lack fine-grained analysis. Some studies have revealed that each hand bone has clinical analysis value <ref type="bibr" target="#b0">[1]</ref>, thus making fine-grained segmentation clinically significant. However, this is a challenging task. The hand comprises numerous structures, with a closely related anatomical relationship between the phalanges, metacarpal bones, and epiphysis. Moreover, different categories exhibit similar imaging features, with the epiphysis being particularly indistinguishable from the phalanges and metacarpal bones.</p><p>Fine-grained segmentation demands a model to maintain the inter-slice anatomical relationships while extracting intra-slice detailed features. The 2D convolution excels at capturing dense information but lacks inter-slice information, while the 3D convolution is complementary <ref type="bibr" target="#b5">[6]</ref>. This motivates us to develop a proper adaptive fusion method. Previous studies used 2D/3D layouts to address data anisotropy problems. For example, Wang et al. <ref type="bibr" target="#b16">[17]</ref> propose a 2.5D UNet incorporating 2D and 3D convolutions to improve the accuracy of MR image segmentation. Dong et al. <ref type="bibr" target="#b5">[6]</ref> present a mesh network fusing multi-level features for better anisotropic feature extraction. However, the effectiveness of these methods in capturing fine-grained feature representations is limited due to their relatively fixed convolution distributions and feature fusion approaches. Moreover, the lack of supervision on complex anatomical relationships makes them inevitably suffer from anatomical errors such as missing or confusing categories.</p><p>To overcome the deficiencies of existing methods, this study proposes a novel Adaptive Multi-dimensional Convolutional Network (AMCNet) with an anatomy-constraint loss for fine-grained hand bone segmentation. Our contribution is three-fold. 1) First, to the best of our knowledge, this is the first work to address the challenge of automatic fine-grained hand bone segmentation in 3D US volume. We propose a novel multi-dimensional network to tackle the issue of multiple categories and insignificant feature differences. 2) Second, we propose an adaptive multi-dimensional feature fusion mechanism to dynamically adjust the weights of 2D and 3D convolutional feature layers according to different objectives, thus improving the fine-grained feature representation of the model.</p><p>3) Finally, we propose an anatomy-constraint loss that minimizes the anatomical error and mines hard samples, further improving the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network Design</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the architecture of AMCNet consists of four down-sampling layers, four up-sampling layers, and four skip-connections. Each layer contains an adaptive 2D/3D convolutional module (ACM) which is proposed to dynamically balance inter-layer and intra-layer feature weight through adaptive 2D and 3D convolutions for better representations. In the encoder, each ACM block is followed by a max-pooling layer to compress features. In the decoder, the trilinear interpolate is used to up-sample features. The number of channels across each layer is empirically set to 64, 128, 256, and 512. The output layer uses the 1×1×1 convolutional layer to obtain the segmentation map. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptive 2D/3D Convolutional Module (ACM)</head><p>To enable the model to capture the inter-layer anatomical connection and intralayer dense semantic feature, the ACM is proposed to adaptively fuse the 2D and 3D convolution at different levels. Figure <ref type="figure" target="#fig_0">1</ref>(b) illustrates the architecture of the ACM. Firstly, the feature map F i ∈ R c ×w×h×d passes through 2D and 3D convolutional block respectively to obtain the 2D convolutional feature F 2D ∈ R c×w×h×d and 3D convolutional feature F 3D ∈ R c×w×h×d . Figure <ref type="figure" target="#fig_0">1(c)</ref> shows the details of the 2D and 3D convolutional block, which includes two 1 × 3 × 3 or 3 × 3 × 3 convolution, instance normalization (Instance-Norm), and LeakyRuLU operations. The use of Instance-Norm considers the limitation of batch size in 3D medical image segmentation. Then, the F 2D and F 3D are performed the voxelwise adding and the global average pooling (GAP ) to generate channel-wise statistics F G ∈ R c×1×1×1 , which can be expressed as:</p><formula xml:id="formula_0">F G = GAP (F 2D + F 3D ) = 1 w × h × d w i=1 h j=1 d k=1 (F 2D + F 3D )<label>(1)</label></formula><p>where w, h, and d are the width, height, and depth of the input feature map, respectively. Further, a local cross-channel information interaction attention mechanism is applied for the fusion of multi-dimensional convolutional features. Specifically, the feature map F G is squeezed to a one-dimension tensor of length c, which is the number of channels, and then a one-dimensional (1D) convolution with a kernel size of K is applied for information interaction between channels. The obtained feature layer is re-expanded into a 3D feature map F G , which can be expressed as:</p><formula xml:id="formula_1">F G = G U (C1D K (G S (F G )))<label>(2)</label></formula><p>where C1D K denotes the 1D convolution with the kernel size of K, G S and G U denote the operation of squeezing and re-expanding respectively.</p><p>To adaptively select the feature information from different convolutions, the softmax operation is performed channel-wise to compute the weight vectors α and β corresponding to F 2D and F 3D respectively, which can be expressed as:</p><formula xml:id="formula_2">α i = e AiF G e AiF G + e BiF G , β i = e BiF G e AiF G + e BiF G (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where A, B ∈ R c×c denote the learnable parameters, A i and B i denote to the i-th row of A and B respectively, α i and β i denote to i-th element of α and β respectively.</p><formula xml:id="formula_4">F o = α • F 2D + β • F 3D<label>(4)</label></formula><p>where F o denotes the output feature map of the ACM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Anatomy-Constraint Loss</head><p>Fine-grained hand bone segmentation places stringent demands on the anatomical relationships between categories, but the lack of supervision during model training renders it the primary source of segmentation error. For instance, the epiphysis is highly prone to neglect due to its small and imbalanced occupation, while the index and ring phalanx bones can easily be mistaken due to their symmetrical similarities. To address this issue, we propose the anatomy-constraint loss to facilitate the model's learning of anatomical relations. Anatomical errors occur when pixels significantly deviate from their expected anatomical locations. Thus, we utilize the loss to compute and penalize these deviating pixels. Assume that the Y and P are the label and segmentation map respectively. First, the map representing anatomical errors is generated, where only pixels in the segmentation map that do not correspond to the anatomical relationship are activated. To mitigate subjective labeling errors caused by the unclear boundaries in US images, we perform morphological dilation on the segmentation map P. This operation expands the map and establishes an error tolerance, allowing us to disregard minor random errors and promote training stability. To make it differentiable, we implement this operation with a kernel=3 and stride=1 max-pooling operation. Subsequently, the anatomical error map F E is computed by pixel-wise subtracting the Y and the expanded segmentation map P. The resulting difference map is then activated by ReLU, which ensures that only errors within the label region and beyond the anatomically acceptable range are penalized. The process can be expressed as:</p><formula xml:id="formula_5">F E = σ R (Y Ci -G mp (P Ci ))<label>(5)</label></formula><p>where Y Ci and P Ci denote the i-th category maps of the label Y and segmentation map P respectively, G mp (•) denotes the max-pooling operation, and σ R (•) denotes the ReLU activation operation.</p><p>Next, we intersect F E with the segmentation map P and label Y, respectively, based on which the cross entropy is computed, which is used to constrain the anatomical error:</p><formula xml:id="formula_6">Loss AC = Loss CE (P F E , Y F E )<label>(6)</label></formula><p>where Loss AC (•) denotes the proposed anatomy-constraint loss, Loss CE (•) denotes the cross-entropy loss, and denotes the intersection operation.</p><p>To reduce the impact of class imbalance on model training and improve the stability of segmentation, we use a combination of Dice loss and anatomyconstraint loss function:</p><formula xml:id="formula_7">L = Loss Dice + γLoss AC (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>where L is the overall loss, Loss Dice denotes the Dice loss, and γ denotes the weight-controlling parameter of the anatomy-constraint loss.</p><p>3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation</head><p>Our method is validated on an in-house dataset, which consists of 103 3D ultrasound volumes collected using device IBUS BE3 with a 12MHz linear transducer from pediatric hand examinations. The mean voxel resolution is 0.088×0.130×0.279 mm 3 and the mean image size is 512×1023×609. Two expert ultrasonographers manually annotated the data based on ITK-snap <ref type="bibr" target="#b17">[18]</ref>. Each phalanx, metacarpal, and epiphysis were labeled with different categories, and there are a total of 39 categories including the background. The dataset was randomly spitted into 75% training set and 25% test set. All data were resized to 256×512 in the transverse plane and maintained the axial size. We extracted 256×512×16 voxels training patches from the resized images as the training samples.</p><p>The training and test phases of the network were implemented by PyTorch on an NVIDIA GeForce RTX 3090 GPU. The network was trained with Adam optimization with momentum of 0.9. The learning rate was set as 10e-3. For the hyperparameter, the kernel size K of 1D convolution in ACM was set as 3 and the weight-controlling parameter γ was set as 0.5. We used the Dice coefficient (DSC), Jaccard Similarity (Jaccard), Recall, F1-score, and Hausdorff Distance (HD95) as evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Comparison</head><p>We compared our network with recent and outstanding medical image segmentation methods, which contain UNet <ref type="bibr" target="#b14">[15]</ref>, UNet++ <ref type="bibr" target="#b18">[19]</ref>, 3D UNet <ref type="bibr" target="#b4">[5]</ref>, VNet <ref type="bibr" target="#b12">[13]</ref>, MNet <ref type="bibr" target="#b5">[6]</ref>, and the transformer baseline SwinUNet <ref type="bibr" target="#b2">[3]</ref>. For a fair comparison, we used publicly available hyperparameters for each model. For the 3D network, the data processing method is consistent with ours, while for the 2D network, we slice the images along the axis to convert the 3D data into 2D.</p><p>Table <ref type="table" target="#tab_0">1</ref> lists the results. Note that our method achieved the highest quantitative performance of DSC, Jaccard, Recall and F1-score, with values of 0.900, 0.819, 0.871, and 0.803, respectively. These results improved by 1.3%, 2.1%, 0.8%, and 1.3% compared to the best values of other methods. Note that our method outperformed the MNet that is a state-of-the-art (SOTA) method, which demonstrated the effectiveness of adaptive multi-dimensional feature fusion and anatomy-constraint for enhancing model performance. Figure <ref type="figure" target="#fig_1">2</ref> shows the visualization results of our method and comparative methods (Due to page limitations, we only presented the comparison results of our method with baseline and SOTA methods). Compared to other methods, our method has advantages in effectively mining difficult samples, particularly in accurately identifying and classifying clinically important but difficultto-distinguish epiphysis. Additionally, it can effectively learn the anatomical relationships of different categories, reducing the occurrence of category confusion, particularly in the phalanges of the index finger, middle finger, and ring finger. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>To validate the effect of anatomy-constraint loss, we compare the results of training with Loss Dice and the combination of Loss Dice and Loss AC on both 3D UNet and the proposed AMCNet. Table <ref type="table" target="#tab_1">2</ref> lists the results. Note that compared with only Loss Dice , UNet and our method have improved in various metrics after adding Loss AC , boosting 0.6% in DSC and 1.1% in Jaccard. The results indicate that enforcing anatomical constraints to encourage the model to learn anatomical relationships improves the model's feature representation, resulting in better performance. Additionally, to verify the effect of the adaptive multidimensional feature fusion mechanism, we modified the ACM module to only 2D and 3D convolutional blocks, respectively. The results are shown in Table <ref type="table" target="#tab_1">2</ref>. Note that the 2D and 3D feature adaptive fusion mechanism improves model performance. Specifically, under Loss Dice and Loss AC , it has resulted in an increase of 1.0% and 0.9% in DSC, 1.1% and 1.5% in Jaccard, respectively, compared to using only 2D or 3D convolution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Software Development and Application</head><p>Based on the method described above, a user-friendly and extensible module was developed on the 3D Slicer platform <ref type="bibr" target="#b6">[7]</ref> to facilitate user access, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. To use the module, users simply select the input and output file formats on the module interface and click the "apply" button. The software will then automatically perform image preprocessing, call the model for inference, and deliver the segmentation result within twenty seconds (see supplementary material 1). This plugin will be released globally to promote the greater value of the proposed method in clinical applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we have presented an adaptive multi-dimensional convolutional network, called AMCNet, to address the challenge of automatic fine-grained hand bone segmentation in 3D US volume. It adopts an adaptive multi-dimensional feature fusion mechanism to dynamically adjust the weights of 2D and 3D convolutional feature layers according to different objectives. Furthermore, an anatomy-constraint loss is designed to encourage the model to learn anatomical relationships and effectively mine hard samples. Experiments show that our proposed method outperforms other comparison methods and effectively addresses the task of fine-grained hand bone segmentation in ultrasound volume. The proposed method is general and could be applied to more medical segmentation scenarios in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of the proposed AMCNet. The network consists of four downsampling layers and four up-sampling layers. Each layer is composed of an ACM to fuse 2D and 3D convolutional features at different levels. LossDice denotes the Dice loss and LossAC denotes the proposed anatomy-constraint loss.</figDesc><graphic coords="3,59,46,258,95,333,34,175,78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The visualization results. Orange circles indicate obvious segmentation errors. (Color figure online)</figDesc><graphic coords="7,55,98,143,69,340,18,208,45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The extensible module for our method based on the 3D Slicer platform</figDesc><graphic coords="8,41,79,353,09,340,21,201,67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison experiments between the proposed method and the outstanding segmentation methods. Dim denotes to dimension.</figDesc><table><row><cell>Dim</cell><cell>Methods</cell><cell cols="4">DSC↑ Jaccard↑ Recall↑ F1-score↑ HD95 (mm) ↓</cell></row><row><cell>2D</cell><cell>UNet [15]</cell><cell>0.855 0.747</cell><cell>0.837</cell><cell>0.736</cell><cell>1.215</cell></row><row><cell></cell><cell cols="2">UNet++ [19] 0.875 0.779</cell><cell>0.763</cell><cell>0.697</cell><cell>3.328</cell></row><row><cell></cell><cell cols="2">SwinUNet [3] 0.829 0.709</cell><cell>0.796</cell><cell>0.657</cell><cell>1.278</cell></row><row><cell>3D</cell><cell cols="2">3D UNet [5] 0.829 0.709</cell><cell>0.584</cell><cell>0.632</cell><cell>0.960</cell></row><row><cell></cell><cell>VNet [13]</cell><cell>0.864 0.761</cell><cell>0.863</cell><cell>0.730</cell><cell>3.380</cell></row><row><cell cols="2">2D⊕3D MNet [6]</cell><cell>0.887 0.798</cell><cell>0.830</cell><cell>0.790</cell><cell>0.695</cell></row><row><cell></cell><cell>Ours</cell><cell>0.900 0.819</cell><cell>0.871</cell><cell>0.803</cell><cell>1.184</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the effect of LossAC and ACM</figDesc><table><row><cell>Methods</cell><cell cols="4">DSC↑ Jaccard↑ Recall↑ F1-score↑ HD95 (mm) ↓</cell></row><row><cell>3D UNet [15]</cell><cell>0.829 0.709</cell><cell>0.584</cell><cell>0.632</cell><cell>0.960</cell></row><row><cell>3D UNet+LossAC</cell><cell>0.835 0.718</cell><cell>0.692</cell><cell>0.686</cell><cell>0.874</cell></row><row><cell>AMCNet 3D</cell><cell>0.887 0.797</cell><cell>0.787</cell><cell>0.648</cell><cell>2.705</cell></row><row><cell>AMCNet 2D</cell><cell>0.887 0.798</cell><cell>0.842</cell><cell>0.759</cell><cell>4.979</cell></row><row><cell>AMCNet 2D⊕3D</cell><cell>0.894 0.808</cell><cell>0.875</cell><cell>0.800</cell><cell>1.903</cell></row><row><cell>AMCNet 3D+LossAC</cell><cell>0.891 0.804</cell><cell cols="2">0.877 0.787</cell><cell>2.728</cell></row><row><cell>AMCNet 2D+LossAC</cell><cell>0.890 0.808</cell><cell>0.719</cell><cell>0.750</cell><cell>0.748</cell></row><row><cell cols="2">AMCNet 2D⊕3D +LossAC (Ours) 0.900 0.819</cell><cell>0.871</cell><cell>0.803</cell><cell>1.184</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by grants from the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">81971709</rs>; <rs type="grantNumber">M-0019</rs>; <rs type="grantNumber">82011530141</rs>), the <rs type="funder">Foundation of Science and Technology Commission of Shanghai Municipality</rs> (<rs type="grantNumber">20490740700</rs>; <rs type="grantNumber">22Y11911700</rs>), <rs type="funder">Shanghai Jiao Tong University Foundation on Medical and Technological Joint Science Research</rs> (<rs type="grantNumber">YG2021ZD21</rs>; <rs type="grantNumber">YG2021QN72</rs>; <rs type="grantNumber">YG2022QN056</rs>; <rs type="grantNumber">YG2023ZD19</rs>; <rs type="grantNumber">YG2023ZD15</rs>), the <rs type="funder">Funding of Xiamen Science and Technology Bureau</rs> (<rs type="grantNumber">3502Z20221012</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KFkvTmy">
					<idno type="grant-number">81971709</idno>
				</org>
				<org type="funding" xml:id="_dspM6mD">
					<idno type="grant-number">M-0019</idno>
				</org>
				<org type="funding" xml:id="_E7muQWp">
					<idno type="grant-number">82011530141</idno>
				</org>
				<org type="funding" xml:id="_9fxG9jR">
					<idno type="grant-number">20490740700</idno>
				</org>
				<org type="funding" xml:id="_ydZ2kpr">
					<idno type="grant-number">22Y11911700</idno>
				</org>
				<org type="funding" xml:id="_BkMnyKX">
					<idno type="grant-number">YG2021ZD21</idno>
				</org>
				<org type="funding" xml:id="_4x5zCPV">
					<idno type="grant-number">YG2021QN72</idno>
				</org>
				<org type="funding" xml:id="_nutJZSz">
					<idno type="grant-number">YG2022QN056</idno>
				</org>
				<org type="funding" xml:id="_bPk8yta">
					<idno type="grant-number">YG2023ZD19</idno>
				</org>
				<org type="funding" xml:id="_zKZGydk">
					<idno type="grant-number">YG2023ZD15</idno>
				</org>
				<org type="funding" xml:id="_BEY9AqV">
					<idno type="grant-number">3502Z20221012</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_38.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imaging diagnosis of solitary tumors of the phalanges and metacarpals of the hand</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Stacy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Roentgenol</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page" from="106" to="115" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic segmentation of bone surfaces from ultrasound using a filter-layer-guided CNN</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Alsinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="775" to="783" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Swin-Unet: Unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bone lesions of the hand and wrist: systematic approach to imaging evaluation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Cecava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Kephart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Bui-Mansfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemp. Diagn. Radiol</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Çiçek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8_49" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">MNet: rethinking 2D/3D networks for anisotropic medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.04846</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D slicer as an image computing platform for the quantitative imaging network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1323" to="1341" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The reliability of musculoskeletal ultrasound in the detection of cartilage abnormalities at the metacarpo-phalangeal joints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iagnocco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Osteoarthritis Cartilage</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1142" to="1146" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmentation of finger tendon and synovial sheath in ultrasound image using deep convolutional neural network</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Kuok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Eng. Online</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">NHBS-Net: a feature fusion attention network for ultrasound neonatal hip bone segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3446" to="3458" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An efficient end-to-end CNN for segmentation of bone surfaces from ultrasound</title>
		<author>
			<persName><forename type="first">K</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">101766</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A New active contours approach for finger extensor tendon segmentation in ultrasound images using prior knowledge and phase symmetry</title>
		<author>
			<persName><forename type="first">N</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sultan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Veiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coimbra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1261" to="1268" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">V-Net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Orientation-guided graph convolutional network for bone surface segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G C</forename><surname>Bandara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_40</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_40" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022. MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning for automated skeletal bone age assessment in X-ray images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aldinucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="41" to="51" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic segmentation of vestibular schwannoma from T2weighted MRI by deep spatial attention with hardness-weighted loss</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_30" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1116" to="1128" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">UNet++: redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
