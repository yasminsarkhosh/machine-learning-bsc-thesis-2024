<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation</title>
				<funder ref="#_8DgRHQg #_CQdVYDv">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenjing</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiahao</forename><surname>Lei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Vascular Surgery</orgName>
								<orgName type="institution">Shanghai Ninth People&apos;s Hospital</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Vascular Surgery</orgName>
								<orgName type="institution">Shanghai Ninth People&apos;s Hospital</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Sheng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Chaohu Clinical Medcial College</orgName>
								<orgName type="institution" key="instit2">Anhui Medical University</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinhua</forename><surname>Zhou</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Anhui Medical University</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinwu</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Vascular Surgery</orgName>
								<orgName type="institution">Shanghai Ninth People&apos;s Hospital</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">Shanghai Key Laboratory of Tissue Engineering</orgName>
								<orgName type="institution">Shanghai Ninth People&apos;s Hospital</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Shanghai Jiao Tong University School of Medicine</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<email>yangyang@cs.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="662" to="672"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">A866EDDCA36382B70E764553D7E0D229</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_63</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semi-supervised learning</term>
					<term>Uncertainty assessment</term>
					<term>Prototype learning</term>
					<term>Medical image segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised learning (SSL) has emerged as a promising approach for medical image segmentation, while its capacity has still been limited by the difficulty in quantifying the reliability of unlabeled data and the lack of effective strategies for exploiting unlabeled regions with ambiguous predictions. To address these issues, we propose an Uncertainty-informed Prototype Consistency Learning (UPCoL) framework, which learns fused prototype representations from labeled and unlabeled data judiciously by incorporating an entropy-based uncertainty mask. The consistency constraint enforced on prototypes leads to a more discriminative and compact prototype representation for each class, thus optimizing the distribution of hidden embeddings. We experiment with two benchmark datasets of two-class semi-supervised segmentation, left atrium and pancreas, as well as a three-class multi-center dataset of type B aortic dissection. For all three datasets, UPCoL outperforms the state-of-the-art SSL methods, demonstrating the efficacy of the uncertainty-informed prototype learning strategy (Code is available at https://github.com/VivienLu/UPCoL).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The field of medical image segmentation has been increasingly drawn to semisupervised learning (SSL) due to the great difficulty and cost of data labeling.</p><p>By utilizing both labeled and unlabeled data, SSL can significantly reduce the need for labeled training data and address inter-observer variability <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>Typical SSL approaches involve techniques such as self-training, uncertainty estimation, and consistency regularization. Self-training aims to expand labeled training set by selecting the most confident predictions from the unlabeled data to augment the labeled data <ref type="bibr" target="#b23">[23]</ref>. To obtain high-quality pseudo-labels, uncertainty estimation is often employed in self-training models. Various uncertainty estimation methods have been proposed to reduce the influence of ambiguous unlabeled data, e.g., Monte Carlo dropout <ref type="bibr" target="#b27">[27]</ref> and ensemble-based methods <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b22">22]</ref>. Also, some metrics have been defined to quantify the degree of uncertainty. The most widely-used one is information entropy <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b27">27]</ref>, where a threshold or a percentage is set to determine whether an unlabeled sample is reliable, i.e., its predicted label can be used as pseudo-label during the training phase. Besides pseudolabeling, it is common to add a consistency regularization in loss function <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b28">28]</ref>. For instance, UA-MT <ref type="bibr" target="#b27">[27]</ref> and CoraNet <ref type="bibr" target="#b15">[15]</ref> impose consistency constraints on the teacher-student model for specific regions (certain/uncertain area or both). URPC <ref type="bibr" target="#b12">[12]</ref> uses multilevel extraction of multi-scale uncertainty-corrected features to moderate the anomalous pixel of consistency loss.</p><p>Despite current progress, the performance of pseudo-labeling and consistency constraints has been limited for two reasons. First, defining an appropriate quantification criterion for reliability across various tasks can be challenging due to the inherent complexity of uncertainty. Second, most of the consistency constraints are imposed at decision space with the assumption that the decision boundary must be located at the low-density area, while the latent feature space of unlabeled data has not been fully exploited, and the low-density assumption may be incapable to guide model learning in the correct way.</p><p>Recently, prototype alignment has been introduced into SSL. Prototypebased methods have the potential of capturing underlying data structure including unlabeled information, and optimizing the distribution of feature embeddings across various categories <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">17]</ref>. Existing semi-supervised segmentation methods based on prototype learning aim to learn each class prototype from sample averaging and leverage consistency constraints to train the segmentation network. U 2 PL <ref type="bibr" target="#b18">[18]</ref> distinguishes reliable samples among unlabeled data by uncertainty estimation, and constructs prototypes for the whole dataset by class averaging its features with labeled sample and reliable unlabeled features. CISC-R <ref type="bibr" target="#b19">[19]</ref> queries a guiding labeled image that shares similar semantic information with an unlabeled image, then estimates pixel-level similarity between unlabeled features and labeled prototypes, thereby rectifying the pseudo labels with reliable pixellevel precision. CPCL <ref type="bibr" target="#b24">[24]</ref> introduces a cyclic prototype consistency learning framework to exploit unlabeled data and enhance the prototype representation.</p><p>Overall, prototype learning has much room for improvement in semisupervised segmentation. As voxel-level averaging is only reliable for labeled data, current prototype learning approaches rely on labeled data and a small amount of unlabeled data, or learn prototypes separately for labeled and unlabeled data. In this way, they may not fully represent the distribution of the embedding space. Here raises the question:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can we capture the embedding distribution by considering all voxels, including both labeled and unlabeled, and exploit the knowledge of the entire dataset?</head><p>To answer it, we propose to learn fused prototypes through uncertainty-based attention pooling. The fused prototypes represent the most representative and informative examples from both the labeled and unlabeled data for each class. The main contributions of our work can be summarized as follows:</p><p>1) We develop a novel uncertainty-informed prototype consistency learning framework, UPCoL, by considering voxel-level consistency in both latent feature space (i.e., prototype) and decision space. 2) Different from previous studies, we design a fused prototype learning scheme, which jointly learns from labeled and unlabeled data embeddings. 3) For stable prototype learning, we propose a new entropy measure to qualify the reliability of unlabeled voxel and an attention-weighted strategy for fusion. 4) We apply UPCoL to two-class and three-class segmentation tasks. UPCoL outperforms the SOTA SSL methods by large margins. The labeled images go through the student model for supervised learning, while the unlabeled images go through the teacher model for segmentation and uncertainty estimation. In Uncertainty-informed Prototype Fusion module, we utilize the reliability map to fuse prototypes learned from labeled and unlabeled embeddings. The similarity between fused prototypes and feature embeddings at each spatial location is then measured for consistency learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Given a dataset D = {D l , D u }, the labeled set D l = {x l i , y l i } N i=1 contains N samples, and the unlabeled set</p><formula xml:id="formula_0">D u = {x u i } N +M i=N contains M samples, where</formula><p>x l i , x u i ∈ R H×W ×D represent the input with height H, width W , depth D, and y u i ∈ {0, 1, ..., C-1} H×W ×D . The proposed framework UPCoL includes a student model and a self-ensembling teacher model, each consisting of a representation head h and a segmentation head f . Figure <ref type="figure" target="#fig_0">1</ref> shows the overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Uncertainty Assessment</head><p>To assess the uncertainty at both voxel-level and geometry-level, we adopt the same ensemble of classifiers as <ref type="bibr" target="#b22">[22]</ref> using different loss functions, i.e., crossentropy loss, focal loss <ref type="bibr" target="#b9">[9]</ref>, Dice loss, and IoU loss. Unlike <ref type="bibr" target="#b22">[22]</ref>, which simply differentiates certain and uncertain regions based on the result discrepancy of four classifiers, we use the average of four prediction results and define an entropy-based measure to quantify the reliability for each voxel. Specifically, let f</p><formula xml:id="formula_1">(x,y,z) t,i</formula><p>∈ R C denote the softmax probability for voxel at position (x, y, z) in i -th unlabeled image yielded by the segmentation head of the teacher model, where the segmentation result is the average over multiple classifiers (AMC), and C is the number of classes. The entropy is formulated in Eq. ( <ref type="formula" target="#formula_2">1</ref>),</p><formula xml:id="formula_2">H(f (x,y,z) t,i ) = - C-1 c=0 f (x,y,z) t,i (c) log f (x,y,z) t,i (c).<label>(1)</label></formula><p>Intuitively, voxels with high entropy are ambiguous. Thus, a reliability map can be defined accordingly, denoted by Φ (x,y,z) i</p><p>, which enables the model to assign varying degrees of importance to voxels,</p><formula xml:id="formula_3">Φ (x,y,z) i = 1 H × W × D (1 - H(f (x,y,z) t,i ) x,y,z H(f (x,y,z) t,i</formula><p>) ).</p><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prototype Consistency Learning</head><p>Uncertainty-Informed Prototype Fusion. The prototypes from labeled and unlabeled data are first extracted separately. Both of them originate from the feature maps of the 3rd-layer decoder, which are upsampled to the same size as segmentation labels by trilinear interpolation. Let h l s,i be the output feature by the representation head of the student model for the i-th labeled image, and h u t,i be the hidden feature by the teacher representation head for the i-th unlabeled image. B l and B u denote the batch sizes of labeled and unlabeled set respectively, and (x, y, z) denotes voxel coordinate. For labeled prototype, the feature maps are masked directly using ground truth labels, and the prototype of class c is computed via masked average pooling <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b29">29]</ref>:</p><formula xml:id="formula_4">p l c = 1 B l B l i=1 x,y,z h l(x,y,z) s,i 1 y l(x,y,z) i = c x,y,z 1 y l(x,y,z) i = c . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>For unlabeled data, instead of simply averaging features from the same predicted class, UPCoL obtains the prototypes in an uncertainty-informed manner, i.e., using a masked attention pooling based on each voxel's reliability:</p><formula xml:id="formula_6">p u c = 1 B u Bu i=1 x,y,z h u(x,y,z) t,i Φ (x,y,z) i 1 ŷu(x,y,z) i = c</formula><p>x,y,z 1 ŷu(x,y,z)</p><formula xml:id="formula_7">i = c . (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>Temporal Ensembling technique in Mean-Teacher architecture enhances model performance and augments the predictive label quality <ref type="bibr" target="#b16">[16]</ref>, leading to a progressive improvement in the reliability of predictive labels and the refinement of unlabeled prototypes throughout the training process. Thus, we adopt a nonlinear updating strategy to adjust the proportion of unlabeled prototypes for fusing the labeled prototypes and unlabeled prototypes, i.e.,</p><formula xml:id="formula_9">p c = λ lab p l c + λ unlab p u c ,<label>(5)</label></formula><p>where λ lab = 1 1+λcon , λ unlab = λcon 1+λcon , and λ con is the widely-used timedependent Gaussian warming up function <ref type="bibr" target="#b16">[16]</ref>. During the training process, the proportion of labeled prototypes decreases from 1 to 0.5, while the proportion of unlabeled prototypes increases from 0 to 0.5. This adjustment strategy ensures that labeled prototypes remain the primary source of information during training, even as the model gradually gives more attention to the unlabeled prototypes.</p><p>Consistency Learning. We adopt non-parametric metric learning to obtain representative prototypes for each semantic class. The feature-to-prototype similarity is employed to approximate the probability of voxels in each class,</p><formula xml:id="formula_10">s (x,y,z) i = CosSim h (x,y,z) i , p c = h (x,y,z) i • p c max( h (x,y,z) i 2 • p c 2 , )<label>(6)</label></formula><p>where the value of is fixed to 1e -8 , and CosSim(•) denotes cosine similarity. To ensure the accuarcy of the prototype, prototype-based predictions for labeled voxels expect to close to ground truth. And the prototype-based predictions for unlabeled voxels expect to close to segmentor prediction since prototype predictions are considered to be reliable aid. Then the prototype consistency losses for labeled and unlabeled samples are defined respectively as:</p><formula xml:id="formula_11">L l pc = L CE (s l i , y l i ), L u pc = x,y,z Φ (x,y,z) i L CE (s u(x,y,z) i , ŷu(x,y,z) i ),<label>(7)</label></formula><p>where ŷu(x,y,z) i is the student model prediction of the i-th unlabeled sample at (x, y, z). Equation ( <ref type="formula" target="#formula_11">7</ref>) is a variant of expanding training set by pseudo labels of the unlabeled data commonly adopted in SSL, with the difference that we use the reliability-aware pseudo labels at the voxel level. Finally, the total loss of our UPCoL network is shown in Eq. ( <ref type="formula" target="#formula_12">8</ref>),</p><formula xml:id="formula_12">L = L seg + L l pc + λ con L u pc . (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>3 Experimental Results</p><p>Datasets. We evaluate our approach on three datasets: the pancreas dataset (82 CTA scans), the left atrium dataset (100 MR images), and a multi-center dataset for type B aortic dissection (TBAD, 124 CTA scans). The pancreas and left atrium datasets are preprocessed following previous studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28]</ref>.</p><p>The TBAD dataset is well-annotated by experienced radiologists, with 100 scans for training and 24 for test, including both public data <ref type="bibr" target="#b25">[25]</ref> and data collected by our team. The dataset was resampled to 1mm 3 and resized to 128 × 128 × 128, in accordance with <ref type="bibr" target="#b1">[2]</ref>. For all three datasets, we use only 20% of the training data with labels and normalize the voxel intensities to zero mean and unit variance. Implementation Details. We adopt V-Net <ref type="bibr" target="#b13">[13]</ref> as the backbone, and use the results of V-Nets trained with 20% and 100% labeled data as the lower and upper bounds, respectively. For the Mean-Teacher framework, the student network is trained for 10k iterations using Adam optimizer and learning rate 0.001, while the teacher is updated with exponential moving average (EMA) of the student's parameters. The batch size is 3, including 1 labeled and 2 unlabeled samples. Following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b22">22]</ref>, we randomly crop cubes of size 96×96×96 and 112×112×80 for the pancreas and left atrium datasets, respectively. In addition, we use 3fold cross validation and apply data augmentation by rotation within the range (-10 • , 10 • ) and zoom factors within the range (0.9, 1.1) for TBAD dataset during training, as proposed in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. Four performance metrics are adopted, i.e., Dice coefficient (Dice), Jaccard Index (Jac), 95% Hausdorff Distance (95HD), and Average Symmetric Surface Distance (ASD).</p><p>Results on the Left Atrium Dataset and Pancreas-CT Dataset. We compare UPCoL with nine SOTA SSL methods, including consistency-based (MT <ref type="bibr" target="#b16">[16]</ref>, MC-Net <ref type="bibr" target="#b21">[21]</ref>, ASE-Net <ref type="bibr" target="#b5">[6]</ref>, URPC <ref type="bibr" target="#b12">[12]</ref>), uncertainty-based (UA-MT <ref type="bibr" target="#b27">[27]</ref>, MC-Net+ <ref type="bibr" target="#b20">[20]</ref>), and divergence-based (DTC <ref type="bibr" target="#b10">[10]</ref>, SimCVD <ref type="bibr" target="#b26">[26]</ref>, CoraNet <ref type="bibr" target="#b15">[15]</ref>). The results (Table <ref type="table" target="#tab_0">1</ref>) demonstrate a substantial performance gap between the lower and upper bounds due to the limited labeled data. Remarkably, our proposed framework outperforms the theoretical upper bound (the second row) in terms of Dice, Jac, 95HD on the left atrium dataset, and 95HD, ASD on the pancreas dataset, suggesting that the unlabeled knowledge extracted from deep features is reliable and well complements the information that is not captured in the fully supervised prediction phase.</p><p>Results on the Aortic Dissection Dataset. We compare with two SOTA SSL methods, the uncertainty-based method FUSSNet <ref type="bibr" target="#b22">[22]</ref> and embedding-based method URPC <ref type="bibr" target="#b12">[12]</ref>, as well as two common SSL approaches, MT <ref type="bibr" target="#b16">[16]</ref> and UA-MT <ref type="bibr" target="#b27">[27]</ref>. As shown in Table <ref type="table" target="#tab_1">2</ref>, the proposed UPCoL obtains the best segmentation results and outperforms fully-supervised V-Net over 6% on Dice score, but only requires 20% labels. The accuracy of FUSSNet <ref type="bibr" target="#b22">[22]</ref>, URPC <ref type="bibr" target="#b12">[12]</ref>, and UPCoL surpassing the upper bound demonstrates the effectiveness of uncertainty and embedding-based approaches in exploiting the latent information of the data, particularly in challenging classification tasks. We further visualize the segmentation results on test data of different methods in Fig. <ref type="figure" target="#fig_1">2</ref>. As can be seen, UPCoL achieves superior segmentation performance with fewer false positives and superior capability in capturing intricate geometric features, such as the vessel walls between True Lumen (TL) and False Lumen (FL), and effectively smoothing out rough portions of the manual annotation.</p><p>Ablation Study. Here we investigate the contribution of key components, including the mean-teacher architecture (MT), the average multi-classifier (AMC) (to yield segmentation results), and the prototype learning (PL) strategy. As shown in Table <ref type="table" target="#tab_2">3</ref>, the MT model, which enforces a consistency cost between the predictions of student model and teacher model, outperforms vanilla V-Net by a large margin (over 5% on Dice), and AMC can also enhance the MT's performance (over 2% on Dice). Compared to the consistency cost in original MT model <ref type="bibr" target="#b16">[16]</ref> (used by MT and MT+AMC), the prototype consistency leads to better performance. Especially, we compare different prototype learning strategies in three methods, MT+PL, CPCL*, and UPCoL, which have the same backbone and AMC module. MT+PL performs prototype learning only for labeled data, CPCL* learns prototypes for labeled and unlabeled data separately using the same strategy proposed in <ref type="bibr" target="#b24">[24]</ref>, and UPCoL learns fused prototypes. As can be seen, prototype learning using unlabeled data is beneficial for performance improvement, but it requires a well-designed mechanism. Here CPCL* is slightly worse than using only labeled data for prototype learning, potentially due to the isolation of updating labeled and unlabeled prototypes, which may hinder their interaction and prevent the full utilization of knowledge. This highlights the importance of fusing labeled and unlabeled prototypes. UPCoL, on the other hand, successfully merges labeled and unlabeled prototypes through the use of reliability maps, resulting in SOTA performance. This demonstrates the effectiveness of uncertainty-based reliability assessment and prototype fusion in fully leveraging both labeled and unlabeled information. In the Supplementary Materials, visualized results showcase improved predicted labels and unlabeled prototypes as training progresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper presents a novel framework, UPCoL, for semi-supervised segmentation that effectively addresses the issue of label sparsity through uncertaintybased prototype consistency learning. To better utilize unlabeled data, UPCoL employs a quantitative uncertainty measure at the voxel level to assign degrees of attention. UPCoL achieves a careful and effective fusion of unlabeled data with labeled data in the prototype learning process, which leads to exceptional performance on both 2-class and 3-class medical image segmentation tasks. As future work, a possible extension is to allow multiple prototypes for a class with diversified semantic concepts, and a memory-bank-like mechanism could be introduced to learn prototypes from large sample pools more efficiently.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of UPCoL (using Aortic Dissection segmentation for illustration).The labeled images go through the student model for supervised learning, while the unlabeled images go through the teacher model for segmentation and uncertainty estimation. In Uncertainty-informed Prototype Fusion module, we utilize the reliability map to fuse prototypes learned from labeled and unlabeled embeddings. The similarity between fused prototypes and feature embeddings at each spatial location is then measured for consistency learning.</figDesc><graphic coords="3,58,29,265,85,307,09,167,35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of segmentation results on Type B Aortic Dissection dataset.</figDesc><graphic coords="8,55,98,54,59,340,15,146,11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on the Left Atrium and Pancreas datasets.</figDesc><table><row><cell>Method</cell><cell cols="2">Left Atrium</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pancreas-CT</cell></row><row><cell></cell><cell cols="3">Lb Unlb Dice Jac</cell><cell cols="5">95HD ASD Lb Unlb Dice Jac</cell><cell>95HD ASD</cell></row><row><cell>V-Net</cell><cell>16 0</cell><cell cols="5">84.41 73.54 19.94 5.32 12 0</cell><cell cols="2">70.63 56.72 22.54 6.29</cell></row><row><cell>V-Net</cell><cell>80 0</cell><cell cols="5">91.42 84.27 5.15 1.50 62 0</cell><cell cols="2">82.60 70.81 5.61 1.33</cell></row><row><cell>MT [16] (NIPS'17)</cell><cell>16 64</cell><cell cols="5">86.00 76.27 9.75 2.80 12 50</cell><cell cols="2">75.85 61.98 12.59 3.40</cell></row><row><cell cols="2">UA-MT [27] (MICCAI'19) 16 64</cell><cell cols="5">88.88 80.21 7.32 2.26 12 50</cell><cell cols="2">77.26 63.28 11.90 3.06</cell></row><row><cell>DTC [10] (AAAI'21)</cell><cell>16 64</cell><cell cols="5">89.42 80.98 7.32 2.10 12 50</cell><cell cols="2">76.27 62.82 8.70 2.20</cell></row><row><cell cols="2">MC-Net [21] (MICCAI'21) 16 64</cell><cell cols="5">90.34 82.48 6.00 1.77 12 50</cell><cell cols="2">78.17 65.22 6.90 1.55</cell></row><row><cell>MC-Net+ [20] (MIA'22)</cell><cell>16 64</cell><cell cols="5">91.07 83.67 5.84 1.67 12 50</cell><cell cols="2">80.59 68.08 6.47 1.74</cell></row><row><cell>ASE-Net [6] (TMI'22)</cell><cell>16 64</cell><cell cols="5">90.29 82.76 7.18 1.64 --</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CoraNet [15] (TMI'21)</cell><cell>--</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>12 50</cell><cell cols="2">79.49 67.10 11.10 3.06</cell></row><row><cell>SimCVD [26] (TMI'22)</cell><cell>16 64</cell><cell cols="5">90.85 83.80 6.03 1.86 --</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>URPC [12] (MIA'22)</cell><cell>--</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>12 50</cell><cell cols="2">80.02 67.30 8.51 1.98</cell></row><row><cell>UPCoL(Ours)</cell><cell>16 64</cell><cell cols="5">91.69 84.69 4.87 1.56 12 50</cell><cell cols="2">81.78 69.66 3.78 0.63</cell></row><row><cell cols="9">'Lb' and 'Unlb' denote the number of labeled samples and unlabeled samples, respec-</cell></row><row><cell>tively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison on the Aortic Dissection dataset.</figDesc><table><row><cell>Method</cell><cell>Lb Unlb</cell><cell></cell><cell cols="2">Dice(%) ↑</cell><cell cols="2">Jaccard(%) ↑</cell><cell>95HD(voxel) ↓</cell><cell>ASD(voxel) ↓</cell></row><row><cell></cell><cell></cell><cell>TL</cell><cell>FL</cell><cell>Mean TL</cell><cell>FL</cell><cell>Mean TL FL</cell><cell>Mean TL FL Mean</cell></row><row><cell>V-Net</cell><cell>20 0</cell><cell cols="5">55.51 48.98 52.25 39.81 34.79 37.30 7.24 10.17 8.71 1.27 3.19 2.23</cell></row><row><cell>V-Net</cell><cell>100 0</cell><cell cols="5">75.98 64.02 70.00 61.89 50.05 55.97 3.16 7.56 5.36 0.48 2.44 1.46</cell></row><row><cell>MT [16]</cell><cell>20 80</cell><cell cols="5">57.62 49.95 53.78 41.57 35.52 38.54 6.00 8.98 7.49 0.97 2.77 1.87</cell></row><row><cell cols="2">UA-MT [27] 20 80</cell><cell cols="5">70.91 60.66 65.78 56.15 46.24 51.20 4.44 7.94 6.19 0.83 2.37 1.60</cell></row><row><cell cols="2">FUSSNet [22] 20 80</cell><cell cols="5">79.73 65.32 72.53 67.31 51.74 59.52 3.46 7.87 5.67 0.61 2.93 1.77</cell></row><row><cell>URPC [12]</cell><cell>20 80</cell><cell cols="5">81.84 69.15 75.50 70.35 57.00 63.68 4.41 9.13 6.77 0.93 1.11 1.02</cell></row><row><cell>UPCoL</cell><cell>20 80</cell><cell cols="5">82.65 69.74 76.19 71.49 57.42 64.45 2.82 6.81 4.82 0.43 2.22 1.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study results on the pancreas dataset.</figDesc><table><row><cell>Method</cell><cell cols="3">Component indication Metric</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MT</cell><cell>PL</cell><cell>Dice</cell><cell>Jaccard</cell><cell>95HD</cell><cell>ASD</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(voxel)</cell><cell>(voxel)</cell></row><row><cell></cell><cell></cell><cell>Lb Unlb Fuse</cell><cell></cell><cell></cell><cell></cell></row><row><cell>W/o Consistency</cell><cell></cell><cell></cell><cell>70.63</cell><cell>56.72</cell><cell>22.54</cell><cell>6.29</cell></row><row><cell>Prediction Consistency MT</cell><cell></cell><cell></cell><cell>75.85</cell><cell>61.98</cell><cell>12.59</cell><cell>3.40</cell></row><row><cell>MT + AMC</cell><cell></cell><cell></cell><cell>77.97</cell><cell>64.46</cell><cell>9.80</cell><cell>2.52</cell></row><row><cell>Prototype Consistency MT + PL</cell><cell></cell><cell></cell><cell>80.50</cell><cell>68.11</cell><cell>4.49</cell><cell>0.74</cell></row><row><cell>CPCL  *</cell><cell></cell><cell></cell><cell>80.08</cell><cell>67.29</cell><cell>7.68</cell><cell>2.08</cell></row><row><cell>UPCoL</cell><cell></cell><cell></cell><cell cols="3">81.78 69.66 3.78</cell><cell>0.63</cell></row></table><note><p>'Lb', 'Unlb' and 'Fuse' denote the labeled, unlabeled, and fused prototypes, respectively.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (Nos. <rs type="grantNumber">61972251</rs> and <rs type="grantNumber">62272300</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8DgRHQg">
					<idno type="grant-number">61972251</idno>
				</org>
				<org type="funding" xml:id="_CQdVYDv">
					<idno type="grant-number">62272300</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_63.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for network-based cardiac MR image segmentation</title>
		<author>
			<persName><forename type="first">Wenjia</forename><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66185-8_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66185-8_29" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">Louis</forename><surname>Duchesne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Simon</forename></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10434</biblScope>
			<biblScope unit="page" from="253" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully automatic segmentation of type b aortic dissection from CTA images enabled by deep learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Europ. J. Radiol</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page">108713</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d automatic segmentation of aortic computed tomography angiography combining multi-view 2d convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fantazzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cardiovascular Eng. Technol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="576" to="586" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Local and global structure-aware entropy regularized mean teacher model for 3D left atrium segmentation</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Hang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_55</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-8_55" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><surname>Kevin</surname></persName>
		</editor>
		<editor>
			<persName><surname>Racoceanu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Daniel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Leo</forename><surname>Joskowicz</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image segmentation using adversarial consistency learning and dynamic convolution network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Nandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xuming: Shape-aware semi-supervised 3d semantic segmentation for medical images</title>
		<author>
			<persName><forename type="first">Shuailin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Chuyu</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">Anne</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<editor>
			<persName><surname>Abolmaesumi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Purang</surname></persName>
		</editor>
		<editor>
			<persName><surname>Stoyanov</surname></persName>
		</editor>
		<editor>
			<persName><surname>Danail</surname></persName>
		</editor>
		<editor>
			<persName><surname>Mateus</surname></persName>
		</editor>
		<editor>
			<persName><surname>Diana</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria</forename><forename type="middle">A</forename><surname>Zuluaga</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><surname>Kevin</surname></persName>
		</editor>
		<editor>
			<persName><surname>Racoceanu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Daniel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Leo</forename><surname>Joskowicz</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="552" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_54</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-8_54" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transformationconsistent self-ensembling model for semisupervised medical image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="523" to="534" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image segmentation through dual-task consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8801" to="8809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient semi-supervised gross target volume of nasopharyngeal carcinoma segmentation via uncertainty rectified pyramid consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image segmentation via uncertainty rectified pyramid consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102517</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>Ieee</publisher>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
	<note>fourth international conference on 3D vision (3DV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Asdnet: attention based semi-supervised deep networks for medical image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<editor>MICCAI</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inconsistency-aware uncertainty estimation for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="608" to="620" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Panet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="page" from="9197" to="9206" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation using unreliable pseudo-labels</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4248" to="4257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Querying labeled for unlabeled: Cross-image semantic consistency guided semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mutual consistency learning for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">102530</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised left atrium segmentation with mutual consistency training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fussnet: Fusing two sources of uncertainty for semisupervised medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>MICCAI 2022</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="481" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">All-around real label supervision: Cyclic prototype consistency learning for semisupervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3174" to="3184" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagetbad: A 3d computed tomography angiography image dataset for automatic segmentation of type-b aortic dissection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Physiology</title>
		<imprint>
			<biblScope unit="page">1611</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simcvd: Simple contrastive voxel-wise representation distillation for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2228" to="2237" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Uncertainty-aware self-ensembling model for semi-supervised 3d left atrium segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<editor>MICCAI</editor>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reciprocal learning for semi-supervised segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sg-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3855" to="3865" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
