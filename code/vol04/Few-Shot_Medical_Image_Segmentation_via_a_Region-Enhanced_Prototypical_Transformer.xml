<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yazhou</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<postCode>210094</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shidong</forename><surname>Wang</surname></persName>
							<email>shidong.wang@newcastle.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Newcastle University</orgName>
								<address>
									<settlement>Newcastle upon Tyne NE17RU</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Xin</surname></persName>
							<email>tong.xin@newcastle.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">Newcastle University</orgName>
								<address>
									<settlement>Newcastle upon Tyne NE17RU</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haofeng</forename><surname>Zhang</surname></persName>
							<email>zhanghf@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<postCode>210094</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="271" to="280"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">CD84EEB85A8649124DADC4FFFADEBC2D</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_26</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Few-Shot Learning</term>
					<term>Medical Image Segmentation</term>
					<term>Bias Alleviation</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated segmentation of large volumes of medical images is often plagued by the limited availability of fully annotated data and the diversity of organ surface properties resulting from the use of different acquisition protocols for different patients. In this paper, we introduce a more promising few-shot learning-based method named Region-enhanced Prototypical Transformer (RPT) to mitigate the effects of large intraclass diversity/bias. First, a subdivision strategy is introduced to produce a collection of regional prototypes from the foreground of the support prototype. Second, a self-selection mechanism is proposed to incorporate into the Bias-alleviated Transformer (BaT) block to suppress or remove interferences present in the query prototype and regional support prototypes. By stacking BaT blocks, the proposed RPT can iteratively optimize the generated regional prototypes and finally produce rectified and more accurate global prototypes for Few-Shot Medical Image Segmentation (FSMS). Extensive experiments are conducted on three publicly available medical image datasets, and the obtained results show consistent improvements compared to state-of-the-art FSMS methods. The source code is available at: https://github.com/YazhouZhu19/RPT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic medical image segmentation is the implementation of data-driven image segmentation concepts to identify a specific anatomical structure's surface or volume in a medical image ranging from X-ray and ultrasonography to CT and MRI scans. Deep learning algorithms are exquisitely suited for this task because they can generate measurements and segmentations from medical images without the time-consuming manual work as in traditional methods. However, the performance of deep learning algorithms depends heavily on the availability of large-scale, high-quality, fully pixel-wise annotations, which are often expensive to acquire. To this end, few-shot learning is considered as a more promising approach and introduced into the medical image segmentation by <ref type="bibr" target="#b13">[13]</ref>.</p><p>Through revisiting existing FSMS algorithms <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b19">19]</ref>, they can be grouped into two folders, including the interactive method originated from SENet <ref type="bibr" target="#b15">[15]</ref> (shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>) and the prototype networks <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b20">20]</ref> (demonstrated in Fig. <ref type="figure" target="#fig_0">1(b)</ref>). For the interaction-based approach, the ideas of attention <ref type="bibr" target="#b19">[19]</ref>, and contrastive learning <ref type="bibr" target="#b22">[22]</ref> are introduced to work interactively between parallel support and query arms. In contrast, prototype network-based approach almost dominates the FSMS research, such as SSL-ALPNet <ref type="bibr" target="#b13">[13]</ref>, ADNet <ref type="bibr" target="#b4">[5]</ref> and SR&amp;CL <ref type="bibr" target="#b21">[21]</ref>, whose core idea is to obtain semantic-level prototypes by compressing support features, and then make predictions by matching with query features. However, the problem of how to obtain an accurate and representative prototype remains.</p><p>The main reason affecting the representativeness of the prototype is the significant discrepancy between support and query. Specifically, in general, different protocols are taken for different patients, which results in a variety of superficial organ appearances, including the size, shape, and contour of features. In this case, the prototype generated from the support features may not accurately represent the key attributes of the target organ in the query image. In addition, it is also challenging to extract useful information (prototypes of novel classes) from the cluttered background due to the extremely heterogeneous texture between the target and its surroundings, which may contain information belonging to some novel classes or redundant information issue <ref type="bibr" target="#b19">[19]</ref>.</p><p>To mitigate the impact of intra-class diversity, it considers subdividing the foreground of the supporting prototypes to produce some regional prototypes, which are then rectified to suppress or exclude areas inconsistent with the query targets, as illustrated in Fig. <ref type="figure" target="#fig_0">1(c</ref>). Concretely, in the prototype learning stage, multiple subdivided regional prototypes are enhanced with a more accurate class center, which can be derived from the newly designed Regional Prototype Generation (RPG) and Query Prototype Generation (QPG) modules. Then, a designed Region-enhanced Prototypical Transformer (RPT) that is mainly composed of a number of stacked Bias-alleviated Transformer (BaT) blocks, each of which contains the core debiasing function-Search and Filter (S&amp;F) modules, to filter out undesirable prototypes. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, Our contributions are summarized as follows:</p><p>-A Region-enhanced Prototypical Transformer (RPT) consisting of stacked Bias-alleviated Transformer (BaT) blocks is proposed to mitigate the effects of large intra-class variations present in FSMS through Search and Filter (S&amp;F) modules devised based on the self-selection mechanism. -A subdivision strategy is proposed to perform in the foreground of the support prototype to generate multiple regional prototypes, which can be further iteratively optimized by the RPT to produce the optimal prototype. -The proposed method can achieve state-of-the-art performance on three experimental datasets commonly used in medical image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Architecture</head><p>Before introducing the overall architecture, it is necessary to briefly explain how data is processed. Specifically, the 3D supervoxel clustering method <ref type="bibr" target="#b4">[5]</ref> is employed to generate pseudo-masks as supervision, which is learned in a selfsupervised learning manner without any manual annotations. Meta-learningbased episodic tasks can then be constructed using the generated pseudo-masks. Notably, the pseudo-masks obtained by the 3D clustering method is more consistent with the volumetric properties of medical images than the 2D superpixel clustering method adopted in <ref type="bibr" target="#b13">[13]</ref>.</p><p>As depicted in Fig. <ref type="figure" target="#fig_1">2</ref>, the overall architecture includes three main components: the Regional Prototype Generation (RPG) module, the Query Prototype Generation (QPG) module and the Region-enhanced Prototypical Transformer (RPT) consisting of three Bias-alleviated Transformer (BaT) blocks. The pipeline first extracts features from support and query images using a weightshared ResNet-101 <ref type="bibr" target="#b5">[6]</ref> as a backbone, which has been pretrained on the MS-COCO dataset <ref type="bibr" target="#b9">[10]</ref>. We employ the ResNet101 pretrained on MS-COCO for optimal performance, and the comparison with ResNet50 pretrained on Ima-geNet dataset <ref type="bibr" target="#b1">[2]</ref> is also included in the appendix. The extracted features are then taken as the input of the RPG and QPG modules to generate multiple region prototypes, which will be rectified by the following RPT to produce the optimal prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Regional Prototype Generation</head><p>The core problem considered in this paper is what causes prototype bias. By examining the input data, it can be observed that images of healthy and diseased organs have a chance to be considered as support or query. This means that if there are lesioned or edematous regions in some areas of the support images, they will be regarded as biased information which in reality cannot be accurately transferred for the query images containing only healthy organs. When these prototypes that contain the natural heterogeneity of the input images are processed by the Masked Average Pooling (MAP) operation, they inevitably lead to significant intra-class biases.</p><p>To cope with the above problems, we propose a Region Prototype Generation (RPG) module to generate multi-region prototypes by performing subdivisions in the foreground of the support images. Given an input support image I s and the corresponding foreground mask M f , the foreground of this image can be obtained by calculating their product. The foreground image then can be partitioned into N f regions, where N f is set to 10 by default. By using the Voronoi-based partition method <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">23]</ref>, a set of regional masks {V n } N f n=1 can be derived for subsequent use of Masked Average Pooling (MAP) to generate a set of coarse regional prototypes</p><formula xml:id="formula_0">Ps = {p n } N f n=1 , pn ∈ R C . Formally, pn = MAP(F s , V n ) = 1 |V n | HW i=1 F s,i V n,i ,<label>(1)</label></formula><p>where F s ∈ R C×H×W is the feature extracted from the support images and V n denotes the regional masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query Prototype Generation</head><p>Once a set of coarse regional prototypes Ps have been generated for the support images, we can employ the method introduced in <ref type="bibr" target="#b10">[11]</ref> to learn the coarse query prototype Pq ∈ R 1×C . Concretely, it first uses the MAP(•) operator as introduced in Eq. ( <ref type="formula" target="#formula_0">1</ref>) to learn a global support prototype P g = MAP(F s , M s ) with P g ∈ R 1×C , whose output can then be used to calculate the coarse query foreground mask Mf q . Considering that the empirically designed threshold described in <ref type="bibr" target="#b10">[11]</ref> may affect the quality of the Mf q , we hereby introduce a learnable threshold τ . This process can be denoted as</p><formula xml:id="formula_1">Mf q = 1 -σ(S(F q , P g ) -τ ),<label>(2)</label></formula><p>where F q ∈ R C×H×W is feature extracted from query images, S(a, b) = -αcos(a, b) is the negative cosine similarity with a fixed scaling factor α = 20, σ denotes the Sigmoid activation, and τ is obtained by applying one averagepooling and two fully-connected layers (FC) to the query feature, expressed as τ = FC(F q ). After this, the coarse query foreground prototype can be achieved by using Pq = MAP(F q,i , Mf q,i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Region-Enhanced Prototypical Transformer</head><p>The above received prototypes Ps and Pq are taken as input to the proposed Region-enhanced Prototypical Transformer (RPT) to rectify and regenerate the optimal global prototype P s . As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our RPT mainly consists of L stacked Bias-alleviated Transformer (BaT) blocks each of which contains a Search and Filter (S&amp;F) module, and QPG modules that maintain the query prototypes continuously updated. Taking the first BaT block as an example, it calculates an affinity map A = Ps P q ∈ R N f ×1 to reveal the correspondence between the query and N f support regional prototypes by taking an input containing the query prototype Pq and the support prototype Ps ∈ R N f ×C obtained by concatenating all elements in Ps . Then, a selective map S ∈ R N f ×1 can be derived from the proposed self-selection based S&amp;F module by</p><formula xml:id="formula_2">S i (A i ) = 0 if A i &gt;= ξ -∞ otherwise , i ∈ {0, 1, ..., N f } , (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where ξ is the selection threshold achieved by ξ = (min(A) + mean(A))/2, S indicates the chosen regions from the support image that performs compatible with the query at the prototypical level. Then, the heterogeneous or disturbing regions of support foreground will be weeded out with softmax(•) function. The preliminary rectified prototypes Po s ∈ R N f ×C is aggregated as:</p><formula xml:id="formula_4">Po s = softmax( Ps P q + S) Pq . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>The refined Po s will be fed into the following components designed based on the multi-head attention mechanism to produce the output</p><formula xml:id="formula_6">P 1 s ∈ R N f ×C . Formally, Po+1 s = LN(MHA( Po s ) + Po s ), P 1 s = LN(MLP( Po+1 s ) + Po+1 s ),<label>(5)</label></formula><p>where Po+1 s ∈ R N f ×C is the intermediate generated prototype, LN(•) denotes the layer normalization, MHA(•) represents the standard multi-head attention module and MLP(•) is the multilayer perception.</p><p>By stacking multiple BaT blocks, our RPT can iteratively rectify and update all coarse support and the query prototype. Given the prototypes P l-1 s and P l-1 q from the previous BaT block, the updates for the current BaT block are computed by:</p><formula xml:id="formula_7">P l s = BaT(P l-1 s , P l-1 q ), P l q = QPG(GAP(P l s ), F q ),<label>(6)</label></formula><p>where P l s ∈ R N f ×C and P l q ∈ R 1×C (l = 1, 2, ..., L) are updated prototypes, GAP(•) denotes the global average pooling operation. The final output prototypes P s optimized by the RPT can be used to predict the foreground of the query image by using Eq. (2: Mf q = 1σ(S(F q , GAP(P 3 s ))τ ), while its background can be obtained by Mb q = 1 -Mf q accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Objective Function</head><p>The binary cross-entropy loss L ce is adopted to determine the error between the predict masks Mq and the given ground-truth M q . Formally,</p><formula xml:id="formula_8">L ce = - 1 HW H h W w M f q (x, y)log( Mf q (x, y)) + M b q (x, y)log( Mb q (x, y)). (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>Considering the prevalent class imbalance problem in medical image segmentation, the boundary loss <ref type="bibr" target="#b7">[8]</ref> L B is also adopted and it is written as</p><formula xml:id="formula_10">L B (θ) = Ω φG(q)s θ (q)d q , (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where θ denotes the network parameters, Ω denotes the spatial domain, φG : Ω → R denotes the level set representation of the ground-truth boundary, φG(q) = -D G (q) if q ∈ G and φG(q) = D G (q) otherwise, D G is distance map between the boundary of prediction and ground-truth, and s θ (q) : Ω → [0, 1] denotes softmax(•) function.</p><p>Overall, the loss used for training our RPT is defined as</p><formula xml:id="formula_12">L = L ce + ηL dice + (1 -η)L B ,</formula><p>where L dice is the Dice loss <ref type="bibr" target="#b12">[12]</ref>, η is initially set to 1 and decreased by 0.01 every epoch. Experiment Setup: The model is trained for 30k iterations with batch size set to 1. During training, the initial learning rate is set to 1 × 10 -3 with a step decay of 0.8 every 1000 iterations. The values of N f and iterations L are set to 10 and 3, respectively. To simulate the scarcity of labeled data in medical scenarios, all experiments embrace a 1-way 1-shot setting, and 5-fold cross-validation is also carried out in the experiments, where we only record the mean value.</p><p>Evaluation: For a fair comparison, the metric used to evaluate the performance of 2D slices on 3D volumetric ground-truth is the Dice score used in <ref type="bibr" target="#b13">[13]</ref>. Furthermore, two different supervision settings are used to evaluate the generalization ability of the proposed method: in Setting 1, the test classes may appear in the background of the training slices, while in Setting 2, the training slices containing the test classes are removed from the dataset to ensure that the test classes are unseen. Note that Setting 2 is impractical for Card-MRI scans, since all classes typically co-occur on one 2D slice, making label exclusion impossible.</p><p>In addition, as in <ref type="bibr" target="#b13">[13]</ref>, abdominal organs are categorized into upper abdomen  (liver, spleen) and lower abdomen (left, right kidney) to demonstrate whether the learned representations can encode spatial concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quantitative and Qualitative Results</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the performance comparison of the proposed method with stateof-the-art methods, including the vanilla PA-Net <ref type="bibr" target="#b20">[20]</ref>, SE-Net <ref type="bibr" target="#b15">[15]</ref>, ADNet <ref type="bibr" target="#b4">[5]</ref>, CRAPNet <ref type="bibr" target="#b2">[3]</ref>, SSL-ALPNet <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref>, AAS-DCL <ref type="bibr" target="#b22">[22]</ref> and SR&amp;CL <ref type="bibr" target="#b21">[21]</ref> under two experimental settings. From Table <ref type="table" target="#tab_0">1</ref>, it can be seen that the proposed method outperforms all listed methods in terms of the Mean values obtained under two different settings. Especially, the Mean value on Abd-CT dataset under Setting 1 reaches 77.83, which is 3.31 higher than the best result achieved by AAS-DCL. Consistent improvements are also indicated for Card-MRI dataset and can be found in the Appendix. In addition to the quantitative comparisons, qualitative results of our model and the other model on Abd-MRI and Abd-CT are shown in Fig. <ref type="figure">3</ref> (See Appendix for CMR dataset). It is not difficult to see that our model shows considerable bound-preserving and generalization capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Studies</head><p>The ablation studies were conducted on Abd-MRI dataset under Setting 2. As can be seen from Fig. <ref type="figure">4</ref>, the use of three stacked BaT blocks is suggested to obtain the best Dice score. From Table <ref type="table" target="#tab_1">2</ref>, using a combination of boundary and dice loss gives a 0.61 increase in terms of the dice score compared to using only the cross-entropy loss. More ablation study results can be found in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we introduced a Region-enhanced Prototypical Transformer (RPT) to mitigate the impact of large intra-class variations present in medical image segmentation. The model is mainly beneficial from a subdivision-based strategy used for generating a set of regional support prototypes and a self-selection mechanism introduced to the Bias-alleviated Transformer (BaT) blocks. The proposed RPT can iteratively optimize the generated regional prototypes and output a more precise global prototype for predictions. The results of extensive experiments and ablation studies can demonstrate the advancement and effectiveness of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison between previous FSMS models and our model. (a) Interactive model. (b) Prototypical network based model. (c) Our proposed model.</figDesc><graphic coords="2,58,83,54,23,303,61,101,41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed Region-enhanced Prototypical Transformer.</figDesc><graphic coords="4,43,80,74,78,333,67,220,21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Qualitative results of our model on Abd-MRI and Abd-CT.</figDesc><graphic coords="8,52,29,54,05,319,45,129,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative Comparison (in Dice score %) of different methods on abdominal datasets under Setting 1 and Setting 2. The proposed method is comprehensively evaluated on three publicly available datasets, including Abd-MRI, Abd-CT and Card-MRI. Concretely, Abd-MRI<ref type="bibr" target="#b6">[7]</ref> is an abdominal MRI dataset used in the ISBI 2019 Combined Healthy Abdominal Organ Segmentation Challenge. Abd-CT<ref type="bibr" target="#b8">[9]</ref> is an abdominal CT dataset from MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge. Card-MRI<ref type="bibr" target="#b24">[24]</ref> is a cardiac MRI dataset from MICCAI 2019 Multi-Sequence Cardiac MRI Segmentation Challenge. All 3D scans are reformatted into 2D axial and 2D short-axis slices. The abdominal datasets Abd-MRI and Abd-CT share the same categories of labels which includes the liver, spleen, left kidney (LK) and right kidney (RK). The labels for Card-MRI include left ventricular myocardium (LV-MYO), right ventricular myocardium (RV), and blood pool (LV-BP).</figDesc><table><row><cell cols="2">Setting Method</cell><cell>Reference</cell><cell cols="2">Abd-MRI</cell><cell></cell><cell>Abd-CT</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Lower</cell><cell></cell><cell>Upper</cell><cell>Mean Lower</cell><cell>Upper</cell><cell>Mean</cell></row><row><cell></cell><cell></cell><cell></cell><cell>LK</cell><cell>RK</cell><cell>Spleen Liver</cell><cell>LK</cell><cell>RK</cell><cell>Spleen Liver</cell></row><row><cell>1</cell><cell>ADNet [5]</cell><cell>MIA'22</cell><cell cols="4">73.86 85.80 72.29 82.11 78.51 72.13 79.06 63.48 77.24 72.97</cell></row><row><cell></cell><cell cols="2">AAS-DCL [22] ECCV'22</cell><cell cols="4">80.37 86.11 76.24 72.33 78.76 74.58 73.19 72.30 78.04 74.52</cell></row><row><cell></cell><cell>SR&amp;CL [21]</cell><cell cols="5">MICCAI'22 79.34 87.42 76.01 80.23 80.77 73.45 71.22 73.41 76.06 73.53</cell></row><row><cell></cell><cell cols="6">CRAPNet [3] WACV'23 81.95 86.42 74.32 76.46 79.79 74.69 74.18 70.37 75.41 73.66</cell></row><row><cell></cell><cell cols="2">Ours (RPT) -</cell><cell cols="4">80.72 89.82 76.37 82.86 82.44 77.05 79.13 72.58 82.57 77.83</cell></row><row><cell>2</cell><cell>ADNet [5]</cell><cell>MIA'22</cell><cell cols="4">59.64 56.68 59.44 77.03 63.20 48.41 40.52 50.97 70.63 52.63</cell></row><row><cell></cell><cell cols="2">AAS-DCL [22] ECCV'22</cell><cell cols="4">76.90 83.75 74.86 69.94 76.36 64.71 69.95 66.36 71.61 68.16</cell></row><row><cell></cell><cell>SR&amp;CL [21]</cell><cell cols="5">MICCAI'22 77.07 84.24 73.73 75.55 77.65 67.39 63.37 67.36 73.63 67.94</cell></row><row><cell></cell><cell cols="6">CRAPNet [3] WACV'23 74.66 82.77 70.82 73.82 75.52 70.91 67.33 70.17 70.45 69.72</cell></row><row><cell></cell><cell cols="2">Ours (RPT) -</cell><cell cols="4">78.33 86.01 75.46 76.37 79.04 72.99 67.73 70.80 75.24 71.69</cell></row><row><cell cols="3">3 Experiments</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Experimental Datasets:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of the three loss functions.</figDesc><table><row><cell>Lce LB L dice Dice score</cell></row><row><cell>78.43</cell></row><row><cell>78.81</cell></row><row><cell>79.04</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_26.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Voronoi diagrams: a survey of a fundamental geometric data structure</title>
		<author>
			<persName><forename type="first">F</forename><surname>Aurenhammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv. (CSUR)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="405" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Few-shot medical image segmentation with cycle-resemblance attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2488" to="2497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interactive few-shot learning: limited supervision, better medical image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2575" to="2588" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Anomaly detectioninspired few-shot medical image segmentation through self-supervision with supervoxels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">102385</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Chaos challenge-combined (CT-MR) healthy abdominal organ segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Kavur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101950</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boundary loss for highly unbalanced segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bouchtiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="285" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MICCAI multi-atlas labeling beyond the cranial vault-workshop and challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Igelsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Langerak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</title>
		<meeting>MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10602-1_48" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Prototype refinement network for few-shot segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03579</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Loss odyssey in medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102035</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-supervision with superpixels: training few-shot medical image segmentation without annotation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Biffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58526-6_45</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58526-6_45" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12374</biblScope>
			<biblScope unit="page" from="762" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised learning for few-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Biffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1837" to="1848" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">&apos;squeeze &amp; excite&apos; guided few-shot segmentation of volumetric images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pölsterl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101587</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Q-Net: query-informed few-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.11451</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PoissonSeg: semi-supervised few-shot medical image segmentation via poisson learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Bioinformatics and Biomedicine</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1513" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Few-shot medical image segmentation using a global correlation network with discriminative embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">105067</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PANet: few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9197" to="9206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Few-shot medical image segmentation regularized with self-reference and contrastive learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="514" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual contrastive learning with anatomical auxiliary supervision for few-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20044-1_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20044-1_24" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="417" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature-proxy transformer for few-shot segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advance in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multivariate mixture model for myocardial segmentation combining multi-source images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2933" to="2946" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
