<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation</title>
				<funder ref="#_pfWrNV8">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_FdSfYGN">
					<orgName type="full">Shanghai Municipal Science and Technology Major</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yi</forename><surname>Hong</surname></persName>
							<email>yi.hong@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="673" to="681"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">79A8D5C17293AEEC2F0D098F2ECF563D</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_64</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Modality-adaptive fusion</term>
					<term>Missing modality</term>
					<term>Brain tumor segmentation</term>
					<term>Incomplete multi-modal segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Magnetic Resonance Imaging (MRI) plays an important role in multi-modal brain tumor segmentation. However, missing modality is very common in clinical diagnosis, which will lead to severe segmentation performance degradation. In this paper, we propose a simple adaptive multi-modal fusion network for brain tumor segmentation, which has two stages of feature fusion, including a simple average fusion and an adaptive fusion based on an attention mechanism. Both fusion techniques are capable to handle the missing modality situation and contribute to the improvement of segmentation results, especially the adaptive one. We evaluate our method on the BraTS2020 dataset, achieving the state-ofthe-art performance for the incomplete multi-modal brain tumor segmentation, compared to four recent methods. Our A2FSeg (Average and Adaptive Fusion Segmentation network) is simple yet effective and has the capability of handling any number of image modalities for incomplete multi-modal segmentation. Our source code is online and available at https://github.com/Zirui0623/A2FSeg.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extracting brain tumors from medical image scans plays an important role in further analysis and clinical diagnosis. Typically, a brain tumor includes peritumoral edema, enhancing tumor, and non-enhancing tumor core. Since different modalities present different clarity of brain tumor components, we often use multi-modal image scans, such as T1, T1c, T2, and Flair, in the task of brain tumor segmentation <ref type="bibr" target="#b11">[12]</ref>. Works have been done to handle brain tumor segmentation using image scans collected from all four modalities <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>. However, in practice, we face the challenge of collecting all modalities at the same time, with often one or more missing. Therefore, in this paper, we consider the problem of segmenting brain tumors with missing image modalities.</p><p>Current image segmentation methods for handling missing modalities can be divided into three categories, including: 1) brute-force methods: designing individual segmentation networks for each possible modality combination <ref type="bibr" target="#b17">[18]</ref>, 2) completion methods: synthesizing the missing modalities to complete all modalities required for conventional image segmentation methods <ref type="bibr" target="#b15">[16]</ref>, and 3) fusionbased methods: mapping images from different modalities into the same feature space for fusion and then segmenting brain tumors based on the fused features <ref type="bibr" target="#b9">[10]</ref>. Methods in the first category have good segmentation performance; however, they are resource intensive and often require more training time. The performance of methods in the second category is limited by the synthesis quality of the missing modality. The third category often has one single network to take care of different scenarios of missing modalities, which is the most commonly used one in practice.</p><p>To handle various numbers of modal inputs, HeMIS <ref type="bibr" target="#b4">[5]</ref> projects the image features of different modalities into the same feature space, by computing the mean and variance of the feature maps extracted from different modalities as the fused features. To improve the representation of feature fusion, HVED <ref type="bibr" target="#b2">[3]</ref> treats the input of each modality as a Gaussian distribution, and fuses feature maps from different modalities through a Gaussian mixture model. RobustSeg <ref type="bibr" target="#b0">[1]</ref>, on the other hand, decomposes the modality features into modality-invariant content code and modality-specific appearance code, for more accurate fusion and segmentation. Considering the different clarity of brain tumor regions observed in different modalities, RFNet <ref type="bibr" target="#b1">[2]</ref> introduces an attention mechanism to model the relations of modalities and tumor regions adaptively. Based on graph structure and attention mechanism, MFI <ref type="bibr" target="#b20">[21]</ref> is proposed to learn adaptive complementary information between modalities in different missing situations.</p><p>Due to the complexity of current models, we tend to develop a simple model, which adopts a simple average fusion and attention mechanism. These two techniques are demonstrated to be effective in handling missing modalities and multimodal fusion <ref type="bibr" target="#b16">[17]</ref>. Inspired by MAML <ref type="bibr" target="#b19">[20]</ref>, we propose a model called A2FSeg (Average and Adaptive Fusion Segmentation network, see Fig. <ref type="figure" target="#fig_0">1</ref>), which has two fusion steps, i.e., an average fusion and an attention-based adaptive fusion, to integrate features from different modalities for segmentation. Although our fusion idea is quite simple, A2FSeg achieves state-of-the-art (SOTA) performance in the incomplete multimodal brain tumor image segmentation task on the BraTS2020 dataset. Our contributions in this paper are summarized below:</p><p>-We propose a simple multi-modal fusion network, A2FSeg, for brain tumor segmentation, which is general and can be extended to any number of modalities for incomplete image segmentation. -We conduct experiments on the BraTS 2020 dataset and achieve the SOTA segmentation performance, having a mean Dice core of 89.79% for the whole tumor, 82.72% for the tumor core, and 66.71% for the enhancing tumor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Figure <ref type="figure" target="#fig_0">1</ref> presents the network architecture of our A2FSeg. It consists of four modality-specific sub-networks to extract features from each modality, an average fusion module to simply fuse features from available modalities at the first stage, and an adaptive fusion module based on an attention mechanism to adaptively fuse those features again at the second stage.</p><p>Modality-Specific Feature Extraction (MSFE) Module. Before fusion, we first extract features for every single modality, using the nnUNet model <ref type="bibr" target="#b6">[7]</ref> as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In particular, this MSFE model takes a 3D image scan from a specific modality m, i.e., I m ∈ R H×W ×D and m ∈ {T1, T2, T1c, Flair}, and outputs the corresponding image features</p><formula xml:id="formula_0">F m ∈ R C×H f ×W f ×D f .</formula><p>Here, the number of channels is C = 32; H f , W f , and D f are the height, width, and depth of feature maps F m , which share the same size as the input image. For every single modality, each MSFE module is supervised by the image segmentation mask to fasten its convergence and provide a good feature extraction for fusion later. All four MSFEs have the same architecture but with different weights.</p><p>Average Fusion Module. To aggregate image features from different modalities and handle the possibility of missing one or more modalities, we use the average of the available features from different modalities as the first fusion result. That is, we obtain a fused average feature</p><formula xml:id="formula_1">F = 1 Nm Nm m=1 F m .</formula><p>Here, N m is the number of available modalities. For example, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, if only the first two modalities are available at an iteration, then N m = 2, and we will take the average of these two modalities, ignoring those missing ones.</p><p>Adaptive Fusion Module. Since each modality contributes differently to the final tumor segmentation, similar to MAML <ref type="bibr" target="#b19">[20]</ref>, we adopt the attention mechanism to measure the voxel-level contributions of each modality to the final segmentation. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, to generate the attention map for a specific modality m, we take the concatenation of its feature extracted by the MSFE module F m and the mean feature after the average fusion F, which is passed through a convolutional layer to generate the initial attention weights:</p><formula xml:id="formula_2">W m = σ F m F; F m ; θ m , m ∈ {T1, T1c, T2, Flair}.<label>(1)</label></formula><p>Here, F m is a convolutional layer for this specific modality m, and θ m represents the parameters of this layer, and σ is a Sigmoid function. That is, we have an individual convolution layer F m for each modality to generate different weights.</p><p>Due to the possibility of missing modalities, we will have different numbers of feature maps for fusion. To address this issue, we normalize the different attention weights by using a Softmax function:</p><formula xml:id="formula_3">Ŵm = exp (W m ) Nm m exp (W m ) . (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>That is, we only consider feature maps from those available modalities but normalize their contribution to the final fusion result, so that, the fused one has a consistent value range, no matter how many modalities are missing. Then, we perform voxel-wise multiplication of the attention weight with the corresponding modal feature maps. As a result, the adaptively fused feature maps F is calculated by the weighted sum of each modal feature:</p><formula xml:id="formula_5">F = m Ŵm ⊗ F m . (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>Here, ⊗ indicates the voxel-wise multiplication.</p><p>Loss Function. We have multiple segmentation heads, which are distributed in each module of A2FSeg. For each segmentation head, we use the combination of the cross-entropy and the soft dice score as the basic loss function, which is defined as</p><formula xml:id="formula_7">L(ŷ, y) = L CE (ŷ, y) + L Dice (ŷ, y), (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where ŷ and y represent the segmentation prediction and the ground truth, respectively. Based on this basic one, we have the overall loss function defined as</p><formula xml:id="formula_9">L total = m L m (ŷ m , y) + L avg (ŷ avg , y) + L adp (ŷ adp , y), (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where the first term is the basic segmentation loss for each modality m after feature extraction; the second term is the loss for the segmentation output of the average fusion module; and the last term is the segmentation loss for the final output from the adaptive fusion module. 3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>Our experiments are conducted on BraTS2020, which contains 369 multicontrast MRI scans with four modalities: T1, T1c, T2, and Flair. These images went through a sequence of preprocessing steps, including co-registration to the same anatomical template, resampling to the same resolution (1 mm 3 ), and skullstripping. The segmentation masks have three labels, including the whole tumor (abbreviated as Complete), tumor core (abbreviated as Core), and enhancing tumor (abbreviated as Enhancing). These annotations are manually provided by one to four radiologists according to the same annotation protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Settings and Implementation Details</head><p>We implement our model with PyTorch <ref type="bibr" target="#b12">[13]</ref> and perform experiments on an Nvidia RTX3090 GPU. We use the Adam optimizer <ref type="bibr" target="#b7">[8]</ref>, with an initial learning rate of 0.01. Since we use the method of exponential decay of learning rate, the initial learning rate is then multiplied by (1 -#epoch #max_epoch ) 0.9 . Due to the limitation of GPU memory, each volume is randomly cropped into multiple patches with the size of 128 × 128 × 128 for training. The network is trained for 400 epochs. In the inference stage, we use a sliding window to produce the final segmentation prediction of the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results and Comparison to Baseline Methods</head><p>To evaluate the performance of our model, we compare it with four recent models, HeMIS <ref type="bibr" target="#b4">[5]</ref>, U-HVED <ref type="bibr" target="#b2">[3]</ref>, mmFormer <ref type="bibr" target="#b18">[19]</ref>, and MFI <ref type="bibr" target="#b20">[21]</ref>. The dataset is randomly split into 70% for training, 10% for validation, and 20% for testing, and all methods are evaluated on the same dataset and data splitting. We use the Dice score as the metric. As shown in Table <ref type="table" target="#tab_0">1</ref>, our method achieves the best result. For example, our method outperforms the current SOTA method MFI <ref type="bibr" target="#b20">[21]</ref> in most missing-modality cases, including all cases for the whole/complete tumor, 8 out of 15 cases for the tumor core, 12 out of 15 cases for the enhancing tumor. Compared to MFI, for the whole tumor, tumor core, and enhancing tumor regions, we improve the average Dice scores by 0.99%, 0.41%, and 0.77%, respectively. Although the design of our model is quite simple, these results demonstrate its effectiveness for the incomplete multimodel segmentation task of brain tumors. Figure <ref type="figure" target="#fig_1">2</ref> visualizes the segmentation results of samples from the BraTS2020 dataset. With only one Flair image available, the segmentation results of the tumor core and enhancing tumor are poor, because little information on these two regions is observed in the Flair image. With an additional T1c image, the segmentation results of these two regions are significantly improved and quite close to the ground truth. Although adding T1 and T2 images does not greatly improve the segmentation of the tumor core and the enhancing tumor, the boundary of the whole tumor is refined with their help.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> visualizes the contribution to each tumor region from each modality. The numbers are the mean values of the attention maps computed for images in the test set. Overall, in our model, each modality has its contribution to the final segmentation, and no one dominates the result. This is because we have supervision on the segmentation branch of each modality, so that, each modality has the ability to segment each region to some extent. However, we still observe that Flair and T2 modalities have relatively larger contributions to the segmentation of all tumor regions, followed by T1c and then T1. This is probably because the whole tumor area is much clear in Flair and T2 compared to the other two modalities. Each modality shows its preference when segmenting different regions. Flair and T2 are more useful for extracting the peritumoral edema (ED) than the enhancing tumor (ET) and the non-enhancing tumor and  necrosis (NCR/NET); while T1c and T1 are on the opposite and more helpful for extracting ET and NCR/NET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>In this part, we investigate the effectiveness of the average fusion module and the adaptive fusion module, which are two important components of our method. Firstly, we set a baseline model without any modal interaction, that is, with the average fusion module only. Then, we add the adaptive fusion module to the baseline model. Table <ref type="table" target="#tab_1">2</ref> reports this ablation study. With only adding the average fusion module, our method already obtains comparable performance with the current SOTA method MFI. By adding the adaptive fusion module, the dice scores of the three regions further increase by 0.50%, 0.72%, and 0.71%, respectively. This shows that both the average fusion module and the adaptive fusion module are effective in this brain tumor segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>In this paper, we propose an average and adaptive fusion segmentation network (A2FSeg) for the incomplete multi-model brain tumor segmentation task. The essential components of our A2FSeg network are the two stages of feature fusion, including an average fusion and an adaptive fusion. Compare to existing complicated models, our model is much simpler and more effective, which is demonstrated by the best performance on the BraTS 2020 brain tumor segmentation task. The experimental results demonstrate the effectiveness of two techniques, i.e., the average fusion and the attention-based adaptive one, for incomplete modal segmentation tasks.</p><p>Our study brings up the question of whether having complicated models is necessary. If there is no huge gap between different modalities, like in our case where all four modalities are images, the image feature maps are similar and a simple fusion like ours can work. Otherwise, we perhaps need an adaptor or an alignment strategy to fuse different types of features, such as images and audio.</p><p>Also, we observe that a good feature extractor is essential for improving the segmentation results. In this paper, we only explore a reduced UNet for feature extraction. In future work, we will explore other feature extractors, such as Vision Transformer (ViT) or other pre-trained visual foundation models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>. Recently, the segment anything model (SAM) <ref type="bibr" target="#b8">[9]</ref> demonstrates its general ability to extract different regions of interest, which is promising to be adopted as a good starting point for brain tumor segmentation. Besides, our model is general for multi-modal segmentation and we will apply it to other multi-model segmentation tasks to evaluate its generalization on other applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our proposed adaptive multi-modal fusion network (A2FSeg, short for Average and Adaptive Fusion Segmentation network). The dashed lines indicate the possibility of missing some modalities. If so, both the average fusion module and the adaptive fusion module will ignore the missing ones. The final tumor mask is predicted based on feature maps after the adaptive fusion, indicated by the solid red arrows. (Best viewed in color) (Color figure online)</figDesc><graphic coords="2,41,97,60,32,336,37,179,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of our A2FSeg results using a different number of modalities for brain tumor segmentation. Red: peritumoral edema; Blue: enhancing tumor; Green: the necrotic and non-enhancing tumor core. (Color figure online)</figDesc><graphic coords="6,47,31,53,69,329,35,119,83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Summary of the contribution of each modality to each tumor region from the estimated attention maps. ET: enhancing tumor, ED: the peritumoral edema, NCR/NET: non-enhancing tumor and necrosis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison among recent methods, including HeMIS<ref type="bibr" target="#b4">[5]</ref>, U-HVED<ref type="bibr" target="#b2">[3]</ref>, mmFormer<ref type="bibr" target="#b18">[19]</ref>, and MFI<ref type="bibr" target="#b20">[21]</ref>, and ours on BraTS2020 in terms of Dice%. Missing and available modalities are denoted by and , respectively. F indicates Flair, HVED indicates U-HVED, and Former indicates mmFormer because of space issue.</figDesc><table><row><cell>Modalities</cell><cell cols="2">Complete</cell><cell></cell><cell></cell><cell></cell><cell>Core</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Enhancing</cell></row><row><cell cols="5">T 1 T 1c T 2 F Hemis HVED Former MFI</cell><cell>Ours</cell><cell cols="4">Hemis HVED Former MFI</cell><cell>Ours</cell><cell cols="3">Hemis HVED Former MFI</cell><cell>Ours</cell></row><row><cell></cell><cell>87.76</cell><cell>86.49</cell><cell>90.08</cell><cell cols="3">90.60 91.48 66.56</cell><cell>64.42</cell><cell>71.13</cell><cell>75.59</cell><cell cols="2">76.21 44.95</cell><cell>43.32</cell><cell>48.25</cell><cell>51.96</cell><cell>53.80</cell></row><row><cell></cell><cell>85.53</cell><cell>85.14</cell><cell>87.00</cell><cell cols="3">88.38 88.82 65.55</cell><cell>64.87</cell><cell>72.85</cell><cell>75.38</cell><cell cols="2">76.40 43.77</cell><cell>43.31</cell><cell>50.18</cell><cell>52.72</cell><cell>54.46</cell></row><row><cell></cell><cell>90.51</cell><cell>89.87</cell><cell>91.19</cell><cell cols="3">91.65 91.95 70.82</cell><cell>70.55</cell><cell>75.18</cell><cell>77.42</cell><cell cols="2">77.83 48.32</cell><cell>47.86</cell><cell>52.51</cell><cell>54.77</cell><cell>56.10</cell></row><row><cell></cell><cell>72.83</cell><cell>74.31</cell><cell>80.00</cell><cell cols="3">80.16 83.11 83.59</cell><cell>83.96</cell><cell>85.29</cell><cell>85.35</cell><cell cols="2">86.95 75.54</cell><cell>77.34</cell><cell>76.17</cell><cell>76.91</cell><cell>78.01</cell></row><row><cell></cell><cell>91.29</cell><cell>90.45</cell><cell>91.51</cell><cell cols="3">92.36 92.42 86.27</cell><cell>85.78</cell><cell>87.05</cell><cell cols="2">88.67 87.96</cell><cell>76.30</cell><cell>76.29</cell><cell>76.99</cell><cell>77.26</cell><cell>78.07</cell></row><row><cell></cell><cell>86.32</cell><cell>86.82</cell><cell>88.79</cell><cell cols="3">89.53 89.90 85.61</cell><cell>85.11</cell><cell>87.41</cell><cell cols="2">87.83 87.75</cell><cell>75.57</cell><cell>75.68</cell><cell>77.46</cell><cell>76.56</cell><cell>77.85</cell></row><row><cell></cell><cell>91.82</cell><cell>91.46</cell><cell>91.93</cell><cell cols="3">92.38 92.72 86.63</cell><cell>86.06</cell><cell>87.87</cell><cell cols="2">88.56 87.96</cell><cell>76.25</cell><cell>75.47</cell><cell>76.15</cell><cell>76.69</cell><cell>76.96</cell></row><row><cell></cell><cell>75.02</cell><cell>76.64</cell><cell>81.20</cell><cell cols="3">79.91 83.67 61.18</cell><cell>62.78</cell><cell>71.36</cell><cell>72.36</cell><cell cols="2">75.52 37.55</cell><cell>39.46</cell><cell>46.65</cell><cell>50.40</cell><cell>52.58</cell></row><row><cell></cell><cell>90.29</cell><cell>88.81</cell><cell>91.29</cell><cell cols="3">91.45 91.89 71.95</cell><cell>70.18</cell><cell>76.01</cell><cell cols="2">78.22 78.07</cell><cell>48.16</cell><cell>46.53</cell><cell>51.20</cell><cell>55.05 54.00</cell></row><row><cell></cell><cell>86.66</cell><cell>87.13</cell><cell>88.22</cell><cell cols="3">88.03 89.40 67.67</cell><cell>70.21</cell><cell>75.00</cell><cell>75.85</cell><cell cols="2">77.39 44.86</cell><cell>46.95</cell><cell>51.37</cell><cell>54.39</cell><cell>54.58</cell></row><row><cell></cell><cell>90.85</cell><cell>90.34</cell><cell>91.61</cell><cell cols="3">91.67 92.23 72.75</cell><cell>73.22</cell><cell>77.05</cell><cell>78.30</cell><cell cols="2">78.64 48.48</cell><cell>49.45</cell><cell>52.51</cell><cell>55.44 55.34</cell></row><row><cell></cell><cell>77.42</cell><cell>79.40</cell><cell>82.53</cell><cell cols="3">82.50 84.81 84.76</cell><cell>84.94</cell><cell>86.03</cell><cell>86.52</cell><cell cols="2">87.40 75.43</cell><cell>76.56</cell><cell>76.84</cell><cell>76.76</cell><cell>77.80</cell></row><row><cell></cell><cell>91.65</cell><cell>90.97</cell><cell>91.95</cell><cell cols="3">92.24 92.29 86.79</cell><cell>86.61</cell><cell>87.44</cell><cell cols="2">88.84 87.75</cell><cell>76.44</cell><cell>75.79</cell><cell>76.91</cell><cell>77.06 76.75</cell></row><row><cell></cell><cell>86.75</cell><cell>87.72</cell><cell>89.19</cell><cell cols="3">88.81 89.49 86.11</cell><cell>85.36</cell><cell>87.30</cell><cell>87.22</cell><cell>87.16</cell><cell>75.16</cell><cell>75.62</cell><cell>76.37</cell><cell>76.52</cell><cell>77.69</cell></row><row><cell></cell><cell>92.00</cell><cell>91.62</cell><cell>92.26</cell><cell cols="3">92.33 92.71 87.67</cell><cell>86.46</cell><cell>88.13</cell><cell cols="2">88.60 87.74</cell><cell>75.39</cell><cell>75.66</cell><cell>76.08</cell><cell>76.66</cell><cell>76.70</cell></row><row><cell>Means</cell><cell>86.45</cell><cell>86.48</cell><cell>88.58</cell><cell cols="3">88.80 89.79 77.59</cell><cell>77.37</cell><cell>81.01</cell><cell>82.31</cell><cell cols="2">82.72 61.48</cell><cell>61.69</cell><cell>64.38</cell><cell>65.94</cell><cell>66.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of the adaptive fusion model in our method.</figDesc><table><row><cell>Methods</cell><cell cols="3">Complete Core Enhancing Average</cell></row><row><cell>MFI</cell><cell>88.80</cell><cell>82.31 65.94</cell><cell>79.02</cell></row><row><cell cols="2">Baseline (Average fusion module only) 89.29</cell><cell>82.00 66.00</cell><cell>79.10</cell></row><row><cell>+Adaptive fusion module</cell><cell>89.79</cell><cell>82.72 66.71</cell><cell>79.74</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by <rs type="funder">NSFC</rs> <rs type="grantNumber">62203303</rs> and <rs type="funder">Shanghai Municipal Science and Technology Major</rs> Project <rs type="grantNumber">2021SHZDZX0102</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pfWrNV8">
					<idno type="grant-number">62203303</idno>
				</org>
				<org type="funding" xml:id="_FdSfYGN">
					<idno type="grant-number">2021SHZDZX0102</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust multimodal brain tumor segmentation via feature disentanglement and gated fusion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32248-9_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32248-9_50" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019, Part III</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11766</biblScope>
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">RFNET: region-aware fusion network for incomplete multi-modal brain tumor segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3975" to="3984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hetero-modal variational encoder-decoder for joint modality completion and segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dorent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joutard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_9" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019, Part II</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HeMIS: hetero-modal image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guizard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_54</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8_54" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016, Part II</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">NNU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.02643</idno>
		<title level="m">Segment anything</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning based imaging data completion for improved brain disease diagnosis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10443-0_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10443-0_39" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2014, Part III</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Hata</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Barillot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Howe</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8675</biblScope>
			<biblScope unit="page" from="305" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<title level="m">Automatic differentiation in pytorch</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v139/radford21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015, Part III</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Why does synthesized data improve multi-sequence classification?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Tulder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24553-9_65</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24553-9_65" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2015, Part I. LNCS</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9349</biblScope>
			<biblScope unit="page" from="531" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ACN: adversarial co-training network for brain tumor segmentation with missing modalities</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-2_39" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part VII</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12907</biblScope>
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">mmFormer: multimodal medical transformer for incomplete multimodal learning of brain tumor segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_11" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part V</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modality-aware mutual learning for multi-modal medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_56" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part I</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="589" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modality-adaptive feature interaction for brain tumor segmentation with missing modalities</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_18" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part V</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
