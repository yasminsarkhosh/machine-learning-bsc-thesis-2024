<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Amith</forename><surname>Kamath</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ARTORG Center for Biomedical Engineering Research</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonas</forename><surname>Willmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiation Oncology</orgName>
								<orgName type="institution" key="instit1">University Hospital Zurich</orgName>
								<orgName type="institution" key="instit2">University of Zurich</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Proton Therapy</orgName>
								<orgName type="institution">Paul Scherrer Institute</orgName>
								<address>
									<settlement>Villigen</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolaus</forename><surname>Andratschke</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiation Oncology</orgName>
								<orgName type="institution" key="instit1">University Hospital Zurich</orgName>
								<orgName type="institution" key="instit2">University of Zurich</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Mauricio</forename><surname>Reyes</surname></persName>
							<email>mauricio.reyes@unibe.ch</email>
							<affiliation key="aff0">
								<orgName type="department">ARTORG Center for Biomedical Engineering Research</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Radiation Oncology</orgName>
								<orgName type="institution">Bern University Hospital</orgName>
								<address>
									<settlement>Inselspital</settlement>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">University of Bern</orgName>
								<address>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4A887F61C3B95239D2CE39DB27A530ED</idno>
					<idno type="DOI">10.1007/978-3-031-43901-829.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image segmentation</term>
					<term>U-Net</term>
					<term>robustness Supplementary Information The online version contains supplementary material</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The U-Net architecture has become the preferred model used for medical image segmentation tasks. Since its inception, several variants have been proposed. An important component of the U-Net architecture is the use of skip-connections, said to carry over image details on its decoder branch at different scales. However, beyond this intuition, not much is known as to what extent skip-connections of the U-Net are necessary, nor what their interplay is in terms of model robustness when they are subjected to different levels of task complexity. In this study, we analyzed these questions using three variants of the U-Net architecture (the standard U-Net, a "No-Skip" U-Net, and an Attention-Gated U-Net) using controlled experiments on varying synthetic texture images and evaluated these findings on three medical image data sets. We measured task complexity as a function of texture-based similarities between foreground and background distributions. Using this scheme, our findings suggest that the benefit of employing skip-connections is small for low-to-medium complexity tasks, and its benefit appears only when the task complexity becomes large. We report that such incremental benefit is non-linear, with the Attention-Gated U-Net yielding larger improvements. Furthermore, we find that these benefits also bring along robustness degradations on clinical data sets, particularly in out-of-domain scenarios. These results suggest a dependency between task complexity and the choice/design of noise-resilient skip-connections, indicating the need for careful consideration while using these skip-connections.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to the broad success of U-Nets <ref type="bibr" target="#b16">[17]</ref> for image segmentation, it has become the go-to architecture in the medical image computing community. Since its creation in 2015, much research has been dedicated to exploring variants and improvements over the standard base model <ref type="bibr" target="#b2">[3]</ref>. However, Isensee et al. <ref type="bibr" target="#b11">[12]</ref> showed with their not-new-U-Net (nnU-Net) that the success of the U-Net relies on a well-prepared data pipeline incorporating appropriate data normalization, class balancing checks, and preprocessing, rather than on architecture changes. Arguably the two most important challenges at present for medical image segmentation are generalization and robustness. A lack of generalization decreases the performance levels of a model on data sets not well characterized by the training data set, while poor robustness appears when models under-perform on data sets presenting noise or other corruptions <ref type="bibr" target="#b12">[13]</ref>. Modern neural networks have been shown to be highly susceptible to distribution shifts and corruptions that are modality-specific <ref type="bibr" target="#b5">[6]</ref>. While the average accuracy of U-Net-based models has increased over the years, it is evident from the literature that their robustness level has not improved at the same rate <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>One of the key elements of the U-Net are the skip-connections, which propagate information directly (i.e., without further processing) from the encoding to the decoding branch at different scales. Azad et al. <ref type="bibr" target="#b2">[3]</ref> mention that this novel design propagates essential high-resolution contextual information along the network, which encourages the network to re-use the low-level representation along with the high-context representation for accurate localization. Nonetheless, there is no clear evidence supporting this intuition and moreover, there is limited knowledge in the literature describing to what extent skip-connections of the U-Net are necessary, and what their interplay is in terms of model robustness when they are subjected to different levels of task complexity.</p><p>Currently, the U-Net is used more as a "Swiss-army knife" architecture across different image modalities and image quality ranges. In this paper, we describe the interplay between skip-connections and their effective role of "transferring information" into the decoding branch of the U-Net for different degrees of task complexity, based on controlled experiments conducted on synthetic images of varying textures as well as on clinical data comprising Ultrasound (US), Computed tomography (CT), and Magnetic Resonance Imaging (MRI). In this regard, the work of <ref type="bibr" target="#b9">[10]</ref> showed that neural networks are biased toward texture information. Recently, <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> similarly showed the impact of texture modifications on the performance and robustness of trained U-Net models. Contrary to these prior works analyzing the impact of data perturbation to model performance (e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>), in this study we focus on analyzing the role of skipconnections to model performance and its robustness. We hypothesize therefore that skip-connections may not always lead to beneficial effects across varying task complexities as measured with texture modifications. Our major contributions through this paper are: (i) We describe a novel analysis pipeline to evaluate the robustness of image segmentation models as a function of the difference in texture between foreground and background. (ii) We confirm the hypothesis that severing these skip-connections could lead to more robust models, especially in the case of out-of-domain (OOD) test data. Furthermore, we show that severing skip-connections could work better than filtering feature maps from the encoder with attention-gating. (iii) Finally, we also demonstrate failure modes of using skip-connections, where robustness across texture variations appear to be sacrificed in the pursuit of improvements within domain.  evaluate how the model behaves at varying task complexities, we construct training data sets where each training sample is subjected to a linear transformation where its foreground is blended with the background:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Experiment Design</head><formula xml:id="formula_0">I(x | Z(x) = 1) = αI(x | Z(x) = 1) + (1 -α)I(x | Z(x) = 0).</formula><p>By increasing α from zero to one, more of the foreground texture is added in the foreground mask, which otherwise is made up of the background texture (See Fig. <ref type="figure" target="#fig_2">2</ref>), while the background itself is unimpacted. We then quantify the similarity between foreground and background regions by measuring the Kullback-Leibler divergence between their local-binary-pattern (LBP) <ref type="bibr" target="#b15">[16]</ref> histograms. We selected LBP since it is a commonly used and benchmarked texture descriptor in machine learning applications <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>.</p><formula xml:id="formula_1">T S = KL(H(L(I) BG )||H(L(I) F G ) (1) L(I) BG = LBP (I(x | Z(x) = 0) (2) L(I) F G = LBP (I(x | Z(x) = 1)<label>(3)</label></formula><p>where T S refers to the level of texture similarity, H() corresponds to histogram, and L(I) {BG,F G} refers to LBP calculated for BG or FG. The LBP histogram was computed using a 3×3 neighbourhood with 8 points around each pixel in the image. Three U-Net models were trained featuring three different skip-connection strategies: NoSkipU-Net, U-Net, and AGU-Net, representing the absence of skipconnections, the use of an identity transform (i.e., information through skips is kept as is), and filtering information via attention through skip-connections, respectively. Models were trained at different levels of T S between the foreground and background regions, determined based on the Kullback-Leibler divergence of Local Binary Pattern (LBP) histograms, Eq. 1. For each level of α used to create a training set, we trained a model to be evaluated on a synthetic test set using the same α to measure within-domain performance and across a range of α, to measure their out-of-domain robustness. Next, using Eq. 1 and ground truth labels, we computed the T S of images from the test set of the medical data sets and applied corruptions by way of noise or blurring in order to increase and decrease T S depending on the imaging modality being analyzed. Then we evaluated the robustness of these models to texture changes in these data sets. We did this at two levels of task complexity (easier -where T S is higher, and harder, where T S is lower) and different from the original T S. We report all model performances using dice scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Description of Data</head><p>Synthetic Textures: We took two representative grayscale textures from the synthetic toy data set described in <ref type="bibr" target="#b10">[11]</ref> and used them as the background and foreground patterns. These patterns were chosen such that the T S values matched the range of medical data sets described next. We also generated synthetic segmentation masks using bezier curves setup such that the curvature and size of the foreground simulate clinical image segmentation problems. Examples of such images are shown in Fig. <ref type="figure" target="#fig_2">2</ref>. We generated 100 such image-mask pairs at 9 levels (for α ∈ {0.1, 0.2, ..., 0.9}), so that we create training data sets at various task complexities. These images are generated by randomly cropping the grayscale textures to 256 × 256 pixels. 70 of these were used as the training set, 10 were reserved for validation, and the rest of the 20 formed the test set, identically split for all task complexities. Figure <ref type="figure" target="#fig_2">2</ref> show kernel density estimates of each of these 9 data sets along the texture similarity axis. The curve in orange (α = 0.1) indicates that the foreground mask in this set contains only 10% of the actual foreground texture and 90% of the background texture blended together. This represents a situation where it is texturally hard for humans as well as for segmentation models. The data set in green (α = 0.9) shows the reverse ratio -the foreground region now contains 90% of the foreground texture, thereby making it an easier task to segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medical Data Sets:</head><p>We tested the three variants of the U-Net architecture on three medical binary segmentation data sets: a Breast Ultrasound <ref type="bibr" target="#b0">[1]</ref>, a spleen CT and a heart MRI data set <ref type="bibr" target="#b1">[2]</ref>. The breast ultrasound data set contained 647 images, 400 of which were used as training, 100 as validation and 147 as the test set. We used the benign and malignant categories in the breast ultrasound data and excluded images with no foreground to segment (i.e. the "normal" category). The spleen data set contained 899 images, 601 of which were used as training, 82 as validation and 216 as test set images. The heart data set contained 829 images, 563 of which were used as training, 146 as validation, and 120 as test set images. We selected 2D axial slices from the spleen, and sagittal slices from the heart data sets, both of which were originally 3D volumes, such that there is at least one pixel corresponding to the foreground. Care was taken to ensure that 2D slices were selected from 3D volumes and split at the patient level to avoid cross-contamination of images across training/test splits. To vary T S of images in the test set, and to evaluate the robustness of the U-Net variants, speckle noise with variance 0.1 was added to both the foreground and background. This made the textures more similar, hence lowered T S, and essentially rendered them harder to segment. This is shown in the red boxes in Fig. <ref type="figure" target="#fig_3">3</ref>. We also created another test set with textures that are less similar by blurring the background using a Gaussian kernel of variance 3.0 while not blurring the foreground pixels. These are shown in the green boxes in Fig. <ref type="figure" target="#fig_3">3</ref>, where it can be seen they are easier to segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Architecture and Training Settings</head><p>The network architectures were created using MONAI <ref type="bibr" target="#b6">[7]</ref> v1.1 and were all trained with random weight initialization. The U-Net was implemented with one input and output channel, with input image size set to 256 × 256 pixels across all experiments. The model had five levels with 16, 32, 64, 128, 256 channels each for synthetic experiments and six levels (an additional level with 512 channels) for medical image experiments, all intermediate channels with a stride of 2. The ReLU activation was used, and no residual units were included. To reduce stochasticity, no dropout was used in any variant.</p><p>The NoSkipU-Net was identical to the U-Net except for severed skipconnections. This led to the number of channels in the decoder to be smaller as there is no concatenation from the corresponding encoder level. The AGU-Net was setup to be the same as the U-Net, except with attention gating through the skip-connections.</p><p>The training parameters were kept constant across compared models for fair comparison. Our experiments<ref type="foot" target="#foot_0">1</ref> were implemented in Python 3.10.4 using the PyTorch implementation of the adam <ref type="bibr" target="#b13">[14]</ref> optimizer. We set the learning rate to be 1e -3 for synthetic experiments (and 1e -2 for medical image experiments), maintaining it constant without using a learning rate scheduler. No early stopping criteria were used while training, and all models were allowed to train to 100 epochs. We trained our models to optimize the dice loss, and saved the model with the best validation dice (evaluated once every two epochs) for inference on the test set.</p><p>We did not perform any data augmentation that could change the scale of the image content, thereby also changing the texture characteristics. Therefore, we only do a random rotation by 90 degrees with a probability of 0.5 for training, and no other augmentations. We also refrained from fine-tuning hyperparameters and did not perform any ensembling as our study design is not meant to achieve the best possible performance metric as much as it attempts to reliably compare performance across architecture variants while keeping confounding factors to a minimum. We therefore trained each model using the same random seeds (three times) and report the dice score statistics. Training and testing were performed on an NVIDIA A5000 GPU with 24 GB RAM and CUDA version 11.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">On Synthetic Texture Variants</head><p>In-domain (Performance): Figure <ref type="figure" target="#fig_4">4</ref> (left) indicates the relative improvement in dice scores between the three U-Net variants using the NoSkipU-Net as the baseline. To make the interpretation easier, the α value is used as a proxy for T S on the horizontal axis. It is worth noting that for α values &gt; 0.3, there is negligible difference between the dice score performances of all the U-Net variants, indicating their ability with or without the skip-connections to learn the distributions of the foreground and background textures at that level of task complexity. Below α values of 0.3, the benefits of using attention gating in the skip-connections start to appear. This indicates that the benefit of attentiongating as a function of complexity is non-linear: models do not benefit from skip-connections at lower ranges of task complexity, but at larger ones, filtering the information flowing through the skip connections is important. What is interesting is also how the standard U-Net performance is noisy compared to NoSkipU-Net, indicating that passing through the entire encoder feature map to be concatenated with the decoder feature maps may not always be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Out-of-Domain (Robustness):</head><p>Rows in Fig. <ref type="figure" target="#fig_4">4</ref> (right) includes a heatmap to represent the α values that the model was trained on, and columns correspond to the α value it was tested on. The entries in the matrix are normalized dice score differences between AGU-Net and NoSkipU-Net (comparisons between standard U-Net and NoSkipU-Net show similar trends). The diagonal entries here correspond to the AGU-Net plot in Fig. <ref type="figure" target="#fig_4">4</ref> (left). For α values 0.3 and 0.4 in training and 0.9 on testing (corresponding to an out-of-domain testing scenario), the NoSkipU-Net performs better than the AGU-Net, indicating that there indeed are situations where skip-connections cause more harm than benefit. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">On Medical Image Textures</head><p>In-Domain (performance): Looking at the "In-domain" rows in Table <ref type="table" target="#tab_0">1</ref>, on all three data sets, the AGU-Net outperforms both the other variants. However, the relative improvements in performance vary across modalities, with the performance differences on CT being the most stark. On the Ultrasound data set, the NoSkipU-Net performs as well as the standard U-Net, supporting our hypothesis that skip-connections may not always be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Out-of-Domain (Robustness):</head><p>Focusing on the rows "Harder" and "Easier" in Table <ref type="table" target="#tab_0">1</ref>, we observe for the Ultrasound data set that the AGU-Net improves in the easier task, but declines in performance in the harder one. The drop in performance is most pronounced for the U-Net, but moderate for the NoSkipU-Net. For the spleen data set, both the AGU-Net and the standard U-Net demonstrate severe drop in performance in the harder case. However, AGU-Net is better and the standard U-Net is worse than the NoSkipU-Net in the easier texture situations. The heart data set shows the same trend as in the spleen data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>Through extensive experiments using synthetic texture images at various levels of complexity and validating these findings on medical image data sets from three different modalities, we show in this paper that the use of skip-connections can both be beneficial as well as harmful depending on what can be traded off: robustness or performance. A limitation of our work is that we vary only the foreground in synthetic experiments but background variations could demonstrate unexpected asymmetric behavior. We envision the proposed analysis pipeline to be useful in quality assurance frameworks where U-Net variants could be compared to analyse potential failure modes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1 describes our experimental setup to assess the impact of skipconnections in U-Net-like architectures under varying levels of task complexity. Given a set of N pairs of labeled training images {(I, S) i : 1 ≤ i ≤ N }, I ∈ R H×W and S ∈ Z : {0, 1} H×W , corresponding ground-truth segmentation, a deep learning segmentation model M (I) → S is commonly updated by minimizing a standard loss term, such as the binary cross entropy or dice loss. To</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Experimental design to evaluate the role of U-Net's skip-connections under different levels of task complexity. Given training images with controllable background (BG) and foreground (FG) textures, three variants of the U-Net were trained featuring no skip-connections (NoSkipU-Net), standard U-Net (U-Net)<ref type="bibr" target="#b16">[17]</ref>, and Attention-Gated U-Net (AGU-Net)<ref type="bibr" target="#b17">[18]</ref>, each characterizing a different strategy (zeroing information through skips, identity transform and filtering information through skips, respectively). Each model was trained with different levels of texture similarity between background and foreground, based on the Kullback-Leibler divergence of Local Binary Pattern (LBP) histograms for foreground and background regions. For each level of foregroundto-background texture similarity, the performance for each model was recorded indomain, and robustness was measured with out-of-domain texture similarities.</figDesc><graphic coords="3,87,81,323,00,168,07,147,94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Generation of synthetic data samples as a function of blending foreground texture into the background. Numbers in the legend indicate the proportion of foreground blended within the foreground mask.</figDesc><graphic coords="5,61,29,54,62,302,02,150,07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Medical data test sets on the texture similarity (T S) axis with in-domain (dashed gray), easier task (green, low similarity) and harder task (red, high similarity) distributions. Three modalities tested include US, CT, and MR, whose T S are in the same range as synthetic data in Fig. 2.</figDesc><graphic coords="6,57,96,53,87,336,28,82,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Relative performance in-domain (left) across U-Net variants, and out-of-domain robustness metrics (right) for AGU-Net versus NoSkipU-Net.</figDesc><graphic coords="8,87,96,53,93,276,82,91,78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Mean (standard deviation) of Dice scores for each of hard, in-domain and easy textures on the Breast (Ultrasound), Spleen (CT) and Heart (MR) data sets. Best performing model at each texture level is highlighted in bold.</figDesc><table><row><cell>Data set</cell><cell cols="2">Texture level AGU-Net</cell><cell>U-Net</cell><cell>NoSkipU-Net</cell></row><row><cell cols="2">Breast (Ultrasound) Harder</cell><cell cols="3">0.645 (0.291) 0.723 (0.281) 0.735 (0.268)</cell></row><row><cell></cell><cell>In-domain</cell><cell cols="3">0.795 (0.206) 0.762 (0.261) 0.761 (0.258)</cell></row><row><cell></cell><cell>Easier</cell><cell cols="3">0.799 (0.200) 0.735 (0.244) 0.748 (0.243)</cell></row><row><cell>Spleen (CT)</cell><cell>Harder</cell><cell cols="3">0.310 (0.226) 0.074 (0.152) 0.558 (0.265)</cell></row><row><cell></cell><cell>In-domain</cell><cell cols="3">0.927 (0.092) 0.745 (0.275) 0.606 (0.265)</cell></row><row><cell></cell><cell>Easier</cell><cell cols="3">0.809 (0.201) 0.394 (0.354) 0.486 (0.292)</cell></row><row><cell>Heart (MRI)</cell><cell>Harder</cell><cell cols="3">0.139 (0.242) 0.500 (0.316) 0.815 (0.126)</cell></row><row><cell></cell><cell>In-domain</cell><cell cols="3">0.929 (0.055) 0.900 (0.080) 0.833 (0.111)</cell></row><row><cell></cell><cell>Easier</cell><cell cols="3">0.889 (0.073) 0.805 (0.129) 0.823 (0.103)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code to reproduce this is at https://github.com/amithjkamath/to skip or not..</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dataset of breast ultrasound images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Al-Dhabyani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fahmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Brief</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">104863</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The medical segmentation decathlon</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4128</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Medical image segmentation review: the success of u-net</title>
		<author>
			<persName><forename type="first">R</forename><surname>Azad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.14830</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic MRI cardiac multistructures segmentation and diagnosis: is the problem solved?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rood-MRI: Benchmarking the robustness of deep learning segmentation models to out-of-distribution and corrupted data in MRI</title>
		<author>
			<persName><forename type="first">L</forename><surname>Boone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06060</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Monai: An open-source framework for deep learning in healthcare</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.02701</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comprehensive benchmark of local binary pattern algorithms for texture retrieval</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schaefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Pattern Recognition</title>
		<meeting>the 21st International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="2760" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From accuracy to reliability and robustness in cardiac magnetic resonance image segmentation: a review</title>
		<author>
			<persName><forename type="first">F</forename><surname>Galati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zuluaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3936</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<title level="m">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Grid saliency for context explanations of semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">NNU-net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Benchmarking the robustness of semantic segmentation models with respect to common corruptions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kamann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="462" to="483" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluation of LBP and deep texture descriptors with a new robustness benchmark</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46487-9_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46487-95" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="69" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on featured distributions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-10-05">2015. October 5-9, 2015. 2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III 18</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention gated networks: learning to leverage salient regions in medical images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature preserving smoothing provides simple and effective data augmentation for medical image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-812" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2020: 23rd International Conference</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Lima, Peru; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">October 4-8, 2020. 2020</date>
			<biblScope unit="page" from="116" to="126" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Influence of contrast and texture based image modifications on the performance and attention shift of u-net models for brain tissue segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neuroimag</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1012639</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
