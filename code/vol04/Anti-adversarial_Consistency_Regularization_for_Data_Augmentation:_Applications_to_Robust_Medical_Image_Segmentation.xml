<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation</title>
				<funder ref="#_4UaKKns #_2tmkqKw #_G4HMxxS #_ZCTznKJ #_JxYQ7Z6 #_jdfNhey">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_Xzwk6ef">
					<orgName type="full">ITRC</orgName>
				</funder>
				<funder ref="#_YAdDarU">
					<orgName type="full">Korean government (MSIT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hyuna</forename><surname>Cho</surname></persName>
							<email>hyunacho@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<settlement>Pohang</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yubin</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<settlement>Pohang</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Won</forename><forename type="middle">Hwa</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<settlement>Pohang</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="555" to="566"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">19FE333AB95D3E729FF3557AF98C9EA1</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_53</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adversarial attack and defense</term>
					<term>Data augmentation</term>
					<term>Semantic segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern deep learning methods for semantic segmentation require labor-intensive labeling for large-scale datasets with dense pixellevel annotations. Recent data augmentation methods such as dropping, mixing image patches, and adding random noises suggest effective ways to address the labeling issues for natural images. However, they can only be restrictively applied to medical image segmentation as they carry risks of distorting or ignoring the underlying clinical information of local regions of interest in an image. In this paper, we propose a novel data augmentation method for medical image segmentation without losing the semantics of the key objects (e.g., polyps). This is achieved by perturbing the objects with quasi-imperceptible adversarial noises and training a network to expand discriminative regions with a guide of anti-adversarial noises. Such guidance can be realized by a consistency regularization between the two contrasting data, and the strength of regularization is automatically and adaptively controlled considering their prediction uncertainty. Our proposed method significantly outperforms various existing methods with high sensitivity and Dice scores and extensive experiment results with multiple backbones on two datasets validate its effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation aims to segment objects in an image by classifying each pixel into an object class. Training a deep neural network (DNN) for such a task is known to be data-hungry, as labeling dense pixel-level annotations requires laborious and expensive human efforts in practice <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b32">32]</ref>. Furthermore, semantic segmentation in medical imaging suffers from privacy and data sharing issues <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b35">35]</ref> and a lack of experts to secure accurate and clinically meaningful regions of interest (ROIs). This data shortage problem causes overfitting for training DNNs, resulting in the networks being biased by outliers and ignorant of unseen data.</p><p>To alleviate the sample size and overfitting issues, diverse data augmentations have been recently developed. For example, CutMix <ref type="bibr" target="#b31">[31]</ref> and CutOut <ref type="bibr" target="#b3">[4]</ref> augment images by dropping random-sized image patches or replacing the removed regions with a patch from another image. Random Erase <ref type="bibr" target="#b33">[33]</ref> extracts noise from a uniform distribution and injects it into patches. Geometric transformations such as Elastic Transformation <ref type="bibr" target="#b26">[26]</ref> warp images and deform the original shape of objects. Alternatively, feature perturbation methods augment data by perturbing data in feature space <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b22">22]</ref> and logit space <ref type="bibr" target="#b9">[9]</ref>.</p><p>Although these augmentation approaches have been successful for natural images, their usage for medical image semantic segmentation is quite restricted as objects in medical images contain non-rigid morphological characteristics that should be sensitively preserved. For example, basalioma (e.g., pigmented basal cell carcinoma) may look similar to malignant melanoma or mole in terms of color and texture <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b20">20]</ref>, and early-stage colon polyps are mostly small and indistinguishable from background entrail surfaces <ref type="bibr" target="#b14">[14]</ref>. In these cases, the underlying clinical features of target ROIs (e.g., polyp, tumor and cancer) can be distorted if regional colors and textures are modified with blur-based augmentations or geometric transformations. Also, cut-and-paste and crop-based methods carry risks of dropping or distorting key objects such that expensive pixel-level annotations could not be properly used. Considering the ROIs are usually small and underrepresented compared to the backgrounds, the loss of information may cause a fatal class imbalance problem in semantic segmentation tasks.</p><p>In these regards, we tackle these issues with a novel augmentation method without distorting the semantics of objects in image space. This can be achieved by slightly but effectively perturbing target objects with adversarial noises at the object level. We first augment hard samples with adversarial attacks <ref type="bibr" target="#b18">[18]</ref> that deceive a network and defend against such attacks with anti-adversaries. Specifically, multi-step adversarial noises are injected into ROIs to maximize loss and induce false predictions. Conversely, anti-adversaries are obtained with antiadversarial perturbations that minimize a loss which eventually become easier samples to predict. We impose consistency regularization between these contrasting samples by evaluating their prediction ambiguities via supervised losses with true labels. With this regularization, the easier samples provide adaptive guidance to the misclassified data such that the difficult (but object-relevant) pixels can be gradually integrated into the correct prediction. From active learning perspective <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b19">19]</ref>, as vague samples near the decision boundary are augmented and trained, improvement on a downstream prediction task is highly expected.</p><p>We summarize our main contributions as follows: 1) We propose a novel online data augmentation method for semantic segmentation by imposing objectspecific consistency regularization between anti-adversarial and adversarial data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>Our method provides a flexible regularization between differently perturbed data such that a vulnerable network is effectively trained on challenging samples considering their ambiguities. 3) Our method preserves underlying morphological characteristics of medical images by augmenting data with quasiimperceptible perturbation. As a result, our method significantly improves sensitivity and Dice scores over existing augmentation methods on Kvasir-Seg <ref type="bibr" target="#b11">[11]</ref> and ETIS-Larib Polyp DB <ref type="bibr" target="#b25">[25]</ref> benchmarks for medical image segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary: Adversarial Attack and Anti-adversary</head><p>Adversarial attack is an input perturbation method that adds quasiimperceptible noises into images to deceive a DNN. Given an image x, let μ be a noise bounded by l ∞ -norm. While the difference between x and the perturbed sample x = x + μ is hardly noticeable to human perception, a network f θ (•) can be easily fooled (i.e., f θ (x) = f θ (x + μ)) as the μ pushes x across the decision boundary.</p><p>To fool a DNN, Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b8">[8]</ref> perturbs x toward maximizing a loss function L by defining a noise μ as the sign of loss derivative with respect to x as follows: x = x + sign(∇ x L), where controls the magnitude of perturbation. The authors in <ref type="bibr" target="#b18">[18]</ref> proposed an extension of FGSM, i.e., Projected Gradient Descent (PGD), which is an iterative adversarial attack that also finds x with a higher loss. Given an iteratively perturbed sample x t at t-th perturbation where x 0 = x, the x t of PGD is defined as</p><formula xml:id="formula_0">x t = (x t-1 + sign(∇ x L)) for T perturbation steps.</formula><p>Recently, anti-adversarial methods were proposed for the benign purpose to defend against such attacks. The work in <ref type="bibr" target="#b15">[15]</ref> used an anti-adversarial class activation map to identify objects and the authors in <ref type="bibr" target="#b0">[1]</ref> proposed an anti-adversary layer to handle adversaries. In contrast to adversarial attacks, these works find μ that minimizes a loss to make easier samples to predict. Figure <ref type="figure" target="#fig_0">1a</ref> shows multistep adversarial and anti-adversarial perturbations in the latent space. To increase a classification score, the anti-adversarial noises move data away from the decision boundary, which is the opposite direction of the adversarial perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Let {X i } N i=1 be an image set with N samples each paired with corresponding ground truth pixel-level annotations Y i . Our proposed method aims to 1) generate realistic images with adversarial attacks and 2) train a segmentation model f θ (X i ) = Y i for robust semantic segmentation with anti-adversarial consistency regularization (AAC). Figure <ref type="figure" target="#fig_1">2</ref> shows the overall training scheme with three phases: 1) online data augmentation, 2) computing adaptive AAC between differently perturbed samples, and 3) updating the segmentation model using the loss from the augmented and original data. First, we generate plausible images with iterative adversarial and anti-adversarial perturbations. We separate the roles of perturbed data: adversaries are used as training samples and anti-adversaries are used to provide guidance (i.e., pseudo-labels) to learn the adversaries. Specifically, consistency regularization is imposed between these contrasting data by adaptively controlling the regularization magnitude in the next phase. Lastly, considering each sample's ambiguity, the network parameters θ are updated for learning the adversaries along with the given data so that discriminative regions are robustly expanded for challenging samples.</p><p>Data Augmentation with Object-Targeted Adversarial Attack. In many medical applications, false negatives (i.e., failing to diagnose a critical disease) are much more fatal than false positives. To deal with these false negatives, we mainly focus on training a network to learn diverse features at target ROIs (e.g., polyps) where disease-specific variations exist. To do so, we first exclude the background and perturb only the objects in the given image. Given o as the target object class and (p, q) as a pixel coordinate, a masked object is defined as Xi = {(p, q)|X i (p,q) = o}. As in PGD <ref type="bibr" target="#b18">[18]</ref>, we perform iterative perturbations on the Xi for K steps. Given Xi,k as a perturbed sample at k-th step (k = 1, ..., K), the adversarial and anti-adversarial perturbations use the same initial image as X - i,0 = Xi and X + i,0 = Xi , respectively. With this input, the iterative adversarial attack is defined as</p><formula xml:id="formula_1">X - i,k+1 = X - i,k + μ - i,k = X - i,k + sign(∇ X - i,k L(f θ (X - i,k ), Y i ))<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">μ - i,k = argmax μ L(f θ (X - i,k+1</formula><p>), Y i ) is a quasi-imperceptible adversarial noise that fools f θ (•) and is a perturbation magnitude that limits the noise (i.e., |μ (p,q) | ≤ , s.t. (p, q) ∈ Xi ). Similarly, iterative anti-adversarial perturbation is defined as</p><formula xml:id="formula_3">X + i,k+1 = X + i,k + μ + i,k = X + i,k + sign(∇ X + i,k L(f θ (X + i,k ), Y i )).<label>(2)</label></formula><p>In contrast to the adversarial attack in Eq. 1, the anti-adversarial noise</p><formula xml:id="formula_4">μ + i,k = argmin μ L(f θ (X + i,k+1</formula><p>), Y i ) manipulates samples to increase the classification score.</p><p>Note that, generating noises and images are online and training-free as the loss derivatives are calculated with freezed network parameters. The adversaries X - i,1 , ..., X - i,K are used as additional training samples so that the network includes the non-discriminative yet object-relevant features for the prediction. On the other hand, as the anti-adversaries are sufficiently discriminative, we do not use them as training samples. Instead, only the K-th anti-adversary X + i,K (i.e., the most perturbed sample with the lowest loss) is used for downstream consistency regularization to provide informative guidance to the adversaries.</p><p>Computing Adaptive Consistency Toward Anti-adversary. Let X i be either X i or X - i,k . As shown in Fig. <ref type="figure" target="#fig_0">1b</ref>, consistency regularization is imposed between the anti-adversary X + i,K and X i to reduce the gap between samples with different prediction uncertainties. The weight of regularization between X i and X + i,K is automatically determined by evaluating the gap in their prediction quality via supervised losses with ground truth Y i as w(X i , X</p><formula xml:id="formula_5">+ i,K ) = max( 1 2 , l(f θ (X i ), Yi) l(f θ (X i ), Yi) + l(f θ (X + i,K ), Yi) ) = max( 1 2 , l(P i , Yi) l(P i , Yi) + l(P + i,K , Yi) ),<label>(3)</label></formula><p>where l(•) is Dice loss <ref type="bibr" target="#b28">[28]</ref> and P i is the output of f θ (•) for X i . Specifically, if X i is a harder sample to predict than X + i,K , i.e., l(P i , Y i )&gt;l(P + i,K , Y i ), the weight gets larger, and thus consistency regularization is intensified between the images.</p><p>Training a Segmentation Network. Let Ŷ + i,K be a segmentation outcome, i.e., one-hot encoded pseudo-label from the network output P + i,K of anti-adversary X + i,K . Given X i and {X - i,k } K k=1 as training data, the supervised segmentation loss L sup and the consistency regularization R con are defined as</p><formula xml:id="formula_6">L sup = 1 N N i=1 l(P i , Y i ) + 1 NK N i=1 K k=1 l(P - i,k , Y i ) and<label>(4)</label></formula><formula xml:id="formula_7">R con = 1 N N i=1 w(X i , X + i,K )l(P i , Ŷ + i,K ) + 1 NK N i=1 K k=1 w(X - i,k , X + i,K )l(P - i,k , Ŷ + i,K ).<label>(5)</label></formula><p>Using the pseudo-label from anti-adversary as a perturbation of the ground truth, the network is supervised by diverse and realistic labels that contain auxiliary information that the originally given labels do not provide. With a hyperparameter α, the whole training loss L = L sup +αR con is minimized via backpropagation to optimize the network parameters for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Dataset. We conducted experiments on two representative public polyp segmentation datasets: Kvasir-SEG <ref type="bibr" target="#b11">[11]</ref> and ETIS-Larib Polyp DB <ref type="bibr" target="#b25">[25]</ref> (ETIS). Both are comprised of two classes: polyp and background. They provide 1000/196 (Kvasir-SEG/ETIS) input-label pairs in total and we split train/validation/test sets into 80%/10%/10% as in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">29]</ref>. The images of Kvasir-SEG were resized to 512 × 608 (H × W ) and that of ETIS was set with 966 × 1255 resolution.</p><p>Implementation. We implemented our method on Pytorch framework with 4 NVIDIA RTX A6000 GPUs. Adam optimizer with learning rates of 4e-3/1e-4 (Kavsir-SEG/ETIS) were used for 200 epochs with a batch size of 16. We set the number of perturbation steps K as 10 and the magnitude of perturbation as 0.001. The weight α for R con was set to 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines.</head><p>Along with conventional augmentation methods (i.e., random horizontal and vertical flipping denoted as 'Basic' in Table <ref type="table" target="#tab_0">1</ref>), recent methods such as CutMix <ref type="bibr" target="#b31">[31]</ref>, CutOut <ref type="bibr" target="#b3">[4]</ref>, Elastic Transform <ref type="bibr" target="#b26">[26]</ref>, Random Erase <ref type="bibr" target="#b33">[33]</ref>, Drop-Block <ref type="bibr" target="#b7">[7]</ref>, Gaussian Noise Training (GNT) <ref type="bibr" target="#b22">[22]</ref>, Logit Uncertainty (LU) <ref type="bibr" target="#b9">[9]</ref> and Tumor Copy-Paste (TumorCP) <ref type="bibr" target="#b30">[30]</ref> were used as baselines. Their hyperparameters were adopted from the original papers. The Basic augmentation was used in all methods including ours by default. For the training, we used K augmented images with the given images for all baselines as in ours for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation.</head><p>To verify the effectiveness of our method, evaluations are conducted using various popular backbone architectures such as U-Net <ref type="bibr" target="#b21">[21]</ref>, U-Net++ <ref type="bibr" target="#b34">[34]</ref>, LinkNet <ref type="bibr" target="#b1">[2]</ref>, and DeepLabv3+ <ref type="bibr" target="#b2">[3]</ref>. As the evaluation metric, mean Intersection over Union (mIoU) and mean Dice coefficient (mDice) are used for all experiments on test sets. Additionally, we provide recall and precision scores to offer a detailed analysis of class-specific misclassification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Existing Methods</head><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, our method outperforms all baselines for all settings by at most 10.06%p and 5.98%p mIoU margin on Kvasir-SEG and ETIS, respectively. Moreover, in Fig. <ref type="figure" target="#fig_2">3</ref>, our method with U-Net on Kvasir-SEG surpasses the baselines by ∼8.2%p and ∼7.2%p in precision and recall, respectively. Note that, all baselines showed improvements in most cases. However, our method  performed better even compared with the TumorCP which uses seven different augmentations methods together for tumor segmentation. This is because our method preserves the semantics of the key ROIs with small but effective noises unlike geometric transformations <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b30">30]</ref>, drop and cut-and-paste-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33]</ref>. Also, as we augment uncertain samples that deliberately deceive a network as in Active Learning <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b16">16]</ref>, our method is able to sensitively include the challenging (but ROI-relevant) features into prediction, unlike existing noise-based methods that extract noises from known distributions <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b30">30]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis on Anti-adversaries and Adversaries</head><p>In Fig. <ref type="figure" target="#fig_3">4</ref>, we visualize data augmentation results with (anti-) adversarial perturbations on Kvasir-SEG dataset. The perturbed data (c and e) are the addition of noise (d and f) to the given data (a), respectively. Interestingly, while adversaries (c) and anti-adversaries (e) are visually indistinguishable, they induce totally opposite model decisions towards different classes. In Fig. <ref type="figure" target="#fig_4">5</ref>, we qualitatively and quantitatively compared their effects via visualizing perturbation trajectories in the feature space projected with t-SNE <ref type="bibr" target="#b17">[17]</ref> and comparing their supervision losses. In Fig. <ref type="figure" target="#fig_4">5a</ref>, the adversarial attacks send a pixel embedding of a polyp class to the background class, anti-adversarial perturbations push it towards the true class with a higher classification score. Also, loss comparisons in Fig. <ref type="figure" target="#fig_4">5b</ref> and 5c demonstrate that the anti-adversaries (blue) are consistently easier to predict than the given data (grey) and adversaries (red) during the training and their differences get larger as the perturbations are iterated. These results confirm that the anti-adversaries send their pseudo label Ŷ + K closer to the ground truth with a slight change. Therefore, they can be regarded as a perturbation of the ground truth that contain a potential to provide additional information to train a network on the adversaries. We empirically show that Ŷ + K is able to provide such auxiliary information that the true labels do not provide, as our method performs better with R con (i.e., L = L sup +αR con , 92.43% mIoU) than the case without R con (i.e., L = L sup , 92.15% mIoU) using U-Net on Kvasir-SEG. Training samples in Fig. <ref type="figure" target="#fig_5">6</ref> show that the pseudo-labels Ŷ + K can capture detailed abnormalities (marked in red circles) which are not included in the ground truths. Moreover, as the AAC considers sample-level ambiguity, the effect from Ŷ + K is sensitively controlled and a network can selectively learn the under-trained yet object-relevant features from adversarial samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a novel data augmentation method for semantic segmentation using a flexible anti-adversarial consistency regularization. In particular, our method is tailored for medical images that contain small and underrepresented key objects such as a polyp and tumor. With object-level perturbations, our method effectively expands discriminative regions on challenging samples while preserving the morphological characteristics of key objects. Extensive experiments with various backbones and datasets confirm the effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Conceptual illustration of the adversarial attack (red) and anti-adversarial perturbation (blue) in the latent feature space. Given a predicted sample embedding xi, let its true label be a class 1 (C1). The adversarial attack sends the data point toward class 2 (C2) whereas the anti-adversarial perturbation increases its classification score. (b) Adaptive anti-adversarial consistency regularization (AAC) between the adversarially attacked data and the anti-adversary. (Color figure online)</figDesc><graphic coords="3,56,46,54,41,339,73,67,30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An overview of our training scheme. Adversarial and anti-adversarial perturbations are iteratively performed for the objects of a given image Xi. Adversarial noise µ - i,k moves Xi across the decision boundary, whereas anti-adversarial noise µ + i,k pushes Xi away from the boundary. Downstream consistency regularization loss Rcon minimizes the gap between adversaries {X - i,k } K k=1 and anti-adversary X + i,K .</figDesc><graphic coords="4,41,79,54,02,340,21,119,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparisons of precision and recall on the test set of Kvasir-SEG with U-Net.</figDesc><graphic coords="7,106,98,329,03,238,18,69,73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Input data (b) Ground truth label (c) Adversarially perturbed data (d) Adversarial noise (e) Anti-adversarially perturbed data (f) Anti-adversarial noise.</figDesc><graphic coords="8,42,30,239,63,339,88,85,15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Density of pixel embeddings (orange and green), a sample (black) and its perturbations (red and blue) in 2D feature space via t-SNE. (b) and (c) show loss flow comparisons between a sample X and its perturbations X -and X + . Supervised losses are compared w.r.t. epochs and perturbation steps, and the anti-adversaries (blue) always demonstrate the lowest loss (i.e., closer to the ground truth). (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison of ground truths and pseudo labels from anti-adversaries. (a) Input data (b) Ground truth label (c) Pseudo-label Ŷ + K . (Color figure online)</figDesc><graphic coords="9,55,98,54,38,340,27,154,33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison with existing data augmentation methods.</figDesc><table><row><cell>Method</cell><cell>U-Net</cell><cell cols="3">U-Net++ LinkNet DeepLabv3+ mIoU</cell><cell>U-Net</cell><cell cols="3">U-Net++ LinkNet DeepLabv3+ mDice</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Kvasir-SEG</cell><cell></cell><cell></cell><cell></cell></row><row><cell>No Aug.</cell><cell>81.76</cell><cell>63.13</cell><cell>73.92</cell><cell>85.75</cell><cell>86.29</cell><cell>74.75</cell><cell>85.00</cell><cell>89.65</cell></row><row><cell>Basic</cell><cell>89.73</cell><cell>89.94</cell><cell>90.52</cell><cell>90.43</cell><cell>94.63</cell><cell>94.70</cell><cell>95.02</cell><cell>94.97</cell></row><row><cell>CutMix [31]</cell><cell>89.84</cell><cell>90.80</cell><cell>90.56</cell><cell>90.60</cell><cell>94.65</cell><cell>95.18</cell><cell>95.05</cell><cell>95.08</cell></row><row><cell>CutOut [4]</cell><cell>88.63</cell><cell>88.63</cell><cell>89.52</cell><cell>90.70</cell><cell>93.29</cell><cell>93.97</cell><cell>94.47</cell><cell>95.12</cell></row><row><cell>Elastic Trans. [26]</cell><cell>89.71</cell><cell>88.34</cell><cell>89.89</cell><cell>91.44</cell><cell>94.57</cell><cell>93.81</cell><cell>94.63</cell><cell>95.40</cell></row><row><cell>Random Erase [33]</cell><cell>88.73</cell><cell>89.45</cell><cell>90.72</cell><cell>90.94</cell><cell>94.03</cell><cell>94.43</cell><cell>95.14</cell><cell>95.25</cell></row><row><cell>DropBlock [7]</cell><cell>86.88</cell><cell>88.40</cell><cell>90.75</cell><cell>90.22</cell><cell>92.98</cell><cell>93.84</cell><cell>95.15</cell><cell>94.86</cell></row><row><cell>GNT [22]</cell><cell>82.37</cell><cell>88.71</cell><cell>90.32</cell><cell>90.88</cell><cell>90.36</cell><cell>94.02</cell><cell>94.91</cell><cell>95.22</cell></row><row><cell>LU [9]</cell><cell>89.51</cell><cell>90.84</cell><cell>87.71</cell><cell>90.52</cell><cell>94.46</cell><cell>95.20</cell><cell>93.45</cell><cell>95.02</cell></row><row><cell>TumorCP [30]</cell><cell>90.92</cell><cell>91.18</cell><cell>90.87</cell><cell>91.65</cell><cell>95.24</cell><cell>95.39</cell><cell>95.22</cell><cell>95.64</cell></row><row><cell>Ours</cell><cell>92.43</cell><cell>91.43</cell><cell>91.51</cell><cell>92.42</cell><cell>96.07</cell><cell>95.52</cell><cell>95.57</cell><cell>96.06</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ETIS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No Aug.</cell><cell>83.80</cell><cell>83.96</cell><cell>82.18</cell><cell>82.52</cell><cell>91.18</cell><cell>91.28</cell><cell>90.22</cell><cell>90.43</cell></row><row><cell>Basic</cell><cell>86.08</cell><cell>87.02</cell><cell>84.75</cell><cell>82.69</cell><cell>92.52</cell><cell>93.06</cell><cell>91.75</cell><cell>90.52</cell></row><row><cell>CutMix [31]</cell><cell>86.03</cell><cell>86.78</cell><cell>82.52</cell><cell>82.20</cell><cell>92.49</cell><cell>92.92</cell><cell>90.42</cell><cell>90.83</cell></row><row><cell>CutOut [4]</cell><cell>84.37</cell><cell>84.90</cell><cell>86.55</cell><cell>84.50</cell><cell>91.52</cell><cell>91.83</cell><cell>92.79</cell><cell>91.60</cell></row><row><cell>Elastic Trans. [26]</cell><cell>85.12</cell><cell>85.10</cell><cell>86.55</cell><cell>84.13</cell><cell>91.96</cell><cell>91.95</cell><cell>92.79</cell><cell>91.38</cell></row><row><cell>Random Erase [33]</cell><cell>85.20</cell><cell>84.12</cell><cell>82.52</cell><cell>83.63</cell><cell>92.01</cell><cell>91.37</cell><cell>90.43</cell><cell>90.83</cell></row><row><cell>DropBlock [7]</cell><cell>82.52</cell><cell>85.33</cell><cell>82.49</cell><cell>84.27</cell><cell>90.42</cell><cell>92.08</cell><cell>90.41</cell><cell>91.46</cell></row><row><cell>GNT [22]</cell><cell>85.36</cell><cell>84.94</cell><cell>84.19</cell><cell>84.55</cell><cell>92.10</cell><cell>91.86</cell><cell>91.42</cell><cell>91.63</cell></row><row><cell>LU [9]</cell><cell>82.52</cell><cell>82.52</cell><cell>82.43</cell><cell>84.33</cell><cell>90.43</cell><cell>90.42</cell><cell>90.37</cell><cell>91.50</cell></row><row><cell>TumorCP [30]</cell><cell>82.59</cell><cell>84.99</cell><cell>85.69</cell><cell>85.32</cell><cell>90.46</cell><cell>91.89</cell><cell>92.30</cell><cell>92.08</cell></row><row><cell>Ours</cell><cell>88.35</cell><cell>87.62</cell><cell>88.41</cell><cell>85.58</cell><cell>93.81</cell><cell>93.40</cell><cell>93.85</cell><cell>92.23</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This research was supported by <rs type="grantNumber">IITP-2022-0-00290</rs> (<rs type="grantNumber">50%</rs>), <rs type="grantNumber">IITP-2019-0-01906</rs> (<rs type="programName">AI Graduate Program</rs> at <rs type="institution">POSTECH</rs>, <rs type="grantNumber">10%</rs>), <rs type="grantNumber">IITP-2022-2020-0-01461</rs> (<rs type="funder">ITRC</rs>, <rs type="grantNumber">10%</rs>) and <rs type="grantNumber">NRF-2022R1A2C2092336</rs> (<rs type="grantNumber">30%</rs>) funded by the <rs type="funder">Korean government (MSIT)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4UaKKns">
					<idno type="grant-number">IITP-2022-0-00290</idno>
				</org>
				<org type="funding" xml:id="_2tmkqKw">
					<idno type="grant-number">50%</idno>
				</org>
				<org type="funding" xml:id="_G4HMxxS">
					<idno type="grant-number">IITP-2019-0-01906</idno>
					<orgName type="program" subtype="full">AI Graduate Program</orgName>
				</org>
				<org type="funding" xml:id="_ZCTznKJ">
					<idno type="grant-number">10%</idno>
				</org>
				<org type="funding" xml:id="_Xzwk6ef">
					<idno type="grant-number">IITP-2022-2020-0-01461</idno>
				</org>
				<org type="funding" xml:id="_JxYQ7Z6">
					<idno type="grant-number">10%</idno>
				</org>
				<org type="funding" xml:id="_jdfNhey">
					<idno type="grant-number">NRF-2022R1A2C2092336</idno>
				</org>
				<org type="funding" xml:id="_YAdDarU">
					<idno type="grant-number">30%</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combating adversaries with anti-adversaries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alfarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="5992" to="6000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Linknet: exploiting encoder representations for efficient semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Visual Communications and Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PraNet: parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-226" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dermoscopic pattern of pigmented basal cell carcinoma, blue-white variant</title>
		<author>
			<persName><forename type="first">S</forename><surname>Felder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rabinovitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dermatol. Surg</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="569" to="570" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dropblock: a regularization method for convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data augmentation in logit space for medical image classification with limited training data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_45</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-345" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="469" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Doubleu-net: a deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Computer-Based Medical Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="558" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kvasir-SEG: a segmented polyp dataset</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-37734-2_37</idno>
		<idno>978-3-030-37734-2 37</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MMM 2020</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11962</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Uncertainty sampling methods for one-class classifiers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Juszczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML-03, Workshop on Learning with Imbalanced Data Sets II</title>
		<meeting>ICML-03, Workshop on Learning with Imbalanced Data Sets II</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Secure, privacy-preserving and federated machine learning in medical imaging</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Makowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="305" to="311" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time detection of colon polyps during colonoscopy using deep learning: systematic validation with four independent datasets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">8379</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anti-adversarially manipulated attributions for weakly and semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4071" to="4080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Heterogeneous uncertainty sampling for supervised learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Catlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How to measure uncertainty in uncertainty sampling for active learning</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Shaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="122" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated melanoma types and stages classification for dermoscopy images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Talati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Innovations in Power and Advanced Computing Technologies</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple way to make neural networks robust against diverse image corruptions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rusak</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58580-8_4</idno>
		<idno>978-3-030-58580-8 4</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12348</biblScope>
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effective use of synthetic data for urban scene semantic segmentation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="84" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FCN-transformer feature fusion for polyp segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Matuszewski</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-12053-4_65</idno>
		<idno>978-3-031-12053-4 65</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MIUA 2022</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Aviles-Rivero</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Roberts</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Schönlieb</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13413</biblScope>
			<biblScope unit="page" from="892" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in WCE images for early diagnosis of colorectal cancer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MSRF-Net: a multi-scale residual fusion network for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2252" to="2263" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jorge Cardoso</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-67558-9_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-67558-928" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2017</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10553</biblScope>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stepwise feature fusion: Local guides global</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-811" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="110" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TumorCP: a simple but effective object-level data augmentation for tumor segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_55</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-255" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cutmix: regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contrastive learning for label efficient semantic segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10623" to="10633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">UNet++: a nested U-net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-51" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Ziller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Usynin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical imaging deep learning with differential privacy. Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
