<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gene-Induced Multimodal Pre-training for Image-Omic Classification</title>
				<funder ref="#_zwmFUau">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_GWkAwxh">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
				</funder>
				<funder ref="#_eVgPaAZ">
					<orgName type="full">Shanghai Natural Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ting</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200241</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingran</forename><surname>Xie</surname></persName>
							<email>xrxie@stu.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200241</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Renjie</forename><surname>Wan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingli</forename><surname>Li</surname></persName>
							<email>qlli@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200241</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<email>ywang@cee.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200241</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gene-Induced Multimodal Pre-training for Image-Omic Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="508" to="517"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B4C68EAE8CC7E1CE4240BEA0A0B91865</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_49</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multimodal learning â€¢ Whole slide image classification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Histology analysis of the tumor micro-environment integrated with genomic assays is the gold standard for most cancers in modern medicine. This paper proposes a Gene-induced Multimodal Pretraining (GiMP) framework, which jointly incorporates genomics and Whole Slide Images (WSIs) for classification tasks. Our work aims at dealing with the main challenges of multi-modality image-omic classification w.r.t. (1) the patient-level feature extraction difficulties from gigapixel WSIs and tens of thousands of genes, and (2) effective fusion considering high-order relevance modeling. Concretely, we first propose a group multi-head self-attention gene encoder to capture global structured features in gene expression cohorts. We design a masked patch modeling paradigm (MPM) to capture the latent pathological characteristics of different tissues. The mask strategy is randomly masking a fixedlength contiguous subsequence of patch embeddings of a WSI. Finally, we combine the classification tokens of paired modalities and propose a triplet learning module to learn high-order relevance and discriminative patient-level information. After pre-training, a simple fine-tuning can be adopted to obtain the classification results. Experimental results on the TCGA dataset show the superiority of our network architectures and our pre-training framework, achieving 99.47% in accuracy for imageomic classification. The code is publicly available at https://github.com/ huangwudiduan/GIMP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pathological image-omic analysis is the cornerstone of modern medicine and demonstrates promise in a variety of different tasks such as cancer diagnosis and prognosis <ref type="bibr" target="#b11">[12]</ref>. With the recent advance of digital pathology and sequencing technologies, modern cancer screening has jointly incorporated genomics and histology analysis of whole slide images (WSIs).</p><p>Though deep learning techniques have revolutionized medical imaging, designing a task-specific algorithm for image-omic multi-modality analysis is challenging. <ref type="bibr" target="#b0">(1)</ref> The gigapixel WSIs, which generally yield 15,000 foreground patches during pre-processing, make attention-based backbones <ref type="bibr" target="#b5">[6]</ref> hard to extract precise image (WSI)-level representations. (2) Learning features from genomics data which have tens of thousands of genes make models such as Transformer <ref type="bibr" target="#b15">[16]</ref> impractical to use due to its quadratic computation complexity. (3) Image-omic feature fusion <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> may fail to model high-order relevance and the inherent structural characteristics of each modality, making the fusion less effective.</p><p>Specifically, to our knowledge, most multi-modality techniques have been designed for modalities such as chest X-ray and reports <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>, CT and X-ray <ref type="bibr" target="#b17">[18]</ref>, CT and MRI <ref type="bibr" target="#b20">[21]</ref>, H&amp;E cross-staining <ref type="bibr" target="#b21">[22]</ref> via global feature, local feature or multi-granularity alignment. But, none of these works considers the challenges in WSIs and genes processing. Besides, vision-language models in the computer vision community stand out for their remarkable versatility <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Nevertheless, constrained by computing resources, the most commonly used multimodal representation learning strategy, contrastive learning, which relies on a large number of negative samples to avoid model collapse <ref type="bibr" target="#b7">[8]</ref>, is not affordable for gigapixel WSIs analysis. A big domain gap also hampers their usage in leveraging the structural characteristic of tumor micro-environment and genomic assay. Recently, the literature corpus has proposed some methods for accomplishing specific image-omic tasks via Kronecker Product fusion <ref type="bibr" target="#b1">[2]</ref> or co-attention mapping between WSIs and genomics data <ref type="bibr" target="#b2">[3]</ref>. But, the Kronecker product overly concerns feature interactions between modalities while ignoring high-order relevance, w.r.t. decision boundaries across multiple samples, which is critical to classification tasks. As for the co-attention module, it is unidirectional and cannot localize significant regions from genetic data with a large amount of information.</p><p>In this paper, we propose a task-specific framework dubbed Gene-induced Multimodal Pre-training (GiMP) for image-omic classification. Concretely, we first propose a transformer-based gene encoder, Group Multi-head Self Attention (GroupMSA), to capture global structured features in gene expression cohorts. Next, we design a pre-training paradigm for WSIs, Masked Patch Modeling (MPM), masking random patch embeddings from a fixed-length contiguous subsequence of a WSI. We assume that one patch-level feature embedding can be reconstructed by its adjacent patches, and this process enhances the learning ability for pathological characteristics of different tissues. Our MPM only needs to recover the masked patch embeddings in a fixed-length subsequence rather than processing all patches from WSIs. Furthermore, to model the high-order relevance of the two modalities, we combine CLS tokens of paired image and genomic data to form unified representations and propose a triplet learning module to differentiate patient-level positive and negative samples in a mini-batch. It is worth mentioning that although our unified representation fuses features from the whole gene expression cohort and partial WSIs in a mini-batch, we can still learn high-order relevance and discriminative patient-level information between these two modalities in pre-training thanks to the triplet learning module. In addition, note that our proposed method is different from self-supervised pre-training. Specifically, we focus not only on superior representation learning capability, but also category-related feature distributions, w.r.t. intra-and inter-class variation. With the training process going on, complete information from WSIs can be integrated and the fused multimodal representations with high discrimination will make it easier for the classifier to find the classification hyperplane. Experimental results demonstrate that our GiMP achieves significant improvement in accuracy than other image-omic competitors, and our multimodal framework shows competitive performance even without pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Given a multimodal dataset D consisting of pairs of WSI pathological images and genomic data (X I , X G ), our GiMP learns feature representations via accomplishing masked patch modeling and triplets learning. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the overall framework consists of three parts: 1) group-based genetic encoder GroupMSA (Sect. 2.1), 2) efficient patch aggregator (Sect. 2.2) and 3) gene-induced multimodal fusion (Sect. 2.3). In the subsequent sections, we will introduce each part of our proposed framework in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Group Multi-head Self Attention</head><p>In this section, we propose Group Multi-head Self Attention (GroupMSA), a specialized gene encoder to capture structured features in genomic data cohorts. Specifically, inspired by tokenisation techniques in natural language processing <ref type="bibr" target="#b15">[16]</ref>, the input expression cohort X G âˆˆ R Nge is partitioned into N f nonoverlapping fragments, and we then use a linear projection head to acquire fragment features H f âˆˆ R N f Ã—d , where d is the hidden dimension. Next, we introduce an intra-and-inter attention module to capture local and global information in H f . Firstly, the fragment features are divided into groups and there are N gr learnable group tokens linked to each group resulting in (N f /N gr + 1) tokens per group. Then the prepared tokens are fed to a vanilla multi-head self-attention (MSA) block to extract intra-group information. After that, we model cross-group interactions by another MSA layer on the global scale with the locally learned group tokens and a final classification token CLS ge âˆˆ R d . Finally, GroupMSA could learn dense semantics from the genomic data cohort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Patch Aggregator with Efficient Attention Operation</head><p>Let's denote the whole slide pathological image with HÃ—W spatial resolution and C channels by X I âˆˆ R HÃ—W Ã—C . We follow the preprocessing strategy of CLAM <ref type="bibr" target="#b10">[11]</ref> to acquire patch-level embedding sequence, i.e., each foreground patch with 256Ã—256 pixels is fed into an ImageNet-pretrained ResNet50 and the background region is discarded. Let H p = h j | h j âˆˆ R 1024 Np j=1 denote the sequence of patch embeddings corresponding to WSI X I and note that the total patch number N p is image-specific. Since the quadratic computational complexity of the standard self-attention mechanism is usually unaffordable in WSI analysis due to its long instances sequence, we employ Nystrom-based attention algorithm <ref type="bibr" target="#b19">[20]</ref> to aggregate patch embeddings and yield image-level predictions. Specifically, the input sequence H p is first embedded into a d-dimensional feature space and combined with a classification token CLS img , yielding H 0 p âˆˆ R (Np+1)Ã—d . Then we perform different projection operations on H 0 p :</p><formula xml:id="formula_0">Q l = H l p â€¢ W l Q , K l = H l p â€¢ W l K , V l = H l p â€¢ W l V ,<label>(1)</label></formula><formula xml:id="formula_1">H l+1 p = softmax( Q l â€¢ K l âˆš d ) â€¢ softmax( Q l â€¢ K l âˆš d ) -1 â€¢ softmax( Q l â€¢ K l âˆš d ) â€¢ V l , (<label>2</label></formula><formula xml:id="formula_2">) where W l Q , W l K , W l V âˆˆ R dÃ—d are linear mapping matrices, Q l , K l âˆˆ R mÃ—d (m N p )</formula><p>are downsampling matrices obtained from clustering tokens in Q l and K l for layer l âˆˆ {0, 1}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Gene-Induced Multimodal Fusion</head><p>In this section, we first describe the formulation of masked patch modeling. Then we introduce the overall pipeline of our pre-training framework and illustrate how to apply it to downstream classification tasks.</p><p>Masked Patch Modeling. In WSIs, the foreground patches are spatially contiguous, which means the adjacent patches have similar feature embeddings. Thus, we propose a Masked Patch Modeling (MPM) pre-training strategy that masks random patch embeddings from a fixed-length contiguous subsequence</p><formula xml:id="formula_3">H mpm = h j | h j âˆˆ R 1024 L+i</formula><p>j=i in H p and reconstruct the invisible information. The fixed subsequence length L is empirically set to 6,000 and the sequences shorter than L are duplicated to build mini batches. Besides, the masking ratio is set to 50% and the set of masked subscripts is denoted as M âˆˆ R 0.5L . Next, a two-layer Nystrom-based patch aggregator followed by a lightweight reconstruction decoder are adopted to process the masked sequence H mpm and the reconstructed sequence is denoted as</p><formula xml:id="formula_4">H rec = Ä¥j | Ä¥j âˆˆ R 1024 L j=1</formula><p>. Note that we reconstruct the missing feature embeddings rather than the raw pixels of the masked areas, which is different from traditional MIM methods like SimMIM <ref type="bibr" target="#b18">[19]</ref> and MAE <ref type="bibr" target="#b4">[5]</ref>. In this way, the model could consider latent pathological characteristics of different tissues, which makes the pretext task more challenging. The reconstruction L 1 loss is computed by:</p><formula xml:id="formula_5">L rec = L j=1 1[j âˆˆ M] h j -Ä¥j 1 ,<label>(3)</label></formula><p>where 1[â€¢] is the indicator function.</p><p>Gene-Induced Triplet Learning. The transformer-based backbones in the classification task require the CLS token to be able to extract accurate global information, which is even more important yet difficult in WSIs due to the long sequence challenge. In addition, in order to construct the mini-batch, the subsequences we intercept in the MPM pre-training phase may not be sufficiently representative of the image-level characteristics. To overcome these issues, we further propose a gene-induced triplet learning module, which uses pathological images and genomic data as input and extracts high-order and discriminative features via CLS tokens. Firstly, we pre-train the GroupMSA module by patientlevel annotations in advance and froze it in the following iterations. Next, a learnable CLS token CLS img for WSIs is added to the input masked sequence H mpm . After extracting the input patch embeddings and gene sequence separately, we concatenate CLS img and CLS ge as CLS pat âˆˆ R 2d to represent patient-level characteristics.</p><p>Suppose we obtain a triplet list {x, x + , x -} during current iteration, where x, x + , x -are concatenated tokens of anchor CLS pat , positive CLS pat , and negative CLS pat , respectively. To enhance the global modeling capability, i.e., extracting more precise patient-level features, we expect that the distance between the anchor and the positive sample gets closer, while the negative sample is farther away. The loss function for optimizing triplet learning is computed by:</p><formula xml:id="formula_6">L tri = max( x -x + 2 2 + Î´ -x -x -2 2 , 0),<label>(4)</label></formula><p>Î´ indicates a threshold, e.g., Î´ = 0.8. Finally, the loss function for GiMP pretraining is:</p><formula xml:id="formula_7">L pre = L tri + L rec .</formula><p>Multimodal Fine-Tuning. Applying the pre-trained backbone to image-omic classification task is straightforward, since GiMP pre-training allows it to learn representative patient-level features. We use a simple Multi-Layer Perceptron (MLP) head to map CLS pat to the final class predictions P , which can be written as P = softmax(MLP(CLS pat )).</p><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Datasets. We verify the effectiveness of our method on The Caner Genome Atlas (TCGA) non-small cell lung cancer (NSCLC) dataset, which contains two cancer subtypes, i.e., Lung Squamous Cell Carcinoma (LUSC) and Lung Adenocarcinoma (LUAD). After pre-processing <ref type="bibr" target="#b10">[11]</ref>, the patch number extracted from WSIs at 20Ã— magnification varies from 485 to 148,569. We collect corresponding RNA-seq FPKM data for each patient and the length of the input genomic sequence is 60,480. Among 946 image-omic pairs, 470 of them belong to LUAD and 476 cases are LUSC. We randomly split the data into 567 for training, 189 for validation and 190 for testing.</p><p>Implementation Details. The pre-training process of all algorithms is conducted on the training set, without any extra data augmentation. Note that our genetic encoder, GroupMSA, is fully supervised pre-trained on unimodal genetic data to accelerate convergence and it is frozen during GiMP training process. The maximum pre-training epoch for all methods is set to 100 and we finetune the models at the last epoch. During fine-tuning, we evaluate the model on the validation set after every epoch, and save the parameters when it performs the best. AdamW <ref type="bibr" target="#b9">[10]</ref> is used as our optimizer and the learning rate is 10 -4 with cosine decline strategy. The maximum number of fine-tune epoch is 70. At last, we measure the performance on the test set. Training configurations are consistent throughout the fine-tuning process to ensure fair comparisons. All experiments are conducted on a single NVIDIA GeForce RTX 3090.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison Between GiMP and Other Methods</head><p>We conduct comparisons between GiMP and three competitors under different settings. Firstly, we compare our proposed patch aggregator with the current state-of-the-art deep MIL models on unimodal TCGA-NSCLC dataset, i.e., only pathological WSIs are included as input. As shown in Table <ref type="table" target="#tab_0">1</ref>, our proposed patch aggregator outperforms all the compared attention based multiple instance learning baselines in classification accuracy. In particular, 1.6% higher than the ABMIL <ref type="bibr" target="#b5">[6]</ref> 0.7737 DSMIL <ref type="bibr" target="#b8">[9]</ref> 0.7566 CLAM-SB <ref type="bibr" target="#b10">[11]</ref> 0.8519 CLAM-MB <ref type="bibr" target="#b10">[11]</ref> 0.8889 TransMIL <ref type="bibr" target="#b14">[15]</ref> 0.8836 Pathology w/o pre-train GiMP (w/o GroupMSA) 0.8995 PORPOISE <ref type="bibr" target="#b3">[4]</ref> 0.9524 Pathomic Fusion <ref type="bibr" target="#b1">[2]</ref> 0.9684 MCAT <ref type="bibr" target="#b2">[3]</ref> 0.9632 w/o pre-train GiMP (ours) 0.9737 MGCA <ref type="bibr" target="#b16">[17]</ref> 0.9105 BioViL <ref type="bibr" target="#b0">[1]</ref> 0.9316 REFERS <ref type="bibr" target="#b22">[23]</ref> 0 second best compared method TransMIL <ref type="bibr" target="#b14">[15]</ref>. We then explore the superiority of GiMP by comparing to state-of-the-art medical multi-modal approaches. We particularly compare our method to BioViL <ref type="bibr" target="#b0">[1]</ref>, MGCA <ref type="bibr" target="#b16">[17]</ref> and REFERS <ref type="bibr" target="#b22">[23]</ref>, three popular multimodal pre-training algorithms in medical text-image classification task. We can observe in the table that, our GiMP raises ACC from 91.05% to 99.47% on TCGA-NSCLC dataset. Even without pre-training stage, GiMP shows competitive performance compared to PORPOISE <ref type="bibr" target="#b3">[4]</ref>, Pathomic Fusion <ref type="bibr" target="#b1">[2]</ref>, and MCAT <ref type="bibr" target="#b2">[3]</ref>, three influential image-omic classification architectures. We further explore why GiMP works by insightful interpretation of the proposed method with t-SNE visualisation. Figure <ref type="figure" target="#fig_1">2</ref> shows the feature mixtureness of pre-trained CLS pat extracting global information on training set. Compari- son between Fig. <ref type="figure" target="#fig_1">2</ref> (a) and (b) indicates that the addition of the genomic data is indispensable in increasing the inter-class distance and reducing the intra-class distance, which confirms our motivation that gene-induced multimodal fusion could model high-order relevance and yield more discriminative representations. Moreover, compared to the mentioned self-supervised methods BioViL <ref type="bibr" target="#b0">[1]</ref> and MGCA <ref type="bibr" target="#b16">[17]</ref> in Fig. <ref type="figure" target="#fig_1">2 (c</ref>) and (d), CLS pat with GiMP pre-trained are well separated between LUAD and LUSC, i.e., GiMP pays more attention to the categoryrelated feature distribution and could extract more discriminative patient-level features during triplet learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>Table <ref type="table" target="#tab_1">2</ref> summarizes the results of ablation study. We first evaluate the effectiveness of the proposed GroupMSA. In the first two rows, GroupMSA achieves 0.53% improvement compared to SNN <ref type="bibr" target="#b6">[7]</ref>, a popular genetic encoders used in PORPOISE <ref type="bibr" target="#b3">[4]</ref> and Pathomic Fusion <ref type="bibr" target="#b1">[2]</ref>. We then analyze the effect of adding genetic modality during pre-training. The evaluation protocol is first pre-training, and then fine-tuning on downstream multimodal classification task. "Aggregator + MPM" means GiMP only uses WSIs as input and reconstructs the missing patch embeddings during the pre-training phase. Since the fixed subsequence length L = 6000 is used in our setting, it is sometimes smaller than the original patch number, e.g., the maximum size 148,569, the pre-trained model without genetic guidance may be not aware of sufficiently accurate patientlevel characteristics, i.e., ineffectively focused on normal tissues. "Aggregator + Triplet" indicates using unimodal image features to build triplets. We can likewise find that the lack of precise global representation leads to worse performance. Finally, we evaluate the necessity of the MPM module. "Aggregator + GroupMSA + Triplet" means GiMP only combines the CLS tokens of each modality and calculates triplet loss during pre-training. We can observe a performance drop without MPM module, e.g., from 99.47% to 95.26%, which demonstrates that local pathological information is equally critical as high-order relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel multimodal pre-training method to exploit the complementary relationship of genomic data and pathological images. Concretely, we introduce a genetic encoder with structured learning capabilities and an effective gene-induced multimodal fusion module which combines two pretraining objectives, triplet learning and masked patch modeling. Experimental results demonstrate the superior performance of the proposed GiMP compared to other state-of-the-art methods. The contribution of each proposed component of GiMP is also demonstrated in the experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of GiMP pre-training. Given a batch of image-omic pairs, we randomly select a fixed-length patch cohort and mask parts of the patch embeddings. Then we use two modality-specific encoders to capture unimodal features. Two pretraining objectives are considered: 1) building triplets by concatenated CLS tokens of each modality and enhancing the discriminability according to category relations, and 2) reconstructing the missing patch embeddings by its adjacent patches.</figDesc><graphic coords="3,61,80,57,74,294,13,149,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. t-SNE visualization of different methods with CLSpat after pre-training. (a) image-omic GiMP pre-trained, (b) GiMP pre-trained without gene inducing, (c) BioViL [1] pre-trained, (d) MGCA [17] pre-trained.</figDesc><graphic coords="7,50,34,316,07,322,96,66,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Accuracy comparison on the TCGA Lung Cancer dataset. The best results are marked in bold.</figDesc><table><row><cell>Modality</cell><cell>Pre-train</cell><cell>Method</cell><cell>Acc.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on TCGA Lung Cancer dataset. "SNN" means replacing GroupMSA with SNN<ref type="bibr" target="#b6">[7]</ref>. "Triplet" denotes our gene-induced triplet learning module.</figDesc><table><row><cell cols="2">Aggregator GroupMSA Triplet MPM Acc.</cell></row><row><cell>SNN [7]</cell><cell>0.9684</cell></row><row><cell></cell><cell>0.9737</cell></row><row><cell></cell><cell>0.9579</cell></row><row><cell></cell><cell>0.9263</cell></row><row><cell></cell><cell>0.9526</cell></row><row><cell></cell><cell>0.9974</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">62101191</rs>), <rs type="funder">Shanghai Natural Science Foundation</rs> (Grant No. <rs type="grantNumber">21ZR1420800</rs>), and the <rs type="funder">Science and Technology Commission of Shanghai Municipality</rs> (Grant No. <rs type="grantNumber">22DZ2229004</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zwmFUau">
					<idno type="grant-number">62101191</idno>
				</org>
				<org type="funding" xml:id="_eVgPaAZ">
					<idno type="grant-number">21ZR1420800</idno>
				</org>
				<org type="funding" xml:id="_GWkAwxh">
					<idno type="grant-number">22DZ2229004</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_49.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Making the most of text semantics to improve biomedical vision-language processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boecking</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20059-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20059-5_1" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13696</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pathomic fusion: an integrated framework for fusing histopathology and genomic features for cancer diagnosis and prognosis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="757" to="770" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal co-attention transformer for survival prediction in gigapixel whole slide images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pan-cancer integrative histology-genomic analysis via multimodal deep learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NeurIPS</title>
		<meeting>the NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A mutual information maximization perspective of language representation learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Eliceiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in Adam</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>CoRR abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Data-efficient and weakly supervised computational pathology on whole-slide images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="555" to="570" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The 2022 world health organization classification of tumours of the urinary system and male genital organs-part a: renal, penile, and testicular tumours</title>
		<author>
			<persName><forename type="first">H</forename><surname>Moch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Urol</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Hierarchical textconditional image generation with CLIP latents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR abs/2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">TransMIL: transformer based correlated multiple instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-granularity crossmodal alignment for generalized medical visual representation learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vardhanabhuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR abs/2210.06044</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">UniMiSS: universal medical self-supervised learning via breaking dimensionality barrier</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19803-8_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19803-8_33" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13681</biblScope>
			<biblScope unit="page" from="558" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SimMIM: a simple framework for masked image modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">NystrÃ¶mformer: a nystrÃ¶m-based algorithm for approximating self-attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Toward unpaired multimodal medical image segmentation via learning structured semantic consistency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR abs/2206.10571</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CS-CO: a hybrid self-supervised visual representation learning method for H&amp;E-stained histopathological images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">102539</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generalized radiograph representation learning via cross-supervision between images and free-text radiology reports</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
