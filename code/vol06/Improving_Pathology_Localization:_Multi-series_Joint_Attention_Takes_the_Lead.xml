<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Pathology Localization: Multi-series Joint Attention Takes the Lead</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ashwin</forename><surname>Raju</surname></persName>
							<email>ashwin.raju@coverahealth.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Covera Health</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Micha</forename><surname>Kornreich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Covera Health</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Colin</forename><surname>Hansen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Covera Health</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Browning</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Covera Health</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jayashri</forename><surname>Pawar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Covera Health</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Herzog</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Covera Health</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Odry</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Covera Health</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Covera Health</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Pathology Localization: Multi-series Joint Attention Takes the Lead</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">66AB7744906D8C4F75EA80F6097B5612</idno>
					<idno type="DOI">10.1007/978-3-031-43987-225.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pathology localization</term>
					<term>Multi-series</term>
					<term>Self-attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated magnetic resonance imaging (MRI) pathology localization can significantly reduce inter-reader variability and the time expert radiologists need to make a diagnosis. Many automated localization pipelines only operate on a single series at a time and are unable to capture inter-series relationships of pathology features. However, some pathologies require the joint consideration of multiple series to be accurately located in the face of highly anisotropic volumes and unique anatomies. To efficiently and accurately localize a pathology, we propose a Multi-series jOint ATtention localization framework (MOAT) for MRI, which shares information among different MRI series to jointly predict the pathological location(s) in each MRI series. The framework allows different MRI series to share latent representations with each other allowing each series to get location guidance from the others and enforcing consistency between the predicted locations. Extensive experiments on three knee MRI pathology datasets, including medial compartment cartilage (MCC) high-grade defects, medial meniscus (MM) tear and displaced fragment/flap (DF) with 2729, 2355, and 4608 studies respectively, show that our proposed method outperforms the state of the art approaches by 3.4 to 8.0 mm on L1 distance, 6 to 27% on specificity and 5 to 14% on sensitivity across different pathologies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>MRI is an essential diagnostic and investigative tool in clinical and research settings. Expert radiologists rely on multiple MRI series of varying acquisition parameters and orientations to capture different aspects of the underlying anatomy and diagnose any defect or pathology that may be present. For a knee study, it is typical to acquire MRI series with coronal, sagittal, and axial orientations using proton density (PD), proton density fat suppressed (PDFS) or T2-weighted fat suppressed series (T2FS) for each study. When series are analyzed in concert, a radiologist can make a more effective diagnosis and mark down the location of any corresponding defect in each series. The defect location is typically represented as a single point <ref type="bibr" target="#b2">[3]</ref> regardless of the defect size as a balance of effectiveness and efficiency.</p><p>In recent years, convolutional neural networks (CNNs) have achieved promising results in pathology localization. Many approaches rely on generating a multi-variate Gaussian heatmap, where the peak of the distribution represents the pathology localization. Hourglass <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>, an encoder-decoder style architecture <ref type="bibr" target="#b8">[9]</ref>, is a mainstream model to generate a Gaussian heatmap. It uses a series of convolutional and pooling layers to extract features from the input image followed by upsampling and convolutional layers to generate the Gaussian heatmap. However, Hourglass-based methods can be overly resource-intensive when applied to 3D volumes <ref type="bibr" target="#b10">[11]</ref>. To overcome this, regression-based models are becoming popular for detecting defects wherein a fully-connected layer is used on top of the encoder blocks to directly predict the location. These methods also alleviate the need for heatmap generation and post-processing methods to compute the location. Recently, transformer-based models have emerged as a promising trend in localization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>, and their performance has exceeded that of encoder-decoder based methods on single MRI volumes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>. With the availability of multiple series, we propose a framework that imitates a clinical workflow, by simultaneously analyzing multiple series and paying attention to the location that corresponds to a pathology across multiple series.</p><p>To do this, we design a framework that utilizes self-attention across multiple series and we further add a mask to allow the model to focus on relevant areas, which we term as Masked Self-Attention (MSA). To predict the pathology location, we use a transformer decoder with an encoder-based initialization of the reference points. This approach provides a strong initial guess of the pathology location, improving the accuracy of the model's predictions. Overall, our framework leverages the strengths of both self-attention and encoder-decoder architectures to enhance the performance of pathology localization.</p><p>Specifically, our contributions are:</p><p>-We introduce a framework that enables the simultaneous use of multiple series from an MRI study, allowing for the sharing of pathology information across different series through Masked Self-Attention. -We design a transformer-based decoder model to predict consistent locations across series in an MRI study, which reduces the network's parameters compared to standard heatmap-based approaches. -Through extensive experiments on three knee pathologies, we demonstrate the effectiveness and efficiency of our framework, showing the benefits of Masked Self-Attention and a Pathology localization decoder to accurately predict pathology locations.</p><p>Overall, our framework represents a promising step towards more consistent and accurate localization, which could have important applications in medical diagnosis and treatment.</p><p>Fig. <ref type="figure">1</ref>. Overview. More than 1 series are passed to encoders that have shared parameters. "Stem", "layer1", "layer2", "layer3" and "layer4" follows the ResNet <ref type="bibr" target="#b11">[12]</ref> architecture convention. We perform Masked Self-Attention starting from layer 2. The Pathology localization decoder accepts feature maps from layer 2 to layer 4 and uses a query for each series to perform deformable cross attention to generate pathological landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Our Architecture</head><p>We aim to produce a reliable pathology location for each series in a given study if a location is available for that series. More formally, we assume that we are given a dataset, D = {X i , Y i } N i=1 , with N denoting the total number of studies in the dataset, X i and Y i denoting the set of series and corresponding location for each series. Due to different acquisition protocols, the number of series in each X i can vary. Similarly, each Y i can have a different number of location. Our goal is to predict a pathology location for each series and its corresponding confidence score. Figure <ref type="figure">1</ref> outlines our framework which can accept multiple series to generate a more accurate locations for each series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Backbone</head><p>Our framework contains a backbone, which is responsible for generating multilevel feature maps. The multi-level feature maps are then fed into the pathology localization decoder. We use a 3D ResNet50 <ref type="bibr" target="#b11">[12]</ref>, which accepts the volume as the input and generates multiple feature maps. Each series has its own backbone with the weights been shared. Given an input X k i ∈ R d×w×h , denoting a series k from the study i, we extract multiple feature maps of resolutions</p><formula xml:id="formula_0">F j ∈ ( d 1 , w 8 , h 8 ), ( d 2 , w 16 , h 16 ), ( d 4 , w 32 , h 32 )</formula><p>for each series k. We adhere to common standards by initializing the 3D ResNet50 backbone with pretrained weights. Prior work fine-tunes weights from the ImageNet dataset, but this may not be optimal if the target dataset has different characteristics. Our pretrained model for medical image analysis is based on ConVIRT <ref type="bibr" target="#b14">[15]</ref>, which uses visual representations and descriptive text from our internal dataset that contains 35433 image and text pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Masked Self-attention</head><p>To explore the complementary information between different series, we use Masked Self-Attention inspired from <ref type="bibr" target="#b1">[2]</ref> which we call MSA, a powerful tool commonly used in multi-modality <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> models that enable to capture longrange dependencies between features. More formally, we denote the latent feature maps R l = {F j l } J j=1 , where j and l represents j th series and l th layer,</p><formula xml:id="formula_1">F j l ∈ (C in × d × w × h ) with C in</formula><p>representing the number of channels, d representing the depth dimension, and w and h representing the width and height dimensions, respectively. We concatenate the features F j l along the depth dimension d and add position embedding on the concatenated features. The transformer uses a linear projection for computing the set of queries, keys and values Q, K and V respectively. We adhere to the naming conventions used in <ref type="bibr" target="#b7">[8]</ref>.</p><formula xml:id="formula_2">Q = R l .U q , K = R l .U k , V = R l .U v (1)</formula><p>where</p><formula xml:id="formula_3">U q ∈ R Cin×Cq , U k ∈ R Cin×C k and U v ∈ R Cin×Cv .</formula><p>The self-attention is calculated by taking the dot products between Q and K and then aggregating the values for each query,</p><formula xml:id="formula_4">A = Softmax M l-1 + B + QK T √ C k V (2) M l-1 = 0 if M l-1 = 1 -∞ otherwise (3)</formula><p>where, the attention mask M l-1 ∈ {0, 1} is a binarized output (thresholded at δ t ) of the the resized mask prediction of the previous (l -1)-th layer. δ t is empirically set to 0.15. The attention mask ignores the features that are not relevant to the pathology and attends to pathological features. B is a mask to handle missing series and it shares the same equation as 3.</p><p>Finally, the transformer uses a non-linear transformation to calculate the output features, R l+1 , which shares the same resolution as that of R l .</p><formula xml:id="formula_5">R l+1 = MLP(A) + R l (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>The transformer applies the attention mechanism 3 L times to generate a deep representation learning among the features. This approach allows the transformer model to effectively capture the relationships between different input positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Pathology Localization Decoder</head><p>The localization decoder follows the transformer decoder paradigm, using a query, reference points, and input feature maps to predict a location and corresponding score. The decoder has N identical layers, each consisting of crossattention and feed forward networks (FFNs). The query Q ∈ R 1×256 and reference points R ∈ R 3 go through each layer, generating an updated Q as input for the next layer. Unlike Deformable DETR <ref type="bibr" target="#b16">[17]</ref>, the decoder initializes reference points by taking the last layer of the backbone feature map and applying Global Average Pooling, followed by a fully connected layer to generate the initial reference point. The localization refinement stage outputs location and scores for each layer N i , similar to Deformable DETR, providing fast convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Loss Functions</head><p>The model generates a single location ŷl ∈ R 3 , score y s and auxiliary heatmap outputs H for each series in a given study. The goal of our framework is to generate one reasonable location and its corresponding score for each series. Since there may be multiple locations annotated for a series, we use the Hungarian Matching function <ref type="bibr" target="#b16">[17]</ref> to find optimal matching with the prediction to one of many ground truth locations. This is similar to the approach used in DETR. The Masked Self-Attention in our framework uses heatmaps generated from the previous layers. To ensure accurate heatmap generation, we apply an auxiliary heatmap loss using Mean Square Error (MSE) between the generated heatmap and the ground truth Gaussian heatmap, where the loss is defined as,</p><formula xml:id="formula_7">L heatmap = K i=1 (x -h i ) 2<label>(5)</label></formula><p>where K is the number of intermediate heatmaps generated, x and h i are ground truth heatmap and predicted heatmap. To penalize the predicted location, we use the Huber loss defined as,</p><formula xml:id="formula_8">L point = N i=1 1 2 (y i l -ŷi l ) 2 if (y i l -ŷi l ) &lt; δ δ((y i l -ŷi l ) -1 2 δ) otherwise (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where δ is empirically set to 0.3. The distance of a pathology does not differ more than λ (which can be calculated from the dataset) across series. With this information, we enforce proximity between the world coordinates which can be converted from the predicted volume coordinates across different series. We employ a Margin L1 loss, which penalizes the distance between points if they exceed the margin. Formally,</p><formula xml:id="formula_10">L cons = N i=1 N j=i+1 max(0, L1(wc ŷi l , wc ŷj l ) -λ) (7)</formula><p>where N is the number of series in a given study, wc ŷl is the world coordinates converted from volume coordinates.</p><p>We then formulate the confidence score loss by considering the sum over the series of the binary cross entropy between the ground truth confidence score and predicted confidence score, formally defined as,</p><formula xml:id="formula_11">L score = N i=1 -(y i s log(p i ) + (1 -y i s ) log(1 -p i ))<label>(8)</label></formula><p>Overall, the entire loss for a given study is formulated as,</p><formula xml:id="formula_12">L = w 1 L point + w 2 L score + w 3 L cons + w 4 L heatmap (9)</formula><p>We set the hyper parameter w 1 , w 2 , w 3 and w 4 as 10, 1, 0.1, 1 respectively. These values are empirically set based on the validation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details, Datasets and Evaluation Protocols</head><p>Implementation Details. Our model was implemented in Pytorch 1.13.1 on a NVIDIA A6000 GPU. We used an AdamW <ref type="bibr" target="#b4">[5]</ref> optimizer with a weight decay of 10 -4 . The initial learning rate for encoder was empirically set as 10 -5 and 10 -4 for all other modules. Before running the pathology detection, we perform a preprocessing step similar to <ref type="bibr" target="#b2">[3]</ref> and resize the volume to 28×128×128. Furthermore, we clip the intensity of the images at the 1st and 99th percentile, followed by an intensity normalization to ensure a mean of 0 and standard deviation of 1.</p><p>Other hyper-parameters are mentioned in the supplementary paper.</p><p>Datasets. The study is limited to secondary use of existing HIPPA-based deidentified data. No IRB required. We primarily conduct our experiments using knee MRI datasets, with a specific focus on MM tear, MM displaced fragment flap (DF), and MCC defect. Studies were collected at over 25 different institutions, and differed in scanner manufacturers, magnetic field strengths, and imaging protocols. The pathological locations were annotated by American Board certified sub-specialists radiologists. The most common series types included fat-suppressed (FS) sagittal (Sag), coronal (Cor) and axial (Ax) orientations, using either T2-weighted (T2) or proton-density (PD) protocols. For pathology detection, we use CorFS, SagFS, and SagPD. The dataset statistics that we use for training, validation and test are shown in Table <ref type="table" target="#tab_0">1</ref>. Evaluation Protocols. A useful pathology detection device should point the user to the correct location of a pathology. For model evaluation, we use the L1 distance between the predicted location to any annotation of the same pathology, labeled on the same series. To evaluate the pathology localization in a given study, we use the predicted pathology localization mask, which is obtained by thresholding the confidence score. However, this alone does not provide a complete picture of the model's performance. To evaluate our confidence score's performance, we analyze the specificity and sensitivity of the confidence scores. We report the mean over all series in the test studies in Table <ref type="table">2</ref> Table <ref type="table">2</ref>. Quantitative results. We show the L1 distance measured in (mm), Sensitivity (Sn), and Specificity (Sp) score for different models. "*" refers to the models that were trained with different hyper-parameters from their mentioned ones. The results are evaluated on the test dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with SOTA Methods</head><p>Heatmap-Based Architectures. The proposed architecture was compared to two other models, the Gaussian Ball approach <ref type="bibr" target="#b2">[3]</ref> which utilizes a UNet architecture to generate a heatmap and KNEEL <ref type="bibr" target="#b10">[11]</ref>, which uses an hourglass network architecture to predict the Gaussian heatmap. Two variants of UNet were compared, one with MSA and one without. The threshold was set for each model which balanced sensitivity and specificity on the validation data. The comparison revealed that the sensitivity and specificity of the proposed MOAT model were 14 to 27% and 15 to 17% higher, respectively, than those of the other models. Additionally, the L1 distance of the heatmap-based model was approximately 5.4 to 8.0 mm higher than that of MOAT for all true positives. Overall, the results suggest that MOAT outperforms the other models in terms of sensitivity, specificity, and L1 distance.</p><p>Regression-Based Architectures. We compared our proposed architecture with several other methods: 1) a simple regression method that removes the pathology localization decoder and uses a fully connected layer to predict the pathology locations, 2) DETR, 3) deformable DETR <ref type="bibr" target="#b16">[17]</ref>, and 4) Poseur <ref type="bibr" target="#b5">[6]</ref>, which uses Residual Log estimation. We adopt our ConVIRT pretrained encoder and add MSA to all the regression models to ensure a fair comparison. MOAT, which has 63.4G FLOPs, is highly efficient when compared to State-Of-The-Art (SOTA) regression models and has L1 distance lower than other models (4.7 mm) and the highest sensitivity and specificity among the models. We attach the standard deviation scores for each model in the supplementary section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>We first analyze the importance of MSA to our framework by training models with and without MSA. As MSA is a variant of self-attention, we also experiment with self-attention and with an attention mechanism <ref type="bibr" target="#b12">[13]</ref> that was popular prior to self-attention. Table <ref type="table" target="#tab_3">4</ref> shows the L1 distance for Medial Meniscus Tear (MM Tear) pathology, where our MSA which is a variant of self-attention is able to achieve the lowest L1 distance. Similarly, we analyze the weight factor for consistency loss, as different weight factor yields different results. From Table <ref type="table" target="#tab_2">3</ref>, we can see that the lowest L1 distance was obtained when the weight factor was 0.1. All the ablation studies were performed on the MM Tear validation dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose MOAT, a framework for performing localization in multi-series MRI studies which benefits from the ability to share relevant information across series via a novel application of self-attention. We increase the efficiency of the MOAT model by using a pathology localization decoder which is a variant of deformable decoder and initializes the reference points from the backbone of the model. We evaluate the effectiveness of our proposed framework (MOAT) on three challenging pathologies from knee MRI and find that it represents a significant improvement over several SOTA localization techniques. Moving forward, we aim to apply our framework to pathologies from other body parts with multiple series.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Cor, Sag, SagPD refers to Coronal FS, Sagittal FS, Sagittal PD respectively. Values under series refer to number of series where defect locations are available. Negatives refer to number of studies that does not have a pathology.</figDesc><table><row><cell cols="2">Pathology Train</cell><cell></cell><cell>Validation</cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell></cell><cell cols="6">Negatives Cor Sag SagPD Union Cor Sag SagPD Union Cor Sag SagPD Union</cell></row><row><cell cols="2">MM Tear 1215</cell><cell>1466 1173 1146</cell><cell>2679 975 843 771</cell><cell>975</cell><cell>954 857 765</cell><cell>954</cell></row><row><cell>DF</cell><cell>673</cell><cell>387 306 243</cell><cell>1862 247 176 148</cell><cell>277</cell><cell>190 146 106</cell><cell>216</cell></row><row><cell>MCC</cell><cell>1000</cell><cell>759 797 364</cell><cell>1926 360 387 152</cell><cell>437</cell><cell>304 317 136</cell><cell>366</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on MM Tear dataset to analyze the need for Masked Self-Attention.</figDesc><table><row><cell>Methods</cell><cell>L1 distance ↓</cell></row><row><cell cols="2">No Masked Self-Attention 11.3</cell></row><row><cell>self-attention</cell><cell>8.8</cell></row><row><cell>Masked Self-Attention</cell><cell>6.1</cell></row><row><cell>CBAM [13]</cell><cell>16.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on MM Tear to analyze the weight factor for the consistency loss.</figDesc><table><row><cell cols="2">Consistency(w3) L1 distance ↓</cell></row><row><cell>10</cell><cell>11.2</cell></row><row><cell>1</cell><cell>6.4</cell></row><row><cell>0.1</cell><cell>6.1</cell></row><row><cell>0.01</cell><cell>5.1</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-813" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Masked-attention mask transformer for universal image segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining mixed-format labels for AI-based pathology detection pipeline in a large-scale knee MRI study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kornreich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-118" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SDMT: spatial dependence multi-task transformer network for 3D knee MRI segmentation and landmark localization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Mao</surname></persName>
		</author>
		<title level="m">Poseur: direct human pose regression with transformers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lymph node detection in T2 MRI with transformers</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Mathai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Diagnosis</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="855" to="859" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>SPIE</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-modal fusion transformer for end-to-end autonomous driving</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Everything at once -multi-modal fusion transformer for video retrieval</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shvetsova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20020" to="20029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kneel: knee anatomical landmark localization using hourglass networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tiulpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Melekhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saarakkala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>CoRR abs/1711.11248</idno>
		<ptr target="http://arxiv.org/abs/1711.11248" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CBAM: convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Vitpose+: vision transformer foundation model for generic body pose estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.04246</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive learning of medical visual representations from paired images and text</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A novel method for 3D knee anatomical landmark localization by combining global and local features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">52</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<title level="m">Deformable DETR: deformable transformers for end-to-end object detection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
