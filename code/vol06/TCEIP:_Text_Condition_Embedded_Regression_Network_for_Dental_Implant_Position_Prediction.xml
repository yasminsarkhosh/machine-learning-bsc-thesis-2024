<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction</title>
				<funder ref="#_B9HsCjD">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_Zy8aDWb #_7eVeSW5">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_pjBjACE">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_UCGQYFu">
					<orgName type="full">Shenzhen Municipal Science and Technology Innovation Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinquan</forename><surname>Yang</surname></persName>
							<email>yangxinquan2021@email.szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">AI Research Center for Medical Image Analysis and Diagnosis</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">National Engineering Laboratory for Big Data System Computing Technology</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinheng</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">AI Research Center for Medical Image Analysis and Diagnosis</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">National Engineering Laboratory for Big Data System Computing Technology</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuguang</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Stomatology</orgName>
								<orgName type="institution">Shenzhen University General Hospital</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">AI Research Center for Medical Image Analysis and Diagnosis</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">National Engineering Laboratory for Big Data System Computing Technology</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Stomatology</orgName>
								<orgName type="institution">Shenzhen University General Hospital</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linlin</forename><surname>Shen</surname></persName>
							<email>llshen@szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">AI Research Center for Medical Image Analysis and Diagnosis</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">National Engineering Laboratory for Big Data System Computing Technology</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongqiang</forename><surname>Deng</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Stomatology</orgName>
								<orgName type="institution">Shenzhen University General Hospital</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="317" to="326"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">7986C12FAB90094D8DDBDB011A93845C</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_31</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dental Implant</term>
					<term>Deep Learning</term>
					<term>Text Guided Detection</term>
					<term>Cross-Modal Interaction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When deep neural network has been proposed to assist the dentist in designing the location of dental implant, most of them are targeting simple cases where only one missing tooth is available. As a result, literature works do not work well when there are multiple missing teeth and easily generate false predictions when the teeth are sparsely distributed. In this paper, we are trying to integrate a weak supervision text, the target region, to the implant position regression network, to address above issues. We propose a text condition embedded implant position regression network (TCEIP), to embed the text condition into the encoder-decoder framework for improvement of the regression performance. A cross-modal interaction that consists of cross-modal attention (CMA) and knowledge alignment module (KAM) is proposed to facilitate the interaction between features of images and texts. The CMA module performs a cross-attention between the image feature and the text condition, and the KAM mitigates the knowledge gap between the image feature and the image encoder of the CLIP. Extensive experiments on a dental implant dataset through five-fold cross-validation demonstrated that the proposed TCEIP achieves superior performance than existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>According to a systematic research study <ref type="bibr" target="#b1">[2]</ref>, periodontal disease is the world's 11th most prevalent oral condition, which potentially causes tooth loss in adults, especially the aged <ref type="bibr" target="#b7">[8]</ref>. One of the most appropriate treatments for such a defect/dentition loss is prosthesis implanting, in which the surgical guide is usually used. However, dentists must load the Cone-beam computed tomography (CBCT) data into the surgical guide design software to estimate the implant position, which is tedious and inefficient. In contrast, deep learning-based methods show great potential to efficiently assist the dentist in locating the implant position <ref type="bibr" target="#b6">[7]</ref>.</p><p>Recently, deep learning-based methods have achieved great success in the task of implant position estimation. Kurt et al. <ref type="bibr" target="#b3">[4]</ref> and Widiasri et al. <ref type="bibr" target="#b10">[11]</ref> utilized the convolutional neural network (CNN) to locate the oral bone, e.g., the alveolar bone, maxillary sinus and jaw bone, which determines the implant position indirectly. Different from these implant depth measuring methods, Yang et al. <ref type="bibr" target="#b12">[13]</ref> developed a transformer-based implant position regression network (Implant-Former), which directly predicts the implant position on the 2D axial view of tooth crown images and projects the prediction results back to the tooth root by the space transform algorithm. However, these methods generally consider simple situations, in which only one missing tooth is available. When confronting some special cases, such as multiple missing teeth and sparse teeth disturbance in Fig. <ref type="figure" target="#fig_0">1</ref>(a), the above methods may fail to determine the correct implant position. In contrast, clinically, dentists have a subjective expertise about where the implant should be planted, which motivates us that, additional indications or conditions from dentists may help predict an accurate implant position.</p><p>In recent years, great success has been witnessed in Vision-Language Pretraining (VLP). For example, Radford <ref type="bibr" target="#b8">[9]</ref> proposed Contrastive Language-Image Pretraining (CLIP) to learn diverse visual concepts from 400 million image-text pairs automatically, which can be used for vision tasks like object detection <ref type="bibr" target="#b16">[17]</ref> and segmentation <ref type="bibr" target="#b11">[12]</ref>. In this paper, we found that CLIP has the ability to learn the position relationship among instances. We showcase examples in Fig. <ref type="figure" target="#fig_0">1(b</ref>) that the image-text pair with the word 'left' get a higher matching score than others, as the position of baby is on the left of the billboard.</p><p>Motivated by the above observation in dental implant and the property of CLIP, in this paper, we integrate a text condition from the CLIP to assist the implant position regression. According to the natural distribution, we divide teeth regions into three categories in Fig. <ref type="figure" target="#fig_0">1(c),</ref><ref type="figure">i</ref>.e., left, middle, and right. Specifically, during training, one of the text prompts, i.e., 'right', 'middle', and 'left' is paired with the crown image as input, in which the text prompt works as a guidance or condition. The crown image is processed by an encoder-decoder network for final location regression. In addition, to facilitate the interaction between features in two modalities, a cross-modal interaction that consists of cross-modal attention (CMA) and knowledge alignment module (KAM), is devised. The CMA module fuses conditional information, i.e., text prompt, to the encoderdecoder. This brings additional indications or conditions from the dentist to help the implant position regression. However, a knowledge gap may exist between our encoder-decoder and CLIP. To mitigate the problem, the KAM is proposed to distill the encoded-decoded features of crown images to the space of CLIP, which brings significant localization improvements. In inference, given an image, the dentist just simply gives a conditioning text like "let's implant a prosthesis on the left", the network will preferentially seek a suitable location on the left for implant prosthesis.</p><p>Main contributions of this paper can be summarized as follows: 1) To the best of our knowledge, the proposed TCEIP is the first text condition embedded implant position regression network that integrates a text embedding of CLIP to guide the prediction of implant position. (2) A cross-modal interaction that consists of a cross-modal attention (CMA) and knowledge alignment module (KAM) is devised to facilitate the interaction between features that representing image and text. ( <ref type="formula" target="#formula_4">3</ref>) Extensive experiments on a dental implant dataset demonstrated the proposed TCEIP achieves superior performance than the existing methods, especially for patients with multiple missing teeth or sparse teeth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Given a tooth crown image with single or multiple implant regions, the proposed TCEIP aims to give a precise implant location conditioned by text indications from the dentist, i.e., a description of position like 'left', 'right', or 'middle'. An overview of TCEIP is presented in Fig. <ref type="figure" target="#fig_1">2</ref>. It mainly consists of four parts: i) Encoder and Decoder, ii) Conditional Text Embedding, iii) Cross-Modal Interaction Module, and iv) Heatmap Regression Network. After obtaining the predicted coordinates of the implant at the tooth crown, we adopt the space transformation algorithm <ref type="bibr" target="#b12">[13]</ref> to fit a centerline of implant to project the coordinates to the tooth root, where the real implant location can be acquired. Next, we will introduce these modules in detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Encoder and Decoder</head><p>We employ the widely used ResNet <ref type="bibr" target="#b2">[3]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conditional Text Embedding</head><p>To integrate the text condition provided by a dentist, we utilize the CLIP to extract the text embedding. Specifically, additional input of text, e.g., 'left', 'middle', or 'right', is processed by the CLIP Text Encoder to obtain a conditional text embedding v t ∈ R 1×D . As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, to interact with the image features from ResNet layers, a series of transformation f (•) and g(•) over v t are performed as follow:</p><formula xml:id="formula_0">V t = g(f (v t )) ∈ R H×W ×1 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where f (•) repeats text embedding v t from R 1×D to R 1×HW and g(•) then reshapes it to R H×W ×1 . This operation ensures better interaction between image and text in the same feature space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-Modal Interaction</head><p>High-resolution features from the aforementioned decoder can be directly used to regress the implant position. However, it cannot work well in situations of multiple teeth loss or sparse teeth disturbance. In addition, although we have extracted the conditional text embedding from the CLIP to assist the network regression, there exists a big difference with the feature of encoder-decoder in the feature space. To tackle these issues, we propose cross-modal interaction, including i) Cross-Modal Attention and ii) Knowledge Alignment module, to integrate the text condition provided by the dentist.  C+1) . The CMA module can be formulated as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Modal Attention</head><formula xml:id="formula_2">F et , F dt ∈ R H×W ×(</formula><formula xml:id="formula_3">O = γ(Softmax(θ(M)β(F))α(F)) + F,<label>(2)</label></formula><p>where four independent 1×1 convolutions α, β, θ, and γ are used to map image and fusion features to the space for cross-modal attention. At first, M and F are passed into θ(•), β(•) and α(•) for channel transformation, respectively. Following the transformed feature, M θ and F β perform multiplication via a Softmax activation function to take a cross-attention with F α . In the end, the output feature of the cross-attention via γ(•) for feature smoothing is added with F. Given the above operations, the cross-modal features O et and O dt are obtained and passed to the next layer.</p><p>Knowledge Alignment Module. The above operations only consider the interaction between features in two modalities. A problem is that text embeddings from pre-trained text encoder of CLIP are not well aligned with the image features initialized by ImageNet pre-training. This knowledge shift potentially weakens the proposed cross-modal interaction to assist the prediction of implant position. To mitigate this problem, we propose the knowledge alignment module (KAM) to gradually align image features to the feature space of pre-trained CLIP. Motivated by knowledge distillation <ref type="bibr" target="#b9">[10]</ref>, we formulate the proposed knowledge alignment as follows:</p><formula xml:id="formula_4">L align = |m e 4 -m clip |, (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where m e 4 ∈ R 1×D is the transformed feature of M e 4 after attention pooling operation <ref type="bibr" target="#b0">[1]</ref> and dimension reduction with convolution. m clip ∈ R 1×D is the image embedding extracted by the CLIP Image Encoder. Using this criteria, the encoder of TCEIP approximates the CLIP image encoder and consequently aligns the image features of the encoder with the CLIP text embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Heatmap Regression Network</head><p>The heatmap regression network is used for locating the implant position, which consists of the heatmap and the offset head. The output of the heatmap head is the center localization of implant position, which is formed as a heatmap G ∈ [0, 1] H×W . Following <ref type="bibr" target="#b4">[5]</ref>, given coordinate of the ground truth implant location ( tx , ty ), we apply a 2D Gaussian kernel to get the target heatmap:</p><formula xml:id="formula_6">G xy = exp(- (x -tx ) 2 + (y -ty ) 2 2σ 2 ) (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where σ is an object size-adaptive standard deviation. The predicted heatmap is optimized by the focal loss <ref type="bibr" target="#b5">[6]</ref>:</p><formula xml:id="formula_8">L h = -1 N xy (1 -Ĝxy ) λ log( Ĝxy ) i fG xy = 1 (1 -Ĝxy ) ϕ log( Ĝxy ) λ log(1 -Ĝxy ) otherwise<label>(5)</label></formula><p>where λ and ϕ are the hyper-parameters of the focal loss, Ĝ is the predicted heatmap and N is the number of implant annotation in image. The offset head computes the discretization error caused by the downsampling operation, which is used to further refine the predicted location. The local offset loss L o is optimized by the L1 loss. The overall training loss of network is:</p><formula xml:id="formula_9">L = L h + L o + L align (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Coordinate Projection</head><p>The output of TCEIP is the coordinate of implant at the tooth crown. To obtain the real implant location at the tooth root, we fit a centerline of implant using the predicted implant position of TCEIP and then extend the centerline to the root area, which is identical as <ref type="bibr" target="#b12">[13]</ref>. By this means, the intersections of implant centerline with 2D slices of root image, i.e. the implant position at the tooth root area, can be obtained. 3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>The dental implant dataset was collected by <ref type="bibr" target="#b12">[13]</ref>, which contains 3045 2D slices of tooth crown images. The implant position annotations are annotated by three experienced dentists. The input image size of network is set as 512 × 512. We use a batch size of 8, Adam optimizer and a learning rate of 0.001 for the network training. Total training epochs is 80 and the learning rate is divided by 10 when epoch = {40, 60}. The same data augmentation methods in <ref type="bibr" target="#b12">[13]</ref> was employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Analysis</head><p>We use the same evaluation criteria in <ref type="bibr" target="#b12">[13]</ref>, i.e., average precision (AP) to evaluate the performance of our network. As high accurate position prediction is required in clinical practice, the IOU threshold is set as 0.75. Five-fold crossvalidation was performed for all our experiments. Transformer-based ImplantFormer ViT-Base-ResNet-50 13.7 ± 0.2045 Deformable DETR <ref type="bibr" target="#b18">[19]</ref> 12.8 ± 0.1417 CNN-based CenterNet <ref type="bibr" target="#b17">[18]</ref> ResNet-50 10.9 ± 0.2457 ATSS <ref type="bibr" target="#b15">[16]</ref> 12.1 ± 0.2694 VFNet <ref type="bibr" target="#b14">[15]</ref> 11.8 ± 0.8734 RepPoints <ref type="bibr" target="#b13">[14]</ref> 11.2 ± 0.1858 TCEIP 17.8 ± 0.3956 Ablation Studies. To evaluate the effectiveness of the proposed network, we conduct ablation experiments to investigate the effect of each component in Table <ref type="table" target="#tab_1">1</ref>. We can observe from the second row of the table that the introduction of text condition improves the performance by 3.7%, demonstrating the validity of using text condition to assist the implant position prediction. When combining both text condition and KAM, the improvement reaches 4.8%. As shown in the table's last three rows, both feature fusion operation and CAM improve AP value by 1.4% and 0.8%, respectively. When combining all these components, the improvement reaches 6.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to the Mainstream Detectors.</head><p>To demonstrate the superior performance of the proposed TCEIP, we compare the AP value with the mainstream detectors in Table <ref type="table" target="#tab_2">2</ref>. Only the anchor-free detector is used for comparison, due to the reason that no useful texture is available around the center of the implant. As the teeth are missing, the anchor-based detectors can not regress the implant position successfully. From the table we can observe that, the transformer-based methods perform better than the CNN-based networks (e.g., ImplantFormer achieved 13.7% AP, which is 1.6% higher than the best-performed anchor-free network -ATSS). The proposed TCEIP achieves the best AP value -17.8%, among all benchmarks, which surpasses the Implant-Former with a large gap. The experimental results proved the effectiveness of our method. In Fig. <ref type="figure" target="#fig_4">4</ref>, we choose two best-performed detectors from the CNN-based (e.g., ATSS) and transformer-based (e.g., ImplantFormer) methods for visual comparison, to further demonstrate the superiority of TCEIP in the implant position prediction. The first row of the figure is a patient with sparse teeth, and the second and third rows are a patient with two missing teeth. We can observe from the figure that both the ATSS and ImplantFormer generate false positive detection, except for the TCEIP. Moreover, the implant position predicted by the TCEIP is more accurate. These visual results demonstrated the effectiveness of using text condition to assist the implant position prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we introduce TCEIP, a text condition embedded implant position regression network, which integrate additional condition from the CLIP to guide the prediction of implant position. A cross-modal attention (CMA) and knowledge alignment module (KAM) is devised to facilitate the interaction between features in two modalities. Extensive experiments on a dental implant dataset through five-fold cross-validation demonstrated that the proposed TCEIP achieves superior performance than the existing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) The 2D axial view of tooth crown images captured from different patients, where the pink and blue circles denote the implant and sparse teeth regions, respectively. (b) The matching score of the CLIP for a pair of image and text. (c) The division teeth region. (Color figure online)</figDesc><graphic coords="3,58,98,53,96,334,51,148,75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The network architecture of the proposed prediction network.</figDesc><graphic coords="4,44,79,99,11,334,60,168,19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>as the encoder of TCEIP. It mainly consists of four layers and each layer contains multiple residual blocks. Given a tooth crown image I r , a set of feature maps, i.e., {M e 1 , M e 2 , M e 3 , M e 4 }, can be accordingly extracted by the ResNet layers. Each feature map has a spatial and channel dimension. To ensure fine-grained heatmap regression, three deconvolution layers are adopted as the Decoder to recover high-resolution features. It consecutively upsamples feature map M e 4 as high-resolution feature representations, in which a set of recovered features {M d 1 , M d 2 , M d 3 } can be extracted. Feature maps M e 1 , M e 4 and M d 3 will be further employed in the proposed modules, where M e 1 and M d 3 have the same spatial dimension R 128×128×C and M e 4 ∈ R 16×16× Ĉ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The architecture of the proposed cross-modal attention module.</figDesc><graphic coords="5,58,98,112,28,334,42,117,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visual comparison of the predicted results with different detectors. The yellow and red circles represent the predicted implant position and ground-truth position, respectively. The blue ellipses denote false positive detections. (Color figure online)</figDesc><graphic coords="7,58,98,165,32,334,60,240,70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>To enable the transformed text embedding V t better interact with intermediate features of the encoder and decoder, we design and plug a cross-modal attention (CMA) module into the shallow layers of the encoder and the final deconvolution layer. The architecture of CMA is illustrated in Fig.3. Specifically, the CMA module creates cross-attention between image features M e 1 and fusion featureF et = [M e 1 |Vt ] in the encoder, and image features M d 3 and fusion feature F dt = [M d 3 |V t ] in the decoder, where</figDesc><table /><note><p>Module.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The ablation experiments of each components in TCEIP.</figDesc><table><row><cell cols="2">Network KAM Text Condition Feature Fusion CMA AP75%</cell></row><row><cell>TCEIP</cell><cell>10.9 ± 0.2457</cell></row><row><cell></cell><cell>14.6 ± 0.4151</cell></row><row><cell></cell><cell>15.7 ± 0.3524</cell></row><row><cell></cell><cell>16.5 ± 0.3891</cell></row><row><cell></cell><cell>17.1 ± 0.2958</cell></row><row><cell></cell><cell>17.8 ± 0.3956</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the proposed method with other mainstream detectors.</figDesc><table><row><cell>Methods</cell><cell>Network</cell><cell>Backbone</cell><cell>AP75%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">82261138629</rs>; <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> under Grant <rs type="grantNumber">2023A1515010688</rs> and <rs type="grantNumber">2021A1515220072</rs>; <rs type="funder">Shenzhen Municipal Science and Technology Innovation Council</rs> under Grant <rs type="grantNumber">JCYJ2022053110141 2030</rs> and <rs type="grantNumber">JCYJ20220530155811025</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pjBjACE">
					<idno type="grant-number">82261138629</idno>
				</org>
				<org type="funding" xml:id="_Zy8aDWb">
					<idno type="grant-number">2023A1515010688</idno>
				</org>
				<org type="funding" xml:id="_7eVeSW5">
					<idno type="grant-number">2021A1515220072</idno>
				</org>
				<org type="funding" xml:id="_UCGQYFu">
					<idno type="grant-number">JCYJ2022053110141 2030</idno>
				</org>
				<org type="funding" xml:id="_B9HsCjD">
					<idno type="grant-number">JCYJ20220530155811025</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 31.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Trends in dental implant use in the US, 1999-2016, and projections to 2026</title>
		<author>
			<persName><forename type="first">H</forename><surname>Elani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Starr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gallucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Dent. Res</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1424" to="1430" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A deep learning approach for dental implant planning in cone-beam computed tomography images</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bayrakdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cornernet: detecting objects as paired keypoints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transfer learning via artificial intelligence for guiding implant placement in the posterior mandible: an in vitro study</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global prevalence of periodontal disease and lack of its surveillance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nazir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Al-Khalifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alhareky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gaffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Almas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. World J</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Khattak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.03482</idno>
		<title level="m">Bridging the gap between object and image-level representations for open-vocabulary detection</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dental-yolo: alveolar bone and mandibular canal detection on cone beam computed tomography images for dental implant planning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Widiasri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="101483" to="101494" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clims: cross language image matching for weakly supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4483" to="4492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">ImplantFormer: vision transformer based implant position regression using dental CBCT data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.16467</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RepPoints: point set representation for object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9657" to="9666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">VarifocalNet: an IoU-aware dense object detector</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sunderhauf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8514" to="8523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting twentythousand classes using image-level supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1363</biblScope>
			<biblScope unit="page" from="350" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<title level="m">Deformable DETR: deformable transformers for end-to-end object detection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
