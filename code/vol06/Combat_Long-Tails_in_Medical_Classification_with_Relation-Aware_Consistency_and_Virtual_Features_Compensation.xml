<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation</title>
				<funder ref="#_pWZpw64">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Li</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Sha Tin, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yupei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">The Centre for Intelligent Multidimensional Data Analysis (CIMDA)</orgName>
								<address>
									<addrLine>Pak Shek Kok</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiushi</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tan</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The Hang Seng University of Hong Kong</orgName>
								<address>
									<settlement>Sha Tin, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zhen</forename><surname>Chen</surname></persName>
							<email>zhen.chen@cair-cas.org.hk</email>
							<affiliation key="aff4">
								<orgName type="department">Centre for Artificial Intelligence and Robotics (CAIR)</orgName>
								<orgName type="institution">HKISI-CAS</orgName>
								<address>
									<settlement>Pak Shek Kok, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="14" to="23"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">08C6E140481D22EE82DB7AE820563498</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Class Imbalance</term>
					<term>Dermoscopy</term>
					<term>Representation Learning L. Pan and Y. Zhang-Equal contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning techniques have achieved promising performance for computer-aided diagnosis, which is beneficial to alleviate the workload of clinicians. However, due to the scarcity of diseased samples, medical image datasets suffer from an inherent imbalance, and lead diagnostic algorithms biased to majority categories. This degrades the diagnostic performance, especially in recognizing rare categories. Existing works formulate this challenge as long-tails and adopt decoupling strategies to mitigate the effect of the biased classifier. But these works only use the imbalanced dataset to train the encoder and resample data to re-train the classifier by discarding the samples of head categories, thereby restricting the diagnostic performance. To address these problems, we propose a Multi-view Relation-aware Consistency and Virtual Features Compensation (MRC-VFC) framework for long-tailed medical image classification in two stages. In the first stage, we devise a Multi-view Relation-aware Consistency (MRC) for representation learning, which provides the training of encoders with unbiased guidance in addition to the imbalanced supervision. In the second stage, to produce an impartial classifier, we propose the Virtual Features Compensation (VFC) to recalibrate the classifier by generating massive balanced virtual features. Compared with the resampling, VFC compensates the minority classes to optimize an unbiased classifier with preserving complete knowledge of the majority ones. Extensive experiments on two long-tailed public benchmarks confirm that our MRC-VFC framework remarkably outperforms state-of-the-art algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed the great success of deep learning techniques in various applications on computer-aided diagnosis <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref>. However, the challenge of class imbalance inherently exists in medical datasets due to the scarcity of target diseases <ref type="bibr" target="#b6">[7]</ref>, where normal samples are significantly more than diseased samples. This challenge leads the model training biased to the majority categories <ref type="bibr" target="#b12">[13]</ref> and severely impairs the performance of diagnostic models in real-world scenarios <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. Therefore, it is urgent to improve the performance of diagnostic models in clinical applications, especially to achieve balanced recognition of minority categories.</p><p>Technically, the issue of class imbalance is formulated as a long-tailed problem in existing works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref>, where a few head classes contain numerous samples while the tail classes comprise only a few instances <ref type="bibr" target="#b27">[28]</ref>. To address this issue, most of the previous methods have typically attempted to rebalance the data distribution through under-sampling the head classes <ref type="bibr" target="#b1">[2]</ref>, over-sampling the tail classes <ref type="bibr" target="#b20">[21]</ref>, or reweighting the contribution of different classes during the optimization process <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. Nevertheless, these resampling methods can encounter a decrease in performance on certain datasets since the total information volume of the dataset is either unchanged or even reduced <ref type="bibr" target="#b28">[29]</ref>. Recent advantages in longtailed medical image classification have been achieved by two-stage methods, which first train the model on the entire dataset and then fine-tune the classifier in the second stage using rebalancing techniques to counteract the class imbalance <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref>. By decoupling the training of encoders and classifiers, the two-stage methods can recalibrate the biased classifiers and utilize all of the training samples to enhance representation learning for the encoder.</p><p>Although the aforementioned decoupling methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> have somewhat alleviated the long-tails, the classification performance degradation in the minority classes remains unsolved, which can be attributed to two challenges. First, in the first stage, the decoupling methods train the model on the imbalanced dataset, which is insufficient for representation learning in the rare classes due to the scarcity of samples <ref type="bibr" target="#b16">[17]</ref>. To this end, improving the first-stage training strategy to render effective supervision on representation learning is in great demand. The second problem lies in the second stage, where decoupling methods freeze the pre-trained encoder and fine-tune the classifier <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Traditional rebalancing techniques, such as resampling and reweighting, are used by the decoupling methods to eliminate the bias in the classifier. However, these rebalancing strategies have intrinsic drawbacks, e.g., resampling-based methods discard the samples of head classes, and reweighting cannot eliminate the imbalance with simple coefficients <ref type="bibr" target="#b25">[26]</ref>. Thus, a novel approach that can perform balanced classifier training by generating abundant features is desired to recalibrate the classifier and preserve the representation quality of the encoder.</p><p>To address the above two challenges, we propose the MRC-VFC framework that adopts the decoupling strategy to enhance the first-stage representation learning with Multi-view Relation-aware Consistency (MRC) and recalibrate the classifier using Virtual Features Compensation (VFC). Specifically, in the first stage, to boost the representation learning under limited samples, we build a twostream architecture to perform representation learning with the MRC module, which encourages the model to capture semantic information from images under different data perturbations. In the second stage, to recalibrate the classifier, we propose to generate virtual features from multivariate Gaussian distribution with the expectation-maximization algorithm, which can compensate for tail classes and preserves the correlations among features. In this way, the proposed MRC-VFC framework can rectify the biases in the encoder and classifier, and construct a balanced and representative feature space to improve the performance for rare diseases. Experiments on two public dermoscopic datasets prove that our MRC-VFC framework outperforms state-of-the-art methods for long-tailed diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, our MRC-VFC framework follows the decoupling strategy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref> to combat the long-tailed challenges in two stages. In the first stage, we introduce the Multi-view Relation-aware Consistency (MRC) to boost representation learning for the encoder g. In the second stage, the proposed Virtual Features Compensation (VFC) recalibrates the classifier f by generating massive balanced virtual features, which compensates the tails classes without dropping the samples of the head classes. By enhancing the encoder with MRC and recalibrating the classifier with VFC, our MRC-VFC framework can perform effective and balanced training on long-tailed medical datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-view Relation-Aware Consistency</head><p>The representation learning towards the decoupling models is insufficient <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. To boost the representation learning, we propose the Multi-view Relationaware Consistency to encourage the encoder to apprehend the inherent semantic features of the input images under different data augmentations. Specifically, we build a student neural network f • g for the strong augmented input x s and duplicate a teacher model f • g for the weak augmented input x w . The two models are constrained by the MRC module to promote the consistency for different perturbations of the same input. The parameters of the teacher model are updated via an exponential moving average of the student parameters <ref type="bibr" target="#b23">[24]</ref>.</p><p>To motivate the student model to learn from the data representations but the ill distributions, we propose multi-view constraints on the consistency of two models at various phases. A straightforward solution is to encourage identical predictions for different augmentations of the same input image, as follows:</p><formula xml:id="formula_0">L prob = 1 B KL(f • g(x s ), f • g (x w )),<label>(1)</label></formula><p>where KL(•, •) refers to the Kullback-Leibler divergence to measure the difference between two outputs. As this loss function calculates the variance of classifier output, the supervision for the encoders is less effective. To this end, the proposed MRC measures the sample-wise and channel-wise similarity between the feature maps of two encoders to regularize the consistency of the encoders. We first define the correlations of individuals and feature channels as indicates the similarities across feature channels. Thus, the consistency between the feature maps of two models can be defined as:</p><formula xml:id="formula_1">S b (z) = z • z and S c (z) = z • z, where z = g(x s ) ∈ R B×C is</formula><formula xml:id="formula_2">L batch = 1 B ||S b (g(x s )) -S b (g (x w ))|| 2 , (<label>2</label></formula><formula xml:id="formula_3">)</formula><formula xml:id="formula_4">L channel = 1 C ||S c (g(x s )) -S c (g (x w ))|| 2 . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>Furthermore, we also adopt the cross-entropy loss</p><formula xml:id="formula_6">L CE = 1 B L(f • g(x w ), y)</formula><p>, where y denotes the ground truth, between the predictions and ground truth to ensure that the optimization will not be misled to a trivial solution. The overall loss function is summarized as</p><formula xml:id="formula_7">L stage1 = L CE + λ 1 L batch + λ 2 L channel + λ 3 L prob ,</formula><p>where λ 1 , λ 2 and λ 3 are coefficients to control the trade-off of each loss term. By introducing extra semantic constraints, the MRC can enhance the representation capacity of encoders. The feature space generated by the encoders is more balanced with abundant semantics, thereby facilitating the MRC-VFC framework to combat long-tails in medical diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Virtual Features Compensation</head><p>Recalling the introduction of decoupling methods, the two-stage methods <ref type="bibr" target="#b13">[14]</ref> decouple the training of the encoder and classifier to eliminate the bias in the classifier while retaining the representation learning of the encoder. However, most existing decoupling approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> employ the resampling strategy in the second stage to rebalance the data class distribution, causing the intrinsic drawbacks of the resampling of discarding the head class samples. To handle this issue, we propose Virtual Features Compensation, which generates virtual features z k ∈ R N k ×C for each class k under multivariate Gaussian distribution <ref type="bibr" target="#b0">[1]</ref> to combat the long-tailed problem. Different from existing resampling methods <ref type="bibr" target="#b1">[2]</ref>, the feature vectors produced by the VFC module preserve the correlations among classes and the semantic information from the encoder. Given the k-th class, we first calculate the class-wise Gaussian distribution with mean μ k and covariance Σ k , as follows:</p><formula xml:id="formula_8">μ k = 1 N k x∈X k g I (x), Σ k = 1 N k -1 x∈X k (x -μ k ) (x -μ k ),<label>(4)</label></formula><p>where X k denotes the set of all samples in the k-th class, and g I (•) denotes the encoder trained in the first stage on the imbalanced dataset and N k is the sample number of the k-th class. We then randomly sample R feature vectors for each category from the corresponding Gaussian distribution N (μ k , Σ k ) to build the unbiased feature space, as {V k ∈ R R×C } K k=1 . We re-initialize the classifier and then calibrated it under cross-entropy loss, as follows:</p><formula xml:id="formula_9">L M stage2 = 1 RK K k=1 v i ∈V k L CE (f (v i ), y), (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where K is the number of categories in the dataset. As the Gaussian distribution is calculated according to the statistics from the first-stage feature space, to further alleviate the potential bias, we employ the expectation-maximization algorithm <ref type="bibr" target="#b19">[20]</ref> to iteratively fine-tune the classifier and encoder. At the expectation step, we freeze the classifier and supervise the encoder with extra balancing constraints to avoid being re-contaminated by the long-tailed label space. Thus, we adopt the generalized cross-entropy (GCE) loss <ref type="bibr" target="#b29">[30]</ref> for the expectation step as follows:</p><formula xml:id="formula_11">L E stage2 = 1 N x∈X (1 -(f • g I (x)y) q ) q , (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>where q is a hyper-parameter to control the trade-off between the imbalance calibration and the classification task. At the maximization step, we freeze the encoder and train the classifier on the impartial feature space. By enriching the semantic features with balanced virtual features, our MRC-VFC framework can improve the classification performance in long-tailed datasets, especially the performance of minority categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>To evaluate the performance on long-tailed medical image classification, we construct two dermatology datasets from ISIC<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b24">[25]</ref> following <ref type="bibr" target="#b11">[12]</ref>. In particular, we construct the ISIC-2019-LT dataset as the long-tailed version of ISIC 2019 challenge<ref type="foot" target="#foot_1">2</ref> , which includes 8 diagnostic categories of dermoscopic images. We sample the subset from Pareto distribution <ref type="bibr" target="#b7">[8]</ref> as</p><formula xml:id="formula_13">N c = N 0 (r -(k-1) ) c</formula><p>, where the imbalance factor r = N 0 /N k-1 is defined as the sample number of the head class N 0 divided by the tail one N k-1 . We adopt three imbalance factors for ISIC-2019-LT, as r = {100, 200, 500}. Furthermore, the ISIC-Archive-LT dataset <ref type="bibr" target="#b11">[12]</ref> is sampled from ISIC Archive with a larger imbalance factor r ≈ 1000 and contains dermoscopic images of 14 classes. We randomly split these two datasets into train, validation and test sets as 7:1:2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We implement the proposed MRC-VFC framework with the PyTorch library <ref type="bibr" target="#b21">[22]</ref>, and employ ResNet-18 <ref type="bibr" target="#b10">[11]</ref> as the encoder for both long-tailed datasets. All the experiments are done on four NVIDIA GeForce GTX 1080 Ti GPUs with a batch size of 128. All images are resized to 224 × 224 pixels. In the first stage of MRC-VFC, we train the model using Stochastic Gradient Descent (SGD) with a learning rate of 0.01. For the strong augmentation <ref type="bibr" target="#b2">[3]</ref>, we utilize the random flip, blur, rotate, distortion, color jitter, grid dropout, and normalization, and adopt the random flip and the same normalization for the weak augmentation. In the second stage, we use SGD with a learning rate of 1 × 10 -5 for optimizing the classifier and 1 × 10 -6 for optimizing the encoder, respectively. The loss weights λ 1 , λ 2 and λ 3 in the first stage are set as 10, 10 and 5, and the q in the second stage is set as 0.8. We set training epochs as 100 for the first stage and 500 for the second stage. The source code is available at https://github.com/jhonP-Li/ MRC VFC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison on ISIC-2019-LT Dataset</head><p>We evaluate the performance of our MRC-VFC framework with state-of-the-art methods for long-tailed medical image classification, including (i) baselines: finetuning classification models with cross-entropy loss (CE), random data resampling methods (RS), and MixUp <ref type="bibr" target="#b26">[27]</ref>; (ii) recent loss reweighting methods: Generalized Cross-Entropy with Sparse Regularization (GCE+SR) <ref type="bibr" target="#b31">[32]</ref>, Seesaw loss <ref type="bibr" target="#b25">[26]</ref>, focal loss <ref type="bibr" target="#b15">[16]</ref>, and Class-Balancing (CB) loss <ref type="bibr" target="#b7">[8]</ref>; (iii) recent works for longtailed medical image classification: Flat-aware Cross-stage Distillation (FCD) <ref type="bibr" target="#b14">[15]</ref>, and Flexible Sampling (FS) <ref type="bibr" target="#b11">[12]</ref>. As illustrated in Table <ref type="table" target="#tab_0">1</ref>, we compare our MRC-VFC framework with the aforementioned methods on the ISIC-2019-LT dataset under different imbalance factors. Among these methods, our MRC-VFC framework achieves the best performance with an accuracy of 77.41%, 75.98%, and 74.62% under the imbalance factor of 100, 200, and 500, respectively. Noticeably, compared with the state-ofthe-art decoupling method FCD <ref type="bibr" target="#b14">[15]</ref> on long-tailed medical image classification, our MRC-VFC framework surpasses it by a large margin of 11.03% accuracy when the imbalance factor is 500, demonstrating the effectiveness of representation learning and virtual features compensation in our framework. Furthermore, our MRC-VFC framework outperforms FS <ref type="bibr" target="#b11">[12]</ref>, which improves the resampling strategy and achieves the best performance on the ISIC-2019-LT dataset, with an accuracy increase of 9.4% under imbalance factor = 500. These experimental results demonstrate the superiority of our MRC-VFC framework over existing approaches in long-tailed medical image classification tasks. Ablation Study. We perform the ablation study to validate the effectiveness of our proposed MRC and VFC modules on two long-tailed datasets. As shown in Table <ref type="table" target="#tab_0">1</ref> and 2, both MRC and VFC modules remarkably improve the performance over the baselines. In particular, we apply two ablative baselines of the proposed MRC-VFC framework by disabling the MRC (denoted as Ours w/o MRC) and the VFC (denoted as Ours w/o VFC) individually. In detail, as shown in Table <ref type="table" target="#tab_0">1</ref>, when the imbalance factor is 500, the accuracy increases by 4.49% and 7.14% for MRC and VFC, respectively. In addition, as illustrated in Table <ref type="table" target="#tab_1">2</ref>, the mean accuracy of all classes in the ISIC-Archive-LT shows an improvement of 2.40% and 2.92% for MRC and VFC correspondingly. The ablation study verifies the effectiveness of our MRC and VFC modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison on ISIC-Archive-LT Dataset</head><p>To comprehensively evaluate our MRC-VFC framework, we further perform the comparison with state-of-the-art algorithms on a more challenging ISIC-Archive-LT dataset for long-tailed diagnosis. As illustrated in Table <ref type="table" target="#tab_1">2</ref>, our MRC-VFC framework achieves the best overall performance with an accuracy of 67.84% among state-of-the-art algorithms, and results in a balanced performance over different classes, i.e., 69.71% for head classes and 70.34% for tail classes. Compared with the advanced decoupling method <ref type="bibr" target="#b14">[15]</ref> for medical image diagnosis, our MRC-VFC framework significantly improves the accuracy with 4.73% in medium classes and 8.87% in tail classes, respectively.</p><p>Performance Analysis on Head/Tail Classes. We further present the performance of several head and tail classes in Fig. <ref type="figure" target="#fig_2">2</ref>. Our MRC-VFC framework outperforms FS <ref type="bibr" target="#b11">[12]</ref> on both tail and head classes, and significantly promotes the performance of tail classes, thereby effectively alleviating the affect of long-tailed problems on medical image diagnosis. These comparisons confirm the advantage of our MRC-VFC framework in more challenging long-tailed scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>To address the long-tails in computer-aided diagnosis, we propose the MRC-VFC framework to improve medical image classification with balanced perfor-mance in two stages. In the first stage, we design the MRC to facilitate the representation learning of the encoder by introducing multi-view relation-aware consistency. In the second stage, to recalibrate the classifier, we propose the VFC to train an unbias classifier for the MRC-VFC framework by generating massive virtual features. Extensive experiments on the two long-tailed dermatology datasets demonstrate the effectiveness of the proposed MRC-VFC framework, which outperforms state-of-the-art algorithms remarkably.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The MRC-VFC framework. In stage 1, we perform the representation learning with the MRC module for the encoder on the imbalanced dataset. In stage 2, we recalibrate the classifier with VFC in two-step of the expectation and maximization.</figDesc><graphic coords="3,61,02,79,19,300,22,111,31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the output features of the encoder, and B and C are the batch size and channel number. S b (z) denotes the Gram matrix of feature z, representing the correlations among individuals, and S c (z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The performance comparison of head/tail classes in ISIC-Archive-LT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art algorithms on the ISIC-2019-LT dataset.</figDesc><table><row><cell>ISIC-2019-LT</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">Acc (%) @ Factor = 100 Acc (%) @ Factor = 200 Acc (%) @ Factor = 500</cell></row><row><cell>CE</cell><cell>56.91</cell><cell>53.77</cell><cell>43.89</cell></row><row><cell>RS</cell><cell>61.41</cell><cell>55.12</cell><cell>47.76</cell></row><row><cell>MixUp [27]</cell><cell>59.85</cell><cell>54.23</cell><cell>43.11</cell></row><row><cell cols="2">GCE+SR [32] 64.57</cell><cell>58.28</cell><cell>54.36</cell></row><row><cell cols="2">Seesaw loss [26] 68.82</cell><cell>65.84</cell><cell>62.92</cell></row><row><cell cols="2">Focal loss [16] 67.54</cell><cell>65.93</cell><cell>61.66</cell></row><row><cell>CB loss [8]</cell><cell>67.54</cell><cell>66.70</cell><cell>61.89</cell></row><row><cell>FCD [15]</cell><cell>70.15</cell><cell>68.82</cell><cell>63.59</cell></row><row><cell>FS [12]</cell><cell>71.97</cell><cell>69.30</cell><cell>65.22</cell></row><row><cell cols="2">Ours w/o MRC 75.04</cell><cell>73.13</cell><cell>70.13</cell></row><row><cell cols="2">Ours w/o VFC 72.91</cell><cell>71.07</cell><cell>67.48</cell></row><row><cell>Ours</cell><cell>77.41</cell><cell>75.98</cell><cell>74.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with state-of-the-art algorithms on the ISIC-Archive-LT dataset.</figDesc><table><row><cell>ISIC-Archive-LT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">Head (Acc%) Medium (Acc%) Tail (Acc%) All (Acc%)</cell></row><row><cell>CE</cell><cell>71.31</cell><cell>49.22</cell><cell>38.17</cell><cell>52.90</cell></row><row><cell>RS</cell><cell>70.17</cell><cell>55.29</cell><cell>34.29</cell><cell>53.25</cell></row><row><cell cols="2">GCE+SR [32] 64.93</cell><cell>57.26</cell><cell>38.22</cell><cell>53.47</cell></row><row><cell cols="2">Seesaw loss [26] 70.26</cell><cell>55.98</cell><cell>42.14</cell><cell>59.46</cell></row><row><cell cols="2">Focal loss [16] 69.57</cell><cell>56.21</cell><cell>39.65</cell><cell>57.81</cell></row><row><cell>CB loss [8]</cell><cell>64.98</cell><cell>57.01</cell><cell>61.61</cell><cell>61.20</cell></row><row><cell>FCD [15]</cell><cell>66.39</cell><cell>61.17</cell><cell>60.54</cell><cell>62.70</cell></row><row><cell>FS [12]</cell><cell>68.69</cell><cell>58.74</cell><cell>64.48</cell><cell>63.97</cell></row><row><cell cols="2">Ours w/o MRC 69.06</cell><cell>62.14</cell><cell>65.12</cell><cell>65.44</cell></row><row><cell cols="2">Ours w/o VFC 65.11</cell><cell>62.35</cell><cell>67.30</cell><cell>64.92</cell></row><row><cell>Ours</cell><cell>69.71</cell><cell>63.47</cell><cell>70.34</cell><cell>67.84</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.isic-archive.com/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://challenge.isic-archive.com/landing/2019/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported in part by the <rs type="programName">InnoHK program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pWZpw64">
					<orgName type="program" subtype="full">InnoHK program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The multivariate gaussian probability distribution</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ahrendt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">203</biblScope>
		</imprint>
		<respStmt>
			<orgName>Technical University of Denmark</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Albumentations: fast and flexible image augmentations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Super-resolution enhanced medical image diagnosis with sample affinity interaction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1377" to="1389" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint spatial-wavelet dualstream network for super-resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ibragimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_18</idno>
		<idno>978-3-030-59722-1 18</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="184" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Personalized retrogress-resilient federated learning toward imbalanced medical data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3663" to="3674" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clinically applicable deep learning for diagnosis and referral in retinal disease</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1342" to="1350" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="issue">7639</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flexible sampling for long-tailed skin lesion classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ju</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_44</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page">44</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring balanced feature spaces for representation learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flat-aware cross-stage distilled framework for imbalanced medical image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-821" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep representation learning on longtailed data: a learnable embedding augmentation perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">No fear of heterogeneity: classifier calibration for federated learning with non-IID data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5972" to="5984" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The expectation-maximization algorithm</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Process. Mag</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Survey of resampling techniques for improving classification performance in unbalanced datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>More</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06048</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep neural network models for computational histopathology: a survey</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Srinidhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ciga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101813</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04596</idno>
		<title level="m">Deep long-tailed learning: a survey</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bag of tricks for long-tailed visual recognition with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3447" to="3455" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">BBN: bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9719" to="9728" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning with noisy labels via sparse regularization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="72" to="81" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
