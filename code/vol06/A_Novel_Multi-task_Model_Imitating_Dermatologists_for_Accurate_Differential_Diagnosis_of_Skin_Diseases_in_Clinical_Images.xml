<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images</title>
				<funder ref="#_rsKhKvS">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_Ew6dvkB">
					<orgName type="full">Ministry of Industry and Information Technology of the People&apos;s Republic of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yan-Jie</forename><surname>Zhou</surname></persName>
							<email>zhouyanjie.zyj@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuping</forename><surname>Duan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<orgName type="institution">Beijing Normal University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Sir Run Run Shaw Hospital</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Na</forename><surname>Jin</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Sir Run Run Shaw Hospital</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyong</forename><surname>Man</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">The Second Affiliated Hospital Zhejiang University School of Medicine</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Xiangya Hospital Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="202" to="212"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B8E91BC2B632440FDD86E33D3FCEDF2C</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_20</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Skin disease</term>
					<term>Multi-task learning</term>
					<term>Vision transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skin diseases are among the most prevalent health issues, and accurate computer-aided diagnosis methods are of importance for both dermatologists and patients. However, most of the existing methods overlook the essential domain knowledge required for skin disease diagnosis. A novel multi-task model, namely DermImitFormer, is proposed to fill this gap by imitating dermatologists' diagnostic procedures and strategies. Through multi-task learning, the model simultaneously predicts body parts and lesion attributes in addition to the disease itself, enhancing diagnosis accuracy and improving diagnosis interpretability. The designed lesion selection module mimics dermatologists' zoom-in action, effectively highlighting the local lesion features from noisy backgrounds. Additionally, the presented cross-interaction module explicitly models the complicated diagnostic reasoning between body parts, lesion attributes, and diseases. To provide a more robust evaluation of the proposed method, a large-scale clinical image dataset of skin diseases with significantly more cases than existing datasets has been established. Extensive experiments on three different datasets consistently demonstrate the state-of-the-art recognition performance of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As the largest organ in the human body, the skin is an important barrier protecting the internal organs and tissues from harmful external substances, such as sun exposure, pollution, and microorganisms <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b11">10]</ref>. In recent years, the increasing number of deaths by skin diseases has aroused widespread public concern <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b18">17]</ref>. Due to the complexity of skin diseases and the shortage of dermatological expertise resources, developing an automatic and accurate skin disease diagnosis framework is of great necessity.</p><p>Among non-invasive skin imaging techniques, dermoscopy is currently widely used in the diagnosis of many skin diseases <ref type="bibr">[1,</ref><ref type="bibr" target="#b8">7]</ref>, but it is technically demanding and not necessary for many common skin diseases. Clinical images, on the contrary, can be easily acquired through consumer-grade cameras, increasingly utilized in teledermatology, but their diagnostic value is underestimated. Recently, deep learning-based methods have received great attention in clinical skin disease image recognition and achieved promising results <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b6">5,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b27">26]</ref>. Sun et al. <ref type="bibr" target="#b19">[18]</ref> released a clinical image dataset of skin diseases, namely SD-198, containing 6,584 images from 198 different categories. The results demonstrate that deep features from convolutional neural networks (CNNs) outperform handcrafted features in exploiting structural and semantic information. Gupta et al. <ref type="bibr" target="#b6">[5]</ref> proposed a dual stream network that employs class activation maps to localize discriminative regions of the skin disease and exploit local features from detected regions to improve classification performance.</p><p>Although these approaches have achieved impressive results, most of them neglect the domain knowledge of dermatology and lack interpretability in diagnosis basis and results. In a typical inspection, dermatologists give an initial evaluation with the consideration of both global information, e.g. body part, and local information, e.g. the attributes of skin lesions, and further information including the patient's medical history or additional examination is required to draw a diagnostic conclusion from several possible skin diseases. Recognizing skin diseases from clinical images presents various challenges that can be summarized as follows: (1) Clinical images taken by portable electronic devices (e.g. mobile phones) often have cluttered backgrounds, posing difficulty in accurately locating lesions. (2) Skin diseases exhibit high intra-class variability in lesion appearance, but low inter-class variability, thereby making discrimination challenging. (3) The diagnostic reasoning of dermatologists is empirical and complicated, which makes it hard to simulate and model.</p><p>To tackle the above issues and leverage the domain knowledge of dermatology, we propose a novel multi-task model, namely DermImitFormer. The model is designed to imitate the diagnostic process of dermatologists (as shown in Fig. <ref type="figure" target="#fig_0">1</ref>), by employing three distinct modules or strategies. Firstly, the multi-task learning strategy provides extra body parts and lesion attributes predictions, which enhances the differential diagnosis accuracy with the additional correlation from multiple predictions and improves the interpretability of diagnosis with more supporting information. Secondly, a lesion selection module is designed to imitate dermatologists' zoom-in action, effectively highlighting the local lesion features from noisy backgrounds. Thirdly, a cross-interaction module explicitly models the complicated diagnostic reasoning between body parts, lesion attributes, and diseases, increasing the feature alignments and decreasing gradient conflicts from different tasks. Last but not least, we build a new dataset containing 57,246 clinical images. The dataset includes 49 most common skin diseases, covering 80% of the consultation scenarios, 15 body parts, and 27 lesion attributes, following the International League of Dermatological Societies (ILDS) guideline <ref type="bibr" target="#b14">[13]</ref>. The main contributions can be summarized as follows: (1) A novel multitask model DermImitFormer is proposed to imitate dermatologists' diagnostic processes, providing outputs of diseases, body parts, and lesion attributes for improved clinical interpretability and accuracy. (2) A lesion selection module is presented to encourage the model to learn more distinctive lesion features. A cross-interaction module is designed to effectively fuse three different feature representations. (3) A large-scale clinical image dataset of skin diseases is established, containing significantly more cases than existing datasets, and closer to the real data distribution of clinical routine. More importantly, our proposed approach achieves the leading recognition performance on three different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The architecture of the proposed multi-task model DermImitFormer is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. It takes the clinical image as input and outputs the classification results of skin diseases, body parts, and attributes in an end-to-end manner. During diagnostic processes, dermatologists consider local and global contextual features of the entire clinical image, including shape, size, distribution, texture, location, etc. To effectively capture these visual features, we use the vision transformer (ViT) <ref type="bibr" target="#b5">[4]</ref> as the shared backbone. Three separate task-specific heads are then utilized to predict diseases, body parts, and attributes, respectively, with each head containing two independent ViT layers. In particular, in the task-specific heads of diseases and attributes, the extracted features of each layer are separated into the image features and the patch features. These two groups of features are fed into the lesion selection module (LSM), to select the most informative lesion tokens. Finally, the feature representations of diseases, body parts, and attributes are delivered to the cross-interaction module (CIM) to generate a more comprehensive representation for the final differential diagnosis. Shared Backbone. Following the ViT model, an input image X is divided to N p squared patches {x n , n ∈ {1, 2, ..., N p }}, where N p = (H × W )/P 2 , P is the side length of a squared patch, H and W are the height and width of the image, respectively. Then, the patches are flattened and linearly projected into patch tokens with a learnable position embedding, denoted as t n , n ∈ {1, 2, ..., N p }. Together with an extra class token t 0 , the network inputs are represented as t n ∈ R D , n ∈ {0, 1, ..., N p } with a dimension of D. Finally, the tokens are fed to L consecutive transformer layers to obtain the preliminary image features.</p><p>Lesion Selection Module. As introduced above, skin diseases have high variability in lesion appearance and distribution. Thus, it requires the model to concentrate on lesion patches so as to describe the attributes and associated diseases precisely. The multi-head self-attention (MHSA) block in ViT generates global attention, weighing the informativeness of each token. Inspired by <ref type="bibr" target="#b20">[19]</ref>, we introduce a lesion selection module (LSM), which guides the transformer encoder to select the tokens that are most relevant to lesions at different levels. Specifically, for each attention head in MHSA blocks, we compute the attention matrix Np+1) , where m ∈ {1, 2, ..., N h }, N h denoting the number of heads, Q and K the Query and Key representations of the block inputs, respectively. The first row calculates the similarities between the class token and each patch token. As the class token is utilized for classification, the higher the value, the more informative each token is. We apply softmax to the first row and the first column of A m , denoted as a m 0,n and a m n,0 , n ∈ {1, 2, ..., N p }, representing the attention scores between the class token and other tokens:</p><formula xml:id="formula_0">A m = Softmax(QK T / √ D) ∈ R (Np+1)×(</formula><formula xml:id="formula_1">a m 0,n = e A m 0,n Np i=1 e A m 0,i , a m n,0 = e A m n,0 Np i=1 e A m i,0 , s n = 1 N h N h m=1 a m 0,n • a m n,0<label>(1)</label></formula><p>The mutual attention score s n is calculated across all attention heads. Thereafter, we select the top K tokens according to s n for two task heads as l k d and l k a , k ∈ {1, 2, ..., K}. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Interaction Module.</head><p>A diagnostic process of skin diseases takes multiple visual information into account, which is relatively complicated and difficult to model in an analytical way. Simple fusion operations such as concatenation are insufficient to simulate the diagnostic logic. Thus, partially inspired by <ref type="bibr" target="#b22">[21]</ref>, the CIM is designed to learn complicated correlations between disease, body part, and attribute. The detailed module schematic is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Firstly, the features of body-part and disease are integrated to enhance global representations by a cross-attention block. For example, the fusion between the class token of disease and patch tokens of body-part is:</p><formula xml:id="formula_2">z b = LN (GAP (p 1 b , p 2 b , ......, p Np b ))<label>(2)</label></formula><formula xml:id="formula_3">Q = LN (g D ) W Q BD , K = z b W K BD , V = z b W V BD (<label>3</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">g B D = LN (g D ) + linear(softmax( QK T F/N h )V)<label>(4)</label></formula><p>where g B , g D are the class token, p i b , p i d , i ∈ {1, 2, ..., N p } the corresponding patch tokens. GAP and LN denote the global average pooling and layer normalization, respectively. W Q BD , W K BD , W V BD ∈ R F ×F denote learnable parameters. F denotes the dimension of features. g D B is computed from the patch tokens of disease and the class token of body-part in the same fashion. Similarly, we can obtain the fused class tokens (g D A and g A D ) and the fused local class tokens (l D A and l A D ) between attribute and disease. Note that the disease class token g D is replaced by g B D in the later computations, and local class tokens l A and l D in Fig. <ref type="figure" target="#fig_2">3</ref> are generated by GAP on selected local patch tokens from LSM. Finally, these mutually enhanced features from CIM are concatenated together to generate more accurate predictions of diseases, body parts, and attributes.</p><p>Learning and Optimization. We argue that joint training can enhance the feature representation for each task. Thus, we define a multi-task loss as follows: </p><formula xml:id="formula_6">L x = - 1 N s Ns i=1 nx j=1 y ij log (p ij ), x ∈ {d, d } (5) L h = - 1 N s Ns i=1 n h j=1 y ij log (p ij ) + (1 -y ij ) log (1 -p ij ), h ∈ {a, b}<label>(6)</label></formula><formula xml:id="formula_7">L = L d + L d + L b + L a (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>where N s denotes the number of samples, n x , n h the number of classes for each task, and p ij , y ij the prediction and label, respectively. Notably, body parts and attributes are defined as multi-label classification tasks, optimized with the binary cross-entropy loss, as shown in Eq. 6. The correspondence of x and h is shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>Datasets. The proposed DermImitFormer is evaluated on three different clinical skin image datasets including an in-house dataset and two public benchmarks.</p><p>(1) Derm-49: We establish a large-scale clinical image dataset of skin diseases, collected from three cooperative hospitals and a teledermatology platform. The 57,246 images in the dataset were annotated with the diagnostic ground truth of skin disease, body parts, and lesions attributes from the patient records. We clean up the ground truth into 49 skin diseases, 15 body parts, and 27 lesion attributes following the ILDS guidelines <ref type="bibr" target="#b14">[13]</ref>.</p><p>(2) SD-198 <ref type="bibr" target="#b19">[18]</ref>: It is one of the largest publicly available datasets in this field containing 198 skin diseases and 6,584 clinical images collected through digital cameras or mobile phones. ( <ref type="formula" target="#formula_3">3</ref>) PAD-UFES-20 <ref type="bibr" target="#b16">[15]</ref>: The dataset contains 2,298 samples of 6 skin diseases.</p><p>Each sample contains a clinical image and a set of metadata with labels such as diseases and body parts.</p><p>Implementation Details. The DermImitFormer is initialized with the pretrained ViT-B/16 backbone and optimized with SGD method (initial learning rate 0.003, momentum 0.95, and weight decay 10 -5 ) for 100 epochs on 4 NVIDIA Tesla V100 GPUs with a batch size of 96. We define the input size i.e. H = W = 384 that produces a total of 576 spatial tokens i.e. N p = 576 for a ViT-B backbone. K = 24 in the LSM module. For data augmentation, we employed the Cutmix <ref type="bibr" target="#b25">[24]</ref> with a probability of 0.5 and Beta(0.3, 0.3) during optimization. We adopt precision, recall, F1-score, and accuracy as the evaluation metrics.    <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b16">15]</ref>, and transformer-based methods <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b13">12]</ref>, achieving the state-of-the-art classification performance. In particular, the performance of Der-mImitFormer is better than that of DermImitFormer-ST in Single-Task mode (w/o CIM), which further indicates the effectiveness of the multi-task learning strategy and CIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, DermImitFormer, a multi-task model, has been proposed to better utilize dermatologists' domain knowledge by mimicking their subjective diagnostic procedures. Extensive experiments demonstrate that our approach achieves state-of-the-art recognition performance in two public benchmarks and a largescale in-house dataset, which highlights the potential of our approach to be employed in real clinical environments and showcases the value of leveraging domain knowledge in the development of machine learning models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The relationship between dermatologists' diagnostic procedures and our proposed model (best viewed in color).</figDesc><graphic coords="2,58,98,53,66,334,63,105,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overall architecture of the multi-task imitation model (DermImitFormer) with shared backbone and task-specific heads.</figDesc><graphic coords="3,44,61,303,14,119,44,80,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Schematic of cross-interaction module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparative results on Derm-49 dataset. Red, blue, green, and purple fonts denote diseases on heads, faces, hands, and feet (best viewed in color).</figDesc><graphic coords="7,44,79,327,02,334,66,244,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation study for DermImitFormer on Derm-49 dataset. D, B, and A denote the task-specific head of diseases, body parts, and attributes, respectively.</figDesc><table><row><cell cols="3">Dimension LSM Fusion</cell><cell cols="2">F1-score (%)</cell><cell></cell><cell>Accuracy(%)</cell></row><row><cell></cell><cell></cell><cell cols="5">Concat CIM Disease Body part Attribute Disease</cell></row><row><cell>D</cell><cell></cell><cell></cell><cell>76.2</cell><cell>-</cell><cell>-</cell><cell>80.4</cell></row><row><cell>D</cell><cell>✓</cell><cell></cell><cell>77.8</cell><cell>-</cell><cell>-</cell><cell>82.0</cell></row><row><cell>D + B</cell><cell>✓</cell><cell>✓</cell><cell>78.1</cell><cell>85.0</cell><cell>-</cell><cell>82.4</cell></row><row><cell>D + A</cell><cell>✓</cell><cell>✓</cell><cell>78.4</cell><cell>-</cell><cell>68.7</cell><cell>82.6</cell></row><row><cell cols="2">D + B + A ✓</cell><cell>✓</cell><cell>79.1</cell><cell>85.1</cell><cell>69.0</cell><cell>82.9</cell></row><row><cell cols="2">D + B + A ✓</cell><cell>✓</cell><cell>79.5</cell><cell>85.9</cell><cell>70.4</cell><cell>83.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Ablation Study. The experiment is conducted based on ViT-B/16 and the results are reported in Table1. (1) LSM: Quantitative results demonstrate that the designed LSM yields 1.6% improvement in accuracy. Qualitative results are shown in Fig.4(a), which depicts the attention maps obtained from the last transformer layer. Without LSM, vision transformers would struggle of localizing lesions and produce noisy attention maps. With LSM, the attention maps are more discriminative and lesions are localized precisely, regardless of variations in terms of scale and distribution. (2) Multi-task learning: The models are trained with a shared backbone and different combinations of task-specific heads. The results show that multi-task learning (D+B+A) increases the F1score from 77.8 to 79.1. (3) CIM: Quantitative results show that the presented CIM can further improve the F1-score of diseases to 79.5. Notably, the p-value of 1.03e-05 (&lt;0.01) is calculated by comparing the results of 5-fold cross-validation with the baseline, illustrating the significance of our model. In particular, the representation with fused features of body parts and attributes can improve the recognition performance of diseases. As shown in Fig.4(b) and (c), statistics show that the classification performance of these diseases is improved by the multi-task learning strategy and CIM. For instance, rosacea and tinea versicolor share the same attributes of macule and papular, but rosacea typically affects the face. By fusing the representation of body parts, the F1-score of rosacea is increased by 4.5%. Similarly, our model improves the recognition accuracy of diseases with distinctive lesion attributes such as skin tags, urticaria, etc. Meanwhile, the extra information about body parts and attributes improves the interpretability of diagnoses.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison to state-of-the-art methods on Derm-49 dataset (top) and two public benchmarks: SD-198 dataset (mid), PAD-UFES-20 dataset (bottom). To evaluate the effectiveness of our proposed DermImitFormer, we conduct a comparison with various state-of-the-art methods on three different datasets. The results are reported in Table2. (1) Derm-49: Compared with other state-of-the-art approaches, our proposed DermImitFormer achieves the leading classification performance in our established dataset with the 5-fold cross-validation splits. (2) SD-198: Since the dataset does not contain labels of lesion attributes and body parts, the proposed DermImitFormer in Single-Task mode (w/o CIM) is implemented in the experiment. The result is based on the provided 5-fold cross-validation splits. Quantitative results in Table2(mid) demonstrate that our proposed DermImitFormer-ST achieves state-of-the-art classification performance. In contrast to other approaches, our model can precisely localize more discriminative lesion regions and thus has superior classification accuracy. (3) PAD-UFES-20: The dataset contains labels of diseases and body parts. Thus, the proposed DermImitFormer with different modes is evaluated in the experiment by the 5-fold cross-validation splits. Quantitative results in Table2(bottom) demonstrate that our proposed model outperforms the CNN-based</figDesc><table /><note><p>Results.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2020YFC2008703</rs>) and the Project of <rs type="projectName">Intelligent Management Software for Multimodal Medical Big Data for New Generation Information Technology</rs>, the <rs type="funder">Ministry of Industry and Information Technology of the People's Republic of China</rs> (<rs type="grantNumber">TC210804V</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_rsKhKvS">
					<idno type="grant-number">2020YFC2008703</idno>
					<orgName type="project" subtype="full">Intelligent Management Software for Multimodal Medical Big Data for New Generation Information Technology</orgName>
				</org>
				<org type="funding" xml:id="_Ew6dvkB">
					<idno type="grant-number">TC210804V</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aux-D</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T-Enc</forename></persName>
		</author>
		<idno>61.6 ± 5.1</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Epiluminescence microscopy: a useful tool for the diagnosis of pigmented skin lesions for formally trained dermatologists</title>
		<author>
			<persName><forename type="first">M</forename><surname>Binder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Dermatol</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="286" to="291" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">PCCT: progressive class-center triplet loss for imbalanced medical image classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.04793</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interactive attention sampling network for clinical skin disease image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-88010-1_33</idno>
		<idno>978-3-030-88010-1 33</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">PRCV 2021</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13021</biblScope>
			<biblScope unit="page" from="398" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dual stream network with selective optimization for skin disease recognition in consumer grade images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Narayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition (ICPR)</title>
		<meeting>the International Conference on Pattern Recognition (ICPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5262" to="5269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dermx: an end-to-end framework for explainable automated dermatological diagnosis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jalaboi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orbes-Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jørgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galimzianova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">102647</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diagnostic accuracy of dermoscopy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pehamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Binder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Oncol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning approaches for prognosis of automated skin disease</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Kshirsagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shitharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alshareef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Albishry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Balachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Life</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">426</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Class-center involved triplet loss for skin disease classification on imbalanced data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Biomedical Imaging (ISBI)</title>
		<meeting>the International Symposium on Biomedical Imaging (ISBI)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning in skin disease image recognition: a review</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="208264" to="208280" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A deep learning system for differential diagnosis of skin diseases</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="900" to="908" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The 2016 international league of dermatological societies&apos; revised glossary for the description of cutaneous lesions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sterry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bolognia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br. J. Dermatol</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1351" to="1358" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A deep learning based multimodal fusion model for skin lesion diagnosis using smartphone collected clinical images and metadata</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Surg</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1029991</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The impact of patient clinical information on automated skin cancer detection</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Krohling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page">103545</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incidence estimate of nonmelanoma skin cancer (keratinocyte carcinomas) in the us population</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Weinstock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Coldiron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Dermatol</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1081" to="1086" />
			<date type="published" when="2012">2012. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cancer statistics, 2022</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jemal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="33" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A benchmark for automatic visual classification of clinical skin disease images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46466-413" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9910</biblScope>
			<biblScope unit="page" from="206" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Feature fusion vision transformer for fine-grained visual categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02341</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning differential diagnosis of skin conditions with co-occurrence supervision using graph convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59713-9_33</idno>
		<idno>978-3-030-59713-9 33</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12262</biblScope>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Remixformer: a transformer model for precision skin tumor differential diagnosis via multi-modal imaging and non-imaging data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_60</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-860" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="624" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auxiliary decoder and classifier for imbalanced skin disease diagnosis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. Conf. Ser</title>
		<imprint>
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12046</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-paced balance learning for clinical skin disease recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2832" to="2846" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cutmix: regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Medical image classification using synergic deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="10" to="19" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention residual learning for skin lesion classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2092" to="2103" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
