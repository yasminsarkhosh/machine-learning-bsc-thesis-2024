<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy</title>
				<funder ref="#_rCmUzwG">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_tzUcwM9">
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)</orgName>
				</funder>
				<funder>
					<orgName type="full">Erlangen National High Performance Computing Center of the Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
				</funder>
				<funder ref="#_SHDTYGJ">
					<orgName type="full">EmpkinS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jann-Ole</forename><surname>Henningson</surname></persName>
							<email>jann-ole.henningson@fau.de</email>
							<idno type="ORCID">0000-0002-4413-3708</idno>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marion</forename><surname>Semmler</surname></persName>
							<idno type="ORCID">0000-0001-6753-8102</idno>
							<affiliation key="aff1">
								<orgName type="department">Division of Phoniatrics and Pediatric Audiology at the Department of Otorhinolaryngology, Head and Neck Surgery</orgName>
								<orgName type="institution">University Hospital Erlangen</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<postCode>91054</postCode>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Döllinger</surname></persName>
							<idno type="ORCID">0000-0003-2717-4820</idno>
							<affiliation key="aff1">
								<orgName type="department">Division of Phoniatrics and Pediatric Audiology at the Department of Otorhinolaryngology, Head and Neck Surgery</orgName>
								<orgName type="institution">University Hospital Erlangen</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<postCode>91054</postCode>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
							<idno type="ORCID">0000-0001-8699-3442</idno>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="34" to="43"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2E56230952A6AA38D3AD2F7F7C3AE080</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human Vocal Folds</term>
					<term>Laryngoscopy</term>
					<term>Keypoint Detection</term>
					<term>Semantic Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, phoniatric diagnostics has seen a surge of interest in structured light-based high-speed video endoscopy, as it enables the observation of oscillating human vocal folds in vertical direction. However, structured light laryngoscopy suffers from practical problems: specular reflections interfere with the projected pattern, mucosal tissue dilates the pattern, and lastly the algorithms need to deal with huge amounts of data generated by a high-speed video camera. To address these issues, we propose a neural approach for the joint semantic segmentation and keypoint detection in structured light high-speed video endoscopy that improves the robustness, accuracy, and performance of current human vocal fold reconstruction pipelines. Major contributions are the reformulation of one channel of a semantic segmentation approach as a single-channel heatmap regression problem, and the prediction of sub-pixel accurate 2D point locations through weighted least squares in a fully-differentiable manner with negligible computational cost. Lastly, we expand the publicly available Human Laser Endoscopic dataset to also include segmentations of the human vocal folds itself. The source code and dataset are available at: github.com/Henningson/SSSLsquared</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The voice is an essential aspect of human communication and plays a critical role in expressing emotions, conveying information, and establishing personal connections. An impaired function of the voice can have significant negative impacts on an individual. Malign changes of human vocal folds are conventionally observed by the use of (high-speed) video endoscopy that measures their 2D deformation in image space. However, it was shown that their dynamics contain a significant vertical deformation. This led to the development of varying methods for the 3D reconstruction of human vocal folds during phonation. In these works, especially active reconstruction systems have been researched that project a pattern onto the surface of vocal folds <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref>. All these systems need to deal with issues introduced by the structured light system, in particular a) specular reflections from the endoscopic light source which occlude the structured light pattern (cp. Fig. <ref type="figure" target="#fig_0">1</ref>), b) a dilation of the structured light pattern through subsurface scattering effects in mucosal tissue, and c) vasts amount of data generated by high-speed cameras recording with up to 4000 frames per second. Furthermore, the introduction of a structured light source, e.g. a laser projection unit (LPU), increases the form-factor of the endoscope, which makes recording uncomfortable for the patient. In current systems, the video processing happens offline, which means that often unnecessarily long footage is recorded to be sure that an appropriate sequence is contained, or-even worse-that a patient has to show up again, because the recorded sequence is not of sufficient quality. Ideally, recording should thus happen in a very short time (seconds) and provide immediate feedback to the operator, which is only possible if the segmentation and pattern detection happen close to real time. To address all these practical issues, we present a novel method for the highly efficient and accurate segmentation, localization, and tracking of human vocal folds and projection patterns in laser-supported high-speed video endoscopy. An overview of our pipeline is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. It is based on two stages: First, a convolutional neural network predicts a segmentation of the vocal folds, the glottal area, and projected laser dots. Secondly, we compute sub-pixel accurate 2D point locations based on the pixel-level laser dot class probabilities in a weighted least-squares manner that further increase prediction accuracy. This approach can provide immediate feedback about the success of the recording to the physician, e.g. in form of the number of successfully tracked laser dots. Furthermore, this method can not only be used in vocal fold 3D reconstruction pipelines but also allows for the analysis of clinically relevant 2D features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work can properly be assumed to be simultaneously a heatmap regression as well as a semantic segmentation approach. Deep learning based medical semantic segmentation has been extensively studied in recent years with novel architectures, loss-functions, regularizations, data augmentations, holistic training and optimization approaches <ref type="bibr" target="#b23">[23]</ref>. Here, we will focus on the specific application of semantic segmentation in laryngoscopy. Deep Learning in Laryngoscopy. In the realm of laryngoscopy works involving deep learning are few and far between. Most of these works focus on the segmentation of the glottal gap over time. The glottal dynamics give information about the patients underlying conditions, all the while being an easily detectable feature. Fehling et al. were the first to propose a CNN-based method that also infers a segmentation of the human vocal folds itself <ref type="bibr" target="#b7">[8]</ref>. Their method uses a general U-Net architecture extended with Long Short-Term Memory cells to also take temporal information into account. Pedersen et al. <ref type="bibr" target="#b17">[17]</ref>  Keypoint Detection can generally be separated into regression-based approaches that infer keypoint positions directly from the images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b26">26]</ref> and heatmap-based approaches that model the likelihood of the existence of a keypoint, i.e. landmark, via channel-wise 2D Gaussians and determining channelwise global maxima via argmax(). This leads to obvious quantization errors, that are addressed in recent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">25]</ref>. Most related to our approach is the work by Sharan et al. that proposes a method for determining the position of sutures by reformulating a single-channel binary segmentation to find local maxima and calculating their positions through general center of gravity estimation <ref type="bibr" target="#b21">[21]</ref>. In case of human vocal folds, there have been works regarding laser dot detection in structured light laryngoscopy. However, these suffer from either a manual labeling step <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref> or work only on a per-image basis <ref type="bibr" target="#b10">[10]</ref>, thus being susceptible to artifacts introduced through specular highlights. In a similar vein, none of the mentioned methods apply promising deep learning techniques. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Given an isotropic light source and a material having subsurface scattering properties, the energy density of the penetrating light follows an exponential falloff <ref type="bibr" target="#b12">[12]</ref>. In case of laser-based structured light endoscopy, this means that we can observe a bleeding effect, i.e. diffusion, of the respective collimated laser beams in mucosal tissue. Note that the energy density of laser beams is Gaussian distributed as well, amplifying this effect. Our method is now based on the assumption, that a pixel-level (binary-or multi-class) classifier will follow this exponential falloff in its predictions, and we can estimate sub-pixel accurate point positions through Gaussian fitting (cp. Fig. <ref type="figure" target="#fig_3">3</ref>). This allows us to model the keypoint detection as a semantic segmentation task, such that we can jointly estimate the human vocal folds, the glottal gap, as well as the laserpoints' positions using only a single inference step. The method can be properly divided into two parts. At first, an arbitrary hourglass-style CNN (in our case we use the U-Net architecture) estimates semantic segmentations of the glottal gap, vocal folds and the 2D laserdots position for a fixed videosequence in a supervised manner. Next, we extract these local maxima lying above a certain threshold and use weighted least squares to fit a Gaussian function into the windowed regions. In this way, we can estimate semantic segmentations as well as the position of keypoints in a single inference pass, which significantly speeds up computation.</p><p>Feature Extraction. Current heatmap regression approaches use the argmax(.) or topk(.) functions to estimate channel-wise global maxima, i.e. a single point per channel. In case of an 31 by 31 LPU this would necessitate such an approach to estimate 961 channels; easily exceeding maintainable memory usage. Thus our method needs to allow multiple points to be on a single channel. However, this makes taking the argmax(.) or topk(.) functions to extract local maxima infeasible, or outright impossible. Thus, to extract an arbitrary amount of local maxima adhering to a certain quality, we use dilation filtering on a thresholded and Gaussian blurred image. More precisely we calculate</p><formula xml:id="formula_0">I T = I &gt; [T (I, θ) * G ⊕ B],</formula><p>where T (x, y) depicts a basic thresholding operation, G a Gaussian kernel, B a general box kernel with a 0 at the center, and ⊕ the dilation operator. Finally we can easily retrieve local maxima by just extracting every non-zero element of I T . We then span a window I ij of size k ∈ N around the non-zero discrete points p i . For improved comprehensibility, we are dropping the subscripts in further explanations, and explain the algorithm for a single point p. Next, we need to estimate a Gaussian function. Note that, a Gaussian function is of the form f (x) = Ae -(x-μ) 2 /2σ 2 , where x = μ is the peak, A the peaks height, and σ defines the functions width. Gaussian fitting approaches can generally be separated into two types: the first ones employ non-linear least squares optimization techniques <ref type="bibr" target="#b24">[24]</ref>, while others use the result of Caruana et al. in that the logarithm of a Gaussian is a polynomial equation <ref type="bibr" target="#b1">[2]</ref>.</p><p>Caruanas Algorithm. As stated previously, Caruanas algorithm is based on the observation that by taking the logarithm of the Gaussian function, we generate a polynomial equation (Eq. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ln(f (x)) = ln(</head><formula xml:id="formula_1">A) + -(x -μ) 2 2σ 2 = ln(A) - -μ 2 2σ 2 + 2μx 2σ 2 - x 2 2σ 2<label>(1)</label></formula><p>Note that the last equation is in polynomial form ln(y) = ax 2 + bx + c, with c = ln(A) --μ 2 2σ 2 , b = μ σ 2 and a = -1 2σ 2 . By defining the error function δ = ln(f (x)) -(ax 2 + bx + c) and differentiating the sum of residuals gives a linear system of equations (Eq. 2). ⎡</p><formula xml:id="formula_2">⎣ N x x 2 x x 2 x 3 x 2 x 3 x 4 ⎤ ⎦ ⎡ ⎣ a b c ⎤ ⎦ = ⎡ ⎣ ln(ŷ) x ln(ŷ) x 2 ln(ŷ) ⎤ ⎦<label>(2)</label></formula><p>After solving the linear system, we can finally retrieve μ, σ and A with μ = -b/2c, σ = -1/2c, and A = e a-b 2 /4c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guos Algorithm.</head><p>Due to the logarithmic nature of Caruanas algorithm, it is very susceptible towards outliers. Guo <ref type="bibr" target="#b8">[9]</ref> addresses these problems through a weighted least-squares regimen, by introducing an additive noise term η and reformulating the cost function to (Eq. 3). Similarly, by differentiating the sum of 2 , we retrieve a linear system of the form given in Eq. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= y[ln(y</head><formula xml:id="formula_3">+ η) -(a + bx + cx 2 )] ≈ y[ln(y) -(a + bx + cx 2 )] + η. (<label>3</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">⎡ ⎣ ŷ2 x ŷ2 x 2 ŷ2 x ŷ2 x 2 ŷ2 x 3 ŷ2 x 2 ŷ2 x 3 ŷ2 x 4 ŷ2 ⎤ ⎦ ⎡ ⎣ a b c ⎤ ⎦ = ⎡ ⎣ ŷ2 ln(ŷ) x ŷ2 ln(ŷ) x 2 ŷ2 ln(ŷ) ⎤ ⎦<label>(4)</label></formula><p>The parameters of the Gaussian function μ, σ and A can be calculated similar to Caruanas algorithm. Recall that polynomial regression has an analytical solution, where the vector of estimated polynomial regression coefficients is β = (X T X) -1 X T ŷ. This formulation is easily parallelizable on a GPU and fully differentiable. Hence, we can efficiently compute the Gaussian coefficients necessary for determining the subpixel position of local maxima. Finally, we can calculate the points position p by simple addition using Eq. 5.</p><formula xml:id="formula_6">p = µ + p (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We implemented our code in Python (3.10.6), using the PyTorch (1.12.1) <ref type="bibr" target="#b15">[15]</ref> and kornia (0.6.8) <ref type="bibr" target="#b5">[6]</ref> libraries. We evaluate our code as well as the comparison methods on an Nvidia RTX 3080 GPU. We follow the respective methods closely for training. For data augmentation, we use vertical and horizontal flipping, affine and perspective transformations, as well as gamma correction and brightness modulation. We opted to use data augmentation strategies that mimic data that is likely to occur in laryngoscopy. We evaluate all approaches using a kfold cross validation scheme with k = 5, on an expanded version of the publicly available Human-Laser Endoscopic (HLE) dataset <ref type="bibr" target="#b10">[10]</ref> that also includes segmentations of the human vocal fold itself. We evaluate on subjects that were not contained in the training sets. The data generation scheme for the human vocal fold segmentation is described further below. For our baseline, we use dilation filtering and the moment method on images segmented using the ground-truth labels similar to <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref>. We opt to use the ground-truth labels to show how good these methods may become given perfect information. We train our approach via Stochastic Gradient Descent, with a learning rate of 10 -1 for 100 epochs and update the learning rate via a polynomial learning rate scheduler similar to nnU-Net <ref type="bibr" target="#b11">[11]</ref>. For estimating the keypoints we use a window size of 7, a Gaussian blur of size 5 and set the keypoint threshold to 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HLE++.</head><p>The HLE dataset is a publicly available dataset consisting of 10 labeled in-vivo recordings of human vocal folds during phonation <ref type="bibr" target="#b10">[10]</ref>, where each recording contains one healthy subject. The labels include segmentations of the glottal gap, the glottal mid-and outline as well as the 2D image space positions of the laser dots projected onto the superior surface of the vocal folds. We expand the dataset to also include segmentation labels for the vocal folds itself. Due to the high framerate of the recordings, motion stemming from the manually recording physician is minimal and can be assumed to be linear. Thus, to generate the vocal fold segmentation masks, we generate a segmentation mask of the vocal fold region manually and calculate its centroid in the first and last frame of each video. Then, we linearly interpolate between the measured centroids. To account for the glottal gap inside each frame, we set</p><formula xml:id="formula_7">F i = F i \ G i</formula><p>, where F i is the vocal fold segmentation at frame i and G i the glottal segmentation, respectively.</p><p>Quantitative and Qualitative Evaluation. Table <ref type="table" target="#tab_0">1</ref> shows a quantitative evaluation of different neural network architectures that have been used in laryngoscopy <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">18]</ref> or medical keypoint detection tasks <ref type="bibr" target="#b21">[21]</ref>. In general, we could reformulate this segmentation task as a 3D segmentation task, in which we model the width and height of the images as first and second dimension, and lastly time as our third dimension. However, due to the poor optimization of 3D Convolutions on GPUs <ref type="bibr" target="#b13">[13]</ref> and the large amounts of data generated in high-speed video  <ref type="table" target="#tab_0">1</ref>). However, frame jumps can be seen inbetween sequences. Since HLE was generated with a single recording unit, we assume that a properly trained 2D-CNN can infer occluded point positions based on the surrounding laser points as well as the observed topology of the vocal folds itself. However, we believe that it generalizes less well to arbitrary point patterns than a network architecture including temporal information. For the segmentation tasks, we measure the foreground IoU and DICE scores. For the keypoints, we evaluate the precision and the F1-score. We count a keypoint prediction as true positive, when its distance to its closest ground-truth point does not exceed 2 pixels. In Fig. <ref type="figure" target="#fig_4">4</ref> an example of a prediction over 5 frames is given, showing that neural networks can infer proper point positions even in case of occlusions. A further qualitative assessment of point predictions, vocal fold and glottal gap segmentations is given in Fig. <ref type="figure" target="#fig_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a method for the simultaneous segmentation and laser dot localization in structured light high-speed video laryngoscopy. The general idea is that we can dedicate one channel of the output of a general multiclass segmentation model to learn Gaussian heatmaps depicting the locations of multiple unlabeled keypoints. To robustly handle noise, we propose to use Gaussian regression on a per local-maxima basis that estimates sub-pixel accurate keypoints with negligible computational overhead. Our pipeline is very accurate and robust, and can give feedback about the success of a recording within a fraction of a second. Additionally, we extended the publicly available HLE Dataset to include segmentations of the human vocal fold itself. For future work, it would be beneficial to investigate how this method generalizes to arbitrary projection patterns and non-healthy subjects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Recording setup. From left to right: a) single frame of a recorded video containing laser dots and specular reflections, b) same frame with highlighted laser dots and specular reflections, c) illustration of recording setup with endoscope and laser projection unit (LPU) looking at the vocal folds.</figDesc><graphic coords="2,57,48,321,77,337,33,64,69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Our method receives an image sequence and computes a pixel-wise classification of the glottal gap, vocal folds, and laser points. Next, we estimate 2D keypoint locations on the softmaxed laser point output of our model via weighted least squares.</figDesc><graphic coords="3,44,79,172,73,334,48,151,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>use off-the-shelf U-Nets to estimate glottal gap and vocal folds itself. Cho et al. [3] compare different segmentation architectures including CNN6, VGG16, Inception V3 and XCeption. Döllinger et al. [4] have shown that pretraining CNNs for human vocal fold segmentation can boost the respective CNNs performance. To the best of our knowledge, no publication has specifically targeted segmentation and detection in structured light laryngoscopy via deep learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. We extract windows around local maxima through dilation filtering and fit a Gaussian function into the window to estimate sub-pixel accurate keypoints.</figDesc><graphic coords="4,70,98,215,51,310,63,86,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. In-vivo dynamics of a human vocal fold in laser-based structured light endoscopy. Due to the endoscopic light source and moist mucosal tissue, we see an abundance of specular reflections occluding the laserdots. In green: keypoint detection of our approach. It successfully detects laser dots in occluded regions. (Color figure online)</figDesc><graphic coords="6,56,97,54,29,339,01,62,41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Qualitative Assessment of the inferred point positions and segmentations. Left to right: Input, Predicted Keypoints, GT Keypoints, Predicted Segmentation, GT Segmentation, Pixelwise Error.</figDesc><graphic coords="7,52,29,53,96,319,90,157,75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation of the precision and F1-score of predicted keypoints, IoU and DICE score of inferred segmentations as well as the inference time for a single image and the frames per second on an Nvidia RTX 3080 GPU. Net architecture, in which we employ channel-wise 3D convolutions inside the bottleneck as well as the output layers. This allows us to predict segmentations sequence-wise; drastically lowering inference times on a per frame basis while keeping the number of floating point operations minimal. Interestingly, this architecture achieves similar results to a standard U-Net architecture (see Table</figDesc><table><row><cell></cell><cell cols="3">Precision↑ F1-Score↑ IoU↑</cell><cell>DICE↑</cell><cell cols="2">Inf. Speed(ms)↓ FPS↑</cell></row><row><cell>Baseline</cell><cell>0.64</cell><cell>0.6923</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell></row><row><cell cols="6">U-LSTM [8] 0.70 ± 0.41 0.58 ± 0.32 0.52 ± 0.18 0.77 ± 0.08 65.57 ± 0.31</cell><cell>15</cell></row><row><cell cols="6">U-Net [18] 0.92 ± 0.08 0.88 ± 0.04 0.68 ± 0.08 0.88 ± 0.02 4.54 ± 0.03</cell><cell>220</cell></row><row><cell cols="4">Sharan [21] 0.17 ± 0.19 0.16 ± 0.17 ✗</cell><cell>✗</cell><cell>5.97 ± 0.25</cell><cell>168</cell></row><row><cell cols="6">2.5D U-Net 0.90 ± 0.08 0.81 ± 0.05 0.65 ± 0.06 0.87 ± 0.02 1.08 ± 0.01</cell><cell>926</cell></row><row><cell cols="7">recordings, this would create a serious bottleneck in real-time 3D pipelines. Thus,</cell></row><row><cell cols="3">we also evaluate a 2.5D U-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We thank <rs type="person">Dominik Penk</rs> and <rs type="person">Bernhard Egger</rs> for their valuable feedback.This work was supported by <rs type="funder">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)</rs> under grant <rs type="grantNumber">STA662/6-1</rs>, Project<rs type="grantNumber">-ID 448240908</rs> and (partly) funded by the <rs type="grantNumber">DFG -SFB 1483 -Project-ID 442419336</rs>, <rs type="funder">EmpkinS</rs>. The authors gratefully acknowledge the scientific support and HPC resources provided by the <rs type="funder">Erlangen National High Performance Computing Center of the Friedrich-Alexander-Universität Erlangen-Nürnberg</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tzUcwM9">
					<idno type="grant-number">STA662/6-1</idno>
				</org>
				<org type="funding" xml:id="_rCmUzwG">
					<idno type="grant-number">-ID 448240908</idno>
				</org>
				<org type="funding" xml:id="_SHDTYGJ">
					<idno type="grant-number">DFG -SFB 1483 -Project-ID 442419336</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 4.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Subpixel heatmap regression for facial landmark localization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.02360" />
	</analytic>
	<monogr>
		<title level="m">32nd British Machine Vision Conference 2021, BMVC 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Additional capabilities of a fast algorithm for the resolution of spectra</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Searle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Shupack</surname></persName>
		</author>
		<idno type="DOI">10.1021/ac00169a011</idno>
		<ptr target="https://doi.org/10.1021/ac00169a011" />
	</analytic>
	<monogr>
		<title level="j">Anal. Chem</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="1896" to="1900" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparison of convolutional neural network models for determination of vocal fold normality in laryngoscopic images</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvoice.2020.08.003</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0892199720302927" />
	</analytic>
	<monogr>
		<title level="j">J. Voice</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="590" to="598" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Re-training of convolutional neural networks for glottis segmentation in endoscopic high-speed videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Döllinger</surname></persName>
		</author>
		<idno type="DOI">10.3390/app12199791</idno>
		<ptr target="https://www.mdpi.com/2076-3417/12/19/9791" />
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">9791</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A connexionist approach for robust and precise facial feature detection in complex scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPA.2005.195430</idno>
		<ptr target="https://doi.org/10.1109/ISPA" />
		<imprint>
			<date type="published" when="2005">2005. 2005. 195430</date>
			<biblScope unit="page" from="316" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kornia: an open source differentiable computer vision library for pytorch</title>
		<author>
			<persName><forename type="first">E</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1910.02190.pdf" />
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Subpixel face landmarks using heatmaps and a bag of tricks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W F</forename><surname>Earp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samacoïts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noinongyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boonpunmongkol</surname></persName>
		</author>
		<idno>CoRR abs/2103.03059</idno>
		<ptr target="https://arxiv.org/abs/2103.03059" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fully automatic segmentation of glottis and vocal folds in endoscopic laryngeal high-speed videos using a deep convolutional LSTM network</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Fehling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lohscheller</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0227791</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0227791" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A simple algorithm for fitting a gaussian function</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>DSP tips and tricks</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<idno type="DOI">10.1109/MSP.2011.941846</idno>
		<ptr target="https://doi.org/10.1109/MSP.2011.941846" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="134" to="137" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time 3D reconstruction of human vocal folds via high-speed laser-endoscopy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Henningson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Döllinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Semmler</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-11" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Meth</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A practical model for subsurface light transport</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Marschner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<idno type="DOI">10.1145/383259.383319</idno>
		<ptr target="https://doi.org/10.1145/383259.383319" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
	<note>SIGGRAPH 2001, Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimizing small channel 3D convolution on GPU with tensor core</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.parco.2022.102954</idno>
		<ptr target="https://doi.org/10.1016/j.parco.2022.102954" />
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page">102954</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Three-dimensional optical reconstruction of vocal fold kinematics using high-speed videomicroscopy with a laser projection system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Luegmair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kobler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Döllinger</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2015.2445921</idno>
		<ptr target="https://doi.org/10.1109/TMI.2015.2445921" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2572" to="2582" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pytorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">In vivo measurement of pediatric vocal fold motion using structured light laser projection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Donohue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Unnikrishnan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvoice.2013.03.004</idno>
		<ptr target="https://doi.org/10.1016/j.jvoice.2013.03.004" />
	</analytic>
	<monogr>
		<title level="j">J. Voice: Off. J. Voice Found</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="463" to="472" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Localization and quantification of glottal gaps on deep learning segmentation of vocal folds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eeg</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-023-27980-y</idno>
		<ptr target="https://doi.org/10.1038/s41598-023-27980-y" />
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">878</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<idno type="arXiv">arXiv:1505.04597[cs.CV</idno>
		<ptr target="http://lmb.informatik.uni-freiburg.de/Publications/2015/RFB15a" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3D reconstruction of human laryngeal dynamics based on endoscopic high-speed recordings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Semmler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kniesburges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Birk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ziethe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Döllinger</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2016.2521419</idno>
		<ptr target="https://doi.org/10.1109/TMI.2016.2521419" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1615" to="1624" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Endoscopic laser-based 3D imaging for functional voice diagnostics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Semmler</surname></persName>
		</author>
		<idno type="DOI">10.3390/app7060600</idno>
		<ptr target="https://doi.org/10.3390/app7060600" />
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">600</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Point detection through multi-instance deep heatmap regression for sutures in endoscopy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sharan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-021-02523-w</idno>
		<ptr target="https://doi.org/10.1007/s11548-021-02523-w" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2107" to="2117" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Globally tuned cascade pose regression via back propagation with application in 2D face pose estimation and heart segmentation in 3D CT images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xiong</surname></persName>
		</author>
		<idno>ArXiv abs/1503.08843</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey on deep learning-based architectures for semantic segmentation on 2D images</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ulku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Akagündüz</surname></persName>
		</author>
		<idno type="DOI">10.1080/08839514.2022.2032924</idno>
		<ptr target="https://doi.org/10.1080/08839514.2022.2032924" />
	</analytic>
	<monogr>
		<title level="j">Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2032924</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Historical development of the Newton-Raphson method</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ypma</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2132904" />
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="531" to="551" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Heatmap regression via randomized rounding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="8276" to="8289" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detecting anatomical landmarks from limited medical imaging data using two-stage task-oriented deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2017.2721106</idno>
		<ptr target="https://doi.org/10.1109/TIP.2017.2721106" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4753" to="4764" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
