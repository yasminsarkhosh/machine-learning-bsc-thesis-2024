<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification</title>
				<funder ref="#_2Xy2Gs3">
					<orgName type="full">National Key Research and Development Project</orgName>
				</funder>
				<funder ref="#_v7WV3Yc">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_JugPXCJ">
					<orgName type="full">Hong Kong Innovation and Technology Fund</orgName>
				</funder>
				<funder ref="#_sYHZGCq #_AEWF9Sf">
					<orgName type="full">Japanese Ministry for Education, Science, Culture and Sports (MEXT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luyang</forename><surname>Luo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Sir Run Run Shaw Hospital</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruofeng</forename><surname>Tong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Research Center for Healthcare Data Science</orgName>
								<address>
									<addrLine>Zhejiang Lab</addrLine>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yen-Wei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">College of Information Science and Engineering</orgName>
								<orgName type="institution">Ritsumeikan University</orgName>
								<address>
									<settlement>Kusatsu</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongjie</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Sir Run Run Shaw Hospital</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lanfen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Chemical and Biological Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="467" to="476"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F566D27265FADE21D5E8740922DB3900</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_45</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multiple Instance Learning</term>
					<term>Whole Slide Image</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Whole Slide Image (WSI) classification remains a challenge due to their extremely high resolution and the absence of fine-grained labels. Presently, WSI classification is usually regarded as a Multiple Instance Learning (MIL) problem when only slide-level labels are available. MIL methods involve a patch embedding module and a bag-level classification module, but they are prohibitively expensive to be trained in an end-to-end manner. Therefore, existing methods usually train them separately, or directly skip the training of the embedder. Such schemes hinder the patch embedder's access to slide-level semantic labels, resulting in inconsistency within the entire MIL pipeline. To overcome this issue, we propose a novel framework called Iteratively Coupled MIL (ICMIL), which bridges the loss back-propagation process from the bag-level classifier to the patch embedder. In ICMIL, we use category information in the bag-level classifier to guide the patch-level fine-tuning of the patch feature extractor. The refined embedder then generates better instance representations for achieving a more accurate bag-level classifier. By coupling the patch embedder and bag classifier at a low cost, our proposed framework enables information exchange between the two modules, benefiting the entire MIL classification model. We tested our framework on two datasets using three different backbones, and our experimental results demonstrate consistent performance improvements over state-of-the-art MIL methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Whole slide scanning is increasingly used in disease diagnosis and pathological research to visualize tissue samples. Compared to traditional microscopebased observation, whole slide scanning converts glass slides into gigapixel digital images that can be conveniently stored and analyzed. However, the high resolution of WSIs also makes their automated classification challenging <ref type="bibr" target="#b14">[15]</ref>. Patchbased classification is a common solution to this problem <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref>. It predicts the slide-level label by first predicting the labels of small, tiled patches in a WSI. This approach allows for the direct application of existing image classification models, but requires additional patch-level labeling. Unfortunately, patch-level labeling by histopathology experts is expensive and time-consuming. Therefore, many weakly-supervised <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref> and semi-supervised <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> methods have been proposed to generate patch-level pseudo labels at a lower cost. However, the lack of reliable supervision directly hinders the performance of these methods, and serious class-imbalance problems could arise, as tumor patches may only account for a small portion of the entire WSI <ref type="bibr" target="#b11">[12]</ref>.</p><p>In contrast, MIL-based methods have become increasingly preferred due to their only demand for slide-level labels <ref type="bibr" target="#b17">[18]</ref>. The typical pipeline of MIL methods is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, where WSIs are treated as bags, and tiled patches are considered as instances. The aim is to predict whether there are positive instances, such as tumor patches, in a bag, and if so, the bag is considered positive as well. In practice, a fixed ImageNet pre-trained feature extractor g(•) is usually used to convert the tiled patches in a WSI into feature maps due to limited GPU memory. These instance features are then aggregated by a(•) into a slide-level feature vector to be sent to the bag-level classifier f (•) for MIL training. Due to the high computational cost, end-to-end training of the feature extractor and bag classifier is prohibitive, especially for high-resolution WSIs. As a result, many methods focus solely on improving a(•) or f (•), leaving g(•) untrained on the WSI dataset (as shown in Fig. <ref type="figure" target="#fig_1">2(b)</ref>). However, the domain shift between WSI and natural images may lead to sub-optimal representations, so recently there have been methods proposed to fine-tune g(•) using self-supervised techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref> or weakly-supervised techniques <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref> (as shown in Fig. <ref type="figure" target="#fig_1">2(c)</ref>). Nevertheless, since these two processes are still trained separately with different supervision signals, they lack joint optimization and may still leads to inconsistency within the entire MIL pipeline. To address the challenges mentioned above, we propose a novel MIL framework called ICMIL, which can iteratively couple the patch feature embedding process with the bag-level classification process to enhance the effectiveness of MIL training (as illustrated in Fig. <ref type="figure" target="#fig_1">2(d)</ref>). Unlike previous works that mainly focused on designing sophisticated instance aggregators a(•) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref> or bag classifiers f (•) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>, we aim to bridge the loss back-propagation process from f (•) to g(•) to improve g(•)'s ability to perceive slide-level labels. Specifically, we propose to use the bag-level classifier f (•) to initialize an instance-level classifier f (•), enabling f (•) to use the category knowledge learned from bag-level features to determine each instance's category. In this regard, we further propose a teacher-student <ref type="bibr" target="#b6">[7]</ref> approach to effectively generate pseudo labels and simultaneously fine-tune g(•). After fine-tuning, the domain shift problem is alleviated in g(•), leading to better patch representations. The new representations can be used to train a better bag-level classifier in return for the next round of iteration.</p><p>In summary, our contributions are: <ref type="bibr" target="#b0">(1)</ref> We propose ICMIL which bridges the loss propagation from the bag classifier to the patch embedder by iteratively coupling them during training. This framework fine-tunes the patch embedder based on the bag-level classifier, and the refined embeddings, in turn, help train a more accurate bag-level classifier. <ref type="bibr" target="#b1">(2)</ref> We propose a teacher-student approach to achieve effective and robust knowledge transfer from the bag-level classifier f (•) to the instance-level representation embedder g(•). (3) We conduct extensive experiments on two datasets using three different backbones and demonstrate the effectiveness of our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Iterative Coupling of Embedder and Bag Classifier in ICMIL</head><p>The general idea of ICMIL is shown in Fig. <ref type="figure" target="#fig_2">3</ref>, which is inspired by the Expectation-Maximization (EM) algorithm. EM has been used with MIL in some previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref>, but it was only treated as an assisting tool for aiding the training of either g(•) or f (•) in the traditional MIL pipelines. In contrast, we are the first to consider the optimization of the entire MIL pipeline as an EM alike problem, utilizing EM for coupling g(•) and f (•) together iteratively. To begin with, we first employ a traditional approach to train a bag-level classifier f (•) on a given dataset, with patch embeddings generated by a fixed ResNet50 <ref type="bibr" target="#b5">[6]</ref> pre-trained on ImageNet <ref type="bibr" target="#b18">[19]</ref> (step 1 in Fig. <ref type="figure" target="#fig_2">3</ref>). Subsequently, this f (•) is considered as the initialization of a hidden instance classifer f (•), generating pseudo-labels for each instance-level representation. This operation is feasible when the bag-level representations aggregated by a(•) are in the same hidden space as the instance representations, and most aggregation methods (e.g., max pooling, attention-based) satisfy this condition since they essentially make linear combinations of instance-level representations.</p><p>Next, we freeze the weights of f (•) and fine-tune g(•) with the generated pseudo-labels (step 2 in Fig. <ref type="figure" target="#fig_2">3</ref>), of which the detailed implementation is presented in Sect. 2.3. After this, g(•) is fine-tuned for the specific WSI dataset, which allows it to generate improved representations for each instance, thereby enhancing the performance of f (•). Moreover, with a better f (•), we can use the iterative coupling technique again, resulting in further performance gains and mitigation to the distribution inconsistencies between instance-and bag-level embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Instance Aggregation Method in ICMIL</head><p>Although most instance aggregators are compatible with ICMIL, they still have an impact on the efficiency and effectiveness of ICMIL. In addition to that a(•) has to project the bag representations to the same hidden space as the instance representations, it also should avoid being over-complicated. Otherwise, a(•) may lead to larger difference between the decision boundaries of bag-level classifer f (•) and instance-level classifier f (•), which may cause ICMIL taking more time to converge.</p><p>Therefore, in our experiments, we choose to use the attention-based instance aggregation method <ref type="bibr" target="#b8">[9]</ref> which has been widely used in many of the existing MIL frameworks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>. For a bag that contains K instances, attention-based aggregation method firstly learns an attention score for each instance. Then, the aggregated bag-level representation H is defined as:</p><formula xml:id="formula_0">H = K k=1 a k h k ,<label>(1)</label></formula><p>where a k is the attention score for the k-th instance h k in the bag. Obviously, H and h k remains in the same hidden space, satisfying the prerequisite of ICMIL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Label Propagation from Bag Classifier to Embedder</head><p>We propose a novel teacher-student model for accurate and robust label propagation from f (•) to g(•). The model's architecture is depicted in Fig. <ref type="figure" target="#fig_3">4</ref>. In contrast to the conventional approach of generating all pseudo labels and retraining g(•) from scratch, our proposed method can simultaneously process the pseudo label generation and g(•) fine-tuning tasks, making it more flexible. Moreover, incorporating augmented inputs in the training process allows for the better utilization of supervision signals, resulting in a more robust g(•). We also introduce a learnable f (•) to self-adaptively modifying the instance-level decision boundary for more effective fine-tuning of the embedder. Specifically, we freeze the weights of g(•) and f (•) and set them as the teacher. We then train a student patch embedding network, g (•), to learn category knowledge from the teacher. For a given patch input x, the teacher generates the corresponding pseudo label, while the student receives an augmented image x and attempts to generate a similar prediction to that of the teacher through a consistency loss L c . This loss function is defined as:</p><formula xml:id="formula_1">L c = C c=1 f (x) c log f (x) c f (x ) c ,<label>(2)</label></formula><p>where f (•) and f (•) are teacher classifer and student classifier respectively, f (•) c indicates the c-th channel of f (•), and C is the total number of channels. Additionally, during training, a learnable instance-level classifier is used on the student to back-propagate the gradients to g (•). The initial weights of f (•) are the same as those of f (•), as the differences in the instance-and bag-level classification boundaries is expected to be minor. To make f (•) not so different from f (•) during training, a weight similarity loss, L w , is further imposed to constrain it by drawing closer their each layer's outputs under the same input. By applying L w , the patch embeddings from g (•) can still suit the bag-level classification task well, rather than being tailored solely for the instance-level classifier f (•). L w is defined as:</p><formula xml:id="formula_2">L w = L l=1 C c=1 f (x) l c log f (x) l c f (x) l c , (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where f (•) l c indicates the c-th channel of l-th layer's output in f (•). The overall loss function for this step is L c + αL w , with α set to 0.5 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Our experiments utilized two datasets, with the first being the publicly available breast cancer dataset, Camelyon16 <ref type="bibr" target="#b0">[1]</ref>. This dataset consists of a total of 399 WSIs, with 159 normal and 111 metastasis WSIs for the training set, and the remaining 129 for test. Although patch-level labels are officially provided in Camelyon16, they were not used in our experiments.</p><p>The second dataset is a private hepatocellular carcinoma (HCC) dataset collected from Sir Run Run Shaw Hospital, Hangzhou, China. This dataset comprises a total of 1140 valid tumor WSIs scanned at 40× magnification, and the objective is to identify the severity of each case based on the Edmondson-Steiner (ES) grading. The ground truth labels are binary classes of low risk and high risk, which were provided by experienced pathologists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>For Camelyon16, we tiled the WSIs into 256×256 patches on 20× magnification using the official code of <ref type="bibr" target="#b24">[25]</ref>, while for the HCC dataset the patches are 384×384 on 40× magnification following the pathologists' advice. For both datasets, we used an ImageNet pre-trained ResNet50 to initialize g(•). The instance embedding process was the same of <ref type="bibr" target="#b15">[16]</ref>, which means for each patch, it would be </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>(w/ AB-MIL) 90.0 firstly embedded into a 1024-dimension vector, and then be projected to a 512dimension hidden space for further bag-level training. For the training of bag classifier f (•), we used an initial learning rate of 2e-4 with Adam <ref type="bibr" target="#b10">[11]</ref> optimizer for 200 epochs with batch size being 1. Camelyon16 results are reported on the official test split, while the HCC dataset used a 7:1:2 split for training, validation and test. For the training of patch embedder g(•), we used an initial learning rate of 1e-5 with Adam <ref type="bibr" target="#b10">[11]</ref> optimizer with the batch size being 100. Three metrics were used for evaluation. Namely, area under curve (AUC), F1 score, and slide-level accuracy (Acc). Experiments were all conducted on a Nvidia Tesla M40 (12GB).</p><formula xml:id="formula_5">(+4<label>.6) 80.5 (+2.5) 86.6 (+2.1) 87.1 (+5.9) 88.3</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results</head><p>Ablation Study. The results of ablation studies are presented in Table <ref type="table" target="#tab_0">1</ref>. From Table <ref type="table" target="#tab_0">1</ref>(a), we can learn that as the number of ICMIL iteration increases, the performance will also go up until reaching a stable point. Since the number of instances is very large in WSI datasets, we empirically recommend to choose to run ICMIL one iteration for fine-tuning g(•) to achieve the balance between performance gain and time consumption. From Table <ref type="table" target="#tab_0">1</ref>(b), it is shown that our teacher-student-based method outperforms the naïve "pseudo label generation" method for fine-tuning g(•), which demontrates the effectiveness of introducing the learnable instance-level classifier f (•).</p><p>Comparison with Other Methods. Experimental results are presented in Table <ref type="table" target="#tab_1">2</ref>. As shown, our ICMIL framework consistently improves the performance of three different MIL baselines (i.e., Max Pool, AB-MIL, and DTFD-MIL), demonstrating the effectiveness of bridging the loss back-propagation from bag calssifier to embedder. It proves that a more suitable patch embedding can greatly enhance the overall MIL classification framework. When used with the state-of-the-art MIL method DTFD-MIL, ICMIL further increases its performance on Camelyon16 by 0.5% AUC, 2.1% F1, and 1.6% Acc. Results on the HCC dataset also proves the effectiveness of ICMIL, despite the minor difference on the relative performance of baseline methods. Mean Pooling performs better on this dataset due to the large area of tumor in the WSIs (about 60% patches are tumor patches), which mitigates the impact of average pooling on instances. Also, the performance differences among different vanilla MIL methods tends to be smaller on this dataset since risk grading is a harder task than Camelyon16. In this situation, the quality of instance representations plays a crucial role in generating more separable bag-level representations. As a result, after applying ICMIL on the MIL baselines, these methods all gain great performance boost on the HCC dataset. Furthermore, Fig. <ref type="figure" target="#fig_5">5</ref> displays the instance-level and bag-level representations of Camelyon16 dataset before and after applying ICMIL on AB-MIL backbone.</p><p>The results indicate that one iteration of g(•) fine-tuning in ICMIL significantly improves the instance-level representations, leading to a better aggregated baglevel representation naturally. Besides, the bag-level representations are also more closely aligned with the instance representations, proving that ICMIL can reduce the inconsistencies between g(•) and f (•) by coupling them together for training, resulting in a better separability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we propose ICMIL, a novel framework that iteratively couples the feature extraction and bag classification stages to improve the accuracy of MIL models. ICMIL leverages the category knowledge in the bag classifier as pseudo supervision for embedder fine-tuning, bridging the loss propagation from classifier to embedder. We also design a two-stream model to efficiently facilitate such knowledge transfer in ICMIL. The fine-tuned patch embedder can provide more accurate instance embeddings, in return benefiting the bag classifier. The experimental results show that our method brings consistent improvement to existing MIL backbones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The typical pipeline of traditional MIL methods on WSIs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison between ICMIL and existing methods. (a) Ordinary end-to-end classification pipeline. (b) MIL methods that use fixed pre-trained ResNet50 as g(•). (c) MIL methods that introduce extra self-supervised fine-tuning of g(•). (d) Our proposed ICMIL which can bridge the loss back-propagation process from f (•) to g(•) by iteratively coupling them during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The core idea of ICMIL: iteratively, 1 fix the embedder g(•) and train the bag classifier f (•), 2 fix the classifier f (•) and fine-tune the instance embedder g(•).</figDesc><graphic coords="4,69,81,53,81,213,16,89,89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. A schematic view of the proposed teacher-student alike model for label propagation from f (•) to g(•) (mainly in step 2 ), and its position in ICMIL pipeline.</figDesc><graphic coords="5,63,33,61,25,333,31,141,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visualization of the instance-and bag-level representations before and after ICMIL training. We sample one instance from one bag w/ Max Pooling. Only one iteration of ICMIL is used to achieve the right figure.</figDesc><graphic coords="8,41,79,54,44,340,24,147,43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results of ablation studies on Camelyon16 with AB-MIL.</figDesc><table><row><cell cols="7">(a) Ablation study on the ICMIL iteration times</cell><cell></cell><cell cols="2">(b) Loss Propagation</cell></row><row><cell>ICMIL Iterations</cell><cell>0</cell><cell>0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>2.5</cell><cell>3</cell><cell cols="2">Method Naïve Ours</cell></row><row><cell>AUC</cell><cell cols="7">85.4 88.8 90.0 89.7 90.5 90.4 90.0</cell><cell>AUC</cell><cell>88.5 90.0</cell></row><row><cell>F1</cell><cell cols="7">78.0 79.4 80.5 80.1 82.0 80.7 81.7</cell><cell>F1</cell><cell>78.8 80.5</cell></row><row><cell>Acc</cell><cell cols="7">84.5 85.0 86.6 86.0 85.8 86.9 86.6</cell><cell>Acc</cell><cell>83.9 86.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with other methods on Camelyon16 and HCC datasets, where † indicates the corresponding Camelyon16 results are cited from<ref type="bibr" target="#b24">[25]</ref>. Best results are in bold, while the second best ones are underlined.</figDesc><table><row><cell>Method</cell><cell cols="3">Loss Propagation Camelyon16</cell><cell></cell><cell>HCC</cell><cell></cell></row><row><cell></cell><cell>g(•) f (•) f →g</cell><cell cols="6">AUC(%) F1(%) Acc(%) AUC(%) F1(%) Acc(%)</cell></row><row><cell>Mean Pooling</cell><cell></cell><cell>60.3</cell><cell>44.1</cell><cell>70.1</cell><cell>76.4</cell><cell>83.1</cell><cell>73.7</cell></row><row><cell>Max Pooling</cell><cell></cell><cell>79.5</cell><cell>70.6</cell><cell>80.3</cell><cell>80.1</cell><cell>84.3</cell><cell>76.8</cell></row><row><cell>RNN-MIL  † [2]</cell><cell></cell><cell>87.5</cell><cell>79.8</cell><cell>84.4</cell><cell>79.4</cell><cell>84.1</cell><cell>75.5</cell></row><row><cell>AB-MIL  † [9]</cell><cell></cell><cell>85.4</cell><cell>78.0</cell><cell>84.5</cell><cell>81.2</cell><cell>86.0</cell><cell>78.1</cell></row><row><cell>DS-MIL  † [12]</cell><cell></cell><cell>89.9</cell><cell>81.5</cell><cell>85.6</cell><cell>86.1</cell><cell>86.6</cell><cell>81.4</cell></row><row><cell>CLAM-SB  † [16]</cell><cell></cell><cell>87.1</cell><cell>77.5</cell><cell>83.7</cell><cell>82.1</cell><cell>84.3</cell><cell>77.1</cell></row><row><cell>CLAM-MB  † [16]</cell><cell></cell><cell>87.8</cell><cell>77.4</cell><cell>82.3</cell><cell>81.7</cell><cell>83.7</cell><cell>76.3</cell></row><row><cell>TransMIL  † [20]</cell><cell></cell><cell>90.6</cell><cell>79.7</cell><cell>85.8</cell><cell>81.2</cell><cell>84.4</cell><cell>76.7</cell></row><row><cell>DTFD-MIL [25]</cell><cell></cell><cell>93.2</cell><cell>84.9</cell><cell>89.0</cell><cell>83.0</cell><cell>85.5</cell><cell>78.1</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(w/ Max Pooling)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. This work was supported by the <rs type="funder">National Key Research and Development Project</rs> (No. <rs type="grantNumber">2022YFC2504605</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62202403</rs>) and <rs type="funder">Hong Kong Innovation and Technology Fund</rs> (No. <rs type="grantNumber">PRP/034/22FX</rs>). It was also supported in part by the <rs type="grantName">Grant in Aid for Scientific Research</rs> from the <rs type="funder">Japanese Ministry for Education, Science, Culture and Sports (MEXT)</rs> under the Grant No. <rs type="grantNumber">20KK0234</rs>, <rs type="grantNumber">21H03470</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2Xy2Gs3">
					<idno type="grant-number">2022YFC2504605</idno>
				</org>
				<org type="funding" xml:id="_v7WV3Yc">
					<idno type="grant-number">62202403</idno>
				</org>
				<org type="funding" xml:id="_JugPXCJ">
					<idno type="grant-number">PRP/034/22FX</idno>
					<orgName type="grant-name">Grant in Aid for Scientific Research</orgName>
				</org>
				<org type="funding" xml:id="_sYHZGCq">
					<idno type="grant-number">20KK0234</idno>
				</org>
				<org type="funding" xml:id="_AEWF9Sf">
					<idno type="grant-number">21H03470</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 45.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2199" to="2210" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clinical-grade computational pathology using weakly supervised deep learning on whole slide images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Campanella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1301" to="1309" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for evaluation of microvascular invasion in hepatocellular carcinoma from tumor areas of histology images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hepatol. Int</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="590" to="602" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling vision transformers to gigapixel images via hierarchical self-supervised learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16144" to="16155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning-based classification of hepatocellular nodular lesions on whole-slide histopathologic images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1948" to="1961" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Patch-based convolutional neural network for whole slide tissue image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Kurc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Saltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2424" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Label-efficient deep learning in medical image analysis: challenges and future directions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12484</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Eliceiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14318" to="14328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multiple instance learning via iterative self-paced supervised contrastive learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.09452</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smile: sparse-attention based multiple instance contrastive learning for glioma sub-type classification using pathological images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">MICCAI Workshop on Computational Pathology</title>
		<imprint>
			<biblScope unit="page" from="159" to="169" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">AI-based pathology predicts origins for cancers of unknown primary</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">594</biblScope>
			<biblScope unit="page" from="106" to="110" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data-efficient and weakly supervised computational pathology on whole-slide images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="555" to="570" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with expectationmaximization multi-instance learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58526-6_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58526-643" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12374</biblScope>
			<biblScope unit="page" from="729" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TransMIL: transformer based correlated multiple instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised driven consistency training for annotation efficient histopathology image analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Srinidhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">102256</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Instance-level label propagation with multi-instance learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2943" to="2949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">UD-MIL: uncertainty-driven deep multiple instance learning for oct image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3431" to="3442" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Whole slide image classification via iterative patch labelling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1408" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DTFD-MIL: double-tier feature distillation multiple instance learning for histopathology whole slide image classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18802" to="18812" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
