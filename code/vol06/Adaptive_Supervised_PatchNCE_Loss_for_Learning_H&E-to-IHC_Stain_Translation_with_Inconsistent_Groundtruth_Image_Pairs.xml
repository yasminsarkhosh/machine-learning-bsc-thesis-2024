<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Supervised PatchNCE Loss for Learning H&amp;E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fangda</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
							<email>huzhiqiang@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">Sensetime Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Chen</surname></persName>
							<email>chenwen@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">Sensetime Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Avinash</forename><surname>Kak</surname></persName>
							<email>kak@purdue.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Supervised PatchNCE Loss for Learning H&amp;E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="632" to="641"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">3FA8C61D02CC4F2534DF8D2E83B283D8</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Generative Adversarial Network â€¢ Contrastive Learning H&amp;E-to-IHC Stain Translation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Immunohistochemical (IHC) staining highlights the molecular information critical to diagnostics in tissue samples. However, compared to H&amp;E staining, IHC staining can be much more expensive in terms of both labor and the laboratory equipment required. This motivates recent research that demonstrates that the correlations between the morphological information present in the H&amp;E-stained slides and the molecular information in the IHC-stained slides can be used for H&amp;Eto-IHC stain translation. However, due to a lack of pixel-perfect H&amp;E-IHC groundtruth pairs, most existing methods have resorted to relying on expert annotations. To remedy this situation, we present a new loss function, Adaptive Supervised PatchNCE (ASP), to directly deal with the input to target inconsistencies in a proposed H&amp;E-to-IHC imageto-image translation framework. The ASP loss is built upon a patchbased contrastive learning criterion, named Supervised PatchNCE (SP), and augments it further with weight scheduling to mitigate the negative impact of noisy supervision. Lastly, we introduce the Multi-IHC Stain Translation (MIST) dataset, which contains aligned H&amp;E-IHC patches for 4 different IHC stains critical to breast cancer diagnosis. In our experiment, we demonstrate that our proposed method outperforms existing image-to-image translation methods for stain translation to multiple IHC stains. All of our code and datasets are available at https://github.com/ lifangda01/AdaptiveSupervisedPatchNCE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Immunohistochemical (IHC) staining is a widely used technique in pathology for visualizing abnormal cells that are often found in tumors. IHC chromogens highlight the presence of certain antigens or proteins by staining their corresponding antibodies. For instance, the HER2 (human epidermal growth factor receptor 2) biomarker is associated with aggressive breast tumor development and is essential in forming a precise treatment plan. Despite its capability to provide highly valuable diagnostic information, the process of IHC staining is very labor-intensive, time-consuming and requires specialized histotechnologists and laboratory equipments <ref type="bibr" target="#b1">[2]</ref>. Such factors hinder the general availability of IHC staining in histopathological applications.</p><p>At the other end of the spectrum, H&amp;E (Hematoxylin and Eosin) staining, as the gold standard in histological staining, highlights the tissue structures and cell morphology. In routine diagnostics, on account of its much lower cost, an H&amp;E-stained slide is prepared by pathologists in order to determine whether or not to also apply the IHC stains for a more precise assessment of the disease. Therefore, it is of great interest to have an algorithm that can automatically translate an H&amp;E-stained slide into one that could be considered to have been stained with IHC while accurately predicting the target expression levels.</p><p>To that end, researchers have recently proposed to use GAN-based Image-to-Image Translation (I2IT) algorithms for transforming H&amp;E-stained slides into IHC. Despite the progress, the outstanding challenge in training such I2IT frameworks is the lack of aligned H&amp;E-IHC image pairs, or in other words, the inconsistencies in the H&amp;E-IHC groundtruth pairs. To explain, since re-staining a slice is physically infeasible, a matching pair of H&amp;E-IHC slices are taken from two depth-wise consecutive cuts of the same tissue then stained and scanned separately. This inevitably prevents pixel-perfect image correspondences due to the slice-to-slice changes in cell morphology, staining-induced degradation (e.g. tissue-tearing), imaging artifacts that may vary among slices (e.g. camera out-offocus) and multi-slice registration errors. An example pair of patches is shown in Fig. <ref type="figure" target="#fig_0">1</ref> and another pair with significant inconsistencies is shown in Fig. <ref type="figure" target="#fig_1">2</ref>(a)(c). In the latter, comparing the groundtruth IHC image to the input H&amp;E image, one can clearly see the inconsistencies -nearly the entire left half of the tissue present in the H&amp;E image is missing.</p><p>As a result, recent advances in H&amp;E-to-IHC I2IT have mostly avoided using the inconsistent GT pairs and instead have imposed the cycle-consistency constraint <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref>. Moreover, existing approaches have also exploited using expert annotations such as per-cell labels <ref type="bibr" target="#b8">[9]</ref>, semantic masks <ref type="bibr" target="#b7">[8]</ref> and patch-level labels <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref>. As for the prior works that directly utilize the H&amp;E-IHC pairs for supervision, a variant of Pix2Pix <ref type="bibr" target="#b3">[4]</ref> that uses a Gaussian Pyramid based reconstruction loss to accommodate the noisy GT is proposed in <ref type="bibr" target="#b6">[7]</ref>. However, the robustness of such approaches that punish absolute errors in the generated image to dealing with GT inconsistencies remains unclear.</p><p>In this paper, we argue that the IHC slides, despite the disparities vis-a-vis their H&amp;E counterparts, can still serve as useful targets for stain translation. The work we present in this paper is based on the important realization that even when pairs of consecutive tissue slices do not yield images that are pixel-perfect aligned, it is highly likely that the corresponding patches in the two stains share the same diagnostic label. For example, if the levels of expression in a region of the HER2 slide are high, the corresponding region in the H&amp;E slide is highly likely to contain a high density of cancerous cells. Therefore, we set our goal to meaningfully leverage such correlations to benefit the H&amp;E-to-IHC I2IT while being resilient to any inconsistencies.</p><p>Toward this goal, we propose a supervised patchwise contrastive loss named the Adaptive Supervised PatchNCE (ASP) loss. Our formulation of this loss was inspired by the recent research findings that contrastive loss benefits model robustness under label noise <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>. Furthermore, based on the observation that any dissimilarity between the patch embeddings at corresponding locations in the generated and groundtruth IHC images is indicative to the level of inconsistency of the GT at that location, we employ an adaptive weighting scheme in ASP. By down-weighting the contrastive loss at locations with low similarities, i.e. high inconsistencies, our proposed ASP loss helps the network learn more robustly.</p><p>Lastly, to support further research in virtual IHC-restaining, we present the Multi-IHC Stain Translation (MIST) as a new public dataset. The MIST dataset contains 4k+ training and 1k testing aligned H&amp;E-IHC patches for each of the following IHC stains that are critical for breast cancer diagnostics: HER2, Ki67, ER (Estrogen Receptor) and PR (Progesterone Receptor). We evaluated existing I2IT methods and ours for multiple IHC stains and demonstrate the superior performance achieved by our method both qualitatively and quantitatively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Supervised PatchNCE (SP) Loss</head><p>Before getting to our ASP loss, we need to first introduce the SP loss as a robust means to learning from inconsistent GT image pairs. The SP loss was inspired by the findings in recent literature that demonstrate the positive effect of contrastive learning on boosting model robustness against label noise <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>. It takes the same form as the PatchNCE loss as introduced in <ref type="bibr" target="#b10">[11]</ref>, except that it is applied on the generated-GT image pair (instead of the input-generated pair).</p><p>The goal of the PatchNCE loss is to ensure the content is consistent across translation by maximizing the mutual information between the input and the corresponding output. It does so by minimizing a patch-based InfoNCE loss <ref type="bibr" target="#b9">[10]</ref>, which encourages the network to associate the corresponding patches with each other in the learned embedding space, while disassociating them from the noncorresponding ones. Mathematically, the InfoNCE loss takes the form:</p><formula xml:id="formula_0">L InfoNCE (v, v + , v -) = -log exp (v â€¢ v + /Ï„ ) exp (v â€¢ v + /Ï„ ) + N n=1 exp (v â€¢ v - n /Ï„ ) ,<label>(1)</label></formula><p>where v, v + and v -are the embeddings of the anchor, positive and negative samples, respectively. With InfoNCE, the PatchNCE loss is set up as follows:</p><p>given the anchor embedding áº‘Y of a patch in the output image, the positive z X is the embedding of the corresponding patch from the input image, while the negatives z X are embeddings of the non-corresponding ones, i.e.</p><formula xml:id="formula_1">L PatchNCE = L InfoNCE (áº‘ Y , z X , z X ).</formula><p>As for the SP loss, given the embedding of an output patch áº‘Y as anchor, we now designate the embedding of the corresponding patch in the groundtruth image z Y as the positive and the embeddings of the non-corresponding ones z Y as the negatives. We then use the same InfoNCE-based contrastive learning objective, i.e. L SP = L InfoNCE (áº‘ Y , z Y , z Y ). A depiction of both the PatchNCE loss and the SP loss is given in Fig. <ref type="figure" target="#fig_0">1</ref>. It is worth noting that, despite the fact that a similar patchwise constrastive loss was proposed in <ref type="bibr" target="#b0">[1]</ref> for supervised I2IT, it is one of our contributions in this paper to explicitly exploit the robustness of this contrastive loss in the context of H&amp;E-to-IHC translation where the GT pairs can be highly inconsistent for reasons mentioned previously. We think that the key factor behind the robustness of L SP to inconsistent GT compared to, say, the MSE loss, is its relativeness. Instead of using an absolute loss term that may not work well on inconsistent groundtruth pairs, L SP punishes dissimilarities between the anchor and the positive in the learned latent space, relative to those between the anchor and the negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Adaptive Supervised PatchNCE (ASP) Loss</head><p>To learn selectively from more consistent groundtruth locations, we further propose to augment the Supervised PatchNCE loss in an adaptive manner. The key idea here is to automatically recognize patch locations that are inconsistent and adapt the SP loss so that the severely inconsistent patch locations will have lesser effects on training. To measure the consistency at a given patch location, we use the cosine similarity between the embeddings of the generated IHC patch and the corresponding GT patch. In Fig. <ref type="figure" target="#fig_1">2</ref>, we show an example pair of generated vs GT IHC images that contain significant inconsistencies and their anchor-positive similarity heat map. For pairs of embeddings produced by a trained network, a high similarity value indicates good correspondence between the groundtruth patches while a low similarity value indicates inconsistencies.</p><p>Directly motivated by this observation, we first propose a weighting scheme for the SP loss. More specifically, we assign lower weights to patch locations that have low anchor-positive similarity values to alleviate the negative impacts the inconsistent targets may have on training. At training time t, the weight is a function of the anchor-positive cosine similarity. Examples of the weight function h(â€¢) are shown in Fig. <ref type="figure" target="#fig_2">3(b)</ref>. The weight functions are monotonic increasing so that the more confident patch locations are always treated with more importance.</p><p>In order to make the weighting scheme work in practice, we must also account for the phase of training. The intuition is that, during the initial phase of training, the network is not going to be able to discriminate between consistent patch locations from those that are inconsistent. Additionally, as shown in Fig. <ref type="figure" target="#fig_2">3(a)</ref>, the histograms of the anchor-positive similarity evolve rather slowly over the training epochs. Therefore, it would not make sense to reinforce the weighting function in the beginning of the training as much as near the end of the training.</p><p>To that end, we further augment the weight so that it is also a function of the training iterations. Such scheduling of the weights is done so that in the beginning of the training, the weights are uniform in order not to wrongly bias the network when the embeddings are still indiscriminative. And as training progresses, the selective weighting scheme is gradually enforced so that the inconsistent patch locations are treated with reduced weights. We call this gradual process of shifting the learning focus weight scheduling. Let t denote the current iteration and T the total number of training iterations. Then weight scheduling is achieved by using a scheduling function g( t T ). Various options of g(â€¢) are shown in Fig. <ref type="figure" target="#fig_2">3(c</ref>). Subsequently, combining the weighting function with the scheduling function, we can write the following formula for the final weight:</p><formula xml:id="formula_2">w t (v, v + ) = 1 -g t T Ã— 1.0 + g t T Ã— h(v â€¢ v + ). (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>We refer to the new augmented Supervised PatchNCE loss as the Adaptive Supervised PatchNCE (ASP) loss, which can be expressed as:   where W l t = s w l,s t is a normalization factor that maintains the total magnitude of the loss after applying the weights. Finally, the overall learning objective for our generator is as follows:</p><formula xml:id="formula_4">L ASP (G, H, X, Y, t) = E (x,y )âˆ¼(X,Y ) L l=1 S l s=1 w t (áº‘ l,s Y , z l,s Y ) W l t â€¢L InfoNCE (áº‘ l,s Y , z l,s Y , z l,s Y ),<label>(3)</label></formula><formula xml:id="formula_5">L adv + Î» PatchNCE L PatchNCE + Î» ASP L ASP + Î» GP L GP , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where L GP is the Gaussian Pyramid based reconstruction loss from <ref type="bibr" target="#b6">[7]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets. The following datasets are used in our experiments: the Breast Cancer Immunohistochemical (BCI) challenge dataset <ref type="bibr" target="#b6">[7]</ref> and our own MIST dataset that is now in the public domain. The publicly available portion of BCI contains 3396 H H&amp;E-HER2 patches for training and 500 of the same for testing. Note that we have additionally normalized the brightness levels of all BCI images to the same level. Due to the page limit, from the MIST dataset, here we only present detailed results on HER2 and ER. For MIST HER2 , we extracted 4642 paired patches for training and 1000 for testing from 64 WSIs. And for MIST ER , we extracted 4153 patches for training, and 1000 for testing from 56 WSIs. All WSIs were taken at 20Ã— magnification. All patches are of size 1024 Ã— 1024 and non-overlapping. Additional results on MIST Ki67 and MIST PR are provided in the Supplementary Materials.</p><p>Implementation Details. For all of our models, we used ResNet-6Blocks as the generator and a 5-layer PatchGAN as the discriminator. We trained our networks with random 512 Ã— 512 crops and a batch size of one. The Adam optimizer <ref type="bibr" target="#b4">[5]</ref> was used with a linear decay scheduler (as shown in Fig. <ref type="figure" target="#fig_2">3(c)</ref>) and an initial learning rate of 2 Ã— 10 -4 . The hyperparameters in Eq. ( <ref type="formula" target="#formula_5">4</ref>) are set as: Î» PatchNCE = 10.0, Î» ASP = 10.0 and Î» GP = 10.0.</p><p>Evaluation Metrics. We compare the methods using both paired and unpaired evaluation metrics. To compare a pair of images, generated and groundtruth, we use the standard SSIM (Structural Similarity Index Measure) and PHV (Perceptual Hash Value) as described in <ref type="bibr" target="#b7">[8]</ref>. As for the unpaired metrics, we use the FID (FrÃ©chet Inception Distance) and the KID (Kernel Inception Distance).</p><p>Qualitative Evaluations. In Fig. <ref type="figure" target="#fig_3">4</ref>, we compare visually the generated IHC images by our framework. It can be observed that by using either L SP or L ASP , the pathological representations in the generated images are significantly more accurate. And by using L ASP , such representations appear to be more consistent. Quantitative Evaluations. The full results comparing existing I2IT methods to ours are tabulated in Tab. 1. Overall, it can be observed that the proposed framework with the ASP loss consistently outperforms existing methods across all three datasets. For those methods, Fig. <ref type="figure" target="#fig_4">5</ref> visually illustrates the extent of hallucinations which we believe is the reason for their poor quantitative performance. Subsequently, in Tab.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we have proposed the Adaptive Supervised PatchNCE (ASP) loss for learning H&amp;E-to-IHC stain translation with inconsistent GT image pairs. The adaptive logic in ASP is based on the intuition that inconsistent patch locations should contribute less to learning. We demonstrated that our proposed framework is able to achieve significant improvements both qualitatively and quantitatively over the existing approaches for translations to multiple IHC stains. Finally, we have made public our Multi-IHC Stain Translation dataset with the hope to assist further research towards accurate H&amp;E-to-IHC stain translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the PatchNCE loss from [11] and the Supervised PatchNCE (SP) loss. The patch embeddings z are extracted by a shared network F .</figDesc><graphic coords="3,61,80,357,71,300,91,199,57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Input H&amp;E image x, (b) generated IHC image Å·, (c) groundtruth IHC image y, and (d) heat map of the anchor-positive cosine similarities produced by a trained network at corresponding locations: Cs = z s Å· â€¢ z s y , where s is index of the spatial location.</figDesc><graphic coords="6,70,98,58,19,310,87,84,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Histograms of the anchor-positive similarity values at different epochs; (b) The weight h(â€¢) as a function of Cs; (c) The scheduling function g(â€¢).</figDesc><graphic coords="6,70,47,214,28,311,23,103,30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Left to right: (a) Input H&amp;E image; (b) Groundtruth IHC image; (c) Generated image without LSP; (d) With LSP; (e) With L (lambda,linear) ASP .</figDesc><graphic coords="6,58,98,368,12,334,54,205,09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example H&amp;E-to-HER2 translations by three baselines. False morphological alterations cause the tissue structures in the translated images to no longer match those in the input H&amp;E image, especially the nuclei. Zoom in for sharper visualization.</figDesc><graphic coords="9,43,29,53,72,337,33,88,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluations on all three datasets using both paired and unpaired metrics. The best values are highlighted. Ours is with L Note that KID values multiplied by 1000 are shown. CUT is from<ref type="bibr" target="#b10">[11]</ref> </figDesc><table><row><cell>(lambda,linear) ASP</cell><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies comparing different adaptive strategies for the SP loss. The best and second best values are highlighted. Note that LSP is L</figDesc><table><row><cell>(zero,uniform) ASP</cell><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2, we further provide results using different weighting and scheduling functions in our proposed L ASP . With L SP already being a strong baseline, using different adaptive strategies can provide further gains in performance. It is also worth noting that if reinforced prematurely, adaptive weighting can lead to inferior convergence, e.g. L</figDesc><table><row><cell>(linear,top) ASP</cell><cell>.</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 61.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contrastive feature loss for image prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1934" to="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Can pathology diagnostic services for cancer be stratified and serve global health?</title>
		<author>
			<persName><forename type="first">F</forename><surname>Anglade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Milner</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Brock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="2431" to="2438" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contrastive learning improves model robustness under label noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2703" to="2708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unpaired multi-domain stain transfer for kidney histopathological images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1630" to="1637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bci: breast cancer immunohistochemical image generation through pyramid pix2pix</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1815" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unpaired stain transfer using pathology-consistent constrained generative adversarial networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1977" to="1989" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predict ki-67 positive cells in h&amp;e-stained images using deep learning independently from ihc-stained images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Mol. Biosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">183</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58545-7_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58545-719" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12354</biblScope>
			<biblScope unit="page" from="319" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Investigating why contrastive learning benefits robustness against label noise</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Whitecross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="24851" to="24871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised pr virtual staining for breast histopathological images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-723" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contrast to divide: self-supervised pre-training for learning with noisy labels</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1657" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
