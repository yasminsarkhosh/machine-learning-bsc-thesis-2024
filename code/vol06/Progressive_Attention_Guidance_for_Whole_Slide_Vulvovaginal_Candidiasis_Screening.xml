<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiangdong</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Honglin</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maosong</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luyan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lichi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
							<email>qianwang@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="233" to="242"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">713921C614B9D4A69852E941F30F656E</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_23</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Whole slide image</term>
					<term>Vulvovaginal Candidiasis</term>
					<term>Attention-Guided</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vulvovaginal candidiasis (VVC) is the most prevalent human candidal infection, estimated to afflict approximately 75% of all women at least once in their lifetime. It will lead to several symptoms including pruritus, vaginal soreness, and so on. Automatic whole slide image (WSI) classification is highly demanded, for the huge burden of disease control and prevention. However, the WSI-based computer-aided VCC screening method is still vacant due to the scarce labeled data and unique properties of candida. Candida in WSI is challenging to be captured by conventional classification models due to its distinctive elongated shape, the small proportion of their spatial distribution, and the style gap from WSIs. To make the model focus on the candida easier, we propose an attention-guided method, which can obtain a robust diagnosis classification model. Specifically, we first use a pre-trained detection model as prior instruction to initialize the classification model. Then we design a Skip Self-Attention module to refine the attention onto the fined-grained features of candida. Finally, we use a contrastive learning method to alleviate the overfitting caused by the style gap of WSIs and suppress the attention to false positive regions. Our experimental results demonstrate that our framework achieves state-of-the-art performance. Code and example data are available at https://github.com/caijd2000/ MICCAI2023-VVC-Screening.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vulvovaginal candidiasis (VVC) is a type of fungal infection caused by candida, which results in discomforting symptoms, including itching and burning in the genital area <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref>. It is the most prevalent human candidal infection, estimated to afflict approximately 75% of all women at least once in their lifetime <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>, resulting in huge consumption of medical resources. Currently, thin-layer cytology (TCT) <ref type="bibr" target="#b5">[6]</ref> is one of the main tools for screening cervical abnormalities. Manual reading upon whole slide image (WSI) of TCT is time-consuming and labor-intensive, which limits the efficiency and scale of disease screening. Therefore, automatic computer-aided screening for candida would be a valuable asset, which is low-cost and effective in the fight against infection.</p><p>Previous studies for computer-aided VVC diagnosis were mainly based on pap smears rather than WSIs. For example, Momenzadeh et al. <ref type="bibr" target="#b10">[11]</ref> implemented automatic diagnosis based on machine learning. Peng et al. <ref type="bibr" target="#b12">[13]</ref> compared different CNN models on VVC classification. Some works also applied deep learning to classify candida in other body parts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24]</ref>. In recent years, TCT has become mainstream in cervical disease screening compared to pap smear <ref type="bibr" target="#b7">[8]</ref>. Many systems of automatic computer-aided WSI screening have been designed for cytopathology <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, and histopathology <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>. However, partially due to the limited data and annotation, screening for candidiasis is mostly understudied.</p><p>Computer-aided diagnosis for candidiasis through WSI is highly challenging (see examples in Fig. <ref type="figure" target="#fig_0">1</ref>). (1) Candida is hard to localize in a large WSI, especially due to its long-stripe shape, low-contrast appearance, and often occlusion with respect to nearby cells. The representation of candida is easily dominated by other objects in deep layers of a network. (2) In addition to occupying only a small image space for each candida, the overall candida quantity in WSIs is also low compared to the number of other cells. The class imbalance makes it difficult to conduct discriminative learning and to find candida. (3) The staining of different samples leads to the huge style gap between WSIs. While collecting more candida data may contribute to a more robust network, such efforts are dwarfed by the inhomogeneity of WSIs, which adds to the risk of overfitting. All of the above issues make it difficult for diagnostic models to focus on candida, thus resulting in poor classification performance and generalization capability.</p><p>In this paper, we find that the attention for a deep network to focus on candida is the key to the high performance of the screening task. And we propose a series of strategies to make the model focus on candida progressively. Our contributions are summarized into three parts: <ref type="bibr" target="#b0">(1)</ref> We use a detection task to pre-train the encoder of the classification model, moving the network's attention away from individual cells and onto candida-like objects; <ref type="bibr" target="#b1">(2)</ref> We propose skip self-attention (SSA) to take into account multi-scale semantic and texture fea- tures, improving network attention to the candida that is severely occluded or with long hyphae; (3) Contrastive learning <ref type="bibr" target="#b2">[3]</ref> is applied to alleviate the overfitting risk caused by the style gap and to improve the ability to discern candida.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We use a hierarchical framework for cervical candida screening, concerning the huge size of WSI and the infeasibility of handling a WSI scan in one shot. The overall pipeline of our framework is presented in Fig. <ref type="figure" target="#fig_1">2</ref>. Given a WSI, we first crop it into multiple images, each of which is sized 1024×1024. For each cropped image, we conduct image-level classification to find out whether it suffers from suspicious candida infection. The image-level classifier produces a score and feature representation of the image under consideration. Then scores and features from all cropped images are reorganized and aggregated by a transformer for final classification by a fully connected (FC) layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Detection Task for Pre-training</head><p>We use a pre-trained detection model as prior to initialize the classification model. In experimental exploration, we find that, if we train the detection network directly, the bounding-box annotation indicates the location of candida and can rapidly establish a rough understanding of the morphology of candida. However, the positioning task coming with detection lacks enough granularity, resulting in relatively low precision to discern cell edges or folding from candida. Meanwhile, directly training a classification model is usually easier to converge. However, in such a task, as candida occupies only a few pixels in an image, it is difficult for the classifier to focus on the target. That, the attention of the classifier may spread across the entire image, leading to overfitted training quickly.</p><p>Therefore, we argue that the detection and classification tasks are complementary to solve our problem. Particularly, we pre-train a detector and inherit its advantages in the classifier. We use Retinanet <ref type="bibr" target="#b9">[10]</ref>, which is composed of a backbone attached with FPN (Feature Pyramid Network, FPN, <ref type="bibr" target="#b8">[9]</ref>) and a detection head, as shown in (Fig. <ref type="figure">3</ref>). We chose the same encoder architecture (Resnet <ref type="bibr" target="#b4">[5]</ref>) for the detection and classification networks. To train the encoder with the detection task (Fig. <ref type="figure">3</ref>), we use bounding-box annotations to supervise Fig. <ref type="figure">3</ref>. Attention Guided Image-level Classification (corresponding to the classification model in Fig. <ref type="figure" target="#fig_1">2</ref>). The same parameters are used between modules marked "shared".</p><p>Retinanet. We then initialize the classification network by directly loading the encoder parameters and freezing the first few layers during the training of the classification network. Note that pre-training not only discards the complex positioning task but also makes it easier for the classification network to converge especially in the early stage of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer with Skip Self-Attention (SSA)</head><p>We design a novel skip self-attention (SSA) module to fuse discriminative features of candida from different scales. At a fine-grained level, the hyphae and spores of candida are usually the basis for judging. Yet we need to distinguish them from easily distorting factors such as contaminants in WSIs and edges of nearby cells. At a coarse-grained level, there is the phenomenon that a candida usually links multiple host cells and yields a string of them. Thus it is necessary to combine long-range visual cues that span several cells to derive the decision related to candida.</p><p>CNN-based methods have achieved excellent performance in computer-aided diagnosis including cervical cancer <ref type="bibr" target="#b21">[22]</ref>. However, the unique shape and appearance of the candidate incur troubles for CNN-based classifiers, whose spatial field of view can be relatively limited. In recent years, vision transformer (ViT) has been widely used in visual tasks for its global attention mechanism <ref type="bibr" target="#b13">[14]</ref>, sensitivity to shape information in images <ref type="bibr" target="#b18">[19]</ref>, and robustness to occlusion <ref type="bibr" target="#b11">[12]</ref>. Nevertheless, such a transformer can be hard to train for our task, due to the large image size, huge network parameters, and huge demand for training data. Therefore, to adapt to the shape and appearance of candida, we propose the SSA module and apply it to ViT for efficient learning. Specifically, we use the pre-trained CNN-based encoder to extract features for each cropped image. The feature maps extracted after the first layer is considered low-level, which contains fine-grained texture information. On the contrary, the feature maps extracted from the last layer are high-level, which represents semantics regarding candida. To combine the low-and high-level features, we regard the low-level features as queries (Q), and the high-level features as keys (K) and values (V). For each patch in the ViT scheme, the transformer decoder computes the attention between low-and high-level features and combines them. The class token 'CLS' is used for the final classification. The combined feature maps can offer more representative information so that the classifier focuses more on different scales to long-range candida. Meanwhile, the extra SSA structure is simple, which causes a low computation burden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contrastive Learning</head><p>As mentioned in Sect. 1, the style gap is another problem, which makes overfitting more severe. In this part, we adopt the strategy of contrastive learning to alleviate such problems and further optimize the attention of the network. Our approach has two key goals: (1) to ensure that the features from the original image remain consistent after undergoing various image augmentations, and (2) to construct an image without the region of candida, resulting in highly dissimilar features compared to the original.</p><p>Inspired by a weakly supervised learning segmentation method <ref type="bibr" target="#b6">[7]</ref>, we construct a contrastive learning method, which will be described in detail in the following sections, as shown in Fig. <ref type="figure">3</ref>.</p><p>To achieve this, we use augmentation and the attention map generated during the training process to construct three types of images and apply contrastive learning to the features extracted from them. For a given image I, we use image augmentation to generate I aug and use the encoder attached with SSA to extract feature, F c aug . The attention map A is transformed from F c aug by an attention extractor. The attention extractor uses FC (the same params as that of the classifier) to reduce the channels of features (except the class token) to 2 (candida and others), then reshape the features representing candida to a feature map, and applies bilinear interpolation to upsample it to the same size of I. Equation 1 normalizes A to the interval [0,1], obtaining M to represent the likelihood of candida distribution. We get the masked image I masked by subtracting M from I.</p><formula xml:id="formula_0">M = 1 1 + e -s(A-σ) ,<label>(1)</label></formula><p>where σ and s are used to adjust the range of values, set to 0.5 and 10.</p><p>As shown in Fig. <ref type="figure">3</ref>, the features F , F aug and F masked from the three types of images I, I aug and I masked by the shared classifier. In our task, we hope that the style gap does not affect the feature extraction of the image, so the distance between F aug and F should be attracted. At the same time, we hope that F masked should not contain the characteristics of candida, which is repelled from F aug . To achieve our goal, we introduce triplet loss <ref type="bibr" target="#b14">[15]</ref> for contrastive learning as shown in the first part of Eq. 2.</p><p>In addition, we use two constraints, leading to more stable and robust training. If our network has effective attention, the masked image should not contain any candida, so the score of the Candida category after the mask S(I masked ) should be minimized. We use the attention mining loss to handle this, as shown in the second part of Eq. 2. Additionally, we need the attention to cover only the partial area around candida, without false positive regions. Otherwise, attention maps that cover the whole image can also result in low L tri . We take the average grayscale of attention map M as a restriction, as shown in the last part of Eq. 2. L tri ,L am , and L focus are combined as L cl to constrain each other and take full advantage of contrastive learning, as shown in Eq. 2.</p><formula xml:id="formula_1">L cl = L tri + L am + L focus = T riplet(F aug , F orig , F masked ) + S(I masked ) + M . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Finally, we use the cross-entropy loss L ce to calculate the classification loss with labels. The total loss during training can be expressed as shown in Eq. 3. α is a hyper-parameter, set to 0.1.</p><formula xml:id="formula_3">L = L ce (S(I aug ), labels) + αL cl .</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Aggregated Classification for WSI</head><p>With the strategies above, we have built the classifier for all cropped images in a WSI. Then we can finish the pipeline of WSI-level classification, which is shown as part of Fig. <ref type="figure" target="#fig_1">2</ref>. Specifically, for each cropped image, we conduct image-level classification to find out whether it suffers from suspicious candida infection.</p><p>The image-level classification also produces a score, as well as the feature representation of the image under consideration. Then, we reorganize features from all images by ranking their scores and preserving that with top-k scores. We complete the aggregation of the top-k features by the transformer and make the WSI-level decision by an FC layer in the final.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>Datasets and Experimental Setup. Our samples were collected by a collaborating clinical institute in 2021. Each sample is scanned into a WSI following standard cytology protocol, which can be further cropped to 500 images sized 1024×1024.</p><p>For pre-training the detector, we prepare 1467 images with the size of 1024×1024 pixels, all of which have bounding-box annotations. The ratio of training and validation is 4:1.</p><p>For training of the image-level classification model, we use 1940 positive images (1467 of which are used in detector pre-training) and 2093 negative images. All images used to pre-train the detector are categorized as training data here. The rest 473 images are split in 5-fold cross-validation, from which we collect experimental results and report later. The ratio of training, validation, and testing is 3:1:1. At the WSI level, we use two datasets. Dataset-Small is balanced with 100 positive WSIs and 100 negative WSIs. We conduct a 5-fold cross-validation, and the ratio of training, validation, and testing is 3:1:1. We further validate upon an imbalanced Dataset-Large of 7654 WSIs. There are only 140 positive WSIs in this dataset, which is closer to real world. These two WSI-level datasets have no overlay with the data used to train the above detection and classification tasks.</p><p>For implementation details, the models are implemented by PyTorch and trained on 4 Nvidia Tesla V100S GPUs. All parameters are optimized by Adam for 100 epochs with the initial learning rate of 3 × 10 -4 . The batch sizes of the detection task, image-level classification, and WSI-level classification are 8, 8, 16, respectively. To aggregate WSI classification, we use top-10 cropped images and their features. We report the performance using five common metrics: area under the receiver operating characteristic curve (AUC), accuracy (ACC), sensitivity (Sen), specificity (Spe), and F1-score.</p><p>Comparisons for Image-Level Classification. We conduct an ablation study to evaluate the contribution of pre-training (PT), skip self-attention (SSA), and contrastive learning (CL) for the image-level classification, as shown in Table <ref type="table" target="#tab_0">1</ref>. It is observed that with all our proposed components, the network reaches the highest AUC 97.20, which is 11.49% higher than the baseline. PT shows improvement in all situations, as a reasonable initial focus provides a solid foundation. SSA and CL can bring 2.89% and 6.38% improvement respectively compared to the method without each of them. It shows that SSA and CL can perform better when the model already has the basic ability to localize candida, i.e., after PT.</p><p>To verify whether our model focuses on important regions of the input image for accurate classification, we visualize the model's attention using Grad-CAM <ref type="bibr" target="#b15">[16]</ref>. We present two examples in Fig. <ref type="figure" target="#fig_2">4</ref>. We can see in Fig. <ref type="figure" target="#fig_2">4</ref>(b) that the baseline's attention is very scattered spatially. After PT, the model can focus on the candida area, edges of cells, and folds that resemble candida, as shown in Fig. <ref type="figure" target="#fig_2">4(c</ref>). After adding the SSA module, more texture information is used to distinguish with cells, as shown in Fig. <ref type="figure" target="#fig_2">4(d)</ref>. Finally, CL helps the model better narrow its attention, focusing on the most important part as shown in Fig. <ref type="figure" target="#fig_2">4(e)</ref>. These comparisons demonstrate that our proposed method effectively guides and corrects the model's attention.</p><p>Comparisons for WSI-Level Classification. We compare our proposed method to other methods in the whole slide of cervical disease screening. To save computation, we did not verify the performance of the methods that performed too poorly on Dataset-Small. The detection-based method <ref type="bibr" target="#b22">[23]</ref> uses a detection network to get suspicious candida and classify WSIs with average confidence. Resnet trained without our method is the same as the baseline in Table <ref type="table" target="#tab_0">1</ref>. At the WSI level, we compare our method with traditional classifiers and a multiinstance learning method TransMIL <ref type="bibr" target="#b16">[17]</ref>. We both considered the original Trans-MIL with pre-trained Resnet-50 and the modified version with our image-level encoder. Table <ref type="table" target="#tab_1">2</ref> shows that our method reaches the highest AUC of 95.78% and is the most stable. Our attention-based method brings 6% improvement of accuracy on Data-Small compared to other methods with the same WSI-level method 'Threshold'. Transformer shows a better capacity of feature aggregation than other WSI-level classifiers, raising the AUC on Dataset-Large to 84.18%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We introduced a novel attention-guided method for VVC screening, which can progressively correct the attention of the model. We pre-train a detection task for the initialization, then add SSA to fuse features from coarse and fine-grained, and finally narrower attention with contrastive learning. After obtaining accurate attention and good generalization for the image-level classifier, we reorganized and ensemble features from slices, and make a diagnosis. Both numerical metrics and visualization results show the effectiveness of our model. In the future, we would like to explore the method of weakly supervised learning to make use of a huge number of unlabeled images and jointly train the image-level and WSI-level models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of WSIs (usually about 20000 × 20000 pixels), a cropped image of 1024 × 1024 pixels from the WSI, and zoom-in views of candida and its position (indicated by the red arrows and annotation). (Color figure online)</figDesc><graphic coords="2,49,80,53,90,324,19,85,81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The pipeline of proposed WSI-based VVC screening system.</figDesc><graphic coords="3,57,48,53,69,337,36,74,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) The original image, where the green box indicates candida (enlarged in the left) and the red box shows the prediction of the detection model. Other figures are Grad-CAM of (b) baseline, (c) baseline+PT, (d) baseline+PT+SSA, (e) base-line+PT+SSA+CL. (Color figure online)</figDesc><graphic coords="8,44,79,53,75,334,48,124,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,44,79,54,59,334,48,192,10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Image-level classification results and ablation study on the three contributions of our method. (PT: pre-training; SSA: skip self-attention; CL: contrastive loss). ± 2.14 79.79 ± 2.25 83.20 ± 2.35 76.55 ± 3.90 79.63 ± 1.66 87.24 ± 2.19 85.28 ± 1.45 90.88 ± 1.98 80.14 ± 3.07 85.43 ± 0.90 84.97 ± 3.78 81.55 ± 2.59 87.30 ± 3.05 76.27 ± 4.74 81.81 ± 2.63 87.69 ± 3.77 86.54 ± 0.98 89.59 ± 0.33 83.54 ± 1.80 86.85 ± 0.89 89.44 ± 3.27 89.32 ± 0.96 93.41 ± 1.40 85.56 ± 1.27 89.21 ± 1.32 94.31 ± 1.75 92.54 ± 2.51 93.85 ± 3.21 91.22 ± 3.36 92.27 ± 2.58 90.82 ± 2.57 89.84 ± 0.84 92.31 ± 1.41 87.54 ± 1.49 89.56 ± 1.27 97.20 ± 1.46 93.89 ± 1.20 92.35 ± 1.75 95.22 ± 0.96 93.47 ± 1.18</figDesc><table><row><cell>PT SSA CL AUC</cell><cell>ACC</cell><cell>Sen</cell><cell>Spe</cell><cell>F1</cell></row><row><cell>85.71</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparision of different methods for WSI classification. ± 3.71 89.50 ± 3.67 87.99 ± 7.55 80.46 86.33 67.85 Ours Threshold 92.50 ± 5.36 86.00 ± 6.44 83.04 ± 8.71 82.59 88.14 64.29 Ours MLP 94.36 ± 3.50 91.00 ± 6.44 85.11 ± 13.16 83.40 87.32 67.86 Ours TransMIL 95.35 ± 1.48 91.00 ± 3.21 85.74 ± 4.25 81.19 86.78 62.86 Ours Transformer 95.78 ± 2.25 91.64 ± 3.17 85.55 ± 5.91 84.18 87.67 68.57</figDesc><table><row><cell>Method</cell><cell></cell><cell>Dataset-Small</cell><cell></cell><cell></cell><cell>Dataset-Large</cell></row><row><cell cols="2">Image-level WSI-level</cell><cell>AUC</cell><cell>ACC</cell><cell>Sen</cell><cell>AUC ACC Sen</cell></row><row><cell cols="2">Detection Threshold</cell><cell cols="4">88.57 ± 9.56 80.00 ± 10.0 79.03 ± 14.4 \</cell><cell>\</cell><cell>\</cell></row><row><cell>Resnet</cell><cell>Threshold</cell><cell cols="4">88.75 ± 6.58 77.50 ± 9.35 82.17 ± 13.0 \</cell><cell>\</cell><cell>\</cell></row><row><cell>Resnet</cell><cell>TransMIL</cell><cell>93.85</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Estimation of direct healthcare costs of fungal diseases in the united states</title>
		<author>
			<persName><forename type="first">K</forename><surname>Benedict</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Beer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Infect. Dis</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1791" to="1797" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A deep learning approach to capture the essence of candida albicans morphologies</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bettauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microbiol. Spectr</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1472" to="1494" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vulvovaginal candidiasis: epidemiology, microbiology and risk factors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Azeredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Crit. Rev. Microbio</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="905" to="927" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The papanicolaou test for cervical cancer detection: a triumph and a tragedy</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Koss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="737" to="743" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tell me where to look: guided attention inference network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9215" to="9223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The diagnostic accuracy of TCT+ HPV-DNA for cervical cancer: systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Transl. Med</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">761</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic diagnosis of vulvovaginal candidiasis from pap smear images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Momenzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sehhati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehri Dehnavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rabbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Microsc</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="308" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intriguing properties of vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="23296" to="23308" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficiently recognition of vaginal micro-ecological environment based on convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on E-health Networking, Application &amp; Services (HEALTHCOM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12116" to="12128" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FaceNet: a unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07450</idno>
		<title level="m">Grad-CAM: Why did you say that? arXiv preprint</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TransMIL: transformer based correlated multiple instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vulvovaginal candidosis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Sobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="issue">9577</biblScope>
			<biblScope unit="page" from="1961" to="1971" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Tuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07197</idno>
		<title level="m">Are convolutional neural networks or transformers more like human vision? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vulvovaginal candidiasis: a current understanding and burning questions</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Fungi</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DTFD-MIL: double-tier feature distillation multiple instance learning for histopathology whole slide image classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18802" to="18812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Whole slide cervical cancer screening using graph attention network and supervised contrastive learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-7_20" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022. MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical pathology screening for cervical abnormality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">101892</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning approach to describe and classify fungi microscopic images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zieliński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sroka-Oleksiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rymarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piekarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brzychczy-Włoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">234806</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
