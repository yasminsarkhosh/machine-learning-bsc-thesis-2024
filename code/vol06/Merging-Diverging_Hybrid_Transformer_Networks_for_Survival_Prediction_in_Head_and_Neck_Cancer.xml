<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer</title>
				<funder ref="#_fVvPX9h">
					<orgName type="full">Australian Research Council</orgName>
					<orgName type="abbreviated">ARC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingyuan</forename><surname>Meng</surname></persName>
							<idno type="ORCID">0000-0002-9562-1613</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Translational Medicine</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Lei</forename><surname>Bi</surname></persName>
							<email>lei.bi@sjtu.edu.cn</email>
							<idno type="ORCID">0000-0001-9759-0200</idno>
							<affiliation key="aff1">
								<orgName type="department">Institute of Translational Medicine</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Fulham</surname></persName>
							<idno type="ORCID">0000-0003-0602-6319</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Molecular Imaging</orgName>
								<orgName type="institution">Royal Prince Alfred Hospital</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dagan</forename><surname>Feng</surname></persName>
							<idno type="ORCID">0000-0002-3381-214X</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Med-X Research Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinman</forename><surname>Kim</surname></persName>
							<idno type="ORCID">0000-0001-5960-1060</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="400" to="410"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2D906828E4C818D7226D82D1D2A0A916</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_39</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Survival Prediction</term>
					<term>Transformer</term>
					<term>Head and Neck Cancer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Survival prediction is crucial for cancer patients as it provides early prognostic information for treatment planning. Recently, deep survival models based on deep learning and medical images have shown promising performance for survival prediction. However, existing deep survival models are not well developed in utilizing multi-modality images (e.g., PET-CT) and in extracting regionspecific information (e.g., the prognostic information in Primary Tumor (PT) and Metastatic Lymph Node (MLN) regions). In view of this, we propose a mergingdiverging learning framework for survival prediction from multi-modality images. This framework has a merging encoder to fuse multi-modality information and a diverging decoder to extract region-specific information. In the merging encoder, we propose a Hybrid Parallel Cross-Attention (HPCA) block to effectively fuse multi-modality features via parallel convolutional layers and cross-attention transformers. In the diverging decoder, we propose a Region-specific Attention Gate (RAG) block to screen out the features related to lesion regions. Our framework is demonstrated on survival prediction from PET-CT images in Head and Neck (H&amp;N) cancer, by designing an X-shape merging-diverging hybrid transformer network (named XSurv). Our XSurv combines the complementary information in PET and CT images and extracts the region-specific prognostic information in PT and MLN regions. Extensive experiments on the public dataset of HEad and neCK TumOR segmentation and outcome prediction challenge (HECKTOR 2022) demonstrate that our XSurv outperforms state-of-the-art survival prediction methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Head and Neck (H&amp;N) cancer refers to malignant tumors in H&amp;N regions, which is among the most common cancers worldwide <ref type="bibr" target="#b0">[1]</ref>. Survival prediction, a regression task that models the survival outcomes of patients, is crucial for H&amp;N cancer patients: it provides early prognostic information to guide treatment planning and potentially improves the overall survival outcomes of patients <ref type="bibr" target="#b1">[2]</ref>. Multi-modality imaging of Positron Emission Tomography -Computed Tomography (PET-CT) has been shown to benefit survival prediction as it offers both anatomical (CT) and metabolic (PET) information about tumors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Therefore, survival prediction from PET-CT images in H&amp;N cancer has attracted wide attention and serves as a key research area. For instance, HEad and neCK TumOR segmentation and outcome prediction challenges (HECKTOR) have been held for the last three years to facilitate the development of new algorithms for survival prediction from PET-CT images in H&amp;N cancer <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>.</p><p>Traditional survival prediction methods are usually based on radiomics <ref type="bibr" target="#b7">[8]</ref>, where handcrafted radiomics features are extracted from pre-segmented tumor regions and then are modeled by statistical survival models, such as the Cox Proportional Hazard (CoxPH) model <ref type="bibr" target="#b8">[9]</ref>. In addition, deep survival models based on deep learning have been proposed to perform end-to-end survival prediction from medical images, where pre-segmented tumor masks are often unrequired <ref type="bibr" target="#b9">[10]</ref>. Deep survival models usually adopt Convolutional Neural Networks (CNNs) to extract image features, and recently Visual Transformers (ViT) have been adopted for its capabilities to capture long-range dependency within images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. These deep survival models have shown the potential to outperform traditional survival prediction methods <ref type="bibr" target="#b12">[13]</ref>. For survival prediction in H&amp;N cancer, deep survival models have achieved top performance in the HECKTOR 2021/2022 and are regarded as state-of-the-art <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. Nevertheless, we identified that existing deep survival models still have two main limitations.</p><p>Firstly, existing deep survival models are underdeveloped in utilizing complementary multi-modality information, such as the metabolic and anatomical information in PET and CT images. For survival prediction in H&amp;N cancer, existing methods usually use single imaging modality <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> or rely on early fusion (i.e., concatenating multi-modality images as multi-channel inputs) to combine multi-modality information <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b18">19]</ref>. In addition, late fusion has been used for survival prediction in other diseases such as gliomas and tuberculosis <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, where multi-modality features were extracted by multiple independent encoders with resultant features fused. However, early fusion has difficulties in extracting intra-modality information due to entangled (concatenated) images for feature extraction, while late fusion has difficulties in extracting inter-modality information due to fully independent feature extraction. Recently, Tang et al. <ref type="bibr" target="#b21">[22]</ref> attempted to address this limitation by proposing a Multi-scale Non-local Attention Fusion (MNAF) block for survival prediction of glioma patients, in which multi-modality features were fused via non-local attention mechanism <ref type="bibr" target="#b22">[23]</ref> at multiple scales. However, the performance of this method heavily relies on using tumor segmentation masks as inputs, which limits its generalizability.</p><p>Secondly, although deep survival models have advantages in performing end-to-end survival prediction without requiring tumor masks, this also incurs difficulties in extracting region-specific information, such as the prognostic information in Primary Tumor (PT) and Metastatic Lymph Node (MLN) regions. To address this limitation, recent deep survival models adopted multi-task learning for joint tumor segmentation and survival prediction, to implicitly guide the model to extract features related to tumor regions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. However, most of them only considered PT segmentation and ignored the prognostic information in MLN regions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. Meng et al. <ref type="bibr" target="#b15">[16]</ref> performed survival prediction with joint PT-MLN segmentation and achieved one of the top performances in HECKTOR 2022. However, this method extracted entangled features related to both PT and MLN regions, which incurs difficulties in discovering the prognostic information in PT-/MLN-only regions.</p><p>In this study, we design an X-shape merging-diverging hybrid transformer network (named XSurv, Fig. <ref type="figure" target="#fig_0">1</ref>) for survival prediction in H&amp;N cancer. Our XSurv has a merging encoder to fuse complementary anatomical and metabolic information in PET and CT images and has a diverging decoder to extract region-specific prognostic information in PT and MLN regions. Our technical contributions in XSurv are three folds: (i) We propose a merging-diverging learning framework for survival prediction. This framework is specialized in leveraging multi-modality images and extracting regionspecific information, which potentially could be applied to many survival prediction tasks with multi-modality imaging. (ii) We propose a Hybrid Parallel Cross-Attention (HPCA) block for multi-modality feature learning, where both local intra-modality and global inter-modality features are learned via parallel convolutional layers and crossattention transformers. (iii) We propose a Region-specific Attention Gate (RAG) block for region-specific feature extraction, which screens out the features related to lesion regions. Extensive experiments on the public dataset of HECKTOR 2022 <ref type="bibr" target="#b6">[7]</ref> demonstrate that our XSurv outperforms state-of-the-art survival prediction methods, including the top-performing methods in HECKTOR 2022. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the overall architecture of our XSurv, which presents an X-shape architecture consisting of a merging encoder for multi-modality feature learning and a diverging decoder for region-specific feature extraction. The encoder includes two PET-/CT-specific feature learning branches with HPCA blocks (refer to Sect. 2.1), while the decoder includes two PT-/MLN-specific feature extraction branches with RAG blocks (refer to Sect. 2.2). Our XSurv performs joint survival prediction and segmentation, where the two decoder branches are trained to perform PT/MLN segmentation and provide PT-/MLN-related deep features for survival prediction (refer to Sect. 2.3). Our XSurv also can be enhanced by leveraging the radiomics features extracted from the XSurv-segmented PT/MLN regions (refer to Sect. 2.4). Our implementation is provided at https://github.com/MungoMeng/Survival-XSurv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PET-CT Merging Encoder</head><p>Assuming N conv , N self , and N cross are three architecture parameters, each encoder branch consists of N conv Conv blocks, N self Hybrid Parallel Self-Attention (HPSA) blocks, and N cross HPCA blocks. Max pooling is applied between blocks and the features before max pooling are propagated to the decoder through skip connections. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>(a), HPCA blocks perform parallel convolution and cross-attention operations. The convolution operations are realized using successive convolutional layers with residual connections, while the cross-attention operations are realized using Swin Transformer <ref type="bibr" target="#b27">[27]</ref> where the input x in (from the same encoder branch) is projected as Q and the input x cross (from the other encoder branch) is projected as K and V . In addition, Conv blocks perform the same convolution operations as HPCA blocks but discard cross-attention operations; HPSA blocks share the same overall architecture with HPCA blocks but perform self-attention within the input x in (i.e., the x in is projected as Q, K and V ). Conv and HPSA blocks are used first and then followed by HPCA blocks, which enables the XSurv to learn both intra-and inter-modality information. In this study, we set N conv , N self , and N cross as 1, 1, and 3, as this setting achieved the best validation results (refer to the supplementary materials). Other architecture details are also presented in the supplementary materials.</p><p>The idea of adopting convolutions and transformers in parallel has been explored for segmentation <ref type="bibr" target="#b28">[28]</ref>, which suggests that parallelly aggregating global and local information is beneficial for feature learning. In this study, we extend this idea to multimodality feature learning, which parallelly aggregates global inter-modality and local intra-modality information via HPCA blocks, to discover inter-modality interactions while preserving intra-modality characteristics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">PT-MLN Diverging Decoder</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, each decoder branch is symmetric to the encoder branch and thus includes a total of (N conv +N self +N cross ) Conv blocks. The features propagated from skip connections are fed into RAG blocks for feature diverging before entering the Conv blocks in two decoder branches, where the output of the former Conv block is upsampled and concatenated with the output of the RAG block. As shown in Fig. <ref type="figure" target="#fig_1">2</ref> The output of the last Conv block in the PT/MLN branch is fed into a segmentation head, which generates PT/MLN segmentation masks using a sigmoid-activated 1 × 1 × 1 convolutional layer. In addition, the outputs of all but not the first Conv blocks in the PT/MLN branches are fed into global averaging pooling layers to derive PT-/MLNrelated deep features. Finally, all deep features are fed into a survival prediction head, which maps the deep features into a survival score using two fully-connected layers with dropout, L2 regularization, and sigmoid activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-task Learning</head><p>Following existing multi-task deep survival models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>, our XSurv is endto-end trained for survival prediction and PT-MLN segmentation using a combined loss: L = L Surv +λ(L PT +L MLN ), where the λ is a parameter to balance the survival prediction term L Surv and the PT/MLN segmentation terms L PT /MLN . We follow <ref type="bibr" target="#b14">[15]</ref> to adopt a negative log-likelihood loss <ref type="bibr" target="#b31">[30]</ref> as the L Surv . For the L PT /MLN , we adopt the sum of Dice <ref type="bibr" target="#b32">[31]</ref> and Focal <ref type="bibr" target="#b33">[32]</ref> losses. The loss functions are detailed in the supplementary materials. The λ is set as 1 in the experiments as default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Radiomics Enhancement</head><p>Our XSurv also can be enhanced by leveraging radiomics features (denoted as Radio-XSurv). Following <ref type="bibr" target="#b15">[16]</ref>, radiomics features are extracted from the XSurv-segmented PT/MLN regions via Pyradiomics <ref type="bibr" target="#b34">[33]</ref> and selected by Least Absolute Shrinkage and Selection Operator (LASSO) regression. The process of radiomics feature extraction is provided in the supplementary materials. Then, a CoxPH model <ref type="bibr" target="#b8">[9]</ref> is adopted to integrate the selected radiomics features and the XSurv-predicted survival score to make the final prediction. In addition, clinical indicators (e.g., age, gender) also can be integrated by the CoxPH model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Preprocessing</head><p>We adopted the training dataset of HECKTOR 2022 (refer to https://hecktor.grand-cha llenge.org/), including 488 H&amp;N cancer patients acquired from seven medical centers <ref type="bibr" target="#b6">[7]</ref>, while the testing dataset was excluded as its ground-truth labels are not released. Each patient underwent pretreatment PET/CT and has clinical indicators. We present the distributions of all clinical indicators in the supplementary materials. Recurrence-Free Survival (RFS), including time-to-event in days and censored-or-not status, was provided as ground truth for survival prediction, while PT and MLN annotations were provided for segmentation. The patients from two centers (CHUM and CHUV) were used for testing and other patients for training, which split the data into 386/102 patients in training/testing sets. We trained and validated models using 5-fold cross-validation within the training set and evaluated them in the testing set.</p><p>We resampled PET-CT images into isotropic voxels where 1 voxel corresponds to 1 mm 3 . Each image was cropped to 160 × 160 × 160 voxels with the tumor located in the center. PET images were standardized using Z-score normalization, while CT images were clipped to <ref type="bibr">[-1024, 1024]</ref> and then mapped to [-1, 1]. In addition, we performed univariate and multivariate Cox analyses on the clinical indicators to screen out the prognostic indicators with significant relevance to RFS (P &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We implemented our XSurv using PyTorch on a 12 GB GeForce GTX Titan X GPU. Our XSurv was trained for 12,000 iterations using an Adam optimizer with a batch size of 2. Each training batch included the same number of censored and uncensored samples. The learning rate was set as 1e-4 initially and then reset to 5e-5 and 1e-5 at the 4,000 th and 8,000 th training iteration. Data augmentation was applied in real-time during training to minimize overfitting, including random affine transformations and random cropping to 112 × 112 × 112 voxels. Validation was performed after every 200 training iterations and the model achieving the highest validation result was preserved.</p><p>In our experiments, one training iteration (including data augmentation) took roughly 4.2 s, and one inference iteration took roughly 0.61 s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Settings</head><p>We compared our XSurv to six state-of-the-art survival prediction methods, including two traditional radiomics-based methods and four deep survival models. The included traditional methods are CoxPH <ref type="bibr" target="#b8">[9]</ref> and Individual Coefficient Approximation for Risk Estimation (ICARE) <ref type="bibr" target="#b35">[34]</ref>. For traditional methods, radiomics features were extracted from the provided ground-truth tumor regions and selected by LASSO regression. The included deep survival models are Deep Multi-Task Logistic Regression and CoxPH ensemble (DeepMTLR-CoxPH) <ref type="bibr" target="#b13">[14]</ref>, Transformer-based Multimodal networks for Segmentation and Survival prediction (TMSS) <ref type="bibr" target="#b10">[11]</ref>, Deep Multi-task Survival model (DeepMTS) <ref type="bibr" target="#b23">[24]</ref>, and Radiomics-enhanced DeepMTS (Radio-DeepMTS) <ref type="bibr" target="#b15">[16]</ref>. DeepMTLR-CoxPH, ICARE, and Radio-DeepMTS achieved top performance in HECKTOR 2021 and 2022. For a fair comparison, all methods took the same preprocessed images and clinical indicators as inputs. Survival prediction and segmentation were evaluated using Concordance index (C-index) and Dice Similarity Coefficient (DSC), which are the standard evaluation metrics in the challenges <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">35]</ref>.</p><p>We also performed two ablation studies on the encoder and decoder separately: (i) We replaced HPCA/HPSA blocks with Conv blocks and compared different strategies to combine PET-CT images. (ii) We removed RAG blocks and compared different strategies to extract PT/MLN-related information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>The comparison between our XSurv and the state-of-the-art methods is presented in Table <ref type="table" target="#tab_0">1</ref>. Our XSurv achieved a higher C-index than all compared methods, which demonstrates that our XSurv has achieved state-of-the-art performance in survival prediction of H&amp;N cancer. When radiomics enhancement was adopted in XSurv and DeepMTS, our Radio-XSurv also outperformed the Radio-DeepMTS and achieved the highest Cindex. Moreover, the segmentation results of multi-task deep survival models (TMSS, DeepMTS, and XSurv) are also reported in Table <ref type="table" target="#tab_0">1</ref>. Our XSurv achieved higher DSCs than TMSS and DeepMTS, which demonstrates that our XSurv can locate PT and MLN more precisely and this infers that our XSurv has better learning capability. We attribute these performance improvements to the use of our proposed merging-diverging learning framework, HPCA block, and RAG block, which can be evidenced by ablation studies.</p><p>The ablation study on the PET-CT merging encoder is shown in Table <ref type="table" target="#tab_1">2</ref>. We found that using PET alone resulted in a higher C-index than using both PET-CT with early or late fusion. This finding is consistent with Wang et al. <ref type="bibr" target="#b18">[19]</ref>'s study, which suggests that early and late fusion cannot effectively leverage the complementary information in PET-CT images. As we have mentioned, early and late fusion have difficulties in extracting intra-and inter-modality information, respectively. Our encoder first adopts Conv/HPSA blocks to extract intra-modality information and then leverages HPCA blocks to discover their interactions, which achieved the highest C-index. For PT and MLN segmentation, our encoder also achieved the highest DSCs, which indicates that our encoder also can improve segmentation. In addition, MNAF blocks <ref type="bibr" target="#b21">[22]</ref> were compared and showed poor performance. This is likely attributed to the fact that leveraging non-local attention at multiple scales has corrupted local spatial information, which degraded the segmentation performance and distracted the model from PT and MLN regions. To relieve this problem, in Tang et al.'s study <ref type="bibr" target="#b21">[22]</ref>, tumor segmentation masks were fed into the model as explicit guidance to tumor regions. However, it is intractable to have segmentation masks at the inference stage in clinical practice.</p><p>The ablation study on the PT-MLN diverging decoder is shown in Table <ref type="table" target="#tab_2">3</ref>. We found that, even without adopting AG, using a dual-branch decoder for PT and MLN segmentation resulted in a higher C-index than using a single-branch decoder, which demonstrates the effectiveness of our diverging decoder design. Adopting vanilla AG <ref type="bibr" target="#b30">[29]</ref> or RAG in the dual-branch decoder further improved survival prediction. Compared to the vanilla AG, our RAG contributed to a larger improvement, and this enabled our decoder to achieve the highest C-index. In the supplementary materials, we visualized the attention maps produced by RAG blocks, where the attention maps can precisely locate PT/MLN regions and screen out PT-/MLN-related features. For PT and MLN segmentation, using a single-branch decoder for PT-or MLN-only segmentation achieved the highest DSCs. This is expected as the model can leverage all its capabilities to segment only one target. Nevertheless, our decoder still achieved the second-best DSCs in both PT and MLN segmentation with a small gap. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have outlined an X-shape merging-diverging hybrid transformer network (XSurv) for survival prediction from PET-CT images in H&amp;N cancer. Within the XSurv, we propose a merging-diverging learning framework, a Hybrid Parallel Cross-Attention (HPCA) block, and a Region-specific Attention Gate (RAG) block, to learn complementary information from multi-modality images and extract region-specific prognostic information for survival prediction. Extensive experiments have shown that the proposed framework and blocks enable our XSurv to outperform state-of-the-art survival prediction methods on the well-benchmarked HECKTOR 2022 dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of our XSurv. The architecture parameters N conv , N self , and N cross are set as 1, 1, and 3 for illustration. Survival prediction head is omitted here for clarity.</figDesc><graphic coords="3,41,79,375,95,340,21,137,41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The detailed architecture of the proposed (a) Hybrid Parallel Cross-Attention (HPCA) block and (b) Region-specific Attention Gate (RAG) block.</figDesc><graphic coords="5,41,79,57,08,340,15,118,33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b), RAG blocks generate three softmax-activated spatial attention maps α PT , α MLN , and α B that correspond to PT, MLN, and background regions. These attention maps are computed based on the contextual information provided by the gating signals g PT and g MLN (which are the outputs of the former Conv blocks in the PT and MLN branches). The attention maps α PT and α MLN are multiplied with the features x skip that are propagated from skip connections, which spatially diverge the features x skip into PT-and MLNrelated features x PT and x MLN . Different from the vanilla Attention Gate (AG) block [29], RAG blocks leverage the gating signals from two decoder branches and generate mutually exclusive (softmax-activated) attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between XSurv and state-of-the-art survival prediction methods.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Survival</cell><cell>PT segmentation</cell><cell>MLN</cell></row><row><cell></cell><cell></cell><cell>prediction</cell><cell>(DSC)</cell><cell>segmentation</cell></row><row><cell></cell><cell></cell><cell>(C-index)</cell><cell></cell><cell>(DSC)</cell></row><row><cell>CoxPH [9]</cell><cell cols="3">Radiomics 0.745 ± 0.024 * /</cell><cell>/</cell></row><row><cell>ICARE [34]</cell><cell cols="3">Radiomics 0.765 ± 0.019 * /</cell><cell>/</cell></row><row><cell>DeepMTLR-CoxPH</cell><cell>CNN</cell><cell cols="2">0.748 ± 0.025 * /</cell><cell>/</cell></row><row><cell>[14]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TMSS [11]</cell><cell cols="3">ViT + CNN 0.761 ± 0.028 * 0.784 ± 0.015 *</cell><cell>0.724 ± 0.018 *</cell></row><row><cell>DeepMTS [24]</cell><cell>CNN</cell><cell cols="2">0.757 ± 0.022 * 0.754 ± 0.010 *</cell><cell>0.715 ± 0.013 *</cell></row><row><cell>XSurv (Ours)</cell><cell>Hybrid</cell><cell>0.782 ± 0.018</cell><cell>0.800 ± 0.006</cell><cell>0.754 ± 0.008</cell></row><row><cell cols="2">Radio-DeepMTS [16] CNN +</cell><cell cols="2">0.776 ± 0.018  ‡ 0.754 ± 0.010  ‡</cell><cell>0.715 ± 0.013  ‡</cell></row><row><cell></cell><cell>Radiomics</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Radio-XSurv (Ours) Hybrid +</cell><cell>0.798 ± 0.015</cell><cell>0.800 ± 0.006</cell><cell>0.754 ± 0.008</cell></row><row><cell></cell><cell>Radiomics</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Bold: the best result in each column is in bold. ±: standard deviation.</cell><cell></cell></row><row><cell cols="5">*: P&lt;0.05, in comparison to XSurv.  ‡: P&lt;0.05, in comparison to Radio-XSurv.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the PET-CT merging encoder. the best result in each column is in bold. SBE: single-branch encoder. DBE: dual-branch encoder. C e : the channel numbers or embedding dimensions used in the encoder.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Survival</cell><cell>PT segmentation</cell><cell>MLN</cell></row><row><cell></cell><cell></cell><cell>prediction</cell><cell>(DSC)</cell><cell>segmentation</cell></row><row><cell></cell><cell></cell><cell>(C-index)</cell><cell></cell><cell>(DSC)</cell></row><row><cell>SBE with C e = [16,</cell><cell>Only PET</cell><cell>0.767</cell><cell>0.753</cell><cell>0.699</cell></row><row><cell>32, 64, 128, 256]</cell><cell>Only CT</cell><cell>0.637</cell><cell>0.630</cell><cell>0.702</cell></row><row><cell></cell><cell>Early fusion</cell><cell>0.755</cell><cell>0.783</cell><cell>0.722</cell></row><row><cell>DBE with C e = [8,</cell><cell>Late fusion</cell><cell>0.762</cell><cell>0.796</cell><cell>0.744</cell></row><row><cell>16, 32, 64, 128]</cell><cell>MNAF [22]</cell><cell>0.688</cell><cell>0.741</cell><cell>0.683</cell></row><row><cell></cell><cell>Ours</cell><cell>0.782</cell><cell>0.800</cell><cell>0.754</cell></row><row><cell>Bold:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on the PT-MLN diverging decoder.Bold: the best result in each column is in bold. SBD: single-branch decoder. DBD: dual-branch decoder. C d : the channel numbers used in the decoder.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>Survival</cell><cell>PT segmentation</cell><cell>MLN</cell></row><row><cell></cell><cell></cell><cell>prediction</cell><cell>(DSC)</cell><cell>segmentation</cell></row><row><cell></cell><cell></cell><cell>(C-index)</cell><cell></cell><cell>(DSC)</cell></row><row><cell>SBD with C d =</cell><cell>Only PT</cell><cell>0.751</cell><cell>0.803</cell><cell>/</cell></row><row><cell>[256, 128, 64, 32, 16]</cell><cell>Only MLN PT and MLN</cell><cell>0.746 0.765</cell><cell>/ 0.790</cell><cell>0.758 0.734</cell></row><row><cell>DBD with C d =</cell><cell>No AG</cell><cell>0.770</cell><cell>0.792</cell><cell>0.740</cell></row><row><cell>[128, 64, 32, 16, 8]</cell><cell>Vanilla AG [29]</cell><cell>0.774</cell><cell>0.795</cell><cell>0.745</cell></row><row><cell></cell><cell>Ours</cell><cell>0.782</cell><cell>0.800</cell><cell>0.754</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by <rs type="funder">Australian Research Council (ARC)</rs> under Grant <rs type="grantNumber">DP200103748</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fVvPX9h">
					<idno type="grant-number">DP200103748</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global cancer statistics</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Parkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pisani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="108" />
			<date type="published" when="2002">2002. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning in head and neck tumor multiomics diagnosis and analysis: review of the literature</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Genet</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">624820</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparison of PET and CT radiomics for prediction of local tumor control in head and neck squamous cell carcinoma</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bogowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Oncol</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1531" to="1536" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prediction of 5-year progression-free survival in advanced nasopharyngeal carcinoma with pretreatment PET/CT using multi-modality deep learning-based radiomics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Oncol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">899351</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Overview of the HECKTOR challenge at MICCAI 2020: automatic head and neck tumor segmentation in PET/CT</title>
		<author>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-67194-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-67194-5_1" />
	</analytic>
	<monogr>
		<title level="m">HECKTOR 2020</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12603</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overview of the HECKTOR challenge at MICCAI 2021: automatic head and neck tumor segmentation and outcome prediction in PET/CT images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-98253-9_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-98253-9_1" />
	</analytic>
	<monogr>
		<title level="m">HECKTOR 2021</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13209</biblScope>
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overview of the HECKTOR challenge at MICCAI 2022: automatic head and neck tumor segmentation and outcome prediction in PET/CT</title>
		<author>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-27420-6_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-27420-6_1" />
	</analytic>
	<monogr>
		<title level="m">HECKTOR 2022</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13626</biblScope>
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Radiomics: images are more than pictures, they are data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Gillies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Kinahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hricak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">278</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="563" to="577" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Regression models and life-tables</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. Ser. B (Methodol.)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="202" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A systematic review on machine learning and deep learning techniques in cancer survival prediction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Deepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gunavathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prog. Biophys. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="62" to="71" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TMSS: an end-to-end transformer-based multimodal network for segmentation and survival prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sobirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al Majzoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yaqub</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_31" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="319" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-transSP: multimodal transformer for survival prediction of nasopharyngeal carcinoma patients</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_23" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="234" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From handcrafted to deep-learning-based cancer radiomics: challenges and opportunities</title>
		<author>
			<persName><forename type="first">P</forename><surname>Afshar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="132" to="160" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An ensemble approach for patient prognosis of head and neck tumor using multimodal data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Saeed</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-98253-9_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-98253-9_26" />
	</analytic>
	<monogr>
		<title level="m">HECKTOR 2021</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13209</biblScope>
			<biblScope unit="page" from="278" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progression free survival prediction for head and neck cancer using deep learning based on clinical and PET/CT imaging data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Naser</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-98253-9_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-98253-9_27" />
	</analytic>
	<monogr>
		<title level="m">HECKTOR 2021</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13209</biblScope>
			<biblScope unit="page" from="287" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Radiomics-enhanced deep multi-task learning for outcome prediction in head and neck cancer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-27420-6_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-27420-6_14" />
	</analytic>
	<monogr>
		<title level="m">HECKTOR 2022</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13626</biblScope>
			<biblScope unit="page" from="135" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning in head &amp; neck cancer outcome prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Diamant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">2764</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Prediction of the local treatment outcome in patients with oropharyngeal squamous cell carcinoma using deep learning analysis of pretreatment FDG-PET images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fujima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Cancer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">900</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning based time-to-event analysis with PET, CT and joint PET/CT for head and neck cancer prognosis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page">106948</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">M^2Net: multi-modal multi-channel network for overall survival time prediction of brain tumor patients</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59713-9_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59713-9_22" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fusing modalities by multiplexed graph neural networks for outcome prediction in tuberculosis</title>
		<author>
			<persName><forename type="first">D'</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MMMNA-net for overall survival time prediction of brain tumor patients</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3805" to="3808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DeepMTS: deep multi-task learning for survival prediction in patients with advanced nasopharyngeal carcinoma using pretreatment PET/CT</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4497" to="4507" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-task deep learning for joint tumor segmentation and outcome prediction in head and neck cancer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-98253-9_15</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-98253-9_15" />
	</analytic>
	<monogr>
		<title level="m">HECKTOR 2021</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13209</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-task deep segmentation and radiomics for automatic prognosis in head and neck cancer</title>
		<author>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PRIME 2021</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12928</biblScope>
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87602-9_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87602-9_14" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PHTrans: parallelly aggregating global and local representations for medical image segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_23" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention gated networks: learning to leverage salient regions in medical images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A scalable discrete-time survival model for neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Gensheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">6257</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">V-Net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Computational radiomics system to decode the radiographic phenotype</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Griethuysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. Res</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="104" to="e107" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simplicity is all you need: out-of-the-box nnUNet followed by binaryweighted radiomic model for segmentation and outcome prediction in head and neck PET/CT</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rebaud</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-27420-6_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-27420-6_13" />
	</analytic>
	<monogr>
		<title level="m">HECKTOR 2022</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13626</biblScope>
			<biblScope unit="page" from="121" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Biomedical image analysis competitions: the state of current participation practice</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eisenmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08568</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
