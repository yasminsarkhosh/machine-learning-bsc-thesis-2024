<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview</title>
				<funder ref="#_STcGzJ6 #_43E7hCX #_KmPktHe">
					<orgName type="full">Foundation of Science and Technology Commission of Shanghai Municipality</orgName>
				</funder>
				<funder ref="#_6VewYTm #_BmGFVhr #_Wz3mbu4 #_ZJ6vD77">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_3eY34vJ">
					<orgName type="full">Shanghai Pudong Science and Technology Development Fund</orgName>
				</funder>
				<funder ref="#_Z9mM84k #_G7vCUb8">
					<orgName type="full">Shanghai Jiao Tong University Foundation on Medical and Technological Joint Science Research</orgName>
				</funder>
				<funder ref="#_jcwXjH3">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Nsks2mp">
					<orgName type="full">Funding of Xiamen Science and Technology Bureau</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yizhou</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Biomedical Manufacturing and Life Quality Engineering</orgName>
								<orgName type="department" key="dep2">School of Mechanical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiaojun</forename><surname>Chen</surname></persName>
							<email>xiaojunchen@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Biomedical Manufacturing and Life Quality Engineering</orgName>
								<orgName type="department" key="dep2">School of Mechanical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Medical Robotics</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="137" to="147"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F5BC1B98B080EFD78473B14AE5A6C83A</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_14</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic segmentation</term>
					<term>3D teeth reconstruction</term>
					<term>Generative adversarial network (GAN)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Orthodontic treatment typically lasts for two years, and its outcome cannot be predicted intuitively in advance. In this paper, we propose a semantic-guided and knowledge-based generative framework to predict the visual outcome of orthodontic treatment from a single frontal photo. The framework involves four steps. Firstly, we perform tooth semantic segmentation and mouth cavity segmentation and extract category-specific teeth contours from frontal images. Secondly, we deform the established tooth-row templates to match the projected contours with the detected ones to reconstruct 3D teeth models. Thirdly, we apply a teeth alignment algorithm to simulate the orthodontic treatment. Finally, we train a semantic-guided generative adversarial network to predict the visual outcome of teeth alignment. Quantitative tests are conducted to evaluate the proposed framework, and the results are as follows: the tooth semantic segmentation model achieves a mean intersection of union of 0.834 for the anterior teeth, the average symmetric surface distance error of our 3D teeth reconstruction method is 0.626 mm on the test cases, and the image generation model has an average Fréchet inception distance of 6.847 over all the test images. These evaluation results demonstrate the practicality of our framework in orthodontics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Orthodontic treatment aims to correct misaligned teeth and restore normal occlusion. Patients are required to wear dental braces or clear aligners for a duration of one to three years, reported by <ref type="bibr" target="#b20">[21]</ref>, with only a vague expectation of the treatment result. Therefore, a generative framework is needed to enable patients to preview treatment outcomes and assist those considering orthodontic treatment in making decisions. Such framework may involve multiple research fields, such as semantic segmentation, 3D reconstruction, and image generation.</p><p>Deep learning methods have achieved great success in image-related tasks. In the field of tooth semantic segmentation, there exist plenty of studies targeting on different data modalities, such as dental mesh scanning <ref type="bibr" target="#b30">[31]</ref>, point cloud <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>, cone beam CT image <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, panoramic dental X-ray image <ref type="bibr" target="#b27">[28]</ref>, and 2D natural image <ref type="bibr" target="#b31">[32]</ref>. Regarding image generation, the family of generative adversarial networks (GAN) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b22">23]</ref> and the emerging diffusion models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref> can generate diverse high-fidelity images. Although the diffusion models can overcome the mode collapse problem and excel in image diversity compared to GAN <ref type="bibr" target="#b6">[7]</ref>, their repeated reverse process at inference stage prolongs the execution time excessively, limiting their application in real-time situations.</p><p>When it comes to 3D teeth reconstruction, both template-based and deeplearning-based frameworks offer unique benefits. Wu et al. employed a templatebased approach by adapting their pre-designed teeth template to match teeth contours extracted from a set of images <ref type="bibr" target="#b28">[29]</ref>. Similarly, Wirtz et al. proposed an optimization-based pipeline that uses five intra-oral photos to restore the 3D arrangement of teeth <ref type="bibr" target="#b26">[27]</ref>. Liang et al. restored 3D teeth using convolution neural networks (CNN) from a single panoramic radiograph <ref type="bibr" target="#b17">[18]</ref>. However, while deep CNNs have a strong generalization ability compared to template-based methods, it often struggles to precisely and reasonably restore occluded objects.</p><p>Predicting the smiling portrait after orthodontic treatment has recently gained much attention. Yang et al. developed three deep neural networks to extract teeth contours from smiling images, arrange 3D teeth models, and generate images of post-treatment teeth arrangement, respectively <ref type="bibr" target="#b18">[19]</ref>. However, their framework requires a single frontal smiling image and the corresponding unaligned 3D teeth model from dental scanning, which may be difficult for general users to obtain. In contrast, Chen et al. proposed a StyleGAN generator with a latent space editing method that utilizes GAN inversion to discover the optimal aligned teeth appearance from a single image <ref type="bibr" target="#b2">[3]</ref>. Although their method takes only a frontal image as input and manipulates the teeth structure and appearance implicitly in image space, it may overestimate the treatment effect and result in inaccurate visual outcomes.</p><p>In this study, we propose an explainable generative framework, which is semantic-guided and knowledge-based, to predict teeth alignment after orthodontic treatment. Previous works have either required 3D teeth model as additional input and predicted its alignment using neural networks <ref type="bibr" target="#b18">[19]</ref>, or directly utilized an end-to-end StyleGAN to predict the final orthodontic outcome <ref type="bibr" target="#b2">[3]</ref>. In contrast, our approach requires only a single frontal image as input, restores the 3D teeth model through a template-based algorithm, and explicitly incorporates orthodontists' experience, resulting in a more observable and explainable process. Our contributions are therefore three-fold: 1) we introduce a region-boundary feature fusion module to enhance the 2D tooth semantic segmentation results; 2) we employ statistical priors and reconstruct 3D teeth models from teeth semantic boundaries extracted in a single frontal image; 3) by incorporating an orthodontic simulation algorithm and a pSpGAN style encoder <ref type="bibr" target="#b22">[23]</ref>, we can yield more realistic and explainable visualization of the post-treatment teeth appearance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The proposed generative framework (Fig. <ref type="figure" target="#fig_0">1</ref>) consists of four parts: semantic segmentation in frontal images, template-based 3D teeth reconstruction, orthodontic treatment simulation, and semantic-guided image generation of mouth cavity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Segmentation in Frontal Images</head><p>The tooth areas and mouth cavity are our region of interest for semantic segmentation in each frontal image. As a preprocessing step, the rectangle mouth area is firstly extracted from frontal images by the minimal bounding box that encloses the facial key points around the mouth, which are detected by dlib toolbox <ref type="bibr" target="#b16">[17]</ref>. We then use two separate segmentation models to handle these mouth images, considering one-hot encoding and the integrity of mouth cavity mask. A standard U-Net <ref type="bibr" target="#b23">[24]</ref> is trained with soft dice loss <ref type="bibr" target="#b19">[20]</ref> to predict mouth cavity.</p><p>The tooth semantic segmentation model (Fig. <ref type="figure" target="#fig_1">2</ref>) is a dual-branch U-Net3+ based network that predicts tooth regions and contours simultaneously. We employ a standard U-Net3+ <ref type="bibr" target="#b11">[12]</ref> encoder and two identical U-Net3+ decoders for tooth region and contour segmentation. Such inter-related multi-task learning can enhance the performance of each task and mitigate overfitting. The teeth are manually labeled using FDI World Dental Federation notation, resulting in a total of 33 classes, including background.</p><p>To generate a more precise tooth segmentation map, we introduce the regionboundary feature fusion module, which merges the tooth region and boundary information, i.e., the last hidden feature maps of the two decoders. The module is constructed as a stack of convolutional layers, which incorporates an improved atrous spatial pyramid pooling (ASPP) module <ref type="bibr" target="#b3">[4]</ref>. This ASPP module employs atrous separable convolution and global pooling to capture long-range information. The integration of ASPP has a dilation rate of 6, and the number of filters in each convolutional layer, except the output layer, is set to 64. These three outputs are supervised by soft dice loss <ref type="bibr" target="#b19">[20]</ref> during training. Some post-process techniques are added to filter segmented regions and obtain smoother tooth contours. The predicted binary contours are dilated and used to divide the semantic segmentation map into multiple connected regions. The pixels in each region are classified by its dominant tooth label. Background components are ignored and small ones are removed. Once two regions have duplicate labels, a drop-or-relabel strategy is performed on the region away from the center of central incisors. The connected regions are processed sequentially from central incisors to the third molars. Figure <ref type="figure" target="#fig_2">3</ref> shows the changes of tooth region prediction before and after the post process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Template-Based 3D Teeth Reconstruction</head><p>To achieve 3D tooth reconstruction from a single frontal image, we deform the parametric templates of the upper and lower tooth rows to match the projected contours with the extracted teeth contours in semantic segmentation.</p><p>The parametric tooth-row template is a statistical model that characterizes the shape, scale, and pose of each tooth in a tooth row. To describe the shape of each tooth, we construct a group of morphable shape models <ref type="bibr" target="#b0">[1]</ref>. We model the pose (orientation and position) of each tooth as a multivariate normal distribution as Wu et al. <ref type="bibr" target="#b28">[29]</ref> did. Additionally, we suppose that the scales of all teeth follows a multivariate normal distribution. The mean shape and average of the tooth scales and poses are used to generate a standard tooth-row template.</p><p>The optimization-based 3D teeth reconstruction following <ref type="bibr" target="#b28">[29]</ref> is an iterative process alternating between searching for point correspondences between the projected and segmented teeth contours and updating the parameters of the tooth-row templates. The parameters that require estimation are the camera parameters, the relative pose between the upper and lower tooth rows, and the scales, poses, and shape parameters of each tooth.</p><p>The point correspondences are established by Eq. ( <ref type="formula" target="#formula_0">1</ref>) considering the semantic information in teeth contours, where c τ i and n τ i are the position and normal of the detected contour point i of tooth τ in image space, ĉj τ and nj τ are those of the projected contour point j, &lt; •, • &gt; denotes inner product, and σ angle = 0.3 is a fine-tuned hyper parameter in <ref type="bibr" target="#b28">[29]</ref>.</p><formula xml:id="formula_0">ĉi τ = arg min ĉj τ ||c τ i -ĉj τ || 2 2 • exp - &lt; n τ i , nj τ &gt; σ angle 2<label>(1)</label></formula><p>We use L as the objective function to minimize, expressed in Eq. ( <ref type="formula">2</ref>), which comprises an image-space contour loss <ref type="bibr" target="#b28">[29]</ref> and a regularization term L prior described by Mahalanobis distance in probability space, where N is the number of detected contour points, λ n = 50 and λ p = 25 are the fine-tuned weights.</p><formula xml:id="formula_1">L = 1 N τ i ||c τ i -ĉi τ || 2 2 + λ n &lt; c τ i -ĉi τ , ni τ &gt; 2 + λ p L prior (2)</formula><p>The regularization term L prior in Eq. ( <ref type="formula" target="#formula_2">3</ref>) is the negative log likelihood of the distributions of the vector of tooth scales, denoted by s, the pose vector of tooth τ , denoted by p τ , and the shape vector of tooth τ , denoted by b τ . The covariance matrices Σ s and Σ τ p are obtained in building tooth-row templates.</p><formula xml:id="formula_2">L prior = (s -s) T Σ -1 s (s -s) + τ (p τ -p τ ) T Σ τ p -1 (p τ -p τ ) + ||b τ || 2 2<label>(3)</label></formula><p>During optimization, we first optimize the camera parameters and the relative pose of tooth rows for 10 iterations and optimize all parameters for 20 iterations. Afterward, we use Poisson surface reconstruction <ref type="bibr" target="#b15">[16]</ref> to transform the surface point clouds into 3D meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Orthodontic Treatment Simulation</head><p>We implement a naive teeth alignment algorithm to mimic orthodontic treatment. The symmetrical beta function (Eq. ( <ref type="formula" target="#formula_3">4</ref>)) is used to approximate the dental arch curve of a tooth row <ref type="bibr" target="#b1">[2]</ref>. Its parameters W and D can be estimated through linear regression by fitting the positions of tooth landmarks <ref type="bibr" target="#b1">[2]</ref>.</p><formula xml:id="formula_3">β(x; D, W ) = 3.0314 * D * 1 2 + x W 0.8 1 2 - x W 0.8<label>(4)</label></formula><p>We assume that the established dental arch curves are parallel to the occlusal plane. Each reconstructed tooth is translated towards its expected position in the dental arch and rotated to its standard orientation while preserving its shape. The teeth gaps are then reduced along the dental arch curve with collision detection performed. The relative pose between tooth rows is re-calculated to achieve a normal occlusion. Finally, the aligned 3D teeth models are projected with the same camera parameters to generate the semantic image output of the simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Semantic-Guided Image Generation</head><p>The idea behind the semantic-guided image generation model is to decompose an image into an orthogonal representation of its style and structure. By manipulating the structural or style information, we can control the characteristics of the generated image. Improving upon <ref type="bibr" target="#b18">[19]</ref>, we replace the naive style encoder in <ref type="bibr" target="#b18">[19]</ref> with the PixelStylePixel style encoder <ref type="bibr" target="#b22">[23]</ref> to capture the multi-level style features and use semantic teeth image instead of teeth contours as input to better guide the generation process. The architecture is illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>. At training stage, the model learns the style and structural encoding of teeth images and attempts to restore the original image. Gradient penalty <ref type="bibr" target="#b8">[9]</ref> and path length regularization <ref type="bibr" target="#b14">[15]</ref> are applied to stabilize the training process. We use the same loss function as <ref type="bibr" target="#b18">[19]</ref> did and take a standard UNet encoder connected with dense layers as the discriminator. At inference stage, the semantic teeth image output from orthodontic simulation is used to control the generated teeth structure. To remove the boundary artifacts, we dilate the input mouth cavity map and use Gaussian filtering to smooth the sharp edges after image patching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>We collected 225 digital dental scans with labelled teeth and their intra-oral photos, as well as 5610 frontal intra-oral images, of which 3300 were labelled, and 4330 smiling images, of which 2000 were labelled, from our partner hospitals. The digital dental scans were divided into two groups, 130 scans for building morphable shape models and tooth-row templates and the remaining 95 scans for 3D teeth reconstruction evaluation. The labelled 3300 intra-oral images and 2000 smiling images were randomly split into training (90%) and labelled test (10%) datasets. The segmentation accuracy was computed on the labelled test data, and the synthetic image quality was evaluated on the unlabelled test data. All the models were trained and evaluated on an NVIDIA GeForce RTX 3090 GPU. We trained the segmentation models for 100 epochs with a batch size of 4, and trained the image generation models for 300 epochs with a batch size of 8. The input and output size of the image segmentation and generation models are 256 × 256. The training was started from scratch and the learning rate was set to 10 -4 . We saved the models with the minimal loss on the labelled test data. At the inference stage, our method takes approximately 15 s to run a single case on average, with the 3D reconstruction stage accounting for the majority of the execution time, tests performed solely on an Intel 12700H CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head><p>Ablation Study of Tooth Segmentation Model. We conduct an ablation study to explore the improvement of segmentation accuracy brought by the dualbranch network architecture and the region-boundary feature fusion module. Segmentation accuracy is measured by the mean intersection over union (mIoU)  metric for different groups of tooth labels. The results listed in Table <ref type="table" target="#tab_0">1</ref> show that the proposed region-boundary feature fusion module assisted with dual-branch architecture can further enhance the segmentation accuracy for UNet and its variant. Our tooth segmentation model can predict quite accurately the region of the frontal teeth with a mIoU of 0.834.</p><p>Accuracy of 3D Teeth Reconstruction. We reconstruct the 3D teeth models of the 95 test cases from their intra-oral photos. The restored teeth models are aligned with their ground truth by global similarity registration. We compare the reconstruction error using different metrics, shown in Table <ref type="table" target="#tab_1">2</ref>, with the method of <ref type="bibr" target="#b26">[27]</ref> that reconstructs teeth models from five intra-oral photos and the nearest retrieval that selects the most similar teeth mesh in the 135 teeth meshes for building tooth-row templates. The results show that our teeth reconstruction method significantly outperforms the method of <ref type="bibr" target="#b26">[27]</ref> and nearest retrieval.</p><p>Image Generation Quality. We use Fréchet inception distance (FID) <ref type="bibr" target="#b9">[10]</ref> to evaluate the quality of images generated different generators on the unlabelled test data, results listed in Table <ref type="table" target="#tab_2">3</ref>. The multi-level style features captured by pSpGAN improve greatly the image quality from the quantitative comparison (Table <ref type="table" target="#tab_2">3</ref>) and the visual perception (Fig. <ref type="figure">5</ref>). Our semantic-guided pSpGAN that takes semantic teeth image as input can further increase the constrast of different teeth and yield sharper boundaries. We test our framework on some images in Flickr-Faces-HQ dataset <ref type="bibr" target="#b13">[14]</ref> to visualize virtual teeth alignment, shown in Fig. <ref type="figure">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In conclusion, we develop a semantic-guided generative framework to predict the orthodontic treatment visual outcome. It comprises tooth semantic segmentation, template-based 3D teeth reconstruction, orthodontic treatment simulation, and semantic-guided mouth cavity generation. The results of quantitative tests show that the proposed framework has a potential for orthodontic application.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of (a) the proposed generative framework for orthodontic treatment outcome prediction, and (b) the framework's outputs at each stage with images from left to right: input mouth image, tooth semantic segmentation map, input image overlaid with semi-transparent 3D reconstruction teeth mesh, projection of orthodontic treatment simulation output, and orthodontic visual outcome prediction.</figDesc><graphic coords="3,55,98,238,28,340,63,98,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed tooth semantic segmentation model with (a) coarse region segmentation, (b) auxiliary boundary segmentation, and (c) final region segmentation generated by the region-boundary feature fusion module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the effect of the segmentation post-process algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Architecture of the semantic-guided image generation model. The multi-level style feature maps are extracted from the residual backbone and encoded into twelve 512-dim style vectors through the map2style networks [23], structural information are compressed and skip connected through the structure encoder, and structure and style features are entangled in the StyleGAN-based generator with weight modulation [15].</figDesc><graphic coords="6,55,29,54,56,313,72,96,13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Comparison of the orthodontic treatment outcome predictions generated by different models: TSynNet [19], contour-guided pSpGAN, and semantic-guided pSp-GAN.</figDesc><graphic coords="8,41,79,237,26,340,21,85,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Segmentation accuracy on the test data measured by mean intersection of union for different groups of tooth labels and for different network architectures where DB denotes dual-branch architecture and RBFF denotes region-boundary feature fusion. Group A has 32 tooth classes, group B has 28 classes with the third molars excluded, and group C has 24 classes with the second and third molars excluded.</figDesc><table><row><cell>Settings UNet</cell><cell></cell><cell></cell><cell>UNet3+</cell></row><row><cell cols="2">Baseline DB</cell><cell cols="3">RBFF+DB Baseline DB</cell><cell>RBFF+DB</cell></row><row><cell>Group A 0.679</cell><cell cols="2">0.697 0.708</cell><cell>0.686</cell><cell>0.699 0.730</cell></row><row><cell>Group B 0.764</cell><cell cols="2">0.774 0.789</cell><cell>0.766</cell><cell>0.780 0.803</cell></row><row><cell>Group C 0.800</cell><cell cols="2">0.809 0.820</cell><cell>0.800</cell><cell>0.816 0.834</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Teeth reconstruction error (avg.± std.) on all the teeth of the 95 test cases (ASSD: average symmetric surface distance, HD: Hausdorff distance, CD: Chamfer distance, DSC: Dice similarity coefficient).</figDesc><table><row><cell>Methods</cell><cell>ASSD(mm)↓</cell><cell>HD(mm)↓</cell><cell cols="2">CD(mm 2 )↓ DSC↑</cell></row><row><cell cols="4">Wirtz et al. [27] 0.848±0.379 [27] 2.627±0.915 [27] -</cell><cell>0.659±0.140 [27]</cell></row><row><cell cols="2">Nearest retrieval 0.802±0.355</cell><cell>2.213±0.891</cell><cell cols="2">2.140±2.219 0.653±0.158</cell></row><row><cell>Ours</cell><cell>0.626±0.265</cell><cell>1.776±0.723</cell><cell cols="2">1.272±1.364 0.732±0.125</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Average Fréchet inception distance of different generators on the test data.</figDesc><table><row><cell>Model</cell><cell cols="3">TSynNet Contour-guided pSpGAN Semantic-guided pSpGAN</cell></row><row><cell cols="2">Test smiling images 11.343</cell><cell>7.292</cell><cell>6.501</cell></row><row><cell>All test images</cell><cell>20.133</cell><cell>7.832</cell><cell>6.847</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by grants from the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">81971709</rs>; <rs type="grantNumber">M-0019</rs>; <rs type="grantNumber">82011530141</rs>), the <rs type="funder">Foundation of Science and Technology Commission of Shanghai Municipality</rs> (<rs type="grantNumber">20490740700</rs>; <rs type="grantNumber">22Y11911700</rs>), <rs type="funder">Shanghai Pudong Science and Technology Development Fund</rs> (<rs type="grantNumber">PKX2021-R04</rs>), <rs type="funder">Shanghai Jiao Tong University Foundation on Medical and Technological Joint Science Research</rs> (<rs type="grantNumber">YG2021ZD21</rs>; <rs type="grantNumber">YG2021QN72</rs>; <rs type="grantNumber">YG2022QN056</rs>; <rs type="grantNumber">YG2023ZD19</rs>; <rs type="grantNumber">YG2023ZD15</rs>), and the <rs type="funder">Funding of Xiamen Science and Technology Bureau</rs> (No. <rs type="grantNumber">3502Z20221012</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jcwXjH3">
					<idno type="grant-number">81971709</idno>
				</org>
				<org type="funding" xml:id="_6VewYTm">
					<idno type="grant-number">M-0019</idno>
				</org>
				<org type="funding" xml:id="_STcGzJ6">
					<idno type="grant-number">82011530141</idno>
				</org>
				<org type="funding" xml:id="_43E7hCX">
					<idno type="grant-number">20490740700</idno>
				</org>
				<org type="funding" xml:id="_KmPktHe">
					<idno type="grant-number">22Y11911700</idno>
				</org>
				<org type="funding" xml:id="_3eY34vJ">
					<idno type="grant-number">PKX2021-R04</idno>
				</org>
				<org type="funding" xml:id="_Z9mM84k">
					<idno type="grant-number">YG2021ZD21</idno>
				</org>
				<org type="funding" xml:id="_G7vCUb8">
					<idno type="grant-number">YG2021QN72</idno>
				</org>
				<org type="funding" xml:id="_BmGFVhr">
					<idno type="grant-number">YG2022QN056</idno>
				</org>
				<org type="funding" xml:id="_Wz3mbu4">
					<idno type="grant-number">YG2023ZD19</idno>
				</org>
				<org type="funding" xml:id="_Nsks2mp">
					<idno type="grant-number">YG2023ZD15</idno>
				</org>
				<org type="funding" xml:id="_ZJ6vD77">
					<idno type="grant-number">3502Z20221012</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 14.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 26th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The form of the human dental arch</title>
		<author>
			<persName><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Hnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Fender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Legan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Angle Orthod</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Orthoaligner: image-based teeth alignment prediction via latent style manipulation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visual Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3617" to="3629" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic segmentation of individual tooth in dental CBCT images from tooth surface map by a multi-task FCN</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="97296" to="97309" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pose-aware instance segmentation framework from cone beam CT images for tooth segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">103720</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local NASH equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unet 3+: a full-scale connected UNet for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1055" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="852" to="863" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Poisson surface reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bolitho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Eurographics Symposium on Geometry Processing</title>
		<meeting>the Fourth Eurographics Symposium on Geometry Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dlib-ml: a machine learning toolkit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">X2Teeth: 3D teeth reconstruction from a single panoramic radiograph</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59713-9_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59713-939" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020, Part II</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12262</biblScope>
			<biblScope unit="page" from="400" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">iorthopredictor: model-guided deep prediction of teeth alignment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lingchen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">216</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Loss odyssey in medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102035</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Factors affecting the duration of orthodontic treatment: a systematic review</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mavreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Athanasiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Orthod</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="386" to="395" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Glide: towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Encoding in style: a stylegan encoder for image-to-image translation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2287" to="2296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015, Part III</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Palette: image-to-image diffusion models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2022 Conference Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3D tooth instance segmentation learning objectness and affinity in point cloud</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput. Commun. Appl. (TOMM)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic model-based 3-D reconstruction of the teeth from five photographs with predefined viewing directions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wirtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Noll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wesarg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing</title>
		<imprint>
			<biblScope unit="volume">11596</biblScope>
			<biblScope unit="page" from="198" to="212" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic teeth segmentation in panoramic x-ray images using a coupled shape model in combination with a neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wirtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mirashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wesarg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_81</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-381" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018, Part IV</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="712" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Model-based teeth reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="220" to="221" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning approach to semantic segmentation in 3D point cloud intra-oral scans of teeth</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Zanjani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="557" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic 3D teeth semantic segmentation with mesh augmentation network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 3rd International Conference on Pattern Recognition and Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="136" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tooth detection and segmentation with mask R-CNN</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="70" to="072" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
