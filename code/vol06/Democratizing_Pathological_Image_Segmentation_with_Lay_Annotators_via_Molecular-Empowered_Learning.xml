<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning</title>
				<funder ref="#_KuC4SfA #_xHYUc55">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder ref="#_yKqRXPV">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruining</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37215</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanwei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37215</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peize</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37215</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiacheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37215</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lucas</forename><forename type="middle">W</forename><surname>Remedios</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37215</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saydolimkhon</forename><surname>Agzamkhodjaev</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37215</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zuhayr</forename><surname>Asad</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37215</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37215</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Can</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37215</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaohong</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Vanderbilt University Medical Center</orgName>
								<address>
									<postCode>37232</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yihan</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Vanderbilt University Medical Center</orgName>
								<address>
									<postCode>37232</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yucheng</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA Corporation</orgName>
								<address>
									<settlement>Santa</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Clara and Bethesda</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haichun</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Vanderbilt University Medical Center</orgName>
								<address>
									<postCode>37232</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yuankai</forename><surname>Huo</surname></persName>
							<email>yuankai.huo@vanderbilt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37215</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="497" to="507"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">93C31BCDA5520B7FB88BE426DA889078</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_48</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image annotation</term>
					<term>Registration</term>
					<term>Noisy label learning</term>
					<term>Pathology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-class cell segmentation in high-resolution Giga-pixel whole slide images (WSI) is critical for various clinical applications. Training such an AI model typically requires labor-intensive pixel-wise manual annotation from experienced domain experts (e.g., pathologists). Moreover, such annotation is error-prone when differentiating fine-grained cell types (e.g., podocyte and mesangial cells) via the naked human eye. In this study, we assess the feasibility of democratizing pathological AI deployment by only using lay annotators (annotators without medical domain knowledge). The contribution of this paper is threefold: (1) We proposed a molecular-empowered learning scheme for multiclass cell segmentation using partial labels from lay annotators; (2) The proposed method integrated Giga-pixel level molecular-morphology cross-modality registration, molecular-informed annotation, and molecular-oriented segmentation model, so as to achieve significantly superior performance via 3 lay annotators as compared with 2 experienced pathologists; (3) A deep corrective learning (learning with imperfect labels) method is proposed to further improve the segmentation performance using partially annotated noisy data. From the experimental results, our learning method achieved F1 = 0.8496 using molecular-informed annotations from lay annotators, which is better than conventional morphologybased annotations (F1 = 0.7015) from experienced pathologists. Our method democratizes the development of a pathological segmentation deep model to the lay annotator level, which consequently scales up the learning process similar to a non-medical computer vision task. The official implementation and cell annotations are publicly available at https://github.com/hrlblab/MolecularEL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-class cell segmentation is an essential technique for analyzing tissue samples in digital pathology. Accurate cell quantification assists pathologists in identifying and diagnosing diseases <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">29]</ref> as well as obtaining detailed information about the progression of the disease <ref type="bibr" target="#b24">[23]</ref>, its severity <ref type="bibr" target="#b29">[28]</ref>, and the effectiveness of treatment <ref type="bibr" target="#b15">[15]</ref>. For example, the distribution and density of podocyte and mesangial cells in the glomerulus offer a faint signal of functional injury in renal pathology <ref type="bibr" target="#b14">[14]</ref>. The cell-level characterization is challenging for experienced pathologists due to the decades of expensive medical training, long annotation time, large variability <ref type="bibr" target="#b31">[30]</ref>, and low accuracy, while it is impractical to hire massive experienced pathologists for cell annotation.</p><p>Previous works proposed several computer vision tools to perform automated or semi-automated cell segmentation on pathological images <ref type="bibr" target="#b17">[17]</ref>, including Annota-torJ <ref type="bibr" target="#b12">[12]</ref>, NuClick <ref type="bibr" target="#b16">[16]</ref>, QuPath <ref type="bibr" target="#b1">[2]</ref>, etc. Such software is able to mark nuclei, cells, and multi-cellular structures by compiling pre-trained segmentation models <ref type="bibr" target="#b11">[11]</ref>, color deconvolution <ref type="bibr" target="#b26">[25]</ref>, or statistical analysis <ref type="bibr" target="#b23">[22]</ref>. However, those automatic approaches still heavily rely on the morphology of cells from pathological Periodic acid-Schiff (PAS) images, thus demanding intensive human intervention for extra supervision and correction. Recently, immunofluorescence (IF) staining imaging has been widely used to visualize multiple biomolecules simultaneously in a single sample using fluorescently labeled antibodies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">20]</ref>. Such technology can accurately serve as a guide to studying the heterogeneity of cellular populations, providing reliable information for cell annotation. Furthermore, crowd-sourcing technologies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b19">19]</ref> were introduced generate better annotation for AI learning from multiple annotations.</p><p>In this paper, we proposed a holistic molecular-empowered learning scheme that democratizes AI pathological image segmentation by employing only lay annotators (Fig. <ref type="figure" target="#fig_0">1</ref>). The learning pipeline consists of (1) morphology-molecular multi-modality image registration, (2) molecular-informed layman annotation, and (3) molecularoriented corrective learning. The pipeline alleviates the difficulties at the R&amp;D from the expert level (e.g., experienced pathologists) while relegating annotation to the lay annotator level (e.g., non-expert undergraduate students), all while enhancing both the accuracy and efficiency of the cell-level annotations. An efficient semi-supervised learning strategy is proposed to offset the impact of noisy label learning on lay annotations. The contribution of this paper is three-fold:</p><p>• We propose a molecular-empowered learning scheme for multi-class cell segmentation using partial labels from lay annotators; • The molecular-empowered learning scheme integrates (1) Giga-pixel level molecular-morphology cross-modality registration, (2) molecular-informed annotation, and (3) molecular-oriented segmentation model to achieve statistically a significantly superior performance via lay annotators as compared with experienced pathologists; • A deep corrective learning method is proposed to further maximize the cell segmentation accuracy using partially annotated noisy annotation from lay annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The overall pipeline of the entire labeling and auto-quantification pipeline is presented in Fig. <ref type="figure" target="#fig_1">2</ref>. Molecular images are aligned with anatomical images in order to provide accurate guidance for cell labeling by using multi-scale registration. After this registration, a functional unit segmentation model is implemented to localize the regions of glomeruli. Within those glomeruli, lay annotators label multiple cell types by using the pair-wise molecular images and anatomical images in ImageJ <ref type="bibr" target="#b12">[12]</ref>. A partial-label learning model with a molecular-oriented corrective learning strategy is employed so as to diminish the gap between labels from lay annotators and gold standard labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Morphology-Molecular Multi-modality Registration</head><p>Multi-modality, multi-scale registration is deployed to ensure the pixel-to-pixel correspondence (alignment) between molecular IF and PAS images at both the WSI and regional levels. To maintain the morphological characteristics of the functional unit structure, a slide-wise multi-modality registration pipeline (Map3D) <ref type="bibr" target="#b7">[8]</ref> is employed to register the molecular images to anatomical images. The first stage is global alignment. The Map3D approach was employed to achieve reliable translation on WSIs when encountering missing tissues and staining variations. The output of this stage is a pairwise affine matrix M Map3D (t) from Eq. ( <ref type="formula">1</ref>).</p><formula xml:id="formula_0">M Map3D = arg min N i=1 ||A(x IF i , M) -x P AS i || Af fMap3D (1)</formula><p>To achieve a more precise pixel-level correspondence, Autograd Image Registration Laboratory (AIRLab) <ref type="bibr" target="#b28">[27]</ref> was utilized to calibrate the registration performance at the second stage. The output of this step is M AIRLab (t) from Eq. <ref type="bibr" target="#b1">(2)</ref>. where i is the index of pixel x i in the image I, with N pixels. The two-stage registration (Map3D + AIRLab) affine matrix for each pair is presented in Eq. ( <ref type="formula" target="#formula_2">3</ref>).</p><formula xml:id="formula_1">M AIRLab = arg min A MMap3D N i=1 ||A(x IF i , M) -x P AS i || Af f AIRLab (2)</formula><formula xml:id="formula_2">M = (M Map3D , M AIRLab )<label>(3)</label></formula><p>In Eq. ( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>), A indicates the affine registration. The affine matrix M Map3D (t) from Map3D is applied to obtain pair-wise image regions. The ||.|| Af fMap3D and ||.|| Af f AIRLab in Eq. ( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>) indicates the different similarity metrics for two affine registrations, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Molecular-Informed Annotation</head><p>After aligning molecular images with PAS images, an automatic multi-class functional units segmentation pipeline Omni-Seg <ref type="bibr" target="#b6">[7]</ref> is deployed to locate the tuft unit on the images. With the tuft masks, the molecular images then manifest heterogeneous cells with different color signals on pathological images during the molecular-informed annotation. Each anatomical image attains a binary mask for each cell type, in the form of a partial label. Following the same process, the pathologist examines both anatomical images and molecular images to generate a gold standard for this study (Fig. <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Molecular-Oriented Corrective Learning for Partial Label Segmentation</head><p>The lack of molecular expertise as well as the variability in the quality of staining in molecular images can cause annotations provided by non-specialists to be unreliable and error-prone. Therefore, we propose a corrective learning strategy (in Fig. <ref type="figure" target="#fig_2">3</ref>) to efficiently train the model with noise labels, so as to achieve the comparable performance of training the same model using the gold standard annotations.</p><p>Inspired by confidence learning <ref type="bibr" target="#b22">[21]</ref> and similarity attention <ref type="bibr" target="#b18">[18]</ref>, top-k pixel feature embeddings at the annotation regions with higher confidences from the prediction probability (W , defined as confidence score in Eq. ( <ref type="formula">4</ref>)) are selected as critical representations for the current cell type from the decoder(in Eq. ( <ref type="formula" target="#formula_3">5</ref>)).</p><formula xml:id="formula_3">W = f (X; θ)[:, 1] (4) top -k(k, E, W, Y ) = (e 1 , w 1 ), (e 2 , w 2 ), ..., (e k , w k ) ∩ Y ∈ (E, W )<label>(5)</label></formula><p>where k denotes the number of selected embedding features. E is the embedding map from the last layer of the decoder, while Y is the lay annotation.</p><p>We then implement a cosine similarity score S between the embedding from an arbitrary pixel to those from critical embedding features as Eq. <ref type="bibr" target="#b5">(6)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S(e i , e top</head><formula xml:id="formula_4">-k ) = M m=1 (e i × e top-k ) M m=1 (e i ) 2 × M m=1 (e top-k ) 2 (6)</formula><p>where m denotes the channel of the feature embeddings.</p><p>Since the labels from lay annotators might be noisy and erroneous, the W and S are applied in following Eq. ( <ref type="formula">7</ref>) to highlight the regions where both the model and lay annotation agree on the current cell type, when calculating the loss function in Eq. <ref type="bibr" target="#b7">(8)</ref>.</p><formula xml:id="formula_5">ω(W ) = exp(W ) × Y, ω(S) = S × Y (7) L(Y, f (X; θ)) = (L Dice (Y, f (X; θ))) + L BCE (Y, f (X; θ)))) × ω(W ) × ω(S) (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data and Experiments</head><p>Data. 11 PAS staining WSIs, including 3 injured glomerulus slides, were collected with pair-wise IF images for the process. The stained tissues were scanned at a 20× magnification. After multi-modality multi-scale registration, 1,147 patches for podocyte cells, and 789 patches for mesangial cells were generated and annotated. Each patch has 512×512 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morphology-Molecular Multi-modality Registration.</head><p>The slide-level global translation from Map3D was deployed at a 5× magnification, which is 2 µm per pixel. The 4096×4096 pixels PAS image regions with 1024 pixels overlapping were tiled on anatomical WSIs at a 20× magnification, which is 0.5 µm per pixel.</p><p>Molecular-Empowered Annotation. The automatic tuft segmentation and molecular knowledge images assisted the lay annotators with identifying glomeruli and cells. ImageJ (version v1.53t) was used throughout the entire annotation process. "Synchronize Windows" was used to display cursors across the modalities with spatial correlations for annotation. "ROI Manager" was used to store all of the cell binary masks for each cell type.</p><p>Molecular-Oriented Corrective Learning. Patches were randomly split into training, validation, and testing sets -with a ratio of 6:1:3, respectively -at the WSI level. The distribution of injured glomeruli and normal glomeruli were balanced in the split.</p><p>Experimental Setting. 2 experienced pathologists and 3 lay annotators without any specialized knowledge were included in the experiment. All anatomical and molecular patches of glomerular structures are extracted from WSI on a workstation equipped with a 12-core Intel Xeon W-2265 Processor, and NVIDIA RTXA6000 GPU. An 8-core AMD Ryzen 7 5800X Processor workstation with XP-PEN Artist 15.6 Pro Wacom is used for drawing the contour of each cell. Annotating 1 cell type on 1 WSI requires 9 h, while staining and scanning 24 IF WSIs (as a batch) requires 3 h. The experimental setup for the 2 experts and the 3 lay annotators is kept strictly the same to ensure a fair comparison.</p><p>Evaluation Metrics. 100 patches from the testing set with a balanced number of injuries and normal glomeruli were captured by the pathologists for evaluating morphology-based annotation and molecuar-informed annotation. The annotation from one pathologist (over 20 years' experience) with both anatomical and molecular images as gold standard (Fig. <ref type="figure" target="#fig_0">1</ref>). The balanced F-score (F1) was used as the major metric for this study. The Fleiss' kappa was used to compute the inter-rater variability between experts and lay annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Figure <ref type="figure" target="#fig_3">4</ref>, Fig. <ref type="figure" target="#fig_4">5</ref> and Table <ref type="table" target="#tab_0">1</ref> indicate the annotation performance from the naked human eye with expert knowledge and the lay annotator with molecular-informed learning. As  shown, our learning method achieved better annotation with higher F1 scores with fewer false positive and false negative regions as compared with the pathologist's annotations. Statistically, the Fleiss' kappa test shows that the molecular-informed annotation by lay-annotators has higher annotation agreements than the morphology-based annotation by experts. This demonstrates the benefits of reducing the expertise requirement to a layman's level and improving accuracy in pathological cell annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance on Multi-class Cell Segmentation</head><p>In Table <ref type="table" target="#tab_1">2</ref>, we compared the proposed partial label segmentation method to baseline models, including (1) multiple individual models (U-Nets <ref type="bibr" target="#b25">[24]</ref>, DeepLabv3s <ref type="bibr" target="#b3">[4]</ref>, and Residual U-Nets <ref type="bibr" target="#b27">[26]</ref>), (2) multi-head models (Multi-class <ref type="bibr" target="#b10">[10]</ref>, Multi-Kidney <ref type="bibr" target="#b2">[3]</ref>), and (3) single dynamic networks with noisy label learning (Omni-Seg <ref type="bibr" target="#b6">[7]</ref>). Our results found that the partial label paradigm shows superior performance on multi-class cell segmentation. The proposed model particularly demonstrates better quantification in the normal glomeruli, which contain large amounts of cells.</p><p>To evaluate the performance of molecular-oriented corrective learning on imperfect lay annotation, we also implemented two noisy label learning strategies Confidence Learning (CL) <ref type="bibr" target="#b22">[21]</ref> and Partial Label Loss (PLL) <ref type="bibr" target="#b18">[18]</ref> with the proposed Molecular-oriented corrective learning (MOCL) on our proposed partial label model. As a result, the proposed molecular-oriented corrective learning alleviated the error between lay annotation and the gold standard in the learning stage, especially in the injured glomeruli that incorporate more blunders in the annotation due to the identification difficulty from morphology changing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>The purpose of corrective learning is to alleviate the noise and distillate the correct information, so as to improve the model performance using lay annotation. Four designs of corrective learning with different utilization of similarity losses and confidence losses were evaluated with lay annotation in Table <ref type="table" target="#tab_2">3</ref>. Each score is used in either an exponential function or a linear function (Eq. ( <ref type="formula">7</ref>)), when multiplying and calculating the loss function (Eq. ( <ref type="formula">8</ref>)). The bold configuration was selected as the final design.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed a holistic, molecular-empowered learning solution to alleviate the difficulties of developing a multi-class cell segmentation deep learning model from the expert level to the lay annotator level, enhancing the accuracy and efficiency of cell-level annotation. An efficient corrective learning strategy is proposed to offset the impact of noisy label learning from lay annotation. The results demonstrate the feasibility of democratizing the deployment of a pathology AI model while only relying on lay annotators.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overall idea of this work. The left panel shows the standard annotation process (PAS only) for developing pathological segmentation models. The middle panel shows our molecularinformed annotation (with both PAS and IF images) that allows for better annotation quality from lay annotators as compared with the left panel. The right panel presents the gold standard annotation for this study, where the annotations are obtained by experienced pathologists upon both PAS and IF images.</figDesc><graphic coords="2,74,31,54,32,275,71,147,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The framework of the proposed molecular-empowered learning scheme. The molecular-empowered learning pipeline consists of (1) morphology-molecular multi-modality image registration, (2) molecular-informed layman annotation, and (3) molecular-oriented corrective learning. It democratizes AI pathological image segmentation by employing only lay annotators.</figDesc><graphic coords="4,75,30,54,29,273,16,255,43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The molecuar-oriented corrective learning in partial label model. A corrective learning are applied to highlight the regions where both the model and lay annotation agree on the current cell type, when calculating the loss function.</figDesc><graphic coords="5,55,98,53,99,340,54,76,21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Annotation accuracy using learning different strategies. This figure compares the annotation performance using different strategies. Note that the molecular-informed annotation only employed lay annotators, while the remaining results were from an experienced renal pathologist.</figDesc><graphic coords="7,116,70,81,11,237,28,135,37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Annotation accuracy between 2 experts and 3 lay annotators. This figure compares the annotation performance between morphology-based annotation by 2 experts and molecularinformed annotation by 3 lay annotators. Overall, the molecular-informed annotation achieved better F1 scores than morphology-based annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Annotation</figDesc><table><row><cell>Method</cell><cell cols="2">Injured glomeruli</cell><cell cols="2">Normal glomeruli</cell><cell>Average</cell><cell></cell><cell cols="2">Fleiss' kappa</cell></row><row><cell></cell><cell cols="8">Podocyte Mesangial Podocyte Mesangial Podocyte Mesangial Podocyte Mesangial</cell></row><row><cell>Morphology-based</cell><cell>0.6964</cell><cell>0.6941</cell><cell>0.7067</cell><cell>0.6208</cell><cell>0.7015</cell><cell>0.6567</cell><cell>0.3973</cell><cell>0.4161</cell></row><row><cell>annotation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(2 pathologists with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PAS)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Molecular-informed</cell><cell>0.8374</cell><cell>0.8434</cell><cell>0.8619</cell><cell>0.8511</cell><cell>0.8496</cell><cell>0.8473</cell><cell>0.6406</cell><cell>0.5978</cell></row><row><cell>annotation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(3 lay annotators with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PAS+IF)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>p-value</cell><cell cols="2">p&lt;0.001 p&lt;0.001</cell><cell cols="2">p&lt;0.001 p&lt;0.001</cell><cell cols="2">p&lt;0.001 p&lt;0.001</cell><cell>N/A</cell><cell>N/A</cell></row></table><note><p>accuracy from only anatomical morphology and molecular-informed annotation. Average F1 scores and Fleiss' kappa between 2 experts and 3 lay annotators are reported.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance of deep learning based multi-class cell segmentation. F1 are reported. G.S. denotes gold standard dataset, *L.A. denotes lay annotation dataset</figDesc><table><row><cell>Method</cell><cell cols="2">Data Injured glomeruli</cell><cell cols="2">Normal glomeruli</cell><cell>Average</cell><cell></cell></row><row><cell></cell><cell cols="6">Podocyte Mesangial Podocyte Mesangial Podocyte Mesangial</cell></row><row><cell>U-Nets [24]</cell><cell>G.S 0.6719</cell><cell>0.6867</cell><cell>0.7203</cell><cell>0.6229</cell><cell>0.6944</cell><cell>0.6617</cell></row><row><cell>DeepLabV3s [4]</cell><cell>G.S 0.7127</cell><cell>0.6680</cell><cell>0.7395</cell><cell>0.6163</cell><cell>0.7251</cell><cell>0.6476</cell></row><row><cell cols="2">Residual U-Nets [26] G.S 0.6968</cell><cell>0.6913</cell><cell>0.7481</cell><cell>0.6601</cell><cell>0.7207</cell><cell>0.6790</cell></row><row><cell>Multi-class [10]</cell><cell>G.S 0.5201</cell><cell>0.4984</cell><cell>0.4992</cell><cell>0.4993</cell><cell>0.5214</cell><cell>0.4987</cell></row><row><cell>Multi-kidney [3]</cell><cell>G.S. 0.6735</cell><cell>0.6734</cell><cell>0.7542</cell><cell>0.6581</cell><cell>0.7108</cell><cell>0.6691</cell></row><row><cell>Omni-Seg [7]</cell><cell>G.S 0.7115</cell><cell>0.6970</cell><cell>0.7746</cell><cell>0.6895</cell><cell>0.7407</cell><cell>0.6940</cell></row><row><cell>Omni-Seg [7]</cell><cell>L.A 0.6941</cell><cell>0.7083</cell><cell>0.7703</cell><cell>0.6822</cell><cell>0.7295</cell><cell>0.6980</cell></row><row><cell>CL [21]</cell><cell>L.A 0.7047</cell><cell>0.6961</cell><cell>0.7536</cell><cell>0.6754</cell><cell>0.7274</cell><cell>0.6879</cell></row><row><cell>PLL [9]</cell><cell>L.A 0.6276</cell><cell>0.6853</cell><cell>0.6825</cell><cell>0.6268</cell><cell>0.6531</cell><cell>0.6622</cell></row><row><cell>MOCL(Ours)</cell><cell>L.A 0.7198</cell><cell>0.7157</cell><cell>0.7657</cell><cell>0.6830</cell><cell>0.7411</cell><cell>0.7028</cell></row></table><note><p>*</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on different molecular-oriented corrective learning design.</figDesc><table><row><cell cols="5">Confidence score Similarity score Podocyte F1 Masengial F1 Average F1</cell></row><row><cell>Linear</cell><cell>Linear</cell><cell>0.7255</cell><cell>0.6843</cell><cell>0.7049</cell></row><row><cell>Linear</cell><cell>Exponent</cell><cell>0.7300</cell><cell>0.6987</cell><cell>0.7144</cell></row><row><cell>Exponent</cell><cell>Linear</cell><cell>0.7411</cell><cell>0.7028</cell><cell>0.7219</cell></row><row><cell>Exponent</cell><cell>Exponent</cell><cell>0.7304</cell><cell>0.6911</cell><cell>0.7108</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported in part by <rs type="funder">NIH</rs> <rs type="grantNumber">R01DK135597</rs>(Huo), DoD <rs type="grantNumber">HT9425-23-1-0003</rs>(HCY), and <rs type="funder">NIH</rs> <rs type="grantNumber">NIDDK DK56942</rs>(ABF).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KuC4SfA">
					<idno type="grant-number">R01DK135597</idno>
				</org>
				<org type="funding" xml:id="_xHYUc55">
					<idno type="grant-number">HT9425-23-1-0003</idno>
				</org>
				<org type="funding" xml:id="_yKqRXPV">
					<idno type="grant-number">NIDDK DK56942</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NuCLS: a scalable crowdsourcing approach and dataset for nucleus classification and segmentation in breast cancer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Amgad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GigaScience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Qupath: open source software for digital pathology image analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bankhead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning-based segmentation and quantification in experimental kidney histopathology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bouteldja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Soc. Nephrol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="68" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Advanced algorithmic approaches to medical image segmentation: State-of-the-art applications in cardiology, neurology, mammography and pathology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="541" to="558" />
		</imprint>
	</monogr>
	<note>Cell image segmentation for diagnostic pathology</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fluorescently labeled therapeutic antibodies for detection of microscopic melanoma</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Deep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Zinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Laryngoscope</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2681" to="2689" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Single dynamic network for multi-label renal pathology image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Asad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="304" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Map3d: registration-based multi-object tracking on 3d serial whole slide images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1924" to="1933" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pointly-supervised panoptic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="319" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20056-4_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20056-419" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-structure segmentation from partially labeled datasets. application to body composition measurements on CT scans</title>
		<author>
			<persName><forename type="first">G</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Washko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>San José Estépar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00946-5_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00946-522" />
	</analytic>
	<monogr>
		<title level="m">RAMBO/BIA/TIA -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11040</biblScope>
			<biblScope unit="page" from="215" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hover-net: simultaneous segmentation and classification of nuclei in multi-tissue histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101563</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">AnnotatorJ: an imageJ plugin to ease hand annotation of cellular compartments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hollandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Á</forename><surname>Diósdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hollandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moshkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Horváth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Biol. Cell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="2179" to="2186" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data quality from crowdsourcing: a study of annotation selection criteria</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Hsueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing</title>
		<meeting>the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interactions between podocytes, mesangial cells, and glomerular endothelial cells in glomerular diseases</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Imig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Elmarakby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pavlov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Physiol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">488</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mast cell quantification in normal peritoneum and during peritoneal dialysis treatment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiménez-Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Pathol. Lab. Med</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1188" to="1192" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nuclick: a deep learning framework for interactive segmentation of microscopic images</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Koohbanani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jahanifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Tajadin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101771</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A review of current systems for annotation of cell and tissue images in digital pathology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Korzynska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Roszkowiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Siemion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biocybernetics Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1436" to="1453" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Eliceiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14318" to="14328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Is crowd-algorithm collaboration an advanced alternative to crowdsourcing on cytology slides?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Marzahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bildverarbeitung für die Medizin</title>
		<imprint>
			<biblScope unit="page" from="26" to="31" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-658-29267-6_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-658-29267-65" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>Wiesbaden</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effects of an unlabeled loading dose on tumor-specific uptake of a fluorescently labeled antibody for optical surgical navigation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Imaging Biol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="610" to="616" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Confident learning: estimating uncertainty in dataset labels</title>
		<author>
			<persName><forename type="first">C</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1373" to="1411" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistical methods for quantitative mass spectrometry proteomic experiments with labeling</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Oberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinf</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Htlv-1 proviral load in peripheral blood mononuclear cells quantified in 100 ham/tsp patients: a marker of disease progression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Olindo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurol. Sci</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="53" to="59" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Quantification of histochemical staining by color deconvolution</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Ruifrok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anal. Quant. Cytol. Histol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="299" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated assessment of glomerulosclerosis and tubular atrophy using deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Salvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">101930</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Andermatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09907</idno>
		<title level="m">Airlab: autograd image registration laboratory</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quantification of dengue virus specific t cell responses and correlation with viral load and clinical disease severity in acute dengue infection</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Wijeratne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Neglected Trop. Dis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">6540</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust nucleus/cell detection and segmentation in digital pathology and microscopy images: a comprehensive review</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="234" to="263" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep-learning-driven quantification of interstitial fibrosis in digitized kidney biopsies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Pathol</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1442" to="1453" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
