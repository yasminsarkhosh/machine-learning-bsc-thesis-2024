<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network</title>
				<funder ref="#_UpVrFZT #_thatWXK">
					<orgName type="full">Beijing Science and Technology Project</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanzhen</forename><surname>Liu</surname></persName>
							<email>yanzhenliu@buaa.edu.cn</email>
							<idno type="ORCID">0000-0003-3766-1171</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">Advanced Innovation Center for Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">School of Biological Science and Medical Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Biomechanics and Mechanobiology</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100083</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sutuke</forename><surname>Yibulayimu</surname></persName>
							<idno type="ORCID">0000-0001-5115-9255</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">Advanced Innovation Center for Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">School of Biological Science and Medical Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Biomechanics and Mechanobiology</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100083</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yudi</forename><surname>Sang</surname></persName>
							<idno type="ORCID">0000-0002-9971-2993</idno>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Rossum Robot Technology Co., Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gang</forename><surname>Zhu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Rossum Robot Technology Co., Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
							<email>wangyu@buaa.edu.cn</email>
							<idno type="ORCID">0000-0003-0467-6465</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">Advanced Innovation Center for Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">School of Biological Science and Medical Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Biomechanics and Mechanobiology</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100083</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunpeng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Orthopaedics and Traumatology</orgName>
								<orgName type="institution">Beijing Jishuitan Hospital</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinbao</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Orthopaedics and Traumatology</orgName>
								<orgName type="institution">Beijing Jishuitan Hospital</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="312" to="321"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">5779E3E5A114539003C089984423D01D</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_30</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CT segmentation</term>
					<term>Pelvic fracture</term>
					<term>Reduction planning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pelvic fracture is a severe type of high-energy injury. Segmentation of pelvic fractures from 3D CT images is important for trauma diagnosis, evaluation, and treatment planning. Manual delineation of the fracture surface can be done in a slice-by-slice fashion but is slow and error-prone. Automatic fracture segmentation is challenged by the complex structure of pelvic bones and the large variations in fracture types and shapes. This study proposes a deep-learning method for automatic pelvic fracture segmentation. Our approach consists of two consecutive networks. The anatomical segmentation network extracts left and right ilia and sacrum from CT scans. Then, the fracture segmentation network further isolates the fragments in each masked bone region. We design and integrate a distance-weighted loss into a 3D U-net to improve accuracy near the fracture site. In addition, multi-scale deep supervision and a smooth transition strategy are used to facilitate training. We built a dataset containing 100 CT scans with fractured pelvis and manually annotated the fractures. A five-fold cross-validation experiment shows that our method outperformed max-flow segmentation and network without distance weighting, achieving a global Dice of 99.38%, a local Dice of 93.79%, and an Hausdorff distance of 17.12 mm. We have made our dataset and source code publicly available and expect them to facilitate further pelvic research, especially reduction planning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pelvic fracture is a severe type of high-energy injury, with a fatality rate greater than 50%, ranking the first among all complex fractures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>. Surgical planning and reduction tasks are challenged by the complex pelvic structure, as well as the surrounding muscle groups, ligaments, neurovascular and other tissues. Robotic fracture reduction surgery has been studied and put into clinical use in recent years, and has successfully increased reduction precision and reduced radiation exposure <ref type="bibr" target="#b1">[2]</ref>. Accurate segmentation of pelvic fracture is required in both manual and automatic reduction planning, which aim to find the optimal target location to restore the healthy morphology of pelvic bones. Segmenting pelvic fragments from CT is challenging due to the uncertain shape and irregular position of the bone fragments and the complex collision fracture surface. Therefore, surgeons typically annotate the anatomy of pelvic fractures in a semi-automatic way. First, by tuning thresholds and selecting seed points, adaptive thresholding and region-growing methods are used to extract bone regions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref>. Then, the fracture surfaces are manually delineated by outlining the fragments in 3D view or even modifying the masks in a slice-by-slice fashion. Usually, this tedious process can take more than 30 min, especially when the fractured fragments are collided or not completely separated.</p><p>Several studies have been proposed to provide more efficient tools for operators. A semi-automatic graph-cut method based on continuous max-flow has been proposed for pelvic fracture segmentation, but it still requires the manual selection of seed points and trail-and-error <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref>. Fully automatic max-flow segmentation based on graph cut and boundary enhancement filter is useful when fragments are separated, but it often fails on fragments that are collided or compressed <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref>. Learning-based bone segmentation has been successfully applied to various anatomy, including the pelvis, rib, skull, etc. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Some deep learning methods have been proposed to detect fractures <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>, but the output from these methods cannot provide a fully automated solution for subsequent operations. In FracNet, rib fracture detection was formulated as a segmentation problem, but with a resultant Dice of 71.5%, it merely outlined the fracture site coarsely without delineating the fracture surface <ref type="bibr" target="#b6">[7]</ref>. Learning-based methods that directly deal with fracture segmentation have rarely been studied.</p><p>Fracture segmentation is still a challenging task for the learning-based method because (1) compared to the more common organ/tumor segmentation tasks where the model can implicitly learn the shape prior of an object, it is difficult to learn the shape information of a bone fragment due to the large variations in fracture types and shapes <ref type="bibr" target="#b9">[10]</ref>; (2) the fracture surface itself can take various forms including large space (fragments isolated and moved), small gap (fragments isolated but not moved), crease (fragments not completely isolated), compression (fragments collided), and their combinations, resulting in quite different image intensity profiles around the fracture site; and (3) the variable number of bone fragments in pelvic fracture makes it difficult to prescribe a consistent labeling strategy that applies to every type and case.</p><p>This paper proposes a deep learning-based method to segment pelvic fracture fragments from preoperative CT images automatically. Our major contribution includes three aspects. <ref type="bibr" target="#b0">(1)</ref> We proposed a complete automatic pipeline for pelvic fractures segmentation, which is the first learning-based pelvic fracture segmentation method to the best of our knowledge. <ref type="bibr" target="#b1">(2)</ref> We designed a novel multi-scale distance-weighted loss and integrated it into the deeply supervised training of the fracture segmentation network to boost accuracy near the fracture site. (3) We established a comprehensive pelvic fracture CT dataset and provided ground-truth annotations. Our dataset and source code are publicly available at https://github.com/YzzLiu/FracSegNet. We expect them to facilitate further pelvis-related research, including but not limited to fracture identification, segmentation, and subsequent reduction planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Overall Segmentation Framework</head><p>Our study aims to automatically segment the major and minor fragments of target bones (left and right ilia and sacrum) from CT scans. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, our method consists of three steps. In the first step, an anatomical segmentation network is used to extract the pelvic bones from the CT scan. With a cascaded 3D nn-Unet architecture, the network is pre-trained on a set of healthy pelvic CT images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> and further refined on our fractured dataset. In the second step, a fracture segmentation network is used to separate the bone fragments from each iliac and sacral region extracted from the first step. To define a consistent labeling rule that is applicable to all fracture types, we prescribe three labels for each bone, namely the background, the main fragment, and other fragments. The main fragment is the largest fragment at the center. In the third step, isolated components are further separated and labeled, and small isolated bone fragments are removed to form the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fracture Segmentation Network</head><p>The contact fracture surface (CFS) is the part where the bones collide and overlap due to compression and impact, and is the most challenging part for human operators to draw. We are particularly concerned about the segmentation performance in this region. Therefore, we introduce guidance into the network training using fracture distance map (FDM). A 3D UNet is selected as the base model <ref type="bibr" target="#b5">[6]</ref>. The model learns a non-linear mapping relationship M : X → Y , where X and Y are the input and ground truth of a training sample, respectively.</p><p>Fracture Distance Map. The FDM is computed on the ground-truth segmentation of each data sample before training. This representation provides information about the boundary, shape, and position of the object to be segmented. First, CFS regions are identified by comparing the labels within each voxel's neighbourhood. Then, we calculate the distance of each foreground voxel to the nearest CFS as its distance value D v , and divide it by the maximum.</p><formula xml:id="formula_0">D v = I(Y v ≥ 1)min u∈CF S ||v -u|| 2 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><formula xml:id="formula_2">Dv = D v max v∈V D v , (<label>2</label></formula><formula xml:id="formula_3">)</formula><formula xml:id="formula_4">where v = (h, w, d) ∈ V is the voxel index, Y is the ground-truth segmentation, I(Y v ≥ 1)</formula><p>is the indicator function for foreground, and D is the normalized distance. The distance is then used to calculate the FDM weight Ŵ using the following formula:</p><formula xml:id="formula_5">W v = λ back + I (Y v ≥ 1) 1 -λ back 1 + e λFDM Dv-5 , (<label>3</label></formula><formula xml:id="formula_6">) Ŵv = W v • |V | v∈V W v . (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>To ensure the equivalence of the loss among different samples, the weights are normalized so that the average is always 1.</p><p>FDM-Weighted Loss. The FDM weight Ŵ is then used to calculate the weighted Dice and cross-entropy losses to emphasize the performance near CFS by assigning larger weights to those pixels.</p><formula xml:id="formula_8">L dice = 1 - 2 |L| l∈L v∈V Ŵv P l v Y l v v∈V Ŵv P l v + v∈V Ŵv Y l v , (<label>5</label></formula><formula xml:id="formula_9">)</formula><formula xml:id="formula_10">L ce = - 1 |V ||L| v∈V l∈L Ŵv Y l v log P l v , (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>where L is the number of labels, P l v , Y l v are the network output prediction and the one-hot encoding form of the ground truth for the l th label of the v th voxel. The overall loss is their weighted sum:</p><formula xml:id="formula_12">L total = L dice + λ ce L ce , (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>where λ ce is a balancing weight.</p><p>Multi-scale Deep Supervision. We use a multi-scale deep supervision strategy in model training to learn different features more effectively <ref type="bibr" target="#b19">[20]</ref>. The deep layers mainly capture the global features with shape/structural information, whereas the shallow layers focus more on local features that help delineate fracture surfaces. We add auxiliary losses to the decoder at different resolution levels (except the lowest resolution level). The losses are calculated using the corresponding down-sampled FDM Ŵ n v , and down-sampled ground truth Y n v . We calculate the output of the n th level L n by changing λ F DM in Eq. ( <ref type="formula" target="#formula_5">3</ref>). The λ F DM of each layer decreases by a factor of 2 as the depth increases, i.e., λ n+1 = λ n /2. In this way, the local CFS information are assigned more attention in the shallow layers, while the weights become more uniform in the deep layers.</p><p>Smooth Transition. To stabilize network training, we use a smooth transition strategy to maintain the model's attention on global features at the early stage of training and gradually shift the attention towards the fracture site as the model evolves <ref type="bibr" target="#b13">[14]</ref>. The smooth transition dynamically adjusts the proportion of the FDM in the overall weight matrix based on the number of training iterations. The dynamic weight is calculated using the following formula:</p><formula xml:id="formula_14">W st = I (t &lt; τ) 1 1 + δ J + δ 1 + δ Ŵ + I (t ≥ τ ) Ŵ ,<label>(8)</label></formula><p>where δ = -ln(1 -t τ ), J is an all-ones matrix with the same size as the input volume, t is the current iteration number, and τ is a hyper-parameter. The dynamic weight W st is adjusted by controlling the relative proportion of J and Ŵ . The transition terminates when the epoch reaches τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Post-processing</head><p>Connected component analysis (CCA) has been widely used in segmentation <ref type="bibr" target="#b2">[3]</ref>, but is usually unsuitable for fracture segmentation because of the collision between fragments. However, after identifying and removing the main central fragment from the previous step, other fragments are naturally separated. Therefore, in the post-processing step, we further isolate the remaining other fragments by CCA. The isolated components are then assigned different labels. In addition, we remove fragments smaller than a certain threshold, which has no significant impact on planning and robotic surgery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Data and Annotation</head><p>Although large-scale datasets on pelvic segmentation have been studied in some research <ref type="bibr" target="#b12">[13]</ref>, to the best of our knowledge, currently there is no well-annotated fractured pelvic dataset publicly available. Therefore, we curated a dataset of 100 preoperative CT scans covering all common types of pelvic fractures. These data is collected from 100 patients (aged 18-74 years, 41 females) who were to undergo pelvic reduction surgery at Beijing Jishuitan Hospital between 2018 and 2022, under IRB approval (202009-04). The CT scans were acquired on a Toshiba Aquilion scanner. The average voxel spacing is 0.82 × 0.82 × 0.94 mm 3 . The average image shape is 480 × 397 × 310.</p><p>To generate ground-truth labels for bone fragments, a pre-trained segmentation network was used to create initial segmentations for the ilium and sacrum <ref type="bibr" target="#b12">[13]</ref>. Then, these labels were further modified and annotated by two annotators and checked by a senior expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation</head><p>We compared the proposed method (FDMSS-UNet) against the network without smooth transition and deep supervision (FDM-UNet) and the network without distance weighting (UNet) in an ablation study. In addition, we also compared the traditional max-flow segmentation method. In the five-fold cross-validation, each model was trained for 2000 epochs per fold. The network input was augmented eight times by mirror flip. The learning rate in ADAM optimizer was set to 0.0001. λ back was set to 0.2. λ ce was set to 1. The initial λ F DM was set to 16. The termination number for smooth transition τ was set to 500 epochs.</p><p>The models were implemented in PyTorch 1.12. The experiments were performed on an Intel Xeon CPU with 40 cores, a 256 GB memory, and a Quadro RTX 5000 GPU. The comprehensive code and pertinent details are provided at https://github.com/YzzLiu/FracSegNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head><p>We calculate the Dice similarity coefficient (DSC) and the 95th percentile of Hausdorff Distance (HD) to evaluation the performance. In addition, we evaluated the local Dice (LDSC) within the 10 mm range near the CFS to assess the performance in the critical areas. We reported the performance on iliac main fragment (I-main), iliac other fragment(s) (I-other), sacral main fragment (Smain), sacral other fragment(s) (S-other), and all together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Figure <ref type="figure" target="#fig_1">2</ref> shows a qualitative comparison among different methods. Max-flow was able to generate reasonable segmentation only when the CFS is clear and mostly non-contact. UNet correctly identified the fracture fragments, but was often confused by the complicated CFS regions, resulting in imprecise fracture lines. The introduction of FDM weighting and deep supervision with smooth transition successfully improved the performance near the CFS, and achieved the overall best result.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the quantitative results. The results on the main fragments were better than other fragments due to the larger proportion. The deep learning methods had much higher success rates in identifying the fragments, resulting in significantly better results in minor fragments than max-flow, with p &lt; 0.05 in paired t-test. Introducing the FDM significantly improved the prediction accuracy in the CFS region (p &lt; 0.05). Although FDM-UNet achieved the best LDSC results in several parts, it compromised the global performance of DSC and HD significantly, compared to FDMSS-UNet. The deep supervision and smooth transition strategies stabilized the training, and achieved the overall best results, with balanced local and global performance.</p><p>The average inference time for the fracture segmentation network was 12 s. The overall running time for processing a pelvic CT was 0.5 to 2 min, depending on the image size and the number of fractured bones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>We have introduced a pelvic fracture CT segmentation method based on deep convolutional networks. A fracture segmentation network was trained with a distance-weighted loss and multi-scale deep supervision to improve fracture surface delineation. We evaluated our method on 100 pelvic fracture CT scans and made our dataset and ground truth publicly available. The experiments demonstrated the method's effectiveness on various types of pelvic fractures. The FDM weighted loss, along with multi-scale deep supervision and smooth transition, improved the segmentation performance significantly, especially in the areas near fracture lines. Our method provides a convenient tool for pelvis-related research and clinical applications, and has the potential to support subsequent automatic fracture reduction planning. One obstacle for deep learning-based fracture segmentation is the variable number of bone fragments in different cases. The ultimate goal of this study is to perform automatic fracture reduction planning for robotic surgery, where the main bone fragment is held and moved to the planned location by a robotic arm, whereas minor fragments are either moved by the surgeons' hands or simply ignored. In such a scenario, we found isolating the minor fragments usually unnecessary. Therefore, to define a consistent labeling strategy in annotation, we restrict the number of fragments of each bone to three. This rule of labelling applies to all 100 cases we encountered. Minor fragments within each label can be further isolated by CCA or handcrafting when needed by other tasks.</p><p>We utilize a multi-scale distance-weighted loss to guide the network to learn features near the fracture site more effectively, boosting the local accuracy without compromising the overall performance. In semi-automatic pipelines where human operators are allowed to modify and refine the network predictions, an accurate initial segmentation near the fracture site is highly desirable because the fracture surface itself is much more complicated, often intertwined and hard to draw by manual operations. Therefore, with the emphasis on fracture surface, even when the prediction from the network is inaccurate, manual operations on 3D view can suffice for most modifications, eliminating the need for the inefficient slice-by-slice handcrafting. In future studies, we plan to integrate the proposed method into an interactive segmentation and reduction planning software and evaluate the overall performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed pelvic fracture segmentation framework.</figDesc><graphic coords="3,44,79,54,14,334,60,230,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pelvic fracture segmentation results from different methods.</figDesc><graphic coords="7,44,31,54,59,335,44,226,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative segmentation results using different methods. (L)DSC is reported in percentage value, and HD is in mm. Best values are shown in bold. Statistically significant differences compared to FDMSS-UNet are indicated by * (p &lt; 0.05).</figDesc><table><row><cell></cell><cell>I-main</cell><cell>I-other</cell><cell>S-main</cell><cell>S-other</cell><cell>All</cell></row><row><cell>Max-flow</cell><cell cols="5">DSC 94.52 ± 0.56* 24.67 ± 3.72* 89.83 ± 1.89* 23.00 ± 3.18* 69.21 ± 4.01*</cell></row><row><cell></cell><cell cols="5">LDSC 74.53 ± 0.99* 24.24 ± 3.26* 62.06 ± 1.91* 27.31 ± 3.40* 54.11 ± 3.20*</cell></row><row><cell></cell><cell>HD95 68.56*</cell><cell>77.45*</cell><cell>45.16*</cell><cell>90.76*</cell><cell>73.95*</cell></row><row><cell>UNet</cell><cell cols="5">DSC 99.38 ± 0.13* 92.88 ± 1.30* 98.66 ± 0.33* 87.91 ± 1.55* 96.60 ± 0.89*</cell></row><row><cell></cell><cell cols="5">LDSC 90.68 ± 1.22* 86.58 ± 1.69* 84.44 ± 1.77* 78.50 ± 2.50* 86.86 ± 1.70*</cell></row><row><cell></cell><cell>HD95 43.42*</cell><cell>50.21*</cell><cell>27.14*</cell><cell>76.69*</cell><cell>46.11*</cell></row><row><cell>FDM-UNet</cell><cell cols="5">DSC 99.37 ± 0.13* 96.15 ± 0.79* 98.73 ± 0.27* 92.20 ± 0.74* 97.88 ± 0.51*</cell></row><row><cell></cell><cell cols="5">LDSC 94.43 ± 0.34 93.94 ± 0.36 92.88 ± 0.39* 91.80 ± 0.51 93.81 ± 0.38</cell></row><row><cell></cell><cell>HD95 34.75*</cell><cell>52.03*</cell><cell>28.99*</cell><cell>65.26*</cell><cell>45.79*</cell></row><row><cell cols="6">FDMSS-UNet DSC 99.78 ± 0.03 99.05 ± 0.10 99.57 ± 0.04 97.87 ± 0.17 99.38 ± 0.09</cell></row><row><cell></cell><cell cols="2">LDSC 94.79 ± 0.51 93.75±0.56</cell><cell>91.02±1.62</cell><cell cols="2">93.65 ± 0.70 93.79±0.78</cell></row><row><cell></cell><cell>HD95 13.24</cell><cell>19.43</cell><cell>14.85</cell><cell>19.89</cell><cell>17.12</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">Beijing Science and Technology Project</rs> (Grants No. <rs type="grantNumber">Z221100003522007</rs> and <rs type="grantNumber">Z201100005420033</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_UpVrFZT">
					<idno type="grant-number">Z221100003522007</idno>
				</org>
				<org type="funding" xml:id="_thatWXK">
					<idno type="grant-number">Z201100005420033</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-automatic segmentation of fractured pelvic bones for surgical planning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fornaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harders</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-11615-5_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-11615-59" />
	</analytic>
	<monogr>
		<title level="m">ISBMS 2010</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bello</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Cotin</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5958</biblScope>
			<biblScope unit="page" from="82" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robot-assisted autonomous reduction of a displaced pelvic fracture: a case report and brief literature review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Med</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1598</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Head and neck tumor segmentation with deeplysupervised 3D UNet and progression-free survival prediction with linear model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ghimire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-98253-9_13</idno>
		<idno>978-3-030-98253-9 13</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">HECKTOR 2021</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Oreiller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hatt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13209</biblScope>
			<biblScope unit="page" from="141" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fracture reduction planning and guidance in orthopaedic trauma surgery via multi-body image registration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">101917</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automated design of deep learning methods for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jäger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08128</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep-learning-assisted detection and segmentation of rib fractures from CT scans: development and validation of FracNet</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EBioMedicine</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page">103106</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Basics of computer-assisted orthopaedic surgery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kowal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Nolte</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-36691-1_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-36691-11" />
	</analytic>
	<monogr>
		<title level="m">Navigation and MIS in Orthopedic Surgery</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Stiehl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Konermann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Haaker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Digioia</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="2" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fully automatic and fast segmentation of the femur bone from 3D-CT images with no shape prior</title>
		<author>
			<persName><forename type="first">M</forename><surname>Krčah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Blanc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2087" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient cascaded V-Net optimization for lower extremity CT segmentation validated using bone morphology assessment</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Kuiper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Orthop. Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2894" to="2907" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Algorithm for segmentation and reduction of fractured bones in computer-aided preoperative surgery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Essomba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Biomedical and Bioinformatics Engineering</title>
		<meeting>the 3rd International Conference on Biomedical and Bioinformatics Engineering</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="12" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning with context encoding for single-stage cranial bone labeling and landmark localization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Linguraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Porras</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-128" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="286" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning to segment pelvic bones: large-scale CT datasets and baseline models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="749" to="756" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A variant form of 3D-UNet for infant brain segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Usama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="613" to="623" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated fractured bone segmentation and labeling from CT images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Ruikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Hegadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Syst</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computer-assisted orthopaedic surgery and robotic surgery in total hip arthroplasty</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sugano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Orthop. Surg</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep neural networks for automatic detection of osteoporotic vertebral fractures on CT scans</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hassanpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="8" to="15" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detecting pelvic fracture on 3D-CT using deep convolutional neural networks with multi-orientated slab images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ukai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph cuts and shape constraint based automatic femoral head segmentation in CT images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Symposium on Image Computing and Digital Medicine</title>
		<meeting>the Third International Symposium on Image Computing and Digital Medicine</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-scale attention and deep supervision-based 3D UNet for automatic liver segmentation from CT</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Biosci. Eng. MBE</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1297" to="1316" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An automated fracture detection from pelvic CT images with 3-D convolutional neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Symposium on Community-Centric Systems (CcS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A spatially continuous max-flow and mincut framework for binary labeling problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="559" to="587" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
