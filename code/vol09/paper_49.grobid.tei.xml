<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration</title>
				<funder ref="#_gpJyHmn">
					<orgName type="full">China Scholarship Council</orgName>
				</funder>
				<funder ref="#_GNWTRzq">
					<orgName type="full">Bavarian State Ministry of Science and Arts within the framework of the &quot;Digitaler Herz-OP</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
							<email>baochang.zhang@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">German Heart Center Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shahrooz</forename><surname>Faghihroohi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">Farid</forename><surname>Azampour</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuting</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Ghotbi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">HELIOS Hospital west of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heribert</forename><surname>Schunkert</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">German Heart Center Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">German Centre for Cardiovascular Research</orgName>
								<orgName type="institution">Munich Heart Alliance</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="515" to="524"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">32A9BBDFE67DE86354AD1784B2A24F97</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_49</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>X-ray/CT Registration</term>
					<term>Self-supervised Learning</term>
					<term>Patient-Specific Model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The accurate estimation of X-ray source pose in relation to pre-operative images is crucial for minimally invasive procedures. However, existing deep learning-based automatic registration methods often have one or some limitations, including heavy reliance on subsequent conventional refinement steps, requiring manual annotation for training, or ignoring the patient's anatomical specificity. To address these limitations, we propose a patient-specific and self-supervised end-to-end framework. Our approach utilizes patient's preoperative CT to generate simulated X-rays that include patient-specific information. We propose a self-supervised regression neural network trained on the simulated patient-specific X-rays to predict six degrees of freedom pose of the X-ray source. In our proposed network, regularized autoencoder and multi-head self-attention mechanism are employed to encourage the model to automatically capture patient-specific salient information that supports accurate pose estimation, and Incremental Learning strategy is adopted for network training to avoid over-fitting and promote network performance. Meanwhile, an novel refinement model is proposed, which provides a way to obtain gradients with respect to the pose parameters to further refine the pose predicted by the regression network. Our method achieves a mean projection distance of 3.01 mm with a success rate of 100% on simulated X-rays, and a mean projection distance of 1.55 mm on X-rays. The code is available at github.com/BaochangZhang/PSSS_registration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Augmentation of intra-operative X-ray images using the pre-operative data (e.g., treatment plan) has the potential to reduce procedure time and improve patient outcomes in minimally invasive procedures. However, surgeons must rely on their clinical knowledge to perform a mental mapping between pre-and intraoperative information, since the pre-and intra-operative images are based on different coordinate systems. To utilize pre-operative data efficiently, accurate pose estimation of the X-ray source relative to pre-operative images (or called registration between pre-and intra-operative data) is necessary and beneficial to relieve surgeon's mental load and improve patient outcomes.</p><p>Although 2D/3D registration methods for medical images have been widely researched and systematically reviewed <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b17">17]</ref>, developing an automatic endto-end registration method remains an open issue. For conventional intensitybased registration, it is formulated as an iterative optimization problem based on similarity measures. A novel similarity measure called weighted local mutual information is proposed to perform solid vascular 2D-3D registration <ref type="bibr" target="#b11">[11]</ref> but has a limited capture range, becoming inefficient and prone to local minima if initial registration error is large. A good approach that directly predicts the spatial mapping relationship between simulated X-rays and real X-rays using a neural network is put forward <ref type="bibr" target="#b12">[12]</ref> but requires an initialization. Some supervised learning tasks, such as anatomical landmark detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">3]</ref>, are defined to develop a robust initialization scheme. When used to initialize an optimizer <ref type="bibr" target="#b14">[14]</ref> for refinement, they can lead to a fully automatic 2D/3D registration solution <ref type="bibr" target="#b3">[3]</ref>. But extensive manual annotation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">3]</ref> or pairwise clinical data <ref type="bibr" target="#b12">[12]</ref> is needed for training, which is time-and labor-consuming when expanding to new anatomies, and the robustness of these methods might be challenged due to the neglect of patient's anatomical specificity <ref type="bibr" target="#b0">[1]</ref>. A patient-specific landmark refinement scheme is then proposed, which contributes to model's robustness when applied intraoperatively <ref type="bibr" target="#b2">[2]</ref>. Nevertheless, the final performance of this automatic registration method still relies on conventional refinement step based on derivative-free optimizer (i.e., BOBYQA), which limits the computational efficiency of deep learning-based registration. Meanwhile, some studies employ regression neural networks to directly predict slice's pose relative to pre-operative 3D image data <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b15">15]</ref>. While the simplicity of these approaches is appealing, the applicability of these methods is constrained by their performance and are more suitable as initialization.</p><p>In this paper, we propose a purely self-supervised and patient-specific end-toend framework for fully automatic registration of single-view X-ray to preoperative CT. Our main contributions are as follows: <ref type="bibr" target="#b0">(1)</ref> The proposed method eliminates the need for manual annotation, relying instead on self-supervision from simulated patient-specific X-rays and corresponding automatically labeled poses, which makes the registration method easier to extend to new medical applications. (2) Regularized autoencoder and multi-head self-attention mechanism are embedded to encourage the model to capture patient-specific salient information automatically, therefore improving the robustness of registration; and an novel refinement model is proposed to further improve registration accuracy.</p><p>(3) The proposed method has been successfully evaluated on X-rays, achieving an average run-time of around 2.5 s, which meets the requirements for clinical applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>An overview of the proposed framework is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, which can be mainly divided into two parts that are introduced hereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pose Regression Model</head><p>Network Architecture. A regression neural network architecture is developed to estimate the six degrees of freedom (DOF) pose from an input X-ray image, which consists of a regularized autoencoder, a multi-head self-attention block (MHSAB), a learnable 2D position embedding, and a multilayer perceptron (MLP), as shown in Fig. <ref type="figure" target="#fig_0">1f</ref> .</p><p>Regularized Autoencoder. The autoencoder consists of a carefully selected backbone and a decoder. The first six layers of EfficientNet-b0 <ref type="bibr" target="#b16">[16]</ref> are chosen as the backbone, where the first convolution layer is adapted by setting the input channel to 1 (shown as blue trapezoidal block in Fig. <ref type="figure" target="#fig_0">1f</ref> ). For image-based pose estimation, edge, as an important structural information, is more advantageous than intensity value. Hence, a structure aware loss function L s is defined based on zero-normalized cross-correlation(ZNCC) to constrain the features extracted by encoder to contain necessary structure information, which is formulated as,</p><formula xml:id="formula_0">L s (G I , Y ) = 1 σ GI σ Y 1 N u, v (G I (u, v) -E (G I )) (Y (u, v) -E(Y ))<label>(1)</label></formula><formula xml:id="formula_1">G I = ∂I ∂u 2 + ∂I ∂v 2 (2)</formula><p>Here, Y is the output of the decoder, I is the input image, G I is the normalized gradient of the input image, and N is the number of pixels in the input image. E() is the expectation operator, and σ is standard deviation operator.</p><p>Multi-head Self-attention Block and Positional Embedding. First, inspired by <ref type="bibr" target="#b20">[19]</ref>, a 3 × 1 convolutional layer, a 1 × 3 convolutional layer, and a 1 × 1 convolutional layer are used to generate the query (q ∈ R n×hw×C/n ), key (k ∈ R n×hw×C/n ), and value (v ∈ R n×hw×C/n ) representations in the features (f ∈ R h×w×C ) extracted by backbone respectively, which capture the edge information in the horizontal and vertical orientation. And the attention weight (A ∈ R n×hw×hw ) is computed by measuring the similarity between q and k according to,</p><formula xml:id="formula_2">A = Sof tmax qk T C/n (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where n is the number of heads, C is number of channels, and h, w are the height and width of features. Using the computed attention weights, the output of MHSAB f a is computed as,</p><formula xml:id="formula_4">f a = Av + f (4)</formula><p>Second, in order to make the proposed method more sensitive to spatial transformation, 2D learnable positional embedding is employed to explicitly incorporate the object's position information.</p><p>Incremental Learning Strategy. Based on Incremental Learning strategy, the network is trained for 100 epochs with a batch size of 32 using the Adam optimizer (learning rate is 0.002, decay of 0.5 per 10 epochs). After training for 40 epochs, the training dataset will be automatically updated if the loss computed on the validation set does not change frequently (i.e., less than 10 percent of the maximum of loss) within 20 epochs, which allows the network to observe a wider range of poses while avoiding over-fitting. Final loss function is a weighted sum of the structure aware loss L s for autoencoder and L2 Loss between the predicted pose and the ground truth, which is formulated as,</p><formula xml:id="formula_5">loss (I, p) = P (I) -p 2 + αL s (G I , D(I)) (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where α is set as 5, I is the input DRR image, p is the ground truth of pose, P (I) is predicted pose, and D(I) is the output of autoencoder.</p><p>Pre-processing and Data Augmentation. In order to improve the contrast of the digitally reconstructed radiographs (DRRs) and reduce the gap to real X-rays, the DRR is first normalized by Z-score and then normalized using the sigmoid function, which maps the image into the interval [0, 1] with a mean of 0.5. Then contrast-limited adaptive histogram equalization (CLAHE) is employed to enhance the contrast. For data augmentation, random brightness adjustment, random adding offset, adding Gaussian noise, and adding Poisson noise are adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Refinement Model</head><p>A novel refinement model is proposed to further refine the pose predicted by regression network as shown in Fig. <ref type="figure" target="#fig_0">1(d</ref>). Specifically, inspired by DeepDRR <ref type="bibr" target="#b18">[18]</ref>, a differentiable DeepDRR (Diff-DeepDRR) generator is developed. Our proposed Diff-DeepDRR generator offers two main advantages compared with DeepDRR, including the ability to generate a 256 2 pixel DRR image in just 15.6 ms, making it fast enough for online refinement, and providing a way to obtain gradients with respect to the pose parameters. The refinement model takes the input Xray image with an unknown pose as the fixed image and has six learnable pose parameters initialized by the prediction of pose regression network. Then the proposed Diff-DeepDRR generator utilizes the six learnable pose parameters to generate DRR image online, which is considered as the moving image. The ZNCC is then used as the loss function to minimize the distance between the fixed image and the moving image. With the powerful PyTorch auto-grad engine, the six pose parameters are learned iteratively. For each refinement process, the refinement model is online optimized for 100 iterations using an Adam optimizer with a learning rate of 5.0 for translational parameters and 0.05 for rotational parameters, and outputs the pose with the minimal loss score.</p><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>Our method is evaluated on six DRR datasets and one X-ray dataset. The six DRR datasets are generated from five common preoperative CTs and one specific CT with previous surgical implants, while the X-ray dataset is obtained from a Pelvis phantom containing a metal bead landmark inside. The X-ray dataset includes a CBCT volume and ten X-ray images with varying poses. For each CT or CBCT, 12800 DRR images with a reduced resolution of 256 2 pixels are generated using the DeepDRR method, which are divided into training (50%), validation (25%), and test (25%) sets. The DRR generation system is configured based on the geometry of a mobile C-arm, with a 432 mm detector, 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Design and Evaluation Metrics</head><p>To better understand our work, we conduct a series of experiments, for example, 1) A typical pose regression model consists of a CNN backbone and an MLP. To find an efficient backbone for this task, EfficientNet, ResNet <ref type="bibr" target="#b5">[5]</ref>, and DenseNet <ref type="bibr" target="#b6">[6]</ref> were studied. 2) A detailed ablation experiment was performed, where you can learn the evolution process and the superiority of our proposed method. 3) Through the experiment on X-ray data, you can know whether the proposed method trained only on DRRs can be generalized to X-ray applications.</p><p>To validate the performance of our method on DRR datasets, we employed five measurements including 2D mean projection distance (mPD), 3D mean target registration error (mTRE) <ref type="bibr" target="#b7">[7]</ref>, Mean Absolute Error (MAE), and success rate (SR) <ref type="bibr" target="#b2">[2]</ref>, where success is defined as an mTRE of less than 10 mm. Cases with mTRE exceeding 10 mm were excluded from average mPD and mTRE measurements. For the validation on X-rays, projection distance (PD) is used to measure the positional difference of the bead between the X-ray and the final registered DRR. In addition, NCC <ref type="bibr" target="#b13">[13]</ref>, structural similarity index measure (SSIM), and contrast-structure similarity (CSS) <ref type="bibr" target="#b9">[9]</ref> are employed to evaluate the similarity between the input X-ray and final registered DRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Choice of Backbone and Ablation Study</head><p>The performance of three models with different backbones were evaluated on the first DRR dataset as reported in Table <ref type="table" target="#tab_0">1</ref>. Res-backbone <ref type="bibr" target="#b15">[15]</ref> means that the first four layers of ResNet-50 as the backbone; Dense-backbone means that the first six layers of DenseNet-121 as the backbone; Ef-backbone means that the first six layers of EfficientNet-b0 as the backbone. Compared with the other two networks, Ef-backbone achieves a higher SR of 88.66%, and reduces parameter size and FLOPs by an order of magnitude. Therefore, Ef-backbone is chosen and regarded as baseline for further studies. Then, a second set of models was trained and evaluated for a detailed ablation study, and the experimental results are shown in Table <ref type="table" target="#tab_1">2</ref>. A means the aforementioned baseline; B means adding regularized autoencoder; C means adding multi-head self-attention block and position embedding; D means using refinement model; * means the network is trained via Incremental Learning strategy. It is clear that each module we proposed plays a positive role in this task, and our method makes arresting improvements, e.g., compared with baseline, the SR increased from 88.66% to 100%. In addition, observing the visualized self-attention map shown in Fig. <ref type="figure" target="#fig_2">2</ref>(b), we find that the proposed network does automatically capture some salient anatomical regions, which pays more attention to bony structures. Meanwhile, the distribution of attention changes with the view pose, and it seems that the closer to the detector, the bone region gets more attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results on DRR Datasets and X-Ray Dataset</head><p>The experimental results of our proposed method on six DRR datasets are shown in Table <ref type="table" target="#tab_1">2</ref>. It is worth noticing that the success rate of our method has achieved 100% on all datasets and the average of mTRE on six datasets achieves 2.67 mm.  For the X-ray dataset, the quantitative evaluation results on 10 X-ray cases are shown in Table <ref type="table" target="#tab_2">3</ref>, achieving a mPD of 1.55 mm, which demonstrates that our method can be successfully generalized to X-ray. More intuitive results are shown in Fig. <ref type="figure" target="#fig_2">2</ref>. Observing the dotted circled areas on Fig. <ref type="figure" target="#fig_2">2</ref>(e&amp;f), we find that the network only makes coarse predictions for X-rays due to unavoidable artifacts on the CBCT boundaries, and the proposed refinement model facilitates accurate pose estimation, which can be confirmed by the overlapping of edges from the X-rays and the final registered DRRs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Discussion</head><p>In this paper, we present a patient-specific and self-supervised end-to-end approach for automatic X-ray/CT rigid registration. Our method effectively addresses the primary limitations of existing methods, such as requirement of manual annotation, dependency on conventional derivative-free optimization, and patient-specific concerns. When field of view of CT is not smaller than that of X-ray, which is often satisfied in clinical routines, our proposed method would perform very well without any additional post-process. The quantitative and qualitative evaluation results of our proposed method illustrates its superiority and its ability to generalize to X-rays even when trained solely on DRRs.</p><p>For our experiments, the validation on X-ray of phantom cannot fully represent the performance on X-ray of real patients, but it shows that the proposed method has high potential. Meanwhile, domain randomization could reduce the gap between DRR and real X-ray images, which would allow methods validated on phantoms to perform better also on real X-ray. For the runtime aspect, our patient-specific regression network can complete the training phase within one hour using an NVIDIA GPU (Quadro RTX A6000), which meets the requirement of clinical application during pre-operative planning phase. Meanwhile, the proposed network achieves an average inference time of 6ms per image with a size of 256 2 , when considering the run-time of the proposed refinement model, the total cost is approximately 2.5 s, which also fully satisfies the requirement for clinical application during intra-operative phase.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of our proposed framework (a-e) and the proposed regression neural network (f). The framework consists of five parts: (a) DRR generation, (b) Incremental Learning strategy for training, (c) inference phase, (d) refinement model, and (e) outputting the predicted pose.</figDesc><graphic coords="3,55,98,121,55,340,18,299,65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>3 mm pixel size, 742.5 mm source-isocenter distance, and 517.15 mm detector-isocenter distance. The center of the CT or CBCT volume is moved to the isocenter of the device. For pose sampling, each 6 DOF pose consists of three rotational and three translational parameters. The angular and orbital rotation are uniformly sampled from [-40 • , 40 • ], and the angle of in-plane rotation in the detector plane is uniformly sampled from [-10 • , 10 • ]. Translations are uniformly sampled from [-70 mm, 70 mm].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Results on X-ray cases. (a) X-rays, (b) corresponding attention visualization, (c) initial registered DRRs from pose regression network, (d) final registered DRRs from refinement mode, (e) initial registration error map, (f) final registration error map. Red edges in (e&amp;f) come from (a), green edge in (e) comes from (c), green edge in (f) comes from (d). (Color figure online)</figDesc><graphic coords="8,56,79,54,65,310,96,164,05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The experimental results of different backbones. Rx, Ry, Rz, Tx, Ty and Tz are the mAE measured on three rotational parameters and three translational parameters.</figDesc><table><row><cell>backbone</cell><cell cols="8">mTRE↓ (mm) mPD↓ (mm) Rx↓ (degree) Ry↓ (degree) Rz↓ (degree) Tx↓ (mm) Ty↓ (mm) Tz↓ (mm) FLOPs (G) Paras (M) SR↑ (%)</cell></row><row><cell></cell><cell>mean std</cell><cell>mean std</cell><cell>mean std</cell><cell>mean std</cell><cell>mean std</cell><cell>mean std mean std mean std</cell><cell></cell></row><row><cell cols="2">Res-backbone [15] 7.68 3.08</cell><cell cols="2">7.63 2.05 1.39 1.13</cell><cell>1.31 1.05</cell><cell>0.99 0.88</cell><cell>3.06 2.63 3.03 2.45 4.84 3.70 17.2</cell><cell>9.05</cell><cell>67.13</cell></row><row><cell cols="2">Dense-backbone 6.63 2.57</cell><cell cols="2">6.94 2.10 1.11 0.89</cell><cell>1.28 0.93</cell><cell>0.90 0.72</cell><cell>2.48 2.22 2.46 2.14 4.33 3.28 13.9</cell><cell>4.78</cell><cell>81.94</cell></row><row><cell>Ef-backbone</cell><cell>6.71 2.75</cell><cell cols="2">6.83 2.05 0.98 0.78</cell><cell>0.93 0.76</cell><cell>0.71 0.56</cell><cell>2.44 2.05 2.57 1.99 3.92 3.05 1.05</cell><cell>0.91</cell><cell>88.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The results of ablation study &amp; our method's results on 6 DRR datasets. # indicates previous surgical implants are included in this dataset.</figDesc><table><row><cell>_</cell><cell cols="7">mTRE↓ (mm) mPD↓ (mm) Rx↓ (degree) Ry↓ (degree) Rz↓ (degree) Tx↓ (mm) Ty↓ (mm) Tz↓ (mm) SR↑ (%)</cell></row><row><cell></cell><cell>mean std</cell><cell>mean std</cell><cell>mean std</cell><cell>mean std</cell><cell>mean std</cell><cell>mean std mean std mean std</cell></row><row><cell>A</cell><cell>6.71 2.75</cell><cell cols="2">6.83 2.05 0.98 0.78</cell><cell>0.93 0.76</cell><cell>0.71 0.56</cell><cell>2.44 2.05 2.57 1.99 3.92 3.05</cell><cell>88.66</cell></row><row><cell>A+B</cell><cell>6.13 2.50</cell><cell cols="2">6.39 2.07 0.93 0.78</cell><cell>0.99 0.79</cell><cell>0.62 0.52</cell><cell>2.10 1.82 2.33 1.91 3.49 2.73</cell><cell>90.97</cell></row><row><cell>A+C</cell><cell>6.17 2.65</cell><cell cols="2">6.12 1.98 1.01 0.80</cell><cell>0.85 0.68</cell><cell>0.70 0.58</cell><cell>1.99 1.75 2.05 1.74 3.72 2.88</cell><cell>91.34</cell></row><row><cell>A+B+C</cell><cell>5.92 2.57</cell><cell cols="2">6.01 2.02 1.02 0.80</cell><cell>0.83 0.65</cell><cell>0.64 0.54</cell><cell>1.92 1.75 2.05 1.78 3.39 2.71</cell><cell>93.31</cell></row><row><cell>[A+B+C]*</cell><cell>5.43 2.18</cell><cell cols="2">5.80 2.05 0.90 0.73</cell><cell>0.82 0.65</cell><cell>0.57 0.47</cell><cell>1.80 1.53 1.80 1.50 2.89 2.26</cell><cell>95.25</cell></row><row><cell cols="2">[A+B+C]*+D proposed 2.85 1.91</cell><cell cols="6">2.92 1.93 0.41 0.53 0.31 0.39 0.24 0.35 1.00 0.83 0.90 0.84 1.55 1.35 100.00</cell></row><row><cell>dataset1</cell><cell>2.85 1.91</cell><cell cols="2">2.92 1.93 0.41 0.53</cell><cell>0.31 0.39</cell><cell>0.24 0.35</cell><cell cols="2">1.00 0.83 0.90 0.84 1.55 1.35 100.00</cell></row><row><cell>dataset2</cell><cell>2.54 1.28</cell><cell cols="2">3.08 1.72 0.38 0.55</cell><cell>0.35 0.38</cell><cell>0.22 0.29</cell><cell cols="2">0.65 0.60 0.90 0.51 1.38 0.88 100.00</cell></row><row><cell>dataset3</cell><cell>2.71 1.14</cell><cell cols="2">3.30 1.45 0.26 0.31</cell><cell>0.33 0.40</cell><cell>0.23 0.26</cell><cell cols="2">0.84 0.64 1.04 0.71 1.51 1.09 100.00</cell></row><row><cell>dataset4</cell><cell>2.72 1.32</cell><cell cols="2">3.12 1.53 0.27 0.48</cell><cell>0.26 0.33</cell><cell>0.23 0.32</cell><cell cols="2">0.90 0.86 1.04 0.66 1.58 1.10 100.00</cell></row><row><cell>dataset5</cell><cell>2.73 1.03</cell><cell cols="2">3.09 1.22 0.35 0.35</cell><cell>0.35 0.33</cell><cell>0.24 0.22</cell><cell cols="2">0.74 0.69 0.97 0.58 1.52 1.09 100.00</cell></row><row><cell>dataset6 #</cell><cell>2.50 1.48</cell><cell cols="2">2.57 1.67 0.36 0.43</cell><cell>0.27 0.39</cell><cell>0.20 0.27</cell><cell cols="2">0.75 0.64 0.86 1.00 1.44 1.20 100.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The results of our method on X-ray Cases</figDesc><table><row><cell>-</cell><cell>Case1 Case2 Case3 Case4 Case5 Case6 Case7 Case8 Case9 Case10 Mean Std</cell></row><row><cell>PD↓</cell><cell>0.6762 1.8400 1.4251 2.8431 0.6770 1.3244 1.2777 1.4181 1.5259 2.5655 1.5573 0.6687</cell></row><row><cell cols="2">NCC↑ 0.9885 0.9752 0.9867 0.9941 0.9858 0.9870 0.9888 0.9943 0.9913 0.9880 0.9880 0.0051</cell></row><row><cell cols="2">SSIM↑ 0.9395 0.9220 0.9348 0.9736 0.9424 0.9456 0.9436 0.9616 0.9649 0.9412 0.9469 0.0146</cell></row><row><cell cols="2">CSS↑ 0.9427 0.9321 0.9392 0.9750 0.9453 0.9481 0.9463 0.9630 0.9664 0.9448 0.9503 0.0127</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The project was supported by the <rs type="funder">Bavarian State Ministry of Science and Arts within the framework of the "Digitaler Herz-OP</rs>" project under the grant number <rs type="grantNumber">1530/891 02</rs> and the <rs type="funder">China Scholarship Council</rs> (File No.<rs type="grantNumber">202004910390</rs>). We also thank <rs type="person">BrainLab AG</rs> for their partial support.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GNWTRzq">
					<idno type="grant-number">1530/891 02</idno>
				</org>
				<org type="funding" xml:id="_gpJyHmn">
					<idno type="grant-number">202004910390</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">X-ray-transform invariant anatomical landmark detection for pelvic trauma surgery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018, Part IV</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-3_7" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pose-dependent weights and domain randomization for fully automatic X-ray to CT registration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2221" to="2232" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic annotation of hip anatomy in fluoroscopy for robust and efficient 2D/3D registration</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grupp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="759" to="769" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transfer learning for rigid 2D/3D cardiovascular images registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-31723-2_32</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-31723-2_32" />
	</analytic>
	<monogr>
		<title level="m">PRCV 2019, Part II</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11858</biblScope>
			<biblScope unit="page" from="380" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Standardized evaluation methodology for 2-D-3-D registration</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Van De Kraats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Penney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tomazevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van Walsum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Niessen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1177" to="1189" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Breathing-compensated neural networks for real time C-arm pose estimation in lung CT-fluoroscopy registration</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unpaired stain transfer using pathology-consistent constrained generative adversarial networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1977" to="1989" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A review of 3D/2D registration methods for image-guided interventions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Markelj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tomaževič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Likar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pernuš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="642" to="661" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">2D-3D registration with weighted local mutual information in vascular interventions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="162629" to="162638" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dilated FCN for multi-agent 2D/3D medical image registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparison of similarity measures for use in 2-D-3-D medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Penney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Desmedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="586" to="595" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The BOBYQA algorithm for bound constrained optimization without derivatives</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Powell</surname></persName>
		</author>
		<idno>NA2009/06</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">26</biblScope>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Cambridge NA Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time deep pose estimation with geodesic loss for image-to-template rigid registration</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="470" to="481" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">EfficientNet: rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The impact of machine learning on 2D/3D registration for image-guided interventions: a systematic review and perspective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Robot. AI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">716007</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeepDRR -a catalyst for machine learning in fluoroscopyguided procedures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018, Part IV</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-3_12" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
