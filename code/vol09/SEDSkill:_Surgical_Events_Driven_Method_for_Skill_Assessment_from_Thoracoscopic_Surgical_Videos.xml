<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos</title>
				<funder ref="#_Rwy4ECR">
					<orgName type="full">Research Grants Council of the Hong Kong Special Administrative Region, China</orgName>
				</funder>
				<funder ref="#_YX2spkh">
					<orgName type="full">HKUST-BICI Exploratory Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinpeng</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
							<email>xiao.wei.xu@foxmail.com</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Guangdong Cardiovascular Institute</orgName>
								<orgName type="department" key="dep2">Guangdong Provincial People&apos;s Hospital (Guangdong Academy of Medical Sciences)</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaomeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="35" to="45"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">9211D4332E36DAD12F88FCE1D56C2661</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Surgical skill assessment</term>
					<term>Long-form video</term>
					<term>Thoracoscopy-assisted surgery</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Thoracoscopy-assisted mitral valve replacement (MVR) is a crucial treatment for patients with mitral regurgitation and demands exceptional surgical skills to prevent complications and enhance patient outcomes. Consequently, surgical skill assessment (SKA) for MVR is essential for certifying novice surgeons and training purposes. However, current automatic SKA approaches have inherent limitations that include the absence of public thoracoscopy-assisted surgery datasets, exclusion of inter-video relationships, and limited to SKA of a single short surgical action. This paper introduces a novel clinical dataset for MVR, which is the first thoracoscopy-assisted long-form surgery dataset to the best of our knowledge. Our dataset, unlike existing short video clips that contain single surgical action, includes videos of the whole MVR procedure that capture multiple complex skill-related surgical events. To tackle the challenges posed by MVR, we propose a novel method called Surgical Events Driven Skill assessment (SEDSkill). Our key idea is to develop a long-form surgical events-driven method for skill assessment, which is based on the insight that the skill level of a surgeon is closely tied to the occurrence of inappropriate operations such as excessively long suture repairing times. SEDSkill incorporates an event-aware module that automatically localizes skill-related events, thus extracting local semantics from long-form videos. Additionally, we introduce a difference regression block to learn imperceptible discrepancies, which enables precise and accurate surgical skills assessment. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches. Our code is available at https://github.com/xmed-lab/SEDSkill.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Thoracoscopy-assisted mitral valve replacement (MVR) has become routine for the treatment of mitral valve regurgitation <ref type="bibr" target="#b3">[4]</ref>. Compared to other surgeries such as laparoscopic operations, thoracoscopy-assisted MVR requires higher surgical skills due to the intricate structure of the heart, the mitral valve's proximity to other vital cardiac structures, and the geometric limitations of the surgical field <ref type="bibr" target="#b10">[11]</ref>. Improving surgical skills can prevent avoidable complications <ref type="bibr" target="#b8">[9]</ref>, leading to better patient outcomes, such as improved long-term survival and reduced postoperative complications <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Therefore, surgical skill assessment (SKA), i.e., evaluating the skill level of surgeons, is essential in the training and certification of novice surgeons <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b26">26]</ref>.</p><p>Traditionally, SKA has been reliant on manual observation by experienced surgeons either in the operating room or via recorded videos, as described by Reznick in his work on teaching <ref type="bibr" target="#b19">[19]</ref>. However, this method is subjective, timeconsuming, and not very efficient for use in surgical education. To address these limitations, researchers have increasingly focused on developing automatic SKA tools. While current automatic SKA approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22]</ref> have demonstrated success on simulated and laparoscopic datasets, their application to thoracoscopy-assisted MVR poses several challenges. First, to the best of our knowledge, there are no publicly available clinical datasets for thoracoscopyassisted surgery. Second, most existing methods <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17]</ref> focus solely on the global information within a single video to perform SKA, such as regressing a singular skill score from the video. However, these methods disregard the inter-video information, such as subtle differences between various videos, that could be critical in predicting surgical skill scores <ref type="bibr" target="#b12">[13]</ref>. For instance, differences in haemorrhage loss, suture repairing times, and thread twining times among videos can have a significant impact on the final scores. Generally, more thread winding, haemorrhage, and suture repairing can indicate a lower skill level; see Fig. <ref type="figure" target="#fig_0">1(a)</ref>.</p><p>To address the above challenges, we collect a new dataset for SKA, which is the first-ever long-form thoracoscopy-assisted MVR video dataset. Our dataset offers longer video duration and more surgical events with corresponding labels in comparison to the currently available public datasets such as JIGSAWS <ref type="bibr" target="#b7">[8]</ref> or HeiChole <ref type="bibr" target="#b24">[24]</ref>; see Fig. <ref type="figure" target="#fig_0">1(b)</ref>. Then, we present a novel Surgical Events Driven Skill assessment (SEDSkill) method to address the limitations of current automatic methods for MVR assessment. Unlike prior work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">17]</ref>, our key idea is to develop a long-form surgical events-driven method for skill assessment, which is based on the crucial insight that the skill level of a surgeon is closely tied to the occurrence of inappropriate operations such as excessively long suture repairing times. To achieve it, we propose a novel local-global difference method that can learn inter-video relations between both the global long-form and local surgical events correlated semantics. The method includes an event-aware module and a difference regression module. The event-aware module can automatically localize skill-related surgical events and extract their corresponding features to represent the local event semantics. As surgical skill is highly correlated with the occurrence of inappropriate events, this module is crucial for precise SKA. To enable the accurate detection of slight differences between videos, our difference regression module captures the relationships among videos and enhances the model's ability to detect subtle variations. By incorporating video-wise and event-wise difference learning, our framework can capture both local and global inter-video relations, thereby enabling precise SKA.</p><p>In summary, our contributions are three-fold: <ref type="bibr" target="#b0">(1)</ref> We introduce a novel SED-Skill method that aims to design a long-form, surgical events-driven approach for SKA, and it is the first method designed specifically for SKA in thoracoscopic surgical videos. <ref type="bibr" target="#b1">(2)</ref> We propose a local-global difference framework that can learn inter-video relations between both the global long-form and local surgical events correlated semantics, thereby enabling enhanced SKA performance. (3) Experimental results demonstrate that our method outperforms existing SKA methods, as well as methods designed for video quality assessment in computer vision. This indicates the great potential of our method for use in clinical practice. Our code will be publicly released upon paper acceptance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates our SEDSkill framework for SKA, which takes a surgical video as input and regresses a surgical skill score. Our proposed framework consists of two main modules: (a) a basic regression module to output the skill score for each input, (b) a local-global difference module to learn both video-level and event-level inter-video differences for precise assessment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic Regression Module</head><p>The basic regression module aims to regress a surgical skill score, i.e., Y i , for a raw surgical video input V i ∈ R Ti×H×W , where T i is the duration, H and W are the height and width of each frame. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the basic regression module consists of three components: a spatial feature extractor, a temporal feature extractor, and a regression block. Specifically, the video V i is first fed into the spatial feature extractor to obtain the spatial feature, denoted as F S i ∈ R Ti×Ds , where D s is the dimension of features, followed by a temporal feature extractor to model the intra-video relations to generate the global video feature</p><formula xml:id="formula_0">F G i ∈ R T ×Dt .</formula><p>Finally, a regression block consisting of several convolutional, max-pooling layers and a fully connected layer is applied to map F G i to the skill score Ŷi . Then, the loss function is to minimize the differences between predicted Ŷi and the ground-truth Y i as follows:</p><formula xml:id="formula_1">L reg = 1/N N i=1 ( Ŷi -Y i ) 2 , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where N is the number of videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Local-Global Difference Module</head><p>Surgical Event-Aware Module for Local Information. Unlike prior datasets used for skill assessment, our MVR video dataset is much longer, ranging from 30 min to 1 h, and consists of multiple skill-related events such as thread twining, haemorrhage, and suture repairing; see Fig. <ref type="figure" target="#fig_0">1</ref>. As surgical skill is highly correlated to the qualities of surgical events, directly using the basic regression module to predict a score from the long video would consider too many irrelevant parts, thus degrading the regression performance. To encourage the model to focus on the skill-related parts and remove the less informative ones, we devise an event-aware module to localize the skill-related events, i.e., haemorrhage, surgical thread twining and surgical suture repair, from the long videos and extract the local event-level features, as shown in Fig. <ref type="figure" target="#fig_1">2(b)</ref>. Specifically, given the spatial feature, e.g., F S i , we introduce an event detector, which is a transformer-like network that maps the video features to the classified logits and regressed start/end time. The detailed architecture can refer to <ref type="bibr" target="#b28">[28]</ref>. Formally, the prediction of the detector is a set for each time t which can be formulated as:</p><formula xml:id="formula_3">G t = {p t , d s t , d e t },<label>(2)</label></formula><p>where p t ∈ R 4 consists of 4 values (including the background), which indicates the probability of event category, d s t &gt; 0 and d e t &gt; 0 denote the distance between time t to start and end time of events. Note that if p t equals to zero, d s t &gt; 0 and d e t &gt; 0 are not defined. Then, following <ref type="bibr" target="#b28">[28]</ref> the loss function for the detector is defined as:</p><formula xml:id="formula_4">L det = t (L cls + λ loc 1 ct L loc ) /N + ,<label>(3)</label></formula><p>where N + is the number of positive frames, L cls is a focal loss <ref type="bibr" target="#b15">[15]</ref> and L loc is a DIoU loss <ref type="bibr" target="#b29">[29]</ref>. 1 ct is the indicator function to identify where the time t is within a event. λ loc is set to 1 following <ref type="bibr" target="#b28">[28]</ref>. Note that the detector is pre-trained and fixed during the training of the basic regression module and the local-global difference module. After obtaining the pre-trained detector, we generate the event confidence map for each video denoted by</p><formula xml:id="formula_5">A i = [a t ] Ti t=1</formula><p>, where A i ∈ R Ti×1 , a t = max p t is the confidence for each time t and T i is the duration for the V i . Then, the local event-level feature is obtained by the multiplication of the confidence values and the global video feature, i.e., F L i = A i • F S i , where F L i ∈ R Ti×Ds and • is the element-wise multiplication. Local-Global Fusion. We introduce the local-global fusion module to aggregate the local (i.e.event-level) and global (long-form video) semantics. Formally, we can define the local-global fusion as</p><formula xml:id="formula_6">F i = Fusion(F G i , F L i )</formula><p>, where F i ∈ R Ti×(Dt+Ds) . This module can be implemented by different types and we will conduct an ablation study to analyze the effect of this module in Table <ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difference Regression Block.</head><p>Most surgeries of the same type are performed in similar scenes, leading to subtle differences among surgical videos. For example, in MVR, the surgeon first stitches two lines on one side using a needle, and then passes one of the lines through to the other side, connecting it to the extracorporeal circulation tube. Although these procedures are performed in a similar way, the imperceptible discrepancies are very important for accurately assessing surgical skills. Hence, we first leverage the relation block to capture the inter-video semantics. We use the features of the pairwise videos, i.e., F i and F j , for clarity. Since attention <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">23]</ref> is widely used for capturing relations, we formulate the detailed relation block in the attention manner as follows:</p><formula xml:id="formula_7">F j→i = Attention (Q j ; K i ; V i ) = softmax Q j K i √ D V i ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">Q j = F j W q , K i = F i W k and V i = F i W v are</formula><p>linear layers, √ D controls the effect of growing magnitude of dot-product with larger D <ref type="bibr" target="#b23">[23]</ref>. Since F j→i only learn the attentive relation from F j to F i . We then learn the bi-direction attentive relation by</p><formula xml:id="formula_9">F i-j = Relation(F i , F j ) = F j→i + F i→j , where F i→j = Attention (Q i ; K j ; V j ).</formula><p>After that, we use the difference regression block to map F i-j to the difference scores Δ Ŷ . Then, we minimize the error as follows:</p><formula xml:id="formula_10">L dif f = (Δ Ŷ -ΔY ) 2 , (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>where ΔY is the ground-truth of the difference scores between the pair videos, which can be computed by |Y i -Y j |. By optimizing L dif f , the model would be able to distinguish differences between videos for precise SKA. Finally, the overall loss function of our proposed method is as follows:</p><formula xml:id="formula_12">L = L reg + λ dif f L dif f , (<label>6</label></formula><formula xml:id="formula_13">)</formula><p>where λ dif f is the hyper-parameter to control the weight between two loss functions (set to 1 empirically).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets. We collect the data from our collaborating hospitals. The data collection process follows the same protocol in a well-established study <ref type="bibr" target="#b1">[2]</ref>. The whole procedure of the surgery is recorded by a surgeon's view camera. Each surgeon will submit videotapes when performing thoracoscopy-assisted MVR in the operating rooms. We have collected 50 high-resolution videos of thoracoscopy-assisted MVR from surgeons and patients, with a resolution of 1920×1080 and 25 frames per second. Each collected video lasts 30 min -1 h. 50 videos are randomly divided into training and testing subsets containing 38, and 12 videos, respectively. To evaluate skill level, each video will be rated along various dimensions of technical skill on a scale of 1 to 9 (with higher scores indicating more advanced skill) by at least ten authoritative surgeons who are unaware of the identity of the operating surgeon. Furthermore, we also provide the annotations (including the category and corresponding start and end time) for three skill-related events, i.e., haemorrhage, surgical thread twining and surgical suture repair times. The detailed annotation examples are illustrated in Fig. <ref type="figure" target="#fig_0">1(a)</ref>.</p><p>Implementation Details. Our model is implemented on an NVIDIA GeForce RTX 3090 GPU. We use a pre-trained inception-v3 <ref type="bibr" target="#b21">[21]</ref> and MS-TCN <ref type="bibr" target="#b4">[5]</ref> as the spatial and temporal feature extractors, respectively. For each video, we sample one frame per second. As the durations of different videos vary, we resample all videos to 1000 frames. We trained our model using an Adam optimizer with learning rates initialized at 1e -3. The total number of epochs is 200, and the batch size is 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparison with the State-of-the-Art Methods</head><p>We compare our method with existing state-of-the-art methods in action quality assessment (AQA) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">27]</ref> and surgical skill assessment (SKA) <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b18">18]</ref>. Note that the spatial and temporal feature extractors for ViSA, CoRe and TPT as the same as our method. As shown in Table <ref type="table" target="#tab_0">1</ref>, our method achieved the best performance with an MAE score of 1.83 and a Corr score of 0.54. The comparison demonstrates that our method not only outperformed existing SKA methods but also outshined existing AQA methods by a clear margin in surgical skill assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>Effectiveness of Proposed Modules.  Bidirectional (Fi-j) 1.83 0.54 Fig. <ref type="figure">3</ref>. Qualitative results of sampled pairwise videos. For each video, we visualize its confidence values along video frames for skill-related events. Specifically, the green, orange, and blue lines indicate the confidences scores of thread twining (A0), haemorrhage (A1) and suture repairing (A2) along video frames. It is worth noting that the occurrence of inappropriate surgical events such as haemorrhage and more thread twining times is highly correlated with the surgical skill level. Therefore, a lower confidence value indicates a lower probability of an event occurring, leading to a higher skill score.</p><p>methods can achieve comparable performance, indicating that different fusion methods can effectively aggregate local and global information. In this paper, we select concatenation as our default fusion method.</p><p>Effect of the Attention in the Difference Block. In Sect. 2.2, we implement the difference block by the attention, shown in Eq. 4. Here, we conduct the ablation study on the effect of different attentions in Table <ref type="table" target="#tab_4">4</ref>. The results indicate that using bidirectional attention, i.e., F i-j , can achieve better performance, compared with the uidirectional one.</p><p>Qualitative Results. Figure <ref type="figure">3</ref> shows the qualitative results of sampled videos to analyze the effectiveness of our method. The upper video presents a low surgical skill score, i.e., 5.0, while the score for the lower video is higher, i.e., 9.0. By comparing the two videos, the confidence lines generated by our model can find several factors that lower the skill score for the upper video, such as haemorrhage, multiple rewinds, and needle threading. Hence the upper video only obtains a skill score of 5.0, while the lower one achieves the better score, i.e., 8.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper introduces a new surgical video dataset for evaluating thoracoscopyassisted surgical skills. This dataset constitutes the first-ever collection of long surgical videos used for skill assessment from real operating rooms. To address the challenges posed by long-range videos and multiple complex surgical actions in videos, we propose a novel SEDSkill method that incorporates a local-global difference framework. In contrast to current methods that solely rely on intravideo information, our proposed framework leverages local and global difference learning to enhance the model's ability to use inter-video relations for accurate SKA in the MVR scenario.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Visualization of different surgical skill datasets: Our collected dataset, HeiChole [24] and JIGSAWS [8]. (b) Comparison of different datasets. Unlike other datasets where each video typically contains only one surgical action (e.g., dissection or suturing), our MVR dataset provides long-form videos with multiple skill-related events and their corresponding labels.</figDesc><graphic coords="3,57,63,64,40,200,77,126,31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of our proposed SEDSkill. SEDSkill consists of two main components: a basic regression module and a local-global difference module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on our MVR dataset. †: we implement existing action quality assessment methods on our dataset. : we run existing surgical skill assessment methods on our dataset.</figDesc><table><row><cell>Method</cell><cell>MAE Corr</cell></row><row><cell>CoRe † [27]</cell><cell>2.89 0.40</cell></row><row><cell>TPT † [1]</cell><cell>2.68 0.42</cell></row><row><cell cols="2">C3D-LSTM [18] 3.01 0.20</cell></row><row><cell>C3D-SVR [18]</cell><cell>2.92 0.14</cell></row><row><cell>MTL-VF [25]</cell><cell>2.35 0.31</cell></row><row><cell>ViSA [14]</cell><cell>2.31 0.33</cell></row><row><cell>Ours</cell><cell>1.83 0.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of the local-global difference module. "Base" refers to using the basic regression module containing only global information. "EAM" refers to the event-aware module capturing local information. "DRB" indicates the difference regression block extracting inter-video information. Following previous works<ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">14]</ref>, we measured the performance of our model using Spearman's Rank Correlation (Corr) and Mean Absolute Error (MAE). Lower values of Corr and MAE indicate better results.</figDesc><table><row><cell>Method</cell><cell>Base EAM DRB MAE Corr</cell></row><row><cell>Global</cell><cell>2.82 0.25</cell></row><row><cell>Local</cell><cell>2.49 0.33</cell></row><row><cell>Local-global</cell><cell>2.45 0.37</cell></row><row><cell>Global-difference</cell><cell>2.15 0.39</cell></row><row><cell>Local-difference</cell><cell>2.11 0.45</cell></row><row><cell>Full (ours)</cell><cell>1.83 0.54</cell></row><row><cell>Evaluation Metrics.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table2shows the effectiveness of our proposed local-global difference module. We can see that using the local features from the event-aware module (EAM) can outperform the global ones, which indicates the importance of skill-related events. Furthermore, incorporating the difference regression module (DRB) can benefit both local and global features, e.g., improving the MAE of Local from 2.49 to 2.11. Finally, the combination of all proposed modules can achieve the best performance, i.e., the MAE of 1.83.</figDesc><table /><note><p><p><p><p>Analysis of Local-Global Fusion.</p>We explore the effect of different types of local-global fusion in Table</p>3</p>. "Concatenation" indicates concatenating the two features in the feature dimsension. "Multiplication" indicates the element-wise multiplication of the two features. The results show that the different fusion</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Analysis of local-global fusion.</figDesc><table><row><cell></cell><cell>MAE Corr</cell></row><row><cell cols="2">Concatenation 1.83 0.54</cell></row><row><cell cols="2">Multiplication 1.98 0.45</cell></row><row><cell>Attention</cell><cell>1.85 0.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Anlysis of the attention in the difference block. "Unidirectional" and "Bidirectional" indicate the unidirectional and bidirectional attentive relations (See Eq. 4).</figDesc><table><row><cell>Attention</cell><cell>MAE Corr</cell></row><row><cell cols="2">Unidirectional (Fi→j) 1.91 0.47</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported in part by a research grant from <rs type="funder">HKUST-BICI Exploratory Fund</rs> (<rs type="grantNumber">HCIC-004</rs>) and in part by a grant from the <rs type="funder">Research Grants Council of the Hong Kong Special Administrative Region, China</rs> (Project Reference Number: <rs type="grantNumber">T45-401/22-N</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YX2spkh">
					<idno type="grant-number">HCIC-004</idno>
				</org>
				<org type="funding" xml:id="_Rwy4ECR">
					<idno type="grant-number">T45-401/22-N</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Action quality assessment with temporal parsing transformer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19772-7_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19772-725" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022, Part IV</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13664</biblScope>
			<biblScope unit="page" from="422" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surgical skill and complication rates after bariatric surgery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Birkmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1434" to="1442" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Association between surgical technical skill and long-term survival for colon cancer</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Brajcich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Oncol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="129" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mitral valve disease</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carbello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Probl. Cardiol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="425" to="478" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exploiting segment-level semantics for online phase recognition from surgical videos</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11044</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">KFC: an efficient framework for semi-supervised temporal action localization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6869" to="6878" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Support-set based cross-supervision for video grounding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11573" to="11582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">JHU-ISI gesture and skill assessment working set (JIGSAWS): a surgical activity dataset for human motion modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI Workshop: M2cai</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Complications in surgical patients</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Shackford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Osler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Surg</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="611" to="618" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tool detection and operative skill assessment in surgical videos using region-based convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="691" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Significance of thoracoscopy-assisted surgery with a minithoracotomy and hand-assisted laparoscopic surgery for esophageal cancer: the experience of a single surgeon</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kunisaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Gastrointest. Surg</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1939" to="1951" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automation of surgical skill assessment using a three-stage machine learning algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lavanchy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5197</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pairwise contrastive learning network for action quality assessment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13664</biblScope>
			<biblScope unit="page" from="457" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19772-7_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19772-727" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Surgical skill assessment via video semantic aggregation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-139" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Surgical skill assessment on in-vivo clinical data via the clearness of operating field</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32254-0_53</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32254-053" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019, Part V</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11768</biblScope>
			<biblScope unit="page" from="476" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Is motion analysis a valid tool for assessing laparoscopic skill?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ansell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torkington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surg. Endosc</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1468" to="1477" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to score olympic events</title>
		<author>
			<persName><forename type="first">P</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tran Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Teaching and testing technical skills</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Reznick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Surg</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="358" to="361" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The accordion severity grading system of surgical complications</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Strasberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Linehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Hawkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Surg</title>
		<imprint>
			<biblScope unit="volume">250</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="186" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Procedural surgical skill assessment in laparoscopic training environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Uemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="543" to="552" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Comparative validation of machine learning algorithms for surgical workflow and skill analysis with the Heichole benchmark</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.14956</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards accurate and interpretable surgical skill assessment: a video-based method incorporating recognized surgical gestures and skill levels</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yijie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mian</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-064" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020, Part III</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="668" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Teaching the surgical craft: from selection to certification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Wanzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Reznick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Probl. Surg</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="583" to="659" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Group-aware contrastive regression for action quality assessment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7919" to="7928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ActionFormer: localizing moments of actions with transformers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19772-7_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19772-729" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022, Part IV</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13664</biblScope>
			<biblScope unit="page" from="492" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distance-IoU loss: faster and better learning for bounding box regression</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12993" to="13000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
