<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sylvain</forename><surname>Thibeault</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MedICAL</orgName>
								<address>
									<settlement>Polytechnique Montreal, Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Parent</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sainte-Justine Hospital Research Center</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Samuel</forename><surname>Kadoury</surname></persName>
							<email>samuel.kadoury@polymtl.ca</email>
							<affiliation key="aff0">
								<orgName type="department">MedICAL</orgName>
								<address>
									<settlement>Polytechnique Montreal, Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sainte-Justine Hospital Research Center</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="79" to="89"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">9BDD2708569BF51D77D76493BAB7003A</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spine surgery</term>
					<term>Prediction model</term>
					<term>Neural kernel fields</term>
					<term>Upright posture</term>
					<term>Disentangled latent representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Minimally invasive spine surgery such as anterior vertebral tethering (AVT), enables the treatment of spinal deformities while seeking to preserve lower back mobility. However the intra-operative positioning and posture of the spine affects surgical outcomes. Forecasting the standing shape from adolescent patients with growing spines remains challenging with many factors influencing corrective spine surgery, but can allow spine surgeons to better prepare and position the spine prior to surgery. We propose a novel intra-operative framework anticipating the standing posture of the spine immediately after surgery from patients with idiopathic scoliosis. The method is based on implicit neural representations, which uses a backbone network to train kernels based on neural splines and estimate network parameters from intra-pose data, by regressing the standing shape on-the-fly using a simple positive definite linear system. To accommodate with the variance in spine appearance, we use a Signed Distance Function for articulated structures (A-SDF) to capture the articulation vectors in a disentangled latent space, using distinct encoding vectors to represent both shape and articulation parameters. The network's loss function incorporates a term regularizing outputs from a pre-trained population growth trajectory to ensure transformations are smooth with respect to the variations seen on first-erect exams. The model was trained on 735 3D spine models and tested on a separate set of 81 patients using pre-and intra-operative models used as inputs. The neural kernel field framework forecasted standing shape outcomes with a mean average error of 1.6 ± 0.6 mm in vertebral points, and generated shapes with IoU scores of 94.0 compared to follow-up models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Surgical treatment remains to this day the main approach to correct severe spinal deformities, which consists of rectifying the alignment of the spine over several Supported by the Canada Research Chairs and NSERC Discovery Grant RGPIN-2020-06558.</p><p>vertebral bodies with intrinsic forces. However major limiting factors remain the loss in spinal mobility, as well as the increased risk of pain and osteoarthritis in years after surgery <ref type="bibr" target="#b5">[1]</ref>. Furthermore, prone positioning during spinal surgery combined with hip extension can greatly affect post-operative outcomes of sagittal alignment and balance, particularly in degenerative surgical cases <ref type="bibr">[2]</ref>. Recent literature has demonstrated the importance to preserve the lumbar positioning, as well as the normal sagittal alignment once surgery has been completed <ref type="bibr" target="#b10">[5,</ref><ref type="bibr" target="#b12">7]</ref>. Therefore, by optimizing the intra-op posture, surgeons can help to reduce long-term complications such as lower back pain and improve spinal mobility. A predictive model anticipating the upright 3D sagittal alignment of patients during spine surgery <ref type="bibr" target="#b20">[15]</ref>, such as anterior vertebral tethering (AVT), may not only have an impact on patients to preserve their post-operative balance, but also provide information about patients at risk of degenerative problems in the lumbar region of the spine following surgery <ref type="bibr" target="#b29">[24]</ref>.</p><p>To interpret the positioning effects during surgery, identifying features leading to reliable predictions of shape representations in postural changes is a deciding factor as shown in <ref type="bibr" target="#b19">[14]</ref>, which estimated correlations with geometrical parameters. However, with the substantial changes during surgery such as the prone positioning of the spine and global orientation of the pelvic area, this becomes very challenging. Previous classification methods were proposed in the literature to estimate 3D articulated shape based on variational auto-encoders <ref type="bibr" target="#b24">[19]</ref> or with spatio-temporal neural networks <ref type="bibr" target="#b15">[10]</ref>, combining soft tissue information of the pre-operative models. The objective here is to address these important obstacles by presenting a forecasting framework used in the surgical workflow, describing the spine shape changes within an implicit neural representation, with a clinical target of estimating Cobb angles within 5 • <ref type="bibr" target="#b19">[14]</ref>.</p><p>Recently, Neural Radiance Fields (NeRF) <ref type="bibr" target="#b17">[12]</ref> have been exploited for synthetic view generation <ref type="bibr" target="#b28">[23]</ref>. The implicit representation provided by the parameters of the neural network captures properties of the objects such as radiance field and density, but can be extended to 3D in order to generate volumetric views and shapes from images <ref type="bibr">[4]</ref>, meshes <ref type="bibr" target="#b21">[16]</ref> or clouds of points <ref type="bibr" target="#b14">[9]</ref>. As opposed to traditional non-rigid and articulated registration methods based on deep learning for aligning pre-op to intra-op 2D X-rays <ref type="bibr" target="#b8">[3,</ref><ref type="bibr" target="#b31">26]</ref>, kernel methods allow to map input data into a different space, where subsequently simpler models can be trained on the new feature space, instead of the original space. They can also integrate prior knowledge <ref type="bibr" target="#b27">[22]</ref>, but have been mostly focused on partial shapes <ref type="bibr" target="#b26">[21]</ref>. However, due to the difficulty to apply neural fields in organ rendering applications in medical imaging, and the important variations in the environment and spine posture/shape which can impact the accuracy of the synthesis process, their adoption has been limited. This paper presents an intra-operative predictive framework for spine surgery, allowing to forecast on-the-fly the standing geometry of the spine following corrective AVT procedures. The proposed forecasting framework uses as input the intra-operative 3D spine model generated from a multi-view Transformer network which integrates a pre-op model (Sect. 2.1). The predictive model is based on neural field representations, capturing the articulation variations in a disentangled latent space intra-operative positioning in the prone position. The method regresses a kernel function (Sect. 2.2) based on neural splines to estimate an implicit function warping the spine geometry between various poses. The inference process is regularized with a piecewise geodesic term, capturing the natural evolution of spine morphology (Sect. 2.3). As an output, the framework produces the geometry of the spine at the first-erect examination in a standing posture following surgery, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Intra-operative 3D Model from Multi-view X-Rays</head><p>The first step of the pipeline consists of inferring a 3D model of the spine in the OR prior to instrumentation, using as input pair of orthogonal C-arm acquisitions I = {I 1 , I 2 } and previously generated pre-op 3D model <ref type="bibr" target="#b11">[6]</ref>, capturing the spine geometry in a lying prone position on the OR table. We use a multi-view 3D reconstruction approach based on Transformers <ref type="bibr" target="#b25">[20]</ref>. The Transformer-based framework integrates both a bi-planar Transformer-based encoder, which combines information and relationships between the multiple calibrated 2D views, and a 3D reconstruction Transformer decoder, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The 3D-reconstruction Transformer decodes and combines features from the biplanar views generated by the encoder, in order to produce a 3D articulated mesh model with probabilistic outputs for every spatial query token. The decoder's attention layers captures the 2D-3D relationship between the resulting grid node of the mesh and input 2D X-rays. Attention layers in the 3D network on the other hand analyzes correlations between 3D mesh landmarks to learn a 3D representation. A conditional prior using the pre-op model is integrated at the input of the 3D module to inject knowledge about the general geometry. By combining these modules (2D-2D, 2D-3D, 3D-3D) into a unified framework, feature correlations can be processed simultaneously using the attention layers in the encoder/decoder networks, generating as an output shape model S = {s 1 , . . . , s m } with s m as a vertebral mesh with a specific topology for the vertebral level m with an associated articulation vector:</p><formula xml:id="formula_0">y i = [T 1 , ; T 1 • T 2 , , . . . , T 1 • T 2 • . . . • T m ]<label>(1)</label></formula><p>representing the m inter-vertebral transformations T i between consecutive vertebrae i and i+1, each consisting of 6 DoF with translation and rotation, expressed with recursive compositions. To accommodate with the Transformer's inability to explore and synthesize multi-view associations at deeper levels, the divergence decay is slowed down within the self-attention layers by augmenting the discrepancy in multi-view embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learnable Shape Deformation Kernel</head><p>We use a training set S of N spine models from a surgical population, where each spine model is articulated into pre-operative, prone and first-erect poses (K = 3), leading to a training set of L = N × K spine models from a population of scoliotic patients. We denote S n,k as the spine model n given a particular shape articulation k, where n ∈ {1, . . . , N}, and k ∈ {1, . . . , K}. We also define s i ∈ R 3 as a sample 3D point from a spine shape in S.</p><p>We first train a kernel based on neural splines using the embeddings generated with a neural network <ref type="bibr" target="#b26">[21]</ref>, in order to map spine input points from the prone to the first-erect position. Input points s i ∈ S, associated to a shape code φ ∈ R D in latent space, with D indicating the dimensionality, are associated with a feature vector ρ(s i |S, Ω), where Ω conditions the parameters of the neural network ρ using the dataset S. Based on the learned feature points of prone and upright spine models, the data-related kernel is given by:</p><formula xml:id="formula_1">K S,Ω (s i , s j ) = K NS ([s i : ρ(s i |S, Ω)], [s j : ρ(s j |S, Ω]) (2)</formula><p>with [c : d] concatenating feature vectors c and d, representing the features of sample points i and j taken from a mesh model, respectively, and K NS representing Neural Spline kernel function. A Convolutional Occupancy Network <ref type="bibr" target="#b22">[17]</ref> is used to generate the features from sampled points in both spaces. Once the space around each input point is discretized into a volumetric grid space, PointNet <ref type="bibr" target="#b23">[18]</ref> is used in every grid cell which contains points, to extract non-zero features from each 3D cell. These are then fed to a 3D FCN, yielding output features of same size.</p><p>The neural network's implicit function is determined by finding coefficients α j associated for each point s j , such that:</p><formula xml:id="formula_2">α = [α j ] 2L j=1 = (G(S, Ω) + λI) -1 y j (3)</formula><p>which solves a 2L × 2L linear system to obtain the standing articulations (y j ), with G(S, Ω) as the gram matrix, such that G(S, Ω) ij = K S,Ω (s i , s j ), with s i and s j are the input sample points of the spine models, and regularized by the λ parameter using the identity matrix I. For new feature points s, the regressed function is evaluated using α in the following term:</p><formula xml:id="formula_3">f s (s, φ) = sj ∈S,φ α j K S,Ω (s, s j )<label>(4)</label></formula><p>which represents the ShapeNet function and maps feature points s from the original prone embedding onto the standing embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Articulation SDF</head><p>Each training shape S n,k is also associated with an articulation vector field, ξ ∈ R d . The shape embedding φ n is common across all instances n with various articulation vectors. We then train ArticulationNet, where for each instance, the shape code is maintained and each individual code for shape is updated. The articulation vector y i defined in Eq.( <ref type="formula" target="#formula_0">1</ref>) is used to define ξ i . The shape function f s trained from kernel regression is integrated into an articulation network <ref type="bibr" target="#b18">[13]</ref> using the signed distance function (A-SDF) which is determined by using s ∈ R 3 as a shape's sample point. We recall that φ and ξ are codes representing the shape and inter-vertebral articulation, respectively.</p><p>The A-SDF for the spine shape provided by f θ is represented with an autodecoder model, which includes the trained shape kernel function f s , while f a describes ArticulationNet:</p><formula xml:id="formula_4">f θ (s, φ, ξ) = f a [f s (s, φ), s, ξ] = v (5)</formula><p>with v ∈ R representing a value of the SDF on the output mesh, where the sign specifies if the sampled spine point falls within or outside the generated 3D surface model, described implicitly with the zero level-set f θ (.) = 0.</p><p>At training, the ground-truth articulation vectors ξ are used to train the parameters of the model and the shape code. Sample points s are combined with the shape code φ to create a feature vector which is used as input to the shape embedding network. A similar process is used for the articulation code ξ, which are also concatenated with sample points s. This vector is used as input to the articulation network in order to predict for each 3D point s the value of the SDF. Hence, the network uses a fully connected layer at the beginning to map the vector in the latent space, while a classification module is added in the final hidden layer to output the series of vertebrae. We define the loss function using the L1 distance term, regressing the SDF values for the M number of 3D points describing each spine shape using the function f θ :</p><formula xml:id="formula_5">L s = 1 M M m=1 f θ (s m , φ, ξ) -v m 1 (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where s m ∈ S is a sample point from the shape space, and v m is the SDF value used as ground-truth for m ∈ {1, . . . , M}. The second loss term evaluates the vertebral constellation and alignment with the inter-vertebral transformations:</p><formula xml:id="formula_7">L p = 1 M M m=1 [CE(f θ (s m , φ, ξ), p m )]<label>(7)</label></formula><p>with p m identifying the vertebral level for sample s m and CE represents the cross-entropy loss. This classification loss enables to separate the constellation of vertebral shapes s m .</p><p>To ensure the predicted shape is anatomically consistent, a regularization term L r is used to ensure generated models fall near a continuous piecewisegeodesic trajectory defined in a spatio-temporal domain. The regularization term integrates pre-generated embeddings capturing the spatiotemporal changes of the spine geometry following surgery <ref type="bibr">[11]</ref>. The piecewise manifold is obtained from nearest points with analogue features in the training data of surgical spine patients in the low-dimensional embedding, producing local regions N (s i ) with samples lying within the shape change spectrum. A Riemannian domain with samples y i is generated, with i a patient model acquired at regular time intervals. Assuming the latent space describes the overall variations in a surgical population and the geodesic trajectory covers the temporal aspects, new labeled points can be regressed within the domain R D , producing continuous curves in the shape/time domain. The overall loss function is estimated as:</p><formula xml:id="formula_8">L(S, φ, ξ) = L s (S, φ, ξ) + β p L p (S, φ, ξ) + β θ L r (8)</formula><p>with β p and β θ weighting the parts and regularization terms, respectively. During training, a random initialization of the shape codes based on Gaussian distributions is used. An optimization procedure is then applied during training for each shape code, which are used for all instances of articulations. Hence the global objective seeks to minimize for all N × K shapes, the following energy:</p><formula xml:id="formula_9">arg min φ,ξ N n=1 K k=1 L(S n,k , φ n , ξ k ).<label>(9)</label></formula><p>At test time, the intra-operative spine model (obtained in 2.1) is given as input, generating the articulated standing pose, with shape and articulation vectors, using back-propagation. The shape and articulation codes φ and ξ are randomly initialized while the network parameters stay fixed. To overcome divergence problems and avoid a local minima for Eq. ( <ref type="formula" target="#formula_9">9</ref>), a sequential process is applied where first articulated codes are estimated while shape outputs are ignored, allowing to capture the global appearance. Then, using the fixed articulated representation, the shape code is re-initialized and optimized only for φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>A mono-centric dataset of 735 spine models was used to train the articulated neural kernel field forecasting model. The dataset included pre-, intra-and postoperative thoraco/lumbar 3D models. Pre-and post-op models were generated from bi-planar X-rays using a stereo-reconstruction method (EOS system, Paris, France), integrating a semi-supervised approach generating vertebral landmarks <ref type="bibr" target="#b11">[6]</ref>. Each patient in the cohort underwent corrective spine surgery, with a main angulation range of [35 For the neural field model, batch size was set at 32, with a dropout rate of 0.4, a learning rate of 0.003, β p = 0.4, β θ = 0.5, λ = 0.25. The AdamW optimizer was used for both models <ref type="bibr" target="#b13">[8]</ref>. Inference time was of 1.2 s on a NVIDIA A100 GPU. We assessed the intra-operative modeling of the spine in a prone position using the Transformer based framework on a separate set of 20 operative patients. Ground-truth models with manually annotated landmarks on pairs of C-arm images were used as basis of comparison, yielding a 3D RMS error of 0.9 ± 0.4 mm, a Dice score (based on overlap of GT and generated vertebral meshes) of 0.94 ± 0.3 and a difference of 0.9 • ± 0.3 in the main spine angulation. These are in the clinical acceptable ranges to work in the field. Finally, the predicted standing spine shape accuracy was evaluated on a holdout set of 81 surgical patients who underwent minimally invasive corrective spine surgery. The cohort had an initial mean angulation of 48 • , and immediate followup exam at 2 weeks. Results are presented in Table <ref type="table">1</ref>. For each predicted standing spine model, errors in 3D vertebral landmarks, IoU measures and Chamfer distance, as well as differences in Cobb and lordosis angles were computed for the inferred shapes, and were compared to state-of-the art spatio-temporal and neural fields models. Ablation experiments were also conducted, demonstrating Table <ref type="table">1</ref>. 3D RMS errors (mm), IoU (%) Chamfer distance, Cobb angle difference ( o ) and lordosis angle difference ( o ) for the proposed articulated neural kernel field (A-NKF) method, compared to ST-ResNet <ref type="bibr" target="#b30">[25]</ref>, Convolutional Occupancy Network <ref type="bibr" target="#b22">[17]</ref>, DenseSDF <ref type="bibr" target="#b21">[16]</ref>, NeuralPull <ref type="bibr" target="#b14">[9]</ref> and ST-Manifold <ref type="bibr" target="#b15">[10]</ref>. The ground-truth standing 3D spine models obtained at follow-up from bi-planar X-rays were used as basis for comparison. Bottom rows shows the ablation experiments. statistically significant improvements (p &lt; 0.05) with the kernel and regularization modules to the overall accuracy. We finally evaluated the performance of the model by measuring the 3D RMS errors versus the input size of points sampled from each shape and tethered vertebral levels. We can observe limitations when using a sparser set of samples points at 250 points with a mean error of 2.8 mm, but a significant increase in performance of the kernel-based neural fields using denser inputs for each model (n=1000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed an online forecasting model predicting the first-erect spine shape based on intra-operative positioning in the OR, capturing the articulated shape constellation changes between the prone and the standing posture with neural kernel fields and an articulation network. Geometric consistency is integrated with the network's training with a pre-trained spine correction geodesic trajectory model used to regularize outputs of ArticulationNet. The model yielded results comparable to ground-truth first-erect 3D geometries in upright positions, based on statistical tests. The neural field network implicitly captures the physiological changes in pose, which can be helpful for planning the optimal posture during spine surgery. Future work will entail evaluating the model in a multi-center study to evaluate the predictive robustness and integrate the tool for real-time applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The proposed forecasting architecture of the standing spine posture following surgery. During training, randomly selected spine shapes in the prone and first-erect standing posture are used to regress a neural spline kernel (KNS). The generated shape in latent space (fs) is then used as input to an articulated signed distance function (A-SDF) capturing the inter-vertebral shape constellation (fa), transforming the signed distance values and infer the new instances from the trained kernel. Identical shape instances can yield multiple articulated instances, regardless of the shape code. At inference time, the shape and articulation code are inferred jointly by f θ from backpropagation based on the input prone shape. A regularization term is added based on a pre-trained trajectory model, capturing the natural changes of spinal curvature.</figDesc><graphic coords="3,64,47,53,84,324,09,174,91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Schematic illustration of the 3D shape inference of the spine from bi-planar X-rays in the prone position using multi-view Transformers. The model incorporates a view-divergence mechanisms to enhance features from the 2D view embeddings.</figDesc><graphic coords="4,39,39,116,63,203,29,131,59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Sample predicted 3D geometric models of the thoraco/lumbar spine in standing posture following corrective procedures. Both examples show the input intraoperative model on the left, and the predicted geometric shape on the right, alongside the first-erect X-ray images at follow-up. (b) Effect of increasing number of tethered segments and number sampled points per model on the overall accuracy.</figDesc><graphic coords="8,42,81,54,14,339,04,128,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• -65 • ]. Vertebral landmarks were validated by a surgeon. During spine surgery, a pair of C-arm images at frontal and 90 • angulations were acquired after prone positioning of the patient on the OR table and before starting the surgical instrumentation. The C-arm image pair was used to generate the input 3D model of the instrumented spine segment. For the Transformer model, batch size was set at 64 and image sizes for training were set at 225 × 225.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C-Occnet</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>St-Manifold</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A-Nkf</forename></persName>
			<affiliation>
				<orgName type="collaboration">no kernel + no regular</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A-Nkf</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>no kernel</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A-Nkf</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>no regular</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A-Nkf</forename><surname>Proposed</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adolescent idiopathic scoliosis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Dis. Primers</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supine imaging is a superior predictor of long-term alignment following adult spinal deformity surgery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Elysee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Glob. Spine J</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="631" to="637" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A comparative analysis of intensity-based 2D-3D registration for intraoperative use in pedicle screw insertion surgeries</title>
		<author>
			<persName><forename type="first">H</forename><surname>Esfandiari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Anglin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Street</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weidert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1725" to="1739" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3D hand shape and pose estimation from a single RGB image</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10833" to="10842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Increasing lumbar lordosis of adult spinal deformity patients via intraoperative prone positioning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Harimaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Lenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mishiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Bridwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Koester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Sides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2406" to="2412" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D reconstruction of the spine from biplanar X-rays using parametric models based on transversal and longitudinal inferences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Humbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De Guise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Godbout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Skalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Eng. Phys</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="687" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Key role of preoperative recumbent films in the treatment of severe sagittal malalignment</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">O</forename><surname>Karikari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spine Deform</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="568" to="575" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">2019</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural-pull: learning signed distance functions from point clouds by learning to pull space onto surfaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13495</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imageguided tethering spine surgery with outcome prediction using Spatio-temporal dynamic networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oulbacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roy-Beaudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="491" to="502" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prediction outcomes for anterior vertebral body growth modulation surgery from discriminant spatiotemporal manifolds</title>
		<author>
			<persName><forename type="first">W</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Turcot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Knez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCARS</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1565" to="1575" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nerf: representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A-SDF: learning disentangled signed distance functions for articulated shape representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13001" to="13011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Three-dimensional spine parameters can differentiate between progressive and nonprogressive patients with AIS at the initial visit: a retrospective analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Nault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mac-Thiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roy-Beaudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Labelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Pediatr. Orthop</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="618" to="623" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Measurement of spinopelvic angles on prone intraoperative longcassette lateral radiographs predicts postoperative standing global alignment in adult spinal deformity surgery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Oren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spine Deform</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="325" to="330" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepSDF: learning continuous signed distance functions for shape representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional occupancy networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58580-8_31</idno>
		<idno>978-3-030-58580-8 31</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12348</biblScope>
			<biblScope unit="page" from="523" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PointNet: deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Threedimensional morphology study of surgical adolescent idiopathic scoliosis patient from encoded geometric models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Thong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Aubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Labelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Spine J</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3104" to="3113" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-view 3D reconstruction with transformers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5722" to="5731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural fields as learnable kernels for 3D reconstruction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18500" to="18510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural splines: fitting 3D surfaces with infinitely-wide neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zorin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9949" to="9958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural fields in visual computing and beyond</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="641" to="676" />
			<date type="published" when="2022">2022</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Risk factors associated with failure to reach minimal clinically important difference after correction surgery in patients with degenerative lumbar scoliosis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spine</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="1669" to="E1676" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal residual networks for citywide crowd flows prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1655" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spineregnet: spine registration network for volumetric MR and CT image by the joint estimation of an affine-elastic deformation field</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">102786</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
