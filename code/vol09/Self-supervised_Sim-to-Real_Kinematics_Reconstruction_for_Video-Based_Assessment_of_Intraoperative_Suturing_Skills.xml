<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills</title>
				<funder ref="#_kRtbrkp">
					<orgName type="full">National Cancer Institute</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Loc</forename><surname>Trinh</surname></persName>
							<email>loctrinh@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><surname>Chu</surname></persName>
							<email>tnchu@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zijun</forename><surname>Cui</surname></persName>
							<email>zijuncui@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anand</forename><surname>Malpani</surname></persName>
							<email>malpani.anand.89@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Mimic Technologies Inc</orgName>
								<address>
									<settlement>Seattle</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cherine</forename><surname>Yang</surname></persName>
							<email>cherine.yang@cshs.org</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Urology</orgName>
								<orgName type="department" key="dep2">Cedars-Sinai Medical Center</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Istabraq</forename><surname>Dalieh</surname></persName>
							<email>idalieh@bu.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Boston University Henry M. Goldman School of Dental Medicine</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alvin</forename><surname>Hui</surname></persName>
							<email>alvin.hui@westernu.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">Western University of Health Sciences</orgName>
								<address>
									<settlement>Pomona</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oscar</forename><surname>Gomez</surname></persName>
							<email>gomez@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Hung</surname></persName>
							<email>andrew.hung@cshs.org</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Urology</orgName>
								<orgName type="department" key="dep2">Cedars-Sinai Medical Center</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">39DE4CDDB2B57E5160E115B4CD2E0588</idno>
					<idno type="DOI">10.1007/978-3-031-43996-468.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vision transformers</term>
					<term>Masked autoencoders</term>
					<term>Self-supervised learning</term>
					<term>sim-to-real generalization</term>
					<term>Suturing skill</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Suturing technical skill scores are strong predictors of patient functional recovery following robot-assisted radical prostatectomy (RARP), but manual assessment of these skills is a time and resource-intensive process. By automating suturing skill scoring through computer vision methods, we can significantly reduce the burden on healthcare professionals and enhance the quality and quantity of educational feedbacks. Although automated skill assessment on simulated virtual reality (VR) environments have been promising, applying vision methods to live ('real') surgical videos has been challenging due to: 1) the lack of kinematic data from the da Vinci R surgical system, a key source of information for determining the movement and trajectory of robotic manipulators and suturing needles, and 2) the lack of training data due to the labor-intensive task of segmenting and scoring individual stitches from live videos. To address these challenges, we developed a self-supervised pre-training paradigm whereby sim-toreal generalizable representations are learned without requiring any live kinematic annotations. Our model is based on a masked autoencoder (MAE), termed as LiveMAE. We augment live stitches with VR images during pre-training and require LiveMAE to reconstruct images from both domains while also predicting the corresponding kinematics. This process learns a visual-to-kinematic mapping that seeks to locate the positions and orientations of surgical manipulators and needles, deriving "kinematics" from live videos without requiring supervision. With</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>an additional skill-specific finetuning step, LiveMAE surpasses supervised learning approaches across 6 technical skill assessments, ranging from 0.56-0.84 AUC (0.70-0.91 AUPRC), with particular improvements of 35.78% in AUC for wrist rotation skills and 8.7% for needle driving skills. Mean-squared error for test VR kinematics was as low as 0.045 for each element of the instrument poses. Our contributions provide the foundation to deliver personalized feedback to surgeons training in VR and performing live prostatectomy procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Previous studies have shown that surgeon performance directly affects patient clinical outcomes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. In one instance, manually rated suturing technical skill scores were the strongest predictors of patient continence recovery following a robot-assisted radical prostatectomy compared to other objective measures of surgeon performance <ref type="bibr" target="#b2">[3]</ref>. Ultimately, the value of skill assessment is not only in its ability to predict surgical outcomes, but also in its function as formative feedback for training surgeons. The need to automate skills assessment is readily apparent, especially since manual assessments by expert raters are subjective, time-consuming, and unscalable <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. View Fig. <ref type="figure" target="#fig_0">1</ref> for problem setup.</p><p>Preliminary work has shown favorable results for automated skill assessments on simulated VR environments, demonstrating the benefits of machine learning (ML) methods. ML approaches for automating suturing technical skills leveraged instrument kinematic (motion-tracking) data as the sole input to recurrent networks have been able to achieve effective area-under-ROC-curve (AUC), up to 0.77 for skill assessment in VR sponge suturing exercises <ref type="bibr" target="#b5">[6]</ref>. Multi-modality approaches that fused information from both kinematics and video modalities have demonstrated increased performance over uni-modal approaches in both VR sponge and tube suturing exercises, reaching up to 0.95 AUC <ref type="bibr" target="#b6">[7]</ref>.</p><p>Despite recent advances, automated skill assessment in live scenarios is still a difficult task due to two main challenges: 1) the lack of kinematic data from the da Vinci R system, and 2) the lack of training data due to the labor-intensive labeling task. Unlike simulated VR environments where kinematic data can be readily available, current live surgical systems do not output motion-tracking data, which is a key source of information for determining the movement and trajectory of robotic manipulators and suturing needles. Moreover, live surgical videos do not have a clear and painted target area for throwing stitches, unlike VR videos, which makes the task additionally difficult. On the other hand, due to the labor-intensive task of segmenting and scoring individual stitches from each surgical video, the quantity of available and labeled training data is quite low, rendering traditional supervised learning approaches ineffective. To address these challenges, we propose LiveMAE which learns sim-to-real generalizable representations without requiring any live kinematic annotations. Leveraging available video and sensor data from previous VR studies, LiveMAE can map from surgical images to instrument kinematics and derive surrogate "kinematic" automatically by learning to reconstruct images from both VR and live stitches while also predicting the corresponding VR kinematics. This creates a shared encoded representation space between the two visual domains while using available kinematic data from only one domain, the VR domain. Moreover, our pre-training strategy is not skill-specific which brings a bonus in improving data efficiency. LiveMAE enjoys up to six times more training data across the six suturing skills seen in Fig. <ref type="figure" target="#fig_0">1c</ref>, especially when we further break down video clips and kinematic sequences into multiple (image, kinematic) pairs. Overall, our main contributions include:</p><p>1. We propose LiveMAE which learns sim-to-real generalizable representations without requiring any live kinematic annotations. 2. We design a pre-training paradigm that increases the number of effective training samples significantly by combining data across suturing skills. 3. We conduct rigorous evaluations to verify the effectiveness of LiveMAE on surgical data collected and labeled across multiple institutions and surgeons. Finetuning on suturing skill assessment tasks yields better performance on 5/6 skills on live surgical videos compared to supervised learning baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Masked autoencoding is a method for self-supervised pre-training of Vision Transformers (ViTs <ref type="bibr" target="#b11">[12]</ref>) on images. It has demonstrated the capability to learn efficient and useful visual representations for downstream tasks such as image classification and segmentation. Our model builds on top of mask autoencoders (MAEs) and we provide a preliminary intro for MAE in Appendix 1.1. The input to our system contains both VR and live surgical data. VR data for a suturing skill s is defined as</p><formula xml:id="formula_0">D V R s = {(x i , k i , y i )} Ns i=0 consisting of segmented video clips x i ∈ R F ×H×W ×3 , aligned kinematic sequence k i ∈ R F ×70</formula><p>, and EASE technical skill score y i ∈ {0, 1} for non-ideal vs ideal performance. F denotes the number of frames in the video clip. Live data for s is similarly D L s = {(x i , y i )} Ms i=0 , except there are no aligned kinematics. Kinematic data has 70 features tracking 10 instruments of interest, each pose contains 3 elements for coordinates and 4 elements for quarternions. There are six technical skill labels, see Fig. <ref type="figure" target="#fig_0">1c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LiveMAE</head><p>Since D L s lacks kinematic information that is crucial for suturing skill assessment, we propose LiveMAE to automatically derive "kinematics" from live videos that can be helpful for downstream prediction. Specifically, we aim to learn a mapping φ : R H×W ×3 → R 70 from images to instrument kinematics using available video and sensor data from D V R s , and subsequently utilizing that mapping φ on live videos. Although the visual style between VR and live surgical videos can differ, this mapping is possible since we know that both simulated VR and live instruments share the exact same dimensions and centered coordinate frames. Our method builds on top of MAE and has three main components: a kinematic decoder, a shared encoder, and an expanded training set. Kinematic Decoder. For mapping from a surgical image to the instrument kinematics, we propose an additional kinematic output head along with a corresponding self-supervised task of reconstructing kinematics from masked input. See Fig. <ref type="figure" target="#fig_1">2a</ref>. The kinematic decoder is also a lightweight series of Transformer blocks that takes in a full sequence of both the (i) encoded visible patches, and (ii) learnable mask tokens. The last layer of the decoder is a linear projection whose number of output channels equals to 70, the dimension of the kinematic data. Similar to the image reconstruction task, which aims to learn visual concepts and semantics by encoding them into a compact representation for reconstruction, we additionally require these representations to contain information regarding possible poses of the surgical instruments. The kinematic decoder also has a reconstruction loss, which computes the mean squared error (MSE) between the reconstructed and original kinematic measurements. Shared Encoder. To learn sim-to-real generalizable representations that generalize across the different visual styles of VR and live videos, we augment live images with VR videos for pre-training. Since we do not have live kinematics, the reconstruction loss from the kinematic decoder will be set to zero for live samples within a training batch. This creates a shared encoded representation space between the two visual domains such that visual concepts and semantics about manipulators and suturing needles can be shared between them. Moreover, as we simultaneously train the kinematic reconstruction task, we are learning a mapping that can generalize to live videos, since two similar positioning in either VR or live should have similar corresponding kinematics.</p><p>Expanded Training Set. Since we have limited surgical data, and the mapping from image to instrument kinematics is not specific to any one suturing skill, we can combine visual and kinematic data across different skills during pre-training. Specifically, we pre-train the model on all data combined across 6 suturing skills to help learn the mapping. In addition, we can further break down video clips and kinematic sequences into F * (N s +M s ) (image, kinematics) pairs to increase the effective training set size without needing heavy data augmentations. These two key facts provide is a unique advantage over traditional supervised learning, since training each skill assessment task required the full video clip to learn temporal signature along with skill-specific scorings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finetuning of LiveMAE for Skill Assessment</head><p>After pre-training, we discard the image decoder and only use the pathway from the encoder to the kinematic decoder as our mapping φ. See Fig. <ref type="figure" target="#fig_1">2b</ref>. We applied φ to our live data D L s and extract a surrogate kinematic sequence for each video clip. The extracted kinematics are embedded by a linear projection with added positional embeddings and processed with a lightweight sequential DistilBERT model. We append a linear layer on top of the pooled output from DistilBERT for classification. We finetune the last layer of φ and the sequential model with the cross-entropy loss using a small learning rate, e.g. 1e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Datasets. We utilize a previously validated suturing assessment tool (EASE <ref type="bibr" target="#b7">[8]</ref>) to evaluate the robotic suturing skill in both VR and live surgery. We collected 156 VR videos and 54 live surgical videos from 43 residents, fellows, and attending urologic surgeons in this 5-center multi-institutional study. VR suturing exercises were completed on the Surgical Science TM Flex VR simulator and live surgical videos of surgeons performing the vesico-urethra anastomosis (VUA) step of a RARP were recorded. Each video was split into stitches, (n = 3448) total, and each stitch was segmented into sub-phrases with 6 binary assessment labels (low vs. high skill). See data breakdown and processing in Appendix 1.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics and Baselines.</head><p>Across the five institutions, we use 5-fold cross-valid ation to evaluate our model, training and validating on data from 4 institutions while testing on the 5th held-out institution. This allows us to test for generalization on unseen cases across both surgeons and medical centers. We measure and report the mean ± std. dev. for the two metrics: (1) Area-under-the-ROC curve (AUC) and ( <ref type="formula">2</ref>) Area-under-the-PR curve (AUPRC) for the 5 test folds.</p><p>To understand the benefits of each data modality, we compare LiveMAE against 3 setups: (1) train/test using only kinematics, (2) train/test using only videos, and (3) train using kinematic and video data while testing only on video (no live kinematics). For kinematics-only baselines, we use two sequential models (1) LSTM recurrent model <ref type="bibr" target="#b8">[9]</ref>, and (2) DistillBERT transformer-based model <ref type="bibr" target="#b9">[10]</ref>. For video-only baselines, we used two models based on pre-trained CNNs(3) ConvLSTM and (4) ConvTransformer. Both used pre-trained AlexNet to extract visual and flow features from the penultimate layer for each frame. The features are then flattened as used as input vectors to the sequential model ( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>). For a multi-modal baseline, we compare against recent work, AuxTransformer <ref type="bibr" target="#b6">[7]</ref>, which uses kinematics as privileged data in the form of an auxiliary loss during training. Unlike our method, they have additional kinematic supervision for the live video domain which we do not have.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Understanding the Benefits of Kinematics</head><p>Table <ref type="table" target="#tab_0">1</ref> presents automated suturing assessment results for each technical skill on VR data from unseen surgeons across the 5 held-out testing institutions. We make 3 key observations: (1) we successfully reproduced assessment performance seen in previous works and showed that sequential models trained on kinematiconly data often achieve the best results (outperforming video and multi-modal on 5/6 skills with high mean AUCs (0.652-0.878) and AUPRC (0.895-0.963). ( <ref type="formula">2</ref>) Vision model trained on video-only data can help with skill assessment, especially in certain skills such as needle hold angle where the angle between the needle tip and the target tissue (largely responsible for high/low score) is better represented visually, opposed kinematic poses. ( <ref type="formula">3</ref>) Lastly, we demonstrated the benefits of using kinematics data as supervisory signals during training, which yields improved performance on video-only baselines where kinematic data are not available during testing, seen with AuxTranformer's numbers. Overall, kinematics provide a wealth of clean motion signals that is essential for skill assessment, which helps to inspire LiveMAE for assessment in live videos. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation of LiveMAE on Live Videos</head><p>Quantitative Results. Table <ref type="table" target="#tab_1">2</ref> presents automated suturing assessment results for each technical skill on Live data across the 5 held-out institutions. We make 3 key observations: (1) Skill assessment on live stitch using LiveMAE or LiveMAEfinetuned often achieves the best results (outperforming supervised baselines and AuxTransformer with mean AUCs (0.550-0.837) and AUPRC (0.733-0.912) with particular improvements of 35.78% in AUC for wrist rotation skills and 8.7% for needle driving skill. (2) LiveMAE can learn generalizable representations from VR to Live using its shared encoder and kinematic mapping, achieving reasonable performance even without fine-tuning in the needle repositioning, hold angle and wrist rotation skills. (3) Clinically, we observe that VR data can directly help with live skill assessment, especially in certain skills such as wrist rotation and wrist rotation withdrawal (+35.78% increase in AUC), where medical students confirmed that the rotation motions (largely responsible for high/low score) are more pronounced in VR suturing videos and less so in Live videos due to how manipulators are visualized in the simulation. Hence training with VR data can help to teach LiveMAE of the desired assessment procedure that is not as clear in Live data and supervised training paradigm. Overall, LiveMAE contributes positively to the task of automated skill assessment, especially in live scenarios where it is not possible to obtain kinematics from the da Vinci R surgical system. Qualitative results. We visualized both reconstructed images and kinematics from masked inputs in Fig. <ref type="figure" target="#fig_2">3</ref>. Top row of Fig. <ref type="figure" target="#fig_2">3</ref> a shows the 75% masked image where only 1/4 of the visible patches are input into the model. The block patterns are input patches to LiveMAE that were not masked. The middle row shows the image's visual reconstruction vs. the original images (last row). We observe that LiveMAE can pick out and reconstruct the positioning of the manipulators quite well. It also does a good job at reconstructing the target tissue, especially in Tube1 and Sheet1. However, we also observe very small reconstruction artifacts in darker/black regions. This can be attributed to the training data, which sometimes contain all black borders that were not cropped out, yielding the confusion between black borders in live videos and black manipulators in the VR videos. In Fig. <ref type="figure" target="#fig_2">3b</ref>, we plot in the top row the original and predicted kinematics of the VR samples in blue and orange, respectively. The bottom row plots their absolute difference. LiveMAE does well in predicting kinematics from unseen samples, especially in Sheet1 where it gets both positioning and orientations correctly for all instruments of interest, off by at most 0.2. In Sponge1 and Tube1, we notice it does a poor job at estimating poses for some of the instruments, namely the orientation of the needle and target positions (index 4-7, 60-70) in Sponge1 and the needle orientation (index 4-7) in Tube1. This can happen in cases where it is hard to see and recognize the needle in the scene, making it difficult to estimate the exact needle orientation, which may explain LiveMAE's poorer performance for the skill Needle hold ratio. and presents a promising direction for future work in diving deeper into CV models to segment out instruments of interest since they can be easily ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Self-supervised learning methods, as utilized in our work, showed that videobased evaluation of suturing technical skills in live surgical videos is achievable with robust performance across multiple institutions. Although current work is limited to using VR data from one setup, namely Surgical Science TM Flex VR, our approach is independent from that system and can be applied on top of other surgical simulation systems with synchronized kinematics and video recordings. Future work will expand on the applications we demonstrated to determine whether it is possible to have a fully autonomous process, or semiautonomously with a "human-in-the-loop".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Suturing skill assessment. (a) The suturing step of RARP is composed of multiple stitches, each of which can also be broken down into three sub-phases (needle handling, driving, and withdrawal). (b) Input video and kinematics data for each VR suturing exercise. Live data do not have kinematics. Colors indicate different instruments such as left/right manipulators and needle/targets. (c) Each sub-phrase can be divided into specific EASE skills [8] and assessed for their quality (low vs high).</figDesc><graphic coords="3,56,79,54,41,310,84,195,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. LiveMAE Overview. (a) Pre-training with a shared encoder between Live and VR images and a kinematic decoder for predicting instrument kinematics. (b) Skillspecific finetuning for suturing skill assessment using pre-trained LiveMAE mapping.</figDesc><graphic coords="4,70,98,160,64,310,12,101,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualizations of reconstructed images and kinematics. Images for different exercises {sponge, tube, sheet} and live videos are presented. (a) Masked inputs and reconstructed images vs original images for held-out VR and live samples. (b) Predicted and original 70 kinematic features for the 4 VR samples. The bottom row plots the absolute difference. MSE for held-out VR kinematics are 0.045 ± 0.001.</figDesc><graphic coords="8,58,98,196,28,334,75,214,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Suturing skill assessments on VR data. Boldfaced denotes best and ± are standard deviations across 5 held-out institutions.</figDesc><table><row><cell></cell><cell></cell><cell>Repositions</cell><cell></cell><cell>HoldRatio</cell><cell></cell><cell>HoldAngle</cell><cell></cell></row><row><cell>Modality</cell><cell>Model</cell><cell>AUC</cell><cell>AUPRC</cell><cell>AUC</cell><cell>AUPRC</cell><cell>AUC</cell><cell>AUPRC</cell></row><row><cell>Kinematics</cell><cell>LSTM</cell><cell cols="6">0.808 ± 0.02 0.888 ± 0.03 0.567 ± 0.09 0.859 ± 0.06 0.469 ± 0.06 0.804 ± 0.07</cell></row><row><cell></cell><cell>Transformer</cell><cell cols="6">0.852 ± 0.03 0.916 ± 0.04 0.652 ±0.07 0.895 ±0.05 0.457 ±0.08 0.796 ± 0.06</cell></row><row><cell>Video</cell><cell>ConvLTSM</cell><cell cols="6">0.715 ± 0.06 0.840 ± 0.04 0.587 ± 0.07 0.883 ± 0.05 0.552 ± 0.02 0.831 ± 0.05</cell></row><row><cell></cell><cell cols="7">ConvTransformer 0.842 ± 0.02 0.912 ± 0.03 0.580 ± 0.03 0.880 ± 0.05 0.560 ±0.06 0.837 ±0.06</cell></row><row><cell cols="8">Video + Kin. AuxTransformer 0.851 ± 0.02 0.912 ± 0.04 0.597 ± 0.07 0.886 ± 0.05 0.557 ± 0.03 0.842 ± 0.06</cell></row><row><cell></cell><cell></cell><cell cols="2">DrivingSmoothness</cell><cell>WristRotation</cell><cell></cell><cell cols="2">WristRotationNW</cell></row><row><cell>Modality</cell><cell>Model</cell><cell>AUC</cell><cell>AUPRC</cell><cell>AUC</cell><cell>AUPRC</cell><cell>AUC</cell><cell>AUPRC</cell></row><row><cell>Kinematics</cell><cell>LSTM</cell><cell cols="6">0.851 ± 0.06 0.953 ± 0.03 0.615 ± 0.07 0.894 ± 0.03 0.724 ± 0.04 0.942 ± 0.03</cell></row><row><cell></cell><cell>Transformer</cell><cell cols="6">0.878 ±0.06 0.963 ±0.02 0.640 ±0.08 0.899 ±0.02 0.725 ±0.07 0.942 ±0.03</cell></row><row><cell>Video</cell><cell>ConvLTSM</cell><cell cols="6">0.851 ± 0.05 0.938 ± 0.03 0.636 ± 0.10 0.897 ± 0.03 0.661 ± 0.03 0.934 ± 0.02</cell></row><row><cell></cell><cell cols="7">ConvTransformer 0.858 ± 0.04 0.956 ± 0.02 0.634 ± 0.10 0.895 ± 0.04 0.700 ± 0.06 0.937 ± 0.02</cell></row><row><cell cols="8">Video + Kin AuxTransformer 0.868 ± 0.06 0.963 ± 0.02 0.649 ± 0.10 0.898 ± 0.04 0.675 ± 0.05 0.935 ± 0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Suturing skill assessment on Live data. Boldfaced denotes best and ± are standard deviations across 5 held-out institutions.</figDesc><table><row><cell></cell><cell></cell><cell>Repositions</cell><cell></cell><cell>HoldRatio</cell><cell></cell><cell>HoldAngle</cell><cell></cell></row><row><cell>Modality</cell><cell>Model</cell><cell>AUC</cell><cell>AUPRC</cell><cell>AUC</cell><cell>AUPRC</cell><cell>AUC</cell><cell>AUPRC</cell></row><row><cell>Video</cell><cell>ConvTransformer</cell><cell cols="6">0.822 ± 0.02 0.905 ± 0.02 0.564 ±0.10 0.697 ±0.15 0.489 ± 0.06 0.813 ± 0.03</cell></row><row><cell cols="2">Video + Kin. AuxTransformer</cell><cell cols="6">0.831 ± 0.02 0.900 ± 0.01 0.466 ± 0.06 0.631 ± 0.19 0.505 ± 0.02 0.805 ± 0.06</cell></row><row><cell></cell><cell cols="7">AuxTransformer-FT 0.828 ± 0.02 0.897 ± 0.01 0.472 ± 0.06 0.630 ± 0.19 0.499 ± 0.05 0.790 ± 0.07</cell></row><row><cell></cell><cell>(Ours) LiveMAE</cell><cell cols="6">0.832 ± 0.03 0.911 ± 0.03 0.430 ± 0.05 0.5930 ± 0.20 0.550 ±0.10 0.844 ±0.07</cell></row><row><cell></cell><cell cols="7">(Ours) LiveMAE-FT 0.837 ±0.01 0.912 ±0.02 0.474 ± 0.03 0.610 ± 0.20 0.489 ± 0.04 0.822 ± 0.04</cell></row><row><cell></cell><cell></cell><cell cols="2">DrivingSmoothness</cell><cell>WristRotation</cell><cell></cell><cell cols="2">WristRotationNW</cell></row><row><cell>Modality</cell><cell>Model</cell><cell>AUC</cell><cell>AUPRC</cell><cell>AUC</cell><cell>AUPRC</cell><cell>AUC</cell><cell>AUPRC</cell></row><row><cell>Video</cell><cell>ConvTransformer</cell><cell cols="6">0.667 ± 0.07 0.894 ± 0.06 0.435 ± 0.06 0.649 ± 0.05 0.445 ± 0.01 0.702 ± 0.05</cell></row><row><cell cols="2">Video + Kin. AuxTransformer</cell><cell cols="6">0.502 ± 0.03 0.830 ± 0.07 0.519 ± 0.04 0.708 ± 0.04 0.519 ± 0.04 0.708 ± 0.04</cell></row><row><cell></cell><cell cols="7">AuxTransformer-FT 0.483 ± 0.04 0.810 ± 0.10 0.517 ± 0.04 0.707 ± 0.04 0.520 ± 0.08 0.753 ± 0.06</cell></row><row><cell></cell><cell>(Ours) LiveMAE</cell><cell cols="6">0.683 ± 0.08 0.878 ± 0.08 0.543 ± 0.13 0.721 ± 0.10 0.486 ± 0.12 0.723 ± 0.12</cell></row><row><cell></cell><cell cols="2">(Ours) LiveMAE-FT 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>.725 ±0.12 0.903 ±0.06 0.562 ±0.08 0.733 ±0.08 0.634 ±0.06 0.826 ±0.01</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This study is supported in part by the <rs type="funder">National Cancer Institute</rs> under Award Number <rs type="grantNumber">1RO1CA251579-01A1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kRtbrkp">
					<idno type="grant-number">1RO1CA251579-01A1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Surgical skill and complication rates after bariatric surgery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Birkmeyer</surname></persName>
		</author>
		<idno type="DOI">10.1056/NEJMsa130062</idno>
		<ptr target="https://doi.org/10.1056/NEJMsa130062" />
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1434" to="1442" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A deep-learning model using automated performance metrics and clinical features to predict urinary continence recovery after robot-assisted radical prostatectomy</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hung</surname></persName>
		</author>
		<idno type="DOI">10.1111/bju.14735</idno>
		<ptr target="https://doi.org/10.1111/bju.14735" />
	</analytic>
	<monogr>
		<title level="j">BJU Int</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="487" to="495" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Survival analysis using surgeon skill metrics and patient factors to predict urinary continence recovery after robot-assisted radical prostatectomy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Trinh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.euf.2021.04.001</idno>
		<ptr target="https://doi.org/10.1016/j.euf.2021.04.001" />
	</analytic>
	<monogr>
		<title level="j">Eur. Urol. Focus. S</title>
		<imprint>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="107" to="00113" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Objective assessment of robotic surgical technical skill: a systematic review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.juro.2018.06.078</idno>
		<ptr target="https://doi.org/10.1016/j.juro.2018.06" />
	</analytic>
	<monogr>
		<title level="j">J. Urol</title>
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Crowdsourcing to assess surgical skill</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Lendvay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kowalewski</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamasurg.2015.2405</idno>
		<ptr target="https://doi.org/10.1001/jamasurg.2015.2405" />
	</analytic>
	<monogr>
		<title level="j">JAMA Surg</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1086" to="1087" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Road to automating robotic suturing skills assessment: battling mislabeling of the ground truth</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hung</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.surg.2021.08.014</idno>
		<idno>Surgery S0039-6060</idno>
		<ptr target="https://doi.org/10.1016/j.surg.2021.08.014" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="784" to="00794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Capturing fine-grained details for video-based automation of suturing skills assessment</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">O</forename><surname>Sunmola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-022-02778-x</idno>
		<idno type="PMID">36282465</idno>
		<idno type="PMCID">PMC9975072</idno>
		<ptr target="https://doi.org/10.1007/s11548-022-02778-x" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="545" to="552" />
			<date type="published" when="2022-10-25">2023. 2022 Oct 25</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Technical skill impacts the success of sequential robotic suturing substeps</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Sanford</surname></persName>
		</author>
		<idno type="DOI">10.1089/end.2021.0417</idno>
		<idno type="PMID">34779231</idno>
		<idno type="PMCID">PMC8861914</idno>
		<ptr target="https://doi.org/10.1089/end.2021.0417" />
	</analytic>
	<monogr>
		<title level="j">J. Endourol</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="278" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-24797-2_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-24797-24" />
	</analytic>
	<monogr>
		<title level="j">Studies in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<biblScope unit="page" from="37" to="45" />
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The association between video-based assessment of intraoperative technical performance and patient outcomes: a systematic review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Balvardi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00464-022-09296-6</idno>
		<idno type="PMID">35556166</idno>
		<ptr target="https://doi.org/10.1007/s00464-022-09296-6" />
	</analytic>
	<monogr>
		<title level="j">Surg. Endosc</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="7938" to="7948" />
			<date type="published" when="2022-05-12">2022. 2022 May 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The effect of technical performance on patient outcomes in surgery: a systematic review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Fecso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szasz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kerezov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Grantcharov</surname></persName>
		</author>
		<idno type="DOI">10.1097/SLA.0000000000001959</idno>
		<idno type="PMID">27537534</idno>
		<ptr target="https://doi.org/10.1097/SLA.0000000000001959" />
	</analytic>
	<monogr>
		<title level="j">Ann Surg</title>
		<imprint>
			<biblScope unit="volume">265</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="501" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
