<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation</title>
				<funder ref="#_XDw3RgC #_yBJBdS6">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_uvdEJpE">
					<orgName type="full">Natural Science Foundation of Fujian Province of China</orgName>
				</funder>
				<funder ref="#_WyS5D8M">
					<orgName type="full">Fujian Provincial Technology Innovation Joint Funds</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Wu</surname></persName>
							<email>wuming@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenkang</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Shi</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Fujian Cancer Hospital</orgName>
								<orgName type="institution" key="instit2">Fujian Medical University Cancer Hospital</orgName>
								<address>
									<settlement>Fuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianhua</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Fujian Cancer Hospital</orgName>
								<orgName type="institution" key="instit2">Fujian Medical University Cancer Hospital</orgName>
								<address>
									<settlement>Fuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sunkui</forename><surname>Ke</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Zhongshan Hospital</orgName>
								<orgName type="institution" key="instit2">Xiamen University</orgName>
								<address>
									<postCode>361004</postCode>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinran</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiongbiao</forename><surname>Luo</surname></persName>
							<email>xiongbiao.luo@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Institute for Data Science in Health and Medicine</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="718" to="727"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">3A73E6AC69EEF0568805E23D300AA4C6</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_69</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic segmentation of colonoscopic intestinal lesions is essential for early diagnosis and treatment of colorectal cancers. Current deep learning-driven methods still get trapped in inaccurate colonoscopic lesion segmentation due to diverse sizes and irregular shapes of different types of polyps and adenomas, noise and artifacts, and illumination variations in colonoscopic video images. This work proposes a new deep learning model called cascade transformer encoded boundary-aware multibranch fusion networks for white-light and narrow-band colorectal lesion segmentation. Specifically, this architecture employs cascade transformers as its encoder to retain both global and local feature representation. It further introduces a boundary-aware multibranch fusion mechanism as a decoder that can enhance blurred lesion edges and extract salient features, and simultaneously suppress image noise and artifacts and illumination changes. Such a newly designed encoder-decoder architecture can preserve lesion appearance feature details while aggregating the semantic global cues at several different feature levels. Additionally, a hybrid spatial-frequency loss function is explored to adaptively concentrate on the loss of important frequency components due to the inherent bias of neural networks. We evaluated our method not only on an in-house database with four types of colorectal lesions with different pathological features, but also on four public databases, with the experimental results showing that our method outperforms state-of-the-art network models. In particular, it can improve the average dice similarity coefficient and intersection over union from (84.3%, 78.4%) to (87.0%, 80.5%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Colorectal cancer (CRC) is the third most commonly diagnosed cancer but ranks second in terms of mortality worldwide <ref type="bibr" target="#b11">[11]</ref>. Intestinal lesions, particularly polyps and adenomas, are usually developed to CRC in many years. Therefore, diagnosis and treatment of colorectal polyps and adenomas at their early stages are essential to reduce morbidity and mortality of CRC. Interventional colonoscopy is routinely performed by surgeons to visually examine colorectal lesions. However these lesions in colonoscopic images are easily omitted and wrongly classified due to limited knowledge and experiences of surgeons. Automatic and accurate segmentation is a promising way to improve colorectal examination.</p><p>Many researchers employ U-shaped network <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b14">13,</ref><ref type="bibr" target="#b19">18]</ref> for colonoscopic polyp segmentation. ResUNet++ <ref type="bibr" target="#b7">[7]</ref> combines residual blocks and atrous spatial pyramid pooling and Zhao et al. <ref type="bibr" target="#b19">[18]</ref> designed a subtraction unit to generate the difference features at multiple levels and constructed a training-free network to supervise polyp-aware features. Unlike a family of U-Net driven segmentation methods, numerous papers have been worked on boundary constraints to segment colorectal polyps. Fan et al. <ref type="bibr" target="#b1">[2]</ref> introduced PraNet with reverse attention to establish the relationship between boundary cues from global feature maps generated by a parallel partial decoder. Both polyp boundary-aware segmentation methods work well but still introduce much false positive. Based on PraNet <ref type="bibr" target="#b1">[2]</ref> and HardNet <ref type="bibr" target="#b0">[1]</ref>, Huang et al. <ref type="bibr" target="#b6">[6]</ref> removed the attention mechanism and replaced Res2Net50 by HardNet to build HardNet-MSEG that can achieve faster segmentation. In addition, Kim et al. <ref type="bibr" target="#b9">[9]</ref> modified PraNet to construct UACANet with parallel axial attention and uncertainty augmented context attention to compute uncertain boundary regions. Although PraNet and UACANet aim to extract ambiguous boundary regions from both saliency and reverse saliency features, they simply set the saliency score to 0.5 that cannot sufficiently detect complete boundaries to separate foreground and background regions. More recently, Shen et al. <ref type="bibr" target="#b10">[10]</ref> introduced task-relevant feature replenishment networks for crosscenter polyp segmentation, while Tian et al. <ref type="bibr" target="#b12">[12]</ref> combined transformers and multiple instance learning to detect polyps in a weakly supervised way.</p><p>Unfortunately, limited field of view and illumination variations usually result in insufficient boundary contrast between intestinal lesions and their surrounding tissues. On the other hand, various polyps and adenomas with different pathological features have similar visual characteristics to intestinal folds. To address these issues mentioned above, we explore a new deep learning architecture called cascade transformer encoded boundary-aware multibranch fusion (CTBMF) networks with cascade transformers and multibranch fusion for polyp and adenoma segmentation in colonoscopic white-light and narrow-band video images. Several technical highlights of this work are summarized as follows. First, we construct cascade transformers that can extract global semantic and subtle boundary features at different resolutions and establish weighted links between global semantic cues and local spatial ones for intermediate reasoning, providing long-range dependencies and a global receptive field for pixel-level segmentation. Next, a hybrid spatial-frequency loss function is defined to compensate for loss features in the spatial domain but available in the frequency domain. Additionally, we built a new colonoscopic lesion image database and will make it publicly available, while this work also conducts a thorough evaluation and comparison on our new database and four publicly available ones (Fig. <ref type="figure" target="#fig_1">2</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approaches</head><p>This section details our CTBMF networks that can refine inaccurate lesion location, rough or blurred boundaries, and unclear textures. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the encoder-decoder architecture of CTMBF with three main modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformer Cascade Encoding</head><p>This work employs a pyramid transformer <ref type="bibr" target="#b16">[15]</ref> to build a transformer cascaded encoder. Let X 0 and X i be input patches and the feature map at stage i, respectively. Overlapping patch embedding (OPE) separates an image into fixed-size patches and linearly embeds them into tokenized images while making adjacent windows overlap by half of a patch. Either key K i or value V i is the input sequence of linear spatial reduction (LSR) that implements layer normalization (LN) and average pooling (AP) to reduce the input dimension:</p><formula xml:id="formula_0">LSR(K i ) = AP(Reshape(LN(K i ⊕ Ω(K i ), R i )W Ki )<label>(1)</label></formula><p>where Ω(•) denotes the output parameters of position embedding, ⊕ is the element-wise addition, W Ki indicates the parameters that reduces the dimension of K i or V i , and R i is the reduction ratio of the attention layers at stage i.</p><p>As the output of LSR is fed into multihead attention, we can obtain attention feature map A j i from head j (j = 1, 2, • • • , N, N is the head number of the attention layer) at stage i:</p><formula xml:id="formula_1">A j i = Attention(QW j Qi , LSR(K i )W j Ki , LSR(V i )W j Vi )<label>(2)</label></formula><p>where Attention(•) is calculated as the original transformer <ref type="bibr" target="#b15">[14]</ref>. Subsequently, the output LSRA</p><formula xml:id="formula_2">(Q i , K i , V i ) of LSRA is LSRA(Q i , K i , V i ) = (A 1 i • • • A j i • • • A N i )W Ai (3)</formula><p>where is the concatenation and W Ai is the linear projection parameters. Then,</p><formula xml:id="formula_3">LSRA(Q i , K i , V i ) is fed into convolutional feed-forward (CFF): CFF(Q i , K i , V i ) = FC(GELU(DC(FC(LN(Ψ )))))<label>(4)</label></formula><formula xml:id="formula_4">Ψ = ((Q i , K i , V i ) ⊕ Ω(Q i , K i , V i )) ⊕ LSRA(Q i , K i , V i ) (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where DC is a 3 × 3 depth-wise convolution <ref type="bibr" target="#b5">[5]</ref> with padding size of between the fully-connected (FC) layer and the Gaussian error linear unit (GELU) <ref type="bibr" target="#b4">[4]</ref> in the feed-forward networks. Eventually, the output feature map X i of the pyramid transformer at stage i can be represented by</p><formula xml:id="formula_6">X i = Reshape(Ψ ⊕ CFF(Q i , K i , V i ))<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Boundary-Aware Multibranch Fusion Decoding</head><p>Boundary-Aware Attention Module. Current methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">9]</ref> detect ambiguous boundaries from both saliency and reverse-saliency maps by predefining a saliency score of 0.5. Unfortunately, a predefined score cannot distinguish foreground and background of different colonoscopic lesions <ref type="bibr" target="#b3">[3]</ref>. Based on <ref type="bibr" target="#b18">[17]</ref>, this work explores an effective boundary-aware attention mechanism to adaptively extract boundary regions. Given the feature map X i with semantic cues and rough appearance details, we perform convolution (Conv) on it and obtain Xi = Conv(X i ), which is further augmented by channel and spatial attentions. The channel attention performs channel maxpooling (CMP), multilayer perceptron (MLP), and sigmoid (SIG) to obtain the intermediate feature map Y i :</p><formula xml:id="formula_7">Y i = Xi ⊗ SIG(MLP(CMP( Xi )))<label>(7)</label></formula><p>where ⊗ indicates the elementwise product. Subsequently, the detail enhanced feature map Z i of the channel-spatial attention is</p><formula xml:id="formula_8">Z i = Y i ⊗ SIG(Conv(SMP(Y i )))<label>(8)</label></formula><p>where SMP indicates spatial maxpooling. We subtract the feature map Xi from the enhanced map Z i to obtain the augmented boundary attention map B i , and also establish the correlation between the neighbor layers X i+1 and X i to generate multilevel boundary map G i :</p><formula xml:id="formula_9">B i = Z i Xi , i = 1, 2, 3, 4 G i = Xi US( Xi+1 ), i = 1, 2, 3<label>(9)</label></formula><p>where and US indicate subtraction and upsampling.</p><p>Residual Multibranch Fusion Module. To highlight salient regions and suppress task-independent feature responses (e.g., blurring), we linearly aggregate B i and G i to generate discriminative boundary attention map D i :</p><formula xml:id="formula_10">D i = SIG(Conv(RELU(Conv(B i ) ⊕ Conv(G i )))),<label>(10)</label></formula><p>where i = 1, 2, 3 and RELU is the rectified linear unit function.</p><p>We obtain the fused feature representation map M i (i = 1, 2, 3, 4) from the elementwise addition or summation of M i+1 , D i , and the residual feature Xi by</p><formula xml:id="formula_11">M i = Conv( Xi ⊕ D i ⊕ US(M i+1 )), i = 1, 2, 3 M 4 = Conv(B 4 )<label>(11)</label></formula><p>Eventually, the output M 1 of the boundary-aware multibranch fusion decoder is represented by the following equation:</p><formula xml:id="formula_12">M 1 = Conv( X1 ⊕ D 1 ⊕ US(M 2 ))<label>(12)</label></formula><p>which precisely combines global semantic features with boundary or appearance details of colorectal lesions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hybrid Spatial-Frequency Loss</head><p>This work proposes a hybrid spatial-frequency loss function H L to train our network architecture for colorectal polyp and adenoma segmentation:</p><formula xml:id="formula_13">H L = S L + F L (<label>13</label></formula><formula xml:id="formula_14">)</formula><p>where S L and F L are a spatial-domain loss and a frequency-domain loss to calculate the total difference between prediction P and ground truth G, respectively. The spatial-domain loss S L consists of a weighted intersection over union loss and a weighted binary cross entropy loss <ref type="bibr" target="#b17">[16]</ref>.</p><p>The frequency-domain loss F L can be computed by <ref type="bibr" target="#b8">[8]</ref> </p><formula xml:id="formula_15">F L = λ 1 W H W -1 u=0 H-1 v=0 γ(u, v)|G(u, v) -P(u, v)| 2<label>(14)</label></formula><p>where W × H is the image size, λ is the coefficient of F L , G(u, v) and P(u, v) are a frequency representation of ground truth G and prediction P using 2-D discrete Fourier transform. γ(u, v) is a spectrum weight matrix that is dynamically determined by a non-uniform distribution on the current loss of each frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Our clinical in-house colonoscopic videos were acquired from various colonoscopic procedures under a protocol approved by the research ethics committee of the university. These white-light and narrow-band colonoscopic images contain four types of colorectal lesions with different pathological features classified by surgeons: (1) 268 cases of hyperplastic polyp, (2) 815 cases of inflammatory polyp, (3) 1363 cases of tubular adenoma, and (4) 143 cases of tubulovillous adenoma. Additionally, four public datasets including Kvasir, ETIS-LaribPolypDB, CVC-ColonDB, and CVC-ClinicDB were also used to evaluate our network model. We implemented CTBMF on PyTorch and trained it with a single NVIDIA RTX3090 to accelerate the calculations for 100 epochs at mini-batch size 16. Factors λ (Eq. ( <ref type="formula" target="#formula_15">14</ref>)) were set to 0.1. We employ the stochastic gradient descent  algorithm to optimize the overall parameters with an original learning rate of 0.0001 for cascade transformer encoding and 0.05 for other parts and use warmup and linear decay strategies to adjust it. The momentum and weight decay were set as 0.9 and 0.0005. Further, we resized input images to 352 × 352 for training and testing and the training time was nearly 1.5 h to achieve the convergence. We employ three metrics to evaluate the segmentation: Dice similarity coefficient (DSC), intersection over union (IoU), and weighted F-measure (F β ).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Figure <ref type="figure" target="#fig_2">3</ref> visually compares the segmentation results of the four methods tested on our in-house and public databases. Our method can accurately segment polyps in white-light and narrow-band colonoscopic images under various scenarios, and CTBMF can successfully extract small, textureless and weak boundary and colorectal lesions. The segmented boundaries of our method are sharper and clear than others especially in textureless lesions that resemble intestinal lining.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the DSC-boxplots to evaluate the quality of segmented polyps and adenomas, which still demonstrate that our method works much better than the others. Figure <ref type="figure" target="#fig_4">5</ref> displays the enhanced feature maps using the boundary-aware attention module. Evidently, small and weak-boundary or textureless lesions can be enhanced with good boundary feature representation. Table <ref type="table" target="#tab_0">1</ref> summarizes the quantitative results in accordance with the three metrics and computational time of four methods. Evidently, CTBMF generally works better than the compared methods on the in-house database with four types of colorectal lesions. Furthermore, we also summarizes the average three metrics computed from all the five databases (the in-house dataset and four public datasets). Our method attains much higher average DSC and IoU of (0.870, 0.805) than the others on the five databases. We performed an ablation study to evaluate the effectiveness of each module used in CTBMF. The baseline is the standard version of cascade pyramid transformers. Modules D 1 , D 2 , D 3 , residual connections, and frequency loss F L are gradually added into the baseline, evaluating the effectiveness of each module and comparing the variants with each other. We tested these modules on the four public databases. Table <ref type="table" target="#tab_1">2</ref> shows all the ablation study results. Each module can improve the segmentation performance. Particularly, the boundary-aware attention module critically improves the average DSC, IoU, and F β .</p><p>Our method generally works better than the other three methods. Several reasons are behind this. First, the cascade-transformer encoder can extract local and global semantic features of colorectal lesions with different pathological characteristics due to its pyramid representation and linear spatial reduction attention. While the pyramid operation extracts multiscale local features, the attention mechanism builds global semantic cues. Both pyramid and attention strategies facilitate the representation of small and textureless intestinal lesions in encoding, enabling to characterize the difference between intestinal folds (linings) and subtle-texture polyps or small adenomas. Next, the boundary-aware attention mechanism drives the multibranch fusion, enhancing the representation of intestinal lesions in weak boundary and nonuniform lighting. Such a mechanism first extracts the channel-spatial attention feature map, from which subtracts the current pyramid transformer's feature map to enhance the boundary information. Also, the multibranch fusion generates multilevel boundary maps by subtracting the next pyramid transformer's upsampling output from the current pyramid transformer's output, further improving the boundary contrast. Additionally, the hybrid spatial-frequency loss was also contributed to the improvement of colorectal lesion segmentation. The frequency-domain information can compensate loss feature information in the spatial domain, leading to a better supervision in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work proposes a new deep learning model of cascade pyramid transformer encoded boundary-aware multibranch fusion networks to automatically segment different colorectal lesions of polyps and adenomas in colonoscopic imaging. While such an architecture employs simple and convolution-free cascade transformers as an encoder to effectively and accurately extract global semantic features, it introduces a boundary-aware attention multibranch fusion module as a decoder to preserve local and global features and enhance structural and boundary information of polyps and adenomas, as well as it uses a hybrid spatialfrequency loss function for training. The thorough experimental results show that our method outperforms the current segmentation models without any preprocessing. In particular, our method attains much higher accuracy on colonoscopic images with small, illumination changes, weak-boundary, textureless, and motion blurring lesions, improving the average dice similarity coefficient and intersection over union from (89.5%, 84.1%) to (90.3%, 84.4%) on our in-house database, from (78.9%, 72.6%) to (83.4%, 76.5%) on the four public databases, and from (84.3%, 78.4%) to (87.0%, 80.5%) on the five databases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. CTBMF consists of cascade transformers, boundary-aware multibranch fusion, and hybrid spatial-frequency loss.</figDesc><graphic coords="3,43,29,125,30,337,39,66,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The boundary-aware multibranch fusion decoder employs the boundary-aware attention module to compute Bi, Gi, and Di and introduces residual multibranch fusion to calculate Mi.</figDesc><graphic coords="3,43,29,257,54,337,33,94,18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual comparison of the segmentation results of using the four different methods tested on those in-house and public datasets.Green and blue show ground truth and prediction. (Color figure online)</figDesc><graphic coords="6,57,48,297,05,337,36,143,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. DSC boxplots of using the four methods evaluated on our in-house and publicly available databases</figDesc><graphic coords="7,44,79,161,09,334,60,152,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Effectiveness of the boundary-aware multibranch fusion decoder generated various boundary-aware feature maps</figDesc><graphic coords="8,57,48,220,94,337,36,144,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results and computational time of using five databases(our in-house and four public databases)</figDesc><table><row><cell>Average</cell><cell>DSC IoU</cell><cell>F β</cell><cell>In-house</cell><cell>Public</cell><cell>Average</cell></row><row><cell>PraNet [2]</cell><cell cols="5">0.813 0.751 0.795 28.7 FPS 30.5 FPS 29.6 FPS</cell></row><row><cell>HardNet [6]</cell><cell cols="5">0.830 0.764 0.812 39.2 FPS 39.9 FPS 39.5 FPS</cell></row><row><cell>UACANet [9]</cell><cell cols="5">0.843 0.784 0.824 16.5 FPS 16.7 FPS 16.6 FPS</cell></row></table><note><p>CTBMF (Ours) 0.870 0.805 0.846 33.1 FPS 33.6 FPS 33.4 FPS</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Public data segmented results of our ablation study</figDesc><table><row><cell cols="2">Modules DSC IoU F β</cell></row><row><cell>D1</cell><cell>0.681 0.593 0.634</cell></row><row><cell>D2</cell><cell>0.822 0.753 0.798</cell></row><row><cell>D3</cell><cell>0.820 0.748 0.793</cell></row><row><cell cols="2">Residual 0.828 0.757 0.802</cell></row><row><cell>FL</cell><cell>0.834 0.764 0.804</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported partly by the <rs type="funder">National Natural Science Foundation of China</rs> under Grants <rs type="grantNumber">61971367</rs> and <rs type="grantNumber">82272133</rs>, the <rs type="funder">Natural Science Foundation of Fujian Province of China</rs> under Grant <rs type="grantNumber">2020J01004</rs>, and the <rs type="funder">Fujian Provincial Technology Innovation Joint Funds</rs> under Grant <rs type="grantNumber">2019Y9091</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XDw3RgC">
					<idno type="grant-number">61971367</idno>
				</org>
				<org type="funding" xml:id="_yBJBdS6">
					<idno type="grant-number">82272133</idno>
				</org>
				<org type="funding" xml:id="_uvdEJpE">
					<idno type="grant-number">2020J01004</idno>
				</org>
				<org type="funding" xml:id="_WyS5D8M">
					<idno type="grant-number">2019Y9091</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hardnet: a low memory traffic network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3552" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PraNet: parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName><forename type="first">D.-F</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-226" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learn to threshold: thresholdnet with confidence-guided manifold mixup for polyp segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1134" to="1146" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Efficient convolutional neural networks for mobile vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">HarDNet-MSEG: a simple encoder-decoder polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07172</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ResUNet++: an advanced architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Focal frequency loss for image reconstruction and synthesis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13919" to="13929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">UACANet: uncertainty augmented context attention for polyp segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2167" to="2175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">UACANet: uncertainty augmented context attention for polyp segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="599" to="608" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global cancer statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="249" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contrastive transformer-based multiple instance learning for weakly supervised polyp frame detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="88" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-89" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DDANet: dual decoder attention network for automatic polyp segmentation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Tomar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-68793-9_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-68793-923" />
	</analytic>
	<monogr>
		<title level="m">ICPR 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12668</biblScope>
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PVT v2: improved baselines with pyramid vision transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Media</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="415" to="424" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">F 3 net: fusion, feedback and focus for salient object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12321" to="12328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CBAM: convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_1</idno>
		<idno>978- 3-030-01234-2 1</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic polyp segmentation via multi-scale subtraction network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-212" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
