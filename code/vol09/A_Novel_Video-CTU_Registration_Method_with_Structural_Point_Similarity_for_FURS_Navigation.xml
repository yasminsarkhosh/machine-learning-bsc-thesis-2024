<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation</title>
				<funder ref="#_Kzs3vCw #_ApZEhkk">
					<orgName type="full">Natural Science Foundation of Fujian Province of China</orgName>
				</funder>
				<funder ref="#_pzdgjFX #_9ppuMVz #_AyjT8nf">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_C5TvDE5">
					<orgName type="full">Fujian Provincial Technology Innovation Joint Funds</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingxian</forename><surname>Yang</surname></persName>
							<email>yangmingxian@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinran</forename><surname>Chen</surname></persName>
							<email>chen@xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Zheng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Fujian Medical University Union Hospital</orgName>
								<address>
									<settlement>Fuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianhui</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Fujian Medical University Union Hospital</orgName>
								<address>
									<settlement>Fuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiongbiao</forename><surname>Luo</surname></persName>
							<email>xiongbiao.luo@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Institute for Data Science in Health and Medicine</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="123" to="132"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F37F8858B995CC33F15DF343E9ABA0B5</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_12</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Surgical navigation</term>
					<term>Flexible Ureteroscopy</term>
					<term>Vision transformer</term>
					<term>Computed tomography urogram</term>
					<term>Ureteroscopic lithotripsy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Flexible ureteroscopy (FURS) navigation remains challenging since ureteroscopic images are poor quality with artifacts such as water and floating matters, leading to a difficulty in directly registering these images to preoperative images. This paper presents a novel 2D-3D registration method with structure point similarity for robust vision-based flexible ureteroscopic navigation without using any external positional sensors. Specifically, this new method first uses vision transformers to extract structural regions of the internal surface of the kidneys in real FURS video images and then generates virtual depth maps by the ray-casting algorithm from preoperative computed tomography urogram (CTU) images. After that, a novel similarity function without using pixel intensity is defined as an intersection of point sets from the extracted structural regions and virtual depth maps for the video-CTU registration optimization. We evaluate our video-CTU registration method on in-house ureteroscopic data acquired from the operating room, with the experimental results showing that our method attains higher accuracy than current methods. Particularly, it can reduce the position and orientation errors from (11.28 mm, 10.8 • ) to (5.39 mm, 8.13 • ).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Flexible ureteroscopy (FURS) is a routinely performed surgical procedure for renal lithotripsy. This procedure inserts a flexible ureteroscope through the blad-der and ureters to get inside the kidneys for diagnosis and treatment of stones and tumors. Unfortunately, such an examination and treatment depends on skills and experiences of surgeons. On the other hand, surgeons may miss stones and tumors and unsuccessfully orientate the ureteroscope inside the kidneys due to limited field of views, just 2D images without depth information, and the complex anatomical structure of the kidneys. To this end, ureteroscope tracking and navigation is increasingly developed as a promising tool to solve these issues.</p><p>Many researchers have developed various methods to boost endoscopic navigation. These methods generally consist of vision-and sensor-based tracking. Han et al. <ref type="bibr" target="#b2">[3]</ref> utilized the porous structures in renal video images to develop a vision-based navigation method for ureteroscopic holmium laser lithotripsy. Zhao et al. <ref type="bibr" target="#b14">[15]</ref> designed a master-slave robotic system to navigate the flexible ureteroscope. Luo et al. <ref type="bibr" target="#b6">[7]</ref> reported a discriminative structural similarity measure driven 2D-3D registration for vision-based bronchoscope tracking. More recently, Huang et al. <ref type="bibr" target="#b3">[4]</ref> developed an image-matching navigation system using shape context for robotic ureteroscopy. Additionally, sensor-based methods are widely sued in surgical navigation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. Zhang et al. <ref type="bibr" target="#b13">[14]</ref> employed electromagnetic sensors to estimate the ureteroscope shape for navigation.</p><p>Although these methods mentioned above work well, ureteroscopic navigation is still a challenging problem. Compared to other endoscopes such as colonoscope and bronchoscope, the diameter of the ureteroscope is smaller, resulting in more limited lighting source and field of view. Particularly, ureteroscopy involves much solids (impurities) and fluids (liquids), making ureteroscopic video images low-quality, as well as these solids and fluids inside the kidneys cannot be regularly observed in computed tomography (CT) images. On the other hand, the complex internal structures such as calyx, papilla, and pyramids of the kidneys are difficult to be observed in CT images. These issues introduce a difficulty in directly aligning ureteroscopic video sequences to CT images, leading to a challenge of image-based continuous ureteroscopic navigation.</p><p>This work aims to explore an accurate and robust vision-based navigation method for FURS procedures without using any external positional sensors. Based on ureteroscopic video images and preoperative computed tomography urogram (CTU) images, we propose a novel video-CTU registration method to precisely locate the flexible ureteroscope in the CTU space. Several highlights of this work are clarified as follows. To the best of our knowledge, this work shows the first study to continuously track the flexible ureteroscope in preoperative data using a vision-based method. Technically, we propose a novel 2D-3D (video-CTU) registration method that introduces a structural point similarity measure without using image pixel intensity information to characterize the difference between the structural regions in real video images and CTU-driven virtual image depth maps. Additionally, our proposed method can successfully deal with solid and fluid ureteroscopic video images and attains higher navigation accuracy than intensity-based 2D-3D registration methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Video-CTU Registration</head><p>Our proposed video-CTU registration method consists of several steps: (1) ureteroscopic structure extraction, (2) virtual depth map generation, and (3) structural point similarity and optimization. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the flowchart of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Ureteroscopic Image Characteristics</head><p>The internal kidneys consist of complex anatomical structures such as pelvis and calyx and also contain solid particles (e.g., stones and impurities) floating in fluids (e.g., water, urine and small blood), resulting in poor image quality during ureteroscopy. Therefore, it is a challenging task to extract meaningful features from these low-quality images for achieving accurate 2D-3D registration.</p><p>Our idea is to introduce specific structures inside the kidneys to boost the video-CTU registration since these structural regions are meaningful features that can facilitate the similarity computation. During ureteroscopy, various anatomical structures observed in ureteroscopic video images indicate different poses of the ureteroscope inside the kidneys. While some structural features such as capillary texture and striations at the tip of the renal pyramids are observed ureteroscopic images, they are not discernible in CT or other preoperative data. Typical structural or texture regions (Columns 1∼3 in Fig. <ref type="figure">2 (b</ref>)) observed both in ureteroscopic video and CTU images are the renal papilla when the ureteroscope gets into the kidneys through the ureter and renal pelvis to reach the major and minor calyxes. Additionally, we also find that the renal pelvis (dark or ultra-low light) regions (Columns 4∼6 in Fig. <ref type="figure">2 (b</ref>)) are also useful to enhance the registration. Hence, this work employs these interior renal structural characteristics to calculate the similarity between ureteroscopic images and CTU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ureteroscopic Structure Extraction</head><p>Deep learning is widely used for medical image segmentation. Lazo et al. <ref type="bibr" target="#b4">[5]</ref> used spatial-temporal ensembles to segment lumen structures in ureteroscopic images. More recently, vision transformers show the potential to precisely segment various medical images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>. This work employs the dense prediction transformer (DPT) <ref type="bibr" target="#b10">[11]</ref> to extract these structural regions from ureteroscopic images. DPT is a general deep learning framework for dense prediction tasks such as semantic segmentation and has three versions of DPT-Base, DPT-Large, and DPT-Hybrid. This work use DPT-Base since it only requires a small number of parameters but provides a high inference speed. DPT-Base consists of a transformer encoder and a convolutional decoder. Its backbone is vision transformers <ref type="bibr" target="#b1">[2]</ref>, where input images are transformed into tokens by non-overlapping patches extraction, followed by a linear projection of their flattened representation. The conventional decoder employs a reassemble operation <ref type="bibr" target="#b10">[11]</ref> to assemble a set of tokens into image-like feature representations at various resolutions:</p><formula xml:id="formula_0">Reassemble D s (t) = (Resample s • Concatenate • Read) (t) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where s is the output size ratio of the feature representation and D is the output feature dimension. Tokens from layers l = {6, 12, 18, 24} are reassembled in DPT-Base. These feature representations are subsequently fused into the final dense prediction. In the structure extraction, we define three classes: Non-structural regions (background), structural regions, and stones. We manually select and annotate ureteroscopic video images for training and testing. Vision transformers require large datasets for training, so we initialize the encoder with weights pretrained on ImageNet and further train it on our in-house database. Figure <ref type="figure">3</ref> displays some segmentation results of ureteroscopic video images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Virtual CTU Depth Map Generation and Thresholding</head><p>This step is to compute depth maps of 2D virtual images generated from CTU images by volume rendering <ref type="bibr" target="#b12">[13]</ref>. Depth maps can represent structural information of virtual images. Note that this work uses CTU to create virtual images since non-contrast CT images cannot capture certain internal structures of the kidneys, although these structures can be observed in ureteroscopic video images. We introduce a ray casting algorithm in volume rendering to generate depth maps of virtual rending images <ref type="bibr" target="#b12">[13]</ref>. The ray casting algorithm is to trace a ray that starts from the viewpoint and passes through a pixel on the screen. When the tracing ray intersects with a voxel, the properties of that voxel will affect the value of corresponding pixel in the final image. For a 3D point (x 0 , y 0 , z 0 ) and a normalized direction vector (r x , r y , r z ) of its casting ray, a corresponding point (x, y, z) at any distance d on the tracing ray is: (x, y, z) = (x 0 + dr x , y 0 + dr y , z 0 + dr z ).</p><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>The tracing ray R(x, y, z, r x , r y , r z ) will stop when it encounters opaque voxels. For 3D point (x,y,z), its depth value V can be calculated by projecting the ray R(x, y, z, r x , r y , r z ) onto the normal vector N(x, y, z) of the image plane:</p><formula xml:id="formula_3">V (x, y, z) = R(x, y, z, r x , r y , r z ) • N(x, y, z),<label>(3)</label></formula><p>where symbol • denotes the dot product.</p><p>To obtain the depth map with structural regions, we define two thresholds t u and t v . Only CT intensity values within [t u , t v ] are opaque voxels that the casting rays cannot pass through in the ray-casting algorithm. According to CTU characteristics <ref type="bibr" target="#b9">[10]</ref>, this work uses our excretory-phase data to generate virtual images and set [t u , t v ] to [-1000, 120], where -1000 represents air and 120 was determined by the physician's experience and characteristics of contrast agents.</p><p>Unfortunately, the accuracy of thresholded structural regions suffers from inaccurate depth maps caused by renal stones and contrast agents. Stones and agents are usually high intensity in CTU images, which result in incorrect depth information of structural regions (e.g., renal papilla). To deal with these issues, we use the segmented stones as a mask to remove these regions with wrong depth. On the other hand, the structural regions usually have larger depth values than the agent-contrasted regions. Therefore, we sort the depth values outside the mask and only use the thresholded structural regions with the largest depth values for the structural point similarity computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Structural Point Similarity and Optimization</head><p>We define a point similarity measure between DPT-base segmented structural regions in ureteroscopic images and thresholded structural regions in virtual depth maps generated by the volume rendering ray casting algorithm.</p><p>The structural point similarity function (cost function) is defined as an intersection of point sets from the extracted real and virtual structural regions: where I i is the ureteroscopic video image at frame i, point sets P i and P v are from the ureteroscopic image extracted structural region E i (a, b) and the thresholded structural region E v (a, b) ((a, b) denotes a point)from the depth map D v of the 2D virtual rendering image I v (p i , q i ), respectively:</p><formula xml:id="formula_4">F(I i , D v ) = 2 |P i P v | |P i | + |P v | , P i ∈ E i (x, y), P v ∈ E v (x, y),<label>(4)</label></formula><formula xml:id="formula_5">D v ∝ I v (p i , q i ),<label>(5)</label></formula><p>where (p i , q i ) is the endoscope position and orientation in the CTU space. Eventually, the optimal pose (p i , qi ) of the ureteroscope in the CTU space can be estimated by maximizing the structural point similarity:</p><formula xml:id="formula_6">(p i , qi ) = arg max pi,qi F(I i , D v ), D v ∈ I v (p i , q i ),<label>(6)</label></formula><p>where Powell method <ref type="bibr" target="#b8">[9]</ref> is used as an optimizer to run this procedure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head><p>We validate our method on clinical ureteroscopic lithotripsy data with video sequences and CTU volumes. Ureteroscopic video images were a size of 400 × 400 pixels, while the space parameters of CTU volumes were 512 × 512 pixels, 361∼665 slices, 0.625∼1.25 mm slice thickness. Three ureteroscopic videos more than 30000 frames were acquired from three ureteroscopic procedures for experiments. While we manually annotated ureteroscopic video images for DPT-base segmentation, three experts also manually generated ureteroscope pose groundtruth data by our developed software, which can manually adjust position and direction parameters of the virtual camera to visually align endoscopic real images to virtual images, evaluating the navigation accuracy of the different methods.  Figure <ref type="figure" target="#fig_2">4</ref> illustrates the navigation results of segmentation, depth maps, extracted structural regions for similarity calculation, and generated 2D virtual images corresponding to estimated ureteroscope poses. Structural regions can be extracted from ureteroscopic images and virtual depth maps. Particularly, we can see that our method generated virtual images (Row 7 in Fig. <ref type="figure" target="#fig_2">4</ref>) resemble real video images (Row 1 in Fig. <ref type="figure" target="#fig_2">4</ref>) much better than Luo et al. <ref type="bibr" target="#b6">[7]</ref> generated ones (Row 6 in Fig. <ref type="figure" target="#fig_2">4</ref>). This implies that our method can estimate the ureteroscope pose much more accurate than Luo et al. <ref type="bibr" target="#b6">[7]</ref>. Table <ref type="table" target="#tab_0">1</ref> summarizes quantitative segmentation results and position and orientation errors. DPT-Base can achieve average segmentation IoU 88.34%, accuracy 92.26%, and DSC 93.71%. The average position and orientation errors of our method were 5.39 mm and 8.14 • , which much outperform the compared method. Table <ref type="table" target="#tab_1">2</ref> shows the results of sensitivity analysis for the threshold values. It can be seen that inappropriate threshold selection can lead to an increase in errors. Figure <ref type="figure" target="#fig_3">5</ref> boxplots estimated position and orientation errors for a statistical analysis of our navigation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Luo et al. Ours</head><p>The effectiveness of our proposed method lies in several aspects. First, renal interior structures are insensitive to solids and fluids inside the kidneys and can precisely characterize ureteroscopic images. Next, we define a structural point similarity measure as intersection of point sets between real and virtual structural regions. Such a measure does not use any point intensity information for the similarity calculation, leading to an accurate and robust similarity characterization under renal floating solids and fluids. Additionally, CTU images can capture more renal anatomical structures inside the kidneys compared to CT slices, still facilitating an accurate similarity computation.</p><p>Our method still suffers from certain limitations. Figure <ref type="figure" target="#fig_4">6</ref> displays some ureteroscopic video images our method fails to track. This is because that the segmentation method cannot successfully extract structural regions, while the ray casting algorithm cannot correctly generate virtual depth maps with structural regions. Both unsuccessfully extracted real and virtual structural regions collapse the similarity characterization. We will improve the segmentation of ureteroscopic video images, while generating more ground-truth data for training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposes a new 2D-3D registration approach for vision-based FURS navigation. Specifically, such an approach can align 2D ureteroscopic video sequences to 3D CTU volumes and successfully locate an ureteroscope into CTU space. Different from intensity-based cost function, a novel structural point similarity measure is proposed to effectively and robustly characterize ureteroscopic video images. The experimental results demonstrate that our proposed method can reduce the navigation errors from (11.28 mm, 10.8 • ) to (5.39 mm, 8.13 • ).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Processing flowchart of our ureteroscopic navigation method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Ureteroscopic video images with various specific structure information</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visual navigation results: Rows 1∼5 show the real video images, segmented structural regions, depth maps, thresholded structural regions, and overlapped regions for similarity computation, while Rows 6∼7 illustrate the tracking results (2D virtual rendering images) of using Luo et al. [7] and our method.</figDesc><graphic coords="6,42,30,54,26,339,40,272,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Boxplotted position and orientation errors of using the two methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Our method fails to track the ureteroscope in CTU: Columns 1∼6 correspond to input images, segmented structural regions, depth maps, thresholded structural regions, overlapped regions, and generated virtual images.</figDesc><graphic coords="8,42,30,54,32,339,43,169,39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>DPT-base segmented results of intersection over union (IoU), accuracy (Acc), and dice similarity coefficient (DSC), and estimated ureteroscope (position, orientation) errors of using the two vision-based navigation methods</figDesc><table><row><cell>Classes</cell><cell>IoU% Acc% DSC% Methods Luo et al. [7]</cell><cell>Our method</cell></row><row><cell cols="3">Background 92.14 98.20 95.91 Case A (18.13 mm, 11.78 • ) (5.66 mm, 7.05 • )</cell></row><row><cell>Structures</cell><cell cols="2">80.09 83.17 88.95 Case B (11.04 mm, 4.87 • ) (2.59 mm, 3.66 • )</cell></row><row><cell cols="3">Renal stones 92.80 95.40 96.27 Case C (8.60 mm, 12.37 • ) (6.20 mm, 10.04 • )</cell></row><row><cell>Average</cell><cell cols="2">88.34 92.26 93.71 Average (11.28 mm, 10.82 • ) (5.39 mm, 8.13 • )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Sensitivity analysis results of threshold in virtual CTU depth map generation. In this work, the threshold range was set to [-1000, 120].</figDesc><table><row><cell>Threshold</cell><cell cols="5">[-1000, 70] [-1000, 95] [-1000, 120] [-1000, 145] [-1000, 170]</cell></row><row><cell>Position Errors</cell><cell cols="2">11.08 mm 8.48 mm</cell><cell>5.39 mm</cell><cell>6.70 mm</cell><cell>5.59 mm</cell></row><row><cell cols="2">Orientation Errors 15.55 •</cell><cell>10.52 •</cell><cell>8.13 •</cell><cell>8.92 •</cell><cell>11.10 •</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grants <rs type="grantNumber">61971367</rs>, <rs type="grantNumber">82272133</rs>, and <rs type="grantNumber">62001403</rs>, in part by the <rs type="funder">Natural Science Foundation of Fujian Province of China</rs> under Grants <rs type="grantNumber">2020J01004</rs> and <rs type="grantNumber">2020J05003</rs>, and in part by the <rs type="funder">Fujian Provincial Technology Innovation Joint Funds</rs> under Grant <rs type="grantNumber">2019Y9091</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pzdgjFX">
					<idno type="grant-number">61971367</idno>
				</org>
				<org type="funding" xml:id="_9ppuMVz">
					<idno type="grant-number">82272133</idno>
				</org>
				<org type="funding" xml:id="_AyjT8nf">
					<idno type="grant-number">62001403</idno>
				</org>
				<org type="funding" xml:id="_Kzs3vCw">
					<idno type="grant-number">2020J01004</idno>
				</org>
				<org type="funding" xml:id="_ApZEhkk">
					<idno type="grant-number">2020J05003</idno>
				</org>
				<org type="funding" xml:id="_C5TvDE5">
					<idno type="grant-number">2019Y9091</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A novel electromagnetic tracking system for surgery navigation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Attivissimo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M L</forename><surname>Lanzolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larizza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brunetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Assist. Surg</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="52" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An image is worth 16 × 16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Endoscopic navigation based on three-dimensional structure registration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2900" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Image-matching based navigation system for robotic ureteroscopy in kidney exploration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<pubPlace>Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Delft University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using spatial-temporal ensembles of convolutional neural networks for lumen segmentation in ureteroscopy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Lazo</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-021-02376-3</idno>
		<ptr target="https://doi.org/10.1007/s11548-021-02376-3" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="922" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A new electromagnetic-video endoscope tracking method via anatomical constraints and historically observed differential evolution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-010" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate multiscale selective fusion of CT and video images for realtime endoscopic camera 3D tracking in robotic surgery</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1386" to="1390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Haslum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Söderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.09038</idno>
		<title level="m">Is it time to replace CNNs with transformers for medical images? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-40065-5</idno>
		<ptr target="https://doi.org/10.1007/978-0-387-40065-5" />
		<title level="m">Numerical Optimization</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What a difference a delay makes! CT urogram: a pictorial essay</title>
		<author>
			<persName><forename type="first">A</forename><surname>Noorbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aganovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fazeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cassidy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Abdom. Radiol</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3919" to="3934" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12179" to="12188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Shamshad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09873</idno>
		<title level="m">Transformers in medical imaging: a survey</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Production Volume Rendering: Design and Implementation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wrenninge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5031</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shape estimation of the anterior part of a flexible ureteroscope for intraoperative navigation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1787" to="1799" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Design and performance investigation of a robot-assisted flexible ureteroscopy system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Bionics Biomech</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
