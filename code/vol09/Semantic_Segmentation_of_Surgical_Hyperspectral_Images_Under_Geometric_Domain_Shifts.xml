<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts</title>
				<funder>
					<orgName type="full">National Center for Tumor Diseases</orgName>
					<orgName type="abbreviated">NCT</orgName>
				</funder>
				<funder>
					<orgName type="full">German Cancer Research Center (DKFZ)</orgName>
				</funder>
				<funder ref="#_ZFuxV8z">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder ref="#_rVU8cfp #_Xm6kdrr">
					<orgName type="full">Committee on Animal Experimentation</orgName>
				</funder>
				<funder>
					<orgName type="full">Helmholtz Association</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jan</forename><surname>Sellner</surname></persName>
							<email>j.sellner@dkfz-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Division of Intelligent Medical Systems (IMSY)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Helmholtz Information and Data Science School for Health</orgName>
								<address>
									<settlement>Karlsruhe, Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">National Center for Tumor Diseases (NCT)</orgName>
								<orgName type="institution">NCT Heidelberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Silvia</forename><surname>Seidlitz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Intelligent Medical Systems (IMSY)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Helmholtz Information and Data Science School for Health</orgName>
								<address>
									<settlement>Karlsruhe, Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">National Center for Tumor Diseases (NCT)</orgName>
								<orgName type="institution">NCT Heidelberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Studier-Fischer</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">National Center for Tumor Diseases (NCT)</orgName>
								<orgName type="institution">NCT Heidelberg</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Department of General, Visceral, and Transplantation Surgery</orgName>
								<orgName type="institution">Heidelberg University Hospital</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Medical Faculty</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alessandro</forename><surname>Motta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Intelligent Medical Systems (IMSY)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Berkin</forename><surname>Özdemir</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of General, Visceral, and Transplantation Surgery</orgName>
								<orgName type="institution">Heidelberg University Hospital</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Medical Faculty</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Beat</forename><forename type="middle">Peter</forename><surname>Müller-Stich</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Felix</forename><surname>Nickel</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Helmholtz Information and Data Science School for Health</orgName>
								<address>
									<settlement>Karlsruhe, Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Department of General, Visceral, and Transplantation Surgery</orgName>
								<orgName type="institution">Heidelberg University Hospital</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Medical Faculty</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lena</forename><surname>Maier-Hein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Intelligent Medical Systems (IMSY)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Helmholtz Information and Data Science School for Health</orgName>
								<address>
									<settlement>Karlsruhe, Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">National Center for Tumor Diseases (NCT)</orgName>
								<orgName type="institution">NCT Heidelberg</orgName>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Medical Faculty</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Partnership Between DKFZ</orgName>
								<orgName type="institution">University Medical Center Heidelberg</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="618" to="627"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">017A178B5EC238D6A356876A4EC2A351</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_59</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>domain generalization</term>
					<term>geometrical domain shifts</term>
					<term>semantic organ segmentation</term>
					<term>hyperspectral imaging</term>
					<term>surgical data science</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robust semantic segmentation of intraoperative image data could pave the way for automatic surgical scene understanding and autonomous robotic surgery. Geometric domain shifts, howeveralthough common in real-world open surgeries due to variations in surgical procedures or situs occlusions -remain a topic largely unaddressed in the field. To address this gap in the literature, we (1) present the first analysis of state-of-the-art (SOA) semantic segmentation networks in the presence of geometric out-of-distribution (OOD) data, and (2) address generalizability with a dedicated augmentation technique termed 'Organ Transplantation' that we adapted from the general computer vision community. According to a comprehensive validation on six different OOD data sets comprising 600 RGB and yperspectral imaging (HSI) cubes from 33 pigs semantically annotated with 19 classes, we demonstrate a large performance drop of SOA organ segmentation networks applied to geometric OOD data. Surprisingly, this holds true not only for conventional RGB data (drop of Dice similarity coefficient (DSC) by 46 %) but also for HSI data (drop by 45 %), despite the latter's rich information content per pixel. Using our augmentation scheme improves on the SOA DSC by up to 67% (RGB) and 90% (HSI)) and renders performance</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated surgical scene segmentation is an important prerequisite for contextaware assistance and autonomous robotic surgery. Recent work showed that deep learning-based surgical scene segmentation can be achieved with high accuracy <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> and even reach human performance levels if using hyperspectral imaging (HSI) instead of RGB data, with the additional benefit of providing functional tissue information <ref type="bibr" target="#b14">[15]</ref>. However, to our knowledge, the important topic of geometric domain shifts commonly present in real-world surgical scenes (e.g., situs occlusions, cf. Fig. <ref type="figure" target="#fig_0">1</ref>) so far remains unaddressed in literature. It is questionable whether the state-of-the-art (SOA) image-based segmentation networks in <ref type="bibr" target="#b14">[15]</ref> are able to generalize towards an out-of-distribution (OOD) context. The only related work by Kitaguchi et al. <ref type="bibr" target="#b9">[10]</ref> showed that surgical instrument segmentation algorithms fail to generalize towards unseen surgery types that involve known instruments in an unknown context. We are not aware of any investigation or methodological contribution on geometric domain shifts in the context of surgical scene segmentation. Generalizability in the presence of domain shifts is being intensively studied by the general machine learning community. Here, data augmentation evolved as a simple, yet powerful technique <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>. In deep learning-based semantic image segmentation, geometric transformations are most common <ref type="bibr" target="#b7">[8]</ref>. This holds particularly true for surgical applications. Our analysis of the SOA (35 publications on tissue or instrument segmentation) exclusively found geometric (e.g., rotating), photometric (e.g., color jittering) and kernel (e.g., Gaussian blur) transformations and only in a single case elastic transformations and Random Erasing (within an image, a rectangular area is blacked out) <ref type="bibr" target="#b21">[22]</ref> being applied. Similarly, augmentations in HSI-based tissue classification are so far limited to geometric transformations. To our knowledge, the potential benefit of complementary transformations proposed for image classification and object detection, such as Hide-and-Seek (an image is divided into a grid of patches that are randomly blacked out) <ref type="bibr" target="#b16">[17]</ref>, Jigsaw (images are divided into a grid of patches and patches are randomly exchanged between images) <ref type="bibr" target="#b1">[2]</ref>, CutMix (a rectangular area is copied from one image onto another image) <ref type="bibr" target="#b20">[21]</ref> and CutPas (an object is placed onto a random background scene) <ref type="bibr" target="#b3">[4]</ref> (cf. Fig. <ref type="figure" target="#fig_1">2</ref>), remains unexplored.</p><p>Given these gaps in the literature, the contribution of this paper is twofold:</p><p>1. We show that geometric domain shifts have disastrous effects on SOA surgical scene segmentation networks for both conventional RGB and HSI data. 2. We demonstrate that topology-altering augmentation techniques adapted from the general computer vision community are capable of addressing these domain shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and Methods</head><p>The following sections describe the network architecture, training setup and augmentation methods (Sect. 2.1), and our experimental design, including an overview of our acquired datasets and validation pipeline (Sect. 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Learning-Based Surgical Scene Segmentation</head><p>Our contribution is based on the assumption that application-specific data augmentation can potentially address geometric domain shifts. Rather than changing the network architecture of previously successful segmentation methods, we adapt the data augmentation.</p><p>Surgery-Inspired Augmentation: Our Organ Transplantation augmentation illustrated in Fig. <ref type="figure" target="#fig_1">2</ref> has been inspired by the image-mixing augmentation CutPas that was originally proposed for object detection <ref type="bibr" target="#b3">[4]</ref> and recently adapted for instance segmentation <ref type="bibr" target="#b4">[5]</ref> and low-cost dataset generation via image synthesis from few real-world images in surgical instrument segmentation <ref type="bibr" target="#b18">[19]</ref>. It is based on placing an organ into an unusual context while keeping shape and texture consistent. This is achieved by transplanting all pixels belonging to one object class (e.g., an organ class or background) into a different surgical scene. Our selection of further computer vision augmentation methods that could potentially improve geometric OOD performance (cf. Fig. <ref type="figure" target="#fig_1">2</ref>) was motivated by the specific conditions encountered in surgical procedures (cf. Sect. 2.2 for an overview). The noise augmentations Hide-and-Seek and Random Erasing black out all pixels inside rectangular regions within an image, thereby generating artificial situs occlusions. Instead of blacking out, the image-mixing techniques Jigsaw and CutMix copy all pixels inside rectangular regions within an image into a different surgical scene. We adapted the image-mixing augmentations to our segmentation task by also copying and pasting the corresponding segmentations. Hence, apart from occluding the underlying situs, image parts/organs occur in an unusual neighborhood.</p><p>Network Architecture and Training: We used a U-Net architecture <ref type="bibr" target="#b12">[13]</ref> with an efficientnet-b5 encoder <ref type="bibr" target="#b17">[18]</ref> pre-trained on ImageNet data and using stochastic weight averaging <ref type="bibr" target="#b5">[6]</ref> for both RGB and HSI data as it achieved human performance level in recent work <ref type="bibr" target="#b14">[15]</ref>. As a pre-processing step, the HSI data was calibrated with white and dark reference images and 1 -normalized to remove the influence of multiplicative illumination changes. Dice and cross-entropy loss were equally weighted to compute the loss function. The Adam optimization algorithm <ref type="bibr" target="#b8">[9]</ref> was used with an exponential learning rate scheduler. Training was performed for 100 epochs with a batch size of five images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experiments</head><p>To study the performance of SOA surgical scene segmentation networks under geometric domain shifts and investigate the generalizability improvements offered by augmentation techniques, we covered the following OOD scenarios:</p><p>(I) Organs in isolation: Abdominal linens are commonly used to protect soft tissue and organs, counteract excessive bleeding, and absorb blood and secretion. Some surgeries (e.g., enteroenterostomy), even require covering all but a single organ. In such cases, an organ needs to be robustly identified without any information on neighboring organs. (II) Organ resections: In resection procedures, parts or even the entirety of an organ are removed and surrounding organs thus need to be identified despite the absence of a common neighbor. (III) Occlusions: Large parts of the situs can be occluded by the surgical procedure itself, introducing OOD neighbors (e.g., gloved hands). The nonoccluded parts of the situs need to be correctly identified.</p><p>Real-World Datasets: In total, we acquired 600 intraoperative HSI cubes from 33 pigs using the HSI system Tivita R Tissue (Diaspective Vision GmbH, Am Salzhaff, Germany). These were semantically annotated with background and 18 tissue classes, namely heart, lung, stomach, small intestine, colon, liver, gallbladder, pancreas, kidney with and without Gerota's fascia, spleen, bladder, subcutaneous fat, skin, muscle, omentum, peritoneum, and major veins. Each HSI cube captures 100 spectral channels in the range between 500nm and 1000nm at an image resolution of 640 × 480 pixels. RGB images were reconstructed by aggregating spectral channels in the blue, green, and red ranges. To study organs in isolation, we acquired 94 images from 25 pigs in which all but a specific organ were covered by abdominal linen for all 18 different organ classes (dataset isolation real ). To study the effect of occlusions, we acquired 142 images of 20 pigs with real-world situs occlusions (dataset occlusion), and 364 occlusion-free images (dataset no-occlusion). Example images are shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Manipulated Data: We complemented our real-world datasets with four manipulated datasets. To simulate organs in isolation, we replaced every pixel in an image I that does not belong to the target label l either with zeros or spectra copied from a background image. We applied this transformation to all images in the dataset original and all target labels l, yielding the datasets isolation zero and isolation bgr. Similarly, we simulated organ resections by replacing all pixels belonging to the target label l either with zeros or background spectra, yielding the datasets removal zero and removal bgr. Example images are shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train-Test Split and Hyperparameter Tuning:</head><p>The SOA surgical scene segmentation algorithms are based on a union of the datasets occlusion and no-occlusion, termed dataset original, which was split into a hold-out test set (166 images from 5 pigs) and a training set (340 images from 15 pigs). To enable a fair comparison, the same train-test split on pig level was used across all networks and scenarios. This also holds for the occlusion scenario, in which the dataset no-occlusion was used instead of original for training. All networks used the geometric transformations shift, scale, rotate, and flip from the SOA prior to applying the augmentation under examination. All hyperparameters were set according to the SOA. Only hyperparameters related to the augmentation under examination, namely the probability p of applying the augmentation, were optimized through a grid search with p ∈ {0.2, 0.4, 0.6, 0.8, 1}. We used five-fold-cross-validation on the datasets original, isolation zero, and isolation bgr to tune p such that good segmentation performance was achieved on both in-distribution and OOD data.</p><p>Validation Strategy: Following the recommendations of the Metrics Reloaded framework <ref type="bibr" target="#b10">[11]</ref>, we combined the Dice similarity coefficient (DSC) <ref type="bibr" target="#b2">[3]</ref> as an overlap-based metric with the boundary-based metric ormalized surface distance (NSD) <ref type="bibr" target="#b11">[12]</ref> for validation for each class l. To respect the hierarchical test set structure, metric aggregation was performed by first macro-averaging the classlevel metric value M l (M ∈ {DSC, NSD}) across all images of one pig and subsequently across pigs. The organ removal experiment required special attention in this context, as multiple M l values per image could be generated corresponding to all the possible neighbour organs that could be removed. In this case, we selected for each l the minimum of all M l values, which corresponds to the segmentation performance obtained after removing the most important neighbour of l. The same class-specific NSD thresholds as in the SOA were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Effects of Geometric Domain Shifts: When applying a SOA segmentation network to geometric OOD data, the performance drops radically (cf. Fig. <ref type="figure" target="#fig_2">3</ref>). Starting from a high DSC for in-distribution data (RBG: 0.83 (standard deviation (SD) 0.10); HSI: 0.86 (SD 0.10)), the performance drops by 10%-46% for RGB and by 5 %-45% for HSI, depending on the experiment. In the organ resection scenario, the largest drop in performance of 63% occurs for the gallbladder upon liver removal (cf. Suppl. Fig. <ref type="figure" target="#fig_0">1</ref>). Similar trends can be observed for the boundary-based metric NSD, as shown in Suppl. Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Performance of Our Method: Figure <ref type="figure" target="#fig_2">3</ref> and Suppl. Fig. <ref type="figure" target="#fig_1">2</ref> show that the Organ Transplantation augmentation (gold) can address geometric domain shifts for both the RGB and HSI modality. The latter yields consistently better results, indicating that the spectral information is crucial in situations with limited context. The performance improvement compared to the baseline ranges from 9 %-67% (DSC) and 15 %-79% (NSD) for RGB, and from 9%-90% (DSC) and 16 %-96% (NSD) for HSI, with the benefit on OOD data being largest for organs in isolation and smallest for situs occlusions. The Organ Transplantation augmentation even slightly improves performance on in-distribution data (original and no-occlusion). Upon encountering situs occlusions, the largest DSC improvement is obtained for the organ classes pancreas (283 %) and stomach (69 %). For organs in isolation, the performance improvement on manipulated data (DSC increased by 57% (HSI) and 61% (RGB) on average) is comparable to that on real data (DSC increased by 50% (HSI) and 46% (RGB)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to SOA Augmentations:</head><p>There is no consistent ranking across all six OOD datasets except for Organ Transplantation always ranking first and baseline usually ranking last (cf. Fig. <ref type="figure" target="#fig_3">4</ref> for DSC-and Suppl. Fig. <ref type="figure" target="#fig_2">3</ref> for NSD-based ranking). Overall, image-mixing augmentations outperform noise augmentations. Augmentations that randomly sample rectangles usually rank better than comparable augmentations using a grid structure (e.g., CutMix vs. Jigsaw). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>To our knowledge, we are the first to show that SOA surgical scene segmentation networks fail under geometric domain shifts. We were particularly surprised by the large performance drop for HSI data, rich in spectral information. Our results clearly indicate that SOA segmentation models rely on context information. Aiming to address the lack of robustness to geometric variations, we adapted so far unexplored topology-altering data augmentation schemes to our target application and analyzed their generalizability on a range of six geometric OOD datasets specifically designed for this study. The Organ Transplantation augmentation outperformed all other augmentations and resulted in similar performance to in-distribution performance on real OOD data. Besides its effectiveness and computational efficiency, we see a key advantage in its potential to reduce the amount of real OOD data required in network training. Our augmentation networks were optimized on simulated OOD data, indicating that image manipulations are a powerful tool for judging geometric OOD performance if real data is unavailable, such as in our resection scenario, which would have required an unfeasible number of animals. With laparoscopic HSI systems only recently becoming available, the investigation and compensation of geometric domain shifts in minimally-invasive surgery could become a key direction for future research. Our proposed augmentation is model-independent, computationally efficient and effective, and thus a valuable tool for addressing geometric domain shifts in semantic scene segmentation of intraoperative HSI and RGB data. Our implementation and models will be made publicly available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. State-of-the-art (SOA) surgical scene segmentation networks show promising results on idealistic datasets. However, in real-world surgeries, geometric domain shifts such as occlusions of the situs by operating staff are common. The generalizability of SOA algorithms towards geometric out-of-distribution (OOD) has not yet been addressed.</figDesc><graphic coords="2,56,46,404,24,339,43,88,51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Organ Transplantation augmentation concept inspired from [4]. Image features and corresponding segmentations of randomly selected organs are transferred between images in one batch (in the example, the stomach is transferred from the left to the right and the spleen from the right to the left image). (b) Illustration of our validation experiments. We assess the generalizability under geometric domain shifts of seven different data augmentation techniques in deep learning-based organ segmentation. We validate the model performance on a range of out-of-distribution (OOD) scenarios, namely (1) organs in isolation (isolation zero, isolation bgr and isolation real ), (2) organ resections (removal zero and removal bgr ), and (3) situs occlusions (occlusion), in addition to in-distribution data (original and no-occlusion (subset of original without occlusions)).</figDesc><graphic coords="4,56,46,54,26,339,43,348,43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Segmentation performance of the hyperspectral imaging (HSI) and RGB modality for all eight test datasets (six out-of-distribution (OOD) and two in-distribution datasets (bold)) comparing the baseline network with the Organ Transplantation network. Each point denotes one out of 19 class-level Dice similarity coefficient (DSC) values after hierarchical aggregation across images and subjects. The boxplots show the quartiles of the class-level DSC. The whiskers extend up to 1.5 times the interquartile range and the median and mean are represented as a solid and dashed line, respectively.</figDesc><graphic coords="7,42,30,54,50,339,55,198,22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Uncertainty-aware ranking of the seven augmentation methods for all six geometric out-of-distribution (OOD) test datasets. Organ Transplantation consistently ranks first and baseline last. The area of each blob for one rank and algorithm is proportional to the relative frequency of that algorithm achieving the respective rank across 1000 bootstrap samples consisting of 19 hierarchically aggregated class-level Dice similarity coefficient (DSC) values each (concept from<ref type="bibr" target="#b19">[20]</ref>). The numbers above the example images denote the overall ranking across datasets (mean of all mean ranks).</figDesc><graphic coords="8,56,46,146,21,339,43,252,52" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements and <rs type="person">Data Usage</rs>. This project was supported by the <rs type="funder">European Research Council (ERC)</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation programme (NEURAL SPICING</rs>, <rs type="grantNumber">101002198</rs>), the <rs type="funder">National Center for Tumor Diseases (NCT) Heidelberg's Surgical Oncology Program</rs>, the <rs type="funder">German Cancer Research Center (DKFZ)</rs>, and the <rs type="funder">Helmholtz Association</rs> under the joint research school HIDSS4Health (Helmholtz Information and Data Science School for Health). The private HSI data was acquired at <rs type="institution">Heidelberg University Hospital</rs> after approval by the <rs type="funder">Committee on Animal Experimentation</rs> (<rs type="grantNumber">G-161/18</rs> and <rs type="grantNumber">G-262/19</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZFuxV8z">
					<idno type="grant-number">101002198</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme (NEURAL SPICING</orgName>
				</org>
				<org type="funding" xml:id="_rVU8cfp">
					<idno type="grant-number">G-161/18</idno>
				</org>
				<org type="funding" xml:id="_Xm6kdrr">
					<idno type="grant-number">G-262/19</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 59.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data augmentation in classification and segmentation: a survey and new strategies</title>
		<author>
			<persName><forename type="first">K</forename><surname>Alomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Aysel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Imaging</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">46</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image block augmentation for one-shot learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3379" to="3386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<title level="m">Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Nashville, TN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2917" to="2927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the International Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature aggregation decoder for segmenting laparoscopic scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barbarisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Taleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Flouty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32695-1_1</idno>
		<idno>OR 2.0/MLCN -2019</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32695-11" />
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11796</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A review on progress in semantic image segmentation and its application to medical images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Neog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SN Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">397</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A Method for Stochastic Optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Limited generalizability of single deep neural network for surgical instrument segmentation in different surgical environments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kitaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Takeshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12575</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<title level="m">Metrics reloaded: pitfalls and recommendations for image analysis validation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clinically applicable segmentation of head and neck anatomy for radiotherapy: deep learning algorithm development and validation study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Internet Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning for semantic segmentation of organs and tissues in laparoscopic surgery</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scheikl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Dir. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">20200016</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust deep learning-based semantic organ segmentation in hyperspectral images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seidlitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102488</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hide-and-seek: forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3544" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">EfficientNet: rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rethinking surgical instrument segmentation: a background image can be all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-134" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Methods and open-source toolkit for analyzing and visualizing challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesenfarth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2369</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CutMix: regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="6022" to="6031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
