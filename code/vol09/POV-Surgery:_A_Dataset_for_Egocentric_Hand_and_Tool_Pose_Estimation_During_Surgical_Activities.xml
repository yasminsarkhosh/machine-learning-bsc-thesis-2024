<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities</title>
				<funder>
					<orgName type="full">Accenture LLP</orgName>
				</funder>
				<funder>
					<orgName type="full">Microsoft Mixed Reality &amp; AI Zurich Lab PhD scholarship</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<email>ruiwang46@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sophokles</forename><surname>Ktistakis</surname></persName>
							<email>ktistaks@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siwei</forename><surname>Zhang</surname></persName>
							<email>siwei.zhang@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mirko</forename><surname>Meboldt</surname></persName>
							<email>meboldtm@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quentin</forename><surname>Lohmeyer</surname></persName>
							<email>qlohmeyer@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="440" to="450"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">25E04CF84076B3A8A47ADC22B0DA8AA7</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_42</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hand Object Pose Estimation</term>
					<term>Deep Learning</term>
					<term>Dataset</term>
					<term>Mixed Reality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The surgical usage of Mixed Reality (MR) has received growing attention in areas such as surgical navigation systems, skill assessment, and robot-assisted surgeries. For such applications, pose estimation for hand and surgical instruments from an egocentric perspective is a fundamental task and has been studied extensively in the computer vision field in recent years. However, the development of this field has been impeded by a lack of datasets, especially in the surgical field, where bloody gloves and reflective metallic tools make it hard to obtain 3D pose annotations for hands and objects using conventional methods. To address this issue, we propose POV-Surgery, a large-scale, synthetic, egocentric dataset focusing on pose estimation for hands with different surgical gloves and three orthopedic surgical instruments, namely scalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329 frames, featuring high-resolution RGB-D video streams with activity annotations, accurate 3D and 2D annotations for hand-object pose, and 2D hand-object segmentation masks. We fine-tune the current SOTA methods on POV-Surgery and further show the generalizability when applying to real-life cases with surgical gloves and tools by extensive evaluations. The code and the dataset are publicly available at http:// batfacewayne.github.io/POV_Surgery_io/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding the movement of surgical instruments and the surgeon's hands is essential in computer-assisted interventions and has various applications, including surgical navigation systems <ref type="bibr" target="#b27">[27]</ref>, surgical skill assessment <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b23">23]</ref> and robot-assisted surgeries <ref type="bibr" target="#b8">[8]</ref>. With the rising interest in using head-mounted Mixed Reality (MR) devices for such applications <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b28">28]</ref>, estimating the 3D pose of hands and objects from the egocentric perspective becomes more important. However, this is more challenging compared to the third-person viewpoint because of the constant self-occlusion of hands and mutual occlusions between hands and objects. While the use of deep neural networks and attention modules has partly addressed this challenge <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22]</ref>, the lack of egocentric datasets to train such models has hindered progress in this field. Most existing datasets that provide 3D hand or hand-object pose annotations focus on the third-person perspective <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b29">29]</ref>. FPHA <ref type="bibr" target="#b9">[9]</ref> proposed the first egocentric hand-object video dataset by attaching magnetic sensors to hands and objects. However, the attached sensors pollute the RGB frames. More recently, H2O <ref type="bibr" target="#b17">[17]</ref> proposed an egocentric video dataset with hand and object pose annotated with a semi-automatic pipeline, based on 2D hand joint detection and object point cloud refinement. However, this pipeline is not applicable to the surgical domain because of the large domain gap between the everyday scenarios in <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b17">17]</ref> and surgical scenarios. For instance, surgeons wear surgical gloves that are often covered with blood during the surgery process, which presents great challenges for vision-based hand keypoint detection methods. Moreover, these datasets focus on large, everyday objects with distinct textures, whereas surgical instruments are often smaller and have featureless, highly reflective metallic surfaces. This results in noisy and incomplete object point clouds when captured with RGB-D cameras. Therefore, in a surgical setting, the annotation approaches proposed in <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b17">17]</ref> are less stable and reliable. Pioneer work in <ref type="bibr" target="#b14">[14]</ref> introduces a small synthetic dataset with blue surgical gloves and a surgical drill, following the synthetic data generation approach in <ref type="bibr" target="#b13">[13]</ref>. However, being a single-image dataset, it ignores the strong temporal context in surgical tasks, which is crucial for accurate and reliable 3D pose estimation <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b24">24]</ref>. Surgical cases have inherent task-specific information and temporal correlations during surgical instrument usage, such as cutting firmly and steadily with a scalpel. Moreover, it lacks diversity, focusing only on one unbloodied blue surgical glove and one instrument, and only provides low-resolution image patches.</p><p>To fill this gap, we propose a novel synthetic data generation pipeline that goes beyond single image cases to synthesize realistic temporal sequences of surgical tool manipulation from an egocentric perspective. It features a body motion capture module to model realistic body movement sequences during artificial surgeries and a hand-object manipulation generation module to model the grasp evolution sequences. With the proposed pipeline, we generate POV-Surgery, a large synthetic egocentric video dataset of surgical activities that features surgical gloves in diverse textures (green, white, and blue) with various bloodstain patterns and three metallic tools that are commonly used in orthopedic surgeries.</p><p>In summary, our contributions are:</p><p>-A novel, easy-to-use, and generalizable synthetic data generation pipeline to generate temporally realistic hand-object manipulations during surgical activities. -POV-Surgery: the first large-scale dataset with egocentric sequences for hand and surgical instrument pose estimation, with diverse, realistic surgical glove textures, and different metallic tools, annotated with accurate 3D/2D handobject poses and 2D hand-object segmentation masks. -Extensive evaluations of existing state-of-the-art (SOTA) hand pose estimation methods on POV-Surgery, revealing their shortcomings when dealing with the unique challenges in surgical cases from egocentric view. -Significantly improved performance for SOTA methods after fine-tuning them on the POV-Surgery training set, on both our synthetic test set and a real-life test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We focus on three tools commonly employed in orthopedic procedures -the scalpel, friem, and diskplacer -each of which requires a unique hand motion. The scalpel requires a side-to-side cutting motion, while the friem uses a quick downward punching motion, similar to using an awl. Finally, the diskplacer requires a screwing motion with the hand. Our pipeline to capture these activities and generate egocentric hand-object manipulation sequences is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-view Body Motion Capture</head><p>To capture body movements during surgery, we used four temporally synchronized ZED stereo cameras on participants during simulated surgeries. The intrinsic camera parameters were provided by ZED SDK and the extrinsic parameters between the four different cameras were calibrated with a chessboard. We adopt the popular <ref type="bibr" target="#b5">[5]</ref> [6] module for SMPLX body reconstruction. OpenPose <ref type="bibr" target="#b2">[2]</ref> with hand -and face-detection modules is first used to detect 2D human skeletons with a confidence threshold of 0.3. The 3D keypoints are obtained via triangulation with camera pose, regularized with bone length. The SMPLX body meshed is optimized by minimizing the 2D re-projection and triangulated 3D skeleton errors. Moreover, we enforce a large smoothness constraint, which regularizes the body and hand pose by constraining the between-frame velocities. It vastly reduces the number of unrealistic body poses. There are two critical differences between the surgical tool and everyday object grasping: surgical tools require to be held in specific poses. Moreover, a surgeon would hold firmly and steadily with a particular pose for some time span during surgeries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hand-Object Manipulation Sequence Generation</head><p>To address this issue, we generate each instrument manipulation sequence by firstly modeling the key poses that are surgically plausible, and then interpolating in between to model pose evolution. The key pose generation pipeline is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The part highlighted in blue is the pose generation component based on GrabNet <ref type="bibr" target="#b25">[25]</ref>. We provide the 3D instrument models to GrabNet with arbitrary initial rotation and sample from a Gaussian distribution in latent space to obtain diverse poses. 500 samples are generated for scalpel, diskplacer, and friem, respectively, followed by manual selection to get the best grasping poses as templates. With a pose template as prior, we perform the re-sampling near it to obtain diverse and similar hand-grasping poses as key poses for each sequence. To improve the plausibility of the grasping pose and hand-object interactions, inspired by <ref type="bibr" target="#b16">[16]</ref>, an optimization module is adopted for post-processing, with the overall loss function defined as:</p><formula xml:id="formula_0">L = α • L penetr + β • L contact + γ • L keypoint ,<label>(1)</label></formula><p>L penetr , L contact , and L keypoint denote the penetration loss, contact loss, and keypoint loss, respectively. And α, β, γ are object-specific scaling factors to balance the loss components. For example, the weight for penetration is smaller for the scalpel than the friem and diskplacer to account for the smaller object size. The penetration loss is defined as the overall penetration distance of the object into the hand mesh:</p><formula xml:id="formula_1">L penetr = 1 |P o in | p∈P o in min i p -V i 2 2 , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where P in denotes the vertices from the object mesh which are inside the hand mesh, and V i denotes the hand vertex. The P o in is defined as the dot product of the vector from the hand mesh vertices to their nearest neighbors on the object mesh. To encourage hand-object contact, a contact loss is defined to minimize the distance from the hand mesh to the object mesh.</p><formula xml:id="formula_3">L contact = j min i V i -P j 2 2 , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where V and P denote vertices from the hand and object mesh, respectively. In addition, we regularize the optimized hand pose by the keypoint displacement, which penalizes hand keypoints that are far away from the initial hand keypoints:</p><formula xml:id="formula_5">L keypoint = i K i -k i 2 , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where K is the refined hand keypoint position and k is the source keypoint position. After the grasping pose refinement, a small portion of the generated hand poses are still unrealistic due to the poor initialization. To this end, a post-selection technique similar to <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b26">26]</ref> is further applied to discard the unrealistic samples with hand-centric interpenetration volume, contact region, and displacement simulation.</p><p>For each hand-object manipulation sequence, we select 30 key grasping poses, hold on, and interpolate in between to model pose evolution within the sequence. The number of frames for the transition phase between every two key poses is randomly sampled from 5 to 30. The interpolated hand poses are also optimized via the pose refinement module with the source keypoint in L keypoint defined as the interpolated keypoints between two key poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Body and Hand Pose Fusion and Camera Pose Calculation</head><p>In previous sections, we individually obtained the body motion and hand-object manipulation sequences. To merge the hand pose into the body pose to create a whole-body grasping sequence, we established an optimization-based approach based on the SMPLX model. The vertices to vertices loss is defined as:</p><formula xml:id="formula_7">L V 2V = vi∈Phand v M (i) -(Rv i + T ) 2 2 , (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where v is the vertices in the target grasping hand, v is the vertices in the SMPLX body model, with M being the vertices map from MANO's right hand to SMPLX body. R and T are the rotation matrix and translation vector applied to the right hand. The right-hand pose of SMPLX, R, and T are optimized with the Trust Region Newton Conjugate Gradient method (Trust-NCG) for 300 iterations to obtain an accurate and stable whole-body grasping pose. R and T are then applied to the grasped object. The egocentric camera pose for each frame is calculated with head vertices position and head orientation. Afterwards, outlier removal and moving average filter are applied to the camera pose sequence to remove temporal jitterings between frames. We use blender <ref type="bibr" target="#b3">[3]</ref> and bpycv packages to render the RGB-D sequences and instance segmentation masks. Diverse textures and scenes of high quality are provided in the dataset: it includes 24 SMPLX textures featuring blue, green, and white surgical gloves textures with various blood patterns and a synthetic surgical room scene created by artists. The Cycle rendering engine and de-noising post-processing are adopted to produce high-quality frames. POV-Surgery provides clean depth maps for depth-based methods or point-cloud-based methods, as the artifact of real depth cameras can be efficiently simulated via previous works as <ref type="bibr" target="#b12">[12]</ref>. A point cloud example generated from an RGB-D frame with added simulated Kinect depth camera noise is provided in Fig. <ref type="figure" target="#fig_2">3</ref>. The POV-Surgery dataset consists of 36 sequences with 55,078 frames in the training set and 17 sequences with 33,161 frames in the testing set, respectively. Three bloodied glove textures and one scene created from a room scanning of a surgery room are only used in the testing set to measure generalizability. Fig. <ref type="figure" target="#fig_2">3</ref> shows the ground truth data samples and the dataset statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Rendering and POV-Surgery Dataset Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>Fig. <ref type="figure">4</ref>. Qualitative results of METRO <ref type="bibr" target="#b18">[18]</ref>, SEMI <ref type="bibr" target="#b19">[19]</ref>, and HANDOCCNET <ref type="bibr" target="#b22">[22]</ref> on the test of of POV-Surgery. The FT denotes fine-tuning. We show the 2D re-projection of the predicted 3D hand joints and object control bounding box overlayed on the input image.</p><p>We evaluate and fine-tune two state-of-the-art hand pose estimation methods: <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b22">22]</ref>, and one hand-object pose estimation <ref type="bibr" target="#b19">[19]</ref> method on our dataset with provided checkpoints in their official repositories. 6 out of 36 sequences from the training set are selected as the validation set for model selection. We continue to train their checkpoints on our synthetic training set, with a reduced learning rate (10 -5 ) and various data augmentation methods such as color jittering, scale and center jittering, hue-saturation-contrast value jittering, and motion blur for better generalizability. Afterwards, we evaluate the performance of those methods on our testing set. We set a baseline for object control point error in pixels: 41.56 from fine-tuning <ref type="bibr" target="#b19">[19]</ref>. The hand quantitative metrics are shown in Table <ref type="table" target="#tab_0">1</ref> and qualitative visualizations are shown in Fig. <ref type="figure">4</ref>, where we highlight the significant performance improvement for existing methods after fine-tuning them on the POV-Surgery dataset.</p><p>To further evaluate the generalizability of the methods fine-tuned on our dataset, we collect 6,557 real-life images with multiple surgical gloves, tools, and backgrounds as the real-life test set. The data capture setup with four stereo cameras is shown in Fig. <ref type="figure" target="#fig_3">5</ref>. We adopt a top-down-based method from <ref type="bibr" target="#b4">[4]</ref> with manually selected hand bounding boxes for 2D hand joint detection. <ref type="bibr" target="#b5">[5]</ref> is used to reconstruct 3D hand poses from different camera observations. We project the hand pose to the egocentric camera view and manually select the frames with accurate hand predictions to obtain reliable 2D hand pose ground truth. We show quantitative examples of the indicated methods and the PCP curve in Fig. <ref type="figure" target="#fig_3">5</ref>.</p><p>After fine-tuning on our synthetic dataset significant performance improvements are achieved for SOTA methods on the real-life test set. Particularly, we observe a similar performance improvement for unseen purple-texture gloves, showing the effectiveness of our POV-Surgery dataset towards the challenging egocentric surgical hand-object interaction scenarios in general. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposes a novel synthetic data generation pipeline that generates hand-tool manipulation temporal sequences. Using the data generation pipeline and focusing on three tools used in orthopedic surgeries: scalpel, diskplacer, and friem, we propose a large, synthetic, and temporal dataset on egocentric surgical hand-object pose estimation, with 88,329 RGB-D frames and diverse bloody surgical gloves patterns. We evaluate and fine-tune three current stateof-the-art methods on the POV-Surgery dataset. We prove the effectiveness of the synthetic dataset by showing the significant performance improvement of the SOTA methods in real-life cases with surgical gloves and tools.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The proposed pipeline to generate synthetic data sequences. (a) shows the multi-stereo-cameras-based body motion capture module. (b) indicates the optimization-based hand-object manipulation sequence generation pipeline. (c) presents the fused hand-body pose and the egocentric camera pose calculation module. (d) highlights the rendering module with which RGB-D sequences are rendered with diverse textures.</figDesc><graphic coords="3,42,30,288,44,339,76,92,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The Hand manipulation sequence generation pipeline consists of three components: grasp pose generation, pose selection, and pose refinement, highlighted in blue, red, and green, respectively. (Color figure online)</figDesc><graphic coords="4,56,46,160,46,339,28,94,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Dataset samples for RGB-D sequences and annotation. An example of the scalpel, friem, and diskplacer, is shown in the first three rows. The fourth row shows an example of the new scene and blood glove patterns that only appear in the test set. (b) shows the statistics on the number of frames for each surgical instrument in the training and testing sets. (c) shows a point cloud created from an RGB-D frame with simulated Kinect noise.</figDesc><graphic coords="6,61,50,207,08,334,12,119,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Ground truth and qualitative results of different methods on the real-life test set. (b) Accuracy with different 2D pixel error thresholds, showing large performance improvement after fine-tuning on POV-Surgery (c) Our multi-camera real-life data capturing set-up.</figDesc><graphic coords="8,60,99,388,22,334,12,152,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,71,79,101,57,280,99,228,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The evaluation result of different methods on the test set of POV-Surgery, where the ft denotes fine-tuned on the training set. P 2d denotes the 2D hand joint reprojection error (in pixels). MPJPE and PVE denote the 3D Mean Per Joint Position Error and Per Vertex Error, respectively. PA denotes procrustes alignment.</figDesc><table><row><cell>Method</cell><cell cols="5">P 2d ↓ MPJPE ↓ PVE ↓ PA-MPJPE ↓ PA-PVE ↓</cell></row><row><cell>METRO [18]</cell><cell>95.11</cell><cell>77.46</cell><cell cols="2">75.06 23.43</cell><cell>22.34</cell></row><row><cell>SEMI [19]</cell><cell cols="2">77.91 115.67</cell><cell cols="2">112.10 12.68</cell><cell>12.76</cell></row><row><cell cols="2">HandOCCNet [22] 64.70</cell><cell>95.19</cell><cell cols="2">90.83 11.71</cell><cell>11.13</cell></row><row><cell>METRO ft</cell><cell>30.49</cell><cell>14.90</cell><cell>13.80</cell><cell>6.36</cell><cell>4.34</cell></row><row><cell>SEMI ft</cell><cell cols="2">13.42 15.14</cell><cell>14.69</cell><cell>4.29</cell><cell>4.23</cell></row><row><cell>HandOCCNet ft</cell><cell>13.80</cell><cell>14.35</cell><cell cols="2">13.73 4.49</cell><cell>4.35</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is part of a research project that has been financially supported by <rs type="funder">Accenture LLP</rs>. <rs type="person">Siwei Zhang</rs> is funded by <rs type="funder">Microsoft Mixed Reality &amp; AI Zurich Lab PhD scholarship</rs>. The authors would like to thank <rs type="person">PD Dr. Michaela Kolbe</rs> for providing the simulation facilities and the students participating in motion capture.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An interactive mixed reality platform for bedside surgical procedures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Azimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="65" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0_7" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">OpenPose: realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hidalgo Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blender -a 3D modelling and rendering package. Blender Foundation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">O</forename><surname>Community</surname></persName>
		</author>
		<ptr target="http://www.blender.org" />
	</analytic>
	<monogr>
		<title level="m">Stichting Blender Foundation</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmpose" />
		<title level="m">OpenMMLab pose estimation toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">EasyMocapmake human motion capture easier</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://github.com/zju3dv/EasyMocap" />
	</analytic>
	<monogr>
		<title level="j">Github</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast and robust multiperson 3D pose estimation and tracking from multiple views</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<editor>T-PAMI</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SurgeonAssist-Net: towards context-aware head-mounted display-based augmented reality for surgical guidance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Ghugre</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-1_64" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="667" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mapping surgeons hand/finger movements to surgical tool motion during conventional microsurgery using machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fattahi Sani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ascione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dogramadzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2150004</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>03n04</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">First-person hand action benchmark with RGB-D videos and 3D hand pose annotations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="409" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07219</idno>
		<title level="m">A real-time spatiotemporal AI model analyzes skill in open surgical videos</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">HOnnotate: a method for 3D annotation of hand and object poses</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hampali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A benchmark for RGB-D visual odometry, 3D reconstruction and slam</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning joint reconstruction of hands and manipulated objects</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11807" to="11816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards markerless surgical tool and hand pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hein</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-021-02369-2</idno>
		<idno>11548-021-02369-2</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surgery</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="799" to="808" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multitask learning for videobased surgical skill assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2020">2020. 2020</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hand-object contact consistency reasoning for human grasps generation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">H2O: two hands manipulating objects for first person interaction recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stühmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10138" to="10148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised 3D hand-object poses estimation with interactions in time</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14687" to="14697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">InterHand2. 6M: a dataset and baseline for 3D interacting hand pose estimation from a single RGB image</title>
		<author>
			<persName><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58565-5_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58565-5_33" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="548" to="564" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XX 16</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mixed reality and deep learning for external ventricular drainage placement: a fast and automatic workflow for emergency treatments</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Palumbo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_15</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_15" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HandOccNet: occlusion-robust 3D hand mesh estimation network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Objective surgical skill assessment: an initial experience by means of a sensory glove paving the way to open surgery simulation?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Surg. Educ</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="910" to="917" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Assembly101: a large-scale multi-view video dataset for understanding procedural activities</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="21096" to="21106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GRAB: a dataset of whole-body human grasping of objects</title>
		<author>
			<persName><forename type="first">O</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<ptr target="https://grab.is.tue.mpg.de" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Capturing hands in action using discriminative salient points and physics simulation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srikantha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="172" to="193" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Instrument detection and pose estimation with rigid part mixtures model in video-assisted surgeries</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wesierski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jezierska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="244" to="265" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How different augmented reality visualizations for drilling affect trajectory deviation, visual attention, and user experience</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luchmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lohmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fürnstahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meboldt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surgery</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Frei-HAND: a dataset for markerless capture of hand pose and shape from single RGB images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="813" to="822" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
