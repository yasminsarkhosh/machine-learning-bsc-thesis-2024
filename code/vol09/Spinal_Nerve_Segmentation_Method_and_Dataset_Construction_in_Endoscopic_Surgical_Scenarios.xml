<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios</title>
				<funder ref="#_W5EybGt">
					<orgName type="full">National Natural Science Foundation of China Incubation Project</orgName>
				</funder>
				<funder ref="#_pKdNZPn #_pA6j2yz #_4ERwYf8 #_wuHjeAs">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_xC3d9sC">
					<orgName type="full">Guangzhou Key R&amp;D Project</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaowu</forename><surname>Peng</surname></persName>
							<email>swpeng@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengcheng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongyu</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Spine Surgery</orgName>
								<orgName type="department" key="dep2">Guangdong Provincial People&apos;s Hospital (Guangdong Academy of Medical Sciences)</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junying</forename><surname>Chen</surname></persName>
							<email>jychense@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunbing</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Spine Surgery</orgName>
								<orgName type="department" key="dep2">Guangdong Provincial People&apos;s Hospital (Guangdong Academy of Medical Sciences)</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Spine Surgery</orgName>
								<orgName type="department" key="dep2">Guangdong Provincial People&apos;s Hospital (Guangdong Academy of Medical Sciences)</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="597" to="606"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">A34E6369AFACE25A899BB18FE7D2D3D7</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_57</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video endoscopic spinal nerve segmentation</term>
					<term>Self-attention</term>
					<term>Inter-frame information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Endoscopic surgery is currently an important treatment method in the field of spinal surgery and avoiding damage to the spinal nerves through video guidance is a key challenge. This paper presents the first real-time segmentation method for spinal nerves in endoscopic surgery, which provides crucial navigational information for surgeons. A finely annotated segmentation dataset of approximately 10,000 consecutive frames recorded during surgery is constructed for the first time for this field, addressing the problem of semantic segmentation. Based on this dataset, we propose FUnet (Frame-Unet), which achieves state-of-the-art performance by utilizing inter-frame information and self-attention mechanisms. We also conduct extended experiments on a similar polyp endoscopy video dataset and show that the model has good generalization ability with advantageous performance. The dataset and code of this work are presented at: https://github.com/ zzzzzzpc/FUnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spinal nerves play a crucial role in the body's sensory, motor, autonomic, and other physiological functions. Injuries to these nerves carry an extremely high risk and may even lead to paralysis. Minimally invasive endoscopic surgery is a common treatment option for spinal conditions, with great care taken to avoid damage to spinal nerves. However, the safety of such surgeries still heavily relies on the experience of the doctors, and there is an urgent need for computers to provide effective auxiliary information, such as real-time neural labeling and guidance in videos. Ongoing studies using deep learning methods to locate spinal nerves in endoscopic videos can be classified into two categories based on the level of visual granularity: coarse-grained and fine-grained tasks.</p><p>Coarse-grained vision task focuses on object detection of spinal nerve locations. In this task, object detection models applied to natural images are widely transferred to endoscopic spinal nerve images. Peng Cui et al. <ref type="bibr" target="#b0">[1]</ref> used the Yolov3 <ref type="bibr" target="#b1">[2]</ref> model to transfer training to the recognition of spinal nerves under endoscopy, which has attracted widespread attention. Sue Min Cho et al. <ref type="bibr" target="#b2">[3]</ref> referred to Kaiming's work and achieved certain results using RetinaNet <ref type="bibr" target="#b3">[4]</ref> for instrument recognition under spinal neuro-endoscopic images. Fine-grained tasks require semantic segmentation of spinal nerves, which provides finer contours and better visualization of the position and morphology of nerves in endoscopic view, leading to greater clinical significance and surgical auxiliary value. However, there are still very few deep learning-based studies on fine-grained segmentation of nerves, one important reason is a lack of semantic segmentation models suitable for spinal nerve endoscopy scenarios. The endoscopic image has the characteristics of blur, blisters, and the lens movement angle is not large, which is quite different from the natural scene image. Therefore, a segmentation model that performs well under natural images may not still be applicable under endoscopic images. Another reason is medical data involves ethical issues such as medical privacy, and the labeling of pixel-level data also relies on professional doctors. Furthermore, endoscopic images of the inside of the spine are often only available during surgery, which is much more difficult than obtaining image datasets from ordinary medical examinations. These lead to the scarcity of labeled data, and ultimately it is difficult to drive the training of neural network models.</p><p>In response to the above two problems, our contribution is as follows:</p><p>• We innovatively propose inter-frame and channel attention modules for the spinal neural segmentation problem. These two modules can be readily inserted into popular traditional segmentation networks such as Unet <ref type="bibr" target="#b4">[5]</ref>, resulting in a segmentation network proposed in this paper, called Frame-Unet (FUnet, Fig. <ref type="figure" target="#fig_0">1</ref>). The purpose of the inter-frame attention module is to capture the highly similar context between certain adjacent frames, which are characterized by the slower movement of the lens and high similarity of information such as background, elastic motion, and texture between frames. Moreover, we devised a channel self-attention module with global information to overcome the loss of long-distance dependent information in traditional convolutional neural networks. FUnet achieved leading results in many indicators of the dataset we created. Furthermore, FUnet was verified on similar endoscopic video datasets (such as polyps), and the results demonstrate that it outperforms others, confirming our model's strong generalization performance instead of overfitting to a single dataset. • We propose the first dataset on endoscopic spinal nerve segmentation from endoscopic surgeries, and each frame is finely labeled by professional labelers. The annotated results are also rechecked by professional surgeons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Details of Dataset</head><p>The dataset was taken by the professional SPINENDOS, SP081375.030 machines, and we collected nearly 10000 consecutive frames of video images, each with a resolution of up to 1080*1080 (Fig. <ref type="figure" target="#fig_1">2</ref>). The dataset aims to address the specific task of avoiding nerves during spinal endoscopic surgery. To this end, we selected typical scenes from the authors' surgical videos that not only have neural tissue as the target but also reflect the visual and motion characteristics of such scenes. Unlike other similar datasets with interval frame labeling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, we labeled each image frame by frame, whether it contains spinal nerves or not. Each frame containing spinal nerves was outlined in detail as a semantic segmentation task. Additionally, we used images containing spinal nerves (approximately 4-5 k images) to split the dataset into training, validation, and test sets for model construction (65%: 17.5%: 17.5%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Architecture</head><p>The overall network architecture is based on a classical Unet network <ref type="bibr" target="#b4">[5]</ref> (Fig. <ref type="figure" target="#fig_0">1</ref>). In the input part of the network, we integrate T frames (T &gt; 1) in a batch to fully utilize and cooperate with the inter-frame attention module (IFA). The input features dimensions are (B, T , C, H , W ), where B is batchsize, C denotes number of channels, H and W are the height and width of the image. Since we use a convolutional neural network for feature extraction, the 5-dimensional feature map is first merged into the convolutional network with B and T channels. Afterwards, the convolutional features are fed into the IFA for inter-frame information integration.</p><p>Unet's skip-connection method is a good complement to the information lost in the down-sampling reduction process, but it is difficult to capture the global contextual information due to the local dependency of the convolutional network. However, this global information is also crucial to the spinal nerve segmentation problem in the endoscopic scenario, so we inserted a channel self-attention module (CSA) in each up-sampling section with the global capability of the self-attention mechanism.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">IFA Module</head><p>To exploit the rich semantic information (such as blur, blisters, and other background semantics) between endoscopic frames, we designed the IFA to correlate the weight assignment between T frames. (Fig. <ref type="figure" target="#fig_2">3</ref>). If the features extracted by convolutional feature extraction are (B × T , C, H , W ), we first need to split the features to obtain (B, C × T , H , W ), which expands the channel dimension and allows subsequent convolutional operations to share the information between frames. After that, the feature matrix is down-sampled by four Attention Down Block's (ADB) channel pyramid architecture to obtain the (B, T , 1, 1) vector, and each (1, 1) weight value of this vector in T dimension will be assigned to the attention weight of the segmentation result of T frames.</p><p>Channel Pyramid. Although the multiscale operation in traditional convolutional neural networks can improve the generalization ability of the model to targets of different sizes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, it is difficult to capture information across frames, and down-sampling losses spatial precision at each cross-frame scale. Meanwhile, in many cases of endoscopic video, only the keyframes are clear enough to localize and segment the target, which can guide other blurred frames.</p><p>Hence, we propose a channel pyramid architecture to compress the channel dimension for capturing the cross-frame multiscale information, as well as to keep the feature map size unchanged in dimensions of height and width for preserving spatial precision. Such channel down-sampling obtains multi-scale information and semantic information in different cross-frame ranges. The result can adjust the frame weight on keyframe segmenting guidance. For detail, the feature matrix obtained by each ADB is compressed in the channel dimension, which avoids the loss of image size information. Like the perceptual field in the multi-scale approach, the number of inter-frame channels in the channel pyramid at different scales represents the magnitude of the scale across frames, and this inter-frame information at different scales is contacted for further fusion calculations, which is used to generate attention weights.</p><p>Attention Down Block (ADB). This module (Fig. <ref type="figure" target="#fig_2">3</ref>) is responsible for downsampling the channel dimension and generating the weight information between frames at one scale. We use the Light-RFB <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> module for channel down-sampling without changing the size in the height and width dimensions. In terms of the generation of attention weights, the feature vector after the adaptive global average pooling operation will be scaled to the T dimension by two 1 × 1 convolutions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">CSA Module</head><p>Inspired by the related work of vision transformer <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, we propose CSA mechanism (Fig. <ref type="figure" target="#fig_3">4</ref>) to address the problem of the lack of global information of traditional convolutional networks under spinal nerve endoscopy videos. Different from the classical vision transformer work, firstly, in the image patching stage, we use the convolution operation to obtain a feature matrix with a smaller dimension (such as B × C × 32 × 32), and the length corresponding to each patch is the length of each pixel channel (32 × 32), which reduces the amount of computation while sharing of information between different patches. This is because, in the process of convolution down-sampling, the convolution kernels will naturally cause overlapping calculations on the feature map, which will lead to the increase of the receptive field and the overlap of information between different patches. We use three fully connected layers Lq, Lk, Lv to generate Query, Key and Value feature vectors, this can be expressed as follows:</p><formula xml:id="formula_0">Query = Lq(X ), Key = Lk(X ), Value = Lv(X ) (1)</formula><p>X ∈ R B×(H ×W )×C is the original feature matrix expanded by the flatten operation. At the same time, to supplement the loss of spatial position information, we supplement the pos embedding vector by addition operation.</p><p>The multi-headed attention mechanism is implemented by a split operation. For calculation of self-attention, we use the common method <ref type="bibr" target="#b13">[14]</ref>, the Query matrix and the transpose matrix of Key are multiplied and then divided by the length d of the embedding vector, and this part of the result will be multiplied with Value after soft-max operation. Finally, this part of the features operated by self-attention will enter the LN layer, and after the sigmoid operation, the dimensions of the final attention weight matrix are consistent with the input vector. The formula for this part is as follows:</p><formula xml:id="formula_1">σ {LN softmax QK T √ d V }<label>(2)</label></formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>Dataset. The self-built dataset is our first contribution. We set up the training set, test set, and validation set. For extended experiments on the polyp dataset, we used the same dataset configuration as PNS-Net <ref type="bibr" target="#b10">[11]</ref>. The only difference is that in the testing phase, we used Valid and Test parts of the CVC-612 dataset <ref type="bibr" target="#b5">[6]</ref> for testing (CVC-612-VT).</p><p>Training. On the self-built dataset and the CVC-612 dataset, we both use learning rate and weight decay of 1e-4 with Adam optimizer. On the self-built dataset, our model converges after about 30 epochs, while on the CVC-616 dataset, we use the same pre-training and fine-tuning approach as PNS-Net. Methods involved in the data augmentation phase include flipping and cropping. A single TITAN RTX 24 GB graphics card is used for training.</p><p>Testing. Five images (T = 5) are input to the IFA and use the same input resolution of 256*448 as PNSNet to ensure consistency in subsequent tests. Our FUnet is capable of inference at 75 fps on a single TITAN RTX 24 GB, which means that real-time endoscopic spinal nerve segmentation is possible in surgery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>On our spinal nerve dataset, we tested the classical and leading medical segmentation networks Unet <ref type="bibr" target="#b4">[5]</ref>, Unet++ <ref type="bibr" target="#b14">[15]</ref>, TransUnet (ViT-Base) <ref type="bibr" target="#b15">[16]</ref>, SwinUnet (ViT-Base) <ref type="bibr" target="#b16">[17]</ref> and PNSNet <ref type="bibr" target="#b10">[11]</ref> respectively. For comparison, we used the default hyperparameter settings of the networks and employed a CNN feature extractor consistent with that of PNSNet. Four metrics that are widely used in the field of medical image segmentation are chosen, maxIOU, maxDice, meanSpe/maxSpe and MAE. The quantitative result is in Table <ref type="table" target="#tab_0">1</ref>. Our FUnet achieves state of the art performance on different medical segmentation metrics. The Our-VT dataset means that we use the validation and test datasets for the testing phase (neither of which is involved in training).</p><p>The qualitative comparison is in Fig. <ref type="figure" target="#fig_4">5</ref>, which shows our FUnet can more accurately segment the contour of the model and the texture of the edges.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Experiments</head><p>The baseline model uses Res2Net <ref type="bibr" target="#b17">[18]</ref> as the basic feature extractor with the Unet model, and in the first output feature layer, we adopt a feature fusion strategy consistent with PNSNet. We have gradually verified the performance of the IFA and CSA modules on the baseline model (Table <ref type="table" target="#tab_1">2</ref>), and experiments have proved that our two modules can stably improve the segmentation performance of spinal nerves in endoscopic scene. A comparison of qualitative results is available in Fig. <ref type="figure" target="#fig_5">6</ref>.  In addition, more extended experiments are carried out under a similar endoscopic polyp dataset CVC-612 <ref type="bibr" target="#b5">[6]</ref> (Table <ref type="table" target="#tab_2">3</ref>, CVC-612-TV means that we used both test and valid parts during the testing phase, the validation part was not visible during the training phase.), and the experiments show that our FUnet has good generalization performance and can adapt to endoscopic segmentation in different scenarios. A comparison of qualitative results is available in Fig. <ref type="figure" target="#fig_6">7</ref>, our FUnet still performs well in its ability to segment the edges of polyps.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose the industry's first semantic segmentation dataset of spinal nerves from endoscopic surgery to date and design the FUnet segmentation network based on inter-frame information and self-attention mechanism. FUnet has achieved state of the art performance on our dataset and shows strong generalization performance on polyp dataset with similar scenes. The IFA and CSA modules of FUnet can be easily incorporated into other networks. We plan to expand the dataset in the future with the help of self-supervised methods, to improve the performance of the model to provide better computer-assisted surgery capabilities for spinal endoscopy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Pipeline of the proposed FUnet, including the inter-frame attention module and channel self-attention module with global information.</figDesc><graphic coords="2,61,29,160,25,301,93,227,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the original and labeled images.</figDesc><graphic coords="4,45,30,227,72,333,67,121,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Pipeline of inter-frame attention Module (IFA)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Pipeline of Channel Self-Attention Module</figDesc><graphic coords="5,110,46,284,60,231,46,159,01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The qualitative results on our dataset, for more results please refer to the supplementary material.</figDesc><graphic coords="7,83,46,418,58,285,64,77,23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The qualitative results on ablation study, for more results please refer to the supplementary material.</figDesc><graphic coords="8,97,80,258,95,228,55,75,55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The qualitative results on CVC-612-Valid. For more results please refer to the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The quantitative results on our spinal nerve datasets. Our-VT means we use the valid and test part for testing. (Validation part is unseen during testing).</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Metrics</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>maxDice↑</cell><cell>maxIOU↑</cell><cell>MAE↓</cell><cell>meanSpe↑</cell></row><row><cell>Our-VT</cell><cell>Unet</cell><cell>0.778</cell><cell>0.702</cell><cell>0.044</cell><cell>0.881</cell></row><row><cell></cell><cell>Unet + +</cell><cell>0.775</cell><cell>0.705</cell><cell>0.026</cell><cell>0.878</cell></row><row><cell></cell><cell>TransUnet</cell><cell>0.825</cell><cell>0.758</cell><cell>0.082</cell><cell>0.881</cell></row><row><cell></cell><cell>SwinUnet</cell><cell>0.823</cell><cell>0.752</cell><cell>0.026</cell><cell>0.877</cell></row><row><cell></cell><cell>PNSNet</cell><cell>0.882</cell><cell>0.823</cell><cell>0.016</cell><cell>0.885</cell></row><row><cell></cell><cell>FUnet</cell><cell>0.890</cell><cell>0.833</cell><cell>0.016</cell><cell>0.885</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies. B* for baseline. I* for IFA module. C* for CSA module. Our-VT means we use the valid and test part for testing.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Metrics</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>maxDice↑</cell><cell>maxIOU↑</cell><cell>MAE↓</cell><cell>meanSpe↑</cell></row><row><cell>Our-VT</cell><cell>B*</cell><cell>0.862</cell><cell>0.798</cell><cell>0.016</cell><cell>0.885</cell></row><row><cell></cell><cell>B* + I*</cell><cell>0.884</cell><cell>0.826</cell><cell>0.015</cell><cell>0.884</cell></row><row><cell></cell><cell>B* + I* + C*</cell><cell>0.890</cell><cell>0.833</cell><cell>0.016</cell><cell>0.885</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The quantitative results on polyp datasets. CVC-612-TV means that we used both test and valid parts during the testing phase.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Metrics</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>maxDice↑</cell><cell>maxIOU↑</cell><cell>MAE↓</cell><cell>maxSpe↑</cell></row><row><cell>CVC-612-VT</cell><cell>Unet</cell><cell>0.727</cell><cell>0.623</cell><cell>0.041</cell><cell>0.971</cell></row><row><cell></cell><cell>Unet + +</cell><cell>0.713</cell><cell>0.603</cell><cell>0.042</cell><cell>0.963</cell></row><row><cell></cell><cell>PNSNet</cell><cell>0.866</cell><cell>0.797</cell><cell>0.025</cell><cell>0.991</cell></row><row><cell></cell><cell>FUnet</cell><cell>0.873</cell><cell>0.805</cell><cell>0.025</cell><cell>0.989</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is supported by <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (<rs type="grantNumber">2021A1515011349</rs>, <rs type="grantNumber">2021A1515012651</rs>, <rs type="grantNumber">2022A1515111091</rs>, <rs type="grantNumber">2022A1515012557</rs>), the <rs type="funder">National Natural Science Foundation of China Incubation Project</rs> (<rs type="grantNumber">KY0120220040</rs>), and the <rs type="funder">Guangzhou Key R&amp;D Project</rs> (<rs type="grantNumber">202103000053</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pKdNZPn">
					<idno type="grant-number">2021A1515011349</idno>
				</org>
				<org type="funding" xml:id="_pA6j2yz">
					<idno type="grant-number">2021A1515012651</idno>
				</org>
				<org type="funding" xml:id="_4ERwYf8">
					<idno type="grant-number">2022A1515111091</idno>
				</org>
				<org type="funding" xml:id="_wuHjeAs">
					<idno type="grant-number">2022A1515012557</idno>
				</org>
				<org type="funding" xml:id="_W5EybGt">
					<idno type="grant-number">KY0120220040</idno>
				</org>
				<org type="funding" xml:id="_xC3d9sC">
					<idno type="grant-number">202103000053</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tissue recognition in spinal endoscopic surgery using deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th International Conference on Awareness Science and Technology (iCAST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: an incremental improvement</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic tip detection of surgical instruments in biportal endoscopic spine surgery</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page">104384</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">WM-DOVA maps for accurate polyp highlighting in colonoscopy: validation vs. saliency maps from physicians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fernández -Esparrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards automatic polyp detection with a polyp appearance model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilarino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3166" to="3182" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="385" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Progressively normalized self-attention network for video polyp segmentation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_14" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="142" to="152" />
		</imprint>
	</monogr>
	<note>Medical Image Computing and Computer Assisted Intervention -MICCAI 2021</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">UNet++: a nested U-Net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">TransUNet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Swin-UNet: UNet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-8_9" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022 Workshops</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13803</biblScope>
			<biblScope unit="page" from="205" to="218" />
		</imprint>
	</monogr>
	<note>ECCV 2022</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Res2Net: a new multi-scale backbone architecture</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="652" to="662" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
