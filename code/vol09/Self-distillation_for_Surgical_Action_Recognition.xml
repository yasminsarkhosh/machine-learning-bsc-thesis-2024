<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-distillation for Surgical Action Recognition</title>
				<funder ref="#_RFSKjqR #_tNEXkxu #_D9jGhGK #_Ruh7yc7 #_FfKrVXR #_cJcwHxG">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">German Cancer Research Center (DKFZ)</orgName>
				</funder>
				<funder ref="#_8zr7wbb">
					<orgName type="full">Helmholtz Association</orgName>
				</funder>
				<funder ref="#_EB2cRW2">
					<orgName type="full">French</orgName>
				</funder>
				<funder>
					<orgName type="full">Robert Bosch Center for Tumor Diseases</orgName>
					<orgName type="abbreviated">RBCT</orgName>
				</funder>
				<funder>
					<orgName type="full">HELMHOLTZ IMAGING</orgName>
				</funder>
				<funder ref="#_fNpFrjk #_mxTVauB">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Amine</forename><surname>Yamlahi</surname></persName>
							<email>m.elyamlahi@dkfz-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">National Center for Tumor Diseases (NCT)</orgName>
								<orgName type="institution">NCT Heidelberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thuy</forename><forename type="middle">Nuong</forename><surname>Tran</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Godau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">National Center for Tumor Diseases (NCT)</orgName>
								<orgName type="institution">NCT Heidelberg</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">HIDSS4Health -Helmholtz Information and Data Science School for Health</orgName>
								<address>
									<settlement>Karlsruhe, Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Melanie</forename><surname>Schellenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">National Center for Tumor Diseases (NCT)</orgName>
								<orgName type="institution">NCT Heidelberg</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">HIDSS4Health -Helmholtz Information and Data Science School for Health</orgName>
								<address>
									<settlement>Karlsruhe, Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dominik</forename><surname>Michael</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">National Center for Tumor Diseases (NCT)</orgName>
								<orgName type="institution">NCT Heidelberg</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Finn-Henri</forename><surname>Smidt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan-Hinrich</forename><surname>NÃ¶lke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><forename type="middle">J</forename><surname>Adler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">National Center for Tumor Diseases (NCT)</orgName>
								<orgName type="institution">NCT Heidelberg</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minu</forename><forename type="middle">Dietlinde</forename><surname>Tizabi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chinedu</forename><forename type="middle">Innocent</forename><surname>Nwoye</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">ICube Laboratory</orgName>
								<orgName type="institution">University of Strasbourg</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">ICube Laboratory</orgName>
								<orgName type="institution">University of Strasbourg</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lena</forename><surname>Maier-Hein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">National Center for Tumor Diseases (NCT)</orgName>
								<orgName type="institution">NCT Heidelberg</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Medical Faculty</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Partnership between DKFZ</orgName>
								<orgName type="institution">University Medical Center Heidelberg</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-distillation for Surgical Action Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="637" to="646"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">302927CC346DD7A66C48936492AA8F03</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Surgical action recognition</term>
					<term>Self-distillation</term>
					<term>Laparoscopic surgery</term>
					<term>Surgical workflow</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Surgical scene understanding is a key prerequisite for context-aware decision support in the operating room. While deep learning-based approaches have already reached or even surpassed human performance in various fields, the task of surgical action recognition remains a major challenge. With this contribution, we are the first to investigate the concept of self-distillation as a means of addressing class imbalance and potential label ambiguity in surgical video analysis. Our proposed method is a heterogeneous ensemble of three models that use Swin Transformers as backbone and the concepts of self-distillation and multi-task learning as core design choices. According to ablation studies performed with the CholecT45 challenge data via cross-validation, the biggest performance boost is achieved by the usage of soft labels obtained by self-distillation. External validation of our method on an independent test set was achieved by providing a Docker container of our inference model to the challenge organizers. According to their analysis, our method outperforms all other solutions submitted to the latest challenge in the field. Our approach thus shows the potential of self-distillation for becoming an important tool in medical image analysis applications. Code available at https://github.com/IMSY-DKFZ/self-distilled-swin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Surgical scene understanding is an important prerequisite for artificial intelligence (AI)-empowered surgery <ref type="bibr" target="#b12">[12]</ref>, underlying a range of application areas such as context-aware decision support, autonomous robotics, and workflow optimization. One of its key components is the fully-automatic recognition of the surgical action performed at a given point in time -a task not yet solved by state-of-theart-methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">20]</ref>. To advance the field, the CholecTriplet challenge was organized in the scope of the Medical Image Computing and Computer Assisted Interventions (MICCAI) conferences 2021 and 2022. However, according to the organizers analysis <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20]</ref>, the task still remains unsolved. The guiding hypothesis of our work was that self-distillation could address some of the challenges in surgical action recognition, namely the high number of classes (100 in the case of CholecTriplet); high class imbalance, and label ambiguity. Self-distillation builds upon the widespread concept of knowledge distillation (KD) <ref type="bibr" target="#b6">[6]</ref>, in which the knowledge is transferred from one deep model (i.e., a teacher) to another shallow model (i.e., a student). Self-distillation diverges from traditional KD by distilling knowledge within the network itself. While KD is already used in various communities, the purpose of this work was to pioneer the concept of selfdistillation in the context of surgical data science. Based on the CholecTriplet training data set, we developed a method for surgical action recognition (Fig. <ref type="figure" target="#fig_1">2</ref>) that leverages self-distillation, Swin Transformers <ref type="bibr" target="#b11">[11]</ref>, multi-task learning, and ensembling. The following Sect. 2 presents our methodological contribution in detail. Sect. 3 presents ablation studies on the challenge data set that reveal the most important design choices as well as an external validation of our solution on an independent surgical video data set. We conclude with a brief discussion of the most relevant aspects of our work in Sect. 4.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Description and Dataset</head><p>Our study is based on the CholecTriplet Challenge 2022 <ref type="bibr" target="#b18">[18]</ref>, which was conducted under the umbrella of the Endoscopic Vision Challenges (EndoVis) in conjunction with MICCAI. The Surgical Action Recognition task required participants to submit solutions that recognize surgical action triplets in laparoscopic videos, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. The challenge granted access to the CholecT45 <ref type="bibr" target="#b19">[19]</ref> dataset which consists of 45 video recordings of laparoscopic cholecystectomy with a total of 90,489 frames. CholecT45 is annotated with 100 action triplet classes, with one instrument, verb, and target forming a triplet. The annotations include six different instrument classes, ten verbs (denoting the action performed), and 15 targets such as organs, tissues, or foreign bodies (clip, specimen bag, etc.). The theoretical maximum of 6 â¢ 10 â¢ 15 classes was reduced to the above-mentioned 100 based on medical relevance and prevalence. An example image from the CholecT45 dataset containing two triplet annotations can be seen in Fig. <ref type="figure" target="#fig_0">1 (a)</ref>. A chart depicting the highly imbalanced class distribution is shown in Fig. <ref type="figure" target="#fig_0">1 (b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Concept Overview</head><p>As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, our approach is based on the following key components: (1) Swin Transformer: The recently proposed Swin Transformer <ref type="bibr" target="#b11">[11]</ref> architecture was chosen as backbone. (2) Multi-task learning: Based on the success of previous work that leveraged multi-task learning as its training paradigm, we incorporated multiple auxiliary tasks in our architecture, namely the classification of the individual components of the triplet (instrument, verb, and target) as well as the surgical phase. (3) Self-distillation: The core idea of our approach is the usage of soft labels to reduce overconfidence and address label ambiguity. (4) Ensemble: Following common successful training strategies, we implement ensembling to combine the predictions of three trained Swin Transformers of different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementation Details</head><p>Swin Transformer. We base our method on Swin Transformer (SwinT) models of the timm <ref type="bibr" target="#b24">[24]</ref> library and adopted the final classification layer to output the 100 triplet predictions, as well as the individual instruments, verbs, targets and surgical phase as auxiliary tasks to leverage the interconnection between them in a multi-task fashion (+Multi).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-distillation. The concept of self-distillation was achieved by training a</head><p>teacher Swin transformer on one-hot encoded hard labels for 20 epochs, with a batch size of 64, an Adam <ref type="bibr" target="#b10">[10]</ref> optimizer, a learning rate of 2 Ã10 -4 , a cosine annealing scheduler decreasing to a minimum learning rate of 2 Ã10 -6 , and a binary cross-entropy loss function. The model was trained with light augmentations that comprise resizing the images to 224 Ã 224 pixels, horizontal and vertical flips, rotation, brightness and saturation perturbations with a probability of 0.5. We trained five teacher and five student models; one for each fold of the official five-fold cross validation splitting introduced by the challenge. The teacher was trained on four of the five splits of its fold. After convergence, the soft labels (i.e., the sigmoid probabilites) for the same four splits were computed and the student was trained using these soft labels. The validation was performed on the fifth split, using hard (i.e., the original) labels for both the teacher and the student. During inference, the sigmoid probabilities of the five student models were averaged to yield the final result.</p><p>The five teacher models shared a common weight intialization seed. The five student models shared a separate weight initialization seed. The student models were trained for 40 epochs with the same augmentations as the teacher models. We saved the weights on the epoch with the best mean Average Precision (mAP) score based on the validation split for the current fold.</p><p>Ensemble. We combined three trained Swin Transformers (SwinT) of different scales (SwinT base/SwinT large) and configurations for our final ensemble (Ens) model: First, we employed a SwinT base model with multi-task learning of instrument, verb, and target and trained it using self-distillation. Second, we used a SwinT large model using the same approach, and added label smoothing to the soft labels. Third, we included phase annotations as an additional task for the multi-task training of a SwinT base model still employing self-distillation. Please note that every single model mentioned here corresponds to the five aggregated student models of the previous paragraph. All the models were trained using Nvidia GPUs Geforce RTX 3090 and Tesla V100 32GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>The purpose of the experiments was to validate the performance of our method and to quantify the (potential) benefit of each individual component. To this end, we conducted (1) comprehensive ablation studies using the CholecT45 official 5-Fold cross-validation split <ref type="bibr" target="#b17">[17]</ref>, (2) an analysis of the specific benefit of soft labels, and (3) an external validation based on a Docker container submitted to the CholecTriplet 2022 challenge organizers. The Rendezvous Net <ref type="bibr" target="#b18">[18]</ref>, provided by the challenge organizers, served as the benchmark. In line with the challenge design <ref type="bibr" target="#b13">[13]</ref>, we validated the performance using mAP (following the aggregation scheme in <ref type="bibr" target="#b15">[15]</ref>) and the top K=5 Accuracy as metrics. All scores were computed using the ivtmetrics library <ref type="bibr" target="#b17">[17]</ref>.</p><p>Ablation Studies. We designed the ablation studies as follows: We first calculated the performance of our Swin Transformer backbone as a stand-alone triplet classifier (SwinT). Next, we added multiple auxiliary targets (instruments, verbs, targets, and phases) for multi-task classification (+MultiT). As a third component, we implemented self-distillation by training a student model on soft labels, acquired by training the teacher model (+SelfD). The fourth step was the ensembling of three student model SwinT (+Ens). The results are shown in Tab. 1. A single SwinT as model backbone yields a higher mAP for triplet classification (mAP=32.3%) than the benchmark (mAP=28.8%), which corresponds to a relative improvement by 10.3%. The biggest boost was achieved by including selfdistillation, which improved the Triplet mAP and top-5 accuracy by 3.8% points (pp) and 2.4pp, respectively, compared to our baseline. The final model yielded a mAP of 38.5% and a top-5 accuracy of 86.5%, which corresponds to a boost of 6.2pp in mAP and 2.7pp in top-5 accuracy compared to our own baseline, and a relative improvement by 33.7% for mAP compared to the state-of-the-art method. For transparency, we also provide per-video results, depicted in Fig. <ref type="figure" target="#fig_2">3</ref>. With a few exceptions, our final model consistently provides the best results.</p><p>Analysis of Soft Labels. The addition of self-distillation resulted in the highest boost in performance. This holds true despite the fact that the mAP of the teacher model, trained on hard labels, was about 88% on the training set, which is sub-optimal. The question is thus why the poorer soft labels still yielded a performance improvement. While part of the answer is provided in the literature on soft/noisy labels <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b26">26]</ref>, we also speculated that the soft labels may address the issue of ambiguous/erroneous labels in our particular use case. More specifically, we assumed that if the teacher model increases the probability of semantically close triplets in the soft labels, it could lead to an enhanced level of confidence in the student model's prediction of the ground truth, potentially accounting for the observed performance improvement. To investigate whether this is the case, we first defined a pragmatic proxy metric for semantic similarity: the number of identical triplet items (max: two for different triplets). We then selected all frames with only one unique triplet label and retrieved the top five triplets (excluding the reference) with the highest soft label score. Figure <ref type="figure" target="#fig_3">4</ref>, depicts an example of such a comparison. The reference triplet "bipolar, dissect, cystic plate" is shown with five soft label triplets ranked by probability. In the example, the top five triplets share an average of 1.6 components with the reference, indicating that they contain similar semantic information. We found that over all samples, the average number of component matches between reference and top five triplets is 1.0Â±0.002. In contrast, when comparing the reference with five triplets randomly drawn (while respecting prevalence), the agreement is 0.5Â±0.002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Main quantitative results</head><p>Starting from our backbone model -a Swin Transformer (Swin T) -we gradually added individual components, namely multi-task learning (MultiT), self-distillation (SelfD), and ensembling (Ens). Each component addition leads to an increase in mAP and top-5 accuracy, in both cross-validation (left) and independent external validation (right). This shows that self-distillation specifically leads to increased scores for semantically related classes. Independent External Validation. External validation was conducted on the CholecTriplet challenge test set. The results, shown in Table <ref type="table">1</ref>, confirm the results from cross-validation experiments, with the final ensemble scoring 37.4% in mAP. This equals an absolute improvement of 4.7pp and a relative improvement of 14.4% compared to the Rendezvous benchmark (mAP= 32.7%). With a previous version of the method presented in this paper (scoring slightly lower) we won the challenge in 2022 as the only team that explored the concept of self-distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>This paper pioneers the concept of self-distillation in the medical image analysis domain. Specifically, we are the first to tackle key challenges in surgical action recognition, namely the high number of classes and class imbalance, with selfdistillation. Comprehensive ablation studies combined with external validation yielded the following findings:</p><p>1. Swin Transformers, as recently introduced by the computer vision community, can serve as a strong backbone in endoscopic vision tasks. This is suggested by the fact that even our most ablated model, consisting of a single SwinT, surpasses the state-of-the-art surgical action recognition method Rendezvouz. 2. Multi-task learning, here using the classification of instrument, verb, and target as well as of the surgical phase as auxiliary tasks, yielded a notable increase in performance. 3. Self-distillation yielded the biggest boost in performance, suggesting that soft labels are better suited for surgical action recognition. 4. Ensembling increased performance further, as also suggested by various publications in a wide range of fields. Overall, the addition of self-distillation (in combination with the SwinT as a backbone) resulted in the highest performance boost. While label noise has been shown to be beneficial in various work <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b26">26]</ref>, the concept of selfdistillation may not necessarily be intuitive; although the mAP achieved by the teacher model, trained on hard labels, is sub-optimal (32%), the teacher's noisy labels lead to an overall improvement in performance when compared to the (presumably better-quality) hard labels. In the general machine learning literature, the knowledge encoded in noisy labels is referred to as "dark knowledge" because it is not yet well-understood. Aiming to shed light on this topic, our experiments on semantic similarity suggest that soft labels may actually address the issue of ambiguous/erroneous labels. Further analyses with more sophisticated metrics for semantic similarity are, however, needed to support this finding.</p><p>Related work has so far tackled the challenge of surgical action recognition with various strategies including multi-task learning <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b21">21]</ref>, and different attention mechanisms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">19]</ref> incorporated into diverse architectures based on temporal convolutional networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">21]</ref>, transformers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">19]</ref>, or combinations of convolutional neural networks (CNN) with recurrent neural networks (RNN) <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b25">25]</ref> or hidden Markov models (HMM) <ref type="bibr" target="#b22">[22]</ref>. While our approach was particularly successful according to the challenge analysis, the overall performance is still not optimal. Advancing the methods will require more data that features a sufficient number of samples for each triplet and captures the full variability of scenes that might be encountered in practice. From a methodological perspective, future work should be directed to efficiently taking temporal context into account and addressing potential domain shifts <ref type="bibr" target="#b0">[1]</ref>.</p><p>In conclusion, our study is the first to demonstrate the benefit of selfdistillation for surgical vision tasks. Based on the substantial performance boost obtained, the usage of soft labels could become a valuable tool in the endoscopic vision community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Task of surgical action recognition. (a) Each action is represented by a triplet comprising instrument, verb and target. Multiple triplets can be present in one image, as shown in the example. (b) CholecTriplet training data set illustrating the heavy class imbalance. Of 100 possible triplet classes, the prevalence ranges from 0.01% to 44.6%.</figDesc><graphic coords="2,42,30,394,61,339,52,121,36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Approach to surgical action triplet recognition. (a) Our architecture leverages Swin Transformer (SwinT) as a backbone and the concepts of self-distillation, multi-task learning, and ensembling as core strategies. The teacher model is trained on hard labels using binary cross-entropy (BCE) loss. Inferencing the training data, the sigmoid probabilities are used as input in the next step. The student model is trained with the noisy soft labels in a multi-task fashion to minimize the BCE loss, commonly referred to as distillation loss, between the teacher and the student's predictions. (b) Visualization of label distribution in hard and soft labels.</figDesc><graphic coords="3,70,47,54,17,311,38,217,54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Quantitative results on the validation data. (a) mean Average Precision (mAP) plotted separately for each video for five configurations of our method with increasing complexity. A Swin Transformer (SwinT) was gradually complemented by multi-task learning (MultiT), self-distillation (SelfD), and ensembling (Ens). Videos were sorted by mAP score of final ensemble performance from highest to lowest. (b) Corresponding dot-and boxplots of mAP scores, aggregated over all videos</figDesc><graphic coords="6,73,80,54,23,276,28,235,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of generated soft labels with reference (in green) and top 5 soft label triplets ranked by probability (in red). Average number of component matches between reference "bipolar, dissect, cystic plate" and top five triplets is 1.6.</figDesc><graphic coords="8,75,81,53,78,272,20,104,92" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This project was supported by a Twinning Grant of the <rs type="funder">German Cancer Research Center (DKFZ)</rs> and the <rs type="funder">Robert Bosch Center for Tumor Diseases (RBCT)</rs>. Part of this work was funded by <rs type="funder">HELMHOLTZ IMAGING</rs>, a platform of the Helmholtz Information &amp; Data Science Incubator and the <rs type="funder">Helmholtz Association</rs> under the joint research school "<rs type="programName">HIDSS4Health -Helmholtz Information and Data Science School for Health</rs>" and by <rs type="funder">French</rs> state funds managed within the <rs type="programName">Plan Investissements d'Avenir</rs> by the <rs type="funder">ANR</rs> under references: <rs type="programName">National AI Chair AI4ORSafety</rs> [<rs type="grantNumber">ANR-20-CHIA-0029-01</rs>], <rs type="projectName">Labex CAMI</rs> [<rs type="grantNumber">ANR-11-LABX-0004</rs>], <rs type="projectName">DeepSurg</rs> [<rs type="grantNumber">ANR-16-CE33-0009</rs>], <rs type="projectName">IHU Strasbourg</rs> [<rs type="grantNumber">ANR-10-IAHU-02</rs>] and by <rs type="funder">BPI France</rs> under references: project <rs type="projectName">CON-DOR</rs>, project <rs type="grantNumber">5G-OR</rs>. Model Docker evaluation were performed with servers/HPC resources managed by CAMMA, <rs type="person">IHU Strasbourg</rs>, <rs type="person">Unistra MÃ©socentre</rs>, and <rs type="institution">GENCI-IDRIS</rs> [Grant <rs type="grantNumber">2021-AD011011638R1</rs>, <rs type="grantNumber">2021-AD011011638R2</rs>, <rs type="grantNumber">2021-AD011011638R3</rs>].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8zr7wbb">
					<orgName type="program" subtype="full">HIDSS4Health -Helmholtz Information and Data Science School for Health</orgName>
				</org>
				<org type="funding" xml:id="_EB2cRW2">
					<orgName type="program" subtype="full">Plan Investissements d&apos;Avenir</orgName>
				</org>
				<org type="funded-project" xml:id="_fNpFrjk">
					<idno type="grant-number">ANR-20-CHIA-0029-01</idno>
					<orgName type="project" subtype="full">Labex CAMI</orgName>
					<orgName type="program" subtype="full">National AI Chair AI4ORSafety</orgName>
				</org>
				<org type="funded-project" xml:id="_RFSKjqR">
					<idno type="grant-number">ANR-11-LABX-0004</idno>
					<orgName type="project" subtype="full">DeepSurg</orgName>
				</org>
				<org type="funded-project" xml:id="_tNEXkxu">
					<idno type="grant-number">ANR-16-CE33-0009</idno>
					<orgName type="project" subtype="full">IHU Strasbourg</orgName>
				</org>
				<org type="funded-project" xml:id="_mxTVauB">
					<idno type="grant-number">ANR-10-IAHU-02</idno>
					<orgName type="project" subtype="full">CON-DOR</orgName>
				</org>
				<org type="funding" xml:id="_D9jGhGK">
					<idno type="grant-number">5G-OR</idno>
				</org>
				<org type="funding" xml:id="_Ruh7yc7">
					<idno type="grant-number">2021-AD011011638R1</idno>
				</org>
				<org type="funding" xml:id="_FfKrVXR">
					<idno type="grant-number">2021-AD011011638R2</idno>
				</org>
				<org type="funding" xml:id="_cJcwHxG">
					<idno type="grant-number">2021-AD011011638R3</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 61.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Causality matters in medical imaging</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3673</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TeCNO: surgical phase recognition with multi-stage temporal convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">OperA: attention-regularized transformers for surgical phase recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ostler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_58</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-158" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="604" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Biomedical image analysis competitions: The state of current participation practice</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eisenmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08568</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trans-SVNet: accurate phase recognition from surgical videos via hybrid embedding aggregation transformer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="593" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_57</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-157" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sv-rcnet: workflow recognition from surgical videos using recurrent convolutional network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1114" to="1126" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal memory relation network for workflow recognition from surgical video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1911" to="1923" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-knowledge distillation with progressive refinement of targets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6567" to="6576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10">October 2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Surgical data science-from concepts toward clinical translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">102306</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Miccai</forename><surname>Sig</surname></persName>
		</author>
		<author>
			<persName><surname>Challenges</surname></persName>
		</author>
		<ptr target="https://www.miccai.org/special-interest-groups/challenges/miccai-registered-challenges/" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>MICCAI registered challenges</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-distillation amplifies regularization in hilbert space</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3351" to="3361" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cholectriplet 2021: a benchmark challenge for surgical action triplet recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04746</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognition of instrument-tissue interactions in endoscopic videos via action triplets</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-035" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="364" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Data splits and metrics for benchmarking methods on surgical action triplet datasets</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05235</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Surgical action triplet detection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<ptr target="https://cholectriplet2022.grand-challenge.org/" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rendezvous: attention mechanisms for the recognition of surgical action triplets in endoscopic videos</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">102433</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cholectriplet 2022: show me a tool and tell me the tripletan endoscopic vision challenge for surgical action triplet detection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06294</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-task temporal convolutional networks for joint recognition of surgical phases and steps in gastric bypass procedures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramesh</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-021-02388-z</idno>
		<ptr target="https://doi.org/10.1007/s11548-021-02388-z" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1111" to="1119" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Endonet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Teaching yourself: a self-knowledge distillation approach to action recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="105711" to="105723" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4414861" />
		<title level="m">Pytorch image models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning from a tiny dataset of manual annotations: a teacher/student approach for surgical phase recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00033</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regularizing class-wise predictions via selfknowledge distillation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13876" to="13885" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
