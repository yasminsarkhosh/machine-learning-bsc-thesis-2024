<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers</title>
				<funder ref="#_BvfYjMT">
					<orgName type="full">The National Centre for Research and Development, Poland</orgName>
				</funder>
				<funder ref="#_9wwc78k">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Marek</forename><surname>Wodzinski</surname></persName>
							<email>wodzinski@agh.edu.pl</email>
							<idno type="ORCID">0000-0002-8076-6246</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Measurement and Electronics</orgName>
								<orgName type="institution">AGH University of Science and Technology</orgName>
								<address>
									<settlement>Krakow</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Information Systems Institute</orgName>
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO Valais)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mateusz</forename><surname>Daniol</surname></persName>
							<idno type="ORCID">0000-0003-2363-7912</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Measurement and Electronics</orgName>
								<orgName type="institution">AGH University of Science and Technology</orgName>
								<address>
									<settlement>Krakow</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daria</forename><surname>Hemmerling</surname></persName>
							<idno type="ORCID">0000-0002-2193-7690</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Measurement and Electronics</orgName>
								<orgName type="institution">AGH University of Science and Technology</orgName>
								<address>
									<settlement>Krakow</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Miroslaw</forename><surname>Socha</surname></persName>
							<idno type="ORCID">0000-0001-9462-8269</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Measurement and Electronics</orgName>
								<orgName type="institution">AGH University of Science and Technology</orgName>
								<address>
									<settlement>Krakow</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="333" to="343"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F2CE732E72AD2A8A3D1E1FD9685F4C50</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_32</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Cranial Implant Design</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Each year thousands of people suffer from various types of cranial injuries and require personalized implants whose manual design is expensive and time-consuming. Therefore, an automatic, dedicated system to increase the availability of personalized cranial reconstruction is highly desirable. The problem of the automatic cranial defect reconstruction can be formulated as the shape completion task and solved using dedicated deep networks. Currently, the most common approach is to use the volumetric representation and apply deep networks dedicated to image segmentation. However, this approach has several limitations and does not scale well into high-resolution volumes, nor takes into account the data sparsity. In our work, we reformulate the problem into a point cloud completion task. We propose an iterative, transformer-based method to reconstruct the cranial defect at any resolution while also being fast and resource-efficient during training and inference. We compare the proposed methods to the state-of-the-art volumetric approaches and show superior performance in terms of GPU memory consumption while maintaining high-quality of the reconstructed defects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cranial damage is a common outcome of traffic accidents, neurosurgery, and warfare. Each year, thousands of patients require personalized cranial implants <ref type="bibr" target="#b1">[2]</ref>. Nevertheless, the design and production of personalized implants are expensive and time-consuming. Nowadays, it requires trained employees working with computer-aided design (CAD) software <ref type="bibr" target="#b10">[11]</ref>. However, one part of the design pipeline, namely defect reconstruction, can be directly improved by the use of deep learning algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>The problem can be formulated as a shape completion task and solved by dedicated neural networks. Its importance motivated researchers to organize two editions of the AutoImplant challenge, during which researchers proposed several unique contributions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. The winning contributions from the first <ref type="bibr" target="#b2">[3]</ref> and second editions <ref type="bibr" target="#b17">[18]</ref> proposed heavily-augmented U-Net-based networks and treated the problem as segmentation of missing skull fragment. They have shown that data augmentation is crucial to obtain reasonable results <ref type="bibr" target="#b18">[19]</ref>. Other researchers proposed similar encoder-decoder approaches, however, without significant augmentation and thus limited performance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>. Another group of contributions attempted to address not only the raw performance but also the computational efficiency and hardware requirements. One contribution proposed an RNN-based approach using 2-D slices taking into account adjacent slices to enforce the continuity of the segmentation mask <ref type="bibr" target="#b20">[21]</ref>. The contribution by <ref type="bibr">Li et al.</ref> has taken into account the data sparsity and proposed a method for voxel rearrangement in coarse representation using the high-resolution templates <ref type="bibr" target="#b5">[6]</ref>. The method was able to substantially reduce memory usage while maintaining reasonable results. Another contribution by <ref type="bibr">Kroviakov et al.</ref> proposed an approach based on sparse convolutional neural networks <ref type="bibr" target="#b4">[5]</ref> using Minkowski engine <ref type="bibr" target="#b0">[1]</ref>. The method excluded the empty voxels from the input volume and decreased the number of the required convolutions. The work by Yu et al. proposed an approach based on principal component analysis with great generalizability, yet limited raw performance <ref type="bibr" target="#b22">[23]</ref>. Interestingly, methods addressing the computational efficiency could not compete, in terms of the reconstruction quality, with the resource-inefficient methods using dense volumetric representation <ref type="bibr" target="#b6">[7]</ref>.</p><p>The current state-of-the-art solutions, even though they reconstruct the defects accurately, share some common disadvantages. First, they operate in the volumetric domain and require significant computational resources. The GPU memory consumption scales cubically with the volume size. Second, the most successful solutions do not take into account data sparsity. The segmented skulls are binary and occupy only a limited part of the input volume. Thus, using methods dedicated to 3-D multi-channel volumes is resource-inefficient. Third, the final goal of the defect reconstruction is to propose models ready for printing/manufacturing. Working with volumetric representation requires further postprocessing to transfer the reconstructed defect into a manufacturable model.</p><p>Another approach, yet still unexplored, to cranial defect reconstruction is the use of deep networks dedicated to point clouds (PCs) processing. Since the introduction of PointNet <ref type="bibr" target="#b14">[15]</ref> and PointNet++ <ref type="bibr" target="#b15">[16]</ref>, the number of contributions in the area of deep learning for PC processing exploded. Several notable contributions, like PCNet <ref type="bibr" target="#b25">[26]</ref>, PoinTr <ref type="bibr" target="#b23">[24]</ref>, AdaPoinTr <ref type="bibr" target="#b24">[25]</ref>, 3DSGrasp <ref type="bibr" target="#b11">[12]</ref>, MaS <ref type="bibr" target="#b8">[9]</ref>, have been proposed directly to the PC completion task. The goal of the PC completion is to predict a missing part of an incomplete PC.</p><p>The problem of cranial defect reconstruction can be reformulated into PC completion which has several advantages. First, the representation is sparse, and thus requires significantly less memory than the volumetric one. Second, PCs are unordered collections and can be easily splitted and combined, enabling further optimizations. Nevertheless, the current PCs completion methods focus mostly on data representing object surfaces and do not explore large-scale PCs representing solid objects.</p><p>In this work, we reformulate the problem from volumetric segmentation into PC completion. We propose a dedicated method to complete large-scale PCs representing solid objects. We extend the geometric aware transformers <ref type="bibr" target="#b23">[24]</ref> and propose an iterative pipeline to maintain low memory consumption. We compare the proposed approach to the state-of-the-art networks for volumetric segmentation and PC completion. Our approach provides high-quality reconstructions while maintaining computational efficiency and good generalizability into previously unseen cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The input is a 3-D binary volume representing the defective skull. The output is a PC representing the missing skull fragment and (optionally) its meshed and voxelized representation. The processing pipeline consists of: (i) creating the PC from the binary volume, (ii) splitting the PC into a group of coarse PCs, (iii) calculating the missing PC by the geometric aware transformer for each group, (iv) merging the reconstructed coarse PCs, (v) optional voxelization and postprocessing for evaluation. The pipeline is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preprocessing</head><p>The preprocessing starts with converting the binary volume to the PC. The coordinates of the positive voxels are created only from the voxels representing the skull. The PC is normalized to [0-1] range, randomly permuted, and split into N equal groups, where N is calculated based on the number of points in the input PC in a manner that each group contains 32768 randomly sampled input points and outputs 16384 points. Thus, the N depends on the number of positive voxels. The higher the resolution, the more groups are being processed. The number of outputs points is lower than the input points because we assumed that the defect is smaller than the skull.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Network Architecture -Point Cloud Completion Transformer</head><p>We adapt and modify the geometry-aware transformers (PoinTr) <ref type="bibr" target="#b23">[24]</ref>. The PoinTr method was proposed and evaluated on coarse PCs representing object surfaces. The full description of the PoinTr architecture is available in <ref type="bibr" target="#b23">[24]</ref>.</p><p>We modify the network by replacing the FoldingNet <ref type="bibr" target="#b21">[22]</ref> decoder working on 2-D grids with a folding decoder operating on 3-D representation. The original formulation deforms the 2-D grid into the surface of a 3-D object, while the proposed method focuses on solid 3-D models. Moreover, we modify the original k-NN implementation (with quadratic growth of memory consumption with respect to the input size) to an iterative one, to further decrease and stabilize the GPU memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Objective Function</head><p>We train the network using a fully supervised approach where the ground-truth is represented by PCs created from the skull defects. In contrast to other PC completion methods, we employ the Density Aware Chamfer Distance (DACD) <ref type="bibr" target="#b19">[20]</ref>. The objective function enforces the uniform density of the output and handles the unpredictable ratio between the input/output PCs size. We further extend the DACD by calculating the distance between the nearest neighbours for each point and enforcing the distance to be equal. The final objective function is:</p><formula xml:id="formula_0">O(P r , P gt ) = DACD(P r , P gt )+ α S S i=0 k j=0 k l=0 |P r (i)-P r (j)|-|P r (i)-P r (l)|,<label>(1)</label></formula><p>where P r , P gt are the reconstructed and ground-truth PC respectively, S is the number of points in P rec , k is the number of nearest neighbours of point i, α is the weighting parameter. We apply the objective function to all PC ablation studies unless explicitly stated otherwise. The volumetric ablation studies use the soft Dice score. The traditional objective functions like Chamfer Distance (CD) <ref type="bibr" target="#b19">[20]</ref>, Extended Chamfer Distance (ECD) <ref type="bibr" target="#b21">[22]</ref>, or Earth Mover's Distance (EMD) <ref type="bibr" target="#b8">[9]</ref> are not well suited for the discussed application. The CD/ECD provide suboptimal performance for point clouds with uniform density or a substantially different number of samples, tends to collapse, and results in noisy training. The EMD is more stable, however, explicitly assumes bijective mapping (requiring knowledge about the desired number of points) and has high computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Iterative Completion</head><p>The coarse PCs are processed by the network separately. Afterwards, the reconstructed PCs are combined into the final reconstruction. To improve the results, the process may be repeated M times with a different initial PC split and a small Gaussian noise added. The procedure improves the method's performance and closes empty holes in the voxelized representation. The optional multi-step completion is performed only during the inference.</p><p>The iterative completion allows one to significantly reduce the GPU memory usage and the number of network parameters. The PCs are unordered collections and can be easily split and merged. There is no need to process large PCs in one shot, resulting in the linear growth of inference time and almost constant GPU memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Postprocessing</head><p>The reconstructed PCs are converted to mesh and voxelized back to the volumetric representation, mainly for evaluation purposes. The mesh is created by a rolling ball pivoting algorithm using the Open3D library <ref type="bibr" target="#b26">[27]</ref>. The voxelization is also performed using Open3D by the PC renormalization and assigning positive values to voxels containing points in their interior. The voxelized representation is further postprocessed by binary closing and connected component analysis to choose only the largest volume. Then, the overlap area between the reconstructed defect and the defective input is subtracted from the reconstructed defect by logical operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Dataset and Experimental Setup</head><p>We use the SkullBreak and SkullFix datasets <ref type="bibr" target="#b3">[4]</ref> for evaluation. The datasets were used during the AutoImplant I and II challenges and enable comparison to other reconstruction algorithms. The SkullBreak dataset contains 114 highresolution skulls for training and 20 skulls for testing, each with 5 accompanying defects from various classes, resulting in 570 training and 100 testing cases. All volumes in the SkullBreak dataset are 512 × 512 × 512. The SkullFix dataset is represented by 100 training cases mostly located in the back of the skull with a similar appearance, and additional 110 testing cases. The volumes in the SkullFix dataset are 512 × 512 × Z where Z is the number of axial slices. The SkullBreak provides more heterogeneity while the SkullFix is better explored and enables direct comparison to other methods.</p><p>We perform several ablation studies. We check the influence of the input physical spacing on the reconstruction quality, training time, and GPU memory consumption. Moreover, we check the generalizability by measuring the gap between the results on the training and the external testing set for each method. We compare our method to the methods dedicated to PC completion: (i) PCNet <ref type="bibr" target="#b25">[26]</ref>, (ii) PoinTr <ref type="bibr" target="#b23">[24]</ref>, (iii) AdaPoinTr <ref type="bibr" target="#b24">[25]</ref>, as well as to methods dedicated to volumetric defect reconstruction: (i) 3-D VNet, and (ii) 3-D Residual U-Net. Moreover, we compare the reconstruction quality to results reported by other state-of-the-art methods.</p><p>We trained our network separately on the SkullBreak and SkullFix datasets. The results are reported for the external test set containing 100 cases for Skull-Break and 110 cases for the SkullFix datasets, the same as in the methods used for comparison. The models are implemented in PyTorch <ref type="bibr" target="#b12">[13]</ref>, trained using a single RTX GeForce 3090. We augment the input PCs by random permutation, cropping, rotation, and translation. The volumetric ablation studies use random rotation and translation with the same parameters as for the PCs. All the methods were trained until convergence. The hyperparameters are reported in the associated repository <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The comparison in terms of the Dice coefficient (DSC), boundary Dice coefficient (BDSC), 95th percentile of Hausdorff distance (HD95), and Chamfer distance (CD), are shown in Table <ref type="table">1</ref>. Exemplary visualization, presenting both the PC and volumetric outcomes, is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The results of the ablation studies showing the influence of the input size, generalizability, objective function, and the effect of repeating the iterative refinement are presented in Table <ref type="table" target="#tab_1">2</ref>.</p><p>The results present that the quantitative outcomes of the proposed method are comparable to the state-of-the-art methods, however, with significantly lower GPU memory consumption that makes it possible to perform the reconstruction at the highest available resolution. The results are slightly worse when compared to the volumetric methods, however, significantly better than other PC-based approaches.</p><p>Table <ref type="table">1</ref>. Quantitative results on the SkullBreak and SkullFix datasets. The final results are reported for original resolution using the DACD + kNN objective function and 3 iterative refinements (-denotes that results were not reported). The methods used for comparison are reported for the most successful setup (see Table <ref type="table" target="#tab_1">2</ref>). </p><formula xml:id="formula_1">Method SkullBreak SkullFix GPU Mem ↓[GB] DSC ↑ BDSC ↑ HD95 ↓ CD ↓ DSC ↑ BDSC ↑ HD95 ↓ CD ↓ Point Cloud</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>The reconstruction quality of the method is comparable to the volumetric networks, as shown in Table <ref type="table">1</ref>. Meanwhile, the proposed method takes into account the data sparsity, does not require significant computational resources, and scales well with the input size. The proposed method has good generalizability. The gap between the training and testing set is negligible, unlike the volumetric methods that easily overfit and require strong augmentation for practical use. The DACD, as well as the proposed extension, improve the reconstruction quality compared to the CD or ECD by taking into account the uniformity of the expected PC. The original PC completion methods do not scale well with the increase of PC size. The possible reason for this is connected with the noisy kNN graph construction when dealing with large PCs and increasing the number of neighbours is unacceptable from the computational point of view. The proposed method has almost constant memory usage, independent of the input shape, in contrast to both the volumetric methods and PC completion methods without the iterative approach. Interestingly, the proposed method outperforms other methods taking into account the data sparsity. The inference speed is slightly lower than for the volumetric methods (several seconds), however, this application does not require real-time processing and anything in the range of seconds is acceptable. However, the required memory consumption is crucial to ensure that the method can be eventually applied in clinical practice. The disadvantages of the proposed algorithm are connected to long training time, noise at the object boundaries, and holes in the voxelized output. The FoldingNet-based decoder requires a significant number of iterations to converge, thus resulting in training time comparable or even longer than the volumetric methods. Moreover, the voxelization of PCs results in noisy edges and holes that require further morphological postprocessing. What is important, the method has to be extended by another algorithm responsible for converting the reconstruction into implantable implant before being ready to be used in clinical setup.</p><p>In future work, we plan to further reformulate the problem and, similarly to Kroviakov et al. <ref type="bibr" target="#b4">[5]</ref>, use only the skull contours. Since the ultimate goal is to propose models ready for 3-D printing, the interior of the skull defect is not required to create the mesh and STL/OBJ file. Another research direction is connected to the PC augmentation to further increase the network generalizability since it was shown that heavy augmentation is crucial to obtain competitive results <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. Moreover, it is challenging to perform the qualitative evaluation of the context of clinical need and we plan to perform evaluation including clinical experts in the future research.</p><p>To conclude, we proposed a method for cranial defect reconstruction by formulating the problem as the PC completion task. The proposed algorithm achieves comparable results to the best-performing volumetric methods while requiring significantly less computational resources. We plan to further optimize the model by working directly at the skull contour and heavily augmenting the PCs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Visualization of the processing pipeline.</figDesc><graphic coords="3,60,96,403,67,330,52,107,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Exemplary visualization of the reconstructed point clouds/volumes for a case from the SkullBreak dataset. The PCs are shown for the defect only (reconstructed vs ground-truth) for the presentation clarity.</figDesc><graphic coords="6,63,81,383,87,296,92,155,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The ablation studies related to the input size, the objective function, and the number of refinements. The results are reported for the SkullBreak dataset at the original scale (except the CD). The Gen. Gap denotes the difference between the training and testing set in terms of the DSC. Method DSC ↑ BDSC ↑ HD95 ↓ [mm] CD ↓ [mm] GPU Mem ↓ [GB] Gen. Gap ↓ [% DSC]</figDesc><table><row><cell cols="3">Input Size (uniform voxel spacing)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Proposed: original</cell><cell>0.87</cell><cell>0.85</cell><cell>1.91</cell><cell>0.31</cell><cell>∼2.78</cell><cell>4.18</cell></row><row><cell>Proposed: 1 mm</cell><cell>0.83</cell><cell>0.77</cell><cell>2.64</cell><cell>0.46</cell><cell>∼2.69</cell><cell>4.05</cell></row><row><cell>Proposed: 2 mm</cell><cell>0.74</cell><cell>0.71</cell><cell>3.89</cell><cell>0.67</cell><cell>∼2.64</cell><cell>4.57</cell></row><row><cell>Proposed: 4 mm</cell><cell>0.69</cell><cell>0.64</cell><cell>5.12</cell><cell>0.79</cell><cell>∼2.63</cell><cell>3.12</cell></row><row><cell>PCNet: original</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>&gt;24</cell><cell>-</cell></row><row><cell>PCNet: 1 mm</cell><cell>0.37</cell><cell>0.33</cell><cell>10.57</cell><cell>1.76</cell><cell>∼13.22</cell><cell>1.13</cell></row><row><cell>PCNet: 2 mm</cell><cell>0.57</cell><cell>0.53</cell><cell>7.18</cell><cell>1.37</cell><cell>∼5.37</cell><cell>1.33</cell></row><row><cell>PCNet: 4 mm</cell><cell>0.61</cell><cell>0.58</cell><cell>5.77</cell><cell>1.18</cell><cell>∼2.37</cell><cell>3.07</cell></row><row><cell>PoinTr: original</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>&gt;24</cell><cell>-</cell></row><row><cell>PoinTr: 1 mm</cell><cell>0.58</cell><cell>0.55</cell><cell>6.82</cell><cell>1.39</cell><cell>∼21.41</cell><cell>1.89</cell></row><row><cell>PoinTr: 2 mm</cell><cell>0.65</cell><cell>0.64</cell><cell>5.28</cell><cell>0.94</cell><cell>∼6.48</cell><cell>2.19</cell></row><row><cell>PoinTr: 4 mm</cell><cell>0.67</cell><cell>0.66</cell><cell>5.17</cell><cell>0.82</cell><cell>∼3.11</cell><cell>3.98</cell></row><row><cell cols="2">3-D RUNet: original -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>&gt;24</cell><cell>-</cell></row><row><cell>3-D RUNet: 1 mm</cell><cell>0.89</cell><cell>0.91</cell><cell>1.79</cell><cell>0.18</cell><cell>22.47</cell><cell>10.11</cell></row><row><cell>3-D RUNet: 2 mm</cell><cell>0.85</cell><cell>0.85</cell><cell>2.09</cell><cell>0.25</cell><cell>7.84</cell><cell>14.51</cell></row><row><cell>3-D RUNet: 4 mm</cell><cell>0.76</cell><cell>0.77</cell><cell>2.89</cell><cell>0.63</cell><cell>3.78</cell><cell>17.48</cell></row><row><cell cols="5">Objective Function (proposed method, original size, 3 iters)</cell><cell></cell><cell></cell></row><row><cell>DACD + kNN</cell><cell>0.87</cell><cell>0.85</cell><cell>1.91</cell><cell>0.31</cell><cell>∼2.78</cell><cell>4.18</cell></row><row><cell>DACD</cell><cell>0.85</cell><cell>0.81</cell><cell>2.78</cell><cell>0.42</cell><cell>∼2.72</cell><cell>4.22</cell></row><row><cell>ECD</cell><cell>0.75</cell><cell>0.71</cell><cell>4.11</cell><cell>0.68</cell><cell>∼2.72</cell><cell>3.99</cell></row><row><cell>CD</cell><cell>0.83</cell><cell>0.78</cell><cell>2.98</cell><cell>0.28</cell><cell>∼2.72</cell><cell>4.58</cell></row><row><cell cols="5">No. Refinements (proposed method, original size, DACD + kNN)</cell><cell></cell><cell></cell></row><row><cell>1 iters</cell><cell>0.85</cell><cell>0.81</cell><cell>2.51</cell><cell>0.41</cell><cell>∼2.78</cell><cell>4.51</cell></row><row><cell>2 iters</cell><cell>0.87</cell><cell>0.83</cell><cell>1.98</cell><cell>0.32</cell><cell>∼2.78</cell><cell>4.18</cell></row><row><cell>3 iters</cell><cell>0.87</cell><cell>0.85</cell><cell>1.91</cell><cell>0.31</cell><cell>∼2.78</cell><cell>4.18</cell></row><row><cell>4 iters</cell><cell>0.87</cell><cell>0.85</cell><cell>1.90</cell><cell>0.30</cell><cell>∼2.78</cell><cell>4.18</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The project was funded by <rs type="funder">The National Centre for Research and Development, Poland</rs> under Lider Grant no: <rs type="grantNumber">LIDER13/0038/2022</rs> (DeepImplant). We gratefully acknowledge Polish <rs type="grantName">HPC infrastructure PLGrid support</rs> within computational grant no. <rs type="grantNumber">PLG/2023/016239</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BvfYjMT">
					<idno type="grant-number">LIDER13/0038/2022</idno>
					<orgName type="grant-name">HPC infrastructure PLGrid support</orgName>
				</org>
				<org type="funding" xml:id="_9wwc78k">
					<idno type="grant-number">PLG/2023/016239</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">4D spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3070" to="3079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Executive Summary of the Global Neurosurgery Initiative at the Program in Global Surgery and Social Change</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Dewan</surname></persName>
		</author>
		<idno type="DOI">10.3171/2017.11.JNS171500</idno>
		<ptr target="https://doi.org/10.3171/2017.11.JNS171500" />
	</analytic>
	<monogr>
		<title level="j">J. Neurosurg</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1055" to="1064" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Global neurosurgery: the current capacity and deficit in the provision of essential neurosurgical care</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning using augmentation via registration: 1st place solution to the AutoImplant 2020 challenge</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Aizenberg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-64327-0_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-64327-0_6" />
	</analytic>
	<monogr>
		<title level="m">AutoImplant 2020</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Egger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12439</biblScope>
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SkullBreak/SkullFix -dataset for automatic cranial implant design and a benchmark for volumetric shape learning tasks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kodym</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Brief</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparse convolutional neural network for skull reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kroviakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Egger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-92652-6_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-92652-6_7" />
	</analytic>
	<monogr>
		<title level="m">AutoImplant 2021</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Egger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13123</biblScope>
			<biblScope unit="page" from="80" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to rearrange voxels in binary segmentation masks for smooth manifold triangulation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pepe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gsaxner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Egger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-92652-6_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-92652-6_5" />
	</analytic>
	<monogr>
		<title level="m">AutoImplant 2021</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Egger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13123</biblScope>
			<biblScope unit="page" from="45" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<idno type="DOI">10.1007/978-3-030-92652-6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-92652-6" />
	</analytic>
	<monogr>
		<title level="m">AutoImplant 2021</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Egger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13123</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">AutoImplant 2020-first MICCAI challenge on automatic cranial implant design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2329" to="2342" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Morphing and sampling network for dense point cloud completion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.6827</idno>
		<ptr target="https://doi.org/10.1609/aaai.v34i07.6827" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11596" to="11603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A U-Net based system for cranial implant design with preprocessing and learned implant filtering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mahdi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-92652-6_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-92652-6_6" />
	</analytic>
	<monogr>
		<title level="m">AutoImplant 2021</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Egger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13123</biblScope>
			<biblScope unit="page" from="63" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Custom implant design for large cranial defects</title>
		<author>
			<persName><forename type="first">F</forename><surname>Marreiros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2217" to="2230" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">3DSGrasp: 3D shape-completion for robotic grasp</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mohammadi</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2301.00866" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cranial implant design using V-Net based region of interest reconstruction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sindhura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K S S</forename><surname>Gorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gorthi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-92652-6_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-92652-6_10" />
	</analytic>
	<monogr>
		<title level="m">AutoImplant 2021</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Egger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13123</biblScope>
			<biblScope unit="page" from="116" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">PointNet: deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1612.00593" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">PointNet++: deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.02413" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Wodzinski</surname></persName>
		</author>
		<ptr target="https://github.com/MWod/DeepImplant_MICCAI_2023" />
		<title level="m">The associated repository</title>
		<imprint>
			<date type="published" when="2023-07">2023. July 2023</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving the automatic cranial implant design in cranioplasty by linking different datasets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wodzinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hemmerling</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-92652-6_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-92652-6_4" />
	</analytic>
	<monogr>
		<title level="m">AutoImplant 2021</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Egger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13123</biblScope>
			<biblScope unit="page" from="29" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning-based framework for automatic cranial defect reconstruction and implant modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wodzinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">226</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Density-aware chamfer distance as a comprehensive metric for point cloud completion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2111.12702" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cranial implant prediction by learning an ensemble of slice-based skull completion networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-92652-6_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-92652-6_8" />
	</analytic>
	<monogr>
		<title level="m">AutoImplant 2021</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Egger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13123</biblScope>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">FoldingNet: point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1712.07262" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PCA-skull: 3D skull shape modelling using principal component analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Egger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-92652-6_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-92652-6_9" />
	</analytic>
	<monogr>
		<title level="m">AutoImplant 2021</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Egger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13123</biblScope>
			<biblScope unit="page" from="105" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">PoinTr: diverse point cloud completion with geometry-aware transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2108.08839" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">AdaPoinTr: diverse point cloud completion with adaptive geometryaware transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2301.04545" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">PCN: point completion network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1808.00671" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Open3D: a modern library for 3D data processing</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>CoRR abs/1801.09847</idno>
		<ptr target="http://arxiv.org/abs/1801.09847" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
