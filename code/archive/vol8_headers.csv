Paper Title,Header Number,Header Title,Text
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,1,Introduction,"Mitochondria are membrane-bound organelles that generate the primary energy required to power the cell activities, thereby crucial for metabolism. Mitochondrial dysfunction, which occurs when mitochondria are not functioning properly has been witnessed as a major factor in numerous diseases, including noncommunicable chronic diseases (e.g, cardiovascular and cancer), metabolic (e.g, obesity) and neurodegenerative (e.g, Alzheimer and Parkinson) disorders  Earlier works on mitochondria segmentation employ standard image processing and machine learning methods  When designing a attention-based framework for 3D mitochondria instance segmentation, a straightforward way is to compute joint spatio-temporal selfattention where all pairwise interactions are modelled between all spatiotemporal tokens. However, such a joint spatio-temporal attention computation is computation and memory intensive as the number of tokens increases linearly with the number of input slices in the volume. In this work, we look into an alternative way to compute spatio-temporal attention that captures long-range global contextual relationships without significantly increasing the computational complexity. Our contributions are as follows: -We propose a hybrid CNN-transformers based encoder-decoder framework, named STT-UNET. The focus of our design is the introduction of a split spatio-temporal attention (SST) module that captures long-range dependencies within the cubic volume of human and rat mitochondria samples. The SST module independently computes spatial and temporal self-attentions in parallel, which are then later fused through a deformable convolution. -To accurately delineate the region of mitochondria instances from the cluttered background, we further introduce a semantic foreground-background (FG-BG) adversarial loss during the training that aids in learning improved instance-level features. -We conduct experiments on three commonly used benchmarks: Lucchi  MitoEM-R  Our STT-UNET approach achieves superior segmentation performance by accurately segmenting 16% more cell instances in these examples, compared to Res-UNET-R. segmentation performance on all three datasets. On Lucchi test set, our STT-UNET outperforms the recent  Figure "
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,2,Related Work,"Most recent approaches for 3D mitochondria instance segmentation utilize convolution based designs within the ""U-shaped"" 3D encoder-decoder architecture. In such an architecture, the encoder aims to generate a low-dimensional representation of the 3D data by gradually performing the downsampling of the extracted features. On the other hand, the decoder performs upsampling of these extracted feature representations to the input resolution for segmentation prediction. Although such a CNN-based designs  Inspired by ViTs  3 Method"
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,3.1,Baseline Framework,"We base our approach on the recent Res-UNET  having skip connections between first and third layers. The decoder outputs semantic mask and instance boundary, which are then post-processed using connected component labelling to generate final instance masks. We refer to  Limitations: As discussed above, the recent Res-UNET approach utilizes 3D convolutions to handle the volumetric input data. However, 3D convolutions are designed to encode short-range spatio-temporal feature information and struggle to model global contextual dependencies that extend beyond the designated receptive field. In contrast, the self-attention mechanism within the vision transformers possesses the capabilities to effectively encode both local and global long-range dependencies by directly performing a comparison of feature activations at all the space-time locations. In this way, self-attention mechanism goes much beyond the receptive field of the conventional convolutional filters. While self-attention has been shown to be beneficial when combined with convolutional layers for different medical imaging tasks, to the best of our knowledge, no previous attempt to design spatio-temporal self-attention as an exclusive building block for the problem of 3D mitochondria instance segmentation exists in literature. Next, we present our approach that effectively utilizes an efficient spatiotemporal attention mechanism for 3D mitochondria instance segmentation.  with split spatio-temporal attention and an instance segmentation block. The denoising module alleviates the segmentation faults caused by anomalies in the EM images, as in the baseline. The denoising is performed by convolving the current frame with two adjacent frames using predicted kernels, thereby generating the resultant frame by adding the convolution outputs. The resulting denoised output is then processed by our transformer based encoder-decoder with split spatio-temporal attention to generate the semantic masks. Consequently, these semantic masks are post-processed by an instance segmentation module using a connected component labelling scheme, thereby generating the final instancelevel segmentation output prediction. To further enhance the semantic segmentation quality with cluttered background we introduced semantic adversarial loss which leads to improved semantic segmentation in noisy background. Split Spatio-Temporal Attention based Encoder-Decoder: Our STT-UNET framework comprises four encoder and three decoder layers. Within each layer, we introduce a split spatio-temporal attention-based (SST) module, Fig.  where, X s is spatial attention map, X t is temporal attention map and d k is dimension of Q s and K s . To fuse spatial and temporal attention maps, X s and X t , we employ deformable convolution. The deformable convolution generates offsets according to temporal attention map X t and by using these offsets the spatial attention map X s is aligned. The deformable fusion is given as, where, C is no of channels, X is spatially aligned attention map with respect to X t . W is the weight matrix of kernels, X s is spatial attention map, k 0 is starting position of kernel, k n is enumerating along all the positions in kernel size of R and ΔK n is the offset sampled from temporal attention map X t . We empirically observe that fusing spatial and temporal features through a deformable convolution, instead of concatenation through a conv. layer or addition, leads to better performance. The resulting spatio-temporal features of decoder are then input to instance segmentation block to generate final instance masks, as in baseline. Semantic FG-BG Adversarial Loss: As discussed earlier, a common challenge in mitochondria instance segmentation is to accurately delineate the region of mitochondria instances from the cluttered background. To address this, we introduce a semantic foreground-background (FG-BG) adversarial loss during the training to enhance the FG-BG separability. Here, we introduce the auxiliary discriminator network D with two layers of 3D convolutions with stride 2 during the training as shown in Fig.  Consequently, the overall loss for training is: Where, L BCE is BCE loss, λ = 0.5 and L fg-bg is semantic adversarial loss. "
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,4,Experiments,Dataset: We evaluate our approach on three datasets: MitoEM-R  Implementation Details: We implement our approach using Pytorch1.9 
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,4.1,Results,State-of-the-Art Comparison: Table  Our approach achieves promising segmentation results despite the noise in the input samples.    Ablation Study: Table 
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,5,Conclusion,"We propose a hybrid CNN-transformers based encoder-decoder approach for 3D mitochorndia instance segmentation. We introduce a split spatio-temporal attention (SST) module to capture long-range dependencies within the cubic volume of human and rat mitochondria samples. The SST module computes spatial and temporal attention in parallel, which are later fused. Further, we introduce a semantic adversarial loss for better delineation of mitochondria instances from background. Experiments on three datasets demonstrate the effectiveness of our approach, leading to state-of-the-art segmentation performance."
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,,3. 2,
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,,Fig. 2 .,
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,,Fig. 3 .,
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,,,
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,,,
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,,Table 1 .,
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,,Table 2 .,
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,,Table 2,
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,,Table 3 .,
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,,Table 4 .,
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,,Table 5 .,
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,1,Introduction,"Chemical exchange saturation transfer (CEST) is a novel metabolic magnetic resonance imaging (MRI) method that allows to detect molecules in tissue based on chemical exchange of their mobile protons with water protons  Hunger et al. shown that supervised learning can be used to generate B 1robust CEST maps, coining the DeepCEST approach  In this work, we developed a conditional autoencoder (CAE) "
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,2,Methods,"Data Measurements. CEST imaging was performed in seven subjects, including two glioblastoma patients, after written informed consent was obtained to investigate the dependence of CEST effects on B 1 in brain tissue. The local ethics committee approved the study. All volunteers were measured at three B 1 field strengths 0.72 μT, 1.0 μT, and 1.5 μT. A method as described by Mennecke et al.  Conditional Autoencoder. We developed a conditional autoencoder (CAE) to solve the B 1 inhomogeneity problem, which is essential for the generation of metabolic CEST contrast maps at 7T. The left part of Fig.  Physics-Informed Autoencoder. The Lorentzian model and its B 1dispersion can be derived from the underlying spin physics described by the Bloch-McConnell equation system  where L denotes the Lorentz function. The direct saturation pool (water) was defined as ( The remaining other four pools were defined as ) 2 , i ∈ amide, amine, rN OE, ssM T . ( The right part of Fig.  Bound Loss. The peak positions δ i and widths τ i of the pools had to be within certain bounds so that certain neurons in the latent space layer of PIAE would not be exchanged and provide the same pool parameters for all samples. We developed a simple cost function along the lines of the hinge loss  The bound loss increases linearly as the output of the latent space neurons of PIAE exceeds or recede from the boundaries. The lower and upper limits for positions and widths are given in Table  Training and Evaluation. Four healthy volunteers formed the training and validation sets. The test set consisted of the two tumor patients and one healthy subject. To ensure that the outcomes were exclusively based on the CEST-spectrum and not influenced by spatial position, the training was carried out voxel-by-voxel. Consequently, there were approximately one million CESTspectra for the training process. CAE was first trained with MSE loss. In this step, the CAE encoder was fed with the CEST-spectrum of a specific B 1 saturation amplitude, and it generated two CEST-spectra, one for the input B 1 saturation level and the other for the B 1 level injected into the latent space (cf. Fig.  PIAE, on the other hand, was trained with a combination of MSE loss and bound loss. The PIAE loss was described as follows for evaluation we input the uncorrected CEST-spectrum acquired at 1μT and generated corrected CEST-spectra at B 1 0.5, 0.72, 1.0, 1.3, 1.5 μT. PIAE encoder yielded the amplitudes of 5-pool for B 1 corrected CEST-spectrum. Its decoder reconstructed the B 1 B 0 fitted CEST-spectrum. The B 0 correction simply refers to the shift of the position of the water peak to 0 ppm. CEST Quantification. The multi-B 1 CEST-spectra allow quantification of CEST effects (amide, rNOE, amine)  where f i , k i , and r 2i express the concentrations, exchange rates, and relaxation rates of the pools. Z ref defines the sum of all 5 distributions at the resonance frequency of the specific pool in B 1 B 0 corrected CEST-spectrum and w 1 is the frequency of the oscillating field. The amplitudes of CEST contrasts in the Lorentzian function have the B 1 dispersion function given by the labeling efficiency α (Eq. 7). The exchange rate occurs here separately from the concentration, which allows their quantification via the B 1 dispersion. Concentration and exchange rate were fitted as a product and denoted as Z 1 (quantified maps), and k(k+r 2 ) was also fitted with the single term Z 2 using trust-region reflective least squares "
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,3,Results,The comparison of PICAE with the conventional method 
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,4,Discussion,"In this work, we analyzed the use of an autoencoder approach to generate B 1robust CEST contrast maps at arbitrary B 1 levels, which requires multiple acquisitions in conventional methods  The bound loss ensured that the positions of the pools were not interchanged. Quantification was still performed using the nonlinear least squares fit according to Eq. 7. The main reason for this was that it does not affect the acquisition time and it is affected by the Z ref . The training was performed voxel-wise to ensure that the results are based only on the CEST-spectrum. This also results in about 1 million CEST-spectra for the training. Figure "
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,5,Conclusion,"In this work, we propose a PICAE method for evaluating 7T-CEST MRI that accounts for B 1 inhomogeneity in the input and predicts homogeneous metabolic CEST contrasts at arbitrary B 1 levels. The proposed generative and interpretable method enables (i) a reduction of scan time by at least 50%, (ii) the generation of reliable 7T-CEST contrast maps robust to B 1 inhomogeneity at multiple B 1 levels, (iii) a clear physical interpretation of the B 1 correction of the CESTspectra and the fitting of the Lorentzian model to it, and (iv) the quantification of the CEST contrast maps."
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,,Fig. 1 .,
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,,Fig. 2 .,
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,,Fig. 3 .,
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,,Fig. 4 .,
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,,Table 1 .,
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_44.
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,1,Introduction,"In recent years, the overuse and misuse of antibiotics have led to an increase in the rate of bacterial antibiotic resistance worldwide  In this paper, we take Pseudomonas aeruginosa (PA) as the research object and observe the difference in shape and distribution of bacterial aggregates formed by sensitive and multi-drug resistant bacteria through fluorescent images, so we want to use image recognition technology to distinguish these two types of bacteria for the purpose of rapid prediction of antibiotic susceptibility. However, we recognize that this classification task presents several challenges (as shown in Fig.  The above studies show that two deep learning frameworks, CNN and Transformer, are effective in microscopy image classification tasks. CNN is good at extracting local features, but its receptive field is limited by the size of the convolution kernel and cannot effectively capture the global information in the image. Meanwhile, in visual Transformer, its self-attention module is good at capturing feature dependencies over long distances, but ignores local feature information. However, these two kinds of feature information are very important for the classification of microscope images with complex features. To tackle this issue, this paper builds a hybrid model that maximizes the advantages of CNN and Transformer, thus enhancing the feature representation of the network. To achieve the complementary advantages of these two techniques, we propose a parallel dual-branch network named PAS-Net, specifically designed to enable rapid prediction of bacterial antibiotic susceptibility. The main contributions of this study are as follows: 1) We develop a parallel dual-branch classification network to realize the interactive learning of features throughout the whole process through feature interaction unit (FIU), which can better integrate local features of CNN branch (C-branch) and global representations of Transformer branch (T-branch). 2) We propose a more efficient hierarchical multi-head self-attention (HMSA) module, which utilizes a local-to-global attention mechanism to simulate the global information of an image, while effectively reducing the computational costs and memory consumption. To the best of our knowledge, this study represents the first attempt to use deep learning techniques to realize rapid AST based on PA fluorescence images, which provides a new perspective for predicting bacterial antibiotic susceptibility.  Feature dimension mismatch exists between feature map from C-branch and vector sequence from T-branch. Therefore, our network use FIU as a bridge to effectively combine the local features and the global representation in an interactive manner to eliminate the misalignment between the two features, as shown in Fig. "
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,1.1,Method,
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,,CNN→Transformer:,"The feature map is first aligned with the dimensions of the patch embedding by 1 × 1 convolution. Then, the feature resolution is adjusted using the downsampling module to complete the alignment of the spatial dimensions. Finally, the feature maps are summed with the patch embedding of the T-branch. Transformer→CNN: After going through the HMSA module and FFN, the patch embedding is fed back from the T-branch to the C-branch. An up-sampling module needs to be used first for the patch embedding to align the spatial scales. The patch embedding is then aligned to the number of channels of the feature map by 1 × 1 convolution, and added to the feature map of the C-branch."
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,1.3,Hierarchical Multi-head Self-attention,"Figure  respectively, where W Q , W K and W V are three learnable weight matrices with shared parameters that are updated together with the model parameters during training. After that, we compute local attention A 0 within each small grid using the self-attention mechanism, which can be defined as: Then Eq. (  ( The original MSA module computes attention map over the entire input feature, and its computational complexity scale quadratically with spatial dimension N, which can be calculated as: ( In contrast, our HMSA module computes attention map in a hierarchical manner so that A 0 and A 1 are computed within small G × G grids. The computational complexity of HMSA is With this approach, only a limited number of image blocks need to be processed in each step, thus significantly reducing the computational effort of the module from O(N 2 ) to O(NG 2 ), where G 2 is much smaller than N. For example, the size of the input image is 224 × 224, if the patch is divided according to the size of 4 × 4, the division will get (224 / 4) 2 = 3136 patches, i.e., N = 3136. However, we set G to 4, so the computational complexity of the HMSA module is greatly reduced. "
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,2,Experiments and Results,
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,2.1,Experimental Setup,"The fluorescent images of PA come from a local medical school. We screen out 12 multidrug resistant strains and 11 sensitive strains. Our dataset has 2625 fluorescent images of PA, 1233 images of sensitive PA and 1392 images of MDRPA. We randomly divide the data into a training set and a test set in a ratio of 9:1. To better train the network model and prevent overfitting, we perform five data enhancement operations on each image, including horizontal flip, vertical flip and rotation at different angles (90°, 180°, 270°). Finally, our data volume is expanded to 15,750 images, including 14,178 training images and 1,572 test images. To achieve comprehensive and objective assessment of the classification performance of the proposed method, we select eight classification evaluation metrics, including accuracy (Acc), precision (Pre), recall, specificity (Spec), F1-score (F1), Kappa, area under the receiver operating characteristic (ROC) curve (AUC). All experiments are implemented by configuring the PyTorch framework on NVIDIA GTX 2080Ti GPU with 11 GB of memory."
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,2.2,Results,"In this paper, we adopt Conformer  In order to evaluate the classification performance of the proposed method, we choose ten state-of-the-art image classification methods for comparison, including 5 CNN networks: ResNet50  To further analyze and compare the computational complexity of different methods, we compare the number of model parameters (#Param) and the number of floating-point operations per second (FLOPs). In general, the higher the number of parameters and operations, the higher the performance of the model, but at the same time, the greater the computational and storage overhead. It can be seen that the accuracy of ViT is 5% lower than that of ResNet50, but its model complexity is about three times higher. The number of model parameters of PVT-M is similar to that of our PAS-Net, but the accuracy is much worse. The number of parameters of our proposed PAS-Net is 43.4M and FLOPs is 23.37G, indicating that the network achieves a good balance between the number of parameters, FLOPs, accuracy and classification consistency.  To verify the interpretability of the proposed PAS-Net and understand its classification effect more intuitively and effectively, we visualize the results using Grad-CAM, as shown in Fig.  We also use the t-SNE dimensionality reduction algorithm to map the feature vectors learned from the last feature extraction layer of different networks onto a twodimensional plane, as shown in Fig. "
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,2.3,Robustness to HEp-2 Dataset,"To further verify the effectiveness of PAS-Net in fluorescent image classification tasks, we also apply our method to two HEp-2 cell public datasets, ICPR 2012 and I3A Task1. ICPR 2012 dataset uses average class accuracy (ACA) as the evaluation metric, which is the same concept as the accuracy mentioned above, while I3A Task1 uses mean class accuracy (MCA). We select four deep learning techniques for classification of HEp-2 cells for comparison, respectively, and the results are shown in Table "
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,3,Conclusion,"In this paper, we develop a PAS-Net framework for rapid prediction of antibiotic susceptibility from bacterial fluorescence images only. PAS-Net is a parallel dual-branch feature interaction network. FIU is a connecting bridge to align and fuse the local features from the C-branch and the global representation from the T-branch, which enhances the feature representation ability of the network. We design a HMSA module with less computational overhead to improve the computational efficiency of the model. The experimental results demonstrate that our method is feasible and effective in PA fluorescence image classification task, and can assist clinicians in determining bacterial antibiotic susceptibility."
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,,Fig. 1 .,
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,,Fig. 2 .,
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,,Figure 2 Fig. 3 .,
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,,Fig. 4 .,
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,,Fig. 5 .,
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,,Fig. 6 .,
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,,Table 1 .,
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,,Table 2 .,
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,,35 95.81 ± 1.59 96.80 ± 2.36 95.18 ± 1.91 96.28 ± 1.30 92.04,
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,,Table 3 :,
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,1,Introduction,"Understanding the relationship between brain functional connectivity and structural connectivity is a key issue in studying the working mechanisms of the brain  Despite the great success of the above methods, they are far from satisfactory for studying the relationship between brain functional and structural connectivity. Connections in brain regions make up networks, and it is crucial to identify key nodes and connections. Therefore, there are currently at least two difficulties. On the one hand, key brain regions, act as hubs for information transmission in the brain network, have not been completely identified in the joint analysis of brain functional and structural profiles. On the other hand, it has not been clearly studied for the connectome skeleton of brain, which plays a key role in both functional and structural connections of brain. To overcome the above limitations, we propose a transformer-based graph selfsupervised graph reconstruction framework. It can obtain the key connectome regions and skeleton of brain by combining brain function and structure. The method has two main characteristics. For one thing, graph neural networks (GNN)  Experimental results demonstrate the effectiveness of proposed method. First, we obtained low loss values, which indicated that the model achieved the reconstruction task. Second, we obtained the contribution scores of brain ROIs. ROIs with high scores play a role in the transmission of information in the network, are regarded as key connectome regions. Finally, we obtained the connectome skeleton of the brain, which expresses the most key connections of the brain both in functional and structural networks."
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,2,Method,
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,2.1,Overview,"The pipeline of the transformer-based self-supervised graph reconstruction (TSGR) framework proposed in this paper is shown in Fig.  The brain is represented as a graph and used as input for the ScorePool-AE module, then the reconstructed graph and contribution scores of the nodes can be obtained. The framework consists of three main parts, i.e., graph generation, ScorePool-AE module, and reconstruction target. In the graph generation part, we adopt Destrieux Atlas to initialize the brain surface into 148 ROIs and use them as nodes of the graph. Based on the ROIs, functional and structural information are utilized to generate the node features and edge features of the graph, respectively. In the ScorePool-AE module, we design ScorePool to get the contribution score of each node in the graph. In the reconstruction target part, the mean square error (MSE) between the node features of the input graph and the reconstructed graph is applied as the loss function. "
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,2.2,Data and Preprocess,"We used the HCP 900 dataset and randomly selected 98 subjects from it. T1-weighted MRI data were used to reconstruct the brain cortical surface, dMRI were utilized to reconstruct the fiber bundles from white matter, resting-state fMRI (rs-fMRI) and task fMRI showed the functional changes of the brain. Among them, task fMRI data contains a total of seven tasks, which are EMOTION, GAMBLING, LANGUAGE, MOTOR, RELATION, SOCIAL and WM. Standard Freesurfer pipeline including tissue-segmentation and white matter surface (inner surface) reconstruction  To jointly use these three modalities, we aligned them into the same space. A linear image registration method (FLIRT) "
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,2.3,TSGR Framework,"Graph Generation. We define the brain as an undirected graph G = {V , E}. V = {v i |i ∈ 1, 2, . . . , n} represents the set of nodes of the graph. F ∈ R n×D represents the feature matrix of graph nodes. E ∈ R n×n represents the adjacency matrix of the graph. Graph Nodes with Generated Features. The brain surface is divided into 148 ROIs and used as nodes of the graph. fMRI signals are selected to express node features. We compute the Pearson correlation coefficients of the signals among all ROIs to obtain the functional similarity matrix and use it as the feature matrix F of the graph nodes. The length of each node feature D is 148. Graph Edge. For each pair of ROIs, we calculate the total number of fibers directly connected of them and then divide it by the geometric mean of their areas, thus obtaining the structural connectivity matrix S. We set the threshold t s for S to make it sparse and do the binarization to obtain E. E i,j = 1 means that nodes i and j are connected, otherwise E i,j = 0."
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,,ScorePool-AE Module.,"In ScorePool-AE module, we adopt the encoder-decoder structure to implement the graph reconstruction task. The encoder is applied to extract node representations of the graph, then the ScorePool is used to obtain the contribution scores of nodes, and finally the decoder is used to reconstruct the node features of the graph. The encoder is based on the Ugformer  (1) where H (k) is the node representations of the graph at the k-th layer of UGformer. V is the set of all nodes of the graph. Inspired by the pooling operation of the GNN  where denotes the element-wise product and X denotes the input of the decoder. Reconstruction Target. Our TSGR framework reconstructs the graph by predicting the nodes features of the graph. The loss function is MSE between the feature matrix F of reconstructed graph nodes and the feature matrix F of input graph nodes."
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,2.4,Analyzing Brain Key Connectome ROIs and Hierarchical Networks,"In this work, we propose a method that identifies key connectome ROIs and can be applied to the hierarchical analysis of brain networks. First, we obtained the average contribution scores of brain ROIs for all individuals. Second, we verify whether the ROIs with high scores are key connectome ROIs in the functional and structural networks. The functional network is obtained by averaging F over all individuals and setting a threshold. The structural network is the same operation done for S. We use three network centrality metrics, i.e., degree centrality, closeness centrality and PageRank centrality, to measure key connectome ROIs. In addition, we calculate the number of all ROIs participating in the functional network, where the functional networks are obtained by dictionary learning "
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,2.5,Exploring the Connectome Skeleton of Brain ROIs,"This study explores the connectome skeleton of the brain based on important ROIs. The connectome skeleton plays the role in common connections in the brain network and is a core connectivity pattern. First, we obtain the key functional connections of eight (one resting-state and seven tasks) functional networks. Specifically, we calculate the eight functional network connections consisting of Scale-1 and Scale-2 ROIs, and a connection is selected if it exists simultaneously in six and more functional networks. Then, we obtain the intersection between the key functional connections and the structural network connection as the connectome skeleton. Finally, we analyze the connectome skeleton, namely, counting the strength of connectivity between brain regions and the length of fibers in the connectome skeleton.  "
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,3,Experiments and Results,
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,3.1,Experimental Performance Experimental,
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,3.2,Analysis of Key Brain ROIs and Network Hierarchy,"For each set of experiments, the contribution scores of brain ROIs are obtained by averaging the scores of all individuals, as shown in Fig.  In addition, we also investigate the networks composed among the scales. As shown in Fig.  We also analyze the global efficiency of the network and the coupling between the functional and structural networks, as shown in Fig. "
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,3.3,Analysis of Connectome Skeleton in Brain Networks,"We obtain the key connection for the functional network of all eight functional networks, where the functional network consists of connections within the ROIs including Scale-1 and Scale-2. Then we combine the key functional connections and the structural connections to obtain the connectome skeleton, as shown in Fig. "
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,4,Conclusion,"We propose a new transformer-based self-supervised graph reconstruction framework to identify key brain connectome ROIs and connectome skeleton in the joint analysis of brain functional connectivity and structural connectivity. The main contribution of the approach is the use of GNN to fuse brain function and structure and the use of a self-supervised model to identify the ROIs that important for graph reconstruction. The experimental results validate the effectiveness of the method. First, we obtain the scores of ROIs. Second, we verify that the ROIs with high scores are key connectome ROIs. Finally, we obtain the connectome skeleton of the brain. It provides a new approach for analyzing the relationship between functional and structural connectivity, and further analysis will be settled in future work."
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,,Fig. 1 .,
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,,,
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,,Fig. 2 .,
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,,Fig. 3 .,
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,,Fig. 4 .,
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,,Fig. 5 .,
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,1,Introduction,"The synthesis of medical images has great potential in aiding tasks like improving image quality, imputing missing modalities "
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,2,Methodology,"We first review the basic DPM framework for data generation (Sect. 2.1). Then, we introduce our efficient strategy for generating 3D MRI slices (Sect. 2.2) and finally describe the neural architecture of cDPMs (Sect. 2.3)."
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,2.1,Diffusion Probabilistic Model,"The Diffusion Probabilistic Model (DPM)  Forward Diffusion Process (FDP). Let real data X 0 ∼ q sampled from the (real data) distribution q be the input to the FDP. FDP then simulates the diffusion process that turns X 0 after T perturbations into Gaussian noise X T ∼ N (0, I ), where N is the Gaussian distribution with zero mean and the variance being the identity matrix I. This process is formulated as a Markov chain, whose transition kernel q(X t |X t-1 ) at time step t ∈ {0, . . . , T } is defined as The weight β t ∈ (0, 1) is changed so that the chain gradually enforces drift, i.e., adds Gaussian noise to the data. Let α t := 1β t and ᾱt := Given this closed-form solution, we can sample X t at any arbitrary time step t without needing to iterate through the entire Markov chain. Reverse Diffusion Process (RDP). The RDP aims to generate realistic data from random noise X T by approximating the posterior distribution p(X t-1 |X t ). It does so by going through the entire Markov chain from time step T to 0, i.e., Defining the conditional distribution p θ (X t-1 |X t ) := N (X t-1 ; μ θ (X t , t), Σ) with fixed variance Σ, then (according to  with θ (•) being the estimate of a neural network defined by parameters θ. θ minimizes the reconstructing loss defined by the following expected value E X0∼q,t∈[0,...,T], ∼N (0,I) ||θ (X t , t)|| 2  2 , where || • || 2 is the L2 norm, and X t is inferred from Eq. ( "
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,2.2,Conditional Generation with DPM (cDPM),"To synthetically create high-resolution 3D MRI, we propose an efficient cDPM model that learns the interdependencies between 2D slices of an MRI so that it can generate slices based on another set of already synthesized ones (see Fig.  Specifically, given an MRI X ∈ R D×H×W , we randomly sample two sets of slice indexes: the condition set C and the target set P. Let len(•) be the number of slices in a set, then the 'condition' slices are defined as X C ∈ R len(C)×H×W and the 'target' slices as X P ∈ R len(P)×H×W with len(P) ≥ 1. Confining the FDP of Sect. 2.1 just to the target X P , the RDP now aims to reconstruct X P t for each time t = T, T -1, . . . , 0 starting from random noise at t = T and conditioned on X C . Let X t be the subvolume consisting of X P t and X C , then the joint distribution of the Markov chain defined by Eq. (3) now reads Observe that Eq. (  To estimate μ θ ( X t , t) as described in Eq. (  (6) Fig.  As the neural network can now be trained on many different (arbitrary) slice combinations (defined by C and P), the cDPM only requires a relatively small number of MRIs for training. Furthermore, it will learn short-and longrange dependencies across slices as the spatial distance between slices from C and P varies. Learning these dependencies (after being trained for a sufficiently large number of iterations) enables cDPMs to produce 2D slices that, when put together, result in realistic looking, high-resolution 3D MRIs."
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,2.3,Network Architecture,As done by 
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,3,Experiments,
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,3.1,Data,"We use 1262 t1-weighted brain MRIs of subjects from three different datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI-1), UCSF (PI: V. Valcour), and SRI International (PI: E.V. Sullivan and A. Pfefferbaum) "
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,3.2,Implementation Details,"Our experiments are conducted on an NVIDIA A100 GPU using the PyTorch framework. The model is trained using 200,000 iterations with the AdamW optimizer adopting a learning rate of 10 -4 and a batch size of 3. τ max is set to 20. After the training, cDPM generates a synthetic MRI consisting of 128 slices by following the process outlined in Fig. "
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,3.3,Quantitative Comparison,We evaluate the quality of synthetic MRIs based on 3 metrics: (i) computing the distance between synthetic and 500 randomly selected real MRIs via the Maximum-Mean Discrepancy (MMD) score  We compare those scores to ones produced by six recently published methods: (i) 3D-DPM 
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,3.4,Results,"Qualitative Results. The center of the axial, coronal, and sagittal views of five MRIs generated by cDPM shown in Fig.  The synthetic MRIs of cDPM shown in Fig. "
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,4,Conclusion,"We propose a novel conditional DPM (cDPM) for efficiently generating 3D brain MRIs. Starting with random noise, our model can progressively generate MRI slices based on previously generated slices. This conditional scheme enables training the cDPM with limited computational resources and training data. Qualitative and quantitative results demonstrate that the model is able to produce high-fidelity 3D MRIs and outperform popular and recent generative models such as the CCE-GAN and 3D-DPM. Our framework can easily be extended to other imaging modalities and can potentially assist in training deep learning models on a small number of samples."
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,,Fig. 1 .,
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,,Fig. 3 .,
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,,Fig. 4 .,
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,,Fig. 5 .,
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,,Table 1 .,
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,,Keywords,
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,1,Introduction,"Nuclei segmentation is a fundamental step in medical image analysis. Accurately segmenting nuclei helps analyze histopathology images to facilitate clinical diagnosis and prognosis. In recent years, many deep learning based nuclei segmentation methods have been proposed  Generative adversarial network (GANs)  Our contributions are: (1) a diffusion-based data augmentation framework that can generate histopathology images and their segmentation labels from scratch; (2) an unconditional nuclei structure synthesis model and a conditional histopathology image synthesis model; (3) experiments show that with our method, by augmenting only 10% labeled training data, one can obtain segmentation results comparable to the fully-supervised baseline."
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,2,Method,"Our goal is to augment a dataset containing a limited number of labeled images with more samples to improve the segmentation performance. To increase the diversity of labeled images, it is preferred to synthesize both images and their corresponding instance maps. We propose a two-step strategy for generating new labeled images. Both steps are based on diffusion models. The overview of the proposed framework is shown in Fig. "
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,2.1,Unconditional Nuclei Structure Synthesis,"In the first step, we aim to synthesize more instance maps. Since it is not viable to directly generate an instance map, we instead choose to generate its surrogate nuclei structure, which is defined as the concatenation of pixel-level semantic and distance transform. Pixel-level semantic is a binary map where 1 or 0 indicates whether a pixel belongs to a nucleus or not. The distance transform consists of the horizontal and the vertical distance transform, which are obtained by calculating the normalized distance of each pixel in a nucleus to the horizontal and the vertical line passing through the nucleus center  Denote a true nuclei structure as y 0 , which is sampled from real distribution q(y). To maximize data likelihood, the diffusion model defines a forward and a reverse process. In the forward process, small amount of Gaussian noise are successively added to the sample y 0 in T steps by: where t ∼ N (0, I) and {β t ∈ (0, 1)} T t=1 is a variance schedule. The resulting sequence {y 0 , ..., y T } forms a Markov chain. The conditional probability of y t given y t-1 follows a Gaussian distribution: In the reverse process, since q(y t-1 |y t ) cannot be easily estimated, a model p θ (y t-1 |y t ) (typically a neural network) will be learned to approximate q(y t-1 |y t ). Specifically, p θ (y t-1 |y t ) is a also Gaussian distribution: The objective function is the variational lower bound loss: L = L T + L T -1 + ... + L 0 , where every term except L 0 is a KL divergence between two Gaussian distributions. In practice, a simplified version of L t is commonly used  where α t = 1β t and ᾱt = t i=1 α i . Clearly, the optimization objective of the neural network parameterized by θ is to predict the Gaussian noise t from the input y t at time t. After the network is trained, one can progressively denoise a random point from N (0, I) by T steps to produce a new sample: For synthesizing nuclei structures, we train an unconditional DDPM on nuclei structures calculated from real instance maps. Following "
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,2.2,Conditional Histopathology Image Synthesis,"In the second step, we synthesize histopathology images conditioned on nuclei structures. Without any constraint, an unconditional diffusion model will generate diverse samples. There are usually two ways to synthesize images constrained by certain conditions: classifier-guided diffusion  Let θ (x t , t) and θ (x t , t, y) be the noise predictor of unconditional diffusion model p θ (x|y) and conditional diffusion model p θ (x), respectively. The two models can be learned with one neural network. Specifically, p θ (x|y) is trained on paired data (x 0 , y 0 ) and p θ (x) can be trained by randomly discarding y (i.e. y = ∅) with a certain drop rate ∈ (0, 1) so that the model learns unconditional and conditional generation simultaneously. The noise predictor θ (x t , t, y) of classifier-free guidance is a combination of the above two predictors: where θ (x t , t) = θ (x t , t, y = ∅), w is a scalar controlling the strength of classifier-free guidance. Unlike the network of unconditional nuclei structure synthesis which inputs the noisy nuclei structure y t and outputs the prediction of t (y t , t), the network of conditional nuclei image synthesis takes the noisy nuclei image x t and the corresponding nuclei structure y as inputs and the prediction of t (x t , t, y) as output. Therefore, the conditional network should be equipped with the ability to well align the paired histopathology image and nuclei structure. Since nuclei structures and histopathology images have different feature spaces, simply concatenating or passing them through a cross-attention module "
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,3,Experiments and Results,
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,3.1,Implementation Details,"Datasets. We conduct experiments on two datasets: MoNuSeg  For MoNuSeg dataset, we generate 512/512/512/1024 synthetic samples for 10%/20%/50%/100% labeled subsets; for Kumar dataset, 256/256/256/512 synthetic samples are generated for 10%/20%/50%/100% labeled subsets. The synthetic nuclei structures are generate by the nuclei structure synthesis network and the corresponding images are generated by the histopathology image synthesis network with the classifier-free guidance scale w = 2. Each follows the reverse diffusion process with 1000 timesteps  Nuclei segmentation. The effectiveness of the proposed augmentation method can be evaluated by comparing the segmentation performance of using the four labeled subsets and using the corresponding augmented subsets to train a segmentation model. We choose to train two nuclei segmentation models -Hover-Net "
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,3.2,Effectiveness of the Proposed Data Augmentation Method,"Fig.  We then train segmentation models on the four labeled subsets of MoNuSeg and Kumar dataset and corresponding augmented subsets with both real and synthetic labeled images. With a specific labeling proportion, say 10%, we name the original subset as 10% labeled subset and the augmented on as 10% augmented subset. Specially, 100% labeled subset is the fully-supervised baseline. Table  Generalization of the Proposed Data Augmentation. Moreover, we have similar observations when using PFF-Net as the segmentation model. Table "
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,4,Conclusion,"In this paper, we propose a novel diffusion-based data augmentation method for nuclei segmentation in histopathology images. The proposed unconditional nuclei structure synthesis model can generate nuclei structures with realistic nuclei shapes and spatial distribution. The proposed conditional histopathology image synthesis model can generate images of close resemblance to real histopathology images and high diversity. Great alignments between synthetic images and corresponding nuclei structures are ensured by the special design of the conditional diffusion model and classifier-free guidance. By augmenting datasets with a small amount of labeled images, we achieved even better segmentation results than the fully-supervised baseline on some benchmarks. Our work points out the great potential of diffusion models in paired sample synthesis for histopathology images."
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,,:Fig. 1 .,
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,,Fig. 2 .,
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,,Fig. 3 .,
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,,Table 1 .,
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,,Table 2 .,
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 57.
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,1,Introduction,"Mild cognitive impairment (MCI) is a prodromal stage of memory loss or other cognitive loss (e.g., language, visual and spatial perception) in individuals that may progress to Alzheimer's disease (AD), which is a neurological brain disease that leaves individuals without the ability to live independently with daily activities  Resting-state functional magnetic resonance (rs-fMRI) as a non-invasive tool has demonstrated its potential to evaluate brain activity by measuring the blood oxygenation level-dependent (BOLD) signals over time  A brain network can be represented as a graph structure naturally, which consists of brain regions as nodes and connections among brain regions as edges  Currently, with the development of deep learning, various methods have been proposed to diagnose brain disease using rs-fMRI modeled with brain connectivity  Therefore, we propose Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion (FE-STGNN), which mainly focuses on the fusion of local spatial structure information from FC and temporal causal evolution information from EC, for MCI diagnosis using rs-fMRI. As FC focuses on describing the strength of brain connections in each time slice, while EC for characterizing the dynamic information flow. Thus, our motivation is to utilize the causal linkage of EC during time evolution to guide the fusion of FC networks at discrete time slices. We summarize the novelty of the proposed method in three aspects: 1) We propose to model brain connectivity using FC and EC simultaneously, comprehensively considering the structural characteristics of the brain network and time-evolving properties of causal effects between brain regions. 2) Considering that FC networks focus on describing the strength of brain connections, while ECs are more detailed about the directional information flow among brain regions, we design a novel graph fusion framework based on a cross-attention mechanism that leads FCs to aggregate temporal structure information under EC guidance. 3) We track the model in the MCI diagnostic task and evaluate the importance of different brain regions for the impact of disease, which gives an explainable basis in combination with the background of biomedical knowledge."
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,2,Method,Figure 
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,2.1,Local Spatial Structural Features and Short-Term Temporal Characteristics Extraction,"Bi-Graph Construction. To construct dynamic FC and EC networks captured by rs-fMRI, we first partition the T length BOLD signals into multiple time segments using a sliding window with window-length w and stride s. The total number of segments is )} be a set of undirected FC graphs, where V is a finite set of nodes |V | = N corresponding to each brain Regions of Interests (ROI), t varies from 1 to K indicating the time segment, and the adjacency matrix A t F C ∈ R N ×N consisting of edge element e vu can be obtained by: where ρ(v t , u t ) measuring the Pearson Correlation  Spatial Graph Convolution. The existing graph convolution methods are mainly divided into spectral methods and spatial methods. Here, we choose the spatial graph convolution for two reasons. First, the spatial method aggregates information based on the topological structure of the graph and pays close attention to the important spatial structural information in brain networks. Second, the spectral method cannot be applied to directed graphs because of the symmetry requirements of the spectral normalized Laplace matrix. From the perspective of each node, spatial graph convolution can be described as a process of message passing and node updating  where N (v) denotes the neighbors of v in the graph, h l v and h l u are hidden representations of node v and u in l-th layer respectively. e vu as the edge weights are elements of the adjacent matrix, and m l+1 v is the message aggregated from the target node v combined with local neighborhoods. The message functions M l and node update functions U l are all learned differentiable functions. We further aggregate the representations of all nodes in the graph G from the final convolution layer (i.e., L-th layer) to obtain FC and EC graph embeddings } in each time segment respectively, where d H is the feature dimension of the graph embedding. Here, the readout function R can be a simple permutation invariant function such as graph-level pooling function."
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,2.2,Spatio-Temporal Fusion with Dynamic FC and EC,"Fusion Positional Transformer. After obtaining the graph representations of FC and EC in each time segment, we further design a graph fusion network based on attention mechanism, using the causal linkage of EC in time to guide the fusion of FC networks. Considering that the attention itself does not have position awareness, it is treated equally in attention at any time segment. We first introduce a learnable temporal positional encoding P ∈ R K×dH to encode the time sequence properties of the brain network, which is randomly initialized and added to FC embeddings before attention. The attention mechanism can be described as mapping a query and a set of key-value pairs to an output which is computed as a weighted sum of the values. In practice, we first conduct linear transforms on dynamic FCs, which are computed in the form of a matrix combined with the temporal positional encoding. The dynamic EC are also linear transformed in the form of matrix Therefore, both dynamic FC and EC are encoded into high-dimensional latent subspaces, including the query subspace Q ∈ R K×d k , the value subspace V ∈ R K×d k from FC, and the key subspace K ∈ R N ×d k from EC simultaneously, where d k is the dimension of latent subspace. The conducting progress can be formulated as: where W Q , W K , and W V are the weight matrices for Q, K, and V , respectively. We further calculate the spatio-temporal dependencies of the brain network with the dot product of FCs' query and ECs' key. Therefore, the attention of FC and EC fusion can be obtained by: Here, the softmax is used to normalize the spatio-temporal dependencies and the scale √ d k prevents the saturation led by softmax function. To ensure the stable training, we adopt the residual connection formulated as M = M + (H F C +P ). Furthermore, a feed-forward neural network with nonlinear activation combined with another residual connection layer are applied to further improve the prediction conditioned on the embeddings M . As a result, the output of the fusion positional transformer is Û = ReLU MW 0 + b 0 + M , where W 0 and b 0 are learnable weight and bias. Prediction Diagnosis. In the end, the prediction layer leverages a fully connected network to map the output embeddings of the fusion positional transformer into the corresponding label space, therefore we can obtain the prediction result Y = ReLU ÛW 1 + b 1 with W 1 and b 1 are learnable weight and bias. In the training process, we design the loss function as shown in Eq. 7. The first term is used to minimize the error between the real diagnosis result Ŷ and the prediction Y . The second term L reg is the L2 regularization term that helps to avoid an overfitting problem with λ as a hyper-parameter. 3 Experiments"
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,3.1,Dataset and Experimental Settings,In order to verify the performance of our method on Alzheimer's Disease Neuroimaging Initiative (ADNI) data 
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,3.2,Ablation Studies,"Here we conduct ablation studies to verify the effectiveness of 1) the fusion of dynamic FC and EC networks and 2) each module in the proposed FE-STGNN. Specifically, we first replace graph convolutional neural network (GCN) with graph attention network (GAT) in the spatial graph convolution module (Fig.  To compare FC and FC+EC models, we further draw the ROC curves of comparison models in Fig. "
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,3.3,Comparison with Other Methods,We also compare the proposed model against baseline algorithms including three static methods: (1) Support Vector Machine (SVM)  From Table 
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,4,Conclusion,"In this paper, we present a novel FE-STGNN framework for modeling spatiotemporal connectivity patterns of brain networks for MCI diagnosis. To our knowledge, it is one of the earliest studies to use the fusion of FC and EC in a deep-learning fashion to model both spatial and temporal patterns of brain networks at the same time. The ablation studies examine the efficacy of each module of the proposed method as well as the significant benefits of combining FC and EC. The proposed model's superiority is also demonstrated when compared to other methods. We plan to extend the proposed FE-STGNN to diagnose other brain functional diseases in the future."
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,,Fig. 1 .,
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,,Fig. 2 .,
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,,Table 1 .,
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,,Table 2 .,
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_7.
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,1,Introduction,"Fluorescent microscopy is widely used to capture cell nuclei behavior. Mitosis detection is the task of detecting the moment of cell division from time-lapse images (the dotted circles in Fig.  Conventionally tracking-based methods  Unlike cell detection and segmentation, which aims to recognize objects from a single image, mitosis detection aims to identify events from time series of images. Thus, it is necessary to observe differences between multiple frames to make mitosis events annotation. Comprehensively annotating mitosis events is time-consuming, and annotators may be missed mitosis events. Thus, we must carefully review the annotations to ensure that they are comprehensive. Partial annotation has been used as a way to reduce the annotation costs of cell and object detection  Unlike supervised annotation, partial annotation can not treat unannotated areas as regions not containing mitosis events since the regions may contain mitosis events (Fig.  In this paper, we propose a cell-mitosis detection method for fluorescent time-lapse images by generating a fully labeled dataset from partially annotated sequences. We achieve mitosis detection training in a mitosis detection model with the generated dataset. To generate the fully labeled dataset, we should consider two problems: (1) no label indicating regions not containing mitosis cells and (2) few mitosis annotations. We can easily generate the regions not containing mitotic cells by using one image twice. However, such regions do not contribute to identifying mitotic cells and non-mitotic cells since the data do not show natural cell motions. For the training to be effective, the regions not containing mitotic cells should show the natural movements of cells. To generate such regions, we propose frame-order flipping which simply flips the frame order of a consecutive frame pair. As shown in the white rectangles in Fig.  In addition, we can make the most of a few partial annotations by using copy-and-paste-based techniques. Unlike regular copy-and-paste augmentation  Experiments conducted on four types of fluorescent sequences demonstrate that the proposed method outperforms other methods which use partial labels. Related Work. Some methods used partially labeled data to train model "
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,2,Method: Mitosis Detection with Partial Labels,"Our method aims to detect coordinates and timing (t, x, y) of mitosis events from fluorescent sequences. For training, we use time-lapse images I = {I t } T t=1 and partial labels (a set of annotated mitosis cells). Here, I t denotes an image at frame t, and T is the total number of frames. Our method generates a fully labeled dataset t=1 from time-lapse images I and partial labels and then trains a mitosis detection model f θ with the generated dataset. Here, I t is a generated image, and P t is a set of mitotic coordinates contained in (I t-1 , I t ). Since our method trains the network with partial labels, it can eliminate the costs of checking for missed annotations."
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,2.1,Labeled Dataset Generation,"Figure  Since mitosis is the event that a cell divides into two daughter cells, the mitosis event is transformed into an event in which two cells fuse into one by flipping the order (Fig.  Mitosis Label Utilization with Alpha-Blending Pasting: Next, we paste mitosis events to the flipped pair by using copy-and-paste techniques in order to utilize the positive labels effectively. Copy and paste augmentation has been used for supervised augmentation of instance segmentation  ) and P t = {}. Here, N is the total number of partial annotations, while C i t-1 and C i t are images before and after the mitosis of the i-th annotation (Fig. "
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,2.2,Mitosis Detection with Generated Dataset,"We modified a heatmap-based cell detection method  First, we generate individual heatmaps H j t for each pasted coordinate l j = (l j x , l j y ). H j t is defined as H j t (p x , p y ) = exp - , where p x and p y are the coordinates of H j t and σ is a hyper parameter that controls the spread of the peak. The ground truth of the heatmap at t is generated by taking the maximum through the individual heatmaps, H t = max j (H j t ) (H t in Fig. "
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,3,Experiments,"Dataset: We evaluated our method on four datasets. The first set is HeLa  Implementation Details: We implemented our method within the Pytorch framework  Comparisons: We conducted four comparisons that involved training the model with partially labeled data. For the first method, we trained the model by treating unlabeled pixels as non-mitosis ones (Baseline  In Table  The baseline "
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,,Effectiveness of Each Module:,"We performed an ablation study on the HeLa dataset to investigate the effectiveness of the proposed module. We used random augmentation (i.e., random elastic transformation "
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,,Robustness Against Missing Annotations:,We confirmed the robustness of the proposed method against missing annotations on the ES dataset. We changed the missing annotation rate from 0% to 30%. A comparison with the supervised method in terms of F1-score is shown in Fig.  Appearance of Generated Dataset: Fig. 
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,4,Conclusion,"We proposed a mitosis detection method using partially labeled sequences with frame-order flipping and alpha-blending pasting. Our frame-order flipping transforms unlabeled data into non-mitosis labeled data through a simple flipping operation. Moreover, we generate various positive labels with a few positive labels by using alpha-blending pasting. Unlike directly using copy-and-paste, our method generates a natural image. Experiments demonstrated that our method outperforms other methods that use partially annotated sequences on four fluorescent microscopy images."
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,,Fig. 1 .Fig. 2 .,
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,,Fig. 3 .,
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,,Fig. 4 .,
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,,Fig. 5 .Fig. 6 .,
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,,Fig. 8 .Fig. 9 .,
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,,Table 1 .,
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,,Table 2 .,
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 47.
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,1,Introduction,"Machine learning (ML), specifically deep learning (DL), algorithms have shown exceptional performance on numerous medical image analysis tasks  However, a challenging situation arises when relevant subgroups are unrecognized. One solution to this issue is to apply a clustering algorithm to the data, with the goal of identifying the unannotated subgroups. The main objective of unsupervised clustering is to group data points into distinct classes of similar traits. However, due to the complexity and high dimensionality of the medical imaging data and the resulting difficulty in establishing a concrete notion of similarity, extracting low-dimensional characteristics becomes the key to establishing the best criteria for grouping. Unsupervised generative clustering aims to simultaneously address both domain identification and dimensionality reduction. Deep unsupervised clustering algorithms could map the medical imaging data back to their causal factors or underlying domains, such as image acquisition equipment, patient subpopulations, or other meaningful data subgroups. However, there is a practical need to be able to guide the deep clustering model towards the identification of grouping structures in a given dataset that have not been already annotated. To that end, we propose a mechanism that is intended to constrain the model towards identifying clusters in the data that are not associated with given variables of choice (already known class labels or subgroup structures). The resulting algorithmic cluster assignments could then be used to improve ML algorithm training, or for generalizability and robustness evaluation."
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,2,Methods,"We provide a PyTorch-based implementation of all deep clustering algorithms described below (VaDE, CDVaDE, and DEC) in the open source Python package DomId that is publicly available under https://github.com/DIDSR/DomId."
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,2.1,Variational Deep Embedding (VaDE),"Variational Deep Embedding (VaDE)  VaDE is optimized using Stochastic Gradient Variational Bayes  where p(x|z) is modeled by the decoder CNN, and q(z|x) is modeled by the encoder CNN g(x; φ) as Finally, the cluster assignments can be determined via where the probability distributions p(c) and p(z|c) come from the MOG prior of the latent space, with the respective distributional parameters π, μ c , σ 2 c (for c ∈ {1, 2, . . . , D}) optimized by maximizing the ELBO of Eq. (  In all our experiments, we apply VaDE with CNN architectures for the encoder and decoder. The CNN encoder consists of convolution layers with 32, 64, 128 filters, respectively, followed by a fully-connected layer. Respectively, the CNN decoder consists of a fully-connected layer followed by transposed convolution layers with the number of input/output channels decreasing as 128, 64, 32, 3. Batch normalization and the leaky ReLU activation functions are used. "
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,2.2,Conditionally Decoded Variational Deep Embedding (CDVaDE),"We propose the Conditionally Decoded Variational Deep Embedding (CDVaDE) model as an extension to VaDE as shown in Fig.  Since our goal is to find clusters c that are unassociated with the available variables y of choice and to learn latent representations z that do not contain information about y, the generative process of CDVaDE assumes that z, c are jointly independent of y. The changes compared to the generative process of VaDE can also be regarded as imposing a structure on the model, where the encoder learns hidden representations of the image x conditioned to the additional variables y (i.e., q(z|x, y)), but acts as an identity function with respect to y (i.e., y can be regarded as being simply concatenated to the latent space representations z). The decoder then translates this data representation in the form of (z, y) to the input space (i.e., p(x|z, y)). Given that the underlying VAE architecture seeks to efficiently compress the input data x into a learned representation, this incentivizes the model to exclude information about y from the learned variables z and c. The ELBO of CDVaDE can be derived as follows, where we use the fact that by the generative process of CDVaDE it holds that p(x, z, c|y) = p(x|z, y)p(z|c, y)p(c|y) = p(x|z, y)p(z|c)p(c), and we adopt from VaDE the assumption that q(z, c|x) = q(z|x)q(c|x) holds. Hence, once the base VaDE decoder CNN is replaced by its modified version f (z, y; θ) in CDVaDE, there are no further differences between the ELBO loss function of Eq. (  While in this work we present our conditioning mechanism as an extension to VaDE, it can be combined with any deep clustering algorithm that follows an encoder-decoder architecture. In all our experiments, we use the same CNN architectures for the encoder and decoder as in VaDE (see Sect. 2.1)."
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,2.3,Deep Embedding Clustering (DEC),Deep Embedding Clustering (DEC) 
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,2.4,Related Works in Medical Imaging,"A number of studies have been conducted with several approaches of deep clustering for medical imaging data. Typically, clustering is performed on top of features extracted with the use of an encoder neural network, and the cluster assignments are determined by using conventional clustering algorithms, such as k-means, on top of the learned latent representations "
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,3,Experiments,
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,3.1,Colored MNIST,The Colored MNIST is an extension to the classic MNIST dataset 
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,3.2,Application to a Digital Pathology Dataset,"HER2 Dataset. Human epidermal growth factor receptor 2 (HER2 or HER2/neu) is a protein involved in normal cell growth, which plays an important role in the diagnosis and treatment of breast cancer  Deep Clustering Models Applied to the HER2 Dataset. We evaluate the performance and behavior of the DEC, VaDE, and CDVaDE models on the HER2 dataset. We investigate whether the models will learn to distinguish the HER2 class labels, the scanner labels, or other potentially meaningful data subgroups in a fully unsupervised fashion. To investigate the clustering abilities of CDVaDE on the HER2 dataset, we inject the HER2 class labels into the latent embedding space. We hypothesize that this will disincentivize the encoder network from including information related to the HER2 class labels in the latent representations z. Thus, with CDVaDE we aim to guide the clustering towards identifying subgroup structures that are not associated with the HER2 classes, and potentially were not previously recognized. The dimensionality of the latent embedding space was set to d = 500 for all three models.  As illustrated by the bar graphs in Fig. "
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,4,Conclusion,"We investigated deep clustering models for the identification of meaningful subgroups within medical image datasets. The proposed CDVaDE model incorporates a conditioning mechanism that is capable of guiding the clustering model away from subgroup structures that have already been annotated and towards the identification of yet unrecognized image subgroups/domains. Our experimental findings on the HER2 digital pathology dataset surmise that VaDE and DEC are capable of finding, in an unsupervised fashion, image subgroups related to the HER2 class labels, while CDVaDE (conditioned on the HER2 labels) identifies visually distinct subgroups that have a weaker association to the HER2 labels. Because the CDVaDE clusters do not clearly correspond to the scanner labels either, future work involves a review by a pathologist to see whether these subgroups capture meaningful but unannotated characteristics in the images. While CDVaDE can be used as an exploratory tool to unveil unknown subgroups in a given dataset, developing specialized quantitative evaluation metrics for this unsupervised task is inherently difficult and will also be a focus in our future work."
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,,Fig. 1 .,
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,,Fig. 2 .,
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,,Fig. 3 .,
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,,Figure 3,
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,,Fig. 4 .,
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 64.
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,1,Introduction,"Cortical folds have been shown to be related to brain function, cognition, and behavior. Based on the research of the past decades, the cortex can be further decomposed into fine-grained basic morphological patterns, such as gyri and sulci. Gyri are more potential to be functional connection centers, which are responsible for exchanging information among remote gyri and nearby sulci; on the contrary, sulci exchange information directly with their nearby gyri  Recent studies have shown that gyri can be further separated by the number of hinges it comprises, both for anatomical analysis and for functional timing analysis. Thus, the 3-hinge cyclotron fold pattern was gradually introduced and determined. It has been demonstrated that the 3-hinge regions have a thicker cortex  Although these works have achieved great success, there remain several obstacles to studying and comprehending the role of 3-hinge gyral folding patterns. One of the most significant challenges is identifying common and consistent 3-hinge gyrus folding patterns across subjects, which has yet to be resolved and has impeded group-level 3-hinges analysis. The morphology of cortical folds varies greatly between individuals, so identifying stable 3-hinge regions between individuals is difficult. In 2017  To identify the three-hinge regions with multi-modal stability via data-driven approach, we present a joint representation of functional and structural profiles for identifying consistent 3-hinges in this paper. We use functional network representation and fiber connectivity pattern of the DICCCOL system to obtain functional and structural consistency, respectively. And combine these two consistencies to identify 38 3-hinge regions that are consistent in both function and structure. We further compare these results with those based solely on structural data, which deepens our understanding of the 3-hinge region. Our work provides a basis for further inter-group analysis of the 3-hinge gyrus."
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,2,Method,
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,2.1,Dataset and Preprocessing,"We used the Q1 release of Human Connectome Project (HCP)  For diffusion weighted imaging (DWI) data, the parameters are as follows: Spinecho EPI, TR = 5520 ms, TE = 89.5 ms, flip angle = 78°, refocusing flip angle = 160°, FOV 210 × 180 (RO × PE), matrix 168 × 144 (RO × PE), slice thickness 1.25 mm, 111 slices, 1.25 mm isotropic voxels, Multiband factor = 3, and Echo spacing = 0.78 ms. Fiber tracking and cortical surface can be reconstructed from DWI dataset. Please refer to "
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,2.2,Joint Representation of Functional and Structural Profiles,"We introduce a joint representation of functional and structural profiles for identifying common and consistent 3-hinges. The method contains four steps, as shown in Fig.  where P A t j , A s i represents the Pearson correlation coefficient of A t j and A s i , and the threshold is set to 0.1. The fourth step combines the results of the second step and the third step to obtain stable 3-hinges across subjects. Firstly, the overlap in D s and D f j is found as a matching result of 3-hinges between the template and subject. If multiple ones are found, the one with higher functional similarity is selected. Secondly, 10 subjects are selected as referential subjects. The stable sequence is obtained on the template by statistical matching information between the referential subject and the template. According to the stable sequence and matching information, the consistent 3-hinge regions cross subjects are identified. And it can be used to predict consistent 3-hinge regions on new subjects."
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,2.3,"Consistency Analysis from Anatomical, Structural and Functional Perspective","From the previous section, a group of the consistent 3-hinges were identified. Then, it is important to come up with the evaluation operations and check the consistency of these 3-hinges. In this section, the evaluation operations are designed from structural, functional, and anatomical perspectives. For the anatomical perspective, we calculate the voxel-wise distance of 3-hinges across subjects to measure their consistency. We register all subjects into the Montreal Neurological Institute (MNI) standard space. After obtaining the coordinates of 3-hinges, we calculated the voxel-level distance of 3-hinges between different subjects. For the structural perspective, we use the similarity of fiber connection pattern passing through 3-hinges to evaluate the consistency of them. In detail, we count the nerve fibers passing through each 3-hinge region, and then used the trace-map method  For the functional perspective, we count the activation intensity of the 3-hinge regions among different functional networks as vector and used the Pearson correlation coefficient of these vectors across different subjects to quantify the functional consistency of 3-hinge regions."
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,2.4,Comparative Analysis of Consistent 3-hinges for Structural Data and Multimodal Data,Both joint representation of functional and structural profiles and DICCCOL-based Knearest landmark detection method are methods that identify consistent 3-hinges by using templates as bridges. We compare the results of the two methods and study the distribution and corresponding functions of the two groups of 3-hinges on the cortical surface. We register consistent 3-hinges on all subjects to the MNI standard space and calculate the distribution of consistent 3-hinges on the Automated anatomical labelling (AAL) template 
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,3,Result,
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,3.1,Visualization of the Identified Consistent 3-hinges,The consistent 3-hinges of subjects are shown in Fig. 
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,,Fig. 2. Visualization of the distribution of identified 3-hinges in the cerebral cortex,"Through adopting a joint representation of functional and structural profiles, stable sequences of length 65 are determined on the template and average 38 consistent 3hinges can be successfully identified on subjects. Figure  In order to show the consistency of the identified 3-hinges among different subjects subjectively, in Fig. "
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,3.2,Effectiveness of the Proposed Consistent 3-hinges,"After the identification of 3-hinges, it is important to evaluate whether they are common and consistent. To evaluate the consistency of these identified 3-hinges, we conduct quantitative experiments from three perspectives of anatomy, structure, and function to evaluate the performance mentioned in Sect. 2.3. From the anatomical perspective, all the subjects are registered into the MNI standard space via a linear algorithm. Then, for each corresponding 3-hinge, the voxel-level distances are calculated to measure the distance between the template and other subjects. As shown in Table  From the functional perspective, the average similarity of activation intensity of consistent 3-hinges in a functional network based on multi-modal data between template and subject is 0.441, while the similarity based on structural data is 0.366. This indicates that the addition of functional data analysis does greatly improve the functional consistency of the identified 3-hinges. In general, in this section, we verify that the consistency of the identified 3-hinges is significantly stronger than that of the methods based on structural data.  "
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,3.3,Comparative Analysis on the,
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,4,Conclusion,"In this work, we propose a joint representation of functional and structural profiles in a data-driven approach for identifying consistent 3-hinges. We use functional network representation and fiber connectivity pattern of the DICCCOL system to obtain functional and structural consistency, respectively. And combine these two consistencies to identify 38 functionally and structurally consistent 3-hinge regions. Compared with the single-modal method with DICCCOL only, the results obtained by our proposed multimodal method have a more consistent 3-hinge pattern across subjects. And we further analyze that consistent 3-hinge regions based on multimodal data are closer to visual functions, while 3-hinge regions based on structural data are closer to memory and motor function."
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,,Fig. 1 .,
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,,Fig. 3 .,
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,,Fig. 4 .,
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,,Consistent 3 -,
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,,Fig. 5 .,
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,,Table 1 .,
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,1,Introduction,"Brain magnetic resonance imaging (MRI) is widely-used in clinical practice and neuroscience. Many popular toolkits for pre-processing brain MRI scans exist, e.g., FreeSurfer  Recent works have turned to machine learning-based methods to improve preprocessing efficiency. These methods, however, are designed to solve individual sub-tasks, such as SynthStrip  NPP first translates a head MRI scan into a skull-stripped and intensitynormalized brain using a translation module, and then spatially transforms to the standard coordinate space with a spatial transform module. As we demonstrate in our experiments, the design of the architecture is critical for solving these tasks together. Furthermore, NPP offers the flexibility to turn on/off different pre-processing steps at inference time. Our experiments demonstrate that NPP achieves state-of-the-art accuracy in all the sub-tasks we consider."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,2,Methods,
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,2.1,Model,"As shown in Fig.  Geometry-Preserving Translation Module. This module converts a brain MRI scan to a skull-stripped and intensity normalized brain. We implement it using a U-Net style  where ⊗ denotes the element-wise (Hadamard) product. Such a parameterization allows us to impose constraints on χ. In this work, we penalize high-frequencies in χ, via the total variation loss described below. Another advantage of χ is that it can be computed at a lower resolution to boost both training and inference speed, and then up-sampled to the full resolution grid before being multiplied with the input image. This is possible because the multiplier χ is spatially smooth. In contrast, if we have f θ directly compute the output image, doing this at a lower resolution means we will inevitably lose high frequency information. In our experiments, we take advantage of thibass by having the model output the multiplicative field at a grid size that is 1/2 of the original input grid size along each dimension. The scalar field, which solves both skull stripping and intensity normalization, is not range restricted by design. The appropriate values will be learned from the data. In practice, we found that thresholding it at 0.2 yields a good brain mask."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Spatial Transformation,Module. Spatial normalization is implemented as a variant of the Spatial Transformer Network (STN) 
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,2.2,Loss Function,"The objective to minimize is composed of two terms. The first term is a reconstruction loss L rec . In this paper, we use SSIM  where x gt is the pre-processed ground truth images, λ ≥ 0 controls the trade-off between the two loss terms, and • denotes a spatial transformation. Conditioning on λ. Classically, hyperparameters like λ are tuned on a held-out validation set -a computationally-intensive task which requires training multiple models corresponding to different values of λ. To avoid this, we condition on λ in f θ by passing in λ as an input to a separate MLP h φ (λ) (see Fig.  for c ∈ {1, ..., C}. Here, α c and β c denote the scale and bias of channel c, conditioned on λ. This is repeated for every decoder layer, except the final layer."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,3,Experiments,"Training Details. We created a large-scale dataset of 3D T1-weighted (T1w) brain MRI volumes by aggregating 7 datasets: GSP  Architecture Details. f θ is a U-Net-style architecture containing an encoder and decoder with skip connections in between. The encoder and decoder have 5 levels and each level consists of 2 consecutive 3D convolutional layers. Specifically, each 3D convolutional layer is followed by an instance normalization layer and LeakyReLU (negative slope of 0.01). In the bottleneck, we use three transformer blocks to enhance the ability of capturing global information  Baselines. We chose three popular and widely-used tools, SynthStrip "
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,3.1,Runtime Analyses,The primary advantage of NPP is runtime. As shown in Table 
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,3.2,Pre-processing Performance,"We empirically validate the performance of NPP for the three tasks we consider: skull-stripping, intensity normalization, and spatial transformation. Evaluation Datasets. For skull-stripping, we evaluate on the Neuralfeedback skull-stripped repository (NFSR) "
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Metrics.,"For skull-stripping, we quantify performance using the Dice overlap coefficient (Dice), Sensitivity (Sens), Specificity (Spec), mean surface distance (MSD), residual mean surface distance (RMSD), and Hausdorff distance (HD), as defined elsewhere  Results. Figure  Table "
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,3.3,Ablation,"As ablations, we compare the specialized architecture of NPP against a naive U-Net trained to solve all three tasks at once. Additionally, we implemented a different version of our model where the U-Net directly outputs the skullstripped and intensity-normalized image, which is in turn re-sampled with the STN. In this version, we did not have the scalar multiplication field and thus our loss function did not include the total variation term. We call this version U-Net+STN. As another alternative, we trained the U-Net+STN architecture via UMIRGPIT  Results: Tables "
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,4,Conclusion,"In this paper, we propose a novel neural network approach for brain MRI preprocessing. The proposed model, called NPP, disentangles geometry-preserving translation mapping (which includes skull stripping and bias field correction) and spatial transformation. Our experiments demonstrate that NPP can achieve state-of-the-art results for the major tasks of brain MRI pre-processing. Funding. Funding for this project was in part provided by the NIH grant R01AG053949, and the NSF CAREER 1748377 grant."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Fig. 1 .,
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Fig. 2 .,
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Fig. 3 .,
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Fig. 4 .,
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Figure 4 (,
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Fig. 5 .,
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Table 1 .,
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Table 2 .,
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Table 3 .,
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Table 4 .,
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 25.
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,1,Introduction,"Post-Stroke Cognitive Impairment (PSCI) is common following stroke, and more than half of people will suffer from PSCI in the first year after stroke. Therefore, predicting cognitive impairment after stroke is an important task  In recent years, Convolutional Neural Networks (CNN) have shown promising performance in brain disease diagnosis and prediction  To overcome the aforementioned issues of existing methods, we propose a novel multi-modal fusion model to solve this challenging task. Our main contributions are as follows:  (2) Dynamic graphs neural representation is first proposed for PSCI prediction, which can not only integrate multi-modal information but also take subjectspecific brain anatomy into account. An effective missing information compensation module is proposed to reduce the impact of incomplete clinical data. (3) A detailed ablation study is conducted to verify the effectiveness of each proposed module. The proposed method outperforms competing models by a large margin. Additionally, the Top-15 brain structural regions strongly associated with PSCI are uncovered by the proposed method, which further emphasizes the relevance of the proposed method."
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,2,Methodology,Our approach consists of three steps (illustrated in Fig. 
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,2.1,Graph Construction and Node Feature Extraction,We represent the nodes of the graph with 131 structural brain regions obtained from the Hammers' brain atlas 
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,2.2,Missing Information Compensation Module,"When dealing with clinical data, some patients may not have complete records, resulting in missing information. To mitigate the impact of missing data, we propose a module shown in Fig.  When the value of some input values x m , m ∈ M is unknown due to missing tabular data, we can only compute x i = j / ∈M w ij x j + b i and the difference x ix i is the impact the missing tabular data can have. We propose a module to learn this difference to mitigate the effect of missing values by Here, φ 1 represents two normal linear layers, φ 2 is a linear layer without considering the bias, φ 3 represents a normal linear layers, LReLU is a leaky rectified linear activation, and • is the element-wise multiplication operator. When some elements are missing, this module will estimate the offset; and when processing a complete clinical record, the output of Eq. ( "
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,2.3,Dynamic Graph Neural Representation,"We regard the tabular's neural representation R X as global information and the node feature d i in each subject-specific graph G = {D, E|d i ∈ D} as local information (here, E denotes the adjacency matrix): First, we set up an attention mechanism between R X and d i , which will capture the contribution of each node and enhance node feature. Then, a multi-head graph self-attention is introduced for this task, which pays attention to the weight of edges in the graphs. It transforms the static graph to several dynamically subgraphs, influencing the broadcasting process of the graph. Next, we use graph summary block which consists of a normal linear layer and a leaky rectified linear unit activation function (LReLU), to summarize the graph G and output a graph-based global feature R G , i.e., graph's neural representation. Finally, we fuse the graph's neural representation R G and the updated tabular's neural representation RX via concatenation, which can be used as the input of a multilayer perceptron (MLP) for classification as shown in Fig.  Here p is the predicted probability of the correct class, α = 0.2 is the weighting factor for each sample, γ = 0.05 is a tunable focusing parameter. Node Attention Block. This is a parameterized function that learns the mapping between query (q t ∈ R 1×C ) coming from the tabular's neural representation R X , and the corresponding key (k g ∈ R N ×C ) and value (v g ∈ R N ×C ) representations in graph G. Here, N is the number of nodes, C is the channel of node feature. Hence, the node attention block fuses information from tabular (global level) and image data (local/node level) by measuring the correlation between q t and k g , and the attention weight (A 1 ∈ R 1×N ) is computed as follow, Using the computed attention weight, the output of the node attention block is computed to update graph G as, Multi-head Graph Self-attention Block. Inspired by  Then, the shapes of query q g , key k g and updated graph feature G are reshaped as [h, N, C/h]. Here, h is the number of heads. The attention weight (A 2 ∈ R h×N ×N ) is computed by measuring the similarity between q g and k g according to, Using the computed attention weight and considering the fixed adjacency matrix E, the output of graph multi-head self-attention block G ∈ R h×N ×C/h is computed and reshaped as [N, C] for the preparation of starting the next graph neural representation unit. 3 Materials and Experiments"
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,3.1,Data and Preparation,"Our study utilized clinical data from 418 stroke patients, collected within five days of the onset of their acute stroke symptoms, and consisted of both clinical data and image data. Each clinical tabular records contains 20 variables, including 3 variables of basic personal information (age, sex, education) and 17 variables of clinical indicators (i.e., smoking, drinking, hypertension, diabetes, atrial fibrillation, previous stroke, BMI, circulating low-density lipoprotein cholesterol levels, stroke severity, pre-stroke function, cognitive impairment in the acute post-stroke phase, stroke lesion volume, lacunes number, Fazekas score of WMH, Fazekas score of WMH in deep white matter, cerebral microbleeds number, perivascular spaces level). Patients were followed up, and clinical experts classified them as either PSCI negative or PSCI positive based on detailed neuropsychological tests conducted 12 months after stroke onset. Out of the 418 patients, there were 332 positive cases and 86 negative cases. All studies were approved by the local ethics committees. We split the patients into two groups with an equal proportion of PSCI candidates for five-fold cross validation, 80% for training and 20% for testing. In the preparation step, the MRIs are rigidly aligned using "
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,3.2,Evaluation Measures,"To validate the performance of our method, we employed five measures including Balanced Accuracy (BAcc), Classification Accuracy (Acc), Precision (Pre), Sensitivity (Sen), Specificity (Spe), and the area under the receiver operating characteristic curve (AUC). Among them, BAcc is a performance metric used to evaluate the effectiveness of a model on imbalanced datasets."
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,3.3,Experimental Design,"To validate the performance of our method, we perform a series of experiments on our in-house dataset. Based on the multi-modal's information (i.e., clinical data and image data), we designed three experiments with different data inputs. (I) By only using the clinical data as input, the effectiveness of missing information compensation module is studied. (II) By only using the image-based features as input, the importance of graph-based analysis for PSCI prediction is investigated. (III) Using both as input, an ablation experiment is performed, where we learn the evolution process and the superiority of our proposed fusion model. The proposed model is implemented on PyTorch library (version 1.13.0) with one NVIDIA GPU (Quadro RTX A6000), and trained for 400 epochs with a batch size of 32. Adam optimizer is employed with weight decay of 0.008, an initial learning rate of 0.0005; and CosineAnnealingLR scheduling technique is used to adjust the learning rate with the maximum number of iterations of 10."
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,4,Results,Table 
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,4.1,Effectiveness of Missing Information Compensation,"To assess the efficacy of compensating for missing information, a standard MLP model is trained solely on clinical data with missing elements filled in with zeros. Subsequently, the same MLP model is trained with a missing information compensation module, utilizing the identical training set. Comparing the performances of model A and B, it is obvious that the proposed missing information compensation module plays an important role, achieving a BAcc of 0.709 ± 0.054 and AUC score of 0.741 ± 0.072, outperforming Method A."
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,4.2,Importance of Graph-Based Analysis,"To investigate the importance of graph-based analysis for this task, an MLP model embedded with GCNs is trained only on imaging-based features, and the comparison method is a standard MLP model. Comparing the performances of model C and D, it is clear that the suggested graph-based model makes an notable improvement on both BAcc and AUC, with the rise of BAcc from 0.671 to 0.700 and of Auc from 0.648 to 0.711, which demonstrates that the consideration of subject-specific brain anatomy is very important for PSCI prediction. "
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,4.3,Superiority of Proposed Fusion Model,"Finally, we explore how to effectively fuse clinical data and image data for PSCI prediction. As a baseline approach, we directly concatenate the features learned from model B and D. The first evolutionary process is to upgrade static brain map analysis to dynamic brain map analysis, thus the performance of model B+E is explored, where the proposed multi-head graph self-attention block is first embedded into the GCN layers. Then, a better fusion pattern is discovered, i.e., dynamic graph neural representation in our final proposed method. The experimental results demonstrate that B+E model results in a higher BAcc of 0.761 compared to baseline. Furthermore, our final proposed method outperforms all other methods, achieving a BAcc as high as 0.796 and an AUC of 0.800. To explore the contribution of each brain structural region in this task, we first average the attention map A 1 across all subjects and visualize it, as illustrated in Fig. "
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,5,Conclusion and Discussion,"In this study, we tackle the challenge of predicting PSCI using both brain MRI and clinical non-imaging data. Our proposed method utilizes a dynamic graph neural representation that fully leverages the structural information of the brain. By incorporating node attention and self-attention, we effectively merge clinical tabular and image information. Our approach achieves a BAcc of 0.796 on the test dataset. Moreover, visual attention maps allow us to identify the contributions of different brain structures to this prediction task. The results of our study are consistent with prior medical research. However, there are certain brain regions that require further exploration by clinicians. In addition, there are several areas our proposed method is highly clinically relevant: (1) Risk prediction to identify (i) low-risk patients, and (ii) high-risk patients (for those early and targeted treatment and rehabilitation is recommended). ( "
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,,Fig. 1 .,
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,,Fig. 2 .,
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,,Table 1 .,
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,1,Introduction,"Parcellating the cerebellum (i.e., little brain) into neurobiologically meaningful regions of interest (ROIs) plays an important role in both structural and functional analysis  Most existing methods mainly conduct the cerebellar cortex parcellation in the Euclidean space  To address this issue, it is highly necessary to respect the cerebellar geometry, which provides macro-measurable imaging clues characterizing the cytoarchitecture of the cerebellar cortex. Therefore, in this paper, first, we reconstruct the cerebellar surfaces to restore its convoluted geometry. Specifically, we reconstruct the inner cerebellar surface (the interface between the cerebellar gray matter and white matter), and the pial/outer cerebellar cortical surface (the interface between the cerebellar gray matter and cerebrospinal fluid (CSF)). The reconstructed cerebellar surfaces are represented by triangular meshes, which can be regarded as an undirected graph. Second, we develop a graph convolutional neural network-based method to do the parcellation on the reconstructed cerebellar surfaces. Specifically, we first extract the informative surface patches from the cerebellar surfaces with the neighborship determined by their intrinsic geodesic distance. And then, we feed these surface patches into the graph convolutional neural network to train a parcellation model, which learns a highly nonlinear mapping from the geometric features of the training patches to the patch labels. One practical challenge for learning this mapping is the need for a large amount of manually labeled data, which is extremely expensive and time-consuming for vertex-level labeling. Therefore, we split the mapping learning into two steps, in the first step, we use massive patches to learn effective low-dimension representations in the latent space. Leveraging the concept of contrastive learning, we require similar representations for the patches from the same region, while distinct representations for patches from different regions. Then, in the second step, we learn the mapping from the low-dimensional latent representation to the parcellation labels. Of note, the representation learning only requires a tag to denote whether the patches are from the same region or not, which is clearly a weaker supervision compared to the accurate patch-level labels. This paper makes three contributions: 1) we propose a practical pipeline to reconstruct the cerebellar cortical surface that respects its intrinsic geometry; 2) we conduct the parcellation on the original reconstructed cerebellar cortical surfaces, without the requirement to project them onto a simplified shape, like the sphere, which will inevitably introduce distortions during the projection; 3) we leverage the self-representation learning with weak supervision information to reduce the manual labeling cost for the parcellation network training. To the best of our knowledge, this is the first method that conducts the cerebellar cortex parcellation directly using the original reconstructed cerebellar surfaces. "
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,2,Method,"Our cerebellar cortex parcellation method can be divided into 3 steps: 1) we reconstruct the cerebellar surfaces and compute the salient geometric features to characterize the geometry of the cerebellar surfaces; 2) we use weakly supervised information to learn effective latent representations from massive informative surface patches. These representations sit in low dimensional latent space, and they are similar for patches from the same region, while are distinct for patches from different regions; 3) we learn a straightforward mapping from the low dimensional latent space to the parcellation labels."
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,2.1,Cerebellar Surface Reconstruction and Geometric Feature Computation,"Given the tissue map of the cerebellum  With the reconstructed cerebellar surfaces, we can then compute typical geometric features, including the average convexity, mean curvature, curvedness, shape index, and Laplacian spectral embedding "
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,2.2,Weakly Supervised Cerebellar Patch Representation Learning,"Provided with the computed geometric features of the cerebellar surface, we can train a neural network to conduct the parcellation. However, treating the entire cerebellar surface as a single instance would require dense manual labeling for the network training, which is time-consuming and labor-intensive. Therefore, we choose the patch-wise strategy for the parcellation network training. The advantage is that it can significantly enlarge the training samples, while this comes at the cost of requiring the model to possess strong localization ability for the local patch. Directly learning a mapping from the geometric features of a local patch to the parcellation labels is very challenging for the cerebellar surface due to the shape complexity. The local sulci of the cerebellar surface are deeper, and the gyri appear more consistent shape patterns, compared to the cerebral cortical surfaces. This indicates that more training samples are required to enable the network to possess the localization ability from the local patches for a robust parcellation model. To further reduce the cost of the expensive manual labeling, we split the parcellation network training into two steps. In the first step, we learn distinctive latent representations for patches from different regions in a weakly supervised manner. Specifically, we enforce similar representations for patches from the same region, while distinct representations for patches from different regions. This can be achieved with a contrastive learning framework  Following the above motivation, we can formulate the weakly supervised patchbased representation learning below. We first extract the local patches from the cerebellar surface, which can be regarded as an undirected graph (V , E), with V being the vertex set and E being the edge set. For any vertex, we can extract a local patch, which contains a set of vertices v i k whose geodesic distances to v i are bounded by the predefined maximal geodesic distance ρ max , as illustrated in Fig.  Once the local patches are extracted, we feed them into a graph convolution based neural network to learn their latent representation. Herein, we use the widely used residual network  where τ is a hyperparameter (named temperature). It is worth noting that, a) it has been validated that this loss has superior performances in many representation learning tasks than other contrastive losses "
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,2.3,Mapping from Latent Space to Parcellation Labels,"Through representation learning, we have encoded the patches with multiple channels of geometric features into the latent space. This step not only makes the extracted patches more distinct in the latent space from the regional perspective but also significantly reduces the potential feature dimension, which can greatly facilitate the parcellation network training, compared to training the parcellation network directly from the patchwise geometric features. Therefore, given the patch-wise latent representations, we further train a multilayer perceptron (MLP) to accomplish the parcellation task. Specifically, we use the parcellation labels from the manually labeled cerebellar surface to supervise the MLP training. Herein, the patch center label is used as the patch label to train the MLP and the popular cross-entropy loss is adopted. However, since each patch is parcellated independently without considering the spatial consistency, it is possible to generate isolated parcellation labels and cause inconsistency in a geodesic neighborhood. To improve the parcellation, we further use the graph cuts method  , where p v (l v ) is the probability of assigning vertex v as label l v ; E s is the smoothness term, which is defined as: where vertex v * is the direct neighbor of v, and C v,v * (l v , l v * ) is the cost to label vertex v as l v and also label vertex v * as l v * . Herein, we used the formula from "
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,3.1,Dataset and Implementation,"To validate our method, we manually labeled 10 subjects from BCP dataset "
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,3.2,Comparison with the State-of-the-Art Methods,"Since we conducted our parcellation on the original cerebellar surface, which can be regarded as a graph, we compared our method with state-of-the-art graph convolutional neural network based methods "
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,3.3,Different Features' Influence Analysis,We also validated the influence of each geometric feature on the final parcellation performance. We conducted the ablation study by removing parts of the geometric features for the parcellation. Table 
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,4,Conclusion,"In this paper, we propose an automated method for anatomically meaningful cerebellar cortical surface parcellation. We firstly reconstruct the geometric accurate and topologically correct cerebellar surfaces and then compute several widely used geometric features to comprehensively characterize the geometries of the cerebellar surface. Next, we extract local surface patches from the reconstructed cerebellar surfaces with the neighborship defined by the intrinsic geodesic metric. These extracted local surface patches are projected to a low dimensional latent space with a contrastive learning framework, i.e., patches from the same region are enforced to have similar representations, while patches from different regions have distinct representations. After that, we train a neural network to map the latent representations to the parcellation labels. Comparison to the state-of-the-art methods has validated the superior performance of our method. Currently, our work has two limitations, a) the quantitative evaluation is based on a small number of the subjects, due to the expensive manual labeling cost. In the future, we plan to involve and release more manually labeled cerebellar cortical surfaces to further improve the generalizability of the current framework, enhance the representation learning and validate our method on larger datasets; b) a graph cut post-processing is needed to remove potential inconsistent labelling. In the future, we plan to directly add the neighborhood smooth labeling constraint into the network cost function to obtain an end-to-end cerebellar cortical surface parcellation method."
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,,Fig. 1 .,
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,,Fig. 2 .,
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,,Fig. 3 .,
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,,Table 1 .,
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,,Table 2 .,
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,1,Introduction,"Brain disorders (BDs) pose severe challenges to public mental health in the global world. To understand the pathology, and for accurate diagnosis as well, functional MRI (fMRI), one of the MRI modalities, is widely studied for BDs. The fMRI provides assessments of the disease-induced changes in the brain functional connectivity networks (FCNs) among different brain regions of interest (ROIs). And a huge body of studies has successfully built effective classifiers for different BDs based on FCN and deep learning methods  Meta-learning based algorithm is one of the advanced methods to develop a general model based on heterogeneous information from data in different domains or for different tasks. It aims to learn optimal initial parameters for the model (meta-learner) which can be quickly generalized to new tasks, directly or with a few new training data for fine-tuning. There have been extensive discussions in the literature on developing more general models utilizing meta-learning  Upon this evidence, it would be promising to develop a general BD diagnosis model based on FCN and further use meta-learning to solve the above-mentioned issues. However, there are still at least two challenges to achieve this goal. First, how to optimally extract the generalizable common knowledge (features) from the procedure of diagnosing various BDs? Previous methods focused on conventional FCNs (computed using simple linear correlations) and only treated FCN as a vector  In this paper, we develop a novel framework (illustrated in Fig. "
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,2,Methods,
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,2.1,Notation and Problem Formulation,"We define the entire set of BD diagnosis tasks using different datasets with T . For a specific task T j ∈ T (j = 1, 2, ..., 6), we have n pairs of FCN and labeled data , where F is the fMRI data and Y is the label set of all subjects. ROIs are defined in individual fMRI spaces based on the brain atlas  We extract three features from individual fMRI data for the graph deep learning analysis. First, we compute the low-order FCN A low , where A low ∈ R NROI×NROI is a graph adjacency matrix containing the pair-wise correlations among different ROI signals. Second, we compute the high-order FCN A high based on the topological information in A low (described in detail in Sect. 2.3). These two features will be used as edge features. Third, we use the corresponding order adjacency matrix, together with the mean and standard deviation value of ROI signals, as the node features for both lowand high-order graphs  We have meta-training and meta-testing stages for our meta-learner. The corresponding tasks are named as meta-training task T train and meta-testing task T test . The meta-training stage mimics cross-task adaptations, aiming to make the model learning to adapt to new task T test with a small number of samples under initial parameters. To simulate the cross-task scenario, we utilized the episodic training mechanism  i=1 across all meta-training datasets, based on which, the T train is constructed. With constructed T train , we have two loops to update the initial parameters of the meta-learner, which are inner and outer loops. During inner loops, initial parameters update gradients are first estimated using the back-propagation learned from the D train sup . Then, if initial parameters are updated by the first gradient, we further estimate the gradients based on D train que as the outer loop to finally update them. The two loops will be combined to update the initial meta-learner parameters for better fine-tuning on new tasks as detailed in Sect. 2.2. At the meta-testing stage, we adapt the initialized parameters from the meta-training stage to the new T test with a few data and test its performance. We first fine-tune the meta-learner on , and then report classification performance on . Our target is to make the accuracy on D test que as high as possible when the size of the support data s is only a small portion of T test . If s = K, we denote the setting as K-shot classification."
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,2.2,Meta-Learner Training Algorithm,"Our meta-learner consists of two modules, which are 1) multi-view classifier as detailed in Sect. 2.3 and 2) meta-controller as detailed in Sect. 2.4. The pseudo-codes for the meta-learner training algorithm are given in Algorithm 1. During the meta-training stage, our meta-learner algorithm has two iterative parameter update loops, which are inner (lines 3-9) and outer loops (lines 10-14) "
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,2.3,Multi-view Graph Classifier θ c,"As mentioned above, exploring the topological information of the FCN is necessary for BD diagnosis. However, a single low-order FCN view can only illustrate the simple pair-wise relation. So, we additionally construct the high-order FCN, which reflects the correlation among ROIs in terms of their own FC patterns, as calculated below: After constructing high-order adjacency matrix A high as a complementary view, we input them into the multi-view graph classifier parameterized with θ t c at the adaption step t. To extract the disease-related features of different-view graphs, we use convolution in GCN  Then, we use the pooling operation to further extract the global topological features of different graph views. Here, we opt for gPOOL operation  The attention mechanism allows the model to decide which view should rely on for specific tasks more adaptively. Then, we forward attention-weighted features into an MLP and acquire the final prediction label. Here, we use the cross-entropy loss as a constraint to the network."
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,2.4,Meta-Controller θ m,"In machine learning, over-fitting is one of the critical problems that restrict the performance of models, especially in small datasets. Previous works utilize the early stopping to alleviate this problem, but the hand-crafted early stopping parameter is hard to choose. Here, we utilize a neural network to learn the stop policy adaptively, which we call meta-controller parameterized with θ m depicted in Fig.  where D i X L i represent the degree matrix of A i and the feature matrix in the last layer L, respectively; j denotes j-th node which is also the j-th row in those matrices, and • 1 denotes the L1-norm of row vector. For classification losses L sup and ANI values Q sup on D train sup across t adaption steps, we use them to compute the stop probability p t at step t with an LSTM  where o t is the output of the LSTM model at step t and σ is the SoftMax function. Finally, we sample the choices c t by Bernoulli distribution to decide whether we should stop at step t. Since the relation between θ m and θ c is undifferentiable, it is impossible to take direct gradient descent on θ m . We use stochastic policy gradient to optimize θ m . Once we sample the stop choice at step T , we train the meta-controller according to loss changes on D train que across T steps. Given the parameter update trajectory of classifier {θ 0 c , θ 1 c , ...θ T c } during T steps, we calculate the corresponding loss change trajectory of D train que . Based on that, we further define the controller immediate rewards r at step t as the loss change on D train que (caused by parameter update of step t): Then, the accumulative reward R at step t is where T is the total number of steps and R t is the change of classification loss on D train que from step t to the end of adaption. Then we update our meta-controller by policy gradients, which is a typical method in Reinforcement learning  where∇ θm is the gradients over θ m and β 2 is the learning rate."
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,3,Experiments,
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,3.1,Dataset,We use fMRI meta-training data from five datasets including Alzheimer's Disease Neuroimaging Initiative (ADNI) 
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,3.2,Settings,"To ensure a fair comparison, we use three graph convolutional layers, followed by corresponding pooling layers, for all GNN based methods. We use the ADAM optimizer with 1e-4 for learning rate and 5e-4 for weight decay, 100 for epochs, 0.001 for both β 1 and β 2 , respectively. For the inner loop fast adaption, we set the minimum and maximum steps by 4 and 16. In the meta-training stage, we sample D train sup and D train que from all training datasets; and, in the meta-testing stage, we only randomly sample D test sup and D test que from the meta-testing dataset. The size of support data for both meta-training and meta-testing stages is depicted in the first line of Table "
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,3.3,Results and Discussions,In Table 
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,4,Conclusion,"In this work, we focus on the issues of developing a classifier on new BD datasets with small samples and propose a novel framework. A broad of datasets covering elder and youth BDs are used to train the model to estimate the common features among BDs. An adaptive multi-view graph classifier is proposed to enable the model efficiently extract features for different BD diagnosis tasks. In addition, to avoid over-fitting during the adaptions to new data, we utilize a novel meta-controller driven by RL. Extensive experiments demonstrate the effectiveness and generalization of our proposed method. It is expected that advanced graph embedding methods can be integrated into our framework to improve performance. Our work is also promising to be extended to neuroscience studies to reveal both common and unique characteristics of different BDs."
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,,Fig. 1 .,
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,,,
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,,Fig. 2 .,
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,,Algorithm 1 :,
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,,Table 1 .,
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,,Table 2 .,
Community-Aware Transformer for Autism Prediction in fMRI Connectome,1,Introduction,"Autism spectrum disorder (ASD) is a developmental disorder that affects communication, social interaction, and behaviour  Deep learning (DL) models have led to significant advances in various fields including fMRI-based brain connectome analysis. For instance, BrainNetCNN  It is worth noting that our brain is comprised of functional communities  To address this limitation, we propose a novel community-aware transformer for brain network analysis, dubbed Com-BrainTF, which integrates functional community information into the transformer encoder. Com-BrainTF consists of a hierarchical transformer with a local transformer that learns community-specific embeddings, and a global transformer that fuses the whole brain information. The local transformer takes FC matrices as input with its parameters shared across all communities, while, personalized prompt tokens are learnt to differentiate the local transformer embedding functions. The local transformer's output class tokens and node embeddings are passed to the global transformer, and a pooling layer summarizes the final prediction. Our approach enhances the accuracy of fMRI brain connectome analysis and improves understanding of the brain's functional organization. Our key contributions are: 1. We propose a novel local-global hierarchical transformer architecture to efficiently learn and integrate community-aware ROI embeddings for brain connectome analysis by utilizing both ROI-level and community-level information. 2. We avoid over-parameterization by sharing the local transformer parameters and design personalized learnable prompt tokens for each community. 3. We prove the efficacy of Com-BrainTF with quantitative and qualitative experiment results. Our visualization demonstrates the ability of our model to capture functional community patterns that are crucial for ASD vs. Healthy Control (HC) classification."
Community-Aware Transformer for Autism Prediction in fMRI Connectome,2,Method,
Community-Aware Transformer for Autism Prediction in fMRI Connectome,2.1,Overview,"Problem Definition. In brain connectome analysis, we first parcellate the brain into N ROIs based on a given atlas. FC matrix is constructed by calculating the Pearson correlation coefficients between pairs of brain ROIs based on the strength of their fMRI activations. Formally, given a brain graph with N number of nodes, we have a symmetric FC matrix X ∈ R N ×N . Node feature vector of ROI j is defined as the j th row or column of this matrix. Given K functional communities and the membership of ROIs, we rearrange the rows and columns of the FC matrix, resulting in ). This process helps in grouping together regions with similar functional connectivity patterns, facilitating the analysis of inter-community and intra-community connections. Com-BrainTF inputs X k to the community kspecific local transformer and outputs N dimensional tokens followed by a pooling layer and multi-layer perceptrons (MLPs) to predict the output. Overview of Our Pipeline. Human brain connectome is a hierarchical structure with ROIs in the same community having greater similarities compared to inter-community similarities. Therefore, we designed a local-global transformer architecture(Fig. "
Community-Aware Transformer for Autism Prediction in fMRI Connectome,,ASD vs HC,2) 
Community-Aware Transformer for Autism Prediction in fMRI Connectome,2.2,Local-Global Transformer Encoder,"The transformer encoder  Local Transformer: For optimal analysis of fMRI brain connectomes, it is important to incorporate both functional community information and node features. Therefore, we introduce the knowledge of community labels to the network by grouping node features based on community labels producing inputs {X 1 , X 2 , . . . , X K }. However, separate local transformers for each input would result in a significant increase in model parameters. Hence, we use the same local transformer for all inputs, but introduce unique learnable personalized 1D prompt tokens {p 1 , p 2 , . . . , p K }, where p i ∈ R 1×N , that learn to distinguish between node feature matrices of each community and therefore avoid overparameterization of the model. Previous transformer-based models like BNT  Global Transformer: On obtaining community-specific node embeddings and prompt tokens from the local transformer, it is essential to combine this information and design a module to learn the brain network at a global level. Therefore, we introduce a global transformer encoder to learn inter-community dependencies. Input to the global transformer is the concatenated, learnt node feature matrices from the local transformer and a prompt token (Fig.  The resulting attention-enhanced, node embedding matrix Z L is then passed to a pooling layer for further coarsening of the graph. Extensive ablation studies are presented in Sect. 3.3 to justify the choice of inputs and output."
Community-Aware Transformer for Autism Prediction in fMRI Connectome,2.3,Graph Readout Layer,The final step involves aggregating global-level node embeddings to obtain a highlevel representation of the brain graph. We use OCRead layer 
Community-Aware Transformer for Autism Prediction in fMRI Connectome,3,Experiments,
Community-Aware Transformer for Autism Prediction in fMRI Connectome,3.1,Datasets and Experimental Settings,ABIDE is an open-source collection of resting-state functional MRI (rs-fMRI) data from 17 international sites 
Community-Aware Transformer for Autism Prediction in fMRI Connectome,3.2,Quantitative and Qualitative Results,Comparison with Baselines (Quantitative Results). Following the comparison method mentioned in BNT 
Community-Aware Transformer for Autism Prediction in fMRI Connectome,,Interpretibility of Com-BrainTF (Qualitative Results,). We perform extensive experiments to analyze the interpretability of Com-BrainTF. The following results are discussed in detail: 1. Interpretation of the learned attention matrix: Fig. 
Community-Aware Transformer for Autism Prediction in fMRI Connectome,2.,Ability of the prompt's attention vector to differentiate between ASD and HC subjects:,"We investigate the first row of the learned attention matrix, namely the attention of ROIs corresponding to the prompt. The ROI- wise normalized attention scores shown in Fig.  3. Meta-analysis of the important functional networks that influence ASD prediction: DMN and SMN networks have been found to be crucial for ASD prediction based on Fig. "
Community-Aware Transformer for Autism Prediction in fMRI Connectome,3.3,Ablation Studies,"We conduct ablation studies to justify the designs of input and output pairing of the global transformer that facilitates effective learning of node embeddings, consequently resulting in superior performance. The input global transformer prompt token is intentionally retained in all of the experiments because of (i) its capability to identify important communities for the prediction task (Sect. 3.2) and (ii) its capability to learn relationships within and between communities (Sect. 3.2 Fig.  Input: Node Features vs. Prompt Tokens of Local Transformers. We evaluated two input possibilities for the global transformer: (i) use only prompt tokens from the local transformer (ii) incorporate both prompt tokens and updated node features. Table  Output: Cross Entropy Loss on the Learned Node Features vs. Prompt Token. In comparison to vision models like ViT "
Community-Aware Transformer for Autism Prediction in fMRI Connectome,4,Conclusion,"In this work, we introduce Com-BrainTF, a hierarchical, community-aware localglobal transformer architecture for brain network analysis. Our model learns intra-and inter-community aware node embeddings for ASD prediction tasks. With built-in interpretability, Com-BrainTF not only outperforms SOTA on the ABIDE dataset but also detects salient functional networks associated with ASD. We believe that this is the first work leveraging functional community information for brain network analysis using transformer architecture. Our framework is generalizable for the analysis of other neuroimaging modalities, ultimately benefiting neuroimaging research. Our future work includes investigating alternate variants for choosing different atlases and community network parcellations."
Community-Aware Transformer for Autism Prediction in fMRI Connectome,,Fig. 1 .,
Community-Aware Transformer for Autism Prediction in fMRI Connectome,,Fig. 2 .,
Community-Aware Transformer for Autism Prediction in fMRI Connectome,,Table 1 .,
Community-Aware Transformer for Autism Prediction in fMRI Connectome,,Table 2 .,
Community-Aware Transformer for Autism Prediction in fMRI Connectome,,CE loss on node features vs prompt token,
Community-Aware Transformer for Autism Prediction in fMRI Connectome,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 28.
Unified Surface and Volumetric Inference on Functional Imaging Data,1,Introduction,"In recent years, there has been increasing interest in performing surface-based analysis of magnetic resonance neuroimaging (MRI) data in the human cortex. A basic assumption that underpins this approach is that cortical activity will correlate better according to geodesic distance (along the surface) than geometric distance (straight-line). The benefits include improvements in the localisation of functional areas and the establishment of inter-subject correspondence  The objective of this work was to develop a framework for parameter inference that is able to operate in a surface and volumetric manner simultaneously. This would remove the need for separate workflows whilst ensuring that each anatomy of interest is treated in an optimal manner, namely surface-based for the cortex, volumetric for WM, and region-of-interest (ROI) for subcortical GM structures. This principle corresponds directly with the HCP's concept of grayordinates "
Unified Surface and Volumetric Inference on Functional Imaging Data,2,Methods,"Theory. The spatial locations within the brain at which parameter estimates are obtained are hereafter called nodes. A key step in performing inference is to define a mapping between nodes and the voxel data that has been acquired, which is then embedded within a generative model to yield a function that relates physiological parameters to the data they reconstruct. In the case of volumetric inference, this mapping is one-to-one: each node corresponds to exactly one voxel. The concept can be extended to include other types of node: alongside voxels for WM, surface vertices can represent the cortex and volumetric ROIs can represent subcortical GM structures. Collectively these are referred to as hybrid nodes, which correspond to greyordinates in the HCP terminology  In this work, the mapping for hybrid nodes previously introduced in [10] was used. It is constructed by calculating the volume of intersection between individual voxels and the geometric primitives that construct the cortical ribbon, WM tracts, or individual ROIs. Importantly, because each node corresponds by definition to exactly one tissue type, the mapping will be many-to-one in voxels that contain multiple tissues which is an explicit representation of PVE. The mapping takes the form of a non-square sparse matrix (number of nodes > voxels), an example of which is illustrated in Fig.  The choice of model M and corresponding physiological parameters θ is determined by the imaging modality in question. In practice, evaluating this expression for all but the most trivial configurations is infeasible due to the integrations entailed (notably the evidence term in the denominator). A number of numerical approaches have thus been developed, one of which is variational Bayes (VB) which approximates the posterior p with an arbitrary distribution q and uses the free energy F to assess the accuracy of approximation  VB thus turns parameter inference into an optimisation problem: what q best approximates p as measured by the free energy? One means of implementing this is to use stochastic optimisation techniques. A Monte Carlo approximation to the objective function F may be obtained using an average over L randomly drawn samples θ * l from q(θ): This strategy is referred to as stochastic variational Bayes (SVB). By constructing this expression as a computational graph, including the modalityspecific generative model M with its embedded mapping from hybrid nodes to voxels, automatic differentiation techniques may be used to maximise F and thus derive the optimal approximation q to the true posterior p. The volumetric implementation of SVB previously introduced in  A key advantage of the Bayesian approach is that it enables prior information to be incorporated into the inference. Such priors can either be distributional, for example an empirically-derived normal distribution; or spatial, which encode the belief that parameter values should correlate in adjacent regions of the brain  Evaluation. hSVB was evaluated using simulated mutli-delay arterial spin labelling (ASL) data, a perfusion modality which is sensitive to both cerebral blood flow (CBF) and arterial transit time (ATT). Inference on multi-delay ASL data requires fitting a non-linear model with two parameters which is more challenging than for single-delay ASL. A single subject's T1 MPRAGE anatomical image (1.5 mm isotropic resolution, TR 1.9 s, TE 3.74 ms, flip angle 8 • ) was processed to extract the left cortical hemisphere, which defined the ground truth anatomy from which ASL data was simulated. FreeSurfer was used to obtain mesh reconstructions of the white and pial cortical surfaces  A single-compartment well-mixed ASL model was used to represent term M in Eq. 1 and simulate data  hSVB has been implemented using TensorFlow version 2.9 running on Python 3.9.12. Optimisation was performed using RMSProp with a learning rate of 0.1, a decay factor of 0.97, and a sample size of 10. During training, a reversion to previous best state ('mean reversion') was performed when cost did not improve for 50 consecutive epochs; for all experiments, training was run until 20 such reversions had taken place which served as a proxy for convergence. Typically this implied training for around 2000 epochs. A folded normal distribution was used on all model parameters to restrict inference to positive values only. Runtime on a 6 core CPU was around 5 mins per inference, using around 4 GB of RAM. The comparator method was based on BASIL, a conventional volumetric ASL processing workflow  Performance was assessed by calculating the following metrics with respect to the ground truth cortical CBF map: sum of squared differences (SSD) of CBF; SSD of Z-transformed CBF, and Bhattacharyya distance of CBF distribution. The second and third metrics are included because they are sensitive to relative perfusion, whereas the first is sensitive to absolute CBF. A receiver-operator characteristic (ROC) analysis was performed using a binary classifier of varying threshold value t. Recalling that the ground truth CBF map had a mean value of 60 with extrema of ±20, t was set at values of 5 to 15 with an increment of 1. At each t, areas of hypoperfusion were classified with a threshold value of 60 -t and hyperperfusion with 60 + t. The area-under ROC (AUROC) for hypo and hyperperfusion was then calculated and the mean of the two taken. This yielded an AUROC score for each method at varying levels of threshold t. "
Unified Surface and Volumetric Inference on Functional Imaging Data,3,Results,Figure  Figure 
Unified Surface and Volumetric Inference on Functional Imaging Data,4,Discussion,"The results presented here demonstrate that a hybrid approach to parameter inference using SVB (hSVB) offers a compelling alternative for the surface-based analysis of functional neuroimaging data. hSVB can operate directly on volumetric data without pre-processing and is able to able to apply the spatial prior in an anatomically-informed manner that respects tissue boundaries. Applied to simulated ASL data, hSVB demonstrated a number of positive attributes in relation to a conventional volumetric workflow with post-projection (BP). Firstly, greater consistency in performance across voxel sizes was observed. As spatial resolution directly determines the extent of PVE within data, this implied that hSVB is more robust to PVE which is an important source of confound. Secondly, at higher levels of SNR, hSVB was able to deliver estimates that scored substantially better across a variety of metrics, which suggests it is well-placed to exploit future increases in SNR that result from advances in hardware and acquisition. Finally, hSVB was better able to discern relative perfusion differences, i.e., areas of abnormality. The trade-off for this was higher SSD errors in absolute perfusion values at low SNR. A key point of divergence between hSVB and BP observed in this work was in the effect of the spatial prior at low SNR levels. Bayesian inference can be understood as an updating process whereby the prior distribution is modified to the extent that the observed data supports this. For low SNR data, the data will support less deviation from the priors. Referring to Fig.  Two limitations of this work are that the ground truth cortical CBF map used to simulate data (Fig. "
Unified Surface and Volumetric Inference on Functional Imaging Data,,Fig. 1 .Fig. 2 .,
Unified Surface and Volumetric Inference on Functional Imaging Data,,Fig. 3 .,
Unified Surface and Volumetric Inference on Functional Imaging Data,,Fig. 4 .,
Unified Surface and Volumetric Inference on Functional Imaging Data,,Fig. 5 .,
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,1,Introduction,"Modern large multi-site neuroimaging studies have shown increasing power to detect biological variability of interest and provided invaluable insights into the changes underlying neurodevelopmental and neurodegenerative disorders  Inspired by image-to-image translation techniques in the computer vision field  To address these issues, we develop a novel, flexible deep learning-based method to harmonize multi-site cortical surface maps in a vertex-wise manner free of parcellation scheme, thus preserving the spatially detailed cortical measure information after harmonization and enabling comprehensive vertexwise analysis in further studies. Our approach builds on a surface-based autoencoder and uses an adversarial strategy  1. We propose a novel method to fill the critical gap for multi-site vertex-wise cortical data harmonization, while there exist statistical models suitable for ROI-wise data harmonization and CycleGAN model for two-site data; 2. Taking advantage of disentanglement learning and adversarial strategies, we successfully learn a transparent, controllable, and meaningful generative model for efficiently mapping cortical surface data across different sites; 3. To the best of our knowledge, we performed the largest validation on infant cortical data harmonization with 2,342 scans from 4 sites and demonstrate the superior performance of our method compared to other methods."
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,2,Method,
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,2.1,Vanilla Autoencoder (AE),"As shown in Fig.  Herein, E φ consists of 5 repeated spherical 1-ring-convolution+Batch Normalization (BN)+ReLU layers with 4 spherical mean pooling layers between them. The feature channel at each resolution is 8, 16, 32, 64, and 128, respectively. To enable more compact feature extraction, we add a flatten and a linear layer with 512 neurons to the end of the encoder and finally obtain the latent vector z with size 1×512. The decoder D θ first uses a linear layer with 20,736 neurons and then reshapes it into 162×128 to recover the feature map at lowest resolution. Then it gradually upsamples the features and finally reconstructs the input data with its original size, which can be formulated as: x = D θ (z). The training process of an AE thus tries to minimize the reconstruction loss:"
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,2.2,Disentangled Autoencoder (DAE),"To detect and remove site effects from multi-site vertex-wise cortical measurements, we introduce a disentanglement learning strategy for its controllability and interpretability  where W t and W i refer to the site classifiers using latent variables z t and z i , respectively, t i is a one-hot vector encoding the ground-truth site label. The multi-class binary cross entropy loss in Eq. ( "
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,2.3,Cycle-Consistent Disentangled Autoencoder (CDAE),"Previously, a DAE, after successful training, will be directly used for image synthesis  To learn a more controllable and meaningful generative model, we propose to train the decoder with additional constraints on backward mapping. As shown in Fig.  , and enforce the cycle-consistency loss to guarantee the generated surface map is meaningful to the original surface map: We also add the cycle-consistency loss to the latent site-unrelated variable z i to reinforce the correlation between the generated and original surface map: and an explicit correlation loss to further reduce the ambiguity of indirect cycleconsistency losses for better preserving global structural information: where cov denotes the covariance, σ is the standard deviation. Besides, we also use the losses in AE and DAE to train the backward mapping. This means we reuse the site classifier W t to adversarially train the decoder to generate fake surface maps at the target site that cannot be distinguished from the true maps, which is a standard GAN training process "
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,3,Experiments and Results,
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,3.1,Experimental Setting,"We evaluated our method using 4 large infant datasets (S1, S2, S3, and S4) acquired by different scanners and imaging protocols with resolutions ranging from isotropic 0.8 mm 3 (S1) to 1.25 × 1.25 × 1.95 mm 3 (S3). S1  Then the reconstructed cortical surfaces were mapped onto the sphere, nonlinearly aligned based on geometric features, and further resampled with 40,962 vertices "
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,3.2,Results,"Validation on Removing Site Effects. To validate if site effects are successfully removed, we first show the population-level developmental trajectory of average cortical thickness in Fig.  Validation on Preserving Individual Variability. While it is important to validate if a harmonization method removes site effects, it is equally important to show that the method preserves biological variability after harmonization. Following  Validation on Downstream Task. We further investigate how different harmonization methods affect a downstream task. Same as in "
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,4,Conclusion,"In this paper, to address the important yet unsolved problem of multi-site vertexwise cortical data harmonization, we proposed a novel, flexible deep learningbased method, named Cycle-consistent Disentangled Autoencoder (CDAE). Our CDAE takes advantage of the simplicity and transparency of autoencoder, the controllability and interpretability of disentanglement learning, and reinforced meaningfulness of cycle-consistency to successfully learn the complex non-linear mapping among heterogeneous multi-site data, which is inherently difficult and unsuitable for existing methods. Both visual and quantitative results on four datasets with 2,342 scans show the effectiveness of our method on both site effects removal and biological variability preservation. Our method will not only facilitate multi-site vertex-wise neuroimaging data analysis but also inspire novel directions in learning-based data harmonization."
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,,Fig. 1 .,
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,,Fig. 2 .,
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,,Fig. 3 .,
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,,Fig. 4 .,
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,,Fig. 5 .,
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,1,Introduction,"Brachial plexopathy is a form of peripheral neuropathy  Radiation fibrosis, primary and metastatic lung cancer, and metastatic breast cancer account for almost three-fourths of causes  Magnetic resonance imaging (MRI) and ultrasound of the brachial plexus have become two reliable diagnostic tools for brachial plexopathy  Many radiomics studies have experimentally demonstrated that image texture has great potential for differentiation of different tissue types and pathologies  With the goal of classifying normal from abnormal BP, we explored the approach of deep texture learning. This paper constructed a BP dataset with the most commonly used BP MRIs in our clinical practice. Considering the shortcoming of traditional patterns, triple point pattern (TPP) is proposed for the quantitative representation of the heterogeneity of abnormal BP's. In contrast to GLCM-CNN, TPPNet is designed to train models by feeding TPP matrices as the input with a huge number of channels. Finally, we analyze the model's performance in the experimental section. The major contributions of this study include 1) directed triangle construction idea for TPP, 2) huge number of TPP matrices as the heterogeneity representations of BP, 3) TPPNet with 15 layers and huge number of channels, 4) the BP dataset containing MR images and their corresponding ROI masks."
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,2,Materials and Method,
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,2.1,Dataset Preparation and Preprocessing,"Following IRB approval for this study, we search for patients with metastatic breast cancer who had a breast cancer MRI performed between 2010 and 2020 and had morphologically positive BP on the MRI report from our electronic medical records (EMR) in * hospital. Totally, we collect approximate 807 series which include 274 T2, 254 T1 and 279 Post-gadolinium. Since some scans are seriously degraded due to motion artifacts. Therefore, each case underwent several essential image adjustments such as multi-series splitting, two-series merging, slice swapping, artifact checking and boundary corrections. To yield the ROI, firstly, we randomly sampled -40% of the sequences including both normal and abnormal ones that were manually segmented with ITK-snap by two skilled trainees "
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,2.2,Triple Point Pattern (TPP),"Theoretically, some texture pattern methods such as LBP, LTP, and GLDM, are based on single-variance pixel functions  According above requirements, we developed a method to produce a serial of novel texture patterns by introducing a directed triangle idea with an adjacent triple pixel as a ternary group, called triple point pattern (TPP), to extract the local texture information. Then, a statistical method like histogram is employed to count the number of the same type of pixel-triplets within the ROI or throughout the whole image. Finally, a threedimensional (3D) TPP matrix is formed to characterize the image texture globally as the following: Two-dimensional image: where I is a MxN image, x, y, and z is the pixel triplet, x,y,z ∈ [0,L), L is its gray level, p c = (0,0) denotes the concerned pixel such as p 0 in Fig.  where I is a three dimensional image with the shape of MxNxK, p c = (0,0,0), other parameters are similar to the Two-dimensional image. In statistics, a TPP matrix should a 3D distribution of directed triangle. As shown in Fig.  By further analysis, we could find some TPP pairs have an isomorphism relationship since its TPP matrix could be generated by transposing or flipping another TPP matrix on some certain conditions when two triangles formed by the pixel triplet have the relevance of shifting or scaling. For an instance, the TPP matrix by pixel-triplet (p 1 , p 0 , p 2 ) in Fig.  where T denotes matrix transposing, F represents matrix flipping, * is the product operator. Since both T and F are continuous bijective mappings, these three TPPs are isomorphic. In our study, these isomorphic TPP matrices are not dropped from the TPP matrix set because they are equivalent to image rotations and re-scaling. Image scaling can result in the image pixels increasing. The normalization could almost remove the effect of pixel increase caused by scaling transformations. Moreover, TPPs generated by 135°c ould also be treated as affine transformations. Therefore, data augmentation could be omitted when we combine TPP with deep learning for this study. As its definition, the TPP matrix should be a cubic array with the shape of LxLxL where L is the gray level of the image."
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,2.3,TPPNet,"The pipeline of the proposed method TPPNet is illustrated in Fig.  1) Avoidance of image augmentation. Due to the stability of TPP matrix under rotation, scale and affine transformations, image augmentation could be omitted in the preprocessing step which can lead to image deformation. 2) Huge number of channels. TPPNet treats each TPP as an independent channel. For 2D images, there are at least 32 channels if more displacements of TPP is considered. Similarly, we could generate no less than 325 TPPs in 3D images. 3) Simple end-to-end architecture. We integrate the k-fold cross-validation, TPP generation and model training into one framework. Since the TPP matrix is always small, there are only 15 layers in TPP which could reduce the risk of overfitting issue met in deeper neural networks. 4) Free from the interference of multi-texture-pattern arrangements. Since each channel is corresponding with one TPP, it can solve the pattern arrangement issue occurred in GLCM-CNN. 3 Experiments"
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,3.1,Preparations,"Some important specificities of our computing platform contain: one AMD EPYC 7352 24-Core Processor, 1 TB memory and four Nivida A100-SXM GPUs with 320 GB GPU memory. The whole dataset is divided into three subsets according to the MR sequence, i.e. T2, T1 and post-gad. For each subset, both normal cases and abnormal cases were randomly and evenly split into five subgroups. A five-fold cross-validation scheme was employed to generate five cohorts. Totally, 15 cohorts were produced. Each cohort consists of training set, validation set and testing set by the ratio of 6:2:2."
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,3.2,Ablation Studies,"Since all images in our dataset are 3D images, therefore, the initial channel is set 325 which is equal to the TPP number. The loss functions in the following experiments shared categorical_crossentropy. Nadam is adopted as the optimizer with the learning rate of 0.0001 and batch size of 8 for 200 epochs. All performances listed in this section are the average of performances with 5-fold cross-validation."
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,3.2.1,Impact of Gray Level,"The image gray level determines the shape of the TPP matrix. To avoid its sparsity, we compute the TPP matrix set via Eq. (  The models are trained and tested over 15 cohorts. Their performances evaluated by accuracies are listed in Table2 which tells us that T2 sequence yields the highest accuracy of 96.1% when the gray level is 12. T1 and post-gadolinium also get acceptable results with the accuracies of 93.5% and 93.6% respectively. Other performances could be read in supplementary materials. "
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,,Acc,Loss Acc Loss Acc Loss 325 0.948±0.026 0.342±0.126 0.922±0.035 0.363±0.098 0.934±0.034 0.302±0. 124  1 0.863±0.027 0.274±0.091 0.810±0.145 0.458±0.105 0.811±0.047 0.590±0.107 Input channel T2 T1 Pg
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,3.2.2,Impact of Intensity Rescaling Approaches,Rescaling approaches of image intensity could also bring impacts on the BP's differentiation while producing the TPP matrix. The commonly used methods include minmax-linear approach 
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,3.2.3,Multi-Channels vs Solo-Channel,"We carry out experiments to train the TPPNet model and make tests with arc tangent rescaling approach under gray level 16. As a comparison, we test single channel mode as well. By sharing every TPP matrix's label with the original case label, our TPPNet works well by assigning one channel for the initial input. Once the trained model with solo channel is generated, all TPP matrices of the testing set could be tested. Hereafter, we adopted a voting method to determine if the prediction is normal, if the predicted probability is less than 0.5, otherwise, it is considered abnormal. Performances with accuracies and loss are listed in Table "
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,3.3,Comparisons,We evaluated our proposed TPPNet by comparing it to the recent state-of-the-art approaches over our BP dataset including VGG16 
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,4,Conclusions,"In this paper, we develop an approach to carry out the pioneer study of differentiating abnormal BP from normal ones relevant to breast cancer. In particular, TPP is proposed to extract texture features as the representation of BP's heterogeneity from MRIs. Moreover, a TPPNet with huge number of initial channels is designed to train the model. To testify our proposed TPPNet, a BP dataset is constructed with 452 series including three most commonly used MR sequences in clinical practice, i.e. T2, T1 and Post-gadolinium. The best result is yielded when the gray level is 12, intensity rescaling method adopts arc tangent approach. Experimental outcomes also demonstrate that the proposed TPPNet not only exhibit more stable performances but also outperform six famous state-of-the-art approaches over three most commonly used BP MR sequences."
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,,Fig. 1 .,
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,,Fig. 2 .,
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,,Fig. 3 .,
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,,,
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,,Table 1 .,
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,,Table 2 .,
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,,Table 3 .,
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,,.025 0.277±0.107 0.935±0.021 0.307±0.164 0.936±0.027 0.279±0.046,
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,,Table 4 .,
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,,Table 5 .,
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,1,Introduction,"Building and analyzing functional brain network (FBN) based on resting-state magnetic resonance imaging (rs-fMRI) have become a promising approach to functional brain disease diagnosis, e.g., Alzheimer's disease (AD) and Parkinson's disease. Rs-fMRI probes neural activity by the fluctuations in the bloodoxygen-level-dependent (BOLD) signals. The strength of functional connectivities (FCs) between brain regions is measured by the correlation between pairwise BOLD signals. FBN represents the interaction patterns between brain regions during brain functioning and can be used to identify abnormal changes caused by brain diseases  Traditional FBNs are constructed at the individual level using Pearson's correlation  It has been demonstrated that the construction of the common FBN at population level as prior knowledge is beneficial to reduce the effects of the first issue of large variability aforementioned  Another equally important issue with the existing methods is that the current measures of FC strength only take the synchronous functional interaction into account and ignore the potential asynchronous interactions, as illustrated in Fig.  To validate the effectiveness of the proposed method, we conducted experimental studies on rs-fMRI data sets of ADNI2 and ADNI3 for early AD diagnosis, i.e., classification of mild cognitive impairment (MCI) and normal control (NC). The experimental results show that the proposed method consistently achieves state-of-the-art performance."
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,2,Proposed Method,"The overall network of our method is shown in Fig.  by applying 1D temporal convolution for input mapping, where N denotes the number of brain nodes, T denotes frames, and D denotes the feature dimension. Then, before feeding into the L-th Transformer encoder, X 0 needs to be position encoded for incorporating node index as spatial identity. The position encoding matrix P is defined as: P (pos, :, 2dim) = sin(pos/10000 2dim/D ); P (pos, :, 2dim + 1) = cos(pos/10000 2dim/D ), where pos ∈ [0, • • •, N -1] is the node index, the symbol ':' refers to all indexes in the corresponding dimension, and dim indicates the index of the feature dimension. P is added to X 0 , i.e., X 0 = X 0 + P . If we measure the interactions between brain regions directly, we can obtain synchronous FCs. In order to consider the timelagged interaction between brain regions, we use a sliding window to segment the signals into short-time segments, the details are illustrated in Fig.  where f l (•) denotes the transformation function of the l-th encoder layer with input from X l-1 , which is the output of the (l-1)-th layer, and a l (•) denotes the attention module to construct FBN. The details of a l (•) are illustrated in Fig. "
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,2.1,Attention-Based Sparse Common-and-Individual FBN Construction Module (ASCFCM),"In order to respect the sparsity property of the human brain and introduce population-level constraint, we redesign the kernel self-attention module in  Revisiting Kernel Self-attention Mechanism: Here, we first review the kernel attention module proposed in  , where D v is set as D l-1 /H, and H is the number of heads. The resulting attention matrix A = QK T is N × N , and A i,j is actually a Gaussian RBF kernel function  where softmax( A √ T Dv ) is a N × N matrix and it can be viewed as an adaptively built FBN, presenting the dependencies between N brain regions. Sparsity: From Eq. (  The optimization could produce a sparse distribution with proper regularization, as in "
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,,Population-Level Common Pattern:,"As mentioned previously, the above FBN has large variability since it is constructed with the rs-fMRI data of each subject. Previous research indicates that human brains have similar topologies, which can be modeled as prior knowledge separately to regularize the model, helping to alleviate the issue of large variability. Therefore, we parameterize a data-driven common FBN module as a regularization, as shown in Fig. "
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,2.2,Cross Spatiotemporal Asynchronous FCs,"To simulate the time-lagged asynchronous FCs, we segment the BOLD signals l by using a sliding window, as shown in Fig. "
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,3,Experiments,
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,3.1,Data and Preprocessing,Both rs-fMRI data sets of ADNI2 and ADNI3 from the Alzheimer's Disease Neuroimaging Initiative (ADNI) (https://adni.loni.usc.edu/) are used to evaluate the effectiveness of the proposed method with the most challenging task of differentiating MCI from NC. The subject number is 410 (147 MCI vs. 263 NC) for ADNI2 and 425 (168 MCI vs. 257 NC) for ADNI3. The ADNI2 data are acquired on a 3 Tesla (Philips) scanner with TR/TE set at 3000/30 ms and a flip angle of 80 • . The ADNI3 data are acquired with a flip angle of 90 • . The preprocessing is carried out using DPARSFA toolbox (http://rfmri.org/DPARSF) and SPM-12 (ehttps://www.fil.ion.ucl.ac.uk/spm/software/). We perform a standard approach to processing rs-fMRI by following 
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,3.2,Experimental Settings,"We conduct a 5-fold cross validation in our evaluation to obtain stable results, and all the methods involved in the comparison use the same partitions of data for fairness. The performance of the model is measured by four metrics: Area Under the receiver operating characteristic Curve (AUC), accuracy (ACC), sensitivity (SEN), and specificity (SPE). The encoder layers of the proposed network L are set to 5. Asynchronous FCs are explored in the first two layers. The sliding window of each layer has a length of 32 and a step size of 16 while the number of heads H in each layer is set as 2. The model input X raw ∈ R 90×128×1 is transformed into X 0 ∈ R 90×128×6 by a mapping with a kernel size of 3 × 1. The output feature dimension D l of each layer is set as 12. The drop rate of the attention score is 0.2, and the batch size is 16. We train 250 epochs with stochastic gradient descent (SGD), whose learning rate is 0.1 and momentum is 0.9. The code is publicly available at https://github.com/seuzjj/ACIFBN."
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,3.3,Experimental Results,The average performance of the proposed method and the competing methods is summarized in Table 
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,3.4,Ablation Study,"In order to further verify the effectiveness of the proposed method and the contribution of each component, we conduct ablation studies and the results are shown in Table  To verify the effects of the settings during dividing rs-fMRI signals into segments, we set the window lengths as different values in the two asynchronous FCs modeling layers, seen in Fig. "
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,4,Visualization and Conclusion,"The learned individual FBNs carry diagnostic clues, which are visualized in Fig.  In sum, this paper proposes to adaptively construct and analyze asynchronous common and individual functional brain networks within a specially designed Transformer-based network, which is more flexible, discriminative and capable of capturing both synchronous and asynchronous FCs. The experimental results on two ADNI data sets well demonstrate the effectiveness of the proposed method."
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,,Fig. 1 .,
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,,Fig. 2 .,
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,,Fig. 3 .,
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,,Fig. 4 .,
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,,Table 1 .,
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,,Table 2 .,
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,1,Introduction,"The human thalamus is a brain region with connections to the whole cortex  Different approaches have been used to segment thalamic nuclei. Some methods have attempted to register manually labelled histology to MRI  Other methods have relied on specialised MRI sequences to highlight the anatomical boundaries of the thalamus, typically at 7T  One approach that supports training and test data of different modalities is Bayesian segmentation, which combines a probabilistic atlas (derived from one modality) with a likelihood model to compute adaptive segmentations on other modalities using Bayesian inference  Finally, there are also supervised discriminative methods that label the thalamus from dMRI. An early approach by Stough et al. "
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,2,Methods,A summary of our method is shown in Fig. 
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,2.1,"Training Dataset, Preprocessing, and Data Representation","To make the CNN compatible with legacy datasets, we choose a simple representation based on the FA and V1 of the DTI fit at each voxel. DTI only requires 7 measurements and is thus compatible with even the oldest datasets. As in many DTI visualisation tools, we combine the FA and V1 into a single 3 × 1 red-greenblue vector at every voxel. This RGB vector has brightness proportional to the FA, and its colour encodes the direction of V1 as shown in Fig.  To obtain accurate training segmentations from the Bayesian method in FreeSurfer "
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,2.2,Domain Randomisation and Data Augmentation,"We employ domain randomisation and aggressive data augmentation for both our T1 and diffusion data in order to model: (i) the degradation in quality from HCP to more standard acquisition protocols, and (ii) the variability in appearance due to differences in acquisitions and scanners at test time. Domain Randomisation for Resolution: at the crux of our method is the domain randomisation of input resolutions. At every iteration, we randomly sample the voxel dimensions for the T1 and DTI (independently) in two steps. First, we sample a ""coarse"" scalar voxel size from a uniform distribution between 1 and 3 mm. Then, we sample the voxel side length in each direction from a normal distribution centred on this ""coarse"" mean with σ = 0.2 mm. Next, we resample the T1 and RGB channels to the sampled resolution. For the T1, we use a publicly available PV-aware degradation model  Data Augmentations: we also apply a number of geometric and intensity augmentations, some standard, and some specific to our dMRI representation. -Global geometric augmentation: we use random uniform scaling (between 0.85 and 1.15) and rotations about each axis (between -15 and 15 • ). Rotations are applied to the images and also used to reorient the V1 vectors. -Local geometric augmentation: we deform the scans with a piecewise linear deformation field, obtained by linear interpolation on a 5 × 5 × 5 × 3 grid. V1 vectors are reoriented with the PPD method (""preservation of principle direction""  These augmentations are applied to the downsampled images. After that, the augmented images are upscaled back to 0.7 mm isotropic resolution. This ensures that all the channels (including the target segmentations) are defined on the same grid, independently of the intrinsic resolution of the inputs (Fig. "
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,2.3,Loss,"We build on the soft dice loss  where, X l = {x l i } and Y l = {y l i } are the predicted and ground truth probability maps for label l ∈ [0, . . . , L]; G g is the set of label indices in nuclear group g ∈ [1, . . . , 10], label l = 0 corresponds to the background and SDC is the soft Dice"
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,2.4,Architecture and Implementation Details,"Our CNN is a 3D U-net  For validation purposes, we used the Bayesian segmentations of 50 withheld HCP subjects and 14 withheld ADNI subjects. Even though the Bayesian segmentation of ADNI data is not reliable enough to be used as ground truth for evaluation (due to the PV problems described in the Introduction), it is still informative for validation purposes -particularly when combined with HCP data. The final model for each network is chosen based on the validation loss averaged between the HCP and ADNI validation sets."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,3,Experiments and Results,
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,3.1,MRI Data,"We trained on 600 HCP subjects (see Sect. 2.1). For evaluation, we used: HCP: 10 randomly selected subjects (not overlapping with the training data), with manual segmentations of 10 groups of labels (the same as in Sect. 2.3). LOCAL: 21 healthy subjects (9 males, ages 53-80), each with a 1.1 mm isotropic T1 and a test-retest pair of 2.5 mm isotropic dMRI (64 directions, b=1,000). ADNI: 90 subjects from the ADNI, 45 with with Alzheimer's disease (AD) and 45 healthy controls (73.8±7.7 years; 44 females), each with a T1w (1.2×1×1 mm, sagittal) and dMRI (1.35×1.35×2.7 mm, axial, 41 directions, b=1,000) scan."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,3.2,Competing Methods and Ablations,"To the best of our knowledge, the only available tool that can segment T1/dMRI of any resolution is Freesurfer "
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,3.3,Results,Qualitative results are shown in Fig. 
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,,Direct Evaluation with Manual Ground truth Using HCP:,We first evaluated all competing methods and ablations on the 10 manually labelled subjects. Table  Test-Retest Using LOCAL: Table 
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,,Best-Performing CNN:,Considering Table 
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,,Group Study Using ADNI:,"We segmented the ADNI subjects with the bestperforming CNN, and computed volumes of the thalamic nuclei normalised by the intracranial volume (estimated with FreeSurfer). We then computed ROC curves for AD discrimination using a threshold on: (i) the whole thalamic volume; and (ii) the likelihood ratio given by a linear discriminant analysis (LDA, "
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,4,Discussion and Conclusion,"We have presented the first method that can segment the thalamic nuclei from T1 and dMRI data obtained with virtually any acquisition, solving the problems posed by PV to Bayesian methods. Using Bayesian segmentations generated from multiple diffusion models while applying hybrid domain randomisation and augmentation methods, we remarkably improve upon both the accuracy and reliability of our source segmentations. Our tool is robust against misregistration from geometric distortion, which is generally more problematic in frontal and occipital regions. Nuclei volumes resulting from the tool show similar discriminative power to those provided by the Bayesian tool, while improving the utility of whole thalamus measurements and increasing segmentation resolution. Crucially, our use of the FA and V1 representation of dMRI data as input means that our tool is compatible with virtually every dMRI dataset. Publicly sharing this ready-to-use tool as part of FreeSurfer 7.4 will enable neuroimaging studies of the thalamic nuclei without requiring any expertise in neuroanatomy or machine learning, and without any specialised computational resources."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,,Fig. 1 .,
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,,Fig. 2 .,
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,,Fig. 3 .,
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,,Table 2 .,
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,,Acknowledgments. Work primarily funded by ARUK (IRG2019A003,). Additional support by the 
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,1,Introduction,"Zebrafish are widely used model organisms in many experiments due to their fully-sequenced genome, easy genetic manipulation, high fecundity, external fertilization, rapid development and nearly transparent embryos  Existing approaches are mostly based on the automatic alignment of manually identified landmarks or operate in the image domain. However, to the best of our knowledge there is no approach available yet to obtain a spatiotemporal alignment of 3D+t point clouds of biological specimens in an automatic and unsupervised fashion. In  In this work, we present a deep learning-based method for the temporal alignment of 3D+t point clouds of developing embryos. Firstly, an autoencoder is employed to extract descriptive features from the point clouds of each time frame. As an autoencoder designed explicitly for point clouds, FoldingNet "
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,2,Methods,Our method is based on the FoldingNet 
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,2.1,Autoencoder,"FoldingNet: FoldingNet is an autoencoder specifically designed for 3D point clouds  Modified Chamfer Distance: The Chamfer Distance (CD) is one of the most widely used similarity measures for point clouds and is used as the loss function of FoldingNet. The discrepancy between two point clouds is calculated as the sum of the distances between the closest pairs of points as follows: where S in , S out are the input and reconstructed point clouds, respectively. However, this approach does not consider the local density distribution since all points are treated independently. In this work, we introduce the Modified Chamfer Distance (MCD), in which the point-to-region distance replaces the pointto-point distance. The new loss is the summation of the k-nearest-neighbors of each point in the target point cloud and defined as: where d is the Euclidean distance between a point p and its respective nearest neighbor q i . As the embryos grow, the density distribution changes significantly in different parts. The utilized data set "
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,2.2,Regression Network,"The features extracted by the autoencoder are used to generate the temporal alignment of different embryos. We select one embryo as the reference and train an MLP regression network that maps autoencoder-generated feature vectors to frame numbers. The regression MLP consists of a sequence of fully-connected layers with ReLU activation. The input is a latent feature vector with length 256. In each dense layer, the size of the vector is reduced by a factor of two. The penultimate layer converts the 8-dimensional vector directly to the output node. The mean squared error is utilized as the loss function to compare the output to the ground truth: the sequence of time frame indices from 1 to 370. To temporally synchronize a new embryo with the reference, we present extracted features of all its frames to the trained regression network and generate a new index sequence. The desired alignment result is obtained by comparing the generated sequence with the reference embryo (Fig.  3 Experimental Results and Discussions"
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,3.1,Data Sets and Evaluation,"The data set used in this work was published in  To simplify the training approach, a fixed number of points is randomly chosen from each point cloud using the PyTorch Geometric library "
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,3.2,Experimental Settings,"The FoldingNet-based autoencoder is implemented with PyTorch Lightning. The number of neighbors is set to 16 in the KNN-graph layers and to 20 in the MCD. The autoencoder is trained with an initial learning rate of 0.0001 for 250 epochs and a batch size of 1. At each iteration, the embryo from a single time frame with the size 4096 × 3 is given to the network and the encoder converts it to a 1D codeword of length 256, which is the latent feature vector. For network training with the four wild-type embryos, we use a 4-fold crossvalidation scheme with three embryos for training and one for testing in each fold. The regression network is trained for 700 epochs with a learning rate of 0.00001. The hyperparameters of the regression network are determined empirically based on the convergence of the training loss, since there is no validation or test set available. The temporal alignment is validated with the embryo from the test set to make the result independent from training the autoencoder and the embryo is aligned to its shifted variants. To reduce the influence of randomness, we repeat the alignment of each test embryo three times and take the average value of all experiments of the four embryos."
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,3.3,Experimental Results,"Temporal Alignment Results: The resulting alignment with different shifting methods is depicted in Fig.  Visualizing the Learned Representation: To confirm that the autoencoders actually learned a representation suitable for temporal alignment, we visualize a chronologically color-coded scatter plot of the learned 256-dimensional feature vectors of all 370 time frames using the t-SNE algorithm "
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,4,Conclusion,"In this work we present a fully-unsupervised approach to temporally align 3D+t point clouds of zebrafish embryos. A FoldingNet-based autoencoder is implemented to extract low-dimensional descriptive features from large-scaled 3D+t point clouds of embryonic development. Several modifications are made to the network and the loss function to improve their applicability for this application. The embryos are temporally aligned by applying a regression network to the features extracted by the autoencoder. A postprocessing method is designed to provide consistent and accurate alignments. As no frame-accurate ground truth is available yet, we assess the effectiveness of our method via a 4-fold cross validation and a synthetically generated ground truth. An average mismatch of only 3.83 min in a developmental period of 5.3 h is achieved. Finally, we performed several ablation studies to show the impact of rotation and spatial translation of the point clouds to the alignment results. By aligning embryos with different spatial locations and deflected central axis, a relatively small error rate of 5.74 min can still be achieved. According to feedback from a biological expert the achievable manual alignment accuracy is on the order of 30 min and potentially exhibits intra-and inter-rater variabilities. As the first unsupervised method designed for the automatic spatiotemporal alignment of 3D+t point clouds, our method achieves high accuracy and eradicates the need for any human interaction. This will particularly help to minimize human effort, to speedup experimental analysis and to avoid any subjective biases. In future works, our approach could be applied to more data sets and other model organisms with different scales and development periods to further validate its applicability. We're currently conducting an extensive effort to obtain frame-accurate manual labels from multiple raters in a randomized study, to better assess the actual performance that we can expect under real-world conditions including intra-and inter-rater variability. In the long term, we envision an iteratively optimized spatiotemporal average model of multiple wild-type embryos to finally obtain a 3D+t reference atlas that can be used to precisely analyze developmental differences of corresponding anatomical regions across experiments."
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,,Fig. 1 .,
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,,Fig. 2,
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,,Fig. 2 .,
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,,Fig. 3 .,
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,,Fig. 4 .,
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,,Table 1 .,
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_58.
Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition,1,Introduction,"In healthcare, affective computing can help measure the psychological state of patients automatically, especially for those with cognitive deficits. For example, the emotional state of hospitalized patients contributes to the early diagnosis of Parkinson's Disease (PD)  As EEG signals are directly related to high-level cognitive processes, EEGbased emotion recognition draws increasing attention in recent years  To overcome the above limitations, we design a spatial-temporal feature extraction framework based on prior-driven dynamic brain networks and apply it to emotion recognition. Specifically, we treat each electrode of EEG as a node of brain network, and then the dynamic functional connectivity networks (DFCNs) is constructed by Pearson correlation coefficient under non-overlapping time window. Besides, we calculate the correlation between EEG and facial expression across modal channels by cross attention mechanism, as the prior knowledge of DFCNs, and then embed it to above model obtain the final DFCNs representation. Finally, we implemented residual blocks and non-local attention to construct STFENet, so as to extract complex spatial-temporal feature and preserve the long-range dependencies in the time series."
Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition,2,Method,"Figure  Dynamic Brain Networks Construction: Functional Connectivity Networks (FCNs) ignore the temporal changes of brain connectivity. In this paper, we construct Dynamic Functional Connectivity Networks (DFCNs) to solve the above problem. First, each subject's EEG data can be represented as X E ∈ R P ×T ×D , where P represents the number of channels, T is the number of time windows, and D represents the feature dimension. The t-th subsequence feature of P channels can be represented as a matrix where x i (t) ∈ R D represents the t-th subsequence feature extracted from the EEG time series of the i-th channel. According to the divided nonoverlapping sliding time window, we build a functional connectivity network (i.e., matrix) by computing Pearson correlation coefficient between EEG from a pair of channels within the t-th time window: where cov denotes the covariance between two vectors, σ xi(t) denotes the standard deviation of vector x i (t), x i (t) and x j (t) represent the EEG of a pair of channels i and j within the t-th time window, respectively. Thus, the original DFCNs of each subject Prior Knowledge Embedding: Most of the existing multi-modal emotion recognition works aim to extract the features of different modalities respectively for fusion, which always lost the correlation between modalities. Existing studies have found there is high correlation between EEG and facial expression  where W Q and W K are the parameter matrices used to generate query and key, which are updated through network back-propagation during model training. We determined correlation scores across channel-dimension between modalities based on cross attention, by treating one modality as query and the other as key: where, Cor(E, F ) and Cor(F, E) represents the correlation score between the cross-modality channels, d 1 , d 2 are normalized parameters equal to the dimension of K. It is worth noting that softmax is applied to the scoring weight of the equation Eq. 4. However, softmax proved to be overconfident in its results, which would result in the correlation scores of certain time windows being too high or too low, affecting the reliability of the prior knowledge. Therefore, we improve softmax to softplus to solve this problem while ensuring that the correlation matrix is non-negative. The calculated correlation matrix is as follows: At this point, we obtain the correlation between the cross-modal channels, and use it as the prior knowledge of DFCNs construction. We embed the modified prior knowledge into the previously constructed DFCNs by element-wise product: where represents element-wise product. By the embedding of prior knowledge, we obtain the discriminative DFCNs representations with prior knowledge. Spatial-Temporal Feature Extraction: Different from static brain networks, DFCNs can not only describe brain connectivity, but also contain the temporallevel volatility of brain connectivity. Most of the existing methods focus on extracting the temporal and spatial features of EEG separately, and concat them for feature fusion, which ignores the dynamic variations of electrode connectivity in the temporal dimension. 2D convolution has been widely used in the field of computer vision, but it is challenging to capture information at the temporal level. Previous studies has shown that 3D convolution operations can better model spatial information in continuous sequences  where σ is the assigned activation function, b m is the deviation, w T ,P ,P c ,m represents the weight of the convolution kernel connected by the c -th stacking channel to the feature representation of the position (T, P, P ), and v T +T ,P +P ,P +P c represents the characteristic value of the c -th stacking channel at the position (T, P, P ). To better capture the spatial-temporal topological structure in DFCNs, inspired by ResNet's remarkable success  Since the operation of convolution will eventually focus on local areas, longrange dependencies which describe luxuriant emotion-related information will be lost to some extent. To solve this problem, we further introduce non-local block  where W θ and W φ is the weight to be learned, which is realized by 3D convolution in this paper. Then, non-local attention uses the self-attention term  where g is implemented by 1×1×1 convolution in this paper. Then, the non-local block can be defined as: where ""+x"" denotes the residual connection, and W z represents the weight matrix. By the STFENet, we finally effectively extract the spatial-temporal emotion-related information in prior-driven DFCNs for the identification of emotions. "
Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition,3,Experiment Results,"Emotional Database: The DEAP dataset  Data Pre-processing: For the EEG data, The 32-channel EEG signal with a duration of 63 s is down-sampled to 128HZ, and remove the first 3 s pre-trial baseline. Power spectral density (PSD) features is extracted from 3 s time windows with non-overlap through the Welch method in EEG, and 5 frequency bands are adopted, i.e. theta (4-8 Hz), slow alpha (8-10 Hz), alpha (8-12 Hz), beta (12-30 Hz), and gamma (30+ Hz)  Experiment Settings: In our experiment, we adopt the leave one subject out (LOSO) cross-validation strategy to verify the effectiveness of our method. Specifically, the samples are divided into 18 non-overlapping parts according to the subjects. The samples of one subject are selected as the test set while the remaining subjects are selected as the training set for each cross-validation. This process is repeated 18 times, and the average performance of the crossvalidation is taken as the final result. Identification performance is measured by accuracy (ACC) and F1-Score. The proposed method is based on the Pytorch implementation, and the model mentioned in this study is trained on a single GPU (NVIDIA GeForce RTX3080). Adam algorithm is used to optimize this method, and the learning rate and batch size are set to 0.001 and 40, respectively. Results and Discussion: We evaluate the performance of our method by calculating ACC and F1 on both valence and arousal. We also compare our method with many comparison methods, which can be divided into two categories: EEG based methods and multi-modal based methods. More specifically, EEG based methods are Support vector machine (SVM), GraphSleepNet (GSN) "
Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition,4,Conclusion,"In this paper, we develop a spatial-temporal feature extraction framework based on prior-driven DFCNs for multi-modal emotion recognition. In our approach, not only the connectivity between EEG channels but also the dynamics of connectivity over time are jointly learned. Besides, we also calculate the correlation across modalities via cross attention to guide the construction of DFCNs. In addition, we build STFENet based on 3D convolution to model the spatial-temporal features contained in DFCNs to extract emotion-related spatial-temporal information and preserve the long-range dependencies in the time series. Experimental results show that our method outperforms the state-of-the-art methods."
Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition,,Fig. 1 .,
Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition,,Fig. 2 .,
Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition,,,
Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition,,Table 1 .,
Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition,,Table 2 .,
Vertex Correspondence in Cortical Surface Reconstruction,1,Introduction,"The reconstruction of cortical surfaces from brain MRI scans, a fundamental process in neuroimaging, involves extracting the pial surface (outer cerebral cortex layer) and the white matter surface (white-gray matter boundary). Various methods, including FreeSurfer "
Vertex Correspondence in Cortical Surface Reconstruction,2,Methods,"Fig.  In cortical surface reconstruction, template deformation methods transform a mesh template to match the neuroanatomy of the given patient. Let M x be the triangular mesh template, where , storing the indices of the respective vertices that make up the triangles, and r edges E ∈ R r×2 , storing the indices of two adjacent faces that share a common edge. The surface reconstruction algorithm computes the displacement f : R n×3 → R n×3 for the set of vertices V x . In V2C, this displacement is computed by a graph convolutional network, which is conditioned on image features from a convolutional neural network that takes the MRI scan as input. The two sub-networks are connected via feature-sampling modules that map features extracted by the CNN to vertex locations of the meshes. V2C addresses the issue of self-intersections in explicit surface reconstruction methods by incorporating multiple regularization terms into the loss function. Let M y = {V y , F y , E y } be the ground truth mesh, and M ŷ = {V ŷ , F ŷ , E ŷ } the predicted deformed mesh, where Vy ∈ R n×3 , and V y ∈ R m×3 . Note that n = m and therefore there exists no one-to-one mapping for vertex correspondence. The full loss function of V2C consists of a loss term for the CNN and a loss term for the mesh reconstruction, with further details in  where P y ∈ R q×3 , and P ŷ ∈ R q×3 are point clouds sampled from the surfaces of M y and M ŷ respectively. For simplicity, we have omitted the curvature weights. In order to optimize for vertex correspondence, we propose to use a preprocessing step that registers the Mesh M y to the template mesh M x , resulting in a resampled ground truth mesh M y = {V y , F y , E y }, with V y ∈ R n×3 and F y ∈ R o×3 , where each index i ∈ 1, . . . , n represents the same anatomical location in both V x and V y . Instead of the Chamfer loss in Eq. (  where ni is the unit normal of the i-th face of M ŷ . Our method relies on only one regularization term, compared to three in V2C. The regularization factor λ needs to be tuned as a hyperparameter. Our proposed V2CC method and the pre-processing step are presented in Fig. "
Vertex Correspondence in Cortical Surface Reconstruction,3,Experiments and Results,"Evaluation Metrics: To assess the quality of reconstructed cortical surfaces, we employ four metrics. With the average symmetric Chamfer distance (cdist ) and the percentage of self-intersecting faces (%SIF ), we evaluate the reconstructed surfaces' quality. For evaluating vertex correspondence, we use two different approaches for intra-and inter-subject cases. In intra-subject cases, we measure whether the same template vertex moves to the same location when provided with different scans of the same subject. In this case, we use scans that were acquired within a brief period of time to avoid structural changes. We calculate the consistency of vertex locations using the root-mean-square deviation (RMSD) of vertex positions. In inter-subject cases, we assess the ability of our method to map pre-defined parcellation atlases, such as the Desikan-Killany-Tourville (DKT) atlas  Data: For evaluation, we used the ADNI dataset (available at http://adni.loni. usc.edu), which provides MRI T1 scans for subjects with Alzheimer's disease, mild cognitive impairment, and cognitively normal. After removing scans with processing artifacts, we split the data into training, validation, and testing sets, balanced according to diagnosis, age, and sex. As ADNI is a longitudinal study, only the initial (baseline) scan for each subject was used. Our ADNI subset contains 1,155 subjects for training, 169 for validation, and 323 for testing. We used the TRT dataset  Implementation Details: To prepare for training, we pre-processed the data using FreeSurfer v7.2 "
Vertex Correspondence in Cortical Surface Reconstruction,,Results and Discussion:,"We compare the proposed V2CC method to state-ofthe-art models V2C, CFPP, and Topofit on the ADNI dataset with FreeSurfer's fsaverage6 right hemisphere templates as an input mesh. All methods were trained using the resampled ground truth meshes. For baseline models we used hyperparameters proposed by the original method. We show the results in the top part of Table "
Vertex Correspondence in Cortical Surface Reconstruction,,Downstream Applications:,"We hypothesize that our meshes with vertex correspondence to the template can be directly used for downstream applications such as group comparisons or disease classification, without the need for postprocessing steps. We performed a group comparison of subjects with Alzheimer's disease (AD) and healthy controls of the ADNI test set, where we compare cortical thickness measures on a per-vertex level. We present a visualization of p-values in Fig. "
Vertex Correspondence in Cortical Surface Reconstruction,4,Conclusion,"In this work, we proposed V2CC, a novel approach for cortical surface reconstruction that directly provides vertx correspondence. V2CC utilizes a pre-processing step, where ground truth meshes are registered to a template, and directly learns the correspondences by optimizing an L1 loss instead of the commonly used Chamfer loss. We evaluated V2CC on several datasets, including ADNI, TRT, Mindboggle-101, and J-ADNI, and compared it to state-of-the-art methods. Our experimental results show that V2CC achieves comparable performance to previous methods in terms of surface reconstruction accuracy. However, V2CC improves intra-and inter-subject correspondence and disease classification based on cortical thickness. We have evaluated our proposed pre-processing step and loss function with V2C as the backbone network, but the underlying concepts are generic and could also be integrated in other methods like Topofit or CFPP."
Vertex Correspondence in Cortical Surface Reconstruction,,Fig. 1 .,
Vertex Correspondence in Cortical Surface Reconstruction,,Fig. 3 .,
Vertex Correspondence in Cortical Surface Reconstruction,,Fig. 4 .,
Vertex Correspondence in Cortical Surface Reconstruction,,Table 1 .,
Vertex Correspondence in Cortical Surface Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 31.
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,1,Introduction and Related Work,"Deep learning achieves great success in image-based disease classification. However, the computer-aided diagnosis is far from being solved when considering various requirements in real-world applications. As an important one, open set recognition (OSR) specifies that diseases unseen in training could appear in testing  There are many fields related to OSR but are essentially different. In classification with reject options  Most OSR researches focus on natural images, while medical OSR is still in its infancy. In medical fields, representative work like T3PO  This work tackles OSR under the assumption that known features could be assembled compactly in feature embedding space, and remaining sparse regions could be recognized as unknowns. Inspired by this, the Open Margin Cosine Loss (OMCL) is proposed merging two components, Margin Loss with Adaptive Scale (MLAS) and Open-Space Suppression (OSS). The former enhances known feature compactness and the latter recognizes sparse feature space as unknown. Specifically, MLAS introduces the angular margin to the loss function, which reinforces the intra-class compactness and inter-class separability. Besides, a learnable scaling factor is proposed to enhance the generalization capacity. OSS generates feature space descriptors that scatter across a bounded feature space. By categorizing them as unknowns, it opens a classifier by recognizing sparse feature space as unknowns and suppressing the overconfidence of the known. An embedding space example is demonstrated in Fig.  Considering medical OSR is still a nascent field, besides OMCL, we also proposed two publicly available benchmark datasets. One is microscopic images of blood cells, and the other is optical coherence tomography (OCT) of the eye fundus. OMCL shows good adaptability to different image modalities. Our contributions are summarized as follows. Firstly, we propose a novel approach, OMCL for OSR in medical diagnosis. It reinforces intra-class compactness and inter-class separability, and meanwhile recognizes sparse feature space as unknowns. Secondly, an adaptive scaling factor is proposed to enhance the generalization capacity of OMCL. Thirdly, two benchmark datasets are proposed for OSR. Extensive ablation experiments and feature visualization demonstrate the effectiveness of each design. The superiority over state-of-the-art methods indicates the effectiveness of our method and the adaptability of OMCL on different image modalities."
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,2,Method,"In Sect. 2.1, the open set problem and the formation of cosine Softmax are introduced. The two mechanisms MLAS and OSS are sequentially elaborated in Sect. 2.2 and 2.3, followed by the overall formation of OMCL in Sect. 2.4.  Cosine Loss: The cosine Softmax is used as the basis of the OMCL. It transfers feature embeddings from the Euclidian space to a hyperspherical one, where feature differences depend merely on their angular separation rather than spatial distance. Given an image x i , its vectorized feature embedding z i , and its label y i , the derivation progress of the cosine Softmax is"
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,2.1,Preliminaries,"where W j denotes the weights of the last fully-connected layer (bias is set to 0 for simplicity). W j = 1 and z i = s are manually fixed to constant numbers 1 and s by L2 normalization. s is named the scaling factor. cos(θ j,i ) denotes the angle between W j and z i . By doing so, the direction of W j could be regarded as the prototypical direction of class j as shown in Fig. "
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,2.2,Margin Loss with Adaptive Scale (MLAS),"MLAS serves three purposes. 1) By applying angular margin, the intra-class compactness and the inter-class separability are strengthened. 2) The threshold could represent the potential probability of the unknowns, which not only prepares for the open set but also learns more confident probabilities of the knowns. 3) A trainable scaling factor is designed to strengthen the generalization capacity. MLAS is: m, t, and s respectively denote margin, threshold, and learnable scaling factor, with corresponding geometric interpretation demonstrated in Fig.  The threshold t could be regarded as an extra dimension that prepares for unknown classes. Given the conventional input of Softmax as [q 1  i , q 2 i , ..., q C i ] ∈ R C , ours could be understood as [q 1  i , q 2 i , ..., q C i , t] ∈ R C+1 . Since t is added, the class-wise output q c i before Softmax is forced to have a higher value to avoid misclassification (at least larger than t). It reinforces more stringent learning and hence increases the feature compactness in the hyperspherical space. A large s makes the distribution more uniform, and a small s makes it collapses to a point mass. In this work, it is learnable, with a learning rate 0.1× the learning rate of the model. It theoretically offers stronger generalization capacity to various datasets and is experimentally observed to converge to different values in different data trails and could boost performances. LMCL "
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,2.3,Open-Space Suppression (OSS),"OSS generates feature space descriptors of bounded feature space. By categorizing them into an extra C + 1 class, samples in sparse feature space could be recognized as unknown and the overconfidence of the known is suppressed. OSS selects points scattered over the entire feature space, named descriptors, representing pseudo-unknown samples. Different from existing arts that generate pseudo-unknowns by learning from known samples, the OSS selects points scattered over the feature space. It guarantees all space could be possibly considered for simulating the potential unknowns. By competing with the known features, feature space with densely distributed samples is classified as the known, and the sparse space, represented by the descriptors, will be recognized as unknown. In this work, the corresponding descriptor set, with M samples, is  Most similar arts like AL "
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,2.4,Open Margin Cosine Loss (OMCL),"OMCL unifies MLAS and OSS into one formula, which is I i equals 1 if the i-th sample is training data, and equals 0 if it belongs to the feature space descriptors. λ is a weight factor. Since the output of the channel C +1 is fixed as t, no extra weights W C+1 are trained in the last fully-connected layer. As a result, OMCL does not increase the number of trainable weights in a neural network. During testing, just as in other works "
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,3,Result,
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,3.1,"Datasets, Evaluation Metrics, and Implementation Details",Two datasets are adapted as new benchmarks for evaluating the OSR problem. Following protocols in natural images 
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,3.2,Comparison with State-of-the-Art Methods,As demonstrated in Table 
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,3.3,Ablation Studies,"Effectiveness of MLAS and OSS : Table  Ablation Study of Adaptive Scaling Factor : Fig.  Ablation Study of Hyperparameters t, m, and λ: Fig.  Ablation Study of M : Fig.  Feature Visualization: Fig.  Moreover, samples of unknown classes tend to be pushed away from known classes, incidcating the effectiveness of our designs."
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,4,Conclusion,"In this paper, two publicly available benchmark datasets are proposed for evaluating the OSR problem in medical fields. Besides, a novel method called OMCL is proposed, under the assumption that known features could be assembled compactly in feature space and the sparse regions could be recognized as unknowns. The OMCL unifies two mechanisms, MLAS and OSS, into a unified formula. The former reinforces intra-class compactness and inter-class separability of samples in the hyperspherical feature space, and an adaptive scaling factor is proposed to empower the generalization capability. The latter opens a classifier by categorizing sparse regions as unknown using feature space descriptors. Extensive ablation experiments and feature visualization demonstrate the effectiveness of each design. Compared to recent state-of-the-art methods, the proposed OMCL performs superior, measured by ACC, AUROC, and OSCR."
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,,Fig. 1 .,
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,,,
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,,Fig. 2 .,
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,,Fig. 3 .,
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,,Fig. 4 .,
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,,Table 1 .,
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,,Table 2 .,
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_53.
Wasserstein Distance-Preserving Vector Space of Persistent Homology,1,Introduction,"Networks are ubiquitous representations for describing complex, highly interconnected systems that capture intricate patterns of relationships between nodes.  Persistent homology  Approaches that embed persistence diagrams into vector spaces  Recently, it was shown that persistence diagrams are inherently 1dimensional if the topological features of networks are limited to connected components and cycles, and that the Wasserstein distance between these diagrams has a closed form expression  In this work, we present a novel topological vector space (TopVS) that embeds 1-dimensional persistence diagrams representing connected components and cycles for networks of different sizes. Thus, TopVS enables topological machine learning with networks of different sizes and greatly expands the applicability of previous work. Importantly, TopVS preserves the Wasserstein distance in the original space of persistence diagrams. Preservation of the Wasserstein distance ensures the theoretical stability properties of persistence diagrams carry over to the proposed embedding. In addition to the robustness benefits, TopVS also enables the application of a wide variety of Euclidean metric-based learning methods to topological data analysis. Particularly, the utility of TopVS is demonstrated in topology-based classification problems using support vector machines. TopVS is illustrated by classifying measured functional brain networks based on data obtained from subjects with different numbers of electrodes. The results show that TopVS performs very well compared to other competing approaches."
Wasserstein Distance-Preserving Vector Space of Persistent Homology,2,Wasserstein Distance-Preserving Vector Space,
Wasserstein Distance-Preserving Vector Space of Persistent Homology,2.1,One-Dimensional Persistence Diagrams,"Define a network as an undirected weighted graph G = (V, w) with a set of nodes V and a weighted adjacency matrix w = (w ij ). Define a binary graph G with the identical node set V by thresholding the edge weights so that an edge between nodes i and j exists if w ij > . The binary graph is viewed as a 1-skeleton  Persistent homology keeps track of the birth and death of topological features over filtration values . A topological feature that is born at a filtration b i and persists up to a filtration d i , is represented by a point (b i , d i ) in a 2D plane. A set of all the points {(b i , d i )} is called persistence diagram  , respectively "
Wasserstein Distance-Preserving Vector Space of Persistent Homology,2.2,Closed-Form Wasserstein Distance for Different-Size Networks,"The Wasserstein distance between the 1-dimensional persistence diagrams can be obtained using a closed-form solution. Let G 1 and G 2 be two given networks possibly with different node sizes, i.e., their birth and death sets may differ in size. Their underlying empirical distributions on the persistence diagrams for connected components are defined in the form of Dirac masses  where δ(xb) is a Dirac delta centered at the point b. Then the empirical distribution functions are the integration of f G1,B and f G2,B as Then the empirical Wasserstein distance for connected components has a closed-form solution in terms of these pseudoinverses as Similarly, the Wasserstein distance for cycles W p,D (G 1 , G 2 ) is defined in terms of empirical distributions for death sets D(G 1 ) and D(G 2 ). The empirical Wasserstein distances W p,B and W p,D are approximated by computing the Lebesgue integration in (1) numerically as follows. G1,D (n/n)} be pseudoinverses for network G 1 sampled with partitions of equal intervals. Let B(G 2 ) and D(G 2 ) be sampled pseudoinverses for network G 2 with the same partitions of m and n, respectively. Then the approximated Wasserstein distances are given by For a special case when networks G 1 and G 2 have the same number of nodes, i.e., |B(G then exact computation of the Wasserstein distance is achieved using those birth and death sets, and setting m to the cardinality of the birth sets and n to that of the death sets."
Wasserstein Distance-Preserving Vector Space of Persistent Homology,2.3,Vector Representation of Persistence Diagrams,"A collection of 1-dimensional persistence diagrams together with the Wasserstein distance is a metric space. 1-dimensional persistence diagrams can be embedded into a vector space that preserves the Wasserstein metric on the original space of persistence diagrams as follows. Let G 1 , G 2 , ..., G N be N observed networks possibly with different node sizes. Let F -1  Gi,B be a pseudoinverse of network G i . The vector representation of a persistence diagram for connected components in network G i is defined as a vector of the pseudoinverse sampled at 1/m, 2/m, ..., m/m, i.e., v B,i Thus, for p = 1 the proposed vector space describes Manhattan distance, p = 2 Euclidean distance, and p → ∞ the maximum metric, which in turn correspond to the earth mover's distance (W 1 ), 2-Wasserstein distance (W 2 ), and the bottleneck distance (W ∞ ), respectively, in the original space of persistence diagrams. Similarly, we can define a vector space of persistence diagrams for cycles M D = {v D,i } N i=1 with the p-norm metric d p,D . The normed vector space (M B , d p,B ) describes topological space of connected components in networks, while (M D , d p,D ) describes topological space of cycles in networks. The topology of a network viewed as a 1-skeleton is completely characterized by connected components and cycles. Thus, we can fully describe the network topology using both M B and M D as follows. Let  where"
Wasserstein Distance-Preserving Vector Space of Persistent Homology,,2,". Thus, d p,× is a weighted combination of p-Wasserstein distances, and is simply the p-norm metric between vectors constructed by concatenating v B,i and v D,i . The normed vector space (M B × M D , d p,× ) is termed topological vector space (TopVS). Note the form of d p,× given in (4) results in an unnormalized mass after multiplying m and n by their reciprocals given in (  For a special case in which networks G 1 , G 2 , ..., G N have the same number of nodes, the vectors v B,i and v D,i are simply the original birth set B(G i ) and death set D(G i ), respectively, and the p-norm metric d p,× is expressed in terms of exact Wasserstein distances as"
Wasserstein Distance-Preserving Vector Space of Persistent Homology,3,Application to Functional Brain Networks,"Dataset. We evaluate our method using functional brain networks from the anesthesia study reported by  Classification Performance Evaluation. We are interested in whether candidate methods 1) can differentiate arousal states within individual subjects, and 2) generalize their learned knowledge to unknown subjects afterwards. As a result, we consider two different nested cross validation (CV) tasks as follows. 1. For the first task, we classify a collection of brain networks belonging to each subject separately. Specifically, we apply a nested CV comprising an outer loop of stratified 2-fold CV and an inner loop of stratified 3-fold CV. Since we may get a different split of data folds each time, we perform the nested CV for 100 trials and report an average accuracy score and standard deviation for each subject. We also average these individual accuracy scores across subjects (11 × 100 scores) to obtain an overall accuracy. 2. For the second task, we use a different nested CV comprising both outer and inner loops with a leave-one-subject-out scheme. That is, a classifier is trained using all but one test subject. The inner loop is used to determine optimal hyperparameters, while the outer loop is used to assess generalization capacity of the candidate methods to unknown subjects in the population. Method Comparison. Brain networks are used to compare the classification performance of the proposed TopVS relative to that of five state-of-the-art kernel methods and two well-established graph neural network methods. Three of these kernel methods are based on conventional 2-dimensional persistence diagrams for connected components and cycles: the persistence image (PI) vectorization  In addition, we also evaluate two well-established graph neural network methods including graph convolutional networks (GCN)  Results. Results for the first task are summarized in Fig.  In addition, we compute confusion matrices to gain insights into the acrosssubject predictions for the second task, as displayed in Fig. "
Wasserstein Distance-Preserving Vector Space of Persistent Homology,,Potential Impact and,"Limitation. An open problem in neuroscience is identifying an algorithm that reliably extracts a patient's level of consciousness from passively recorded brain signals (i.e., biomarkers) and is robust to inter-patient variability, including where the signals are recorded in the brain. Conveniently, the anesthesia dataset is labeled according to consciousness state, and electrode placement (node location) was dictated solely by clinical considerations and thus varied across patients. Importantly, the relatively robust performance across patients suggests there are reliable topological signatures of consciousness captured by TopVS. The distinction between Wake and Sedated states involves relatively nuanced differences in connectivity, yet TopVS exploits the subtle differences in topology that differentiate these states better than the com-Fig.  peting methods. Our results suggest that the neural correlates of consciousness can be captured in measurements of brain network topology, a longstanding problem of great significance. Additionally, TopVS is a principled framework that connects persistent homology theory with practical applications. Our versatile vector representation can be used with various vector-based statistical and machine learning models, expanding the potential for analyzing extensive and intricate networks beyond the scope of this paper. While TopVS is limited to representing connected components and cycles, assessment of higher-order topological features beyond cycles is of limited value due to their relative rarity and interpretive challenges, and consequent minimal discriminitive power "
Wasserstein Distance-Preserving Vector Space of Persistent Homology,,"2 ,",
Wasserstein Distance-Preserving Vector Space of Persistent Homology,,Fig. 1 .,
Wasserstein Distance-Preserving Vector Space of Persistent Homology,,Fig. 2 .,
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,1,Introduction,"Studies on brain functional dynamics show that the brain network contains a variety of distinct functional configurations (brain states) during the course of brain cognition activities  Recently, graph neural networks (GNNs) have been proven to be helpful in brain network analysis, due to their powerful ability in analyzing graphstructured data  To address the above issues, we propose a learnable subdivision graph neural network to investigate brain networks with heterogeneous functional signatures, and the functional brain state can be combined with corresponding latent subspace for interpretable brain disorder diagnosis. The main contributions of this paper are as follows: 1)We propose a novel Learnable Subdivision Graph Neural Network (LSGNN) model for brain network analysis, which implements the extraction of heterogeneous features of brain networks under various functional configurations. 2) We develop a novel assignment method, that can encode brain networks into multiple latent feature subspaces in a learnable way. 3) Our method instills the interpretability of the latent space corresponding to brain states, which is beneficial to unveiling insights into the relationship between the signature of functional brain networks and cognitive disorder diagnosis."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,2,Method,The framework of the LSGNN method is presented in Fig. 
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,2.1,Preliminary,"Problem Definition. The input of proposed model is a set of N weighted brain networks. For each network G = {V, E, A}, V = {v i } M i=1 involves M nodes defined by the ROIs on a specific brain parcellation  Here, the readout function can be performed using the average pool."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,2.2,Functional Subdivision Block,"Considering the heterogeneity of brain networks induced by intrinsic functional brain states, we propose to encode brain networks into multiple latent feature subspaces corresponding to functional brain states. The key to achieve this goal is to build an assignment matrix that can allocate the feature representation of each dimension in the brain network to the latent subspace related to distinct brain states. Here, we innovatively design a learnable assignment method, which is determined by the embedding matrix and automatically updated with the training of the whole model. Specifically, we first utilize two separate GNN layers to generate embeddings for preliminary node feature matrix H F = GNN feat (A, H; θ feat ) and input embeddings of assignment matrix H S = GNN assig (A, H; θ assig ), respectively. Note that these two GNN layers use the same data as input, but have distinct parameterizations and play separate roles. We continue to transpose the input embeddings of assignment matrix as H T S ∈ R D×M , and the generation of assignment matrix S ∈ R D×C as follows: where the softmax function is applied in a row-wise fashion, and the output dimension of MLP 1 corresponds to a pre-defined number C of brain states. With the assignment matrix and node embedding matrix in hand, we further extract distinct node embeddings in different latent subspaces corresponding to brain states. For each column {S c } C c=1 in the assignment matrix, it is essentially a mask vector where each element represents the probability that the feature is assigned to the brain state c. Therefore, we can obtain the brain network representation in each brain state through the product of feature and its corresponding assignment probability. where R is a repeat function that extends the mask vector to the same dimension of the node embedding matrix, denotes element-wise multiplication, and MLP 2 maps each new node embedding matrix to distinct feature latent subspaces, which characterize heterogeneous information under different brain states."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,2.3,Functional Aggregation Block,"After assigning the node embedding matrix of brain network into distinct latent subspaces, we utilize multiple GNN modules ( each consists of a GNN layer and a graph pooling layer) to obtain the graph embedding of brain network H c ∈ R 1×D corresponding to each brain state respectively. These GNN layers are performed with shared weight to reduce the complexity of the model. Considering that each brain state has different contributions to the final brain network representation, we propose a functional aggregation block based on the self-attention mechanism to aggregate the information into a joint latent space, and acquire a comprehensive brain network representation for brain disorder diagnosis. In practice, the graph embedding in every brain state is first packed together into a matrix where the weight matrices for W q , W k , W v are the learned linear mappings. Therefore, we can calculate the self-attention by mapping a query and a set of key-value pairs, and combining with a mean pooling to obtain the whole graph embedding vector H ∈ R 1×D for the brain network. Here, the dot product is adopted to reduce computational and storage costs, softmax is used to normalize the self-attention and the scale √ D prevents the saturation led by softmax function. Finally, we fed the whole graph embedding vector into a fully connected layer to predict the diagnostic result ŷn ."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,2.4,Objective Function,"In this work, we design an objective function composed of three components: First, we employ conventional supervised cross-entropy objective towards ground-truth y n disorder prediction, defined as Second, since each row of the assignment matrix S i represents the probability of allocating the feature of this dimension to different latent subspaces, it should generally be close to a one-hot vector, i.e., each dimension feature is assigned to each latent subspace. Therefore, we design an entropy loss to reduce the uncertainty of mapping distribution by minimizing entropy H Si : Finally, to ensure the generalization ability of the model and reduce over-fitting, we add an L2 normalization term. To summarize, the total loss of the proposed model can be formulated as: where α, β are hyperparameters that determine the relative importance of feature fusion loss items."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,3,Experiments,
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,3.1,Dataset and Experimental Settings,"We evaluate our framework on publicly available Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset  We compare our proposed model with five different methods, including one conventional model (1) SVM  All deep learning experiments are conducted on NVIDIA GeForce GTX TITAN X GPUs with PyTorch "
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,3.2,Result Analysis,Table 
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,3.3,Ablation Study,"We conduct ablation studies to verify the effectiveness of 1) the learnable assignment method in the FSB module, 2) the self-attention mechanism-based fusion method in the FAB module, and 3) entropy loss L E . Specifically, In the FSB module, we compare the proposed learnable method with a fixed method, where we utilize k-means to cluster feature embedding into several latent subspaces with the same number as the learnable assignment matrix done. In the FAB module, we try a typically simple feature fusion method (i.e., feature concatenation) without self-attention. For the loss function, we conduct comparative experiments on whether to include entropy loss. The classification results of all six methods on the N-E task are listed in Table  Then, for each brain state c, we extract node embedding matrix H Fc and construct a corresponding functional connectivity matrix using Pearson correlation. Finally, we obtain distinct brain states by computing the average results of testing samples on the N-E task. From Fig. "
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,4,Conclusion,"In this paper, we propose a novel learnable subdivision graph neural network method for functional brain network analysis and interpretable cognitive disorder diagnosis. Specifically, brain networks are embedded into multiple latent feature subspaces corresponding to functional configurations in a learnable way. Experimental results of the cognitive disorder diagnosis tasks verify the effectiveness of our proposed method. A direct future direction based on this work is to utilize heterogeneous graph construction techniques to describe brain network patterns. This allows for consideration of brain network heterogeneity from the initial step of brain network modeling, which could lead to a better understanding of brain networks."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,,Fig. 1 .,
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,,Fig. 2 .,
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,,Table 1 .,
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,,Table 2 .,
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_6.
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,1,Introduction,"The human brain undergoes dramatic changes in size, shape, and tissue architecture during the first postnatal year, driven by cellular processes  CSR methods developed so far include FreeSurfer  In this paper, we propose SurfFlow, a geometric deep learning model designed to reconstruct accurate cortical surfaces from infant brain MRI. Our model comprises three cascaded deformation blocks, each responsible for predicting a flow field and constructing a diffeomorphic mapping for each vertex through solving a flow ordinary differential equation (ODE). The flow fields deform template meshes progressively to finally produce accurate genus-zero cortical surfaces. Our work offers a threefold contribution. First, we propose an efficient dualmodal flow-based CSR method, enabling the creation of high-resolution and high-quality mesh representations for complex cortical surfaces. Second, we propose a novel loss function that effectively regularizes the lengths of mesh edges, leading to substantial improvements in mesh quality. Third, our method represents the first attempt in tackling the challenging task of directly reconstructing cortical surfaces from infant brain MRI. Our method outperforms state-of-theart methods by a significant margin, judging based on multiple surface evaluation metrics."
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,2,Methods,
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,2.1,Overview,"As depicted in Fig.  where θ denotes the parameters of the 3D Unet, x 0 is the initial mesh before deformation in each stage. SurfFlow is trained stage-wise: a deformation block is trained with all previous deformation blocks frozen. Following CorticalFlow++ "
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,2.2,Dual-Modal Input,"Infant MRI exhibits three distinct phases during the first year of life. In the infantile phase (≤5 months), gray matter (GM) shows higher signal intensity than white matter (WM) in T1w images. The isointense phase (5-8 months) corresponds to an increase in intensity of WM owing to myelination associated with brain maturation. This significantly lowers the contrast between GM and WM in T1w images and similarly T2w images. In the early adult-like phase (≥ 8 months), the GM intensity is lower than WM in T1w images, similar to the tissue contrast in adult MRI. We propose to use both T1w and T2w images for complementary information needed for accurate surface reconstruction."
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,2.3,Loss Function,"The loss function used for training SurfFlow is a weighted sum of Chamfer distance (CD; L cd ) and a piecewise edge length loss (PELL; L e ), balanced by parameter λ: Chamfer Distance Loss. The Chamfer distance is commonly used as the loss function for surface reconstruction. It measures the distance from a vertex in one mesh P to the closest vertex in another mesh Q bidirectionally: where p and q are vertices in P and Q, respectively. Piecewise Edge Length Loss. We observed that vertices tend to be clustered in surfaces generated by CorticalFlow++ (Fig.  where P denotes the predicted mesh, p is a vertex in P , N (p) consists of the neighbors of p, and and γ are two tunable hyper-parameters that control the range position and width, respectively."
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,2.4,Deformation Computation in DMD Modules,"Following CorticalFlow++, we use the fourth-order Runge-Kutta method to solve the stationary flow ODE over time interval [0, 1] for both accuracy and stability. The solution x 1 is obtained iteratively with where 2 ), and k 4 = V(x tn + k 3 h). V(x) represents the trilinear interpolation of the flow field at position x. h is the step size and is set to 1/30. t n is the time at n-th step, and t n+1 = t n + h."
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,2.5,Implementation Details,Our network was trained stage-wise. We froze the parameters of one deformation block once trained and then start the training of the next block. Each deformation block was trained for 27k iterations. Adam optimizer was used with an initial learning rate of 0.0001. The parameter λ of the loss function was set to 3.0 to balance the two loss terms. We set for PELL based on the average edge length determined from the training set. We set the γ to 4.0 and to 5 × 10 -5 . Instance normalization (IN) 
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,3,Results,
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,3.1,Data,"The dataset includes aligned T1w and T2w image pairs from 121 subjects, 2 weeks to 12 months of age, from the Baby Connectome Project (BCP)  To obtain a smooth starting template, an average convex hull computed from the training dataset was re-meshed and triangularized. The Catmull-Clark subdivision algorithm was then applied to generate enough faces and vertices. Decimation was used to control the number of vertices. These steps were carried out in Blender"
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,3.2,Evaluation Metrics,"For performance evaluation and comparison between different methods, we utilized the following metrics: Chamfer distance (CD), average symmetric surface distance (ASSD), 90% Hausdorff distance (HD), and normal consistency (NC). Their definitions are as follows: where P and Q are respectively the predicted and ground truth (GT) meshes, n p and n q are the normals at p and q, n pq is the normal of the vertex in Q that is closest to p, and n qp is the normal of the vertex in P that is closest to q. "
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,3.3,Results,"We compared SurfFlow with CorticalFlow++ and DeepCSR. To ensure a fair comparison, we modified both CorticalFlow++ and DeepCSR to use dual-modal inputs. Specifically, for DeepCSR, we utilized the finest possible configuration, generating a 512 3 3D grid to predict the signed distance field for surface reconstruction using the marching cube algorithm. As depicted in Table  SurfFlow utilizes PELL to ensure that mesh edge lengths are within the desired range. We observed that this optimization leads to smoother, more uniform, and accurate meshes. In contrast, CorticalFlow++ shows limited accuracy in certain mesh areas, primarily due to its inability to ensure mesh uniformity. This is evident in the zoomed-in areas depicted in Fig. "
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,3.4,Ablation Study,The results of an ablation study (Table 
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,4,Conclusion,We presented SurfFlow-a flow-based deep-learning network to accurately reconstruct cortical surfaces. SurfFlow predicts a flow field to deform a template surface toward a target surface. It produces smooth and uniform surface meshes with sub-millimeter accuracy and outperforms CorticalFlow++ and DeepCSR in terms of surface accuracy and regularity.
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,,Fig. 1 .,
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,,R 1 .Fig. 2 .,
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,,Fig. 3 .,
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,,Fig. 4 .,
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,,Table 1 .,
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,,Table 2 .,
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,1,Introduction,"Diffusion MRI (dMRI) is a powerful medical imaging tool for probing microstructural information based on the restricted diffusion assumption of the water molecules in biological tissues  Advanced dMRI models are typically multi-compartment models with highly non-linear and complex formulations. Accurate parameter estimation from such models requires dense q-space and/or t-space sampling. Deep learning techniques have been proposed to improve estimation accuracy from downsampled q-space data  The previous q-space learning networks  In this work, we proposed a reweighting strategy to reduce the negative effects of noisy label based on uncertainty. Our contributions can be summarized below: 1. We brought up an important problem of the noisy label in dMRI model estimation which was not addressed before. 2. We proposed an attention-based sparse encoder to make the network focus on the key diffusion signal out of many q-space or t-space signals. 3. We developed an uncertainty-based reweighting strategy considering the uncertainty in both dMRI channels and spatial domain for microstructural estimation. 4. We proposed an end-to-end estimation strategy in both q-space and t-space with downsampled q-t space data. In our work, we firstly demonstrated the effectiveness of our attention-based reweighting strategy on a simulation dataset, and then we evaluated our work with three different downsampling strategies (q-space, t-space, and q-t space) on a TD-dMRI dataset of normal and injured rat brains. We tested a TD-dMRI model which estimates the transmembrane water exchange time based on timedependent diffusion kurtosis "
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,2,Method,
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,2.1,q-t Space Sparsity,"In this study, we extended the q-space learning model AEME  where  is the diffusion signal normalized by b0 at different t d , and X ∈ R NΓ×NΥ is the matrix of the mixed sparse dictionary coefficients. Γ ∈ R K×NΓ and Υ ∈ R V ×NΥ are decomposed dictionaries that encode the information in the mixed q-t domain and the spatial domain. H is the noise corresponding to X. Then the sparse encoder can be formulated using the extragradient-based method similar to  where, A 1 denotes a scalar matrix, AU A(•) is the adaptive uncertainty attention function, and H M denotes a nonlinear operator corresponding to the threshold layer in Fig.  So far, we can obtain the sparse representation of the signal."
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,2.2,Adaptive Uncertainty Attention Modelling,"Uncertainty Attention Module. Inspired by the uncertainty modeling mechanism of Bayesian neural networks  CUA is a cluster of different sparse representations of X after dropout in the CUA module, and k is the number of stochastic forwards. Similarly, the SUA module can be defined as: X SUA = SU A(X CUA ). SU A(•) is the spatial-wise uncertainty attention, which is used to model the uncertainty of the spatial-wise information. The SUA of X CUA can be estimated as follows: . U S is the uncertainty of the spatial-wise information, X k SUA is a cluster of different sparse representations of X CUA after dropout in the SUA module. For simplicity and efficiency, in practice, we combined these two kinds of uncertainty together to reweight the whole sparse representation X as: U = V ar(X k ). U is the uncertainty of the sparse representation, X k is a cluster of different sparse representations of X after dropout in both CUA and SUA modules. Adaptive Reweight Mechanism. In this work, we proposed an adaptive reweighting strategy by lowering the loss weight for a patch that may be corrupted by the noise. After the uncertainty U is approximated, We can set a weight tenor U w as below: Then, the impact of noise can be mitigated by an adaptive weight matrix R: where, t is a trainable parameter in the network, which can be modified adaptively. Then X will be reweighted by the R in the loss function as: where, denotes the element-wise multiplication, M (•) is a mapping function corresponding to the mapping networks, P is the observed label of the estimated dMRI model parameters. Network Construction. Following the q-t space sparsity analysis and the adaptive uncertainty mechanism mentioned above, we can incorporate historical information  where,  , and G n+1 and they can be defined as  The overall network can be constructed by repeating the AUA-SE unit n times, and the output will be sent to the mapping networks for mapping the microstructure parameters, which consist of three fully connected layers of the feed-forward networks "
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,2.3,Dataset and Training,"The tDKI model is defined as below  where, K(t) is the kurtosis at different t d , K 0 is the kurtosis at t d = 0, t is the t d and τ m is the water mixing time. K(t) at individual t d is obtained according to the DKE method  Simulation Dataset was formed following the method of Barbieri et al.,  In order to replicate the noisy label problem, we varied the noise level from SNR=10 to 30 in the t-space signal. In the training data, we used a Bayesian method modified from Gustafsson et al.  Rat Brain Dataset was collected on a 7T Bruker scanner from 3 normal rats and 10 rats that underwent a model of ischemic injury by transient Middle Cerebral Artery Occlusion (MCAO). Diffusion gradients were applied in 18 directions per b-value at 3 b-values of 0.8, 1.5, and 2.5 ms/ μ m 2 and 5 t d (50, 80, 100, 150, and 200 ms) with the following acquisition parameters: repetition time/echo time = 2207/18 ms, in-plane resolution = 0.3 × 0.3 mm 2 , 10 slices with a slice thickness of 1 mm. In order to get the gold standard, the DKE toolbox  Training. In this work, the dictionary size of our AUA-dE was set at 301, and the hidden size of the fully connected layer was 75. We used an early stopping strategy and a reducing learning rate with an initial learning rate of 1 × 10 -4 . AdamW was selected as the optimizer with a batch size of 256. The dropout in the AUA-SE was 0.2 for 50 forward processes."
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,3,Experiments and Results,
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,3.1,Ablation Study,"We compared four different methods, including AEME  Here, we used the relative error (percentage of the gold standard) to compare different algorithms. Figure "
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,3.2,Performance Test,"q -Space Downsampling. We used our AUA-dE to estimate kurtosis at different t d using the downsampled q-space, in comparison with an optimization based method DKE  t-Space Downsampling. In the t-space downsampling performance experiment, we used our AUA-dE to estimate the K 0 and τ m based on K(t) at varying t d . We compared our method with the Bayesian method  From Table "
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,4,Conclusion,"In this work, we proposed an adaptive uncertainty guided attention for diffusion MRI models estimation (AUA-dE) to address the noisy label problem in the estimation of TD-dMRI-based microstructural models. We tested our proposed network and module in a rat brain dataset and a simulation dataset. The proposed method showed the highest estimation accuracy in all of these datasets. Meanwhile, we demonstrated its performance on jointly downsampled q-t space data, for which previous algorithms did not work well with the highly accelerated setup (270/54). In the future, we will further investigate our proposed AUA module as a plug-in in different dMRI model estimation networks and also different dMRI models to test its generalizability and robustness."
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,,c,
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,,Fig. 1 .,
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,,Fig. 2 .,
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,,Fig. 3 .,
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,,Table 1 .,
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,,Table 2 .,
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,1,Introduction,"Nuclei detection is a highly challenging task and plays an important role in many biological applications such as cancer diagnosis and drug discovery. Rectangle object detection approaches that use CNN have made great progress in the    last decade  Recently, DETR  In this paper, we introduce CircleFormer, a Transformer-based circular object detection for medical image analysis. Inspired by DAB-DETR, we propose to use an anchor circle (x, y, r) as the query for circular object detection, where (x, y) is the center of the circle and r is the radius. We propose a novel circle cross attention module which enables us to apply circle center (x, y) to extract image features around a circle and make use of circle radius to modulate the cross attention map. In addition, a circle matching loss is adopted in the set-to-set prediction part to process circular predictions. In this way, our design of Circle-Former lends itself to circular object detection. We evaluate our CircleFormer on the public MoNuSeg dataset for nuclei detection in whole slide images. Experimental results show that our method outperforms both CNN-based methods for box detection and circular object detection. It also achieves superior results compared with recently Transformer-based box detection approaches. Meanwhile, we carry out ablation studies to demonstrate the effectiveness of each proposed component. To further study the generalization ability of our approach, we add a simple segmentation branch to CircleFormer following the recent query based instance segmentation models "
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,2,Method,
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,2.1,Overview,Our CircleFormer (Fig. 
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,2.2,Representing Query with Anchor Circle,"Inspired by DAB-DETR, we represent queries in Transformer-based circular object detection with anchor circles. We denote C i = (x i , y i , r i ) as the i-th anchor, x i , y i , r i ∈ R. Its corresponding content part and positional part are Z i ∈ R D and P i ∈ R D , respectively. The positional query P i is calculated by: where positional encoding (PE) generates embeddings from floating point numbers, and the parameters of the MLP are shared among all layers. In Transformer decoder, the self-attention and cross-attention are written as: Self-Attn : Cross-attn : where F x,y ∈ R D denote the image feature at position (x, y) and an MLP (csq) : R D → R D is used to obtain a scaled vector conditioned on content information for a query. By representing a circle query as (x, y, r), we can refine the circle query layer-by-layer in the Transformer decoder. Specifically, each Transformer decoder estimates relative circle information (Δx, Δy, Δr). In this way, the circle query representation is suitable for circular object detection and is able to accelerate the learning convergence via layer-by-layer refinement scheme."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,2.3,Circle Cross Attention,We propose circle-modulated attention and deformable circle cross attention to consider size information of circular object detection in cross attention module. Circle-modulated Attention. The circle radius modulated positional attention map provides benefits to extract image features of objects with different scales.
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,"MA((x, y), (x ref , y","where r i is the radius of the circle anchor A i , and r i,ref is the reference radius calculated by Deformable Circle Cross Attention. We modify standard deformable attention to deformable circle cross attention by applying radius information as constraint. Given an input feature map F ∈ R C×H×W , let i index a query element with content feature Z i and a reference point P i , the deformable circle cross attention feature is calculated by: where m indexes the attention head, k indexes the sampled keys. M and K are the number of multi-heads and the total sampled key number. W m ∈ R D×d , W m ∈ R d×D are the learnable weights and d = D/M . Attn mik denotes attention weight of the k th sampling point in the m th attention head. Δr mik and Δθ mik are radius offset and angle offset, r i,ref is the reference radius. In circle deformable attention, we transform the offset in polar coordinates to Cartesian coordinates so that the reference point ends up in the circle anchor. Rather than initialize the reference points by uniformly sampling within the rectangle as does Deformable DETR, we explore two ways to initialize the reference points within a circle, random sampling (CDA-r) and uniform sampling (CDA-c) (As in Fig. "
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,2.4,Circle Regression,"A circle is predicted from a decoder embedding as ĉi = sigmoid(FFN(f i ) + [A i ]), where f is the decoder embedding. ĉi = (x, ŷ, r) consists of the circle center and circle radius. sigmoid is used to normalize the prediction ĉ to the range [0, 1]. FFN aims to predict the unnormalized box, A i is a circle anchor."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,2.5,Circle Instance Segmentation,"A mask is predicted from a decoder embedding by mi = FFN(FFN(f i ) + f i ), where f is the decoder embedding. mi ∈ R 28×28 is the predicted mask. We use dice and BCE as the segmentation loss: ) between prediction mi and the groundtruth m i ."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,2.6,Generalized Circle IoU,"CircleNet extends intersection over union (IoU) of bounding boxes to circle IoU (cIoU) and shows that the cIOU is a valid overlap metric for detection of circular objects in medical images. To address the difficulty optimizing non-overlapping bounding boxes, generalized IoU (GIoU) "
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,CC,", where C A and C B denotes two circles, and C C is the smallest circle containing these two circles. We show that gCIoU can bring consistent improvement on circular object detection. Figure  Circle Training Loss. Following DETR, i-th each element of the groundtruth set is y i = (l i , c i ), where l i is the target class label (which may be ∅) and c i = (x, y, r). We define the matching cost between the predictions and the groundtruth set as:  Finally, the overall loss is: where σ(i) is the index of prediction ŷ corresponding to the i-th ground truth y after completing the match. m i is the ground truth obtained by RoI Align "
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,3,Experiment,
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,3.1,Dataset and Evaluation,MoNuSeg Dataset. MoNuSeg dataset is a public dataset from the 2018 Multi-Organ Nuclei Segmentation Challenge  Evaluation Metrics. We use AP for nuclei detection evaluation metrics as in CircleNet 
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,3.2,Implementation Details,"Two variants of our proposed method for nuclei detection, CircleFormer and CircleFormer-D are built with a circle cross attention module and a deformable circle cross attention module, respectively. CircleFormer-D-Joint (Ours) extends CircleFormer-D to include instance segmentation as additional output. All the models are with ResNet50 as backbone and the number of Transformer encoders and decoders is set to 6. The MLPs of the prediction heads share the same parameters. Since the maximum number of objects per image in the dataset is close to 1000, we set the number of queries to 1000. The parameter of focal loss for classification is set to α = 0.25, γ = 0.1. λ focal is set to 2.0 in the matching step and λ focal = 1.0 in the final circle loss. We use λ iou = 2.0, λ c = 5.0, λ dice = 8.0 and λ bce = 2.0 in the experiments. All the models are initialized with the COCO pre-trained model "
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,3.3,Main Results,"In Table jointly outputs detection and segmentation results additionally boosts the detection results of CircleFormer-D. Experiments of joint nuclei detection and segmentation are listed in Table  To summarize, our method with only detection head outperforms both CNNbased methods and Transformer based approaches in most evaluation metrics for circular nuclei detection task. Our CircleFormer-D-Joint provides superior results compared to CNN-based and Transformer-based instance segmentation methods. Also, our method with joint detection and segmentation outputs also improves the detection-only setting. We have provided additional visual analysis in the open source code repository."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,3.4,Ablation Studies,"We conduct ablation studies with CircleFormer on the nuclei detection task (Table  In CircleFormer, the proposed circle-Modulated attention (c-MA) improves the performance of box AP from 45.7% to 48.6% box AP (Row 1 and Row 2 in P1). We replaced circle IoU (CIoU) loss with generalized circle IoU (gCIoU) loss, the performance is further boosted by 2.2% (Row 2 and Row 3 in P1). We obtain similar observations of CircleFormer-D. When using standard deformable attention (SDA), learning cIoU loss gives a 1.2% improvement on box AP compared to using box IoU (Row 1 and Row 2 in P2). Replacing CIoU with gCIoU, the performances of SDA (Row 2 and Row 5 in P2), CDA-r (Row 3 and Row 6 in P2) and CDA-c (Row 4 and Row 7 in P2) are boostd by 0.3% box AP, 0.7% box AP and 1.8% box AP, respectively. Results show that the proposed gCIoU is a favorable loss for circular object detection. Two multi-head initialization methods, random sampling (CDA-r) and uniform sampling (CDA-c), achieve similar results (Row 3 and Row 4 in P2) and both surpass SDA by 0.3% box AP (Row 2 and Row 3 in P2). By using gCIoU, CDA-r and CDA-c initialization methods surpasses SDA 1.1% box AP (Row 5 and Row 6 in P2), and 1.8% box AP (Row 5 and Row 7 in P2), respectively. Numbers of Multi-head and Reference Points. We discuss how the number of Multi-heads in the Decoder affects the CircleFormer-D-DETR model. We vary the number of heads for multi-head attention and the performance of the model is shown in the Table "
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,4,Conclusion,"In this paper, we introduce CircleFormer, a Transformer-based circular medical object detection method. It formulates object queries as anchor circles and refines them layer-by-layer in Transformer decoders. In addition, we also present a circle cross attention module to compute the key-to-image similarity which can not only pool image features at the circle center but also leverage scale information of a circle object. We also extend CircleFormer to achieve instance segmentation with circle detection results. To this end, our CircleFormer is specifically designed for circular object analysis with DETR scheme."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,Fig. 1 .,
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,Fig. 3 .,
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,Fig. 4 .,
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,Fig. 2 .,
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,Table 1 .,
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,Table 2 .,
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,Table 3 .,
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,Table 4 .,
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,Table 5 .,
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,1,Introduction,"Live-cell microscopy is a fundamental tool to study the spatio-temporal dynamics of biological systems  In this paper we investigate whether time arrow prediction, i.e. the prediction of the correct order of temporally shuffled image frames extracted from live-cell microscopy videos, can serve as a suitable pretext task to generate meaningful representations of microscopy images. We are motivated by the observation that for most biological systems the temporal dynamics of local image features are closely related to their semantic content: whereas static background regions are time-symmetric, processes such as cell divisions or cell death are inherently timeasymmetric (cf. Fig. "
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,2,Method,"Our proposed Tap pre-training takes as input a set {I} of live-cell microscopy image sequences I ∈ R T ×H×W with the goal to produce a feature extractor f that generates c-dimensional dense representations z = f (x) ∈ R c×H×W from single images x ∈ R H×W (cf. Fig.  Both f and h are trained jointly to minimize the loss where L BCE denotes the standard softmax + binary cross-entropy loss between the ground truth label y and the logits ŷ = h(z), and L Decorr is a loss term that promotes z to be decorrelated across feature channels  Here z ∈ R c×2hw denotes the stacked features z flattened across the non-channel dimensions, and τ is a temperature parameter. Throughout the experiments we use λ = 0.01 and τ = 0.2. Note that instead of creating image pairs from consecutive video frames we can as well choose a custom time step Δt ∈ N and sample x 1 ⊂ I t and x 2 ⊂ I t+Δt , which we empirically found to work better for datasets with high frame rate."
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,,Permutation-equivariant Time Arrow Prediction Head:,"The time arrow prediction task has an inherent symmetry: In other words, h should be equivariant wrt. to permutations of the input. In contrast to common models (e.g. ResNet  with weight matrices L, G ∈ R c×c and a non-linear activation function σ. Note that L operates independently on each temporal axis and thus is trivially permutation equivariant, while G operates on the temporal sum and thus is permutation invariant. The last layer h L includes an additional global average pooling along the spatial dimensions to yield the final logits ŷ ∈ R 2 . Augmentations: To avoid overfitting on artificial image cues that could be discriminative of the temporal order (such as a globally consistent cell drift, or decay of image intensity due to photo-bleaching) we apply the following augmentations (with probability 0.5) to each image patch pair x 1 , x 2 : flips, arbitrary rotations and elastic transformations (jointly for x 1 and x 2 ), translations for x 1 and x 2 (independently), spatial scaling, additive Gaussian noise, and intensity shifting and scaling (jointly+independently)."
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,3,Experiments,
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,3.1,Datasets,To demonstrate the utility of Tap for a diverse set of specimen and microscopy modalities we use the following four different datasets: HeLa. Human cervical cancer cells expressing histone 2B-GFP imaged by fluorescence microscopy every 30 min  Mdck. Madin-Darby canine kidney epithelial cells expressing histone 2B-GFP (cf. Fig.  Flywing. Drosphila melanogaster pupal wing expressing Ecad::GFP (cf. Fig.  We use Δt = 1. Yeast. S. cerevisiae cells (cf. Fig.  For each dataset we heuristically choose Δt to roughly correspond to the time scale of observable biological processes (i.e. larger Δt for higher frame rates). 
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,3.2,Implementation Details:,For the feature extractor f we use a 2D U-Net 
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,3.3,Time Arrow Prediction Pretraining,"We first study how well the time arrow prediction pretext task can be solved depending on different image structures and used data augmentations. To that end, we train Tap networks with an increasing number of augmentations on HeLa and compute the Tap classification accuracy for consecutive image patches x 1 , x 2 that contain either background, interphase (non-dividing) cells, or mitotic (dividing) cells. As shown in Fig.  Next we investigate which regions in full-sized videos are most discriminative for Tap. To that end, we apply a trained Tap network on consecutive fullsized frames x 1 , x 2 and compute the dense attribution map of the classification logits y wrt. to the Tap representations z via Grad-CAM "
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,3.4,Downstream Tasks,"We next investigate whether the learned Tap representations are useful for common supervised downstream tasks, where we especially focus on their utility in the low training data regime. First we test the learned representations on two image-level classification tasks, and later on two dense segmentation tasks. Mitosis Classification on Flywing: Since Tap attribution maps strongly highlight cell divisions, we consider predicting mitotic events an appropriate first downstream task to evaluate Tap. To that end, we generate a dataset of 97k crops of size 2 × 96 × 96 from Flywing and label them as mitotic/nonmitotic (16k/81k) based on available tracking data  Emerging Bud Detection on Yeast: Finally, we test Tap on the challenging task of segmenting emerging buds in phase contrast images of yeast colonies. We train Tap networks on Yeast and generate a dataset of 1205 crops of size 5 × 192 × 192 where we densely label yeast buds in the central frame (defined as buds that appeared less than 13 frames ago) based on available segmentation data "
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,4,Discussion,"We have presented Tap, a self-supervised pretraining scheme that learns biologically meaningful representations from live-cell microscopy videos. We show that Tap uncovers sparse time-asymmetric biological processes and events in raw unlabeled recordings without any human supervision. Furthermore, we demonstrate on a variety of datasets that the learned features can substantially reduce the required amount of annotations for downstream tasks. Although in this work we focus on 2D+t image sequences, the principle of Tap should generalize to 3D+t datasets, for which dense ground truth creation is often prohibitively expensive and therefore the benefits of modern deep learning are not fully tapped into. We leave this to future work, together with the application of Tap to cell tracking algorithms, in which accurate mitosis detection is a crucial component."
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,,Fig. 1 .,
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,,Fig. 2 .,
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,,Fig. 3 .,
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,,Fig. 4 .,
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,,,
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,,Fig. 5 .,
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_52.
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,1,Introduction,"A commonly used method to observe the dynamics of subcellular structures, such as microtubule tips, receptors, and vesicles, is to label them with fluorescent probes and then collect their videos using a fluorescence microscope. Since these subcellular structures are often smaller than the diffraction limit of visible light, they often appear as individual particles with Airy disk-like patterns in fluorescence microscopy images, as shown e.g., in Fig.  Most single particle tracking methods follow a two-step paradigm: particle detection and particle linking. Specifically, particles are detected first in each frame of the image sequence. The detected particles are then linked between consecutive frames to recover their complete trajectories. The contributions of this paper focus on particle linking. Classical particle linking methods  Cell tracking is closely related to particle tracking. There are different classes of cell tracking methods. An important category is tracking-by-evolution  Transformer  In this paper, we propose a Transformer-based single particle tracking method MoTT, which is effective for different motion modes and different density levels of subcellular structures. The main contributions of our work are as follows: "
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,2,Method,Our particle tracking approach follows the two-step paradigm: particle detection and particle linking. We first use the detector DeepBlink 
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,2.1,Hypothesis Tree Construction,"Assuming that the particle linking has been processed up to frame t. In order to find correspondence between the current live tracklets and the detections of frame t + 1, we will build a hypothesis tree of depth d for each live tracklet, with its detection at frame t as the root node. To build the tree beyond the root node, we select m (real) detections of the next frame nearest to the current node as well as another null detection that represents a missing detection as children of the current node. If the current node is null, m (real) detections of the next frame nearest to the parent of the current node are selected. From the hypothesis tree, (m + 1) d hypothesis tracklets will be obtained. Figure "
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,2.2,MoTT Network,"As shown in Fig.  For the generated tracklets from the previous step, the preprocessing is performed to make the length of all live tracklets equal to Δt, to convert position sequence to velocity sequence, and to add the existence flag making the coordinate dimension n+1. See supplementary material for the details of preprocessing. Then the preprocessed live tracklet is fed into the Transformer encoder, while the (m + 1) d preprocessed hypothesis tracklets are fed into the Transformer decoder. The self-attention modules in the encoder and decoder are used to extract features of live tracklets and hypothesis tracklets, respectively. The cross-attention module is used to calculate the affinity between the live tracklet and its multiple candidate tracklets. The classification head outputs the predicted matching probabilities between the live tracklet and (m + 1) d hypothesis tracklets. The regression head outputs the predicted existence probability and velocity of each live tracklet in the next frame. The existence probability represents the probability of the live tracklet existence in the next frame. The predicted velocity can be easily converted to the predicted position. Training. We train the MoTT network in a supervised way, using the crossentropy (CE) loss to supervise the output of the classification head and the mean square error (MSE) loss to supervise the output of the regression head. The target of classification head output is a class index in the range [0, (m + 1) d ) where (m + 1) d is the number of hypothesis tracklets. The target of regression head output is the ground truth of the concatenation of normalized velocity and the existence flag. Inference. In inference, we add a 1D max-pooling layer following the classification head to select the highest probability of the hypothesis tracklets with the same detection at frame t + 1 as the matching probabilities between the live tracklet and the candidate detection at frame t + 1. Then the (m + 1) predicted matching probabilities are normalized by softmax. The matching probabilities between the live tracklet and other detections besides the m + 1 candidate detections are set to zero."
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,2.3,Modeling Discrete Optimization Problem,"To find a one-to-one correspondence solution, we construct a discrete optimization formulation as  The objective function aims at maximizing the sum of matching probabilities under the constraints that each live tracklet is matched to only one detection (real or null), and each real detection is matched by at most one tracklet. This optimization problem is solved by using Gurobi (a solver for mathematical programming) "
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,2.4,Track Management,"The one-to-one correspondence solution generally includes three situations. For each tracklet matched to a real detection, we add the matched real detection to the end of the live tracklet for updating. For each tracklet matched to a null detection, if the predicted existence probability is greater than a threshold p the predicted position is used to substitute for the null detection, else the live tracklet is terminated. In this way, the disconnected tracklets due to missing detections will be kept and be relinked when their detections emerge. For each detection that is not matched to any of the tracklets, a new live tracklet is initialized with this detection. After finishing particle linking on a whole movie, we remove the trajectories of length one, because they are considered false positive detections. See supplementary material for the details of track management."
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,3,Experimental Results,"Datasets. The performance of our method is evaluated on ISBI Particle Tracking Challenge datasets (ISBI PTC, http://bioimageanalysis.org/track/)  Metrics. Metrics α, β, JSC θ , JSC are used to evaluate the method performance  Table "
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,3.1,Quantitative Performance,"Comparison with the SOTA Methods. We compared our single particle tracking method with other SOTA methods, and the quantitative results on the microtubule scenario are shown in Table  Comparison Under the Same Ground Truth Detections. Under the ground truth detections, we compare our particle linking method with LAP  Effectiveness for All Scenarios. We perform our particle linking method using ground truth detections on the four scenarios with three density levels in the ISBI PTC dataset. The results (see the supplementary material) demonstrate the effectiveness of our method for both 2D and 3D single particle tracking."
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,3.2,Robustness Analysis,There are false positives (FPs) and false negatives (FNs) in actual detection results. Early study shows that FNs affect performance more than FPs 
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,4,Conclusion,"In this paper, we proposed a novel Transformer-based method for single particle tracking in fluorescence microscopy images. We exploited the attention mechanism to model complex particle behaviors from past and hypothetical future tracklets. We designed a relinking strategy to alleviate the impact of missed detections due to e.g., low SNRs, and to enhance the robustness of our tracking method. Our experimental results show that our method is effective for all subcellular structures of ISBI Particle Tracking Challenge datasets, which cover different motion modes and different density levels. And our method achieves state-of-the-art performance on the microtubule movies of ISBI PTC datasets. In the future, we will test our method on other live cell fluorescence microscopy image sequences."
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,,Fig. 1 .,
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,,Fig. 2 .,
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,,Fig. 3 .,
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,,Fig. 4 .,
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,,Table 2 .,
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,1,Introduction,"Diffusion MRI (dMRI) is the most widely used technique for studying human brain structural connectivity in vivo  In this work, we develop a novel framework to perform geometrically consistent interpolation of FODs and demonstrate its effectiveness in enhancing the performance of fiber tracking. We decompose each FOD function with multiple peak lobes into components, each with only one peak lobe. Then, we locally model neighboring voxels' single-peak components, consistent in direction, as a vector field flow fitted by polynomials. Each vector field locally represents the geometry of an underlying fiber bundle and continuously determines the direction of single-peak components within the support. Then, a closed-form solution is developed to account for rotations of FODs represented as spherical harmonics and realize the geometrically consistent interpolation of each FOD component, as shown in Fig. "
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,2,Method,
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,2.1,FOD Decomposition,"The fiber orientation distribution (FOD) is an advanced model representing the complicated crossing fiber's geometry  where Y m n is the m th (-n ≤ m ≤ n) real SPHARM basis at the order n (0 ≤ n ≤ N) and u m n is the coefficient for the corresponding basis, U is the vector that represents all the coefficients u m n , and θ and ϕ are the polar and azimuth angles of the spherical coordinates in R 3 . For any FOD function, we expand it using (1) on a unit sphere represented by a triangular mesh, search the peaks on the mesh, and accept the peaks whose value is higher than a threshold THD (e.g., 0.1). For a FOD function with K peak lobes, we solve the following optimization problem for its decomposition: where U k are the coefficients for the decomposed single-peak FOD components, and A k is the matrix that represents the values of SPHARMs at neighboring directions around the k th peak (vertices within two rings of each peak). The first term enforces the sum of the decomposed single-peak components to equal the original FOD; the second term enforces each component to equal the original FOD near the corresponding peak; the third term suppresses each component around other peaks. We show an example of a FOD function decomposition in Fig. "
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,2.2,Modeling Single Peak FOD Components as Flow of Vector Fields,"For each single-peak FOD component, we model it with the flow of a smooth vector field, which supports geometrically consistent interpolations of FOD components. We represent the k th single-peak component of the FOD function at a voxel p 0 as F k p 0 . We choose the peak direction of F k p 0 as the seeding vector v p0 of the local supporting vector field. Then we compute a tube T k p 0 , centering at p 0 , along the direction v p0 with a radius r and a height h (Fig.  where v d p t (1 ≤ d ≤ 3) represent d th component of the vector at voxel p t . The second term regulates the second-order coefficients for smoothness. The polynomials are used to model the vector field V k p 0 within the tube T k p 0 that represents the k th underlying fiber bundle locally around the voxel p 0 ."
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,2.3,Rotation Calculation for SPHARM-Based FODs,"For a target point q where we perform the interpolation, we choose the nearest voxel p 0 , which has been augmented with a set of tubes {T k p 0 } and vector fields {V k p 0 } through the computation of Sect. 2.2. For each vector field V k p 0 , we compute the vector v q at point q using its polynomial representation. Each of the corresponding k th single-peak FOD component F k p t from voxels within one voxel distance to q are used for interpolation. First, we rotate each single-peak component F k p t so that its peak direction is aligned with the vector v q . An easy way to compute the rotation is R t = exp([r] × ), where r is a vector with its direction determined by the crossing product between the peak vectors v pt and v q , and its length is the angle between v pt and v q ; [•] × is the cross-product matrix of a vector  where (θ r ,ϕ r ) is the coordinate acquired by rotating the coordinate (θ ,ϕ) with the inverse rotation R -1 . We represent (4) using the SPHARMs: where v m n and u m n are coefficients for FOD R and FOD, respectively. The key to computing coefficients v m n is representing the SPHARM function Y m n (θ r , ϕ r ) by a linear combination of Y m n (θ, ϕ); namely, finding the transformation of SPHARMs under a coordinate system rotation. For rotation R -1 , we follow Wigner's work  where Z γ and Z α are the rotations around the current z-axis by angles γ and α, respectively; Y β is the rotation around the current y-axis by an angle β. We transform the real SPHARMs into complex SPHARMs for more straightforward computation. Based on a group symmetry argument  where W n is a (2n + 1) complex vector that represents the n th -order complex SPHARMs; D n is a (2n + 1)-by-(2n + 1) matrix whose elements are represented as: where the first and third terms correspond to the rotations Z α and Z γ in (  where P is the Jacobi polynomial, and other elements of this matrix can be induced by the symmetry property "
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,2.4,Evaluation Methods,"We compare the proposed FOD interpolation method with the linear interpolation of SPHARM coefficients, the most used method in FOD-based tractography. We measure the quality of the interpolated FOD functions based on down-sampling; we down-simple a ground truth FOD volume data to half the resolution, interpolate the down-sampled data to the original resolution, and measure the interpolated FOD functions against the ground truth data based on two metrics. The first metric is to measure the sharpness of the interpolated FOD functions, which indicates the specificity and accuracy of the FOD function. Inspired by the full width at half maximum (FWHM) in signal processing, we define the full area at half maximum (FAHM) of a FOD function f as FAHM (f ) = area({x : f (x) > max(f /2)})/4π . The metric FAHM is more sensitive to boating effects than entropy  We also evaluate the effectiveness of the proposed method on tractography. We upsample the FOD volume images to super-resolution images, including the cortical spinal tract (CST) area that connects the cortical surface to the internal capsule. Then, we run the popular tractography from the MRTrix "
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,3,Experiment Results,"We evaluated the FOD interpolation using 40 HCP subjects  The HCP FOD data is used as the ground truth for down-sampling-based evaluation. We show the FODs from one interpolated slice of a subject in Fig.  We up-sampled the 40 HCP FOD volume images around the CST region to superresolution images with an isotropic resolution of 0.25 mm. Then, we ran FOD-based probabilistic tractography on the original and up-sampled FOD data using the iFOD1 algorithm of the MRtrix software "
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,4,Conclusion,"We propose a novel interpolation method for FOD function with enhanced consistency of fiber geometry. The experiments show that our method provides a more accurate interpolation of FODs and can generate super-resolution FODs via upsampling to improve the tractography's performance significantly. In future work, we will integrate the proposed FOD interpolation with tractography algorithms and validate its performance in reducing false positives and negatives in challenging fiber bundles."
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,,Fig. 1 .,
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,,Fig. 2 .,
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,,Fig. 3 .,
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,,Fig. 4 .,
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,,Fig. 5 .,
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,,Fig. 6 .,
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,,Fig. 7 .,
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,1,Introduction,"Brain functional network refers to the integrator of information exchange between different neurons, neuron clusters or brain regions. It can not only reveal the working mechanism and developmental changes of the brain  As a powerful neuroimaging tool, the resting-sate functional Magnetic Resonance Imaging (rs-fMRI) constructs brain functional networks by capturing the changes of blood oxygen level-dependent (BOLD) signals and computing their correlation between different regions of interest (ROIs)  Neglecting Heterogeneity . Although BC-GCN  Overlooking Global Structures. Attention mechanism can help the model focus on crucial connections. Inspired by this, GAT model "
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,2,Methodology,An overview of the proposed PH-BTN is illustrated in Fig. 
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,2.1,Path-Based Heterogeneous Graph Generation,"Path-Based Heterogeneous Brain Network. To retain the heterogeneity and path-based structure, we encode the brain functional network captured by rs-fMRI as a path-based heterogeneous graph G = (V, E, P), where V = {v i } N i=1 ∈ R N is the node set of size N defined by ROIs on a specific brain atlas, E = [e ij ] ∈ R N ×N ×D is the edge set constructed by using the Pearson's correlation coefficient (PCC) between a sub-series of BOLD signals between nodes, with each edge e ij initialized with D-dimensional edge features h ij , and P is the path set along with a path type mapping function ψ : P → R, where R denotes the graph path types, |R| ≥ 2. Graph Path. In graph theory, a graph path p is composed of a finite sequence of n edges, where n denotes that this path is a n-hop path p n . For example, a 2-hop path p 2 ikj between node v i and v j can be represented as p 2 ikj = {e ik , e kj }, where e ij is the edge between v i and v j and i = j = k. In particular, the 0-hop path indicates the self-loop of the node. According to ablation results of multihops  , where P n ij means the set of all n-hop paths between node v i and v j , P 0 ij = {e ij |i = j} and Particularly, when combining P 0 ij and P 1 ij , we can gain the sequence {e ik , e kj }, where i = k = j or i = k = j. This conjunctive sequence of P 0 ij and P 1 ij can be regarded as a special 2-hop paths where k denotes the index of intermediate node. Thus, the multi-hop paths between node v i and v j can be recorded as Heterogeneous Graph Path. In order to obtain the type of paths, we introduce a brain partition (e.g., frontal, parietal, temporal, occipital and insular) as prior to define heterogeneous paths. It is intuitive to define the types of graph path by edges. However, with the increase of edges, the complexity of the path type definition will increase greatly. Thus, we directly regard the type of intermediate node v k as the type of graph path p ikj . For instance, if v k in the frontal lobe, we have ψ(p ikj ) = frontal. Considering the fact that the feature spaces of different types of paths are not completely irrelevant (as shown in Fig. "
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,2.2,HP-GTC: Heterogeneous Path Graph Transformer Convolution Module to Learn Compact Features,"After brain network construction, we present the HP-GTC module to extract and aggregate specific and common features from different types of graph paths. As illustrated in Fig.  HPConv. Considering that different heterogeneous paths have different distributions and contain different information, we design a novel graph convolution to learn the edge features of each path type independently. Under each path type, the brain network can be regarded as a homogeneous graph. Inspired by Li et al.  where h under the graph path type r. Intuitively, the HPConv layer extracts edge features in specific and common path spaces, so as to ensure that the edge features learned in different specific feature spaces are independent and those learned in common spaces can retain global and shared information. HPTrans. Note that different types of graph paths would have different impacts on a specific edge, and the same type of graph paths may similarly affect the specific edge. To model these characteristics, we propose an attention mechanism to capture these heterogeneous and common information to learn more effective edge features. Inspired by Kan et al.  of HPConv layer for learning more compact edge features. The HPTrans layer is formulated as follows: where H denotes the output features, and the weight parameters 2 linearly map edge features to different feature spaces, and then adopt L2-normalized operator norm(•) to generate the corresponding representations Q, K, V for subsequent attention matrix A calculation and feature aggregation. For simplicity of illustration, in this paper, we only consider the single-head self-attention and assume V = H . The extension to the multi-head attention is straightforward, and we omit bias terms for simplicity. To further enhance influential features and alleviate the over-smoothing problem of GNN, we adopt SE block in the HP-GTC module. Therefore, the final formulation of HP-GTC module can be represented as below: where H l is the edge feature matrix of l th HP-GTC module."
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,2.3,Readout and Prediction,"Lastly, inspired by Li et al.  Then H Gm is sent to a multi-layer perceptron (MLP) to give the final prediction ŷm , where G m denotes the m th graph sampled in subject s. Specifically, the final prediction of subject s is the average or weighted voting result of its' all samples during inferring."
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,3,Experimental Results,Dataset and Implementation Details. We validated our PH-BTN on the Baby Connectome Project (BCP) dataset 
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,4,Conclusion,"In this paper, we propose a novel network, namely PH-BTN, for encoding pathbased heterogeneous brain networks for analyzing brain functional connectivity. Different from most existing methods, our proposed model considers the path significance and heterogeneity by heterogeneous graph convolution, and incorporates global brain structure and key connections by Transformer mechanism. Experiments and visualization of age and gender prediction on the BCP dataset show the superiority and effectivity of our PH-BTN model. Moreover, the proposed PH-BTN offers a new way to understand neural development, explore sexual differences, and ultimately benefit neuroimaging research. In future work, we will extend and validate our methods on larger benchmark datasets."
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,,Fig. 1 .,
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,,Fig. 2 .,
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,,,
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,,Fig. 3 .,
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,,Table 1 .,
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_32.
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,1,Introduction,"Epilepsy is a debilitating neurological disorder characterized by spontaneous and recurring seizures  Computer-aided tools for scalp EEG almost exclusively focus on the task of (temporal) seizure detection. Early works approached the problem via feature engineering and explored spectral  A few works have explored the difficult task of localizing the SOZ via post hoc evaluations of deep networks trained for seizure detection. For example, the authors of  In this paper, we present DeepSOZ, a robust model for joint seizure detection and SOZ localization from multichannel scalp EEG. Our model consists of a spatial transformer encoder to combine cross-channel information and LSTM layers to capture dynamic activity for window-wise seizure detection. In parallel, we use a novel attention-weighted multi-instance pooling to supervise seizure-level SOZ localization at the single channel resolution. We curate a large evaluation dataset from the publicly available TUH seizure corpus by creating SOZ labels from the clinician notes for each patient. We perform extensive window-level, seizure-level, and patient-level evaluations of our model. Additionally, we analyze the consistency of predictions across seizure occurrences, which has not previously been reported for SOZ localization. Quantifying the error variance is the first step in establishing trust in DeepSOZ for clinical translation."
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,2,Methodology,Figure 
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,2.1,The DeepSOZ Model,"where LN (•) denotes layer normalization, and F F (•) represents a learned two layer feed-forward network with ReLU activation. The MHA(•) operation uses parallel self attentions to map the input data into a set of projections, as guided by the other channels in the montage. Formally, let n index the attention head. The attention weights A t n ∈ R 20×20 captures global (1) and cross-channel  Xt as follows: where ξ(•) represents the softmax function, and d is our model dimension. The attention A t n is multiplied by the value matrix Xt to generate the output for head n. These outputs are concatenated and fed into a linear layer to produce MHA(•). Finally, these MHA outputs are passed into a two layer feed forward neural network with ReLU activation, post residual connections and layer normalization to generate the hidden encoding H t ∈ R 20×200 . The matrices W Q n , W K n , and W V n are trained parameters of the encoder. For simplicity, we set the model dimension d to be the same as our input x t i (d = 200 in this work), and we specify 8 attention heads in the MHA operation."
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,LSTM for Temporal Seizure Detection:,"We use a bidirectional LSTM to capture evolving patterns in the global encodings of the one-second EEG windows, i.e., {h t 0 } T t=1 . We use a single LSTM layer with 100 hidden units to process the global encodings and capture both long-term and short-term dependencies. The output of the LSTM is passed into a linear layer, followed by a softmax function, to generate window-wise predictions Ŝt ∈ [0, 1]. Here, Ŝt represents the posterior probability of seizure versus baseline activity at window t."
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Attention-Weighted Multi-Instance Pooling for SOZ Localization:,We treat the localization task as a multi-instance learning problem to predict a channel-wise posterior distribution for the SOZ vector {y i } 19  i=1 by computing a weighted average of the hidden representations from the transformer. We first map the channel-wise encodings {h t i } 19 i=1 ∈ R 200 to scalars ŷt i using the same linear layer across channels. We use the predicted seizure probability Ŝt as our attention to compute the final SOZ prediction as follows: where σ(•) is the sigmoid function. The final patient-level predictions are obtained by averaging ŷi across all seizure recordings for that patient.
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,2.2,Loss Function and Model Training,"We train DeepSOZ in two stages. First, the transformer and LSTM layers are trained for window-wise seizure detection using weighted cross entropy loss: where the weight δ = 0.8 is fixed based on the ratio of non-seizure to seizure activity in the dataset. DeepSOZ is then finetuned for SOZ localization. To avoid catastrophic forgetting of the detection task, we freeze the LSTM layers and provide a weak supervision for detection via the loss function: where the ||.|| 1 penalizes the L1 norm to encourage sparsity in predicted ŷ"
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,2.3,Model Validation,"We evaluate DeepSOZ using bootstrapped 5-fold nested cross validation. Within each training fold, we select the learning rate and seizure detection threshold through a grid search with a fixed dropout of 0.15. We use PyTorch v1.9.0 with Adam  Seizure Detection: At the window level, we report sensitivity, specificity, and area under the receiver operating characteristic curve (AU-ROC). At the seizure level, we adopt the strategy of  To eliminate spikes, we smooth the output predictions using a 30 s window and count only the contiguous intervals beyond the calibrated detection threshold as seizure predictions. Following the standard of "
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Baseline Comparisons:,We compare the performance of DeepSOZ with one model ablation and four state-of-the-art methods from the literature. Our ablation replaces the attention-weighted multi-instance pooling in DeepSOZ with a standard maxpool operation within the prediction seizure window (DeepSOZmax). Our baselines consist of the CNN-BLSTM model for seizure detection developed by 
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,3,Experimental Results,
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Data and Preprocessing:,"We validate DeepSOZ on 642 EEG recordings from 120 adult epilepsy patients in the publicly available Temple University Hospital (TUH) corpus  Seizure Detection Performance: Table  The TGCN and CNN-BLSTM baselines achieve notably worse AU-ROC values, establishing the power of a transformer encoder in extracting more meaningful features. SZTrack is trained using the published strategy in  SOZ Localization Performance: Table "
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,4,Conclusion,"We have introduced DeepSOZ for joint seizure detection and SOZ localization from scalp EEG. DeepSOZ leverages a self-attention mechanism to generate informative global and channel-wise latent representations that strategically fuse multi-channel information. The subsequent recurrent layers and attentionweighted pooling allow DeepSOZ to generalize across a heterogeneous cohort. We validate DeepSOZ on data from 120 epilepsy patients and report improved detection and localization performance over numerous baselines. Finally, we quantify the prediction uncertainty as a first step towards building trust in the model."
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Fig. 1 .,
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Fig. 2 .,
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Fig. 3 .,
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Architecture Spatial Transformer Encoder:,
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Table 1 .,
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Table 2 .,
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Table 3 .,
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 18.
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,1,Introduction,"Whole slide image (WSI) classification is a critical task in computational pathology enabling disease diagnosis and subtyping using automatic tools. Owing to the paucity of patch-level annotations, multiple instance learning (MIL)  Most existing MIL methods do not fine-tune their feature extractor together with their classification task; this stems from the requirement for far larger GPU memory than is available currently due to the gigapixel nature of WSIs, e.g. training a WSI at 10x magnification may require more than 300 Gb of GPU memory. Recently, researchers have started to explore optimization methods to enable end-to-end training of the entire network and entire WSI within GPU memory "
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,,Motivation:,"To improve WSI-level analysis, we explore end-to-end training of the entire network using SSL pretrained ViTs. To achieve this, we use the patch batching and gradient retaining techniques in  To address the subpar performance of SSL-pretrained vision transformers, we utilize the prompt tuning techniques. Initially proposed in natural language processing, a prompt is a trainable or a pre-defined natural language statement that is provided as additional input to a transformer to guide the neural network towards learning a specific task or objective  In this paper, we propose a novel framework, Prompt-MIL, which uses prompts for WSI-level classification tasks within an MIL paradigm. Our contributions are: -Fine-tuning: Unlike existing works in histopathology image analysis, Prompt-MIL is fine-tuned using prompts rather than conventional full finetuning methods. -Task-specific representation learning: Our framework employs an SSL pretrained ViT feature extractor with a trainable prompt that calibrates the representations making them task-specific. By doing so, only the prompt parameters together with the classifier, are optimized. This avoids potential overfitting while still injecting task-specific knowledge into the learned representations. Extensive experiments on three public WSI datasets, TCGA-BRCA, TCGA-CRC, and BRIGHT demonstrate the superiority of Prompt-MIL over conventional MIL methods, achieving a relative improvement of 1.49%-4.03% in accuracy and 0.25%-8.97% in AUROC by using only less than 0.3% additional parameters. Compared to the conventional full fine-tuning approach, we finetune less than 1.3% of the parameters, yet achieve a relative improvement of 1.29%-13.61% in accuracy and 3.22%-27.18% in AUROC. Moreover, compared to the full fine-tuning approach, our method reduces GPU memory consumption by 38%-45% and trains 21%-27% faster. To the best of our knowledge, this is the first work where prompts are explored for WSI classification. While our method is quite simple, it is versatile as it is agnostic to the MIL scheme and can be easily applied to different MIL methods. Our code is available at https://github.com/cvlab-stonybrook/PromptMIL."
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,2,Method,"Our Prompt-MIL framework consists of three components: a frozen feature model to extract features of tissue patches, a classifier that performs an MIL scheme of feature aggregation and classification of the WSIs, and a trainable prompt. Given a WSI and its label y, the image is tiled into n tissue patches/instances {x 1 , x 2 , . . . , x n } at a predefined magnification. As shown in Fig.  where h i denotes the feature of the i th patch, h is the concatenation of all h i , and P = {p i , i = 1, 2, . . . , k} is the trainable prompt consisting of k trainable tokens. The classifier G(•) applies an MIL scheme to predict the label ŷ and calculate the loss L as: where the L cls is a classification loss."
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,2.1,Visual Prompt Tuning,"The visual prompt tuning is the key component of our framework. As shown in Fig.  where t 0 i is the embedding token of z i and T 0 z is the collection of such tokens. These tokens T 0 z are concatenated with a class token t 0 cls and a prompt P: The class token is used to aggregate information from all other tokens. The prompt consists of k trainable tokens P = {p i |i = 1, 2, . . . , k}. The concatenation is fed into l layers of the Transformer encoders: where p i j is the j th output prompt token of the i th Transformer encoder and T i P is the collection of all k such output prompt tokens, which are not trainable. The output feature of x i is defined as the last class token:"
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,2.2,Optimization,"Our overall loss function is defined as where only the parameters of the G(•) and the prompt P are optimized, while the feature extractor model F (•) is frozen. Training the entire pipeline in an end-to-end fashion on gigapixel images is infeasible using the current hardware. To address this issue, we utilize the patch batching and gradient retaining techniques from  In the second step (step②), we feed h into the classifier G(•) to calculate the loss L and update the parameters of G(•) by back-propagate the loss. The back-propagated gradients g = ∂L/∂h on h are retained for the next step. Finally (step③), we feed the input batches into the feature model F (•) again and use the output h and the retained gradients g from the last step to update the trainable prompt tokens. In particular, the gradients on the j th prompt token p j are calculated as: where g i is the gradient calculated with respect to h i . To sum up, in each step, we only update either F or G given the current batch, which avoid storing the gradients of the whole framework for all the input patches. This patch batching and gradient retaining techniques make the end-to-end training feasible. In this study, we use DSMIL "
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,3,Experiments and Discussion,
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,3.1,Datasets,We assessed Prompt-MIL using three histopathological WSI datasets: TCGA-BRCA 
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,3.2,Implementation Details,We cropped non-overlapping 224 × 224 sized patches in all our experiments and used ViT-Tiny (ViT-T/16) 
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,3.3,Results,"We chose overall accuracy and Area Under Receiver Operating Characteristic curve (AUROC) as the evaluation metrics. Evaluation of Prompt Tuning Performance: We compared the proposed Prompt-MIL with two baselines: 1) a conventional MIL model with a frozen feature extractor  The computationally intensive full fine-tuning method under-performed conventional MIL and Prompt-MIL. Compared to the full fine-tuning method, our method achieved a relative improvement of 1.29% to 13.61% in accuracy and 3.22% to 27.18% in AUROC on the three datasets. Due to the relatively small amount of slide-level labels (few hundred to a few thousands) fully fine tuning 5M parameters in the feature model might suffer from overfitting. In contrast, our method contained less than 1.3% of parameters compared to full fine-tuning, leading to robust training.  Evaluation of Time and GPU Memory Efficiency: Prompt-MIL is an efficient method requiring less GPU memory to train and running much faster than full fine-tuning methods. We evaluated the training speed and memory consumption of our method and compared to the full fine-tuning baseline on four different sized WSIs in the BRIGHT dataset. As shown in Table  Evaluation on the Pathological Foundation Models: We demonstrated our Prompt-MIL also had a better performance when used with the pathological foundation model. Foundational models refer to those trained on large-scale pathology datasets (e.g. the entire TCGA Pan-cancer dataset "
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,4,Conclusion,"In this work, we introduced a new framework, Prompt-MIL, which combines the use of Multiple Instance Learning (MIL) with prompts to improve the performance of WSI classification. Prompt-MIL adopts a prompt tuning mechanism rather than a conventional full fine-tuning of the entire feature representation. In such a scheme, only a small fraction of parameters calibrates the pretrained representations to encode task-specific information, so the entire training can be performed in an end-to-end manner. We applied our proposed method to three publicly available datasets. Extensive experiments demonstrated the superiority of Prompt-MIL over the conventional MIL as well as the conventional fully finetuning methods. Moreover, by fine-tuning much fewer parameters compared to fully fine-tuning, our method is GPU memory efficient and fast. Our proposed approach also showed promising potentials in transferring foundation models. We will further explore the task-specific features that are captured by our prompt toward explainability of these models."
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,,,
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,,Fig. 1 .,
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,,Table 1 .,
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,,Table 2 .,
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,,Table 3 .,
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,,Table 4 .,
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,1,Introduction,"Multiple Sclerosis (MS) is a severe central nervous system disease with a highly nonlinear disease course where periodic relapses impair the patient's quality of life. Clinical studies show that relapses co-occur with the appearance of new inflammatory MS lesions in MR images  While recent approaches applied convolutional neural networks (CNN) to directly learn features from MR image space  To solve this problem, we use concepts from geometric deep learning. Specifically, we propose a two-stage pipeline. First, the lesions in the MRI scans are segmented using a state-of-the-art 3D segmentation algorithm  Contributions. Our contribution is threefold:  (2) We propose a two-stage pipeline that effectively captures inherent MS variations in MRI scans, thus generating an effective global representation. (3) We develop a self-pruning module, which assigns an importance score to each lesion and reduces the task complexity by prioritizing the critical lesions. Additionally, the assigned per-lesion importance score improves our model's explainability."
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,2,Methodology,"Overview. The objective is to predict MS inflammatory disease activity, i.e., to classify if new or significantly enlarged inflammatory lesions appear in the follow-up after the initial MRI scans. We denote the dataset as D(X, y), where X is the set of lesion patches extracted from MR scans, and y ∈ {0, 1} is the inflammatory disease activity status. For patient i, multiple lesion patches {x i 1 , x i 2 , ...x i n } can exist, where n is the total number of lesions. We aim to learn a mapping function f : Please note that our formulation differs from existing methods  Lesion Detection and Feature Extraction. We focus on the individual lesions instead of processing whole-brain MRI scans. This is important because MS lesions typically comprise less than 1% of voxels in the MRI scan. With this strategy, the graph model can aggregate lesion-level features for an effective patient-specific representation. First, we detect the lesions using a state-of-theart nn-Unet  Lesion Graph Processor. In the second stage, we generate a patient-specific graph G(V, E, Z) from the detected lesions. The lesions act as vertices V of this graph and are initialized with the crop-derived features Z. The spatial location s of the lesions is used to determine their connectivity E using a k-Nearest Neighbor (kNN) graph method  ). τ is a scalar that controls the contribution of distant lesions. Hence the final graph connectivity can be represented as: where N (v i ) are the nodes directly connected to the node v i . Constructing a graph from the lesions is instrumental in two aspects:  (2) It is possible to incorporate meaningful lesion properties such as spatial proximity and the number of lesions. Please note that separate graphs are created for individual patients. Thus, MS inflammatory disease activity prediction is formulated as a graph-level classification task. The graph G(V, E, Z) can be processed using message-passing neural networks (such as GCN  Self-pruning Module. The number of lesions can vary substantially among the patients, including the possibility of false positives in the segmentation stage. As such, it is crucial to recognize the most relevant lesions for the final prediction. In addition, this will bring inherent explainability and make it easier for a doctor to validate model predictions. To accomplish this, the enriched lesion features Ẑ are passed through a self-pruning module (SPM). The SPM produces a binary mask M for each lesion to determine whether a lesion contributes to the classification. The SPM uses a learnable projection vector p to compute importance scores ( Ẑ p/|| p||) for the lesions. These scores are scaled with a sigmoid layer. We retain the high-scoring lesions and discard the rest, which is formulated as: where σ(x) = 1/(1 + e -x ) is the sigmoid function and top-r(•) is an operator which selects a fraction r of the lesions based on high importance score. r is a hyper-parameter in our setup. Since the masking process is part of the forward pass through the model during both training and inference stages and not a post hoc modification, we refer to it as self-pruning of nodes. Features of the remaining nodes ( Ẑ = Ẑ ⊗ M ) are passed to the classification head. It should be noted that the existence of multiple lesions is a typical characteristic of MS. Hence, a crucial aspect of MS management is that clinicians must identify signs of inflammatory disease activity in MR images to make treatment decisions  Classification Head. The classification head consists of a readout layer aggregating all the remaining node's features to produce a single feature vector ẑ for the entire graph. This graph-level feature is passed through an MLP to obtain the final prediction ŷ. We train our model using a binary cross-entropy loss. where N is the number of patients, y i is the ground truth inflammatory disease activity information and ŷi is the model prediction."
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,3,Experiments,Datasets and Image Preprocessing. Our approach is evaluated on a cohort of 430 MS patients collected following approval from the local IRB 
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,,Feature Extraction and Training Configuration.,"We use an nn-Unet  The lesions are connected using a k-nearest neighbor algorithm with k = 5. Further, these connections (edges) are weighted using τ = 0.01 (Eq. 1). Two message-passing layers with hidden dimensions of 64 and 8, respectively, process the generated graph to enrich lesion features. Next, the enriched features are passed through the SPM. The SPM uses a learnable projection vector p ∈ R 8 and sigmoid activation to learn the importance score. Based on this importance score, a mask is produced to select r = 0.5 (i.e., 50%) of the highest-scoring lesions and discard the rest. Next, a sum aggregation is used as the readout function. These aggregated features are passed through 2 feed-forward layers with hidden dimensions of size 8. Finally, the features are passed to a sigmoid function to obtain the final prediction. The model is trained for 300 epochs using AdamW optimizer  Evaluation Strategy, Classifier, and Metrics. We report our results on MS inflammatory disease activity prediction for the clinically relevant one and twoyear intervals "
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,4,Results,"Quantitative Comparison. Table  The Effectiveness of Graph Structure. Since the lesion feature extractor generates rich lesion features, one may argue that the graph structure is unwarranted. There are two alternatives to using a graph, (i) completely discard the graph structure, use a feed-forward layer to enrich the lesion features further, and aggregate them to perform classification  On the other hand, the GAT model learns an attention weight and ignores the pre-defined edge weights. However, it still computes these coefficients on only the connected nodes. We can go further, completely ignore the distances and instead use a fully connected graph. The TransformerConv  The Contribution of the Self-Pruning Module (SPM). The SPM selects a subset of lesions for the final prediction during the training and evaluation phases. However, the proposed classification method can work without it. In this case, none of the lesions is discarded during the readout operation. Table "
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,,Analysis of Hyperparameters.,"The retention ratio r and the number of neighbors k used for building the graph are the two critical hyperparameters in our proposed framework. We discuss the effect of the retention ratio r here and defer discussion about k to the appendix. Effect of Retention Ratio r. The retention ratio r ∈ (0, 1] controls the fraction of lesions retained after the self-pruning module. If we set its value to 1, all the lesions are retained for the final prediction and thus, bypassing the self-pruning module. Any other value implies that we ignore at least a few lesions in the readout layer. Since the number of lesions can vary across graphs, we retain (N.r) lesions after the self-pruning layer. To find the optimal r, we test our model with r between 0.1 and 1.0. The results are summarized in Fig. "
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,5,Conclusion,"Predicting MS inflammatory disease activity is a clinically relevant, albeit challenging task. In this work, we propose a two-stage graph-based pipeline that surpasses existing CNN-based methods by decoupling the tasks of detecting and learning rich semantic features for lesions. We also propose a self-pruning module that further improves model generalizability by handling variations in the number of lesions within patients. Most importantly, we frame the MS inflammatory disease activity prediction as a graph classification problem. We hope our work provides a new perspective and leads to cutting-edge research at the intersection of graph processing and MS inflammatory disease activity prediction."
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,,Fig. 1 .,
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,,Fig. 2 .,
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,,Fig. 3 .,
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,,Table 1 .,
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,,Table 2 .,
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,,Table 3 .,
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,,Table 4 .,
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,1,Introduction,"Healthy brain aging follows specific patterns  Recently, graph-based methods have been explored for brain age estimation as graphs can inherently combine multimodal information by integrating the subjects' neuroimaging information as node features and, through a similarity metric, the associations among subjects through as edges that connect these nodes  GCNs  A way to address this problem is through adaptive graph learning  are connected based on the imaging features and do not take advantage of the associations of the non-imaging information for the edges. In  Contributions. This paper has the following contributions: 1) We combine imaging and non-imaging information in an attention-based framework to learn adaptively an optimized graph structure for brain age estimation. 2) We propose a novel graph loss that enables end-to-end training for the task of brain age regression. 3) Our framework is inherently interpretable as the attention mechanism allows us to rank all imaging and non-imaging phenotypes according to their significance for the task. 4) We evaluate our method on the UK Biobank (UKBB) and achieve state-of-the-art results on the tasks of brain age regression and classification. The code can be found on GitHub at: https://github.com/ bintsi/adaptive-graph-learning."
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,2,Methods,"Given a set of N subjects with M features X = [x 1 , ..., x N ] ∈ R N ×M and labels y ∈ R N , a population graph is defined as G = {V, E}, where V is a set of nodes, one per subject, and E is a set of paired nodes that specifies the connectivity of the graph, meaning the edges of the graph. To create an optimized set of edges for the task of brain age estimation, we leverage a set of non-imaging phenotypes q i ∈ R Q and a set of imaging phenotypes s i ∈ R S per subject i through an attention-based framework. The imaging phenotypes are a subset of the imaging features s i ⊆ x i . The phenotypes are selected according to "
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,,Attention Weights Extraction.,"Based on the assumption that not all phenotypes are equally important for the construction of the graph, we train a MLP g θ , with parameters θ, which takes as input both the non-imaging features q i and the imaging features s i for every subject i and outputs an attention weight vector a ∈ R Q+S , where every element of a corresponds to a specific phenotype. Intuitively, we expect that the features that are relevant to brain age estimation will get attention weights close to 1, and close to 0 otherwise. Since we are interested in the overall relevance of the phenotypes for the task, the output weights need to be global and apply to all of the nodes. To do so, we average the attention weights across subjects and normalize them between 0 and 1. Edge Extraction. The weighted phenotypes for each subject i are calculated as f i = a (q i s i ), where f i ∈ R Q+S , a are the attention weights produced by the MLP g θ , (• •) denotes the concatenation function between two vectors and denotes the Hadamard product. We further define the probability p ij (f i , f j ; θ, t) of a pair of nodes (i, j) ∈ V to be connected in Eq. (  where t is a learnable parameter and d is a distance function that calculates the distance between the weighted phenotypes of two nodes. To keep the memory cost low, we create a sparse k-degree graph. We use the Gumbel-Top-k trick  Optimization. The extracted graph is used as input, along with the imaging features X, which are used as node features, to a GCN g ψ , with parameters ψ, which comprises of a number of graph convolutional layers, followed by fully connected layers. The pipeline is trained end-to-end with a loss function L that consists of two components and is defined as in Eq. (  The first component, L GCN , is optimizing the GCN, g ψ . For regression we use the Huber loss  The second component, L graph , optimizes the MLP g θ , whose output are the phenotypes' attention weights. However, the graph is sparse with discrete edges and hence the network cannot be trained with backpropagation as is. To alleviate this issue, we formulate our graph loss in a way that rewards edges that lead to correct predictions and penalize edges that lead to wrong predictions. Inspired by  where ρ(•, •) is the reward function which is defined in Eq. (4): Here ε is the null model's prediction (i.e. the average brain age of the training set). Intuitively, when the prediction error |y ig ψ (x i )| is smaller than the null model's prediction then the reward function will be negative. In turn, this will encourage the maximization of p ij so that L graph is minimized."
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,3,Experiments,"Dataset. The proposed framework is evaluated on the UKBB  Baselines. Given that a GCN trained on a meaningless graph can perform even worse than a simple regressor/classifier, our first baseline is a Linear/Logistic Regression model. For the second baseline, we construct a static graph based on a similarity metric, more specifically cosine similarity, of a set of features (either node features or non-imaging and imaging phenotypes) using the kNN rule with k = 5 and train a GCN on that graph. Using euclidean distance as the similarity metric leads to worse performance and is therefore not explored for the baselines. We also compare our method with DGM, which is the state-ofthe-art on graph learning for medical applications  Implementation Details. The GCN architecture uses ReLU activations and consists of one graph convolutional layer with 512 units and one fully connected layer with 128 units before the regression/classification layer. The number and the dimensions of the layers are determined through hyperparameter search based on validation performance. The networks are trained with the AdamW optimizer  (3), is ε = 6. We use PyTorch "
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,3.1,Results,"Regression. For the evaluation of the pipeline in the regression task, we use Mean Absolute Error (MAE), and the Pearson Correlation Coefficient (r score). A summary of the performance of the proposed and competing methods is available at Table  Classification. For the classification task, we divide the subjects into four balanced classes. The metrics used for the classification task to evaluate the performance of the model are accuracy, the area under the ROC curve (AUC), which is defined as the average of the AUC curve of every class, and the Macro F1-score (Table "
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,3.2,Ablation Studies,"Number of Phenotypes. We perform an ablation test to investigate the effect of the number of features used for the extraction of the edges. We used only non-imaging phenotypes, only imaging phenotypes, or a combination of both (Table  Distance Metrics. Moreover, we explore how different distance metrics affect performance. Here, euclidean, cosine, and hyperbolic "
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,3.3,Interpretability,"A very important advantage of the pipeline is that the graph extracted through the training is interpretable, in terms of why two nodes are connected or not. Visualizing the attention weights given to the phenotypes, we get an understanding of the features that are the most relevant to brain aging. The regression problem is clinically more important, thus we will focus on this in this section. A similar trend was presented for the classification task as well. The imaging and non-imaging phenotypes that were given the highest attention scores in the construction of the graph can be seen in Fig.  Apart from the attention weights, we also visualize the population graph that was used as the static graph of the baseline (Fig. "
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,4,Conclusion,"In this paper, we propose an end-to-end pipeline for adaptive population-graph learning that is optimized for brain age estimation. We integrate multimodal information and use attention scores for the construction of the graph, while also increasing interpretability. The graph is sparse, which minimizes the computational costs. We implement the approach both for node regression and node classification and show that we outperform both the static graph-based and state-of-the-art approaches. We finally provide an insight into the most important phenotypes for graph construction, which are in agreement with related neurobiological literature. In future work, we plan to extract node features from the latent space of a CNN. Training end-to-end will focus on latent imaging features that are also important for the GCN. Such features would potentially be more expressive and improve the overall performance. Finally, we leverage the UKBB because of the wide variety of multimodal data it contains, which makes it a perfect fit for brain age estimation but as a next step we also plan to evaluate our framework on different tasks and datasets."
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,,Fig. 2 .,
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,,Fig. 3 .,
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,,Table 1 .,
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,,Table 2 .,
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 19.
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,1,Introduction,"Nucleus classification is to identify the cell types from digital pathology image, assisting pathologists in cancer diagnosis and prognosis  A number of methods  Based on these observations, we develop a learnable Grouping Transformer based Classifier (GTC) that leverages the similarity between nuclei and their cluster representations to infer their types. Specifically, we define a number of nucleus clusters with learnable initial embeddings, and assign nucleus instances to their most correlated clusters by computing the correlations between clusters and nuclei. Next, the cluster embeddings are updated with their affiliated instances, and are further grouped into the categorical representations. Then, the cell types can be well estimated using the correlations between the nuclei and the categorical embeddings. We propose a novel fully transformer-based framework for nuclei detection and classification, by integrating a backbone, a centroid detector, and the grouping-based classifier. However, the transformer framework has a relatively large number of parameters, which could cause high costs in fine-tuning the whole model on large datasets. On the other hand, there exist domain gaps in the pathological images of different organs, staining, and institutions, which makes it necessary to fine-tune models to new applications. Thus, it is of great significance to tune our proposed transformer framework efficiently. Inspired by the prompt tuning methods "
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,2,Methodology,As shown in Fig. 
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,2.1,Transformer-Based Centroid Detector,Backbone. We adopt Swin Transformer  Encoder and Decoder. The encoder and decoder have 3 deformable attention layers 
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,2.2,Grouping Transformer Based Classifier,"In Fig.  where W 1 q and W 1 k are the weights of learnable linear projections, γ ∈ R G×Q are i.i.d random samples drawn from the distribution Gumbel(0, 1) and τ denotes the Softmax temperature. Then we utilize the hard assignment strategy  where argmax(S) returns a 1 × Q vector, and one-hot(•) converts the vector to a binary G × Q matrix. sg is the stop gradient operator for better training of the one-hot function  where g p denotes the embeddings of primary groups, W 1 v and W 1 o are learnable linear weights. To separate the primary groups into the cell categories, we measure the similar matrix between the primary groups g p and learnable class embeddings c e ∈ R C×D to yield advanced class embeddings c a ∈ R C×D , in the same way as Eq.( "
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,2.3,Loss Function,"The proposed method outputs a set of centroid proposals {(x q , y q )|q ∈ {1, • • • , Q}} with a decoder layer, and their corresponding cell-type scores {c q |q ∈ {1, • • • , Q}} with our proposed classifier. To compute the loss with detected centroids, we use the Hungarian algorithm  where ω 1 , ω 2 , ω 3 are weight terms, (x i , y i ) is the i th matched centroid coordinates, (x i , ŷi ) is the target coordinates. c i and c j denote the categorical scores of matched and unmatched samples, respectively. As the target of unmatched samples, ĉj is set to an empty category. FL(•) is the Focal Loss "
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,2.4,Grouping Prompts Based Tuning,"To avoid the inefficient fine-tuning of the backbone, we propose a new and simple learning strategy based on grouping prompts, as shown in Fig.  For a typical Swin-Transformer backbone, an input pathological image I ∈ R H×W ×3 is divided into HW E 2 image patches of size E × E. We first embed each image patch into a D-dimensional latent space via a linear projection. Then we randomly initialize the grouping prompts g ∈ R G×D as learnable parameters, and concatenate them with the patch embeddings as input. Note that in the backbone, input patch embeddings are separated into different local windows and the grouping prompts are also inserted into each window, as shown in Fig.  3 Experiments and Results"
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,3.1,Datasets and Implementation Details,"CoNSeP 1 [10] is a colorectal nuclear dataset with three types, consisting of 41 H&E stained image tiles from 16 colorectal adenocarcinoma whole-slide images (WSIs). The WSIs are at 20× magnification and the size of the slides is 500 × 500. We split them following the official partition  is a breast cancer dataset with three types and consists of 120 image tiles from 113 patients. The WSIs are at 20× magnification and the size of the slides ranges from 465 × 465 to 504 × 504. We follow the work  Lizard 3 "
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,3.2,Comparison with the State-of-the-Art,"The proposed method is compared with the state-of-the-art models: the existing methods for detecting and classifying cells in pathological images, i.e., Hover-Net "
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,3.3,Ablation Analysis,"The strengths of the grouping transformer based classifier and the grouping prompts are verified on CoNSeP dataset, as shown in Table  Prompt-based Grouping Transformer (PGT) is our proposed detection and classification architecture with grouping prompts and the GTC (in Fig. "
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,4,Conclusion,"We propose a new prompt-based grouping transformer framework that is fully transformer-based, and can achieve end-to-end nuclei detection and classification. In our framework, a grouping-based classifier groups nucleus features into cluster and category embeddings whose correlations with nuclei are used for identifying cell types. We further propose a novel learning scheme, which shares group embeddings with prompt tokens and extracts features guided by nuclei groups with less tuning costs. The results not only suggest that our method can obtain competitive performance on nuclei classification, but also indicate that the proposed prompt learning strategy can enhance the tuning efficiency."
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,,Fig. 1 .,
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,,Fig. 2 .,
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,,Fig. 3 .,
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,,Fig. 4 .,
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,,Table 1 .,
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,,Table 2 .,
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,,Table 3 .,
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,,Table 4 .,
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_55.
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,1,Introduction,Diffusion magnetic resonance imaging (dMRI) is sensitive to water diffusion in biological tissue. Analytical models of dMRI signals have played an essential role in quantifying tissue microstructure in clinical studies. Several methods have been developed to use dMRI signals measured with a single or multiple b-values to estimate compartment-specific diffusivity or diffusion propagators  This work introduces the maximum entropy (ME) estimation method for more accurate estimation of RDD functions by adapting theories and techniques developed for the classical Hausdorff moment problems 
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,2,Method,
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,2.1,On the Hausdorff Moment Problem,"where x • θ denotes the inner product between x and θ.  represents the set of all feasible indices. Next, let s k := s(x k ) which satisfies that where p(θ) = e -xmin•θ p(θ) is a scaled RDD function adjusted based on the non-zero minimum TE. The Hausdorff moment problem focuses on the existence of distribution functions that satisfy a sequence of power moments  with ). Thus, s k can be considered as the power moments of the density function f (γ) on the interval Γ. Therefore the problem of estimating RDD functions using finite rdMRI measurements is equivalent to a multivariate Hausdorff moment problem. We note that the unit interval is usually considered in Hausdorff moment problems. This can be obtained by changing the variable ]. Thus γ takes the value on the unit interval I 2 whose moments can be computed using linear transforms of s k . To simplify notations, the analysis in the following subsection will be based on s k and the distribution of γ."
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,2.2,Maximum-Entropy Estimation,"The RDD functions that satisfy the rdMRI data may not be unique. In moment problems, the maximum entropy (ME) method is a standard approach to estimate probability distributions and power spectral density functions  The first ME problem is developed based on Eq. (3) as below: where the objective function is the Shannon differential entropy of p(θ). The solutions to ME problems have been extensively investigated in moment problems  for some coefficients λ k with k ∈ K. The optimal parameters λ k need to be solved to satisfy the constraints in Eq. (  Based on Eq. (  where p(θ) is the pre-scaled RDD to adjust for the nonzero t min . The optimal solution to Eq. (  It is noted that Eq. (  , which has a different form than the solution in Eq.  The third ME-RDD is estimated based on the new variable γ as in Eq. (  Γ By changing the variable γ back to θ, Eq. (  It is interesting to note that the above objective function is equal to minimizing the Kullback-Leibler divergence, i.e., the relative entropy, between p(θ) and δ b δ t e -δ •θ . The optimal solution has the following form Then, pme3 λ (θ) is scaled back to obtain It is noted the difference between p me1 λ (θ) and p me2 λ (θ) is related to the nonzero offset x min . The two solutions are equal if the shortest TE is zero, but it is impossible in practice. The difference between p me2 λ (θ) and p me3 λ (θ) is related to the sampling rate δ b and δ t . The difference is less significant, with a higher sampling rate in the b-value and TEs."
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,2.3,Dual Energy Minimization Problems,"The optimal values of λ k in Eqs. (  For the solution in Eq. (  It can be shown that Thus, the Hessian matrix of Δ 1 (λ) positive definite, indicating that Δ 1 (λ) is a convex function. If Δ 1 (λ) has a finite minimizer, then the minimizer satisfies that Therefore, the optimal parameters for p me1 λ (θ) can be obtained from the minimizer of Δ 1 (λ). The optimal λ for Eq. (  In this paper, the energy minimization problem was solved using a customized Newton algorithm with the Armijo line-search method "
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,3,Examples,
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,3.1,Synthetic Data,"The proposed algorithms were examined using synthetic rdMRI data with an RDD function consisting of three Gaussian components with the mean at (1.5 µm 2 /ms, 10 ms -1 ), (0.5 µm 2 /ms, 40 ms -1 ), and (1.5 µm 2 /ms, 40 ms -1 ) with the volume fraction being 0.2, 0.5 and 0.3, respectively. D and R were uncorrelated in each component, with the standard deviation being 0.01 µm 2 /ms and 5ms -1 . Simulated rdMRI signals had b-values at b = 0, 0.5 ms/μm 2 , . . . , 5 ms/µm 2 and TEs at t = 50 ms, 75 ms, . . . , 200 ms, which can be achieved using an advanced MRI scanner for in vivo human brains such as the connectom scanner "
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,3.2,Comparison Methods,"For comparison, we applied the basis-function representation method, similar to the methods in  where θ n are a set of predefined points on a discrete grid in Θ. We solved a constrained L 2 minimization problem to find the optimal non-negative coefficient c n with minimum L 2 norm. To evaluate the performances, we computed the error of the center of mass (CE) of the estimated RDD functions in three regions defined using a watershed clustering approach; see the top left figure in Fig.  The CE in diffusivity and re Moreover, we also computed the volume-fraction error (VFE), i.e., VFE = K k=1 |F est (k)-F true (k)|, of the estimated RDD, where F est (k) and F true (k) denote the true and estimated volume fraction for each component."
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,3.3,In Vivo rdMRI,The proposed ME algorithms were applied to an in vivo rdMRI dataset acquired in our previous work 
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,4,Results,The first two figures in Fig. 
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,5,Summary,"In summary, this work introduced a maximum-entropy framework for estimating the relaxation-diffusion distribution functions using rdMRI. To our knowledge, this is the first work showing that the estimation of multidimensional RDD functions is equivalent to the classical multivariate Hausdorff moment problem. Although this work focuses on the two dimensional RDD functions, the results generalize to the special cases for one dimensional relaxation or diffusion distribution functions. The contributions of this work also include the development of three algorithms to estimate RDD functions and the comparisons with the standard basis-function approach. The ME-RDD functions can be estimated using convex optimization algorithms. Experimental results have shown that the proposed methods provide more accurate parameters for each component and more accurate volume fractions compared to the standard basis function methods. Moreover, results based on in vivo data have shown that the proposed ME-RDD can resolve multiple components that cannot be distinguished by the basis function approach. The better performance ME-RDD functions compared to basis-function methods may relate to the superior performance of ME spectral estimation methods compared to Fourier transform-based methods "
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,,,
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,,Fig. 1 .,
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,,Fig. 2 .,
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,,Fig. 3 .,
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,,Figure 3,
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,,,
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,1,Introduction,"Diffusion microstructure imaging has drawn increasing research attention in recent years. A number of powerful microstructure models have been proposed and shown great success in both clinical and research sides. Typical examples include diffusion kurtosis imaging (DKI)  To resolve this problem, deep learning has been introduced to learn the mapping between high-quality microstructure indices and q-space undersampled dMRI data. For instance, Golkov et al.  Recently, inspired by the fact that dMRI data live in a joint x-q space  To this end, we propose 3D-HGT, an advanced microstructure estimation model capable of making full use of 3D x-space information and q-space information jointly. Specifically, we design an efficient q-space learning module based on simple graph convolution (SGC)  Fig. "
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,2,3D Hybrid Graph Transformer,
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,2.1,Network Overview,"As shown in Fig.  The input dMRI data X ∈ R L×W ×H×G has four dimensions, where L, W , H, and G denote the length, width, height, and number of gradient directions, respectively. In our model, the data is first reshaped into a two-dimensional tensor X ∈ R LW H×G and fed into the SGC network to learn the q-space features efficiently. The output X q is then reshaped to four dimensions Xq ∈ R L×W ×H×G and enters into the x-space learning module followed by convolutional layers. Finally, our 3D-HGT generates microstructure predictions Y ∈ R L×W ×H×M , where M denotes the number of microstructure indices."
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,2.2,Efficient q-Space Learning Module,"According to  Different from HGT, 3D-HGT learns in q-space with SGC, which removes the non-linearity between the graph convolutional layers and collapses K graph convolutional layers into one. In this way, the complex computation of a multilayer network is reduced to a few matrix multiplications. Thus, we can use fast matrix multiplication to speed up computation, reduce network redundancy, and improve computational efficiency. Mathematically, the feature provided by our q-space learning module is as follows: where K denotes the number of SGC layers, Â is the normalized version of A with self-loop added, and Θ is the product of the weight matrices learned by the q-space learning module."
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,2.3,3D x-Space Learning Module,"The overall design of the x-space learning module is a U-shaped network composed of three parts: an encoder, a bottleneck, and a decoder. The three components are made up of cascaded self-attention transformer blocks. The encoder and decoder are similar in structure, with two self-attention layers. However, there are two key differences between them. Firstly, the former is composed of two local volume-based layers, while the latter has two wide volumebased layers. Secondly, before entering the network, data from the encoder passes through an embedding layer, while data from decoders pass through an extension layer for feature integration before outputting results. The bottleneck layer comprises two LSAs and one WSA, which allows skip attention to integrate shallow and deep attention between the encoder and decoder layers. Notably, we omit the pooling or un-pooling operations in the encoder-decoder architecture since pooling can lead to the loss of features, especially when dealing with small-sized 3D patches. The Embedding Layer. The embedding layer divides the input Xq into highdimensional patches, which are then sent into the transformer block. Unlike the nnFormer, our embedding layer consists of two convolutional layers, a GELU  Local Volume-Based Multi-head Self-attention (LSA). Based on Swin Transformer  Let Q, K, V ∈ R S L ×S W ×S H ×D for LSA be the query, key, and value matrices, and B ∈ R S L ×S W ×S H be the relative position matrix, where {S L , S W , S H } denotes the size of the local volume and D is the dimension of query/key. The self-attention is then computed in each 3D local volume as follows: The LSA computational complexity can be expressed as follows: where N P is the size of a patch, and C is the length of the embedding sequence. Wide Volume-Based Multi-head Self-attention (WSA). Although LSA is efficient, its receptive field is limited. Thus, by increasing the size of the window to {m × S L , m × S W , m × S H } with m denoting a constant (two by default), we have WSA, which improves the global context awareness ability of LSA. The computational complexity of WSA is as follows: We extend the self-attention field of view in the bottleneck using three wide-field transformer blocks, where six WSA layers are used. Skip Attention. This component effectively combines shallow and deep attention. A single-layer neural network decomposes the output X l at layer l of the encoder into a key matrix K l and a value matrix V l , while the output of X l at layer l of the decoder is used as a query matrix Q l . Mathematically, the self-attention is defined as: where B l is the relative position encoding matrix, and D l is the dimension of query/key."
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,3,Experiments,
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,3.1,Implementation Details,"Experimental Settings. We use the mean square error as the loss function, Adam as the optimizer, and an initial learning rate of 9e-4. We set the number of epochs to 100, batch size to 2, and iterations to 10. The angular threshold θ for constructing graph is set at 45 • , and the number of graph convolutional layers (i.e., K) is set to 2. In the x-space learning module, the embedding dimension is set to 192, and the window sizes of the LSA and WSA layers are set to 4 and 8, respectively. The model was implemented using PyTorch 1.11 and PyTorch-Geometric 2.1.0, and trained on a server equipped with an RTX 3090 GPU. Comparison Methods. We compare our 3D-HGT with various methods, including AMICO "
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,3.2,Dataset and Evaluation Metrics,"Dataset. Following  The validation and test sets are also processed in the same way. In our experiments, the ratio between training, validation, and test sets is 10:1:10. We train our model to predict NODDI-derived indices, including intracellular volume fraction (ICVF), isotropic volume fraction (ISOVF), and orientation dispersion index (ODI). The gold standard microstructural indices are computed using the complete HCP data with 270 gradients using AMICO  Evaluation Metrics. We evaluate the quality of predicted NODDI-derived indices with the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). "
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,3.3,Experimental Results,Table 
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,3.4,Ablation Study,"To investigate the effectiveness of the proposed modules, we perform ablation experiments with different ablated versions. The ablation results are shown in Table  The q-space learning with SGC also shows advantages over TAGCN. Both ""(B)"" and ""(E)"" improve the performance and computational efficiency in comparison with their corresponding ablated versions equipped with TAGCN, i.e., ""(A)"" and ""(D)"". In particular, compared with ""(D)"", the training time cost of ""(E)"" is reduced by nearly 30 s for one epoch, verifying the high efficiency of our q-space learning module. Effectiveness of 3D x-Space Learning Module. As shown in Table "
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,4,Conclusion,"In this paper, we proposed 3D-HGT, an improved microstructure estimation model that makes full use of 3D x-space information and q-space information. Our x-space learning is achieved with a 3D transformer architecture, allowing the model to thoroughly learn the long-term dependencies of features in the 3D spatial domain. To alleviate the large computational burden associated with 3D x-space learning, we further propose an efficient q-space learning module, which is built with a simplified graph learning architecture. Extensive experiments on the HCP demonstrate that, compared with HGT, 3D-HGT effectively improves the quality of microstructure estimation."
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,,Fig. 2 .,
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,,Table 1 .,
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,,Table 2 .,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,1,Introduction,Gadolinium-Based Contrast Agents (GBCAs) are widely used in MRI scans owing to their capability of improving the border delineation and internal morphology of different pathologies and have extensive clinical applications  Currently MRI dose simulation is done using physics-based models 
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,2,Methods,"Iterative Learning Design: DL based models tend to perform poorly when the training data is highly imbalanced  As shown in Fig.  where P pre , P post , and P low denote the pre-contrast, post-contrast, and predicted low-dose images, respectively and P i-1 denotes the image with a higher enhancement than P i . This way, the intermediate outputs { P i } k i=1 having different enhancement levels, correspond to images with different contrast dose level with a uniform interval. This iterative model essentially learns a gradual dose reduction process, in which each iteration step removes a certain amount of contrast enhancement from the full-dose image."
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,Loss Functions and Model Convergence:,"The proposed iterative model aims to learn a mapping from the post-contrast & pre-contrast images to the synthesized low-dose images P low and is trained with the true 10% low-dose image P low as the ground truth. We used the L 1 and structural similarity index measure (SSIM) losses. To tackle the problem of gradient explosion or vanishing, ""soft labels"" are generated using linear scaling. These ""soft labels"" serve as a reference to the intermediate outputs during the iterative training process and also aid model convergence, without which the model has to directly learn from post-contrast to low-dose. Given k iterations, the soft label {S i } k-1 i=1 for iteration i is calculated as follows: where P post and P pre denote the skull-stripped post-contrast and pre-contrast images. γ = 0.1 represents the dose level of the final prediction, and τ = 0.1 denotes the threshold to extract the estimated contrast uptake U = ReLU( P post -P preτ ). Finally, the total losses are calculated as where L e = L L1 + L SSIM and α = 0.1 and β = 1. The ""soft labels"" are assigned a small loss weight so that they do not overshadow the contribution of the real low-dose image. Additionally, in order to recover the high frequency texture information and to improve the overall perceptual quality, adversarial  Global Transformer (Gformer): Transformer models have risen to prominence in a wide range of computer vision applications  Subsampling Attention: The sub-sampling is a key element in the Gformer block which generates a number of sub-images from the whole image as attention windows as shown in Fig.  d is the subsampled feature map. We set h, d = 0 to avoid any information loss during subsampling. These d 2 sub-feature maps are stacked onto the batch dimension as the attention windows for the transformer block shown in Fig. "
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,(c).,"Rotational Shift: Image rotation has been widely used as a data augmentation technique in preprocessing and model training. Here, to further capture the heterogeneous nature of the contrast uptake areas, we employ the rotational shift as a module to facilitate the representation power of the Gformer. To prevent information loss on the edges due to rotation, only small angles (e.g., 10 • , 20 • ) are used for rotation and residual shortcuts are also applied. Specifically, given the feature map M o ∈ R b×c×h×w , rotational shift is performed around the vertical axis of height/width. The rotated feature map M r ∈ R b×c×h×w is obtained by the following equation: where λ is the rotation angle. (p, q, x, y) and (p , q , x , y ) denote the pixel index in the feature map tensor before and after rotational shift, respectively."
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,3,Experiments and Results,"Dataset: With IRB approval and informed consent, we retrospectively used 126 clinical cases (113 training, 13 testing) from a internal private dataset Evaluation Results: Figure "
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,4,Discussions and Conclusion,"We have proposed a Gformer-based iterative model to simulate low-dose CE images. Extensive experiments and downstream task performance have verified the efficacy and clinical performance of the proposed model compared to other state-of-the art methods. In the future, further reader studies are required to assess the diagnostic equivalence of the simulated low-dose images. The model can be guided using physics-based models "
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,Fig. 1 .,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,P,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,Fig. 2 .,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,20 •,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,Fig. 3 .,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,Fig. 4 .,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,Fig. 5 .,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,Fig. 6 .,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,are suitable for such Iter 1 Iter 2 Iter i Iter k CE 1 CE i-1 CE k Gformer Gformer Gformer Gformer Post-contrast Low-dose Pre-contrast Pre- contrast Gformer Gformer Block CE i-1 CE i (a) (b) (c) Iterative Design .... Norm MHA Norm MLP (d) Transformer Gformer Block Gformer Block Gformer Block Gformer Block Gformer Block Gformer Block Transformer Subsampling Rotational Shift 3×3 Conv,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,CE i CE: Contrast Enhancement,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,Table 1 .,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,Table 2 .,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_9.
Learning Normal Asymmetry Representations for Homologous Brain Structures,1,Introduction,"(Sub)cortical brain structures are approximately symmetrical between the left and right hemispheres  This paper introduces a novel framework for learning NORmal Asymmetries of Homologous cerebral structures (deep NORAH) based on anomaly detection and representation learning. Unlike previous methods that train Siamese neural networks with volume descriptors  In summary, our contributions are as follows: (i) ours is the first unsupervised deep learning model explicitly designed to learn normal asymmetries in homologous brain structures; (ii) although it is trained only with normal data, it can be used to detect diseased samples by quantifying the degree of deviation with respect to a healthy population , unlike existing methods that capture only disease-specific asymmetries "
Learning Normal Asymmetry Representations for Homologous Brain Structures,2,Methods,Figure 
Learning Normal Asymmetry Representations for Homologous Brain Structures,2.1,Pre-training the Shape Characterization Encoder as a CAE,"Our shape encoder f θEN indistinctly map an arbitrarily left or right segmentation x (i) of an homologous structure x, to a feature vector h * (i) that describes its shape. To this end, we apply a warm-up learning phase that trains f θEN as the encoding path of a CAE, using a self-supervised learning loss (Fig.  Formally, let (g θDE •f θEN )(x (i) ) be a convolutional CAE with a decoding path g θDE (h * (i) ) with parameters θ DE that outputs a reconstruction x(i) of the input x (i) from its hidden representation h * (i) . This is achieved by minimizing a mean square error objective (Fig. "
Learning Normal Asymmetry Representations for Homologous Brain Structures,2.2,Learning Normal Asymmetries with a Siamese Network,"Asymmetry Projection Head. The purpose of our APH is to project the shape representation h * obtained by f θEN into a compact embedding z L-R (Fig.  One-Class Deep SVDD. In order to enforce z (L-R) to represent the asymmetry characteristics of normal individuals, we train the Siamese architecture F θ (x) in Fig.  The first term in Eq. 1 is a quadratic loss that penalizes the Euclidean distance of z (L-R) from the center c of a hypersphere S, that is implicitly determined by the distance itself in the representation space. The second term is a classic weight decay regularizer, controlled by λ. Notice that we do not contract S by explicitly penalizing its radius and samples lying outside its boundary, but by minimizing their mean Euclidean distance with respect to c  Deviation-from-Normal-Asymmetry Index. The distance between the asymmetry embedding of an unseen sample x and the center of the learned hypersphere, s(x; c) = F θ (x)c 2 , can be used as a deviation-from-normalasymmetry index: when the input x is a normal sample, its asymmetry embedding is expected to lie in the vicinity of c, then associated to a small s value; on the other hand, if x is the segmentation of a subject with abnormal asymmetries, its associated z (L-R) will lie afar from c, reporting a higher s value."
Learning Normal Asymmetry Representations for Homologous Brain Structures,3,Experimental Setup,"We studied our method for hippocampal asymmetry characterization as a use case. First, we tested its ability to capture deviations in asymmetry using synthetically altered hippocampi with increased deformations, in a controlled setting. Then, we indirectly evaluated its performance as a diagnostic tool for neurodegenerative conditions, using s as a deviation-from-normal-asymmetry index. Finally, we performed an ablation study to understand the influence of design factors such as the shape encoder architecture, APH size, and merging operation. Materials. We used a total of 3243 3D T1 brain MRIs, including 2945 from normal control (NC) individuals, 71 from patients with MCI, 179 with AD, and 16 and 32 with right (HSR) and left (HSL) hippocampal sclerosis, respectively. Samples were retrospectively collected from OASIS  The training set was used to learn patterns of normal asymmetry, with NC from ROFFO (63), IXI (539), and 70% of the NC from OASIS. Ages ranged from 19 to 95 years old, to ensure capturing normal variations due to natural aging. 60 NC images were kept aside for validation (see below). The test sets, were used to evaluate the diagnostic ability of our method on different cohorts. TEST-ADNI and TEST-HEC sets include all subjects from ADNI and HEC sets, while TEST-OASIS includes all AD and the remaining 30% of NC from OASIS. Images from different devices were aligned and normalized to a standard reference MNI T1 template using SPM12  Baselines. We compared our model with respect to other multiple approaches. To account for the standard clinical methods, we included the absolute and normalized volume differences (AVD and NVD, respectively), used in  The same backbone architecture was used, but with an additional FC layer that had softmax activation for classification, as in "
Learning Normal Asymmetry Representations for Homologous Brain Structures,4,Results and Discussion,
Learning Normal Asymmetry Representations for Homologous Brain Structures,4.1,Characterization of Normal and Disease Related Asymmetries,"To test our hypothesis that samples with abnormal hippocampal asymmetries deviate from the center of the normal hypersphere, we evaluated the distances s(x; c) between all samples in the validation and test sets and grouped them by disease category. The distribution of these distances is shown in Fig. "
Learning Normal Asymmetry Representations for Homologous Brain Structures,4.2,Comparison with Other Approaches,The evaluation results of different methods are displayed in Table  Ablation Analysis. Table 
Learning Normal Asymmetry Representations for Homologous Brain Structures,5,Conclusions,"We presented a novel anomaly detection-based method for automatically characterizing normal asymmetry in homologous brain structures. Supervised alternatives restrict the definition of normal individuals due to explicitly learning their differences with respect to subjects with a specific condition. This implies that they ignore the asymmetry in control subjects, capturing only the asymmetries induced by the analyzed disease "
Learning Normal Asymmetry Representations for Homologous Brain Structures,,Fig. 1 .,
Learning Normal Asymmetry Representations for Homologous Brain Structures,,Fig. 2 .,
Learning Normal Asymmetry Representations for Homologous Brain Structures,,Fig. 3 .,
Learning Normal Asymmetry Representations for Homologous Brain Structures,,Fig. 4 .,
Learning Normal Asymmetry Representations for Homologous Brain Structures,,Table 1 .,
Learning Normal Asymmetry Representations for Homologous Brain Structures,,w/o FC dim. red.) 0,
Learning Normal Asymmetry Representations for Homologous Brain Structures,,Table 2 .,
Learning Normal Asymmetry Representations for Homologous Brain Structures,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 8.
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,1,Introduction,"Diffusion MRI (dMRI) tractography is the only non-invasive method capable of mapping the complex white matter (WM) connections within the brain  In this work, we propose a novel point-cloud-based strategy that leverages neighboring and whole-brain streamline information to learn local-global streamline representations. Point clouds have been shown to be efficient and effective representations for streamlines  Affine or even nonrigid registration is needed for current tractography parcellation methods  In this study, we propose TractCloud, a registration-free tractography parcellation framework, as illustrated in Fig. "
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,2,Methods,
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,2.1,Training and Testing Datasets,"We utilized a high-quality and large-scale dataset of 1 million labeled streamlines for model training and validation. The dataset was obtained from a WM tractography atlas  For evaluation, we used a total of 120 subjects from four public datasets and one private dataset. These five datasets were independently acquired with different imaging protocols across ages and health conditions. (1) developing HCP (dHCP) "
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,2.2,TractCloud Framework,"Synthetic Transform Data Augmentation. To enable tractography parcellation without registration, we augmented the training data by applying synthetic transform-based augmentation (STA) including rotation, scaling, and translations. These transformations have been used in voxel-based WM segmentation  Module for Local-Global Streamline Representation Learning. We propose a module (Fig. "
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,2.3,Implementation Details,"To learn r i , we used 20 local streamlines (selected from 10, 20, 50, 100) and 500 global streamlines (selected from 100, 300, 500, 1000). Our framework was trained with the Adam optimizer with a learning rate of 0.001 using cross-entropy loss. The epoch was 20, and the batch size was 1024. Training of our registrationfree framework (TractCloud reg-free ) with the large STA dataset took about 22 h and 10.9 GB GPU memory with Pytorch (v1.13) on an NVIDIA RTX A5000 3 Experiments and Results"
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,3.1,Performance on the Labeled Atlas Dataset,"We evaluated our method on the original labeled training dataset (registered and aligned) and its synthetic transform augmented (STA) data (unregistered and unaligned). We divided both the original and STA data into train/validation/test sets with the distribution of 70%/10%/20% by subjects (such that all streamlines from an individual subject were placed into only one set, either train or validation or test). For experimental comparison, we included two deep-learning-based state-of-the-art (SOTA) tractography parcellation methods: DCNN++ "
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,3.2,Performance on the Independently Acquired Testing Datasets,"We performed experiments on five independently acquired, unlabeled testing datasets (dHCP, ABCD, HCP, PPMI, BTP) to evaluate the robustness and generalization ability of our TractCloud reg-free framework on unseen and unregistered data. All compared SOTA methods (DeepWMA, DCNN++) and TractCloud regist were tested on registered tractography, and only TractCloud reg-free was tested on unregistered tractography. Tractography was registered to the space of the training atlas using an affine transform produced by registering the baseline (b = 0) image of each subject to the atlas population mean T2 image using 3D Slicer  As shown in Table  Figure "
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,4,Discussion and Conclusion,"We have demonstrated TractCloud, a registration-free tractography parcellation framework with a novel, learnable, local-global representation of streamlines. Experimental results show that TractCloud can achieve efficient and consistent tractography parcellation results across populations and dMRI acquisitions, with and without registration. The fast inference speed and robust ability to parcellate data in original subject space will allow TractCloud to be useful for analysis of large-scale tractography datasets. Future work can investigate additional data augmentation using local deformations to potentially increase robustness to pathology. Overall, TractCloud demonstrates the feasibility of registration-free tractography parcellation across the lifespan."
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,,Fig. 1 .,
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,,,
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,,Fig. 2 .,
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,,,
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,,Fig. 3 .,
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,,Table 1 .,
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,,Table 3 .,
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_40.
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,1,Introduction,"Recent advances in diffusion MRI (dMRI) and diffusion signal modeling equip brain researchers with an in vivo probe into microscopic tissue compositions  Multi-compartment models are typically used to characterize signals from, for example, intra-and extra-neurite compartments  Here, we propose a unified strategy to estimate using MTE diffusion data (i) compartment specific T 2 relaxation times; (ii) non-T 2 -weighted (non-T 2 w) parameters of multi-scale microstructure; and (iii) non-T 2 w multi-scale fODFs. Our method, called relaxation-diffusion spectrum imaging (RDSI), allows for the direct estimation of non-T 2 w volume fractions and T 2 relaxation times of tissue compartments. We evaluate RDSI using both ex vivo monkey and in vivo human brain MTE data, acquired with fixed diffusion times across multiple b-values. Using RDSI, we demonstrate the TE dependence of T 2 w fODFs. Furthermore, we show the diagnostic potential of RDSI in differentiating tumors and normal tissues."
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,2,Methods,
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,2.1,Multi-compartment Model,"The diffusion-attenuated signal S(τ, b, g) acquired with TE τ , diffusion gradient vector g, and gradient strength b can be modeled as which can be expanded to a multi-compartment model: to account for signals S r (b, g), S h (b, g), and S f (b) and T 2 values of restricted, hindered, and free compartments. The apparent relaxation rates at different b-values, r(b) = 1/T 2 (b), can be estimated using single-shell data acquired with two or more TEs  where the compartment-specific response functions R(b, g, D r ), R(b, g, D h ), and R(b, D f ) are associated with apparent diffusion coefficients D r , D h , and D f , yielding compartment-specific multi-scale fODFs f (D r ), f (D h ), and f (D f ). Operator A relates the spherical harmonics coefficients to fODF amplitudes."
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,2.2,Model Simplification via Spherical Mean,"The spherical mean technique (SMT)  where w(D r ), w(D h ), and w(D f ) are volume fractions and k(b, D r ), k(b, D h ), and k(b, D f ) are spherical means of response functions R(b, g, D r ), R(b, g, D h ), and R(b, D f ), respectively. Based on  where D r , D h , and D f are parameterized by parallel diffusivity λ and perpendicular diffusivity λ ⊥ for the restricted (Λ r ), hindered (Λ h ) and free (Λ f ) compartments. φ is the geometric tortuosity "
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,2.3,Estimation of Relaxation and Diffusion Parameters,"We first solve for the relaxation modulated spherical mean coefficients in  (i) Relaxation modulated spherical mean coefficients. We rewrite (4) in matrix form as where the mean signal S is expressed as the product of the response function spherical mean matrix K and the Kronecker product (•) of relaxation matrix E and volume fraction matrix W. We can solve for X in (  (ii) Relaxation times. With X solved, E and W can be determined by minimizing a constrained non-linear multivariate problem: which can be solved using a gradient based optimizer. Relaxation times can be determined based on E. (iii) fODFs. With E determined, (3) can be rewritten as a strictly convex quadratic programming (QP) problem: which can be solved using the OSQP solver."
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,2.4,Microstructure Indices,"Based on (3) and (  -Microscopic fractional anisotropy  , where 0 is a pulse scale that only depends on the pulse width δ and diffusion time Δ of the diffusion gradients. ), which is independent on ."
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,2.5,Data Acquisition and Processing,Ex Vivo Data. We used an ex vivo monkey dMRI dataset
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3,Results,
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.1,Ex Vivo Data: Compartment-Specific Parameters,Figure 
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.2,In Vivo Data: Compartment-Specific Parameters,Figure  Figure 
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.3,In Vivo Data: Neurite Morphology,Figure  Figures 
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.4,Relation Between Relaxation and Diffusivity,Figure 
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.5,fODFs,Figure 
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,4,Conclusion,"RDSI provides a unified strategy for direct estimation of relaxation-independent volume fractions and compartment-specific relaxation times. Using MTE data, we demonstrated that RDSI can delineate heterogeneous tissue microstructure elusive to STE data. We also showed that RDSI provides information that is conducive to characterizing tissue abnormalities."
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,,Fig. 1 .,
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,,Fig. 2 .,
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,,Fig. 3 .,
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,,Fig. 4 .,
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,,2 b,
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,,Fig. 5 .,
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,,Fig. 6 .,
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,1,Introduction,"Cell recognition serves a key role in exploiting pathological images for disease diagnosis. Clear and accurate cell shapes provide rich details: nucleus structure, cell counts, and cell density of distribution. Hence, pathologists are able to conduct a reliable diagnosis according to the information from the segmented cell, which also improves their experience of routine pathology workflow  In recent years, the advancement of deep learning has facilitated significant success in medical images  Work has been devoted to reducing dependency on manual annotations recently. Qu et al. use points as supervision  CNNs with inductive biases have priority over local features of the nuclei with dense distribution and semi-regular shape. In this paper, we proposed a simple but effective framework for unsupervised cell recognition. Inspired by the strong representation capability of self-supervised learning, we devised the prior self-activation maps (PSM) as the supervision for downstream cell recognition tasks. Firstly, the activation network is initially trained with self-supervised learning like predicting instance-level contrastiveness. Gradient information accumulated in the shallow layers of the activation network is then calculated and aggregated with the raw input information. These features extracted from the activation network are then clustered to generate pseudo masks which are used for downstream cell recognition tasks. In the inferring stage, the networks which are supervised by pseudo masks are directly applied for cell detection or segmentation. To evaluate the effectiveness of PSM, we evaluated our method on two datasets. Our framework achieved comparable performance on cell detection and segmentation on par with supervised methods. Code is available at https://github.com/cpystan/PSM."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,2,Method,The structure of our proposed method is demonstrated in Fig. 
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,2.1,Proxy Task,"We introduce self-supervised learning to encourage the network to focus on the local features in the image. And our experiments show that neural networks are capable of adaptively recognizing nuclei with dense distribution and semi-regular shape. Here, we have experimented with several basic proxy tasks below."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,,ImageNet Pre-training:,"It is straightforward to exploit the models pre-trained on natural images. In this strategy, we directly extract the gradient-weighted feature map in the ImageNet pre-trained network and generate prior self-activation maps. Contrastiveness: Following the contrastive learning  where L dis is the loss function. Z l , Z r , and Z n are representations of the input sample, the positive sample, and the negative sample, respectively. In addition, dif f (•) is a function that measures the difference of embeddings. Similarity: LeCun et al.  Here, maximizing the similarity of two embeddings is equal to minimizing their difference."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,2.2,Prior Self-activation Map,"The self-supervised model U ss is constructed by sequential blocks which contain several convolutional layers, batch normalization layers, and activation layers. The self-activation map of a certain block can be obtained by nonlinearly mapping the weighted feature maps A k : where I am is the prior self-activation map. A k indicates the k-th feature map in the selected layer. α k is the weight of each feature map, which is defined by global-average-pooling the gradients of output z with regard to A k : where i, j denote the height and width of output, respectively, and N indicates the input size. The obtained features are visualized in the format of the heat map which is later transformed to pseudo masks by clustering. Semantic Clustering. We construct a semantic clustering module (SCM) which converts prior self-activation maps to pseudo masks. In SCM, the original information is included to strengthen the detailed features. It is defined as: where I f denotes the fused semantic map, I raw is the raw input, β is the weight of I raw . To generate semantic labels, an unsupervised clustering method K-Means is selected to directly split all pixels into several clusters and obtain foreground and background pixels. Given the semantic map I f and its N pixel features The goal is to find S to reach the minimization of withinclass variances as follows: where c i denotes the centroid of each cluster S i . After clustering, the pseudo mask I sg can be obtained."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,2.3,Downstream Tasks,"In this section, we introduce the training and inferring of cell recognition models. Cell Detection. For the task of cell detection, a detection network is trained under the supervision of pseudo mask I sg . In the inferring stage, the output of the detection network is a score map. Then, it is post-processed to obtain the detection result. The coordinates of cells can be got by searching local extremums in the score map, which is described below: where T (m,n) denotes the predicted label at location of (m, n), p is the value of the score map and D (m,n) indicates the neighborhood of point (m, n). T (m,n) is exactly the detection result. Cell Segmentation. Due to the lack of instance-level supervision, the model does not perform well in distinguishing adjacent objects in the segmentation. To further reduce errors and uncertainties, the Voronoi map I vor which can be transformed from I sg is utilized to encourage the model to focus on instance-wise features. In the Voronoi map, the edges are labeled as background and the seed points are denoted as foreground. Other pixels are ignored. We train the segmentation model with these two types of labels. The training loss function can be formulated as below, where λ is the partition enhancement coefficient. In our experiment, we discovered that false positives hamper the effectiveness of segmentation due to the ambiguity of cell boundaries. Since that, only the background of I sg will be concerned to eliminate the influence of false positives in instance identification."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,3,Experiments,
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,3.1,Implementation Details,"Dataset. We validated the proposed method on the public dataset of Multi-Organ Nuclei Segmentation (MoNuSeg)  In the experiment on BCData, precision (P), recall (R), and F1-score are used to evaluate the detection performance. Predicted points will be matched to ground-truth points one by one. And those unmatched points are regarded as false positives. Precision and recall are: P = T P/(T P + F P ), and R = T P/(T P + F N). In addition, we introduce MP and MN to evaluate the cell counting results. 'MP' and 'MN' denote the mean average error of positive and negative cell numbers. Hyperparameters. Res2Net101 "
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,3.2,Result,"This section includes the discussion of results which are visualized in Fig.  Compared with the method  Detection. Following the benchmark of BCData, metrics of detection and counting are adopted to evaluate the performance as shown in Table  Ablation Study. Ablation experiments are built on MoNuSeg. In our pipeline, the activation network can be divided into four layers which consists of multiple basic units including ReLU, BacthNorm, and Convolution. We exploit the prior self-activation maps generated from different depths in the model after training with the same proxy tasks. As shown in Fig. "
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,4,Conclusion,"In this paper, we proposed the prior self-activation map (PSM) based framework for unsupervised cell segmentation and multi-class detection. The framework is composed of an activation network, a semantic clustering module (SCM), and the networks for cell recognition. The proposed PSM has a strong capability of learning low-level representations to highlight the area of interest without the need for manual labels. SCM is designed to serve as a pipeline between representations from the activation network and the downstream task. And our segmentation and detection network are supervised by the pseudo masks. In the whole training process, no manual annotation is needed. Our unsupervised method was evaluated on two publicly available datasets and obtained competitive results compared to the methods with annotations. In the future, we will apply our PSM to other types of medical images to further release the dependency on annotations."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,,Fig. 1 .,
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,,Fig. 2 .,
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,,Fig. 3 .,
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,,Table 1 .,
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,,Table 2 .,
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,,Table 3 .,
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 54.
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,1,Introduction,"Filamentous objects, such as microtubules, actin filaments, and blood vessels, play a fundamental role in biological systems. For example, microtubules (See Fig.  Since the advent of deep learning, region-based instance segmentation methods  Region-free methods utilize learned embedding  Lacking instance-level labels of filaments is another challenge. Most existing approaches for filaments extraction adopt traditional computer vision techniques such as morphological operations  When a human tries to manually extract filaments in Fig.  Our method is fundamentally different from  The major contributions of this work are as follows: (1). To our knowledge, our method is the first method that converts the instance segmentation into a sequence modeling problem. Our proposed method mimics human tracing and extracting individual filament, tackling the challenges of extracting filamentous objects. "
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,2,Method,Figure 
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,2.1,Pick: Tip Points Detection Module,We adapt the U-shaped structure from 
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,2.2,Trace: A Recurrent Network for Filament Tracing,"Network Description. After tip points are detected, the tracing module will trace and extract each instance. As shown in Fig.  Predicting the Next Points. At each step, the decoder regresses offset maps where each pixel predicts a vector pointing to the center of the next patch. We use Hough Voting to decide the exact coordinates of the next center. Each pixel casts a vote to the next point, generating a heatmap of the number of votes for each pixel. The highest response point will be selected as the next center. Loss Function. We use binary cross entropy (BCE) for the tip prediction. For the tracing module, we use BCE for stop flag and mask prediction and L 1 loss for offset prediction. The final loss for tracing module is T is the number of steps for tracing, and s, A, M stand for stop flag, binary mask, and offset maps. λ 1 , λ 2 , λ 3 are the balance parameters and set as one. Training and Inference. We use patches as input for training, and the labels are the corresponding offset map, binary mask, and stop flag. The offset map is generated by computing the distance vector between pixels in the current patch "
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,3,Experiments,Our model is implemented with Pytorch 
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,3.1,Synthetic Dataset,We first create eight synthetic filament datasets. The statistics of generated datasets are shown in Table 
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,3.2,Microtubule Dataset,"We annotated 15 microscopic images of microtubules with a size of 1376 × 1504. The number of filaments per image is 631 ± 167, and the length of filaments is 104 ± 96. We use a modified U-net  Figure "
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,3.3,P. Rubescens Dataset,"P. rubescens is a type of filamentous cyanobacteria, and Zeder et al. "
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,3.4,C. Elegans Dataset,We further investigate our model's performance on the C. elegans roundworm dataset (Fig.  Table 
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,4,Conclusion,"We present a novel method for filament extraction by transforming the instance segmentation problem to a sequence modeling problem. Our method comprises of a sequential encoder-decoder framework to imitate humans extracting filaments and address the challenges brought by filaments' properties, including crossover, spanning and non-rigidity. The experiments show that our method can achieve better or comparable results on filament datasets from different domains. Our method can alleviate the data shortage problem as our models trained on synthetic dataset achieve a better performance on microtubules and P. rubescens dataset. We also train and evaluate our model on C. elegans dataset, achieving comparable results with thicker and shorter filaments. Our method exhibits limitations in tracing ""Y""-shaped junctions due to the limited directional information in 2D images. Future work will focus on extending the current method to 3D data."
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,,Fig. 1 .,
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,,Fig. 2 .,
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,,Fig. 3 .,
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,,Fig. 4 .,
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,,Fig. 5 .Fig. 6 .,
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,,Fig. 7 .,
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,,Fig. 8 .,
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,,Table 1 .,
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,,Table 2 .,
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 61.
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,1,Introduction,"Brain magnetic resonance imaging (MRI) has been increasingly used to assess future progression of cognitive impairment (CI) in various clinical and research fields by providing structural brain anatomy  Fig.  Even without task-specific category label information, brain anatomical structures provided by auxiliary MRIs can be employed as a prior to boost disease progression prediction performance. Considering that there are a large number of unlabeled MRIs in existing large-scale datasets  To this end, we propose a brain anatomy-guided representation (BAR) learning framework for cognitive impairment prognosis with T1-weighted MRIs, incorporated with brain anatomy prior provided by a brain tissue segmentation task. As shown in Fig. "
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,2,Materials and Proposed Method,"Data and Preprocessing. The pretext model is trained via a tissue segmentation task on auxiliary MRIs (without category label) from ADNI. A total of 9,544 T1-weighted MRIs from 2,370 ADNI subjects with multiple scans are used in this work. To provide accurate brain anatomy, we perform image preprocessing and brain tissue segmentation for these MRIs to generate ground-truth segmentation of three tissues, i.e., white matter (WM), gray matter (GM) and cerebrospinal fluid (CSF), using an in-house toolbox iBEAT  The downstream model is trained on 1) a late-life depression (LLD) study with 309 subjects from two sites  Proposed Method. While it is often challenging to annotate MRIs in practice, there are a large number of MRIs (without task-specific category labels) in existing large-scale datasets. Even without category labels, previous studies propose to extract anatomical features (e.g., ROI volumes of GM segmentation maps) to characterize brain anatomy  (1) Pretext Model for Segmentation. To learn brain anatomical features from MRIs in a data-driven manner, we propose to employ a segmentation task for pretext model training. As shown in the top of Fig.  The decoder takes the 512 feature maps as input and outputs segmentation maps of three tissues (i.e., WM, GM, and CSF), thus guiding the encoder to learn brain anatomical features. The decoder contains four deconvolution blocks with 256, 128, 64 and 4 channels, respectively. Each deconvolution block shares the same architecture as the convolution block in the encoder. The output of the decoder is then fed into a SoftMax layer to get four probability maps that indicate the probability of a voxel belonging to a specific tissue (i.e., background, WM, GM, and CSF). Besides, the reconstruction task can be used to train the pretext model instead of segmentation when lacking ground-truth segmentation maps. For problems without ground-truth segmentation maps, we can resort to an MRI reconstruction task to train the pretext model in an unsupervised manner. (2) Downstream Model for Prediction. As shown in the bottom panel of Fig.  (3) Implementation. The proposed BAR is trained via two steps. 1) The pretext model is first trained on 9,544 MRIs from ADNI, with ground-truth segmentation as supervision. The Adam optimizer  Competing Methods. We compare our BAR with two classic machine learning methods and five SOTA deep learning approaches, including (1) support vector machine (SVM)  From the left of Table  From the right of Table  Results of MCI Detection. The results of different methods in MCI detection (i.e., MCI vs. HC classification) on the DM study are reported in Table  Segmentation Results. The pre-trained pretext model can also be used for brain tissue segmentation in downstream studies. Thus, we visualize brain segmentation maps generated by FSL and our BAR for target MRIs in both LLD and DM studies in Fig.  First, the segmentation results generated by the proposed BAR are generally better than those of FSL in most cases, especially for those cortical surface areas on the two studies. For instance, the WM region in segmentation maps generated by our BAR is much cleaner than that of FSL, indicating that our model is not sensitive to noise in MRI. Even for the LLD study with significant inter-site data heterogeneity, the boundary of WM and GM produced by BAR is more continuous and smoother, which is in line with the brain anatomy prior. Second, for MRIs with severe motion artifacts in the LLD study (IDs: 1240, 1334, and 1653), our method can produce high-quality segmentation maps, and the results are even comparable to those of MRIs without motion artifacts. This demonstrates that our model is robust to motion artifacts. The underlying reason could be that the pretext model is trained on large-scale MRIs, and thus, has good generalization ability when applied to MRIs with different image quality. In addition, both BAR and FSL often achieve better results in the DM study, since DM has relatively higher image quality than LLD. Still, the proposed BAR can achieve better segmentation results in many fine-grained brain regions, such as the putamen region (see HC001 and MCI003) and the vermis region (see HC004). These results demonstrate that our method has good adaptability when applied to classification and segmentation tasks in MRI-based studies. Ablation Study. To validate the effectiveness of the learned brain anatomical MRI features, we further compare the BAR with its two variants (called BAR-B and BAR-R) that use anatomy prior derived from different pretext tasks in CND vs. CN classification on LLD. Specifically, the BAR-B is trained from scratch as a baseline on target data without any pre-trained encoder. The BAR-R trains the pretext model through an MRI reconstruction task in an unsupervised learning manner. As shown in Fig.  Influence of Training Data Size. We also study the influence of training data size on BAR in CND vs. CN classification on LLD. With fixed test data, we randomly select a part of MRIs (i.e., [20%, 40%, • • • , 100%]) from target training data to fine-tune the downstream prediction model. It can be observed from Fig. "
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,4,Conclusion and Future Work,"In this paper, we develop a brain anatomy-guided representation (BAR) learning framework for MRI-based progression prediction of cognitive impairment, incorporated by brain anatomy prior (derived from an auxiliary tissue segmentation task). We validate the proposed BAR on two CI-related studies with T1-weighted MRIs, and the experimental results demonstrate its effectiveness compared with SOTA methods. Besides, the pretext model trained on 9,544 MRIs from ADNI can be well adapted to tissue segmentation in the two CI-related studies. There is significant intra-and inter-site data heterogeneity in LLD with two sites. It is interesting to reduce such heterogeneity using domain adaptation "
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,,Fig. 2 .,
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,,Fig. 3 .,
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,,Table 1 .,
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,,Table 2 .,
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,1,Introduction,"Diffusion-weighted MRI enables visualization of brain white matter structures. It can be used to generate tractography data consisting of millions of synthetic fibers or streamlines for a single subject stored in a tractogram that approximate groups of biological axons  Automated methods built on supervised machine learning algorithms have attained the current state-of-the-art in segmenting tracts  Manual methods are still frequently used for all cases not yet covered by automatic methods, such as certain populations like children, animal species, new acquisition schemes or special tracts of interests. Experts determine regions of interest (ROI) in areas where a particular tract is supposed to traverse or through which it must not pass, and segmentations can be accomplished either (1) by virtually excluding and maintaining streamlines from tractography according to the defined ROI or (2) by using these regions for tract-specific ROI-based tractography. Both approaches require a similar effort, although the latter is more commonly used. The correct definition of ROIs can be time-consuming and challenging, especially for inexperienced users. Despite these limitations, ROI-based techniques are currently without vivid alternatives for segmenting tracts that automated methods cannot handle. Methods to simplify tract segmentation have been proposed before. Clustering approaches were developed to reduce complexity of large amounts of streamlines in the input data  We propose a novel semi-automated tract segmentation method for efficient and intuitive identification of arbitrary white matter tracts. The method employs entropy-based active learning of a random forest classifier trained on features of the dissimilarity representation "
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,2,Methods,
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,2.1,Binary Classification for Tract Segmentation,"To create a segmentation of a white matter tract from an individual wholebrain tractogram T , streamlines which not belong to this tract must be excluded from the tractography data. This is formulated as a binary classification of a streamline s ∈ T , depending on whether it belongs to the target tract t (see Fig.  To perform the classification, supervised models have been trained on various features representing the data. We choose the dissimilarity representation proposed by Olivetti to classify streamlines, which has shown well performance and can be computed quickly for arbitrary data  ..m, is described by its minimum average direct flip distance d MDF to each prototype  where with m being the number of 3D points of the streamlines and Additionally to d MDF , the endpoint distance d END between a streamline and a prototype is calculated, which is equal to d MDF , besides, only the start points x 1 and endpoints x m of the streamline and prototype are respected for the calculation "
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,2.2,Active Learning for Tract Selection,"Commonly, for training classifiers, large amounts of annotated and potentially redundant data are used, leading to high annotation efforts and long training times. Active learning reduces both by training machine learning models with only small and iteratively updated labeled subsets of the originally unlabeled data. The proposed workflow is initialized, as shown in Fig.  Next, a subset S Emax of streamlines with the highest entropy or uncertainty is selected to be labeled by the expert and is added to the training data (Fig.  Since the model selects ambiguous streamlines in the target tract region, utilizing them as supplementary prototypes improves feature expressiveness in this region of interest. This process is repeated iteratively until the expert accepts the prediction."
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3,Experiments,
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3.1,Data,"The proposed technique was tested on a healthy-subject dataset and on a dataset containing tumor cases. The first comprises 21 subjects of the human connectome project (HCP) that were used for testing the automated methods TractSeg and Classifyber  To test the proposed method on pathological data, we used an in-house dataset containing ten presurgical scans of patients with brain tumors. Tractography was performed using probabilistic streamline tractography in MITK Diffusion. To reduce computational costs, we retained one million streamlines that passed through a manually inserted ROI located in an area traversed by the OR "
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3.2,Experimental Setup,"To evaluate the proposed method, we conducted two types of experiments. Manual segmentation experiments using an interactive prototype of atTRACTive were initiated on the tumor data (holistic evaluation). Additionally, reproducible simulations on the freely available HCP and the internal tumor dataset were created (algorithmic evaluation). In order to mimic expert annotation during algorithmic evaluation, class labels were assigned to streamlines using previously generated references. The quality of the predictions was measured by calculating the dice score of the binary mask. The code used for these experiments is publicly available For the algorithmic evaluation, the initial training dataset was created with 20 randomly selected streamlines from the whole-brain tractogram, which have been shown to be a decent number to start training. Since some tracts contain only a fraction of streamlines from the entire tractogram, it might be unlikely that the training dataset will contain any streamline belonging to the target tract. Therefore, two streamlines of the specific tract were further added to the training dataset, and class weights were used to compensate for the class unbalance. According to Fig.  The holistic evaluation was conducted with equal settings, except that the workflow was terminated when the prediction matched the expectation of the expert. To ensure that the initial dataset S rand contained streamlines from the target tract, the expert initiated the active learning workflow by defining a small ROI that included fibers of the tract. S rand was created by randomly sampling only those streamlines that pass through this ROI. To allow comparison between the proposed and traditional ROI-based techniques, the OR of subjects from the tumor dataset were segmented using both approaches by an expert familiar with the respective tool, and the time required was reported to measure efficiency. Note, in all experiments, the classifier is trained from scratch every iteration, prototypes are generated for each subject individually, and the classifier predicts on data from the same subject it is trained with, as it performs subject-individual tract segmentation and is not used as a fully automated method. To ensure a stable active learning setup that generalizes across different datasets, the whole method was developed on the HCP and applied with fixed settings to the tumor data "
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3.3,Results,"In Table  The initial manual experiments with atTRACTive were consistent with the simulations. The prediction aligned with the expectations of the expert at around five to seven iterations taking a mean of 4,5 min, while it took seven minutes on average to delineate the tract with ROI-based segmentation. During the iterations, streamlines around the target tract were suggested for labeling, and the prediction improved. Visual comparison yielded more false-positive streamlines with the ROI-based approach while atTRACTive created more compact tracts. "
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,4,Discussion,"Active learning-based white matter tract segmentation enables the identification of arbitrary pathways and can be applied to cases where fully automated methods are unfeasible. In this work, algorithmic evaluation as well as the implementation of the technique into the GUI-based tool atTRACTive including further holistic manual experiments were conducted. The algorithmic evaluation yielded consistent results from the fifth to the tenth iterations on both the HCP and tumor datasets. As expected, outcomes obtained from the tumor dataset were not quite as good as those of the HCP dataset. This trend is generally observed in clinical datasets, which tend to exhibit lower performance levels compared to high-quality datasets, which could be responsible for the decline in the results. Preliminary manual experiments with atTRACTive indicated active learning to have shorter segmentation times compared to traditional ROI-based techniques. These experiments are in line with the simulations as the generated tracts matched the expectations of the expert after around five to seven iterations, meaning that less than a hundred out of million annotated streamlines are required to train the model. Enhancements to the usability of the prototype are expected to further improve efficiency. A current limitation of atTRACTive is the selection of the initial subset, based on randomly sampling streamlines passing through a manually inserted ROI. This approach does not guarantee that streamlines of the target tract are included in the subset. In that case, the ROI has to be replaced or S rand needs to be regenerated. Future analyses, evaluating the inter-and intra-rater variability compared to other interactive approaches, will be conducted on further tracts. For selected scenarios, the ability of the classifier to generalize by learning from previously annotated subjects will be investigated, which may even allow to train a fully automatic classifier for new tracts once enough data is annotated. To further optimize the method, the feature representation or sampling procedure could be improved. Uncertainty sampling may select redundant streamlines due to similar high entropy values. Instead, annotating samples with high entropy values being highly diverse or correcting false classifications could convey more information. By introducing active learning into tract segmentation, we provide an efficient and intuitive alternative compared to traditional ROI-based approaches. atTRACTive has the potential to interactively assist researchers in identifying arbitrary white matter tracts not captured by existing automated approaches."
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,,Fig. 1 .,
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,,Fig. 2 .,
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,,Fig. 3 .,
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,,Fig. 4 .,
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,,Table 1 .,
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,1,Introduction,"In diffusion MRI, biophysical models offer a non-invasive means of probing the tissue micro-architecture of the human brain. Most models rely on closed-form formulas derived with simplifying assumptions such as short gradient pulse, Gaussian phase distribution, and the absence of compartmental exchange. The reliability and interpretability of these models diminish with deviation from these assumptions. Monte Carlo (MC) simulations  Microstructure fingerprinting (MF)  In this paper, we introduce a novel MF technique with the following key features: 1. MC simulation  Our method is able to efficiently and accurately probe microstructural properties such as cell size and membrane permeability without relying on assumptions associated with closed-form formulas."
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,2,Methods,
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,2.1,Fingerprint Dictionary,"The diffusion MRI signal at each voxel S is a combination of signals from multiple micro-environments, each represented by a signal fingerprint S i : where f [i] is the volume fraction of the i-th fingerprint, J i is a set of parameters characterizing the geometry of the corresponding micro-environment, and P is a set of acquisition parameters (e.g., pulse sequence, pulse duration, etc.). For brevity, we omit P as it is the same for all i's. In  1. Intra-axonal diffusion represented by packed cylinders with radii r ∈ {0.5, 2, 2.5, 3.0, 3.5, 4.0} µm and permeabilities κ's from 0 (impermeable) to 50× 10 -6 µm µs -1 . For b ≤ 3000s mm -2 (typical in most datasets), the diffusion signals of axons with radius from 0 to 2 µm are numerically indistinguishable. The atom for r = 0.5 µm summarizes the distribution in r ∈ [0, 2] µm and the atoms for r ∈  Cylinders are placed in an extra-cellular space to mimic realistic configurations. 2. Extra-axonal diffusion represented using a tensor model with λ λ ⊥ < τ 2 , with geometric tortuosity τ = 2.6. We choose 1.5 3. Intra-soma diffusion represented using impermeable spheres with radii from 0 to 20 µm. 4. Free-water diffusion represented using a tensor model with λ = λ ⊥ > λ soma-max , with λ soma-max = 1.0 × 10 -3 mm 2 s -1 is the maximum apparent intra-soma diffusivity. Parameters are chosen according to previous studies, covering the spectrum of biologically possible values in the human brain "
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,2.2,Solving the Bloch-Torrey Partial Differential Equation (BT-PDE),We employ SpinDoctor 
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,2.3,Solving for Volume Fractions,"From  From Φ, the volume fraction ν[i] of atom i is the 0-th order SH coefficient in ϕ i  Following  1. Solve the full signal (FS) problem: and estimate the FS problem volume fraction ν FS from Φ. 2. Solve the mean signal (MS) problem: 3. Iterative reweighing until convergence: where with ξ being a constant and ν 0 the geometric mean of ν FS and ν MS . We select the regularization parameters γ's using the Akaike information criterion (AIC) to balance the goodness of fit and complexity of the model. There is an empirical lower bound on the soma radius that can be detected via a mixture of somas (isotropic) and axons (anisotropic) when using the mean signal alone "
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,2.4,Radius Bias Correction,"The average axon radius for each voxel can be calculated by averaging the radii of the respective fingerprints, weighted by the volume fractions. Since the volume fractions ν are estimated from the normalized signal model  where ν[i] is the signal fraction and Si (b) is the spherical mean signal of the i-th atom. The unbiased volume fraction f [i] is given by allowing us to derive f [i] from ν[i]: Computing f requires the non-diffusion-weighted signal of each compartment Si (0). In our case, Si (0), scaled by an arbitrary factor, is known from the Spin-Doctor simulation. We define weight which is not affected by the scaling factor and therefore can be used to compute the unbiased weighted-average radius."
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,3,Experiments,"We validate our technique, called microstructure fingerprinting SMSI (MF-SMSI), using both in-silico and in-vivo data. The dictionary and synthetic data were generated with HCP-like parameters: 3 b-shells of 1000, 2000, 3000 s mm -2 , 90 directions per shell, diffusion time Δ = 43.1 ms, and pulse width δ = 10.6 ms "
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,3.1,Volume Fraction,"We evaluate the accuracy of MF-SMSI in volume fraction estimation by generating synthetic data following the model in  where ν and S are used to denote free-water (FW), intra-soma (IS), intra-axonal (IA), and extra-cellular (EC) volume fractions and signals. We set the ground truth volume fractions to ν FW = 0.1, ν IS = 0.2, and ν IA = 0.5. We generated 1000 instances of the signal with SNR = ∞, 50, 30, and 15. MF-SMSI estimates accurately the volume fraction of each compartment (Fig. "
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,3.2,Cell Size and Membrane Permeability,"To investigate the efficacy of our bias correction, we performed three experiments: 1. Axonal radius -We simulated signals for two impermeable axons with radii 2 µm and 4 µm, each with volume fraction 0.5. In Fig. "
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,3.3,In-vivo Data,"We compare our estimates with the axonal radius index r index axon from ActiveAx  Axonal radius and permeability are lower in deep white matter, especially at the body of the corpus callosum and the forceps major, where axons are myelinated and densely packed  The soma radii r soma in the cortical ribbon have a mean value of 11 µm, similar to what was reported in "
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,3.4,Histological Corroboration,"We compare our axonal radius estimate r axon , ActiveAx axonal radius index r index axon , and the effective axon radius r eff from  Figure "
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,4,Conclusion,"We have presented a microstructure fingerprinting method that can provide accurate and reliable measurements of tissue properties beyond diffusivity and anisotropy, allowing quantification of cell size and permeability associated with axons and somas."
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,,Fig. 1 .,
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,,Fig. 2 .,
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,,Fig. 3 .,
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,,Fig. 4 .,
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,,Fig. 5 .,
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,1,Introduction,"Clinical MRI exams account for the overwhelming majority of brain MRI scans acquired worldwide every year  Existing neuroimaging research studies  Over the last two years, machine learning approaches for cortical reconstruction on 1 mm MPRAGEs have emerged. Methods based on signed distance functions (SDF) like DeepCSR "
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,,Contribution:,"Our proposed method allows cortical analysis of brain MRI scans of any orientation, resolution, and MRI contrast without retraining, making it possible to use it out of the box for straightforward analysis of large datasets ""in the wild"". The proposed method combines two modules: a convolutional neural network (CNN) that estimates SDFs of the WM and pial surfaces, and a classical geometry processing module that places the surfaces while satisfying geometric constraints (no self-intersections, spherical topology, regularity). The CNN capitalizes on recent advances in domain randomization to provide robustness against changes in acquisition -in contrast with existing learning approaches that can only process images acquired with the same resolution and MRI contrast as the scans they were trained on. Finally, our method's classical geometry processing gives us geometric guarantees and grants instant access to an array of existing methods for cortical thickness estimation, registration, and parcellation "
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,,Further Related Work:,The parameterization of surfaces as SDFs has been combined with deep neural networks in several domains 
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,2,Methods,Our proposed method (Fig. 
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,2.1,Learning of SDFs,"This module estimates isotropic SDFs of the WM and pial surfaces of both hemispheres in a contrast-and resolution-independent fashion. It utilizes a domain randomization approach based on training a voxel-wise SDF regression CNN with synthetic data, which comprises volumetric segmentations and corresponding surfaces (real images are not used). Such training data can be obtained ""for free"" by running FreeSurfer on isotropic T1 scans (we used the HCP dataset  Given a 3D segmentation and four surface meshes (WM and pial surfaces for each hemisphere; see Fig.  As regression targets, we use voxel-wise SDFs computed from the WM and pial meshes for both hemispheres. The computation of the SDFs would greatly slow down CNN training if performed on the fly. Instead, we precompute them before training and deform them nonlinearly (along with the 3D segmentation) for geometric augmentation during training. While this is only an approximation to the real SDF, it respects the zero-level-set that implicitly defines the surface, and we found it to work well in practice. An example of a synthetic scan and target SDFs used to train the CNN are shown in Fig.  The regression CNN is trained by feeding the synthetic images to the network and optimizing the weights to minimize the L1 norm of the difference between the ground truth and predicted distance maps. In practice, we clip the SDFs at an absolute value of 5 mm to prevent the CNN from wasting capacity trying to model relatively small variations far away from the surfaces of interest. At test time, the input scan is upscaled to the isotropic resolution of the training data and pushed through the CNN to obtain the predicted SDFs."
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,2.2,Geometry Processing for Surface Placement,"To process real clinical scans, we first feed them to the trained CNN to predict the SDFs for the pial and WM surfaces for both hemispheres (Fig.  Specifically: let M = (X, K) denote a triangle mesh, where X = [x 1 , . . . , x V ] represents the coordinates of its V vertices (x v ∈ R 3 ), and K represents the connectivity. Let D w (r) be the SDF for the WM surface estimated by our CNN, where r is the spatial location. The objective function (""energy"") is the following: The first term in Eq. 1 is the fidelity term, which encourages the SDF to be zero on the mesh vertices; we squash the SDF through a tanh function to prevent huge gradients far away from zero. The second and third terms are regularizers that endow the mesh with a spring-like behavior  The pial surface is fitted with a very similar procedure, but using the predicted SDF of the pial surface. Figure "
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,2.3,Implementation Details,Our voxel-wise regression CNN is a 3D U-net 
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,3,Experiments and Results,
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,3.1,Datasets,"-HCP: we used 150 randomly selected subjects (71 males, ages: 29.9±3.4 years) from HCP  The availability of 1 mm MPRAGEs for some of the subjects enables us to process them with FreeSurfer and use the result as ground truth "
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,3.2,Competing Methods,"To the best of our knowledge, the only existing competing method for our proposed algorithm is SynthSR "
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,3.3,Results on the ADNI Dataset,"Figure  We then used the obtained parcellations to study the effect of Alzheimer's disease (AD) on cortical thickness, using a group study between AD subjects and elderly controls. For this purpose, we first fit a general linear model (GLM) to the cortical thickness at every parcel, using age, gender, and AD status as covariates. We then used the model coefficients to correct the thickness estimates for age and gender, and compared the thicknesses of the two groups. Figure  Finally, we studied the effect of aging on cortical thickness using the same GLM as above. Figure "
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,3.4,Results on the Clinical Dataset,"The clinical dataset, despite not being clustered into well defined groups as ADNI, enables us to evaluate our method with the type of data that it is conceived for: a heterogeneous set of brain MRI scans acquired ""in the wild"". Samples of such scans and outputs produced by our method are shown in Fig. "
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,3.5,Discussion and Conclusion,"We have presented a novel method for cortical analysis of clinical brain scans of any MRI contrast and resolution that does not require retraining. To the best of our knowledge, this is the first method seeking to solve this difficult problem. The method runs in 2-3 h but could be sped up by replacing some modules (e.g., the spherical registration) with faster learning methods. Our method provides accurate parcellation across the board, which is helpful in applications like diffusion MRI (e.g., for seeding or constraining tractography with surfaces and parcellations when a T1 scan is unavailable or is difficult to register due to geometric distortion of the diffusion-weighted images). However, we observed increased variability in cortical thickness when processing the highly heterogeneous clinical dataset. Future work will focus on improving the reliability of thickness measurements and provide a confidence for the quality of reconstruction and cortical thickness prediction for lower resolution scans. In such scenarios assessing and modeling geometric covariates (e.g., vertex-wise distance to the nearest slice or angle between surface and acquisition orientation) may help reduce such variability. Our method and the clinical dataset are publicly available, which enables researchers worldwide to capitalize on millions of retrospective clinical scans to perform cortical analysis currently unattainable in research studies, particularly for rare diseases and underrepresented populations."
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,,Fig. 1 .,
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,,Fig. 2 .,
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,,Fig. 3 .,
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,,Fig. 4 .,
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,,Fig. 5 .,
B-Cos Aligned Transformers Learn Human-Interpretable Features,1,Introduction,"Making artificial neural networks more interpretable, transparent, and trustworthy remains one of the biggest challenges in deep learning. They are often still considered black boxes, limiting their application in safety-critical domains such as healthcare. Histopathology is a prime example of this. For years, the number of pathologists has been decreasing while their workload has been increasing  As a result, research in explainable artificial intelligence is thriving  The usual way to interpret transformer-based models is to plot their multihead self-attention scores  • We propose the B-cos Vision Transformer (BvT) as a more explainable alternative to the Vision Transformer (ViT) "
B-Cos Aligned Transformers Learn Human-Interpretable Features,2,Related Work,"Explainability, interpretability, and relevancy are terms used to describe the ability of machine learning models to provide insight into their decision-making process. Although these terms have subtle differences, they are often used interchangeably in the literature  However, it is still controversial whether the above methods can correctly reflect the behavior of the model and accurately explain the learned function (model-faithfulness  Compared to CNNs, there is limited research on understanding transformers beyond attention visualization  and Activation Maximization  On the other hand, the ConceptTransformer "
B-Cos Aligned Transformers Learn Human-Interpretable Features,3,Methods,"We focus on the original Vision Transformer  To extract more information, this process is repeated h times in parallel (multi-headed self-attention). Each self-attention layer is followed by a fullyconnected layer consisting of two linear transformations and a ReLU activation. We propose to replace all linear transforms in the original ViT (Fig.  c(x, w) = cos(∠(x, w)), ∠...angle between vectors with the B-cos* transform  where B ∈ N. Similar to  To see the significance of these changes, we look at Eq. 4 and derive Fig.  Since |c(x, ŵ)| ≤ 1, equality is only achieved if x and w are collinear, i.e., if they are aligned. Intuitively, this forces the weight vector to be more similar to the input. Query, key, and value thus capture more patterns in an image -which the attention mechanism can then attend to. This can be shown visually by plotting the centered kernel alignment (CKA). It measures the similarity between layers by comparing their internal representation structure. Compared to ViTs, BvTs achieve a highly uniform representation across all layers (Fig. "
B-Cos Aligned Transformers Learn Human-Interpretable Features,4,Implementation and Evaluation Details,"Task-Based Evaluation: Cancer classification and segmentation is an important first step for many downstream tasks such as grading or staging. Therefore, we choose this problem as our target. We classify image patches from the public colorectal cancer dataset NCT-CRC-HE-100K  Domain-Expert Evaluation: Our primary objective is to develop an extension of the Vision Transformer that is more transparent and trusted by medical professionals. To assess this, we propose a blinded study with four steps: (i) randomly selecting images from the test set of TCGA-COAD-20X (32 samples) and Munich-AML-Morphology (56 samples), (ii) plotting the last-layer attention and transformer attributions for each image, (iii) anonymizing and randomly shuffling the outputs, (iv) submitting them to two domain experts in histology and cytology for evaluation. Most importantly, we show them all the available saliency maps without pre-selecting them to get their unbiased opinion. Implementation Details: In our experiments, we compare different variants of the B-cos Vision Transformer and the Vision Transformer. Specifically, we implement two versions of ViT: ViT-T/8 and ViT-S/8. They only differ in parameter size (5M for T models and 22M for S models) and use the same patch size of 8. All BvT models (BvT-T/8 and BvT-S/8) are derivatives of the corresponding ViT models. The B-cos transform used in the BvT models has an exponent of B = 2. We use AdamW with a cosine learning rate scheduler for optimization and a separate validation set for hyperparameter selection. Following the findings of "
B-Cos Aligned Transformers Learn Human-Interpretable Features,5,Results and Discussion,"Task-Based Evaluation: When trained from scratch, all BvT models underperform their ViT counterparts by about 2% on NCT-CRC-HE-100K and 3% on Munich AML-Morphology (Table "
B-Cos Aligned Transformers Learn Human-Interpretable Features,,Domain-Expert Evaluation:,"The results show that BvTs are significantly more trustworthy than ViTs (p < 0.05). This indicates that BvT consistently attends to biomedically relevant features such as cancer cells, nuclei, cytoplasm, or membrane "
B-Cos Aligned Transformers Learn Human-Interpretable Features,6,Generalization to Other Architectures,"We aim to explore whether the B-cos transform can enhance the interpretability of other transformer-based architectures. The Swin Transformer (Swin)  In our experiments (Table  Moreover, we would like to emphasize that the modified models have no negative impact on the model's performance. In fact, all metrics remain similar or even improve. The accumulated attention heads (we keep 50% of the mass) demonstrate that Bwin solely focuses on nuclei and other cellular features (Fig. "
B-Cos Aligned Transformers Learn Human-Interpretable Features,7,Conclusion,"We have introduced the B-cos Vision Transformer (BvT) and the B-cos Swin Transformer (Bwin) as two alternatives to the Vision Transformer (ViT) and the Swin Transformer (Swin) that are more interpretable and explainable. These models use the B-cos transform to enforce similarity between weights and inputs. In a blinded study, domain experts clearly preferred both BvT and Bwin over ViT and Swin. We have also shown that BvT is competitive with ViT in terms of quantitative performance. Moreover, using Bwin or transfer learning for BvT, we can even outperform the original models."
B-Cos Aligned Transformers Learn Human-Interpretable Features,,Fig. 1 .,
B-Cos Aligned Transformers Learn Human-Interpretable Features,,Fig. 2 .,
B-Cos Aligned Transformers Learn Human-Interpretable Features,,Fig. 6 .,
B-Cos Aligned Transformers Learn Human-Interpretable Features,,Table 1 .,
B-Cos Aligned Transformers Learn Human-Interpretable Features,,Table 2 .,
B-Cos Aligned Transformers Learn Human-Interpretable Features,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_50.
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,1,Introduction,"Recently, the study of exploring structural-functional relationship raises lots of attentions in neuroscience which helps to reveal individual behaviors of human brain  In this work, we propose the diverse generations with the idea of conditional GAN "
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,2,Method,
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,2.1,Model Overview,The framework of our proposed MCGAN is shown in Fig. 
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,2.2,MCGAN,"The adversarial objective of generating FC from SC with GAN model is defined as: ( where z ∈ R n×n denotes the noise sample from N (0, I) to introduce the diversity of the generated FC. x ∈ R n×n denotes the SC is the condition which instructs the generation of FC "
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,,Multi-contexts Discriminator,"The original discriminator learns the global representation of synthetic or real data to distinguish them. However, it is insufficient to provide powerful feedback to encourage the generator to predict realistic data. Many works improve the discriminator and show that the stronger the discriminator is, the better the generator is  Context Encoding. The context encoding is for feature extraction. For brain network, regions are represented as nodes and the links between regions are represented as edges. Since FC is defined as the correlation between the active brain regions, it represents the edge feature. However, most of the graph convolution networks (GCN) model the node feature extraction. To address the FC encoding, we adopt the edge-based graph convolution method for feature extraction  There are three basic modules, i.e. E2N module (edge to node), N2E module (node to edge), N2G module (node to graph). E2N module. Given a FC E ∈ R n×n , where E i,j denotes the edge between region i and region j, it aggregates the linking edges of region i into a node representation: where w i is trainable weight and N i is the feature of i t h node. N2E module. It propagates the feature of node i and node j to their linking edge: The network stacks the graph convolution layers for FC encoding. A graph convolution layer is comprised of a E2N module and a N2E module for FC feature learning and reserve the structure of FC. Meanwhile, the skip connection  The output layer of the network is comprised of a E2N module and a N2G module to encode the FC representation into the node representation and the graph representation in series. Multi Contexts Discrimination. The original discriminator D(•) classifies the input data to be real or fake. In contrast, our proposed multi-contexts discriminator provides three kind of supervisions, i.e. edge-level, node-level and graph-level, to implicitly and explicitly strengthen the generator. Mathematically, our discriminator loss is comprised of three parts: In the output layer, we first use three MLP to transform the three kinds of context representation. Then the D edge (•), D node (•), D graph (•) respectively classify the transformed edge representation, node representation and graph representation to be real or fake. Correspondingly, the objective of the generator is: The multi contexts supervision improves the discriminator, which implicitly strengthens the generator in adversarial training process. Meanwhile, it feedbacks the fine-grained information to the generator by backpropagation, which explicitly strengthens the generator and encourages it to predict realistic FC."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,,Monte-Carlo Mean Samples of FC,"FC represents the development of brain activities so that it is relatively dynamic. While SC represents the fibers connection to indicate the anatomical structure of brain  where y i denotes multiple real FC of each subject and m can be fixed or dynamic during training, which denotes the monte-carlo sampling, m = 1, 2, ...M , M is the maximum samples for each subject of dataset. The linear combination of FC enlarges the data space for supervision, making the generator predict diverse FC."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,,Reconstruction Loss of Generator,We adopt reconstruction loss in generator to make it predict realistic FC which includes the mean absolute error (MAE) and Pearson's correlation coefficient (PCC) between the generated FC and the training FC sample.
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,,Full Objective,"In summary, the objective function of the MCGAN is defined as: where L adv optimizes the adversarial training of the generator and discriminator, L rec optimizes the generator for realistic prediction."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,3,Experiments,
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,3.1,Setup,"Datasets. We evaluate our MCGAN on the Human Connectome Project (HCP) dataset  Evaluation Metrics. (a) Quality. Instinctively, for each subject, the generated FC should be similar to one of the real FC. Therefore, for each generated FC of a subject, we calculate the minimum mean absolute error and the maximum Pearson's correlation coefficient to evaluate the quality of generation with the most similar real FC. (b) Diversity. We use Frechet Inception distance (FID)  The lower FID score indicates the better quality and diversity. Implementation Details. Both of the generator and discriminator are in condition of SC and we set the almost equivalent parameters for them to ensure the efficient adversarial training. We respectively set the learning rate of 0.0001 and 0.0004 for generator and discriminator. At prediction stage, we generate corresponding number of FC samples with dataset for each subject (i.e. 20 for HCP and 10 for ADNI)."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,3.2,Results,Main Results. We compare our MCGAN with the original GAN  Ablation Study. We conduct ablation study to investigate the contribution of different components and the results are shown in Table  Visualization. We generate multiple FC for each subject and we visualize two FC of each subject. The visualization of the generated and real FC is shown in Fig. 
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,4,Discussion,"Despite this work predicts diverse FC to explore one-to-many SC-FC relationship, the generated FC is not quite diverse enough to some extent. In our implementation, we attempt to use the diffusion model "
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,5,Conclusion,"In this work, for diverse generations, we propose a multi-contexts discriminator based GAN named MCGAN, which provides three kind of supervisions to strengthen the generator, including edge-level, node-level and graph-level. We adopt edge-based graph convolution method for FC encoding and we utilize monte-carlo mean samples to enlarge the FC data for supervision. The experiments show that our method can generate diverse and meaningful FC from SC. We are the first to explore the one-to-many relationship between one subject's individual SC and multiple FC, which helps to reveal individual brain's staticdynamic structural-functional mode. We lead to the future work for further exploration and there are many questions to be resolved."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,,Fig. 2 .,
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,,Fig. 3 .,
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,,Fig. 4 .,
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,,Table 1 .,
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,,Table 2 .,
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,1,Introduction,"In clinical practice, Magnetic Resonance Imaging (MRI) provides important information for diagnosing and monitoring patient conditions  Related Work. Single-image super-resolution (SISR) aims at restoring a highresolution (HR) image from a low-resolution (LR) input from a single sequence and targets applications such as low-field MR upsampling or optimization of MRI acquisition  Originating from shape reconstruction "
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,2,Methods,"In this section, we first formally introduce the problem of joint super-resolution of multi-contrast MRI from only one image per contrast per patient. Next, we describe strategies for embedding information from two contrasts in a shared space. Subsequently, we detail our model architecture and training configuration. Problem Statement. We denote the collection of all 3D coordinates of interest in this anatomical space as Ω = {(x, y, z)} with anatomical function q : Ω → A. The image intensities are a function of the underlying anatomical properties A. Two contrasts C 1 and C 2 can be scanned in a low-resolution subspace Ω 1 , Ω 2 ⊂ Ω. Let us consider g 1 , g 2 : A → R that map from anatomical properties to contrast intensities C 1 and C 2 , respectively. We obtain sparse observations , where f i is composition of g i and q. However, one can easily obtain the global anatomical space Ω by knowing Ω 1 and Ω 2 , e.g., by rigid registration between the two images. In this paper, we aim to estimate f 1 , f 2 : Ω → R given I 1 and I 2 . Joint Multi-contrast Modelling. Since both component-functions f 1 and f 2 operate on a subset of the same input space, we argue that it is beneficial to model them jointly as a single function f : Ω → R 2 and optimize it based on their estimation error incurred in their respective subsets. This will enable information transfer from one contrast to another, thus improving the estimation and preventing over-fitting in single contrasts, bringing consistency to the prediction. To this end, we propose to leverage INR to model a continuous multi-contrast function f from discretely sampled sparse observations I 1 and I 2 . MCSR Setup. Without loss of generalization, let us consider two LR input contrasts scanned in two orthogonal planes p 1 and p 2 , where p 1 , p 2 ∈ {axial, sagittal, coronal}. We assume they are aligned by rigid registration requiring no coordinate transformation. Their corresponding in-plane resolutions are (s 1 ×s 1 ) and (s 2 × s 2 ) and slice thickness is t 1 and t 2 , respectively. Note that s 1 < t 1 and s 2 < t 2 imply high in-plane and low out-of-plane resolution. In the end, we aim to sample an isotropic (s × s × s) grid for both contrasts where s ≤ s 1 , s 2 . Implicit Neural Representations for MCSR. We intend to project the information available in one contrast into another by embedding both in the shared weight space of a neural network. However, a high degree of weight sharing could hinder contrast-specific feature learning. Based on this reasoning, we aim to hit the sweet spot where maximum information exchange can be encouraged without impeding contrast-specific expressiveness. We propose a split-head architecture, as shown in Fig.  where α and β are coefficients for the reconstruction loss of two contrasts. Note that for points {(x, y, z)} ∈ Ω 2 \ Ω 1 , there is no explicit supervision coming from low resolution C 1 . For these points, one can interpret learning C 1 from the loss in C 2 , and vice versa, to be a weakly supervised task."
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,,Implementation and Training.,"Given the rigidly registered LR images, we compute Ω 1 , Ω 2 ∈ Ω in the scanner reference space using their affine matrices. Subsequently, we normalize Ω to the interval [-1, 1] 3 and independently normalize each contrast's intensities to [0, 1]. We use 512-dimensional Fourier Features in the input. Our model consists of a four-layer MLP with a hidden dimension of 1024 for the shared layers and two layers with a hidden dimension of 512 for the heads. We use Adam optimizer with a learning rate of 4e-4 and a Cosine annealing rate scheduler with a batch size of 1000. For the multi-contrast INR models, we use MI as in Eq. 2 for early stopping. Implemented in PyTorch, we train our model on a single A6000 GPU. Please refer to Table  Model Selection and Inference. Since our model is trained on sparse sets of coordinates, it is prone to overfitting them and has little incentive to generalize in out-of-plane predictions for single contrast settings. A remedy to this is to hold random points as a validation set. However, this will reduce the number of training samples and hinder the reconstruction of fine details. For multi-contrast settings, one can exploit the agreement between the two predicted contrasts. Ideally, the network should reach an equilibrium between the contrasts over the training period, where both contrasts optimally benefit from each other. We empirically show that Mutual Information (MI)  Compared to image registration, we do not use MI as a loss for aligning two images; instead, we use it as a quantitative assessment metric. Given two ground truth HR images for a subject, one can compute the optimum state of MI. We observe that the MI between our model predictions converges close to such an optimum state over the training period without any explicit knowledge about it, c.f. Fig. "
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,3,Experiments and Results,"Datasets. To enable fair evaluation between our predictions and the reference HR ground truths, the in-plane SNR between the LR input scan and corresponding ground truth has to match. To synthetically create 2D LR images, it is necessary to downsample out-of-plane in the image domain anisotropically  Metrics. We evaluate our results by employing common SR  Baselines and Ablation. To the best of our knowledge, there are no prior data-driven methods that can perform MCSR on a single-subject basis. Hence, we provide single-subject baselines that operate solely on single contrast and demonstrate the benefit of information transfer from other contrasts with our proposed models.  Quantitative Analysis. Table  Qualitative Analysis. Figure "
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,4,Discussion and Conclusion,"Given the importance and abundance of large multi-parametric retrospective cohorts  In conclusion, we propose the first subject-specific deep learning solution for isotropic 3D super-resolution from anisotropic 2D scans of two different contrasts of complementary views. Our experiments provide evidence of inter-contrast information transfer with the help of INR. Given the supervision of only single subject data and trained within minutes on a single GPU, we believe our framework to be potentially suited for broad clinical applications. Future research will focus on prospectively acquired data, including other anatomies."
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,,Fig. 1 .,
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,,Fig. 2 .,
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,,Table 1 .,
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,,,
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_17.
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,1,Introduction,"In the recent literature, development of foundational models has been the main driving force in artificial intelligence, for example, large language models  In particular, we crawl figures and corresponding captions from scientific documents on PubMed Central, which is a free full-text archive of biomedical and life sciences journal literature at the U.S. National Institutes of Health's National Library of Medicine (NIH/NLM)  In this work, we tackle the above-mentioned limitations by introducing an automatic pipeline to generate dataset with subfigure-subcaption correspondence from scientific documents, including three major stages: medical figure collection, subfigure separation, subcaption separation & alignment. The final dataset, PMC-OA, consisting of 1.65M image-text pairs (not including samples from ROCO), covers a wide scope of diagnostic procedures and diseases, as shown in Fig.  Overall, in this paper, we make the following contributions: First, we propose an automatic pipeline to construct high-quality image-text biomedical datasets from scientific papers, and construct an image-caption dataset via the proposed pipeline, named PMC-OA, which is 8× larger than before. With the proposed pipeline, the dataset can be continuously updated. Second, we pre-train a vision-language model on the constructed image-caption dataset, termed as PMC-CLIP, to serve as a foundation model for biomedical domain. Third, we conduct thorough experiments on various downstream tasks (retrieval, classification, and VQA), and demonstrate state-of-the-art performance. The dataset and pre-trained model will be made available to the community. "
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,2,The PMC-OA Dataset,"In this section, we start by describing the dataset collection procedure in Sect. 2.1, followed by a brief overview of PMC-OA in Sect. 2.2."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,2.1,Dataset Collection,"In this section, we detail the proposed pipeline to create PMC-OA, a large-scale dataset that contains 1.65M image-text pairs. The whole procedure consists of three major stages: (i) medical figure collection, (ii) subfigure separation, (iii) subcaption separation & alignment, as summarised in Fig. "
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Medical Figure Collection (,Step 1 and 2 in Fig.  Subfigure Separation (Step 3 and 4 in Fig. 
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Subcaption Separation and Alignment (,Step 5 and 6 in Fig. 
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,2.2,Dataset Overview,"In this section, we provide a brief statistical overview of the collected dataset PMC-OA with UMLS parser  Discussion. Compared to pioneering works "
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,3,Visual-language Pre-training,"With our constructed image-caption dataset, we further train a visual-language model, termed as PMC-CLIP as shown in Fig.  Architecture. Given N image-caption training pairs, i.e., D = {(I i , T i )| N i=1 }, where I i ∈ R H×W ×C represents images, H, W, C are height, width, channel, and T i represents the paired text. We aim to train a CLIP-style visual-language model with an image encoder Φ visual and a text encoder Φ text . In detail, given a specific image-caption pair (I, T ), we encode it separately with a ResNet-based Φ visual and a BERT-based Φ text , the embedding dimension is denoted as d and the text token length as l: where v represents the embedding for the whole image, T refers to the sentence embedding, and t denotes the embedding for [CLS] token."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Image-Text Contrastive Learning (ITC).,"We implement ITC loss following CLIP  where y i2t , y t2i refer to one-hot matching labels, CE refers to InfoNCE loss "
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Masked Language Modeling (MLM).,"We implement MLM loss following BERT  Total Training Loss. The final loss is defined as L = L ITC + λL MLM , where λ is a hyper-parameter deciding the weight of L MLM , set as 0.5 by default. Disscussion. While we recognize a lot of progress in VLP methodology "
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,4,Experiment Settings,
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,4.1,Pre-training Datasets,ROCO  MedICaT 
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,MIMIC-CXR,"PMC-OA contains 1.65M image-text pairs, which we have explicitly conducted deduplication between ROCO."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,4.2,Downstream Tasks,
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Image-Text Retrieval (ITR). ITR contains both image-to-text(I2T,") and text-to-image(T2I) retrieval. We train PMC-CLIP on different datasets, and sample 2,000 image-text pairs from ROCO's testset for evaluation, following previous works  Classification. We finetune the model for different downstream tasks that focus on image classification. Spcifically, MedMINIST  Visual Question Answering (VQA). We evaluate on the official dataset split of SLAKE "
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,4.3,Implementation Details,"For the visual and text encoders, we adopt ResNet50 "
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,5,Result,"We conduct experiments to validate our proposed dataset, and the effectiveness of model trained on it. In Sec. 5.1, we first compare with existing large-scale biomedical datasets on the image-text retrieval task to demonstrate the superiority of PMC-OA. In Sect. 5.2, we finetune the model (pre-trained on PMC-OA) across three different downstream tasks, namely, retrieval, classification, and visual question answering. And we also perform a thorough empirical study of the pretraining objectives and the model architectures in Sect. 5.3. Note that, for all experiments, we use the default setting: ResNet50 for image encoder, and pre-train with both ITC and MLM objectives, unless specified otherwise."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,5.1,PMC-OA surpasses SOTA large-scale biomedical dataset,As shown in Table  Table 
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Methods,Pretrain Data DataSize I2T T2I R@1 R@5 R@10 R@1 R@5 R@10 
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,5.2,PMC-CLIP achieves SOTA across downstream tasks,"To evaluate the learnt representation in PMC-CLIP, we compare it with several state-of-the-art approaches across various downstream tasks, including imagetext retrieval, image classification, and visual question answering. Image-Text Retrieval. As shown in Table "
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Methods,Pretrain Data DataSize I2T T2I R@1 R@5 R@10 R@1 R@5 R@10 ViLT  Visual Question Answering. VQA requires model to learn finer grain visual and language representations. As Table 
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,5.3,Ablation Study,"Training Objectives. We pre-train PMC-CLIP with different objectives (ITC, MLM) for ablation studies, and summarize the results in the supplementary material (Table "
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Data Collection Pipeline.,"To demonstrate the effectiveness of subfiguresubcaption alignment, we compare PMC-CLIP with the model pretrained on dataset w/o alignment (Table  Visual Backbone. We have also explored different visual backbones, using the same setting as CLIP "
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,6,Conclusion,"In this paper, we present a large-scale dataset in biomedical domain, named PMC-OA, by collecting image-caption pairs from abundant scientific docu-ments. We train a CLIP-style model on PMC-OA, termed as PMC-CLIP, it achieves SOTA performance across various downstream biomedical tasks, including image-text retrieval, image classification, visual question answering. With the automatic collection pipeline, the dataset can be further expanded, which can be beneficial to the research community, fostering development of foundation models in biomedical domain."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Fig. 1 .,
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Fig. 2 .,
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Fig. 3 .,
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Table 2 .,
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Table 3 .,
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Table 4 .,
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 51.
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,1,Introduction,"Computational models of neurodegeneration aim to emulate the underlying physical process of how disease initiates and progresses over the brain from E. Thompson and A. Schroder-Contributed equally to this work as the co-second authors. a mechanistic point of view  Network metrics link spontaneous appearance of pathology to brain connectivity via various mechanistic hypotheses. For instance, Zhou et al.  For the spreading component, dynamical systems models emulate the spatiotemporal propagation process along the brain connectivity architecture  However, such models make specific choices on the underlying mechanism in the particular physical process. The complexity and heterogeneity of neurodegenerative conditions suggests that multiple processes may contribute and vary among individuals. To avoid making such assumptions, Garbarino et al.  In this work, we introduce an alternative model framework that non-linearly couples the effects of spontaneous appearance and spreading. We construct a Bayesian framework with an appropriate sparsity structure to estimate the mechanistic profile, in a similar way to "
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,2,Methodology,
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,2.1,Model Definition,"Baseline Model. One model of disease spreading  where denotes the element-wise product. It constructs the disease progression process from onset to late stage based on the single mechanism of network proximity. It assumes a constant local production rate across regions, thus the growth of concentration only depends on the level of biomarker propagating from the epicenter along the structural connectome to each node. This does not take into account the synergistic effect of other mechanisms. Coupled Model. Following  Network Metrics. The network metrics considered by the model are listed in Table "
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,2.2,Bayesian Framework,"In order to quantify the uncertainty of the estimation, we construct a Bayesian inference framework for our dynamic system, thus we are able to obtain distributions of parameters rather than deterministic values. Parameter Distributions. In this work we focus on modelling the dynamics of tau protein, which is widely hypothesized to be a key causative agent in AD. Its concentration can be measured in vivo by positron emission tomography (PET). We assume for subject i at jth scan time t ij , the measurement of tau concentration ĉ(t ij ) follows a normal distribution where the mean is the model prediction with best-fit parameters and the error is quantified by the standard deviation σ. The time gap δ ij (in years) between the baseline scan and the jth follow-up scan is available in the dataset. However the time from the disease onset to the baseline scan is unknown. Thus we need to estimate such time t onset i such that t ij = t onset i + δ ij . This time parameterization enforces the relevant locations among all scans fixed by given δ ij . Descriptions of the key model parameters are displayed in Table  Feature Selection with Sparsity. We account for the feature importance by estimating the weights of network metrics w = [w 1 , ..., w P ] T ∼ Dirichlet(β 1 , . . . , β P ). We seek the minimal set of network components that explain the data to define the mechanistic profile. Thus, we apply sparsity to the weight in a Bayesian way by introducing the Horseshoe prior  This horseshoe structure includes a global shrinkage parameter τ and local shrinkage parameters λ l , each following a half Cauchy distribution: λ l , τ ∼ HalfCauchy(0, 1). The flat tail from Cauchy Distribution allows the features with strong contribution to remain with a heavy tail of the density, while the sharp rise in density near 0 shrinks the weight of the features with weak signal."
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,2.3,Variational Inference,"Suppose x, z and θ represent the collections of observations, hidden variables and parameters respectively. Due to the complexity of the model structure, the posterior p θ (z | x) we are interested in is often intractable and hard to obtain analytically. Thus we use the variational distribution q φ (z) with φ as the variational parameters to approximate the posterior. The evidence lower bound, ELBO ≡ E q φ (z) [log p θ (x, z)log q φ (z)], can be used to approach the log likelihood, since the gap between them is the Kullback-Leibler divergence between the variational distribution and the posterior, which is larger or equal to 0: Thus the objective of the optimization is to maximize the ELBO. We use a normal distribution with a diagonal covariance matrix as the variational distribution to sample the hidden variables in the latent space, and then use proper parameter transformation to obtain the constrained hidden variables. The process is accomplished with the use of Pyro "
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,3,Experiments and Results,
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,3.1,Data Processing,"Brain Networks. Three types of connectivity were used to extract features for our coupled models: 1) the structural connectome, which contains the number of white matter fibre trajectories; 2) the matrix of the geodesic distance along the cortical surface; 3) the functional connectome, which reflects the synchrony of neural functioning among regions. The structural connectome is an average of 18 young and healthy subjects' connectomes from the Human Connectome Project  Tau-PET Data. We model the dynamics of tau protein measured by PET scans. We use the tau-PET standardized uptake value ratios (SUVRs) downloaded from the Alzheimer's Disease Neuroimaging Initiative (ADNI) 1 (adni.loni.usc.edu)  1 Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/ wp-content/uploads/how to apply/ADNI Acknowledgement List.pdf "
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,,Selection of Subjects.,"We include N = 110 subjects with at least two tau-PET scans, amyloid beta positive status and at least one region with positive tau signal, including healthy, cognitively impaired and AD subjects, since we aim to focus on the people with a potential to accumulate abnormal pathological tangles. We normalise the data of all the subjects (i = 1,..,N) between 0 and 1 by (tau itau min )/(tau maxtau min ) where tau min and tau max are calculated across all the subjects and regions, thus the differences in data scales among subjects and regions are maintained. Setting of Epicentres. For initialization, we assume pathology starts from candidate epicentres, to simulate the full process of disease progression from very early stages, i.e. prior to the baseline scan. We rank 34 pairs of bilateral cortex regions according to the total number of subjects that have positive tau signals, and pick the top eight pairs of regions as the candidate epicentres where the propagation of pathology is likely to start: inferior temporal cortex, banks of the superior temporal sulcus, fusiform gyrus, lateral orbitofrontal cortex, middle temporal gyrus, entorhinal cortex, parahippocampal gyrus and temporal pole. The four epicentres identified by Vogel et al. "
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,3.2,Results,We fix the initial tau level at each candidate epicenter and the end level of the plateau of all the subjects to be 0.01 and 1.5 and fit each subject using the baseline model and our coupled model respectively. For evaluation we use Pearson R correlation between the measured and model fitted values. Figure  Figure 
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,4,Conclusions,"We introduce a new Bayesian modelling framework that couples pathology appearance and spreading, by embedding the mechanistic profiles that consist of combinations of network metrics into the dynamic system of disease spreading. This improves the fitting of the observed pathology pattern, and provides a potential way to subtype subjects according to mechanistic profiles. For future work, we will validate the cohort-level mechanistic profiles derived from the identified subtypes using external datasets, and also verify the subtypes using other algorithms such as the SuStaIn "
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,,Fig. 1 .,
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,,Fig. 2 .Fig. 3 .,
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,,Fig. 4 .,
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,,Table 1 .,
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,,Table 2 .,
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 45.
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,1,Introduction,"Multiple sclerosis (MS) is a common inflammatory disease in the central nervous system (CNS), affecting millions of people worldwide  Deep learning has been widely used for MS lesion segmentation from brain MRI sequences  Overall, the contributions of this work are three-fold: -We propose a simple unified model CoactSeg that can be trained on both newlesion annotated two-time-point data and all-lesion annotated single-timepoint data in the same way, with the same input and output format; -We design a relation regularizer to ensure the longitudinal relations among all and new lesion predictions of the baseline, follow-up, and corresponding differential brains; -We construct an in-house MS-23v1 dataset, which includes 38 Oceania singletime-point 3D FLAIR scans with manual all-lesion annotations by experienced human experts. We will release this dataset publicly. "
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,2,Datasets,"We trained and evaluated our CoactSeg model on two MS segmentation datasets, as shown in Table  Finally, when conducting the mixed training, we used a fixed data split in this paper (i.e., 62 samples for training and 16 for validation in total). Note that we followed the setting of the public challenge "
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,3,Method,
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,3.1,Overview,
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,3.2,Multi-head Architecture,"For single-time-point samples x s ∈ X s , x s b and x s fu are identical as x s , and the difference map becomes an all-zero matrix x 0 d , with p s1 al , p s2 al and p s nl being the corresponding all-lesion and new-lesion predictions of x s . For two-time-point data x t ∈ X t , x t b and x t fu respectively denote the first and second time-point data samples, with p t1 al , p t2 al and p t nl being the all-lesion segmentation results at the first and second time-point and the new-lesion results of x t , respectively. In this way, we unify the learning of both single and two-time-point data with heterogeneous annotations by using the same model F θ , with the same input and output formats. Note that, inspired by semi-supervised learning  where Dice refers to the common Dice loss for medical segmentation tasks. We use a 3D VNet "
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,3.3,Longitudinal Relation Regularization,"Human experts usually identify new MS lesions by comparing the brain MRI scans at different time points. Inspired by this, we further propose a longitudinal relation constraint to compare samples from different time points: where ⊗ is a masking operation. The first term in (  where λ 1 and λ 2 are constants to balance different tasks."
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,4,Results,"Implementation Details. For training, we normalize all inputs as zero mean and unit variance. Then, among common augmentation operations, we use the random flip or rotation to perturb inputs. Since MS lesions are always small, we apply a weighted cropping strategy to extract 3D patches of size 80 × 80 × 80 to relieve the class imbalance problem "
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,,"Performance for MS Lesion Segmentation. Two MS tasks (i.e., newlesion segmentation on MICCAI-21 and all-lesion segmentation on our MS-23v1",Figure 
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,5,Conclusion,"In this paper, we have presented a unified model CoactSeg for new MS lesion segmentation, which can predict new MS lesions according to the two-time-point inputs and their differences while at the same time segmenting all MS lesions. Our model effectively exploits heterogeneous data for training via a multi-head architecture and a relation regularization. Experimental results demonstrated that introducing all-lesion single-time-point data can significantly improve the new-lesion segmentation performance. Moreover, the relation constraint also facilitates the model to capture the longitudinal MS changes, leading to a further performance gain. Our in-house MS-23v1 dataset will be made public to help the MS lesion research. Future works will explore more longitudinal relations to study the fine-grained MS changes as well as consider more powerful constraints to address the domain gap "
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,,Fig. 1 .,
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,,Figure 2,
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,,Figure 2,
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,,Fig. 2 .,
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,,Fig. 3 .,
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,,Figure 3,
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,,Fig. 4 .,
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,,Table 1 .,
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,,Table 2 .,
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,,Table 3 .,
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,1,Introduction,"Structural cerebral abnormalities commonly cause drug-resistant focal epilepsy, which may be cured with surgery. Focal cortical dysplasias (FCDs) are the most common pathology in children and the third most common pathology in adults undergoing epilepsy surgery  There has been significant work seeking to automate the detection of FCDs, with the aim of identifying subtle structural abnormalities in patients with lesions not identified by visual inspection, termed ""MRI negative""  Contributions. We propose a robust surface-based semantic segmentation approach to address the particular challenges of identifying FCDs (Fig. "
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,2,Methods,
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,2.1,Graph Convolutional Network (GCN) for Surface-Based Lesion Segmentation,"We consider the lesion detection problem as a surface-based segmentation task. For this purpose, cortical surface-based features (intensity, curvature, etc.; see Sect. 3.1) are extracted from each brain hemisphere and registered using FreeSurfer  U-Net Architecture. To segment lesions on the icosphere, we created a graphbased re-implementation of nnU-Net  Loss Functions. Following best practices for U-Net segmentation models  Distance Loss. To encourage the network to learn whole-brain context thereby reducing the number of false positives, we added an additional distance regression task. We train the model to predict the normalised geodesic distance d to the lesion boundary for every vertex, by applying an additional loss L dist to the non-lesional prediction, ŷi,0 of the segmentation output for vertex i. We use the mean absolute error loss, weighted by the distance so as not to overly penalise small errors in predicting large distances from the lesion: Classification Loss. To mitigate uncertainty in the correspondence between lesion masks and lesions, we used a weakly-supervised classification loss L class . For the ground truth c, examples were labelled as positive, if any of their vertices were annotated as positive. To predict this sample-level classification, we added a classification head to the deepest level (level 1) of the U-Net. The classification head contained a fully connected layer aggregating over all filters, followed by a fully-connected layer aggregating over all vertices, resulting in the classification output ĉ. This output was trained using cross-entropy: Deep Supervision. To encourage the flow of gradients through the entire U-Net, we use deep supervision at levels dist be the cross-entropy, dice and distance losses applied to outputs at level i, respectively. The model is trained on a weighted sum of all the losses, with w i ds the loss weight at level i:"
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,2.2,Data Augmentation,Data augmentations consisted of spatial augmentations and intensity augmentations (Fig.  3 Experiments and Results
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,3.1,Dataset and Implementation Details,"Dataset. For the following experiments, we used a dataset of post-processed surface-based features and manual lesion masks from 618 patients with FCD and 397 controls  In order to compare performance, the train/validation and test datasets were kept identical to those in the previously published vertex-wise classifier "
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,,Implementation,"Experiments. Using our graph-based adaptation of nnU-Net (GC-nnU-Net) and the previous MLP model as baseline, we ran an ablation study to measure the impact of the proposed auxiliary losses (Table  Evaluations. Model performances were compared according to their Area Under the Curve (AUC), which was calculated by computing the sensitivity and specificity at a range of prediction thresholds. For sensitivity calculations, due to uncertainty in the lesion masks, a lesion was considered detected if the prediction was within 20 mm of the original mask, as this corresponds with the inter-observer variability measured across annotators "
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,3.2,Results,Table 
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,4,Conclusions and Future Work,"This paper presents a robust and generalisable graph convolutional approach for segmenting focal cortical dysplasias using surface-based cortical data. This approach outperforms specificity baselines by 22-27%, which is driven by three newly-proposed components. First, treating the hemispheric surface as a single connected graph allows the network to model spatial context. Second, a classi-fication loss mitigates the impact of imprecise lesion masks by simplifying the task to predicting whether or not a lesion is present in every hemisphere. Third, a distance-from-lesion prediction task penalises false positives and encourages the network to consider the entire hemisphere. The results show a significant increase in specificity, both in terms of presence of any false positive predictions in non-lesional hemispheres and a reduced number of additional clusters in lesional hemispheres. From a translational perspective, this improvement in performance will increase clinical confidence in applying these tools to cases of suspected FCD, while additionally minimising the number of putative lesions an expert neuroradiologist would need to review. Future work will include systematic prospective evaluation of the tool in suspected FCDs and expansion of these approaches to multiple causes of focal epilepsy."
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,,Fig. 1 .,
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,,,
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,,Fig. 2 .,
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,,Table 1 .,
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,,Table 2 .,
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,,Table 3 .,
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,1,Introduction,"Microscopy is one of the most widely used imaging techniques that allows life scientists to analyse cells, tissues and subcellular structures with a high level of detail. However, microscopy images often suffer from degradation such as blur, noise and other artefacts, which may result in an inaccurate quantification and hinder downstream analysis. Therefore, deconvolution techniques are necessary to restore the images to improve their quality, thus increasing the accuracy of downstream tasks  where * represents convolution, y denotes the resulting image of an object x, which has been blurred with a point spread function (PSF) K, and degraded by noise n. Two classic image deconvolution methods widely used in microscopy and medical imaging are Wiener filter  In the computer vision field, numerous deep learning models have been trained on large datasets with the objective to learn a direct mapping between input and output domains  Inspired by the RL algorithm, the Richardson-Lucy Network (RLN)  To address the limitations of existing methods, we propose a novel lightweight model called LUCYD, which integrates the RL deconvolution formula and a Ushaped network. The main contributions of this paper can be summarised as: 1. LUCYD is a lightweight deconvolution method that embeds the RL deconvolution formula into a deep convolutional network that leverages the features extracted by a U-shaped module while maintaining low computational costs for processing 3D microscopy images and a high level of interpretability. 2. The proposed method outperforms existing deconvolution methods on both real and synthetic datasets, based on qualitative assessment and quantitative evaluation metrics, respectively. 3. We show that LUCYD has strong resistance to noise and can generalise well to new and unseen data. This ability makes it a valuable tool for practical applications in microscopy imaging fields where image quality is critical for downstream tasks yet training data are often scarce or unavailable."
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,2,Method,"The overall architecture of the proposed model is illustrated in Fig.  x estimate which aims to recover x in k steps. We bypass the requirement of k-1 preceding iterations with the correction module that generates a mask M to form an intermediate sharp image estimation through a single forward pass, allowing to rapidly process 3D data, as follows: Next, inspired by Li et al.  Specifically, we replace convolutions with a known PSF in steps (a) and (c) with forward projector f and backward projector b, which consist of sets of learnable convolutional layers. The produced update term u allows us to recondition the estimate z from the correction module into a sharp image through multiplication, i.e. the last step of image formation in the RL formula: x = z • u. The whole network can then be expressed as follows, By adhering to the image formation steps as prescribed by the RL formula, we maintain a high degree of interpretability, critical for real-world scenarios, where the accuracy and reliability of the generated results are of utmost importance.  "
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,2.1,Correction Module and Bottleneck,"The proposed correction module and bottleneck architectures consist of encoder blocks (EBs), decoder blocks (DBs), and multi-scale feature fusion blocks to facilitate efficient information exchange across different scales within the model. Feature Encoding. The features of the volumetric input image y ∈ R C×D×H×W are obtained through the first encoder block EB 1 in the correction module, and then encoded by a convolutional layer with a stride 2. Subsequently, the downsampled features are concatenated with the encoded features of the forward projection f from the update module and then fed to the bottleneck encoder EB 2 to integrate the information from both modules. Feature Fusion Block. Similarly to Cho et al.  where up-sampling (↑) and down-sampling (↓) is applied to allow for feature concatenation. The multi-scale features are then combined and processed by 1 × 1 and 3 × 3 convolutional layers, respectively, to allow the decoder blocks DB 1 and DB 2 to utilise information obtained on different scales. The structure of the blocks is shown in Fig.  Feature Decoding. Initially, the refined features are decoded in the bottleneck using a convolutional layer and residual block within the DB 2 . Next, these features are expanded with a convolutional layer to match the dimensions in both the correction and update modules. The resulting features are then concatenated with the output of FFBlock 1 and subsequently fed into decoder DB 2 within the correction module. The features are then mapped to the image dimensions resulting in mask M , which is summed with y to form z."
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,2.2,Update Module,"Inspired by the forward and backward projector functions  During forward projection (FP), shallow features are initially extracted by a single convolutional layer and then refined by a residual block. The output of f is then passed to Richardson-Lucy Division Block (RLDiv) which embeds the division of the raw image y by the channel-wise mean of the refined FP features. Next, we project the division result to a feature map to extract more information about the image. The visualisation of the process is in Fig. "
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,2.3,Loss Function,The entire model is trained end-to-end with a single loss function that combines the Mean Squared Error (MSE) and the Structural Similarity Index Measure (SSIM) as follows: where x is the ground truth sharp image and x is the model estimation of x (Table 
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,3,Experiments,
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,3.1,Setup,"Datasets. We assess the performance of LUCYD on both simulated phantom objects and real microscopy images. To achieve this, we use five sets of 3D grayscale volumes generated by Li et al.  To test the generalization capabilities of our method, we also include two blurry and noisy versions of the dataset, D nuc and D act , which utilize different image degradation processes for embryonic nuclei and membrane data. Additionally, we generate a mixed dataset by applying permutations of three Gaussian blur intensities (σ b = [1.0, 1.2, 1.5]) and three levels of additive Gaussian noise (σ n = [0, 15, 30]) to the ground truth volumes, and then test the ability of the model to generalize to volumes blurred with Gaussian kernels (σ b = [0.5, 2.0]) and additive Gaussian noise (σ n = [20, 50, 70, 100]) levels outside of the training dataset. The model is trained on patches of dimensions 32 × 64 × 64 that are randomly sampled from the training datasets. Moreover, we evaluate the model trained using synthetic phantom shapes on a real 3D light-sheet image of a starfish (private data) and widefield microscopy image of U2OS cell (from the dataset of "
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,3.2,Results,In Table 
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,4,Conclusion,"In this paper, we introduce LUCYD, an innovative technique for deconvolving volumetric microscopy images that combines a classic image deconvolution formula with a U-shaped network. LUCYD takes advantages of both approaches, resulting in a highly efficient method capable of processing 3D data with high efficacy. We have demonstrated through experiments on both synthetic and real microscopy datasets that LUCYD exhibits strong generalization capabilities, as well as robustness to noise. These qualities make it an excellent tool for crossdomain applications in various domains, such as biology and medical imaging. Additionally, the lightweight nature of LUCYD makes it computationally feasible for real-time applications, which can be crucial in various settings."
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,,Fig. 1 .,
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,,Fig. 2 .,
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,,Fig. 3 .,
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,,Fig. 4 .,
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,,Table 1 .,
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,,Table 2 .,
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,,Table 3 .,
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,,.9336/27.63,
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,1,Introduction,"Light-sheet fluorescence microscopy (LSFM), characterized by orthogonal illumination with respect to detection, provides higher imaging speeds than other light microscopies, e.g., confocal microscopy, via gentle optical sectioning  To realize dual-view LSFM fusion, recent pipelines adapt image fusers for natural image fusion to weigh between views by comparing the local clarity of images  Apart from limited FOV, another obstacle that hinders the adoption of natural image fusion methods into dual-view LSFM is the inability to distinguish sample structures from structural artifacts  Here, we propose BigFUSE to realize spatially consistent image fusion and exclude ghost artifacts. Main contributions are summarized as follows: • BigFUSE is the first effort to think of dual-view LSFM fusion using Bayes, which maximizes the conditional probability of fused volume regarding image clarity, given the image formation prior of opposing illumination directions. • The overall focus measure along illumination is modeled as a joint consideration of both global light scattering and local neighboring image qualities in the contourlet domain, which, together with the smoothness of focus-defocus, can be maximized as Likelihood and Prior in Bayesian. • Aided by a reliable initialization, BigFUSE can be efficiently optimized by utilizing expectation-maximum (EM) algorithm."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,2,Methods,An illustration of BigFUSE for dual-view LSFM fusion is given in Fig. 
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,2.1,Revisiting Dual-View LSFM Fusion Using Bayes,"BigFUSE first rethinks dual-view LSFM fusion from a Bayesian perspective, that is, the conditional probability of fused volume in terms of ""in-focusness"", is given on not only a pair of image inputs, but also prior knowledge that these two images are illiminated from opposing orientations respectively: where Y ∈ R M ×N is our predicted fusion with minimal light scattering effect. X a and X b are two image views illuminated by light source a and b, respectively. We choose Y ∈ {X a , X b } depending on their competitive image clarity at each pixel. Priors P denote our empirical favor of X a against X b at each pixel if photons travel through fewer scattering tissues from source a than b, and vice versa. Due to the non-positive light scattering effect along illumination path, there is only one focus-defocus change per column for dual-view LSFM in Fig.  which can be further reformulated by logarithm: where ω i denotes the focus-defocus changeover at i -th column, X :,i is the i -th column of X. Next, estimating ω is decomposed into: (i ) define the column-wise image clarity, i.e., log-likelihood log(p((X a :,i , X b :,i )|ω i )); (ii ) consider the belief on a spatially smooth focus-defocus boundary, namely log-prior log(p(ω))."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,2.2,Image Clarity Characterization with Image Formation Prior,"In LSFM, log-likelihood log(p((X a :,i , X b :,i )|ω i )) can be interpreted as the probability of observing (X a :,i , X b :,i ) given the hypothesis that the focus-defocus change is determined as ω i in the i -th column: where ⊕ is a concatenation, c(•) is the column-wise image clarity to be defined."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,,Estimating Pixel-Level Image Clarity in NSCT.,"To define c(•), BigFUSE first uses NSCT, a shift-invariant image representation technique, to estimate pixel-level focus measures by characterizing salient image structures  where R i,l is local directional band-limited image contrast and Si0 is the smoothed image baseline, whereas Dσ i highlights image features that are distributed only on a few directions, which is helpful to exclude noise  Reweighting Image Clarity Measures by Photon Traveling Path. Pixelindependent focus measures may be adversely sensitive to noise, due to the limited receptive field when characterizing local image clarities. Thus, BigFUSE proposes to integrate pixel-independent image clarity measures along columns by taking into consideration the photon propagation in depth. Specifically, given a pair of pixels (X a m,n , X b m,n ), X a m,n is empirically more in-focus than X a m,n , if photons travel through fewer light-scattering tissues from illumination objective a than from b to get to position (m, n), and vice versa. Therefore, BigFUSE defines column-level image clarity measures as: where A :,i is to model the image deterioration due to light scattering. To visualize photon traveling path, BigFUSE uses OTSU thresholding for foreground segmentation (followed by AlphaShape to generalize bounding polygons), and thus obtains sample boundary, i.e., incident points of light sheet, which we refer to as p u and p l for opposing views a and b respectively. Since the derivative of A :,i implicitly refers to the spatially varying index of refraction within the sample, which is nearly impossible to accurately measure from the physics perspective, we model it using a piecewise linear model, without loss of generality: As a result, log(p((X a :,i , X b :,i )|ω i )) is obtained as summed pixel-level image clarity measures with integral factors conditioned on photon propagation in depth."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,2.3,Least Squares Smoothness of Focus-Defocus Boundary,"With log-likelihood log(p((X a :,i , X b :,i )|ω i )) considering the focus-defocus consistency along illuminations using image formation prior in LSFM, BigFUSE then ensures consistency across columns in p(ω). Specifically, the smoothness of ω is characterized as a window-based polynomial fitness using linear least squares: , c i is the parameters to be estimated, the sliding window is with a size of 2s + 1."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,2.4,Focus-Defocus Boundary Inference via EM,"Finally, in order to estimate the ω together with the fitting parameter c, Big-FUSE reformulates the posterior distribution in Eq. (  where λ is the trade-off parameter. Here, BigFUSE alternates the estimations of ω, and c, and iterates until the method converges. Specifically, given c (n) for the n-th iteration, ω (n+1) i is estimated by maximizing (E-step): which can be solved by iterating over {i|1 ≤ i ≤ M }. BigFUSE then updates c (n+1) i based on least squares estimation: Additionally, A n+1 is updated based on Eq. (  where |Φ| denotes the total number of elements in Φ."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,2.5,Competitive Methods,"We compare BigFUSE to four baseline methods: (i ) DWT  (vi ) P(•): formulated by replacing the weighted summation of pixel-level clarity measures for overall characterization, by a simple average. To access the blind image fusion performance, we adopt three fusion quality metrics, Q mi "
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,3,Results and Discussion,
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,3.1,Evaluation on LSFM Images with Synthetic Blur,"We first evaluate BigFUSE in fusing dual-view LSFM blurred by simulation. Here, we blur a mouse brain sample collected in "
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,3.2,Evaluation on LSFM Images with Real Blur,"BigFUSE is then evaluated against baseline methods on real dual-view LSFM. A large sample volume, zebrafish embryo (282 × 2048 × 2048 for each view), is imaged using a Flamingo Light Sheet Microscope. BigFUSE takes roughly nine minutes to process this zebrafish embryo, using a T4 GPU with 25 GB system RAM and 15 GB GPU RAM. In Fig. "
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,4,Conclusion,"In this paper, we propose BigFUSE, a image fusion pipline with image formation prior. Specifically, image fusion in dual-view LSFM is revisited as inferring a focus-defocus boundary using Bayes, which is essential to exclude ghost artifacts. Furthermore, focus measures are determined based on not only pure image representational engineering in NSCT domain, but also the empirical effects of photon propagation in depth embeded in the opposite illumination directions in dualview LSFM. BigFUSE can be efficiently optimized using EM. Both qualitative and quantitative evaluations show that BigFUSE surpasses other state-of-the-art LSFM fusers by a large margin. BigFUSE will be made accessible."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,,Fig. 1 .,
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,,Fig. 2 .Fig. 3 .,
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,,Fig. 4 .,
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,,Fig. 5 .,
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,,Table 1 .,
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,2,Methods,
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,2.1,Backbone,"In this study, we utilize a Convmixer-like "
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,2.2,Dynamic Hierarchical Prototype Learning,"Prototypes Definition. In this study, we regard feature maps of each channel as corresponding to the response of distinct brain regions relevant to tasks, and cluster spatially-correlated subtle patterns as compact and discriminative parts from a group of channels whose peak responses appear in neighboring location following  where represents the peak response coordinate of the i-th image and Ω represents the number of images in the training set. K-means  Dynamic Hierarchical Prototype Exploring. Inter-regional spatial connectivity is fixed, but the correlation between them is dynamic with disease progression. We argue that there are structural correlations between different regions, just as the complex hierarchical functional connectome in rich-clubs  , where i represents the i-th hierarchy and N i represents the number of clusters at i-th hierarchy, corresponding to the cluster In this paper, i is set as 2. The number of prototypes in the first, second and third hierarchy is set as 16, 8 and 4, respectively. To facilitate optimal clustering of the network during training, we use two fully convolutional layers with two contrastive learning loss functions L node and L edge to approximate the clustering process. With L node , each channel clustering is enforced to become more compact inside and have significant inter-class differences with other clusterings, enabling all prototypes to be well separated: where L is the total number of layers, and N l is the number of clusters in the l-th layer. K l n , γ l n , and φ l n denote the set of all elements, the cluster center (prototype), and the estimation of concentration of the n-th cluster in the l-th layer, respectively. α is a smoothing parameter to prevent small clusters from having overly-large φ. The cluster concentration φ measures the closeness of elements in a cluster. A larger φ indicates more elements in the cluster or smaller total average distance between all elements and the cluster center. Ultimately, L node compels all elements u in K l n to be close to their cluster center γ l n and away from other cluster center at the same level. Similarly, L edge aims to embed the hierarchical correlation between clustering prototypes, which can be expressed as: P arent(γ l n ) represents the parent prototype of the prototype γ l n , and τ is a temperature hyper-parameter. L edge forces all prototypes γ l in the l-th layer to be close to their parent prototype and away from other prototypes within the same level."
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,2.3,Brain Network Graph Construction and Classification,"Through Sect. 2.2, critical brain regions are clustered in a hierarchical semantic latent space. We hereby employ the prototypes regions as nodes and correlations between them as edges to construct structural brain network graphs. We first apply a self-attention mechanism  where denote query, key, and value, respectively. d k represents the dimension of Q, K. N represents the number of critical regions, which is set as 16 in this paper. We then employ GCN to capture the topological interaction in the brain network graph and update features of nodes by performing the following operation: where Â = A+I is the adjacency matrix with inserted self-loops and I denotes an identity matrix. Dii = j=0 Âij is the diagonal degree matrix, and Θ represents learned weights. To prevent the network overfitting, we just use two GCN layers as the encoder to obtain the final graph feature F g ∈ R N ×D×H×W . To achieve the classification, we perform channel squeezing on the backbone feature F b to obtain global features F se ∈ R 1×D×H×W , concatenate it with F g and input them into the classification layer. To this end, the information of critical brain regions are fully learned. Notably, as prototypes are dynamic, constructed brain network graphs are also dynamic, rather than predefined and fixed. This allows DH-ProGCN to model and explore the individual hierarchical information, providing a more personalise brain network representation for every subject."
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,3,Experiments,
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,3.1,Dataset,The data we used are from two public databases: ADNI-1 (http://www.adniinfo.org) and ADNI-2 
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,3.2,Implementation Details,"We first train backbone with 2048 channels in all layers to extract the output features F b with the cross-entropy loss L cls1 . Then the clustering results are initialized by K-means and further optimized by CCL with L node and L edge . Finally, The cross-entropy loss L cls2 is used for the final classification. The overall loss function is defined as: where L node and L edge are explained in Sect. 2.2. Smooth parameter α = 10 and temperature parameter τ = 0.2 following "
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,4,Results,
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,4.1,Comparing with SOTA Methods,Six SOTA methods are used for comparison: 1) LDMIL 
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,,Method,Results in Table 
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,4.2,Ablation Study,"Effect of Dynamic Prototype Learning. To verify the effect of dynamic prototype clustering, we compare 1) ROI-based approach  Effect of Hierarchical Prototype Learning. To evaluate the impact of hierarchical prototype learning, we compare backbone with flattened prototypes clustering (BL+L node ), and hierarchical clustering (BL+L node +L edge ). The results are presented in Fig.  Effect of Dynamic Brain Network Construction. To verify whether our constructed dynamic brain network capability outperforms the fixed architecture, we obtained the fixed brain network graph by directly connecting all critical regions after obtaining hierarchical features and feeding them into the GCN for classification. The results are shown in Fig.  In addition, we visualize the sagittal, coronal and axial views of hierarchical critical regions and their connectome in Fig. "
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,5,Conclusion,"In this paper, we propose a novel dynamic structural brain network construction method named DH-ProGCN. DH-ProGCN could dynamically cluster critical brain regions by the prototype learning, implicitly encode the hierarchical semantic structure of the brain into the latent space by hierarchical prototypes embedding, dynamically construct brain networks by self-attention and extract topology features in the brain network by GCN. Experimental results show that DH-ProGCN outperforms SOTA methods on the MCI conversion task. Essentially, DH-ProGCN has the potential to model hierarchical topological structures in other kinds of medical images. In our future work, we will apply this framework to other kinds of modalities and neurological disorders."
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,,Fig. 1 .,
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,,Fig. 2 .,
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,,Fig. 3 .,
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_12.
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,1,Introduction,"Glioblastomas (GBMs, known as grade IV gliomas) are the most common primary malignant brain tumors with high spatial heterogeneity and varying degrees of aggressiveness  To do so, magnetic resonance imaging (MRI) and its derived radiomics have been widely used to study GBM preoperative prognosis over the last few decades. For example, Anand et al.  Although both MRI and its derived radiomics features have been demonstrated to have predictive power for survival analysis in the aforementioned literature, they do not account for brain's functional alternations caused by tumors, which are clinically significant as biologically-interpretable biomarkers of recovery and therapy. These alternations can be reflected by changes in resting-state functional MRI (fMRI)-derived functional connectivities/connections (FCs) between the blood oxygenation level-dependence (BOLD) time series of paired brain regions. Therefore, the use of FCs to predict overall survival time for GBM has recently attracted increasing attention  Nevertheless, current FC-based survival prediction still suffers from two main deficiencies when applied to GBM prognosis. First, due to mass effect and physical infiltration of GBM in the brain, FCs estimated directly from GBM patients' resting-state fMRI might be inaccurate, especially when the tumors are near or in the regions of interest. Second, resting-state fMRI data are not routinely collected for GBM clinical practices, which restricts the size of annotated datasets such that it is infeasible to train a reliable prediction model based on deep learning for survival prediction. In order to circumvent these issues, in this paper we introduce a novel neuroimaging feature family, namely functional lesion network (FLN) maps that are generated by our augmented lesion network mapping (A-LNM), for overall survival time prediction of GBM patients. Our A-LNM is motivated by lesion network mapping (LNM)  The details of our workflow are described as follows. 1) We first manually segment the whole tumor (regarded as lesion in this paper) on structural MRI for all GBM patients, and the resulting lesion masks are mapped onto a reference brain template, e.g., the MNI152 2mm 3 template. 2) The proposed A-LNM is next used to generate FLN maps for each GBM patient by using resting-state fMRI from a large cohort of healthy subjects. Specifically, for each patient, we correlate the mean BOLD time series of all voxels within the lesion with the BOLD time series of every voxel in the whole brain for all N subjects in the normative cohort, producing N functional disconnection (FDC) maps of voxel-wise correlation values (transformed to zscores). These resulting N FDC maps are partitioned into M disjoint subsets of equal size, and M FLN maps are separately obtained by averaging the FDC maps in each of the M subsets. Similar to data augmentation schemes, we can artificially boost data volume (i.e., FLN maps) up to M times through producing M FLN maps for each patient in the A-LNM, which helps to mitigate the risk of over-fitting and improve the performance of overall survival time prediction when learning a deep neural network from a small sized dataset. For this reason, we propose the name ""augmented LNM (A-LNM)"", compared to the traditional LNM where only one FLN map is generated per patient by averaging all the N FDC maps. 3) Finally, these augmented FLN maps are fed to a 3D ResNet-based backbone network followed by the average pooling operation and fully-connected layers for GBM survival prediction. To our knowledge, this paper is the first to demonstrate a successful extension of LNM for survival prediction in GBM. To evaluate the predictive power of the FLN maps generated by our A-LNM, we conduct extensive experiments on 235 GBM patients in the training dataset of BraTS 2020 "
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2,Materials and Methods,"2.1 Materials GSP1000 Processed Connectome. It publicly released preprocessed restingstate fMRI data of 1000 healthy right-handed subjects with an average age 21.5 ± 2.9 years and approximately equal numbers of males and females from the Brain Genomics Superstruct Project (GSP)  BraTS 2020. It provided an open-access pre-operative imaging training dataset to segment brain tumors of glioblastoma (GBM, belonging to high grade glioma) and low grade glioma (LGG) patients, as well as to predict overall survival time of GBM patients  The union of all the three tumor sub-regions was considered as the whole tumor, which is regarded as the lesion in this paper."
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2.2,Methods,"In this paper, we propose to investigate the feasibility of the novel neuroimaging features, i.e., FLN maps, for overall survival time prediction of GBM patients in the training dataset of the BraTS 2020, in which one patient alive was excluded, and the remaining 235 patients consisted of 89 short-term survivors (less than 10 months), 59 mid-term survivors (between 10 and 15 months), and 87 long-term survivors (more than 15 months). To this end, our framework for the three-class survival classification is shown in Fig.  Lesion Mapping Procedures. As stated above, the whole tumor is referred to as a lesion for each GBM patient. From the manual expert segmentation labels of lesions in the 235 GBM patients of the BraTS 2020, we co-register the lesion masks to the MNI152 2mm 3 template by employing a Symmetric Normalization algorithm in ANTsPy "
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,,Augmented Lesion Network Mapping (A-LNM).,"After lesion mapping, we introduce a modified LNM (called augmented LNM (A-LNM) in this paper) to generate FLN maps for each GBM patient by using resting-state fMRI of all 1000 GSP healthy subjects, as described below. i) For each patient, the lesion is viewed as a seed region to calculate FDC in the healthy subjects with restingstate fMRI. Specifically, to compute FDC, the mean BOLD time series of voxels within each lesion is correlated with the BOLD time series of every voxel in the whole brain for all the 1000 healthy subjects, yielding 1000 FDC maps of voxelwise correlation values (transformed to z-scores), where an FDC map is actually a three-dimensional voxel-wise matrix of size 91 × 109 × 91 (spatial resolution: 2mm 3 voxel size). ii) Different from the commonly used LNM where the resulting 1000 FDC maps are thresholded or averaged to obtain a single FLN map for each patient, the A-LNM generates many FLN maps for each patient in a manner that partitions all the 1000 FDC maps into disjoint subsets of equal size and averages each subset to produce one FLN map. One can clearly see that similar to data augmentation schemes, we artificially boost the number of training samples (i.e., FLN maps) by our A-LNM, which helps to mitigate the risk of over-fitting and improve the performance of overall survival time prediction when learning a deep neural network from such a small sized training set used in this paper. Note that in Sect. 3 of this paper, according to experimental results, we divided the 1000 FDC maps into 100 subsets, and randomly chose 10 out of the resulting 100 FLN maps for each patient as input to the downstream prediction model. Deep Neural Network for Overall Survival Time Prediction. By taking the obtained FLN maps as input, we apply a 3D ResNet-based backbone network transferred from the encoder of MedicalNet "
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3,Experiments and Results,
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3.1,Experimental Settings,"Implementation Details. Our proposed method was implemented in PyTorch 1.13.1 on NVIDIA A100 Tensor Core GPUs. The loss function was the standard cross-entropy loss. The Adam optimizer with the weight decay of 10 -5 was adopted. Three 3D ResNet-based backbones with different numbers of layers (10, 50, and 101) were performed, where the initial learning rates were set as 10 -4 , 10 -4 , and 10 -5 , respectively, and would decrease by a factor of 5 if the classification performance is not improved within 5 epochs. The number of epochs for training was 50, and the batch size was fixed as 64. Performance Evaluation. We evaluated the classification performance of our proposed method using 235 GBM patients in the BraTS 2020 training dataset, because only these 235 patients had both overall survival time and manual expert segmentation labels of lesions. In all experiments, we conducted five-fold crossvalidation ten times in order to reduce the effect of sampling bias. Moreover, the A-LNM was performed ten times randomly to avoid particular data distribution and obtain more reliable results. The classification results were reported in terms of accuracy, macro precision (macro-P), macro recall (macro-R), and macro F1 score (macro-F1), respectively."
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3.2,Comparison Studies,"Quantitative Comparison of Different Prediction Models. As this paper is the first application of FLN maps in the overall survival time prediction for GBM, comparison among the classification performance of different models using the same type of features, i.e., the A-LNM or the LNM derived FLN maps, is demanded for model selection. To validate the effectiveness of the 3D ResNetbased backbones for GBM survival prediction, we made quantitative comparison of a ridge classifier (RC) with PCA "
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3.3,Brain Regions in Relation to GBM Survival,"To identify the most discriminative brain regions associated with overall survival time in GBM, we estimated the relative contribution of each voxel to the classification performance in our proposed method by using the Grad-CAM "
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,4,Conclusion,"In this paper, we introduce a novel neuroimaging feature family, called A-LNM derived FLN maps, for overall survival time prediction of GBM patients. A-LNM was presented to generate plenty of FLN maps for each GBM patient by partitioning the FDC maps obtained from resting-state fMRI of 1000 GSP healthy subjects into disjoint subsets of equal size and averaging each subset. We applied a 3D ResNet-based backbone network to extract features from the generated FLN maps and classify GBM patients into three overall survival time groups. Experimental results on the BraTS 2020 training dataset validated the effectiveness of the A-LNM derived FLN maps for GBM survival prediction. Moreover, a visualization analysis implemented by the Grad-CAM revealed the brain regions associated with GBM survival. In future work, we will try to fuse the FLN maps and MRI-based radiomics features to study their combined predictive power for GBM survival analysis."
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,,Fig. 1 .,
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,,Fig. 2 .,
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,,Table 1 .,
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,,Table 2 .,
Dynamic Functional Connectome Harmonics,1,Introduction,"Nonlinear dimensionality reduction techniques such as diffusion embedding and principal component analysis applied to FC data provide the primary axes along which FC is organized  A growing body of evidence suggests that the strongest components of FC are underpinned by brief but strong patterns of activation, or brain states, resembling the dominant components of FC (e.g., the default mode network) rather than sustained, low-magnitude signal coherence between brain regions implicated in those strongest components of FC  The robustness of connectivity patterns across populations and the reliability and reproducibility of FC gradients across subjects and choices of parameters have been well studied  Previous work has shown that network configuration is dynamic during task and rest, with the community allegiance of brain regions changing over time "
Dynamic Functional Connectome Harmonics,2,Methods,"We used the resting-state fMRI time series of 44 subjects from the HCP minimal preprocessed test-retest dataset mapped to the 32k MSM-All surface atlas  In light of studies demonstrating that thresholding FC matrices increases the reliability of FC gradients  For each FC matrix Â(t) at timepoint t, the normalized graph Laplacian matrix, , with D(t) i,i = Nv j=0 Â(t) i,j was computed. The first N h eigenvectors of L(t), solutions to L(t)ψ(t) k = λ(t) k ψ(t) k , were computed, yielding the functional connectome harmonics of window t, ,924 is the number of vertices, and N h = 7 is the number of harmonics computed. This process is carried out for all windows for each acquisition, yielding the dynamic functional connectome harmonics for a particular subject and window length: To extract a single set of harmonics for each acquisition at each window length, the window-wise harmonics from all windows for that acquisition were stacked horizontally into a large matrix of size N v × (N h N w ) on which PCA was performed, yielding one set of principal components for each acquisition and each subject. We refer to the components obtained via this approach as the most-prevalent harmonics, ψ, as they encapsulate orthogonal patterns of maximum variation across all window-wise harmonics. This process was also repeated on the group level for the test and retest cohorts separately at each window length, yielding the most prevalent harmonics Ψ for the test and retest groups. For computation of Ψ , the matrix on which PCA was performed is of size N v × (N h N w N a N s ), where N a = 2 is the number of acquisitions per subject for either test or retest, and N s = 44 is the number of subjects. To investigate the reproducibility of dynamic functional connectome harmonics as a basis for reconstructing the underlying fMRI signal, we utilize the harmonics as operators on functional timeseries. For a vertex-wise functional time series X = [x(1), . . . , x(N T )], where x(t) is the fMRI activation at timepoint t, we define its N h × N T spectral coefficient timeseries matrix α with respect to harmonics ψ as where each row α k of α is the spectral coefficient timeseries of timeseries X for harmonic ψ k . Each α k of α gives the activation power of harmonic k at each time point, and the L2 norm of α k yields the activation energy E(ψ k , X) = ||α k || of harmonic k across the entire timeseries. Further, we define the harmonic-filtered timeseries X and the representation efficiency ratio γ as A set of harmonics {ψ k |k = 1, . . . , N h } defines a spectral coordinate vector for each vertex i on the surface, specifying a position in the harmonic embedding space. To allow for comparison of spectral coordinates across a series of consecutive time windows, we first compute the most-prevalent harmonics ψ of the time series, and perform Procrustes alignment  . ( In order to evaluate our method of computing flexibility, we also employ a standard community detection based technique for comparison with our method wherein a graph modularity metric is optimized to obtain a community partition using a multi-layer temporal graph "
Dynamic Functional Connectome Harmonics,3,Results,"Our most-prevalent harmonics comprise meaningful patterns of activation on the cortex (Fig.  We evaluated the stability of the group level most-prevalent harmonics across repeated measurements (test and retest group cohorts) and across varying window length using cosine similarity and found high similarity (≥0.70) between all pairings, indicating high stability (Fig.  We investigated how the harmonics relate to functional timeseries by examining the stability of the harmonic activation energy of the first two harmonics, ||α 1 || and ||α 2 ||, the representation efficiency ratio γ, and the correlation between α 1 and α 2 . For  0.5 0.81 ± 0.11 0.79 ± 0.10 0.72 ± 0.13 0.22 ± 0.10 0.17 ± 0.08 0.12 ± 0.06 1 0.83 ± 0.09 0.80 ± 0.09 0.74 ± 0.11 0.41 ± 0.12 0.33 ± 0.11 0.25 ± 0.09 2 0.84 ± 0.09 0.82 ± 0.09 0.75 ± 0.11 0.58 ± 0.11 0.50 ± 0.12 0.40 ± 0.12 3.5 0.86 ± 0.07 0.82 ± 0.09 0.76 ± 0.11 0.70 ± 0.09 0.61 ± 0.11 0.52 ± 0.12 each session and each subject, we computed each quantity using the most-prevalent harmonics ψ of that subject and session and investigated the reliability of these harmonicderived metrics using the intraclass correlation coefficient (ICC), with results summarized in Table "
Dynamic Functional Connectome Harmonics,4,Discussion,"Individual most-prevalent harmonics are found to be similar across acquisitions after Procrustes alignment, while there is notably lower average similarity between dynamic harmonics, particularly at shorter timescales (Table  Our most-prevalent functional connectome harmonics show invariance under change in timescale and strongly resemble FC ""gradients"" in the literature computed on static FC  Our analysis indicates that our individual most-prevalent harmonics ψ are a reproducible basis with which to decompose the underlying fMRI signal. The projection coefficient timeseries of a given harmonic, α k , represents the timecourse of relative activation of ψk . Importantly, we found that the magnitude of these activation timecourses for the first and second most-prevalent harmonic (corresponding to the DMN and TPN), as well as the correlation between them were reliable across repeated measurements in individual subjects, with significantly higher variability across subjects than across scans (Table  Our dynamic functional connectome harmonic definition of flexibility yields a vertex-wise map of cortical flexibility for each subject, requires fewer parameters, and is less computationally expensive than standard definitions of flexibility. Our group-average flexibility result (Fig. "
Dynamic Functional Connectome Harmonics,5,Conclusion,"Our most-prevalent functional connectome harmonics and their dynamic counterparts provide a powerful lens through which to study the organization of FC as well as its dynamics. We demonstrate that the most-prevalent harmonics are invariant when the timescale at which their underlying connectivity matrices are computed is changed, and that they are highly stable across repeated measurements on the group and individual level. Further, we show that our harmonics provide a reliable basis with which to filter fMRI timeseries and to extract the activation timecourses of individual harmonic brain states. Importantly, the reproducibility of these harmonic decompositions indicates that they provide a subject-specific method with which to study the dynamics of specific brain states and their interrelationships. We also present a novel formulation of cortical flexibility defined in terms of dynamic functional connectome harmonics and show that it gives vertex resolution flexibility maps that qualitatively similar results to previous definitions of flexibility which are constrained by free parameters and computational complexity."
Dynamic Functional Connectome Harmonics,,Fig. 1 .,
Dynamic Functional Connectome Harmonics,,Fig. 2 .,
Dynamic Functional Connectome Harmonics,,Table 1 .,
Dynamic Functional Connectome Harmonics,,Table 2 .,
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,1,Introduction,"Recent studies have shown that rs-fMRI based analysis for brain functional connectivity (FC) is effective in helping understand the pathology of brain diseases  The representation learning of brain network heavily relies on the graph structure quality. The existing brain network construction methods  Considering the above issues, we aim to discover useful graph structures via a learnable graph structure from the BOLD signals instead of measuring the associations between brain regions by a similarity estimation. In this paper, we propose an end-to-end unsupervised graph structure learning framework for functional brain network analysis (BrainUSL) directly from the BOLD signals. The unsupervised graph structure learning consists of a graph generation module and the topology-aware encoder. We propose three loss functions to constrain the graph structure learning, including the sparsity-inducing norm, the view consistency regularization and the correlation-guided contrastive loss. Finally, the generated graph structures are used for the graph classification. We evaluate our model on two real medical clinical applications: Bipolar Disorder diagnosis and Major Depressive Disorder diagnosis. The results demonstrate that our BrainUSL achieves remarkable improvements and outperforms state-of-the-art methods. The main contributions of this paper are summarized below: -We propose an end-to-end unsupervised graph structure learning method for functional brain network analysis. -We propose the correlation-guided contrastive loss to model the correlations between graphs by defining the sample correlation estimation matrix. -Our method provides a perspective for disease interpretable analysis and association analysis between BD and MDD. -The experimental results demonstrate the advantage of the proposed method in brain disorder diagnosis."
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,2,Method,"The constructed graph structure of brain network in existing works are often noisy or incomplete. To address this issue, we propose a novel unsupervised graph structure learning method, including the graph generation module for generating optimized sparsity-induced graphs and the topology-aware encoder for capturing the topological information in graphs, as illustrated in Fig. "
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,2.1,Graph Generation Module,"To exploit the information in fMRI signals for generating the optimized sparsityinduced graph structure, we propose a graph generation module, which containing a graph generation module contains BOLD signal feature aggregation (E f ) with a stack of convolutional layers  where K (l) is a convolutional kernel of l-th layer with a kernel size of U and u denotes the BOLD signal element in a brain region of the input x. With the feature learned by E f , we generate the optimized graph A G by calculating the correlation among the nodes."
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,2.2,Topology-Aware Encoder,"The graph topological information is crucial for graph embedding learning. Motivated by BrainNetCNN  where w r ∈ R 1×M and w h ∈ R M ×1 denote the learned vectors of the horizontal and vertical convolution kernel, M denotes the number of ROIs, A and H g denote the adjacency matrix and the edge embeddings. With the learned edge embeddings, we further learn the node embeddings by aggregating the associated edges with the nodes with a learnable layer. More specifically, the node aggregation takes the edge embedding as the inputs and obtains the node embedding from a node-wise view by a 1D convolutional filter. The node aggregation is defined as: where H n ∈ R M ×d is the node embedding, and w n ∈ R 1×M is the learned vector of the filter, and d is the dimensionality of the node embeddings."
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,2.3,Objective Functions,"To better exploit the graph structure, we design three loss functions including a sparsity-inducing norm, a view consistency regularization and a correlationguided contrastive loss. We assume that the sparsity of the generated graphs allows for the preservation of the important edges and removal of noise. To achieve this, we utilize an l 1 norm to remove the irrelevant connections and preserve the sparsity of the generated graphs. Furthermore, we introduce a view consistency regularization to ensure the consistency of two views by maximizing the agreement between the node embeddings learned from the fixed graph structure A P and learnable graph structure A G . The view consistency regularization is defined as L vc = N i=1 sim(e i , êi ), where sim(•, •) is a cosine similarity measure, N denotes the number of samples, êi and e i represent the i-th graph embeddings from A P and A G . The motivation of contrastive learning is to capture the graph embeddings by modeling the correlations between graphs  where 1(•) = {0, 1} is an indicator function, and τ is a temperature factor to control the desired attractiveness strength. The final objective function is formulated as: where α and β are the trade-off hyper-parameters. Finally, based on the generated graphs and pre-trained topology-aware encoder, we leverage the multi-layer perceptron (MLP) with the cross-entropy loss for the graph classification. 3 Experiments and Results"
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,3.1,Dataset and Experimental Details,We evaluated our BrainUSL on a private dataset constructed from Nanjing Medical University (NMU) for BD and MDD diagnosis by repeating the 5-fold crossvalidation 5 times with different random seeds. We deal with the original fMRI data by dpabi 
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,3.2,Classification Results,"We compare our BrainUSL with state-of-the-art models in terms of Accuracy (ACC), Area Under the Curve (AUC), Sensitive (SEN) and Specificity (SPEC). The comparable methods can be grouped into two categories: traditional methods including FC+SVM/RF  Comparison with SOTA. Compared with state-of-the-art methods, the proposed BrainUSL generally achieves the best performance on MDD and BD identification, the results are shown in Table  The results demonstrate that our generated graphs are more discriminative than the graphs constructed by pearson correlation coefficient, which confirms that the quality of the graph structure is critical for functional brain network representation learning, and noisy or redundant connections in brain network impede understanding of disease mechanisms. Ablation Study. There are three parts in our final objective function. Next, we perform a sequence of ablation studies on the three parts of our model. As  shown the third part in Table "
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,3.3,Functional Connectivity Analysis,We use BrainNetViewer 
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,3.4,Association of Brain Diseases,A number of studies 
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,4,Conclusion,"Due to the inevitably error-prone data measurement or collection, the functional brain networks constructed by existing works are often noisy and incomplete. To address this issue, we propose the unsupervised graph structure learning framework for functional brain network analysis (BrainUSL), which generates the sparse and discriminative graph structures according to the characteristics of the graph data itself. We conducted extensive experiments on the NMU dataset which indicate that our BrainUSL achieves promising performance with the SOTA methods. In addition, we discuss the interpretability of our model and find discriminative correlations in functional brain networks for diagnosis."
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,,Fig. 1 .,
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,,Fig. 2 .,
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,,Fig. 3 .,
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,,Fig. 4 .,
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,,Table 1 .,
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_20.
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,1,Introduction,"COVID-19 is an infectious disease caused by Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2); that is a novel virus from the genus of Betacoronovirus of coronavirus genera. In extreme cases, host cells activate intensive immune responses for defense, leading to Acute respiratory distress syndrome (ARDS) and multiple organ failure  Designing a new drug is time-consuming, while drug repurposing can be a real game-changer when it comes to a pandemic. Every newly designed drug candidate should go through a filter of preclinical research, clinical trials, and FDA reviews to gain safety permission to be on the market. Many drugs filter out before reaching the market, while this journey takes about 7-12 years for those few that last  High Throughput Screening (HTS) has proven its worth in facilitating drug repurposing  RxRx19a is an extensive HTS experiment conducted by the Recursion biotechnology company to investigate potential therapeutics of approved drugs for COVID-19 treatment  Previous studies for hit discovery on this dataset used positive and negative control samples to train a model to discriminate healthy from infected cell image representations  Here, we present a novel weakly supervised confident hit prediction pipeline that estimates disease scores regardless of drug side effects and provides a confi-dence score for its prediction. We applied our model on both CellProfiler features  -We are the first to address the hit-discovery methodology's out-of-distribution problem and solve it with a novel weakly-supervised confident hit prediction pipeline. -We exhaustively evaluate our pipeline and show that our results are robust, stable, and sensitive to the cell death and drug toxicity. -We provide the CellProfiler features, deep learning embeddings, and their related pipelines for nowadays the biggest fluorescent microscopy dataset to explore drug efficiency."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,2,Dataset,"Our work uses RxRx19a dataset, which consists of 305, 520 site-level 1024×1024 pixels images captured by fluorescent microscopes in multiple high throughput screening experiments. These experiments were conducted on two tissue types, HRCE and VERO, to explore the efficiency of 1672 FDA or EMA-approved drugs with 6 to 8 different doses against SARS-CoV-2. A drug with a specific dosage was added post-seeding for preparing treatment samples, and then samples were contaminated with the active SARS-CoV-2 virus. Then, samples were fixed and stained with 5 fluorescent colors to indicate DNA, RNA, actin cytoskeleton, Golgi apparatus, and endoplasmic reticulum organelles in cells. RxRx19a is publicly available and licensed under the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/)."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,3,Methods,"Assume X is a set of all single-cell images in our dataset. We partitioned X based on information of wells into three classes X ctrl+ , X ctrl-, and X t . These are sets of all single-cell images from positive control wells (active SARS-CoV-2), negative control wells (mock and UV-irradiated SARS-CoV-2), and contaminated drugged wells (treatment), respectively. We also define X ctrl = X ctrl+ ∪ X ctrl-as a set of all control samples. The function f (x i ), f : X → H, maps each image x i to its embedding h i . Same as partitions in X space, we consider H ctrl+ , H ctrl-, and H t as: A function d : H → [0, 1] returns a disease score for a given embedding, which is a score number indicating adversity of the disease. The lower and closer to 0, the healthier the cell is, and vice versa. For the sake of simplicity, we ignore experimental errors and consider all positive controls cells infected ∀h + ∈ H ctrl+ , d(h + ) = 1, and with the same logic, all negative cells as healthy ∀h -∈ H ctrl-, d(h -) = 0. Yet, the disease score for ∀h t ∈ H t is unknown. If a drug was effective, somehow, it could hinder the virus's activities in the cell after being contaminated. It could be through a direct effect on the virus, entry channels, or cells that make them more resistant to the virus. As a result, with a successful drug, the host cell's morphological features were close to the morphological characteristics of a healthy cell. That is, d(h t ) is close to 0 for a successful drug. In this study, we aim to estimate d (Fig. "
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,3.1,Image Preprocessing and Embedding,We used a retrospective multi-image method to correct field-of-view illumination and calculate illumination functions for each channel across plates 
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,3.2,Data Augmentation with Weak Labels,"We propose a novel data augmentation based on  Our data transformation can be formulated as follows: hi,j,α = αh where α ∼ U(0, 1) is a random number between [0, 1], and h + i ∼ H ctrl+ and h - j ∼ H ctrl-are arbitrary samples from positive and negative controls. A normally distributed noise n ∼ N (μ H ctrl , σ H ctrl ) is calculated for each dimension independently based on the mean and standard deviation of control profiles. This noise is orthogonal to v = Hctrl+ -Hctrland is added to the representation to simulate drug-related morphological changes. v is a vector between positive and negative samples' embedding averages, and the final noise n is in a hyperplane perpendicular to v. And γ is a hyperparameter. Random noise n is not expected to affect the transformed data label because it is orthogonal to the positive and negative samples axis. We consider the weak label for hi,j,α only depended on α, l(h + i ) and l(h - j ): It is remarkable that with this method, we can augment our dataset indefinitely, and overcome the poor generalization issues that are caused by the limited control data. Further, this augmented dataset H which includes control samples H ctrl is used to train and evaluate our confident hit predictor."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,3.3,Confident Hit Predictor,"Machine learning models' predictions about samples from out of their distribution cannot be trusted  We offer a first-in-field confident hit predictor that utilizes the idea of hint and confidence  During training, our model receives hints to predict disease scores. Larger hints lead to lower confidence scores. Using this method, we adjust the prediction probabilities using interpolation between the predicted disease score and the weak label probability distribution. The adjusted prediction is then subjected to MSE loss calculation as follows: where h is an augmented transformed hidden features and l( hi ) is the weak label, and d( hi ) is predicted diseas score. A function c(.) ∈ [0, 1] calculates the confidence score. The adjusted prediction c( h)d( h) + (1c( h))l( h) is closer to the hint score (l( hi )) when the confidence score is low, and vice versa. To prevent the model from always asking for hints and getting stuck in a trivial solution where c always returns 0, a confidence loss,log(c( h)) is added to the loss."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,4,Experiments and Results,
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,4.1,Experimental Settings,"We developed our CellProfiler pipeline with Luigi workflow manager, which was, in our experience, faster and more maintainable in locally hosted systems. For TVN, we use PCA whitening that embeds our representation into 1024 dimensions. Multilayer perceptrons (MLP) are used to estimate the confidence and disease scores. To train our model, we used the zero-shot learning method to test our model's reliability. For this purpose, we left out inactive UV-irradiated SARS-CoV-2 samples in the training process. The model should estimate inactive virus samples as healthy, while inactive viruses can still slightly change the cellular morphology."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,4.2,Representation Quality Assurance,Replicate Reproducibility Test: It is expected to see replicates of the same biological perturbation are significantly similar to each other than a random set of profiles 
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,4.3,Disease Scores Quality Assurance,
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,,Stability of Disease Scores:,"To test the model's stability, we randomly omitted a portion of the control samples before creating a new augmented dataset and training a new model. Then, we compared the correlation between prime and new scores and repeated the test five times. Our model showed enhanced stability due to weak label data augmentation, as opposed to the unstable on-disease score algorithm used in RxRx19a (see Fig. "
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,,Robustness of Disease Scores:,We show the robustness of our method for computing disease scores by accurately predicting scores for new samples in a zero-shot learning setup 
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,4.4,Confidence Scores Quality Assurance,Calibration of Confidence Scores: We have noticed that the entropy of disease score predictions rises as the samples are further away from being entirely healthy or diseased. This reveals that the model demonstrates measurable uncertainty on OOD samples (see Fig. 
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,4.5,Evaluation,"We hypothesize that using confidence will reduce false positives in the hit discovery. Because ground truth labels for drugs in our dataset are not accessible, we cannot directly show false positive rate reduction. Instead, we can show that when we applied the confidence method consensus among top scores, the overlap between discovered hits based on different representations increased. We run our pipeline in both with and without confidence setups for three representations: our CellProfiler features, our single-cell image embedding, and the original proprietary RxRx19a embedding. When we used the confidence score, the mean Jaccard similarity between the top 10 drugs increased from 0.13 to 0.2 (see Table "
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,5,Conclusion,"To safely conclude about hit discovery scores based on cellular morphological features, we need to be concerned about inevitable out-of-distribution phenotypes. We explored one possible solution and proposed a confidence-based weaklysupervised drug efficiency estimation pipeline that was trained to be unsure about out-of-distribution samples. Further, because truth values for drug efficiency scores are unknown, we indirectly defined a metric to calculate false positive rate reduction and showed that this metric improves in the confidence-based setup. We also enhanced our drug efficiency estimation pipeline with a unique weakly-supervised data transformation, simulating contaminated drugged cell phenotypes. Finally, we assessed our pipeline's robustness and stability."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,,Fig. 1 .,
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,,Fig. 2 .,
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,,Table 1 .,
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 65.
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,1,Introduction,"The human brain has evolved to support a set of complementary and temporally varying brain network organizations enabling parallel and higher-order information processing  Conventional mode/component decomposition methods such as Principal Component Analysis (PCA) or Independent Component Analysis (ICA) assume the modes to be static  Under the hood, GraphDMD regards the network sequence as a linear dynamical system (LDS) where a linear operator shifts the current network state one time-point in the future. The LDS assumption, however, is not optimal for modeling functional brain networks that exhibit complex non-linearity such as rapid synchronization and desynchronization as well as transient events  Here, we propose a novel Deep Graph Dynamic Mode Decomposition (Deep-GraphDMD) algorithm that applies to arbitrary non-linear network dynamics while maintaining interpretability in the latent space. Our method uses Koopman operator theory to lift a non-linear dynamical system into a linear space through a set of Koopman eigenfunctions (Fig. "
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,2,Methodology,"Let's assume X ∈ R n×t is a matrix containing the BOLD (blood-oxygen-leveldependent) signal of n brain regions (ROIs) in its rows at t time frames sampled at every kΔt time points, where Δt is the temporal resolution. To compute the dynamic connectivity matrix at time point kΔt, a snapshot X k = X :,k:k+s is taken in a sliding window of s time frames. A correlation matrix G k ∈ R n×n is then computed from X k by taking the pearson correlation between the rows of X k , i.e., G ij k = pearson(x i k , x j k ) where x i k , x j k are the i th and j th row of X k respectively. This yields a sequence of graphs ) is a matrix containing g k in its columns. The goal is to decouple the overlapping spatiotemporal modes from the network sequence G using -1. Graph Dynamic Mode Decomposition algorithm, and 2. a novel Deep Learning-based Graph Dynamic Mode Decomposition algorithm."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,2.1,Graph Dynamic Mode Decomposition,"GraphDMD  where A ∈ R n 2 ×n 2 is a linear operator that shifts the current state g k to the state at the next time frame g k+1 . To extract the low dimensional global network dynamics, GraphDMD projects A into a lower dimensional space Â using tensortrain decomposition, applies eigendecomposition of Â, and projects the eigenvectors back to the original space which we refer to as dynamic modes (DMs). GraphDMD uses tensor-train decomposition to maintain the network structure of g k and thus, the DMs from GraphDMD can be reshaped into n × n adjacency matrix forms. Let's assume these DMs are Φ 1 , Φ 2 , • • • , Φ r where Φ p ∈ C n×n and the corresponding eigenvalues are λ 1 , λ 2 , • • • , λ r where λ p ∈ C (Fig.  where λ p = a p exp(ω p Δt), Φ † is the conjugate transpose of Φ, b p = vec(Φ † p )g 1 is the projection of the initial value onto the DMD modes, a p = ||λ p || is the growth/decay rate and ω p = Im(ln λ p )/Δt is the angular frequency of Φ p ."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,2.2,Adaptation of Graph-DMD for Nonlinear Graph Dynamics,"Since the dynamics of the functional networks are often non-linear, the linearity assumption of Eq. 1 is sub-optimal. In this regard, we resort to Koopman operator theory to transform the non-linear system into an LDS using a set of Koopman eigenfunctions ψ, i.e., ψ(g k+1 ) = Aψ(g k )  two matrices with columns stacked with ψ(g k ) and Y † is the right inverse of Y . After training, we reshape ψ(g k ) into a n × n network ψ(G k ) and generate the latent network sequence ψ(G 1 ), • • • , ψ(G t-s+1 ). We then apply GraphDMD (described in Sect. 2.1) on this latent and linearized network sequence to extract the DMs Φ p and their corresponding λ p . However, there are two unique challenges of learning network embeddings using the DeepGraphDMD model: 1. the edge identity and, thus, the interpretability will be lost in the latent space if we directly embed g k using ψ, and 2. Y † doesn't exist, and thus L lkis can't be computed because Y is low rank with the number of rows n(n-1)"
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,,2,">> the number of columns t -s + 1. To solve the first problem, instead of learning ψ(g k ) directly, we embed the BOLD signal x i k of each ROI independently using the encoder to learn the latent embeddings z i k (Fig.  The second problem arises because the Koopman operator A regresses the value of an edge at the next time-point as a linear combination of all the other edges at the current time-point, i.e., g ij k+1 = N p,q=1 w pq g pq k . This results in O(n Other than L lkis , we also train the autoencoder with a reconstruction loss L recon which is the mean-squared error (MSE) between x i k and the reconstructed output from the decoder xi k . Moreover, a regularizer L reg in the form of an MSE loss between g k and the latent ψ(g k ) is also added. The final loss is the following: where α and β are hyper-parameters. We choose α, β, and other hyperparameters using grid search on the validation set. The network architecture and the values of the hyper-parameters of DeepGraphDMD training are shown in Supplementary Fig. "
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,2.3,Window-Based GraphDMD,"We apply GraphDMD in a short window of size 64 time frames with a step size of 4 time frames instead of the whole sequence G because, in real-world fMRI data, both the frequency and the structure of the DMs can change over time. We then combine the DMs across different sliding windows using the following post-processing steps: Post-processing of the DMs: We first group the DMs within the frequency bins: 0-0.01 Hz, 0.01-0.04 Hz, 0.04-0.08 Hz, 0.08-0.12 Hz, and 0.12-0.16 Hz. We then cluster the DMs within each frequency bin using a clustering algorithm and select the cluster centroids as the representative DMs (except for the first bin where we average the DMs). We chose the optimal clustering algorithm to be Spherical KMeans "
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,3,Experiments,
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,3.1,Dataset,We use rs-fMRI for 840 subjects from the HCP Dense Connectome dataset 2  Each fMRI image was acquired with a temporal resolution (Δt) of 0.72 s and a 2 mm isotropic spatial resolution using a 3T Siemens Skyra scanner. Individual subjects underwent four rs-fMRI runs of 14.4 min each (1200 frames per run). Group-ICA using FSL's MELODIC tool 
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,3.2,Baseline Methods,"We compare GraphDMD and DeepGraphDMD against three decomposition methods: Principal Component Analysis (PCA), Independent Component Analysis (ICA), and standard Dynamic Mode Decomposition (DMD) "
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,3.3,Simulation Study,"We generate a sequence of dynamic adjacency matrices G using Eq. 2 from three time-varying modes Φ 1 , Φ 2 , Φ 3 with corresponding frequencies ω 1 ∼ N (0.1, 0.05), ω 2 ∼ N(1, 0.1), ω 3 ∼ N(2.5, 0.1) (Hz). Each Φ p is a 32 × 32 block diagonal matrices with block sizes 16, 8, and 4. We choose a 1 = 1.01, a 2 = 0.9, a 3 = 1.05 and b 1 = b 2 = b 3 = 1. We simulate the process for k = 1, • • • , 29 time-points yielding a sequence of 30 matrices of shape 32 × 32. We repeat the process ten times with different ω 1 , ω 2 , ω 3 and generate ten matrix sequences. We apply PCA, ICA, and GraphDMD (Sect. 2.1) on G to extract three components and compare them against the ground truth modes using pearson correlation."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,3.4,Application of GraphDMD and DeepGraphDMD in HCP Data,"Comparison of DMs with sFC: The ground truth DMs are unknown for the HCP dataset; however, we can use the sFC as a substitute for the ground truth DM with ω = 0 (static DM). sFC offsets the DMs with ω > 0 as they have both positive and negative cycles, and thus only retain the static DM. For comparison, we compute the pearson correlation between the DM within the frequency bin 0-0.01 Hz and sFC for both GraphDMD and DeepGraphDMD. For PCA and ICA, we take the maximum value of the correlation between sFC and the (PCA or ICA) components."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,,Regression Analysis of Behavioral Measures from HCP:,"In this experiment, we regress the behavioral measures with the DMs within each frequency bin (Sect. 2.3) using Elastic-net. As an input to the Elastic-net, we take the real part of the upper diagonal part of the DM and flatten it into a vector. We then train the Elastic-net in two ways-1. single-band: where we train the Elastic-net independently with the DMs in each frequency bin, and 2. multi-band: we concatenate two DMs in the frequency bins: 0-0.01 Hz and 0.08-0.12 Hz and regress using the concatenated vector. For evaluation, we compute the correlation coefficient r between the predicted and the true values of the measures."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,4,Results,
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,4.1,Simulation Study,In Fig. 
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,4.2,Application of GraphDMD and DeepGraphDMD in HCP Data,"Comparison of DMs with sFC: The average pearson correlations with sFC across all the subjects are 0.6(±0.09), 0.6(±0.09), 0.84(±0.09), and 0.86(±0.05) for PCA, ICA, GraphDMD, and, DeepGraphDMD (Fig. "
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,,Regression Analysis of Behavioral Measures from HCP:,"We show the values of r across different methods in Table  Traditional dynamical functional connectivity analysis methods (such as sliding window-based techniques) consider a sequence of network states. However, our results show that these states can be further decomposed into more atomic network modes. The importance of decoupling these network modes from nonlinearly mixed fMRI signals using DeepGraphDMD has been shown in regressing behavioral measures from HCP data."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,5,Conclusion,"In this paper, we proposed a novel algorithm-DeepGraphDMD-to decouple spatiotemporal network modes in dynamic functional brain networks. Unlike other decomposition methods, DeepGraphDMD accounts for both the non-linear and the time-varying nature of the functional modes. As a result, these functional modes from DeepGraphDMD are more robust compared to their linear counterpart in GraphDMD and are shown to be correlated with fluid and crystallized intelligence measures."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,,Fig. 1 .,
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,,Fig. 2 .,
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,,Table 1 .,
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 35.
