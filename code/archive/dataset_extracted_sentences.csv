,title,extracted_keyword_sent
0,Anatomy-Driven Pathology Detection on Chest X-rays,"however, while image classification labels can be automatically
extracted from electronic health records or radiology reports [7,20], this is
typically not possible for bounding boxes, thus limiting the availability of
large datasets for pathology detection."
1,Anatomy-Driven Pathology Detection on Chest X-rays,"the resulting scarcity of large, publicly available datasets with
pathology bounding boxes limits the use of supervised methods for pathology
detection, such that current approaches typically follow weakly supervised
object detection approaches, where only classification labels are required for
training."
2,Anatomy-Driven Pathology Detection on Chest X-rays,"these
region boxes are easier to annotate -the physiological shape of a healthy
subject's thorax can be learned relatively easily by medical students -and
generalize better than those of pathologies, such that huge labeled datasets are
available [21]."
3,Anatomy-Driven Pathology Detection on Chest X-rays,"-we train
our models on the chest imagenome [21] dataset and evaluate on nih chestx-ray 8
[20], where we found that our loc-adpd model outperforms both, weakly supervised
methods and fully supervised detection with a small training set, while our
mil-adpd model is competitive with supervised detection and slightly outperforms
weakly supervised approaches."
4,Anatomy-Driven Pathology Detection on Chest X-rays,"while chexnet [14] follows the original approach, the method
provided with the nih chestx-ray 8 dataset [20] and the stl method [6] use
logsumexp (lse) pooling [13], while the multimap model [23] uses max-min pooling
as first proposed for the weldon [3] method."
5,Anatomy-Driven Pathology Detection on Chest X-rays,"along with the chest imagenome dataset [21] several
localized pathology classification models have been proposed which use a faster
r-cnn [16] to extract anatomical region features before predicting observed
pathologies for each region using either a linear model or a gcn model based on
pathology co-occurrences."
6,Anatomy-Driven Pathology Detection on Chest X-rays,training dataset.
7,Anatomy-Driven Pathology Detection on Chest X-rays,"we train on the chest imagenome dataset [4,21,22]1 ,
consisting of roughly 240 000 frontal chest x-ray images with corresponding
scene graphs automatically constructed from free-text radiology reports."
8,Anatomy-Driven Pathology Detection on Chest X-rays,"it is
derived from the mimic-cxr dataset [9,10], which is based on imaging studies
from 65 079 patients performed at beth israel deaconess medical center in
boston, us."
9,Anatomy-Driven Pathology Detection on Chest X-rays,"we consider the image-level label for a pathology to be positive if
any region is positively labeled with that pathology.we use the provided
jpg-images [11] 2 and follow the official mimic-cxr training split but only keep
samples containing a scene graph with at least five valid region bounding boxes,
resulting in a total of 234 307 training samples.during training, we use random
resized cropping with size 224 × 224, apply contrast and brightness jittering,
random affine augmentations, and gaussian blurring.evaluation dataset and class
mapping."
10,Anatomy-Driven Pathology Detection on Chest X-rays,"we evaluate our method on the subset of 882 chest x-ray images with
pathology bounding boxes, annotated by radiologists, from the nih chestxray-8
(cxr8) dataset [20] 3 from the national institutes of health clinical center in
the us."
11,Anatomy-Driven Pathology Detection on Chest X-rays,"all images are center-cropped
and resized to 224 × 224.the dataset contains bounding boxes for 8 unique
pathologies."
12,Anatomy-Driven Pathology Detection on Chest X-rays,"additionally, we also compare with a faster-rcnn [16] trained
on a small subset of roughly 500 samples from the cxr8 training set that have
been 2 https://physionet.org/content/mimic-cxr-jpg/2.0.0/ (physionet
credentialed health data license 1.5.0)."
13,Anatomy-Driven Pathology Detection on Chest X-rays,results on the nih chestx-ray 8 dataset [20].
14,Anatomy-Driven Pathology Detection on Chest X-rays,"supervision iou@10-70 iou@10 iou@30 iou@50 box class map ap loc-acc ap loc-acc
ap loc-acc mil-adpd (ours) an pa for all models, we only consider the predicted
boxes with the highest box score per pathology, as the cxr8 dataset never
contains more than one box per pathology."
15,Anatomy-Driven Pathology Detection on Chest X-rays,", 0.7), commonly used thresholds
on this dataset [20]."
16,Anatomy-Driven Pathology Detection on Chest X-rays,"additionally, we report the localization accuracy
(loc-acc) [20], a common localization metric on this dataset, where we use a box
score threshold of 0.7 for our method."
17,Anatomy-Driven Pathology Detection on Chest X-rays,"unlike loc-adpd and mil-adpd, all
baselines were either trained or finetuned on the cxr8 dataset, showing that our
method generalizes well to unseen datasets and that our class mapping is
effective.for detailed results per pathology we refer to the supp."
18,Anatomy-Driven Pathology Detection on Chest X-rays,"our
experiments demonstrate that using anatomical regions as proxies improves
results compared weakly supervised methods and supervised training on little
data, thus providing a promising direction for future research."
19,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"used an auto-encoder along with a gaussian process regression block to select
the best km to describe simulated kinetic data [13]."
20,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"until now, methods used simulated data
[11,13] or static pet [9], were supervised [9,[11][12][13] or predicted
macro-parameters [4,9,12]."
21,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"using the multi-clamp function, each
channel of the logits is restricted to the following parameter spaces:the limits
of the ranges were defined based on the meaning of the parameter (as in v b ),
mathematical requirements (as in the minimum values of k 2 and k 3 , whose sum
can not be zero) [6] or previous knowledge on the dataset derived by the work of
sari et al."
22,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,the dataset is composed of 23 oncological patients with different tumor types.
23,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"dpet data was acquired on a biograph vision quadra for 65 min, over 62 frames."
24,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"the dataset included the label maps of 7 organs
(bones, lungs, heart, liver, kidneys, spleen, aorta) and one image-derived input
function a(t) [bq/ml] from the descending aorta per patient."
25,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"further details on
the dataset are presented elsewhere [16].the pet frames and the label map were
resampled to an isotropic voxel size of 2.5 mm."
26,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"then, the dataset was split
patient-wise into training, validation, and test set, with 10, 4, and 9 patients
respectively."
27,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"details on the dataset split are available in the supplementary
material (table 1)."
28,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"a
possible way to leverage this would be to work on simulated data, yet the
validity of such evaluations strongly depends on how realistic the underlying
simulation models are."
29,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"however, most deep
learning approaches for segmentation require fully or partially labeled training
datasets, which can be time-consuming and expensive to annotate."
30,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"we evaluate our method on the brain tumor segmentation challenge (brats) dataset
[1,2,14], which contains 2,000 cases, each of which includes four 3d volumes
from four different mri modalities: t1, post-contrast enhanced t1 (t1-ce), t2,
and t2 fluid attenuated inversion recovery (t2-flair), as well as a
corresponding segmentation ground-truth mask."
31,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"the official data split divides
these cases by the ratio of 8:1:1 for training, validation, and testing (5,802
positive and 1,073 negative images)."
32,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"we
preprocess the data by slicing each volume along the z-axis to form a total of
193,905 2d images, following the approach of kang et al."
33,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"finally, the proposed ame-cam achieves optimal performance in all modalities of
the brats dataset.compared to the unsupervised baseline (ul), c&f is unable to
separate the tumor and the surrounding tissue due to low contrast, resulting in
low dice scores in all experiments."
34,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"2 shows the
visualization of the cam and segmentation results from all six cam-based
approaches under four different modalities from the brats dataset."
35,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"we aim to demonstrate the superiority of the proposed
attention-based aggregation approach for segmenting tumor regions in t1 mri of
the brats dataset."
36,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"note that we only report the results for t1 mri in the brats
dataset."
37,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,the experiments are done on the t1-ce mri of brats dataset.
38,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"1 and the
multipleexit using results from m 2 and m 3 , and using all exits (ame-cam) on
t1-ce mri in the brats dataset.the comparisons show that the activation map
obtained from the shallow layer m 1 and the deepest layer m 4 result in low dice
scores, around 0.15."
39,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"we then use an
attention model to hierarchically aggregate these activation maps, learning
pixel-wise weighted sums.experimental results on the four modalities of the 2021
brats dataset demonstrate the superiority of our approach compared with other
cam-based weakly-supervised segmentation methods."
40,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"specifically, ame-cam achieves
the highest dice score for all patients in all datasets and modalities."
41,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"in this work, we focus on the scenario where
radiological meta-data (thus, low-confidence labels) are available for a large
amount of images, whereas high-confidence labels, obtained by histological
analysis, are scarce.naive extensions of contrastive learning methods, such as
[5,10,11], from 2d to 3d images may be difficult due to limited gpu memory and
therefore small batch size."
42,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"these works implicitly assume a certain threshold on depth to define
positive and negative samples, which may be difficult to define and may be
different among applications and datasets."
43,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"we compare the proposed method with different contrastive and non-contrastive
methods, that either use no meta-data (simclr [5], byol [10]), or leverage only
discrete labels (supcon [13]), or continuous labels (depth-aware [8])."
44,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"this is the public lihc
dataset from the cancer genome atlas [9], which presents a histological score,
the ishak score, designated as y 2 histo , that differs from the metavir score
present in d 1 histo ."
45,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"similarly to the metavir score in d 1 histo
, we also binarize the ishak score, as proposed in [16,20], which results in two
cohorts of 34 healthy and 15 pathological patients.in all datasets, we select
the slices based on the liver segmentation of the patients."
46,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"for the
latter pretraining dataset, it presents an average slice spacing of 3.23 mm with
a standard deviation of 1.29 mm."
47,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"more
details and an illustration of tinynet are available in the supplementary
material, as well as a full illustration of the algorithm flow.data
augmentation, sampling and optimization."
48,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"cl methods [5,10,11] require strong
data augmentations on input images, in order to strengthen the association
between positive samples [22]."
49,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"data augmentations are computed on
the gpu, using the kornia library [17]."
50,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"all models share the same
data augmentation module, with a batch size of b = 64 and a fixed number of
epochs n epochs = 200."
51,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"then, we train a regularized logistic regression on the frozen
representations of the datasets d 1 histo and d 2 histo ."
52,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"as a baseline, we train a classification algorithm from
scratch (supervised) for each dataset, d 1 histo and d 2 histo , using both
backbone encoders and the same 5-fold crossvalidation strategy."
53,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"finally, we report the
cross-validated results for each model on the aggregated dataset we present in
table 1 the results of all our experiments."
54,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"for each of them, we
report whether the pretraining method integrates the weak label meta-data, the
depth spatial encoding, or both, which is the core of our method."
55,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"first, we can
notice that our method outperforms all other pretraining methods in d 1 histo
and d 1+2 histo , which are the two datasets with more patients."
56,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"for the second dataset d 2 histo , our method
is on par with byol and supcon when using a small encoder and outperforms the
other methods when using a larger backbone.to illustrate the impact of the
proposed method, we report in fig."
57,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"in this work, we proposed a novel kernel-based contrastive learning method that
leverages both continuous and discrete meta-data for pretraining."
58,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"we tested it
on a challenging clinical application, cirrhosis prediction, using three
different datasets, including the lihc public dataset."
59,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"to the best of our
knowledge, this is the first time that a pretraining strategy combining
different kinds of meta-data has been proposed for such application."
60,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"furthermore,
experts may focus on annotating objects they are already aware of, thereby
restricting the possibility of new structural discoveries in large datasets
using deep learning."
61,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"3) our approach outperforms previous
3d unsupervised discovery methods on challenging synthetic datasets and on a
real-world brain tumor segmentation (brats'19) dataset."
62,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"we draw γ from the uniform distribution: to compare with state-of-the-art
unsupervised 3d segmentation methods we follow
[13] and evaluate our method on challenging biologically inspired 3d synthetic
datasets and a real-world brain tumor segmentation (brats'19) dataset.the
synthetic dataset of [13], consists of 120 volumes (80-20-20 split) of size 50 ×
50 × 50."
63,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"the regular variant of the dataset contains
cubical and spherical objects, while the irregular variant contains more complex
shapes."
64,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"pink noise of magnitude m = 0.25 which is commonly seen in biological
data [30] is applied to the volume."
65,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"figure 3 shows sample slices of both
variants.the brats'19 dataset [2,3,21] is an established benchmark for 3d tumor
segmentation of brain mris."
66,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"we compare our method with state-of-the-art unsupervised 3d structure discovery
approaches including clustering using 3d feature learning [23], a 3d
convolutional autoencoder [24], and self-supervised hyperbolic representations
[13].for the synthetic datasets, we used k = 2 (background and cell) for level
1, k = 4 (background, cell, vesicle, mitochondria) for level 2, and k = 8
(background, cell, vesicle, mitochondria, and 4 small protein aggregates) for
level 3 predictions."
67,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"table 1 shows the results for the regular and irregular variants of the
cryo-et-inspired synthetic dataset."
68,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"[7] used 2% of annotated
data, zhao et al."
69,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"we also show in table 2 that features from early decoder stages of the
u-net-based diffusion models better discover larger objects in the hierarchy,
features at intermediate stages better capture intermediate objects, and
features at later stages better find smaller objects.for the brain tumor
segmentation (brats'19) dataset, we use the whole tumor (wt) segmentation mask
for evaluation, which is detectable based on the flair images alone."
70,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"our predictions look smoother and do not capture fine details
of tumor segmentations.we perform ablation studies on the brats'19 dataset
(table 3: below the line)."
71,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"to evaluate the
significance of the diffusion features, we replaced our diffusion feature
extractor with a 3d resnet from med3d [5] trained on 23 medical datasets."
72,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"our method outperforms
existing unsupervised segmentation approaches and discovers meaningful
hierarchical concepts on challenging biologically-inspired synthetic datasets
and on the brats brain tumor dataset."
73,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"while we tested our approach for
unsupervised image segmentation it is conceivable that it could also be useful
in semisupervised settings and that could be applied to data types other than
images."
74,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"the effectiveness of deep
learning models in medical applications is usually based on large,
well-annotated datasets, which in turn necessitates a time-consuming and
expertise-driven annotation process."
75,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"the effectiveness of medical applications during in-site deployment
depends on their ability to generalize to unseen data and remain robust against
data corruption."
76,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"therefore,
we comprehensively evaluate our approach on multiple datasets from various
medical sites to showcase its viability and effectiveness across different
contexts.dual-branch learning has been widely adopted in annotation-efficient
learning to encourage mutual consistency through co-teaching."
77,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"an extensive assessment of our approach through the examination of
four publicly accessible datasets establishes its superiority and clinical
significance."
78,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"in addition, spectrum learning also exhibits advantageous robustness and
generalization against adversarial attacks, data corruption, and distribution
shifts [19]."
79,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"as far as we know, spatial-spectral cross-domain consistency has never
been investigated to promote learning with sparse annotations of medical data."
80,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"consequently, such a scheme can lead to
better feature extraction, more meaningful data representation, and
domain-specific knowledge transmission, thus boosting model generalization and
robustness.entropy-guided pseudo label ensemble learning."
81,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,datasets.
82,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"we employ the sun-seg [10] dataset with scribble annotations for
training and assessing the in-distribution performance."
83,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"this dataset is based on
the sun database [16], which contains 100 different polyp video cases."
84,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"to reduce
data redundancy and memory consumption, we choose the first of every five
consecutive frames in each case."
85,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"we then randomly split the data into 70, 10,
and 20 cases for training, validation, and testing, leaving 6677, 1240, and 1993
frames in the respective split."
86,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"for out-of-distribution evaluation, we utilize
three public datasets, namely kvasir-seg [9], cvc-clinicdb [2], and polypgen [1]
with 1000, 612, and 1537 polyp frames, respectively."
87,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"these datasets are
collected from diversified patients in multiple medical centers with various
data acquisition systems."
88,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"varying data shifts and corruption like motion blur
and specular reflections2 pose significant challenges to model generalization
and robustness.implementation details."
89,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"2, our s 2 me achieves superior in-distribution performance quantitatively and
qualitatively compared with other baselines on the sun-seg [10] dataset."
90,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"regarding generalization and robustness, as indicated in table 2, our method
outperforms other weakly-supervised methods by a significant margin on three
unseen datasets, and even exceeds the fully-supervised upper bound on two of
them 4 ."
91,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"notably, the encouraging performance on unseen datasets exhibits
promising clinical implications in deploying our method to real-world scenarios."
92,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"with extensive in-domain and
out-ofdomain evaluation on four public datasets, our method shows superior
accuracy, generalization, and robustness, indicating its clinical significance
in alleviating data-related issues such as data shift and corruption which are
commonly encountered in the medical field."
93,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"one of the current bottle-necks to obtain better 3d models is the lack of more
abundant and higher quality correspondences in real data.this work introduces
superpoint-e, a new model to extract interest points from endoscopic images."
94,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"we propose to
automatically generate reliable training data from video sequences by tracking
feature points from existing detection methods, which do not require training."
95,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"this idea is supported for example by
recent efforts on collecting new public dataset, to further advance in this
field, such as endoscopic recordings from endoslam [16] and endomapper [1]
datasets."
96,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"superpoint supervision is referred to as homographic adaptation and assumes that
the surfaces are locally plane, which is not the case in our data."
97,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"our training set contains short sequences (4-7 s) from the
complete colonoscopy recordings in endomapper dataset where colmap software was
able to obtain a 3d reconstruction."
98,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"the following experiments demonstrate the proposed feature detection efficacy to
obtain 3d models on real colonoscopy videos, comparing different variations of
our approach and relevant baseline methods.dataset."
99,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"we seek techniques that are
applicable to real medical data, so we train and evaluate with subsequences from
the endomapper dataset [1], which contains a hundred complete endoscopy
recordings obtained during regular medical practice."
100,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"we use colmap 3d
reconstructions obtained from subsequences from this dataset (11260 frames from
65 reconstructions obtained along 14 different videos for training, and 838
frames from 7 reconstructions from 6 different videos for testing)."
101,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"for both metrics, our detector achieves significantly better
results, showcasing the better properties of our detector for 3d
reconstruction.to provide quantitative evaluation of the camera motion
estimation, we use a simulated dataset [3] to have ground truth available for
the camera trajectory."
102,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"we took 5 sequences of 100-150 frames from this dataset,
and we tested the baselines and our model."
103,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"simulated data lacks some of the biggest challenges of endoscopy
images (e.g."
104,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"however, data acquisition for medical
images poses unique challenges due to privacy concerns and the high cost of
manual annotation."
105,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"moreover, pathological images from different tissues or
cancer types often show significant domain shifts, which hamper the
generalization of models trained on one dataset to others."
106,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"[3,5,14] explore how to implicitly align target domain
data with the model trained on the source domain without accessing the source
domain data."
107,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"therefore, how to leverage the existing knowledge of black-box
models to effectively train new models for the target domain without accessing
the source domain data remains a critical challenge.in this paper, we present a
novel source-free domain adaptation framework for cross-tissue cell segmentation
without accessing both source domain data and model parameters, which can
seamlessly integrate heterogeneous models from different source domains into any
cell segmentation network with high generality."
108,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"we only use the source models' predictions on the target data for
knowledge transfer without accessing the source data and parameters."
109,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"also, we
apply two different perturbations (η, η ) to the target domain data and feed
them into the student model and the mean-teacher model respectively."
110,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"dataset and setting: we collect four pathology image datasets to validate our
proposed approach."
111,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"[10] publish a dataset of nucleus segmentation containing 5,060
segmented slides from 10 tcga cancer types."
112,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"we have also included 463 images of
kidney renal clear cell carcinoma (kirc) in our dataset, which are made publicly
available by irshad et al [11]."
113,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"[2] publicly release a dataset
containing tissue slide images and associated clinical data on colorectal cancer
(crc), from which we randomly select 200 patches for our study."
114,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"for each source domain network, we conduct
full-supervision training on the corresponding source domain data and directly
evaluate its performance on target domain data."
115,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"to ensure the reliability of the
results, we use the same data for training, validation, and testing, which
account for 80%, 10%, and 10% of the original data respectively."
116,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"in semi-supervised domain adaptation, we only use 10% of the
target domain data as labeled data.experimental results: to validate our method,
we compare it with the following approaches: (1) cellsegssda [8], an adversarial
learning based semisupervised domain adaptation approach."
117,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"2 demonstrate that our proposed
method exhibits superior performance, even when compared to these white-box
methods, surpassing them in various evaluation metrics and visualization
results.in addition, the experimental results also show that simply combining
multiple source data into a traditional single source will result in performance
degradation in some cases, which also proves the importance of studying
multi-source domain adaptation methods.ablation study: to evaluate the impact of
our proposed methods of weighted logits(wl), pseudo-cutout label(pcl) and
maximize mutual information(mmi) on the model performance, we conduct an
ablation study."
118,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"our proposed multi-source black-box domain adaptation method achieves
competitive performance by solely relying on the source domain outputs, without
the need for access to the source domain data or models, thus avoiding
information leakage from the source domain."
119,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"we
demonstrate the effectiveness of our method on multiple public datasets and
believe it can be readily applied to other domains and adaptation scenarios."
120,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,"yet, acquiring large training datasets and
their corresponding labels, especially from a cohort of patients, can be costly
or even infeasible, which poses a significant challenge in developing a dl model
with high performance [7]."
121,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,"second, even when large-scale datasets are available
through collaborative research from multiple sites, dl models trained on such
datasets may yield sub-optimal solutions due to domain gaps caused by
differences in images acquired from different sites [20]."
122,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,"third, due to the
small number of datasets from each domain, the images for each individual domain
may not capture representative features, limiting the ability of dl models to
generalize across domains [3].domain adaptation (da) has been extensively
studied to alleviate the aforementioned limitations, the goal of which is to
reduce the domain gap caused by the diversity of datasets from different domains
[12,20,26,29,33]."
123,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,"this is due to
sensitive privacy issues in patients' data, particularly in collaborative
research, which restricts access to labels from different domains."
124,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,"• our framework is effective at preserving privacy, since it carries out da
using only pre-trained network parameters, without transferring any patient
data."
125,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,"• we applied our framework to the task of segmenting breast cancer from
ultrasound imaging data, demonstrating its superior performance over competing
uda methods.our results indicate that our framework is effective in improving
the accuracy of breast cancer segmentation from ultrasound images, which could
have potential implications for improving the diagnosis and treatment of breast
cancer."
126,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,"all three databases contain ultrasound
imaging data and segmentation masks for breast cancer, with the masks labeled as
0 (background) and 1 (lesion) using a one-hot encoding."
127,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,"due to the low resolution of
ultrasound images, manual segmentation of breast cancer is challenging even for
expert clinicians, resulting in a sparse number of labeled data."
128,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,"additionally, our framework is
well-suited to a scenario in which access to source domain data is limited, due
to data privacy protocols."
129,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"but these supervised methods
relied heavily on the paired lpet and spet data that are rare in actual clinic
due to radiation exposure and involuntary motions (e.g., respiratory and muscle
relaxation)."
130,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"however, these methods still require lpet to train
models, which contradicts with the fact that only spet scans are conducted in
clinic.fortunately, the recent glowing diffusion model [6] provides us with the
idea for proposing a clinically-applicable pet enhancement approach, whose
training only relies on spet data."
131,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"the keys of
our upete include 1) compressing the 3d pet images into a lower dimensional
space for reducing the computational cost of diffusion model, 2) adopting the
poisson noise, which is the dominant noise in pet imaging [20], to replace the
gaussian noise in the diffusion process for avoiding the introduction of details
that are not existing in pet images, and 3) designing ct-guided cross-attention
to incorporate additional ct images into the inverse process for helping the
recovery of structural details in pet.our work had three main
features/contributions: i) proposing a clinicallyapplicable unsupervised pet
enhancement framework, ii) designing three targeted strategies for improving the
diffusion model, including pet image compression, poisson diffusion, and
ct-guided cross-attention, and iii) achieving better performance than
state-of-the-art methods on the collected pet datasets."
132,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"3 experiments our dataset consists of 100 spet images for training and 30 paired
lpet and spet
images for testing."
133,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"specifically, the spet images are reconstructed by using the 1200 s data between
60-80 min after tracer injection, while the corresponding lpet images are
simultaneously reconstructed by 120 s data uniformly sampled from 1200 s data."
134,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"as a basic data preprocessing, all images are resampled to voxel spacing of 2 ×
2 × 2 mm 3 and resolution of 256 × 256 × 160, while their intensity range is
normalized to [0, 1] by min-max normalization."
135,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"this
suggests that our upete can generate promising results without relying on paired
data, demonstrating its potential for clinical applications.qualitative
comparison: in fig."
136,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"we further evaluate the generalizability of our upete to tracer dose changes by
simulating poisson noise on spet to produce different doses for lpet, which is a
common way to generate noisy pet data [20]."
137,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"the main reason is that the unsupervised learning has the ability to
extract patterns and features from the data based on the inherent structure and
distribution of the data itself [15]."
138,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"in this paper, we have developed a clinically-applicable unsupervised pet
enhancement framework based on the latent diffusion model, which uses only the
clinically-available spet data for training."
139,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"validated by
extensive experiments, our upete achieved better performance than both
state-of-the-art unsupervised and fully-supervised pet enhancement methods, and
showed stronger generalizability to the tracer dose changes.despite the advance
of upete, our current work still suffers from a few limitations such as (1)
lacking theoretical support for our poisson diffusion, which is just an
engineering attempt, and 2) only validating the generalizability of upete on a
simulated dataset."
140,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"in our future work, we will complete the design of poisson
diffusion from theoretical perspective, and collect more real pet datasets
(e.g., head datasets) to comprehensively validate the generalizability of our
upete."
141,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"annotating 2d projections for 3d data is another approach
to using weak segmentation labels, which has garnered popularity recently in the
medical domain."
142,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"[4] train a vessel
segmentation model from unsupervised 2d labels transferred from a publicly
available dataset, however, there is still a gap to be closed between
unsupervised and supervised model performance."
143,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"loss of depth information occurs whenever 3d data is projected
onto a lower dimensional space."
144,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"in natural images, depth loss is inherent
through image acquisition, therefore attempts to recover or model depth have
been employed for 3d natural data."
145,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,dataset.
146,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"we use an in-house dataset of contrast-enhanced abdominal computed
tomography images (cts) in the arterial phase to segment the peripancreatic
arteries [6]."
147,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"the dataset contains binary 3d
annotations of the peripancreatic arteries carried out by two radiologists, each
having annotated half of the dataset."
148,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"for more information about
the dataset, see [6].image augmentation and transformation."
149,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"this
way, both data variance and initialization variance are accounted for through
cross-validation."
150,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"we
implement [13] as a baseline on our dataset, training on up to 3 fixed
orthogonal projections."
151,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"randomly selecting viewpoints for training acts as powerful data augmentation,
which is why we are able to obtain performance comparable to using more fixed
viewpoints."
152,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"we
theorize that this is because the dataset itself contains noisy annotations and
fully supervised models better overfit to the type of data annotation, whereas
our models converge to following the contrast and segmenting more vessels, which
are sometimes wrongfully labeled as background in the ground truth."
153,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"msd are not
very telling in our dataset due to the noisy annotations and the nature of
vessels, as an under-or over-segmented vessel branch can quickly translate into
a large surface distance.the effect of dataset size."
154,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"we vary the size of the
training set from |d tr | = 80 to as little as |d tr | = 10 samples, while
keeping the size of the validation and test sets constant, and train models on
single random viewpoints.in table 2, we compare single random projections
trained with and without depth information at varying dataset sizes to ilustrate
the usefulness of the depth information with different amounts of training data."
155,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"our depth loss offers consistent improvement across multiple dataset sizes and
reduces the overall performance variance."
156,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"the smaller the dataset
size is, the greater the performance boost from the depth."
157,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"we perform a wilcoxon
rank-sum statistical test comparing the individual sample predictions of the
models trained at various dataset sizes with single random orthogonal viewpoints
with or without depth information, obtaining a statistically significant
(p-value of < 0.0001)."
158,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"using a labeled dataset consisting of
single, randomly selected, orthogonal 2d annotations for each training sample
and additional depth information obtained at no extra cost, we obtain accuracy
almost on par with fully supervised models trained on 3d data at a mere fraction
of the annotation cost."
159,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"limitations of our work are that the depth information
relies on the assumption that the vessels exhibit minimal intensity fluctuations
within local neighborhoods, which might not hold on other datasets, where more
sophisticated ray-tracing methods would be more effective in locating the front
and back of projected objects."
160,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"furthermore, careful preprocessing is performed
to eliminate occluders, which would limit its transferability to datasets with
many occluding objects of similar intensities."
161,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"here, we focus on the automated construction of pdms because,
compared to deformation fields, point correspondences are easier to interpret by
clinicians, are computationally efficient for large datasets, and less sensitive
to noise and outliers than deformation fields [5].ssm performance depends on the
underlying process used to generate shape correspondences and the quality of the
input data."
162,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"mesh2ssm leverages unsupervised, permutation-invariant
representation learning to learn the low dimensional nonlinear shape descriptor
directly from mesh data and uses the learned features to generate a
correspondence model of the population."
163,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"mesh2ssm also includes an analysis
network that operates on the learned correspondences to obtain a data-driven
template point cloud (i.e., template point cloud), which can replace the initial
template, and hence reducing the bias that could arise from template selection."
164,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"this vae
branch serves two purposes: (a) serves as a shape analysis module for the
non-linear shape variations and (b) learns a data-specific template from the
latent space of the correspondences that is fed back to the correspondence
generation network.to motivate the need for the mesh feature encoder and study
the effect of the template selection, we considered the box-bump dataset, a
synthetic dataset of 3d shapes of boxes with a moving bump."
165,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"mesh2ssm performs best when the template is a medoid shape, which
makes the case for learning a data-specific template."
166,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"the mesh2ssm model also consists of an analysis branch that acts as a shape
analysis module to capture non-linear shape variations identified by the learned
correspondences {c i } n i=1 and also learns a data-informed template from the
latent space of correspondences to be fed back into the correspondence
generation network during training."
167,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"during
the alternate optimization phase, we generate the data-informed template from
the latent space of sp-vae at regular intervals."
168,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"the learned data-informed
template is used in the correspondence generation module in the subsequent
epochs."
169,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"dataset: we use the publicly available decath-pancreas dataset of 273
segmentations from patients who underwent pancreatic mass resection [24]."
170,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"although the dgcnn mesh autoencoder used in mesh2ssm does not require
the same number of vertices, uniformity across the dataset makes it
computationally efficient; hence, we pad the smallest mesh by randomly repeating
the vertices (akin to padding image for convolutions)."
171,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"similar to the
observations made box-bump dataset, flowssm is affected by the choice of the
template, and the modes of variation differ as the template changes."
172,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"pancreatic cancer mainly presents itself on the head of the
structure [20] and for the decath dataset, we can see the first mode identifies
the change in the shape of the head."
173,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"figure 4.a shows the metrics for the
pancreas dataset."
174,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"using the analysis module of
mesh2ssm, we visualized the top three modes of variation identified by sorting
the latent dimensions of sp-vae based on the standard deviations of the latent
embeddings of the training dataset."
175,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"for the pancreas dataset
with the medoid as the initial template, mesh2ssm with the template feedback
produced more precise models."
176,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"like most deep learning models,
performance of mesh2ssm could be affected by small dataset size, and it can
produce overconfident estimates."
177,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"incorporating template feedback loop via vae [13,21] analysis module helps in
mitigating bias and capturing non-linear characteristics of the data."
178,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"the method
is demonstrated to have superior performance in identifying shape variations
using fewer parameters on synthetic and clinical datasets."
179,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"therefore, there is a growing interest in
developing semi-supervised learning that leverages both labeled and unlabeled
data to improve the performance of image segmentation models [16,27].existing
semi-supervised segmentation methods exploit smoothness assumption, e.g., the
data samples that are closer to each other are more likely to to have the same
label."
180,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"we have seen such
perturbations being be added to natural input images at data-level
[4,9,14,19,21], feature-level [6,17,23,25], and model-level [8,11,12,24,28]."
181,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"mixup
regularization [29] is a data augmentation method used in deep learning to
improve model generalization."
182,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"4) we conduct comprehensive
experiments on adcd [1] and la [26] datasets, showing that our cross-ald
regularization achieves state-of-the-art performance against existing solutions
[8,11,12,14,21,22,28]."
183,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"considering the
resolution of medical images are usually high, we enhance the vanilla svgd [10]
from data-level to feature-level, which is named svgdf."
184,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"let d l and d ul be the labeled and unlabeled dataset, respectively, with p d l
and p d ul being the corresponding data distribution."
185,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"the labeled image x l and segmentation
groundtruth y are sampled from the labeled dataset d l (x l , y ∼ p d l ), and
the unlabeled image sampled from d ul is x ∼ p d ul .given an input x ∼ p d ul
(i.e., the unlabeled data distribution), let us denote the ball constraint
around the image x aswhere is a ball constraint radius with respect to a norm ||
• || p , and x is an adversarial example2 ."
186,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"while vanilla svgd [10]
is difficult to capture semantic meaning of high-resolution data because of
calculating rbf kernel (k) directly on the data-level, we use the feature
extractor φ as a semantic transformation to further enhance the svgd algorithm
performance for medical imaging."
187,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"the
first term is the dice loss, where labeled image x l and segmentation
ground-truth y are sampled from labeled dataset d l ."
188,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"in this section, we conduct several comprehensive experiments using the acdc4
dataset [1] and the la5 dataset [26] for 2d and 3d image segmentation tasks,
respectively."
189,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"we evaluate our model in challenging
semi-supervised scenarios, where only 5% and 10% of the data are labeled and the
remaining data in the training set is treated as unlabeled."
190,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"the cross-ald uses
the u-net [18] and v-net [13] architectures for the acdc and la dataset,
respectively."
191,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"we then illustrate the cross-ad outperforms other recent methods
on acdc and la datasets in sect."
192,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"φ is the decoder of
u-net in acdc dataset, while φ is the decoder of v-net in la dataset."
193,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"we randomly pick three
images from the datasets to generate adversarial particles."
194,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"in table 1 and 2, the cross-ald can significantly outperform
other recent methods with only 5%/10% labeled data training based on the four
metrics."
195,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"especially, our method impressively gains 14.7% and 2.3% dice score
higher than state-of-the-art ss-net using 5% labeled data of acdc and la,
respectively."
196,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"3.2, and train the models with 5% labeled training data of acdc and la."
197,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"we adapt cross-ald to semi-supervised medical image
segmentation to achieve start-of-the-art performance on the acdc and la datasets
compared to many recent methods such as vat [14], ua-mt [28], sassnet [8], dtc
[11], urpc [12] , mc-net [22], and ss-net [21]."
198,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"ultrasound (us) data before and after the tissue deformation (which
can be caused by an external or internal force) are collected and compared to
calculate the displacement map, indicating each individual sample's relative
motion."
199,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"we then present the training and test datasets and finish the section by
demonstrating the network architecture."
200,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"picture loss is added to the data and
smoothness losses of unsupervised training."
201,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"while the
latter involves adjusting trained weights based on the training data and keeping
them fixed during testing, the former relies on iterative refinement that is
adaptable to the test data and does not require any learnable weights."
202,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"the loss function can be written as: where l d
denotes photometric loss which is obtained by comparing the precompressed and
warped compressed rf data, l s is smoothness loss in both axial and lateral
directions."
203,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"we use publicly available data collected from a breast phantom (model 059, cirs:
tissue simulation & phantom technology, norfolk, va) using an alpinion e-cube
r12 research us machine (bothell, wa, usa)."
204,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"this data is available online at http://code.sonography.ai
in [16].in vivo data was collected at johns hopkins hospital from patients with
liver cancer during open-surgical rf thermal ablation by a research antares
siemens system using a vf 10-5 linear array with the sampling frequency of 40
mhz and the center frequency of 6.67 mhz."
205,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"we selected 600 rf frame
pairs of this dataset for the training of the networks.two well-known metrics of
contrast to noise ratio (cnr) and strain ratio (sr) are utilized to evaluate the
compared methods."
206,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"we also employed a similar hyper-parameters and
training schedule for experimental phantom and in vivo data."
207,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"the lateral strains of ultrasound rf data collected from three different
locations of the tissue-mimicking breast phantom are depicted in fig."
208,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"kpicture
further limits the epr values; only a small number of samples are outside of the
physically plausible range.the lateral strain results of in vivo data are
depicted in fig."
209,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"furthermore, 3d imaging data can be collected from 2d arrays to have information
in out-of-plane direction to be able to formulate known operators and picture
loss for anisotropic tissues.it should be noted that after incorporating the
known operators, the inference time of the network increased from an average of
195 ms to 240 ms (having 10 iterations for algorithm 1 and 100 iterations for
algorithm 2)."
210,SLPD: Slide-Level Prototypical Distillation for WSIs,"taking stage two as an example, dino distills the knowledge
from teacher to student by minimizing the cross-entropy between the probability
distributions of two views at region-level:where h(a, b) = -a log b, and p d is
the data distribution that all regions are drawn from."
211,SLPD: Slide-Level Prototypical Distillation for WSIs,"clustering can reveal the representative patterns in the data and
has achieved success in the area of unsupervised representation learning
[4,5,24,26].to characterize the histopathologic features underlying the slides,
a straightforward practice is the global clustering, i.e., clustering the region
embeddings from all the wsis, as shown in the left of fig."
212,SLPD: Slide-Level Prototypical Distillation for WSIs,"meanwhile, this clustering strategy ignores the
hierarchical structure ""region→wsi→whole dataset"" underlying the data, where the
id of the wsi can be served as an extra learning signal."
213,SLPD: Slide-Level Prototypical Distillation for WSIs,"specifically, for a region embedding z belonging to the slide w and
assigned to the prototype c, we first search the top-k nearest neighbors of w in
the dataset based on the semantic similarity, denoted as { ŵk } k k=1 ."
214,SLPD: Slide-Level Prototypical Distillation for WSIs,datasets.
215,SLPD: Slide-Level Prototypical Distillation for WSIs,"we conduct experiments on two public wsi datasets for downstream
tasks."
216,SLPD: Slide-Level Prototypical Distillation for WSIs,"the data splitting scheme is kept
consistent with hipt."
217,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,"(4) ablation and evaluation studies on two public datasets demonstrate our
model's ability to outperform state-of-the-art techniques not only with ideal
labels but also with shifted labels."
218,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,dataset.
219,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,"to validate the effectiveness of our model, we use two public nuclei
segmentation datasets i.e., cpm17 [26] & monuseg [12]."
220,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,"monuseg is a multi-organ nuclei
segmentation dataset consisting of 30 h&e stained images (1000×1000) extracted
from seven different organs."
221,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,"we obtain statistically significant (p-value <0.05) for the aji of all
comparison methods on two datasets in all scenarios."
222,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,"compared to other variants, our proposed
model is more robust to the point shift in both datasets."
223,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,"according to our experimental results, we
established a new state-of-art on two publicly available datasets across
different levels of point annotation imperfections."
224,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"traditional supervised models would not
be appropriate, as acquiring sufficient training data to identify such a broad
range of pathologies is not feasible."
225,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"to the best of our knowledge, this is the first work to
train models to directly identify anomalies on tasks that are deformation-based,
tasks that use poisson blending with patches extracted from external datasets,
and tasks that perform efficient poisson image blending in 3d volumes, which is
in itself a new contribution of our work."
226,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"this is built on the assumption that a reconstruction model will
only be able to correctly reproduce data that is similar to the instances it has
been trained on, e.g."
227,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"[5]
sought to identify more meaningful errors in image reconstructions by comparing
the reconstructions of models trained on only healthy data against those trained
on all available data.however, the general assumption that reconstruction error
is a good basis for an anomaly scoring function has recently been challenged."
228,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"similar to cutpaste [11], [7] fully
replaces 3d patches with data extracted from elsewhere in the same sample, but
then trains the model to segment the patches."
229,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"this allows us to monitor performance and prevent overfitting, all
without the need for real anomalous data."
230,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"each self-supervised task involves
introducing a synthetic anomaly into otherwise normal data whilst also producing
the corresponding label."
231,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"if the training and validation tasks are too similar, the performance
on the validation set may be an overly optimistic estimate of how the model
would perform on unseen real-world anomalies.when performing cross-validation
over all synthetic tasks and data partitions independently, the number of
possible train/validation splits increases significantly, requiring us to train
f • (t n ct ) independent models, where t n is the total number of tasks, t is
the number of tasks used to train each model and f is the number of data folds,
which is computationally expensive."
232,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"instead, as in our case t n = f = 5, we opt
to associate each task with a single fold of the training data (fig."
233,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"in each iteration, the
corresponding data folds are collected and used for training or validation,
depending on which partition forms the majority."
234,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"also, all tasks share a common recipe: the target anomaly mask m h is always a
randomly sized and rotated ellipse or rectangle (ellipsoids/cuboids in 3d); all
anomalies are positioned such that at least 50% of the mask intersects with the
foreground of the image; and after one augmentation is applied, the process is
randomly repeated (based on a fair coin toss, p = 0.5), for up to a maximum of 4
anomalies per image.the intra-dataset blending task."
235,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"the intra-dataset blending task therefore
results from xintra = p oissonblend(x, x , m h ) with x, x ∈ d with samples from
a common dataset d and is therefore similar to the self-supervision task used in
[23] for 2d images.the inter-dataset blending task follows the same process as
intra-dataset blending but uses patches extracted from an external dataset d ,
allowing for a greater variety of structures."
236,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"data: we evaluate our method on t2-weighted brain mr and chest x-ray datasets to
provide direct comparisons to state-of-the-art methods over a wide range of real
anomalies."
237,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"for brain mri we train on the human connectome project (hcp) dataset
[28] which consists of 1113 mri scans of healthy, young adults acquired as part
of a scientific study."
238,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"to evaluate, we use the brain tumor segmentation
challenge 2017 (brats) dataset [1], containing 285 cases with either high or low
grade glioma, and the ischemic stroke lesion segmentation challenge 2015 (isles)
dataset [13], containing 28 cases with ischemic stroke lesions."
239,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"the data from
both test sets was acquired as part of clinical routine."
240,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"the hcp dataset was
resampled to have 1mm isotropic spacing to match the test datasets."
241,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"lastly, samples are
downsampled by a factor of two.for chest x-rays we use the vindr-cxr dataset
[18] including 22 different local labels."
242,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"to
avoid this ambiguity, we replace these samples with leftover training data that
all radiologists have labelled as healthy."
243,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"note that the distribution shift
between training and test data (research vs."
244,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"in particular, we achieve a
pixel-wise ap of 76.2 and 45.9 for brats and isles datasets respectively."
245,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"we, however, only use synthetic validation data."
246,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"this further verifies
that our method of using synthetic data to estimate generalisation works
well.for both vindr-cxr test sets we evaluate at a sample and pixel level,
although previous publications have only reported their results at a sample
level."
247,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"this shows that our use of synthetic
validation data succeeds where their fixed training schedule fails."
248,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"even though their
method uses twice as much training data, as well as some real anomalous data,
our purely synthetic method begins to close the gap (ap of [6] 84.3 vs."
249,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"for the brain datasets, all metrics generally decrease as the
number of training tasks increases."
250,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"this could be due to the distribution shift
between training and test data."
251,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"although more training tasks may increase
sensitivity to diverse irregularities, this can actually become a liability if
there are differences between (healthy) training and test data (e.g."
252,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,a unique aspect of the brain data is the domain shift.
253,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"the
hcp training data was acquired at a much higher isotropic resolution than the
brats and isles test data, which are both anisotropic."
254,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"incorporating greater data augmentations, such
as simulating anisotropic spacing, could further improve results by training the
model to ignore these transformations."
255,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"we also achieve strong results for the
x-ray data, although precise localisation remains a challenging task."
256,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"this enables more robust training
without the need for real anomalous training or validation data."
257,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"the label encoder
encodes the text labels in the dataset into n label features, denoted as l ∈ r n
×c l , where n represents the number of classes in the dataset and c l
represents the dimension of label features."
258,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"here we use
medclip1 as our label encoder, which is a model fine-tuned on the roco dataset
[12] based on clip [14].knowledge encoder."
259,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"clinicalbert is a language
model that has been fine-tuned on the mimic-iii [8] dataset based on biobert
[9].adaptive layer."
260,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"we freeze the label and knowledge encoders for training
efficiency but add an adaptive layer after the text encoders to better tailor
the text features to our dataset."
261,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"we first perform min-max normalization
on it, the formula is as followswhere 1 ≤ c ≤ n means c th class in the dataset."
262,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"cam [20] and grad-cam [15] were
evaluated using the same resnet38 [16] classifier, and the results showed that
cam [20] outperformed grad-cam [15], with miou values of 70.44% and 56.52% on
the luad-histoseg and bcss-wsss datasets, respectively."
263,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"this could be due to the design of transws [18] for single-label
image segmentation, with the segmentation branch simplified to binary
segmentation to reduce the difficulty, while our dataset consists of multilabel
images."
264,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"our proposed method outperformed all
previous methods on both luad-histoseg and bcss-wsss datasets, with improvements
of 2.64% and 5.42% over the second-best method, respectively (table 2)."
265,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"due to its heavy reliance on dataset-specific
post-processing steps, histosegnet [5] failed to produce the desired results on
our datasets."
266,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"as we have previously analyzed since the datasets we used are all
multi-label images, it was challenging for the segmentation branch of transws
[18] to perform well, and it failed to provide an overall benefit to the model."
267,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"specifically, our miou scores exceeded the second-best method by 3.17%
and 3.09% on luad-histoseg and bcss-wsss datasets, respectively."
268,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"the proposed method
achieves the best results on two public datasets, luad-histoseg and bcss-wsss,
demonstrating the superiority of our method."
269,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,"manual editing of vessel geometry
is a tedious and error prone task that requires expert medical knowledge, which
explains the scarcity of curated datasets."
270,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,"although these model-based methods
provide some degree of control and variation in the structures produced, they
often fail to capture the diversity of real anatomical data.in recent years,
deep neural networks led to the development of powerful generative models [30],
such as generative adversarial networks [8,12] and diffusion models [11], which
produced groundbreaking performance in many applications, ranging from image and
video synthesis to molecular design."
271,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,"however, this model is
limited to generating single-channel blood vessels and thus does not support the
generation of more complex, tree-like vessel topologies.in this work we propose
a novel data-driven framework named vesselvae for synthesizing blood vessel
geometry."
272,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,"in contrast to previous data-driven methods, our recursive
network fully exploits the hierarchical organization of the vessel and learns a
low-dimensional manifold encoding branch connectivity along with geometry
features describing the target surface."
273,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,"to the best of our
knowledge, this work is the first to synthesize multi-branch blood vessel trees
by learning from real data."
274,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,"collectively, these supplementary networks streamline the
data transformation process through the model."
275,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,"dataset and pre-processing overview: the raw
meshes from the intraa 3d collection undergo pre-processing using the vmtk
toolkit."
276,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,"the data pre-processing
pipeline and network code were implemented in python and pytorch
framework.training."
277,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,"this means that the amount of memory required to store and
manipulate our training data structures is minimal."
278,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,"by using these metrics,
we can determine how well the generated 3d models of blood vessels match the
original dataset distribution, as well as the diversity of the generated output."
279,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"transfer learning has become a standard practice in medical image analysis as
collecting and annotating data in clinical scenarios can be costly."
280,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"metalr uses meta-learning to automatically
optimize layer-wise lr for fine-tuning.diversity between domains and tasks and
privacy concerns related to pre-training data."
281,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"it can even
be irregular among layers for medical domains far from pre-training data [7]."
282,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"we also enhance the
algorithm's performance and stability with a proportional hyper-lr (lr for lr)
and a validation scheme on training data batches.in summary, this work makes the
following three contributions."
283,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"1) we introduce metalr, a meta-learning-based lr
tuner that can adaptively adjust layerwise lrs based on transfer learning
feedback from various medical domains.2) we enhance metalr with a proportional
hyper-lr and a validation scheme using batched training data to improve the
algorithm's stability and efficacy."
284,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"finally, we
demonstrate the use of a proportional hyper-lr and a validation scheme with
batched training data to enhance performance."
285,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"let (x, y) denote a sample-label pair, and {(x i , y i ) | i = 1, ..., n } be
the training data."
286,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"the validation dataset {(x v i , y v i ) | i = 1, ..., m } is
assumed to be independent and identically distributed as the training dataset."
287,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"the optimal parameters θ * (α) are given by optimization on the training
data."
288,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"at the same time, the best lr tuning scheme α * can be optimized based on
the feedback for θ * (α) from training data d, validation data d v , initial
model parameter {θ 0 1 , ..., θ 0
d }, lrs {α 0 1 , ..., α 0 d }, batch size n, max iteration t; output:final
model parameterstep forward for one step to get { θt 1 (α 4); 7: end for the
validation loss."
289,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"at
the iteration t of training, a training data batch {(x i , y i ) | i = 1, ...,
n} and a validation data batch {(x v i , y v i ) | i = 1, ..., n} are sampled,
where n is the size of the batches."
290,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"so the second
step of metalr is to move the lrs along the meta-objective gradient on the
validation data:where η is the hyper-lr."
291,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"one limitation of metalr is that the lrs are updated using separate validation
data, which reduces the amount of data available for the training process."
292,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"this
can be particularly problematic for medical transfer learning, where the amount
of downstream data has already been limited."
293,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"3, the update of
model parameter θ t j and lr α t j is performed using different datasets to
ensure that the updated θ t j can be evaluated for generalization without being
influenced by the seen data."
294,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"as an alternative, but weaker, approach, we explore
using another batch of training data for eq."
295,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"to ensure the reproducibility of the results, all pre-trained models
(uscl [9], imagenet [11], c2l [28], models genesis [29]) and target datasets
(pocus [5], busi [1], chest x-ray [17], lits [4]) are publicly available."
296,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"in our
work, we consider models pre-trained on both natural and medical image datasets,
with three target modalities and three target organs, which makes our
experimental results more credible."
297,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"the
validation set for the lits segmentation dataset comprises 23 samples from the
training set of size 111."
298,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"the basic metalr algorithm, the proportional hyper-lr, and
batched-training-data validation (as shown in table 2)."
299,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"secondly, part of the training data are split for validation, which can be
detrimental to the performance."
300,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"moreover, although the generalization validation on the training data batch may
introduce bias, providing sufficient training data ultimately benefits the
performance."
301,DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,"on the other hand, feeding modern neural networks with the entire
gigapixel image is not a feasible approach, forcing to crop data into small
patches and use them for training."
302,DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,"our proposal has proven its
effectiveness on two well-known histological datasets, camelyon16 and tcga lung
cancer, obtaining state-of-the-art results on wsi classification."
303,DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,"it is available on the gdc data transfer portal and comprises two subsets of
cancer: lung adenocarcinoma (luad) and lung squamous cell carcinoma (lusc),
counting 541 and 513 wsis, respectively."
304,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"to address this problem, the common paradigm of
transfer learning, which first pre-trains a model on upstream image datasets and
then fine-tunes it on various target tasks, has been widely investigated in
recent years [10,21,30]."
305,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"with the increasing number of pre-trained networks provided by the
community, model repositories like hugging face [25] and pytorch hub [18] enable
researchers to experiment across a large number of downstream datasets and
tasks."
306,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"a brute-force method is to
fine-tune a set of pretrained models with target datasets to find the optimal
one."
307,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"existing methods also measured
the task-relatedness between source and target datasets [6,7,22,28]."
308,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"however,
most of these works require source information available while medical images
have more privacy and ethical issues and fewer datasets are publicly available
than natural images.considering the issues mentioned above, this work focused on
source-free pre-trained model selection for segmentation tasks in the medical
image."
309,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"1, models pre-trained by upstream data constitute the
model bank."
310,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"the main idea is to directly measure the transferability of the
pre-trained models without fully training based on the downstream/target
dataset."
311,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"extensive experiments
have proved the superiority of our method compared with baseline methods.2
methodology in our work, a model bank m consisting of pre-trained models {m i }
k i=1 are
available to be fine-tuned and evaluated with a target dataset, where x j is the
image and y j is the ground truth of segmentation."
312,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"our work is to directly
estimate the transferability score t i s→t without fine-tuning the model on
target datasets."
313,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"the proposed method is intuitive and straightforward: features extracted by the
pre-trained model should be consistent within the class of the target dataset
while representative and various globally."
314,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"therefore, class consistency and
feature variety are considered to estimate the transferability between models
and downstream data.class consistency."
315,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"the pre-trained models are trained with
specific pretext tasks based on the upstream dataset."
316,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"therefore, features
extracted by the pretrained models cannot perfectly distinguish the foreground
and background of target data."
317,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"given a pair of target data x j and x j , the distribution of the
features is modeled with the n-dimensional gaussian distribution."
318,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"the class consistency between
the data pair is measured by the wasserstein distance [16] as follows:whereare
covariance matrices of f k j and f k j ."
319,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"we calculate the
wasserstein distance of the distribution with voxels of the same class in a
sample pair comprised of every two samples in the dataset, and obtained the
following definition of class consistency c consgiven that 3d medical images are
computationally intensive, and prone to causing out-of-memory problems, in the
sliding window inference process for each case, we do not concatenate the output
of each patch into the final prediction result, but directly sample from the
patched output and concatenate them into the final sampled feature matrix."
320,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"for the dataset with n cases, we choose s = 1 and the feature variety f v is
formulated asoverall estimation."
321,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"besides, we
decrease the sampling ratio in the decoder layer close to the bottleneck to
avoid feature redundancy.the final transferability of pre-trained model m to
dataset t t m→t iswhere d is the number of decoder layers used in the
estimation."
322,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"the medical segmentation decathlon (msd) [2] dataset is composed of ten
different datasets with various challenging characteristics, which are widely
used in the medical image analysis field."
323,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"to evaluate the effectiveness of
cc-fv, we conduct extensive experiments on 5 of the msd dataset, including
task03 liver(liver and tumor segmentation), task06 lung(lung nodule
segmentation), task07 pancreas(pancreas and pancreas tumor segmentation), task09
spleen(spleen segmentation), and task10 colon(colon cancer segmentation)."
324,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"all of
the datasets are 3d ct images."
325,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"the public part of the msd dataset is chosen for
our experiments, and each dataset is divided into a training set and a test set
at a scale of 80% and 20%."
326,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"for each dataset, we use the other four datasets to
pre-train the model and fine-tune the model on this dataset to evaluate the
performance as well as the transferability using the correlation between two
ranking sequences of upstream pre-trained models."
327,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"the pearson coefficient also ranges from [-1, 1], and
measures how well the data can be described by a linear equation."
328,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"firstly, it requires costly manual
annotations.secondly, the resulting models may not generalize well to unseen
data domains."
329,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"ssl pre-trains a model
backbone to extract informative representations from unlabeled data."
330,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"pre-training the backbone in a
self-supervised manner enables scaling to larger datasets across multiple data
and task domains."
331,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"in medical imaging, this is particularly useful given the
growing number of available datasets.in this work, we focus on contrastive
learning [8,12], one of the most effective approaches to ssl in computer vision."
332,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"second, we
employ vox2vec to pre-train a fpn architecture on a diverse collection of six
unannotated datasets, totaling over 6,500 ct images of the thorax and abdomen."
333,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"finally, we compare the
pretrained model with the baselines on 22 segmentation tasks on seven ct
datasets in three setups: linear probing, non-linear probing, and fine-tuning."
334,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"for pre-training, we use 6 public ct datasets [1,3,5,15,21,27], totaling more
than 6550 cts, covering abdomen and thorax domains."
335,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"we do not use the
annotations for these datasets during the pre-training stage."
336,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"we evaluate our method on the beyond the cranial vault abdomen (btcv) [19] and
medical segmentation decathlon (msd) [4] datasets."
337,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"the btcv dataset consists of
30 ct scans along with 13 different organ annotations."
338,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"we test our method on 6
ct msd datasets, which include 9 different organ and tumor segmentation tasks."
339,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"the segmentation performance of each model
on btcv and msd datasets is evaluated by the dice score.for our method, the
pre-processing steps are the same for all datasets, as at the pre-training
stage, but in addition, intensities are clipped to (-1350, 1000) hu window and
rescaled to (0, 1).we compare our results with the current state-of-the-art
self-supervised methods [2,26] in medical imaging."
340,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"the mean value and standard deviation of dice score across 5 folds on the btcv
dataset for all models in all evaluation setups are presented in table 1."
341,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"we demonstrate an example of the
excellent performance of vox2vec-fpn in both linear and non-linear probing
regimes in supplementary materials.we reproduce the key results on msd challenge
ct datasets, which contain tumor and organ segmentation tasks."
342,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"by pre-training a fpn backbone to
extract informative representations from unlabeled data, our method scales to
large datasets across multiple task domains."
343,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"we plan to investigate further how the performance of vox2vec
scales with the increasing size of the pre-training dataset and the pre-trained
architecture size."
344,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"another interesting research direction is exploring the
effectiveness of vox2vec with regard to domain adaptation to address the
challenges of domain shift between different medical imaging datasets obtained
from different sources."
345,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"in spite of the huge success,
the deployment of trained segmentation models is often severely impacted by a
distribution shift between the training (or labeled) and test (or unlabeled)
data since the segmentation performance will deteriorate greatly in such
situations."
346,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"domain shift is typically caused by various factors, including
differences in acquisition protocols (e.g., parameters, imaging methods,
modalities) and characteristics of data (e.g., age, gender, the severity of the
disease and so on).domain adaptation (da) has been proposed and investigated to
combat distribution shift in medical image segmentation."
347,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"we
evaluate our proposed method on two multi-domain datasets: 1)."
348,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"the infant brain
mri dataset for cross-age segmentation; 2)."
349,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"the brats2018 dataset for
cross-grade tumor segmentation."
350,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"4.as
the learned prompt and encoder feature capture quite different aspects of the
input data, we cannot achieve good effect by simply using addition,
multiplication or concatenation to serve as the fusion function ψ."
351,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"our proposed method was evaluated using two medical image segmentation da
datasets."
352,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"the first dataset, i.e., cross-age infant segmentation [20], was used
for cross-age infant brain image segmentation, while the second dataset, i.e.,
brats2018 [21], was used for hgg to lgg domain adaptation.the first dataset is
for infant brain segmentation (white matter, gray matter and cerebrospinal
fluid)."
353,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"to build the cross-age dataset, we take advantage 10 brain mris of
6-month-old from iseg2019 [20], and also build 3-month-old and 12-month-old
in-house datasets."
354,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"in this dataset, we collect 11 brain mri for both the
3-month-old and 12-month-old infants."
355,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"we take the 6-month-old data as the source
domain, the 3-month-old and 12-month-old as the target domains.the 2nd dataset
is for brain tumor segmentation (enhancing tumor, peritumoral edema and necrotic
and non-enhancing tumor core), which has 285 mri samples (210 hgg and 75 lgg)."
356,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"as observed, our method demonstrates very good da
ability on the crossage infant segmentation task, which improves about 5.46 dice
and 4.75 dice on 12-month-old and 3-month-old datasets, respectively."
357,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"also, the proposed method shows considerable improvements
over adda and cycada, but very subtle improvements to the sifa and adr methods
(although adr shows a small advantage on the whole category).we also visualize
the segmentation results on a typical test sample of the infant brain dataset in
fig."
358,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"is
prompt-da compatible with adv-da?the corresponding experiments are conducted on
the infant brain dataset and experimental results are shown in table 2."
359,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"experiments on two da datasets with two different segmentation backbones
demonstrate that our proposed method works well on da problems."
360,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"a common challenge for deploying deep learning to clinical problems is the
discrepancy between data distributions across different clinical sites
[6,15,20,28,29]."
361,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"to solve this problem, many unsupervised domain
adaptation (uda) methods [6] have been developed for adapting a model to a new
site with only unlabeled data (target domain) by transferring the knowledge
learned from the original dataset (source domain)."
362,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"however, most uda methods
require sufficient target samples, which are scarce in medical imaging due to
the limited accessibility to patient data."
363,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"previous studies [7,10,24,25] have demonstrated that transferring
the amplitude spectrum of target domain images to a source domain can
effectively convey image style information and diversify training dataset."
364,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"[27]
demonstrated that using a spectral sensitivity map to weigh the amplitude
perturbation is an effective data augmentation."
365,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"however, existing sensitivity
maps only use single-domain labeled data and cannot leverage target domain
information."
366,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"2) sample hardness: existing studies
[11,19] have shown that mining hard-to-learn samples in model training can
enhance the efficiency of data augmentation and improve model generalization
performances."
367,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"therefore, to maximize the use of the limited target domain data,
we incorporate an adversarial approach into the spectral mixing process to
generate the most challenging data augmentations."
368,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"the sensitivity at frequency (i, j) of a
model f trained on the source domain is defined as the prediction error rate
over the whole dataset x s as in (1), where acc denotes the prediction accuracy
using the dodiss map m s and an adversarially learned parameter λ * as a
weighting factor, samix mixes the amplitude spectrum of each source image with
the spectrum of a target image."
369,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,we evaluated samix on two medical image datasets.
370,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"as the availability of target domain images is limited, data efficiency plays a
crucial role in determining the data augmentation performance."
371,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"to assess the efficacy of the components in samix, we conducted an ablation
study with adaptseg+samix and daln+samix (full model) on fundus and camelyon
datasets."
372,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"this makes it expensive and
non-viable for curating large datasets for training large dnn models."
373,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"in another
recent work, [5] has exploited additional unlabeled video data for learning good
representations for downstream gbc classification and obtained performance
similar to [3] using a resnet50 [13] classifier."
374,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"the reliance of both sota
techniques on additional annotations or data, limits their applicability."
375,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"-we formulate the gbc
classification problem as a weakly supervised object detection problem to
mitigate the effect of low inter-class and large intra-class variances, and
solve the difficult gbc detection problem on us images without using the costly
and difficult to obtain additional annotation (bounding box) or video data."
376,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"gallbladder cancer detection in ultrasound images: we use the public gbc us
dataset [3] consisting of 1255 image samples from 218 patients."
377,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"the dataset
contains 990 non-malignant (171 patients) and 265 malignant (47 patients) gb
images (see fig."
378,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"the dataset contains image labels as
well as bounding box annotations showing the malignant regions.note that, we use
only the image labels for training."
379,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"polyp detection in colonoscopy images: we use the publicly available kvasir-seg
[17] dataset consisting of 1000 white light colonoscopy images showing polyps
(see fig."
380,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"since kvasir-seg does not contain any control images, we add 600
non-polyp images randomly sampled from the polypgen [1] dataset.since the
patient information is not available with the data, we use random stratified
splitting for 5-fold cross-validation."
381,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"for gbc
classification, if the model generates bounding boxes for the input image, then
we predict the image to be malignant, since the only object present in the data
is the cancer.mil setup: the decoder of the fine-tuning detr generates r
d-dimensional output embeddings."
382,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"s1.classification performance: we compare our model with the standard cnnbased
and transformer-based classifiers, sota wsod-based classifiers, and sota
classifiers using additional data or annotations (table 3)."
383,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"the current sota gbc detection models require additional bound- [25]
0.829 ± 0.030 0.900 ± 0.040 0.875 ± 0.063 pvtv2 [26] 0.824 ± 0.033 0.887 ± 0.057
0.894 ± 0.076 radformer [4] 0.921 ± 0.062 0.961 ± 0.049 0.923 ± 0.062 additional
data/ annotation uscl [7] 0.889 ± 0.047 0.895 ± 0.054 0.869 ± 0.097 us-ucl [5]
0.920 ± 0.034 0.926 ± 0.043 0.900 ± 0.046 gbcnet [3] 0.921 ± 0.029 0.967 ± 0.023
0.919 ± 0.063 point-beyond-class [18] ts-cam [10] 0.704 ± 0.017 0.394 ± 0.042
0.891 ± 0.054 scm [2] 0.751 ± 0.026 0.523 ± 0.014 0.523 ± 0.016 od-wscl [21]
0.805 ± 0.056 0.609 ± 0.076 0.923 ± 0.034 ws-detr [19] 0.857 ± 0.071 0.812 ±
0.088 0.882 ± 0.034 point-beyond-class [18] 0.953 ± 0.007 0.993 ± 0.004 0.924 ±
0.011 ours 0.878 ± 0.067 0.785 ± 0.102 0.932 ± 0.022 ing box annotation [3] or,
us videos [5,7]."
384,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"however, even without these additional annotations/ data, our
method reaches 86.1% detection sensitivity."
385,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"current sota models for gbc detection require costly
bounding box annotation of the pathological regions, or additional us video
data, which limit their applicability."
386,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"our experiments show that
the approach achieves competitive performance without requiring additional
annotation or data."
387,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"we hope that our technique will simplify the model training
at the hospitals with easily available data locally, enhancing the applicability
and impact of automated gbc detection."
388,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"extensive experiments on three publicly available datasets show the potential of
such models for the processing of gigapixel-sized images, under both weakly and
multi-task schemes."
389,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"camelyon16 [16] is a dataset that consists of resections of lymph nodes, where
each wsi is annotated with a binary label indicating the presence of tumour
tissue in the slide, and all slides containing tumors have a pixel-level
annotation indicating the metastatic region."
390,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"in our experiments, the average patch sequence length
arising from camelyon16 is 6129 (ranging from 127 to 27444).tcga-luad is a tcga
lung adenocarcinoma dataset that contains 541 wsis along with genetic
information about each patient."
391,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"the average sequence length is 10557 (ranging from 85 to
34560).tcga-rcc is a tcga dataset for three kidney cancer subtypes (denoted
kich, kirc, and kirp)."
392,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"we evaluate our method on each dataset by
accuracy and area under receiver operating characteristic curve (auroc)."
393,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"for the
camelyon16 dataset, our method performs on par with trans-mil and the clam
models, while it clearly outperforms the other methods."
394,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"similarly, in the
tcga-luad dataset the proposed model achieves comparable performance with both
clam models, while outperforming transmil and the other methods."
395,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"we note that
tcga-luad proves to be a more challenging dataset for all models."
396,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"moreover, our
method outperforms clam models on the tcga-rcc dataset, while reporting very
similar performance with respect to transmil."
397,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"overall, looking at the average
metrics per model across all three datasets, our proposed method achieves the
highest accuracy and the second highest auroc, only behind clam-mb."
398,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"models a and b show that stacking multiple ssm layers results in
lower accuracy, which was observed over all three datasets, while models c and d
show that modifying the state dimension of the ssm module can have an impact on
the accuracy."
399,Structured State Space Models for Multiple Instance Learning in Digital Pathology,the optimal state space dimension varies depending on the dataset.
400,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"we explored the ability of our model to combine slide-and patch-level
information on the cameylon16 dataset."
401,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"in order to highlight the inherent
ability of ssm models to effectively model long sequences, we performed an
experiment on only the largest wsis of the tcga-rcc dataset."
402,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"indeed, this
dataset contains particularly long sequences (up to 62235 patches at 20x)."
403,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"finally, we demonstrated
that on the longest sequences in our datasets, state space models offer better
performance than competing models, confirming their power in modeling long-range
dependencies."
404,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"this information can be leveraged to assess treatment response, e.g., by
analyzing the evolution of size and morphology for a given tumor [1], but also
for adaptation of (re-)treatment radiotherapy plans that take into account new
tumors.in practice, the development of automatic and reliable lesion tracking
solutions is hindered by the complexity of the data (over different modalities),
the absence of large, annotated datasets, and the difficulties associated with
lesion identification (i.e., varying sizes, poses, shapes, and sparsely
distributed locations)."
405,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"inspired by the pixel-wise contrastive
learning strategy introduced in [5], we choose to learn pixel-wise feature
representations that embed consistent anatomical information from unlabeled
(i.e., without lesion-related annotations) and unpaired (i.e., without the use
of longitudinal scans) data, overcoming barriers to data collection."
406,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"the
reasoning behind this strategy is that simple data augmentation methods cannot
faithfully model inter-subject variability or possible organ deformations."
407,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"furthermore, a significant focus and contribution of our research is
the experimental study at a very large scale: we (1) train a pixel-wise
self-supervised system using a very large and diverse dataset of 52,487 ct
volumes and (2) evaluate on two publicly available datasets."
408,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"notably, one of the
datasets, nlst, presents challenging cases with 68% of lesions being very small
(i.e., radius < 5 mm)."
409,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"the problem of lesion tracking in longitudinal data is typically divided into
two steps: (1) detection of lesions and (2) tracking the same lesion over
multiple time points."
410,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"training exclusively on augmented paired data prevents sam from
accurately representing anatomical changes and deformations that occur over
time."
411,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"1, given an image x ∈ r d×h×w from the training dataset
d, we randomly select two overlapping 3d patches (anchor and query), namely x a
and x q ."
412,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"to create synthetic paired data that mimics appearance changes across
different images, we apply random data augmentation (i.e., random spatial and
intensity-related transformations) to the content of x a and x q ."
413,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"we use data-driven models [14] to extract 37
anatomical landmarks, such as the top right lung, suprasternal notch, tracheal
bifurcation, etc."
414,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"datasets: we train the universal and fine-grained anatomical point matching
model using an in-house ct dataset (variousct)."
415,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"the training dataset contains
52,487 unlabeled 3d ct volumes capturing various anatomies, including chest,
head, abdomen, pelvis, and more.the evaluation is based on two datasets, the
publicly released deep longitudinal study (dls) dataset [8] and the national
lung screening trial (nlst) dataset [12]."
416,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"the dls dataset is a subset of the
deeplesion [11] medical imaging dataset, containing 3891 pairs of lesions with
information on their location and size."
417,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"the dataset covers various types of
lesions across different organs."
418,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"we follow the official data split for dls
dataset and perform evaluation on the testing dataset which comprises 480 lesion
pairs."
419,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"a certified radiologist annotated the testing
data by identifying the location and size of the pulmonary nodules, resulting in
a total of 825 paired annotations."
420,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"the isotropic resolution of all ct volumes is adjusted to
2mm through bilinear interpolation.system training: our learning model is
implemented in pytorch and uses the torchio library [13] for medical data
manipulation and augmentation.we employ a u-net-based encoder-decoder
architecture [2] that utilizes an inflated 3d resnet-18 [3,4] as its encoder,
which extends all 2d convolutions table 1."
421,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"for data augmentation, we apply random cropping, scaling, rotation, and gaussian
noise injections."
422,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"the nlst
testing dataset has a distinctive feature wherein nodules are relatively small,
68% of annotated lesions have a radius of less than 5 mm (compared to 6% in dls
dataset)."
423,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"for the lesion tracking task on dls dataset, we quantitatively compare our
system against existing trackers in table 1."
424,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"hence, for
performance comparison against self-supervised anatomical embedding tracker, we
retrain sam [5] with images from variousct dataset."
425,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"when imposing a maximum distance limit of 10 mm between the ground truth
and prediction, our method increases performance by 1.46%, showing the
importance of the multi-scale approach in lesion on the nlst dataset, our
proposed method obtains a center point matching accuracy of 92.12% (table 2)."
426,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"the method is generic,
it does not require expert annotations or longitudinal data for training and can
generalize to different types of tumors/organs/modalities."
427,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"through large-scale experiments and validation on two longitudinal datasets, we
highlight the superiority of the proposed method in comparison to
state-of-theart."
428,Geometry-Invariant Abnormality Detection,"most recent approaches
have focused on improvements in performance rather than flexibility, thus
limiting approaches to specific input types -little research has been carried
out to generate models unhindered by variations in data geometries."
429,Geometry-Invariant Abnormality Detection,"often,
research assumes certain similarities in data acquisition parameters, from image
dimensions to voxel dimensions and fields-of-view (fov)."
430,Geometry-Invariant Abnormality Detection,"this strong assumption can often
be complex to maintain in the real-world and although image pre-processing steps
can mitigate some of this complexity, test error often largely increases as new
data variations arise."
431,Geometry-Invariant Abnormality Detection,"usually
training data, especially when acquired from differing sources, undergoes
significant preprocessing such that data showcases the same fov and has the same
input dimensions, e.g."
432,Geometry-Invariant Abnormality Detection,by registering data to a population atlas.
433,Geometry-Invariant Abnormality Detection,"given this, the task of generating an anomaly detection model that
works on inputs with a varying resolution, dimension and fov is a topic of
importance and the main focus of this research.unsupervised methods have become
an increasingly prominent field for automatic anomaly detection by eliminating
the necessity of acquiring accurately labelled data [4,7] therefore relaxing the
stringent data requirements of medical imaging."
434,Geometry-Invariant Abnormality Detection,"this approach consists of
training generative models on healthy data, and defining anomalies as deviations
from the defined model of normality during inference."
435,Geometry-Invariant Abnormality Detection,"in [22], the authors explore the advantage of tractably
maximizing the likelihood of the normal data to model the long-range
dependencies of the training data."
436,Geometry-Invariant Abnormality Detection,"the work in [21] takes this method a step
further through multiple samplings from the transformer to generate a
non-parametric kernel density estimation (kde) anomaly map.even though these
methods are state-of-the-art, they have stringent data requirements, such as
having a consistent geometry of the input data, e.g., in a whole-body imaging
scenario, it is not possible to crop a region of interest and feed it to the
algorithm, as this cropped region will be wrongly detected as an anomaly."
437,Geometry-Invariant Abnormality Detection,"through adapting the vq-vae transformer
approach in [21], we showcase that we can train our model on data with varying
fields of view, orientations and resolutions by adding spatial conditioning in
both the vq-vae and transformer."
438,Geometry-Invariant Abnormality Detection,"furthermore, we show that the performance of
our model with spatial conditioning is at least equivalent to, and sometimes
better, than a model trained on whole-body data in all testing scenarios, with
the added flexibility of a ""one model fits all data"" approach."
439,Geometry-Invariant Abnormality Detection,"the vq-vae model provides a data-efficient encoding mechanism-enabling 3d inputs
at their original resolution-while generating a discrete latent representation
that can trivially be learned by a transformer network [20]."
440,Geometry-Invariant Abnormality Detection,"to model the imaging data, we require the discretized latent space z iq
to take the form of a 1d sequence s, which we achieve via a raster scan of the
latent."
441,Geometry-Invariant Abnormality Detection,"as with [21], we additionally
use ct data to condition the transformer via cross-attention using a separate
vq-vae to encode the ct."
442,Geometry-Invariant Abnormality Detection,"similarly the embedding dimension for the ct data and the
spatial conditioning data had an embedding dimension of 256."
443,Geometry-Invariant Abnormality Detection,"we can then reshape the ""healed"" sequence back into its 3d
quantized representation to feed into the vq-vae to generate a healed
reconstruction x r without anomalies.in this work, abnormalities are defined as
deviations between the distribution of ""healed"" reconstructions and the observed
data, measured using a kernel density estimation (kde) approach."
444,Geometry-Invariant Abnormality Detection,"although fully convolutional models can
ingest images of varying dimensions, we have found that using training data with
varying resolutions resulted in poor auto-encoder reconstructions."
445,Geometry-Invariant Abnormality Detection,"we found when
training the vq-vae model on data with varying resolutions and dimensions that
reconstructions showcased unwanted and significant artifacts, while by adding
the coordconv channels this issue was not present (see appendix c for examples)."
446,Geometry-Invariant Abnormality Detection,"note that while the proposed
model assumes only translation and scale changes between samples, it can be
trivially extended to a full affine mapping of the coordinate system (including
rotations/shearing between samples).we used random crops during training to
simulate varying fovs of wholebody data."
447,Geometry-Invariant Abnormality Detection,"for this work we leveraged whole-body pet/ct data from different sources to
explore the efficacy of our approach for varying image geometries."
448,Geometry-Invariant Abnormality Detection,"211 scans
from nsclc radiogenomics [2,3,10,16] combined with 83 scans from a proprietary
dataset constitute our lower resolution dataset with voxel dimensions of 3.6 ×
3.6×3 mm."
449,Geometry-Invariant Abnormality Detection,"from this, we split the data to give 210 training samples, 34
validation and 50 testing."
450,Geometry-Invariant Abnormality Detection,"our higher resolution dataset uses autopet [10,15]
(1014 scans) with voxel dimensions of 2.036 × 2.036 × 3 mm."
451,Geometry-Invariant Abnormality Detection,"from this, 850 scans
are used for training, 64 for validation and 100 for testing.all baseline models
work in a single space with constant dimensions, obtained by registering the
autopet images to the space of the nsclc dataset.for evaluation, we use four
testing sets: a lower resolution set derived from both the nsclc and the private
dataset; a higher resolution set from autopet; a testing set with random crops
of the same nsclc/private testing dataset and finally a testing set that has
been rotated through 90 • using the high resolution testing data."
452,Geometry-Invariant Abnormality Detection,"as the cropped
and rotated dataset cannot be fed into the baseline models, we pad the images to
the common image sizing before inference."
453,Geometry-Invariant Abnormality Detection,the proposed model was trained on the data described in sect.
454,Geometry-Invariant Abnormality Detection,"we additionally showcase the performance of the classic vq-vae +
transformer approach trained on whole-body data only (without the proposed
spatial conditioning), as well as the proposed coordconv model trained with
varying image geometries but without the transformer spatial conditioning to
explicitly showcase the added contribution of both spatial conditionings."
455,Geometry-Invariant Abnormality Detection,"we
can observe that the addition of spatial conditioning improves performance even
against the same model without conditioning trained on whole-body data (mann
whitney u test, p < 0.01 on high resolution and p < 0.001 on cropped data for
dice and auprc)."
456,Geometry-Invariant Abnormality Detection,"for cropped data, models trained on whole-body data fail around
cropping borders, as showcased in fig."
457,Geometry-Invariant Abnormality Detection,"however, by adding the transformer spatial conditioning,
we see improvements across all test sets (most significantly on cropped data and
the rotated data p < 0.001) for both evaluation metrics."
458,Geometry-Invariant Abnormality Detection,"for the rotated data,
we see little performance degradation in the conditioned model thanks to the
spatial conditioning."
459,Geometry-Invariant Abnormality Detection,"generally, the variation scanners and acquisition protocols can cause failures
in models trained on data from single sources."
460,Geometry-Invariant Abnormality Detection,"not only
does the proposed model showcase strong and statistically-significant
performance improvements on varying image resolutions and fov, but also on
whole-body data."
461,Geometry-Invariant Abnormality Detection,"through this, we demonstrate that one can improve the
adaptability and flexibility to varying data geometries while also improving
performance."
462,Geometry-Invariant Abnormality Detection,"such flexibility also increases the pool of potential training
data, as they dont require the same fov."
463,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"besides using the
additional knowledge to improve performance, it can also help to increase
explainability, as has already been shown using the lidc-idri dataset [3]."
464,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"in addition to the enhanced
explainability offered by the proposed approach, to our knowledge the proposed
method outperforms existing studies on the lidc-idri dataset.the main
contributions of our work are:-a novel method that, for the first time to our
knowledge, combines privileged information and prototype learning to provide
increased explanatory power for medical classification tasks."
465,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"-an explainable solution outperforming state-of-the-art
explainable and nonexplainable methods on the lidc-idri dataset.we provide the
code with the model architecture and training algorithm of proto-caps on github."
466,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"randomly initialized, the prototypes are a
representative subset of the training dataset for each attribute after the
training."
467,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"during inference, the predicted attribute value is set to the
ground truth attribute value of the closest prototype, ignoring the learned
dense layers in the attribute head at this stage.the overall loss function is
the following weighted sum, where λ recon = 0.512 was chosen according to [11],
and the prototype weights were chosen empirically: data."
468,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"the proposed approach is evaluated using the publicly available lidc-idri
dataset consisting of 1018 clinical thoracic ct scans from patients with
non-small cell lung cancer (nsclc) [2,3]."
469,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"the
pylidc framework [7] is used to access and process the data."
470,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"five-fold stratified cross-validation was performed using 10 % of the training
data for validation and the best run of three is reported.the algorithm was
implemented using the pytorch framework version 1.13 and cuda version 11.6."
471,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"our data reduction studies show that
the proposed solution is robust to the number of annotated examples, and good
results are obtained even with a 90% reduction in privileged information."
472,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"this
opens the door for application to other datasets by reducing the additional
annotation overhead."
473,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"while recent advances
in datadriven deep learning (dl) have achieved superior segmentation performance
[29], the segmentation task is often constrained by the availability of costly
pixel-wise labeled training datasets."
474,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"in addition, even if static dl models are
trained with extraordinarily large amounts of training datasets in a supervised
learning manner [29], there exists a need for a segmentor to update a trained
model with new data alongside incremental anatomical structures [24].in
real-world scenarios, clinical databases are often sequentially constructed from
various clinical sites with varying imaging protocols [19][20][21]23]."
475,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"furthermore, access to previously used data for training can be
restricted, due to data privacy protocols [17,18]."
476,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"therefore, efficiently
utilizing heterogeneous structure-incremental (hsi) learning is highly desired
for clinical practice to develop a dl model that can be generalized well for
different types of input data and varying structures involved."
477,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"straightforwardly
fine-tuning dl models with either new structures [30] or heterogeneous data [17]
in the absence of the data used for the initial model training, unfortunately,
can easily overwrite previously learned knowledge, i.e., catastrophic forgetting
[14,17,30].at present, satisfactory methods applied in the realistic hsi setting
are largely unavailable."
478,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"early attempts [27] simply used exemplar data in the previous
stage."
479,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"the widely used pooled feature statistics
consistency [5,30] is also not applicable for heterogeneous data, since the
statistics are domain-specific [2]."
480,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"[17] further
proposed to recover the missing old stage data with an additional generative
model, hallucinating realistic data, given only the trained model itself, is a
highly challenging task [31] and may lead to sensitive information leakage [35]."
481,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"specifically, inspired by the self-knowledge distillation [15] and self-training
[38] that utilize the previous prediction for better generalization, we
adaptively construct the hsi pseudo-label with an mmd scheme to smoothly adjust
the contribution of potential noisy old model predictions on heterogeneous data
and progressively learned new model predictions along with the training."
482,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"• the adaptively constructed hsi pseudo-label with
self-training is developed for efficient hsi knowledge distillation.we evaluated
our framework on anatomical structure segmentation tasks from different types of
mri data collected from multiple sites."
483,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"our hsi scheme demonstrated superior
performance in segmenting all structures with diverse data distributions,
surpassing conventional class-incremental methods without considering data
shift, by a large margin."
484,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"for the segmentation model under incremental structures of interest and domain
shift scenarios, we are given an off-the-shelf segmentor f θ 0 : x 0 → y 0
parameterized with θ 0 , which has been trained with the data {x 0 n , y 0 n } n
0 n=1 in an initial source domain d 0 = {x 0 , y 0 }, where x 0 n ∈ r h×w and y
0 n ∈ r h×w are the paired image slice and its segmentation mask with the height
of h and width of w , respectively."
485,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"to alleviate the forgetting through parameter overwriting, caused by both new
structures and data shift, we propose a d 3 f module for flexible decoupling and
integration of old and new knowledge."
486,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"therefore, we propose a
continual batch renormalization (cbrn) to mitigate the feature statistics
divergence between each training batch at a specific stage and the life-long
global data distribution.of note, as a default block in the modern convolutional
neural networks (cnn) [8,37], batch normalization (bn) [11] normalizes the input
feature of each cnn channel z ∈ r hc×wc with its batch-wise statistics, e.g.,
mean μ b and standard deviation σ b , and learnable scaling and shifting factors
{γ, β} as zi = zi-μb σb • γ + β, where i indexes the spatial position in r hc×wc
."
487,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"simply enforcing the same statistics across domains
as [5,30,33] can weaken the model expressiveness [36].the recent brn [10]
proposes to rectify the data shift between each batch and the dataset by using
the moving average μ and σ along with the training:where η ∈ [0, 1] is applied
to balance the global statistics and the current batch.in addition, γ = σb σ and
β = μb -μ σ are used in both training and testing."
488,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"the training of our developed f θ t with d 3 f is supervised with the previous
model f θ t-1 and current stage data {x t n , s t n } n t n=1 ."
489,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"however, with heterogeneous data in different stages, f θ t-1
(x t n ) can be highly unreliable."
490,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"therefore, the weight of old model
prediction can be smoothly decreased along with the training, and f θ t (x t n:i
) gradually represents the target data for the old classes in [: t -1]."
491,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"we incrementally learned coret, enht, and ed structures
throughout three consecutive stages, each following different data
distributions."
492,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"we
compared our framework with the three typical structureincremental (si-only)
segmentation methods, e.g., plop [5], margexcil [18], and ucd [30], which cannot
address the heterogeneous data across stages."
493,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"for the anatomical
structure coret learned in t = 0, the difference between our hsi and these
si-only methods was larger than 10% dsc, which indicates the data shift related
forgetting lead to a more severe performance drop in the early stages."
494,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"hsi-cbrn used dual-flow to
avoid direct overwriting, while the model was not guided by cbrn for more
generalized prediction on heterogeneous data."
495,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"notably, the dual-flow model with
flexible re-parameterization was able to alleviate the overwriting, while our
cbrn was developed to deal with heterogeneous data."
496,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"the improvement can be even
larger, compared with the cross-subset task, since we have much more diverse
input data in the cross-modality setting."
497,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"to alleviate the catastrophic
forgetting alongside continuously varying structures and data shifts, our hsi
resorted to a d 3 f module for learning and integrating old and new knowledge
nimbly."
498,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"in doing so, we were able to achieve divergence awareness with our
cbrn-guided model adaptation for all the data involved."
499,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"our framework was
optimized with a self-entropy regularized hsi pseudo-label distillation scheme
with mmd to efficiently utilize the previous model in different types of mri
data."
500,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"our framework demonstrated superior segmentation performance in learning
new anatomical structures from cross-subset/modality mri data."
501,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"however, the scarcity of annotated data due to high
manual annotation costs results in poorly trained and low generalizable models."
502,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"previous methods have relied on generative adversarial networks (gans) [9,25] or
data augmentation methods [3,13,28] to enhance learning features, but these
methods yielded limited improvements in downstream tasks."
503,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"recently, diffusion
models [6,15] have emerged as promising solutions to this problem, demonstrating
remarkable progress in generating multiple modalities of medical data
[4,10,12,21]."
504,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"in addition, in order to generate
high-quality annotated samples, it is crucial to maintain the consistency
between the polyp morphologies in synthesized images and the original masks,
which current generative models struggle to achieve.to tackle these issues and
inspired by the remarkable success achieved by diffusion models in generating
high-quality ct or mri data [8,11,23], we creatively propose an effective
adaptive refinement semantic diffusion model (arsdm) to generate polyp-contained
colonoscopy images while preserving the original annotations."
505,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"the pipeline of
the data generation and downstream task training is shown in fig."
506,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"(2) large-scale colonoscopy
generation: the proposed approach can be used to generate large-scale datasets
with no/arbitrary annotations, which significantly benefits the medical image
society, laying the foundation for large-scale pre-training models in automatic
colonoscopy analysis."
507,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"the forward
process is a markov chain that gradually adds gaussian noise to the original
data."
508,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"this process can be formulated as the joint distribution q (x 1:t | x 0
):where q (x 0 ) is the original data distribution with x 0 ∼ q (x 0 ), x 1:t
are latents with the same dimension of x 0 and β t is a variance schedule.the
reverse process is aiming to learn a model to reverse the forward process that
reconstructs the original input data, which is defined as:(2) where p (x t ) is
the noised gaussian transition from the forward process at timestep t ."
509,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"through extensive
experiments, we found inaccurate sample images with coarse polyp boundary that
is not aligned properly with the original masks may introduce large biases and
noises to the datasets."
510,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"the detailed procedure of one training iteration is shown in algorithm
1 and the overall loss function is defined as:3 experiments we conducted our
experiments on five public polyp segmentation datasets:
endoscene [20], cvc-clincdb/cvc-612 [1], cvc-colondb [18], etis [14] and kvasir
[7]."
511,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"the evaluations are conducted on the
five datasets separately to verify the learning and generalization capability."
512,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"specifically, data generated by our approach
assists the significant improvements for each model in mdice and miou, with
increases of 6.0% and 5.7% over pranet, 2.1% and 2.7% over sanet, and 0.7% and
0.7% over polyp-pvt."
513,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"we also observe superior ap and f1-scores compared to
centernet, sparse-rcnn, and deformable-detr trained with original data, with
gains of 9.1% and 5.3%, 2.7% and 5.8%, and 3.4% and 6.1%, respectively."
514,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"moreover, we conducted a comprehensive comparison with sota models, noting that
these models were not specifically designed for colonoscopy images and may
generate data that hinder the training process or lack the ability for effective
improvement."
515,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"automatic generation of annotated data is essential for colonoscopy image
analysis, where the scale of existing datasets is limited by the expertise and
time required for manual annotation."
516,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"to evaluate our
approach comprehensively, we conduct polyp segmentation and detection
experiments on five widely used datasets, where experimental results demonstrate
the effectiveness of our approach, in which model performances are greatly
enhanced with little synthesized data."
517,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"large-scale well-annotated
datasets are one of the key components for training deep learning models to
achieve satisfactory results [3,17]."
518,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"to overcome this challenge, a
natural choice is to employ data augmentation to increase the number of training
samples."
519,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"although conventional augmentation techniques [23] such as flipping and
cropping can be directly applied to medical images, they merely improve the
diversity of datasets, thus leading to marginal performance gains [1]."
520,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"another
group of studies employ conditional generative adversarial networks (cgans) [10]
to synthesize visually appealing medical images that closely resemble those in
the original datasets [36,37]."
521,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"while existing works have proven effective in
improving the performance of downstream models to some extent, a sufficient
amount of labeled data is still required to adequately train models to generate
decentquality images."
522,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"a few studies have also demonstrated the potential of diffusion
models for medical image synthesis [19,24].although annotated data is typically
hard to acquire for medical images, unannotated data is often more accessible."
523,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"to mitigate the issue existed in current cgan-based synthetic augmentation
methods [8,[36][37][38], in this work, we propose to leverage the diffusion
model with unlabeled pre-training to reduce the dependency on the amount of
labeled data (see comparisons in fig."
524,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"we propose a novel synthetic
augmentation method, named histodiffusion, which can be pre-trained on
large-scale unannotated datasets and adapted to smallscale annotated datasets
for augmented training."
525,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"specifically, we first employ a latent diffusion model
(ldm) and train it on a collection of unlabeled datasets from multiple sources."
526,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"second, given a
small labeled dataset that does not exist in the pre-training datasets, the
decoder of the ldm is fine-tuned using annotations to adapt to the domain shift."
527,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"we
evaluate our proposed method on a histopathology image dataset of colorectal
cancer (crc)."
528,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"our experimental results show
that once histodiffusion is well pre-trained using large datasets, it can be
applied to any future incoming small dataset with minimal fine-tuning and may
substantially improve the flexibility and efficacy of synthetic augmentation."
529,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"first, we
train an ldm on a large-scale set of unlabeled datasets collected from multiple
sources."
530,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"we then fine-tune the decoder of this pretrained ldm on a small labeled
dataset."
531,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"to enable conditional image synthesis, we also train a latent
classifier on the same labeled dataset to guide the diffusion model in ldm."
532,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"finally, we can train downstream classification models on the expanded
training data, which includes the selected images, and then use them to perform
inference on test data."
533,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"diffusion models (dm) [13,30,32] are probabilistic models that are designed to
learn a data distribution."
534,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"given a sample from the data distribution z 0 ∼ q(z 0
), the dm forward process produces a markov chain z 1 , ."
535,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"to ensure the
latent space z can cover features of various data types, we first pre-train our
proposed his-todiffusion on large-scale unlabeled datasets."
536,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"specifically, we
gather unlabeled images from m different sources to construct a large-scale set
of datasets s = {s 1 , s 2 , ."
537,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"we then train an lae using the data
from s with the following self-reconstruction loss to learn a powerful latent
space z that can describe diverse features: (4) where l rec is the loss
measuring the difference between the output reconstructed image x and the input
ground truth image x."
538,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"once the dm is trained, we can use denoising model θ in the dm reverse
sampling process to synthesize a novel latent z0 ∈ r h×w×c and employ the
trained decoder d to generate a new image x = d(z 0 ), which should satisfy the
similar distribution as the data in s.conditional small-scale fine-tuning."
539,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"to generalize our histodiffusion to the
small-scale labeled dataset s collected from a different source (i.e., s ⊂ s),
we further fine-tune histodiffusion using the labeled data from s ."
540,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"then we only
fine-tune the decoder d using labeled data (x, y) from s with the following loss
function: ld = lrec(x, x) + λcelce(ϕ(x), y) , (5) where l rec (x, x) is the
self-reconstruction loss between the output reconstructed image x = d(e(x)) and
the input ground truth image x."
541,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"to enable conditional image generation with our histodiffusion, we further apply
the classifier-guided diffusion sampling proposed in [4,29,30,33] using the
labeled data (x, y) from small-scale labeled dataset s ."
542,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"we first utilize the
trained encoder e to encode the data x from s as latent z 0 ."
543,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"to further improve the efficacy of synthetic augmentation, we
follow [36] to selectively add synthetic images to the original labeled training
data based on centroid feature distance."
544,Synthetic Augmentation with Large-Scale Unconditional Pre-training,datasets.
545,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"we employ three public datasets of histopathology images during the
large-scale pre-training procedure."
546,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"the first one is the h&e breast cancer
dataset [2], containing 312,320 patches extracted from the hematoxylin & eosin
(h&e) stained human breast cancer tissue micro-array (tma) images [18]."
547,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"the second dataset is pannuke [9], a
pan-cancer histology dataset for nuclei instance segmentation and
classification."
548,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"the pannuke dataset includes 7,901 patches of 19 types of h&e
stained tissues obtained from multiple data sources, and each patch has a
unified size of 256×256 pixels."
549,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"the third dataset is tcga-brca-a2/e2 [34], a
subset derived from the tcga-brca breast cancer histology dataset [20]."
550,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"as for fine-tuning and
evaluation, we employ the nct-crc-he-100k dataset that contains 100,000 patches
from h&e stained histological images of human colorectal cancer (crc) and normal
tissue."
551,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"the resolution of each patch is 224 × 224.to
replicate a scenario where only a small annotated dataset is available for
training, we have opted to utilize a subset of 5,000 (5%) samples for
finetuning."
552,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"it is worth
noting that the labels for these samples have been kept, which allows the
fine-tuning process to be guided by labeled data, leading to better predictions
on the specific task or domain being trained."
553,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"by ensuring that the fine-tuning
process is representative of the entire dataset through even sampling from each
tissue type, we can eliminate bias towards any particular tissue type."
554,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"the related data use
declaration and acknowledgment can be found in our supplementary
materials.evaluation metrics."
555,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"when augmenting the training
dataset with different numbers of images synthesized from histodiffusion and
stylegan2, one can observe that when increasing the ratio of synthesized data to
100%, the fid score of stylegan2 increases quickly and can become even worse
than the one without using image selection strategy."
556,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"histodiffusion leverages multiple unlabeled datasets for large-scale,
unconditional pre-training, while employing a labeled dataset for small-scale
conditional fine-tuning."
557,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"experiment results on a histopathology image dataset
excluded from the pre-training demonstrate that given limited labels,
histodiffusion with image selection remarkably enhances the classification
performance of the baseline model, and can potentially handle any future
incoming small dataset for augmented training using the same pre-trained model."
558,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"a must-have ingredient for training a deep neural network (dnn) is a large
number of labelled data that is not always available in real-world applications."
559,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"this challenge of data annotation becomes even worse for medical image
segmentation tasks that require pixel-level annotation by experts."
560,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"data
augmentation (da) is a recognized approach to tackle this challenge."
561,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"common da
strategies create new samples by using predefined transformations such as
rotation, translation, and colour jitter to existing data, where the performance
gains heavily relies on the choice of augmentation operations and parameters
[1].to mitigate this reliance, recent efforts have focused on learning optimal
augmentation operations for a given task and dataset [3,8,11,15]."
562,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"it thus
provides an excellent candidate for learning shape variations of an object from
the data, and via which to enable shape-based augmentations for medical image
segmentation tasks."
563,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"it thus will be challenging to transfer the
learned shape variations to even the same objects across different locations,
orientations, or sizes in the image, let alone transferring across dataset
(e.g., to transfer the learned shape variations of an organ from one image
modality to another).intuitively, object-centric transformations and
augmentations have the potential to overcome the challenges associated with
global image-level transformations."
564,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"this allows us
to add shape diversity to the objects of interest in an image regardless of
their positions or sizes, eventually facilitating transferring the learned
variations across datasets."
565,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"we demonstrated the effectiveness of the presented
object-centric diffeomorphic augmentation in kidney tumour segmentation,
including using shape variations of kidney tumours learned from the same dataset
(kits [7]), as well as transferring those learned from a larger liver tumour
dataset (lits [2])."
566,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"experimental results showed that it can enrich the
augmentation diversity of other techniques such as tumorcp [13], and improve
kidney tumour segmentation [7] using shape variations learned either within or
outside the same training data."
567,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"we also experimented with squaring and scaling layers for
integration but that resulted in texture loss when learning
transformations.generative modeling: the data generation process can be
described as:where z is the latent variable assumed to follow an isotropic
gaussian prior, p φ (θ|z) is modeled by a neural network parameterized by φ, and
p(x tgt |θ, x src ) follows the deformable transformation as described in
equation ( 1).we define variational approximations of the posterior density as q
ψ (z|x src , x tgt ), modeled by a convolutional neural network that expects two
inputs x src and x tgt ."
568,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"we used two publicly available datasets, lits [2] and kits [7], for our
experiments."
569,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"data: we then used g(z) to generate deformation-based augmentations to increase
the size and diversity of training samples for kidney tumour segmentation on
kits."
570,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"to assess the effect of augmentations on different sizes of labelled data,
we considered training using 25%, 50%, 75%, and 100% of the kits training set."
571,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"we considered two da scenarios: augment with transformations learned from kits
(within-data augmentation) versus from lits (cross-data augmentation).models:
for the base segmentation network, we adopted nnu-net [9] as it contains state
of the art (sota) pipeline for medical image segmentation on most datasets."
572,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"interestingly, cross-data transferring
of the learned augmentations (from lits) outperformed the within-data
augmentation in the majority of the cases."
573,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"firstly, learning of the within-data augmentations is limited to the
percentage of the training set used for segmentation."
574,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"secondly, the transformations present in cross-data are completely unseen in the
segmentation training network which helps in generating more diverse samples."
575,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"note that, as the transformations are learned as variations in object shapes,
they can be transferred easily across datasets surprisingly, the improvements
achieved by the presented augmentation strategy were the most prominent when the
segmentation was trained on 50% and 75% of the kits training set."
576,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"as demonstrated by the
experimental results, this allows us to not only introduce new variations to
unfixed objects like tumours in an image but also transfer the knowledge of
shape variations across datasets."
577,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"in the long term, it would be interesting to explore ways to
transfer knowledge about more general forms of variations across datasets."
578,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"however,
such an approach that scales across longitudinal multimodal data from
comprehensive representations of the clinical routine has yet to be demonstrated
[24]."
579,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"previous studies overcome these
challenges by aggregating over visits and binning time series within a
bidirectional encoder representations from transformers (bert) architecture
[2,14,20,25], limiting their scope to data collected on similar time scales,
such as icu measurements, [11,29], or leveraging graph guided transformers to
handle asynchrony [33]."
580,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"self-attention [31] has become the dominant technique
for learning powerful representations of ehrs with trade-offs in
interpretability and quadratic scaling with the number of visits or bins, which
can be inefficient with data spanning multiple years."
581,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"compared with imaging-only and a baseline that aggregates
longitudinal data into bins, our approach allowed us to incorporate additional
modalities from routinely collected ehrs, which led to improved spn
classification."
582,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"we used smooth interpolation for
continuous variables [4] or a continuous estimate of event density per time for
event data."
583,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"formally, we have dataset d ehr-pulmonary = {l k | k = 1, ."
584,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"we
represent our multimodal datasets d image-ehr and }, where t is the maximum
sequence length."
585,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"following [5,19,32], we intuit that if
medical data is sampled as a cross-sectional manifestation of a continuously
progressing phenotype, we can use a temporal emphasis model (tem) emphasize the
importance of recent observations over older ones."
586,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,datasets.
587,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"next, ehr-pulmonary was the
unlabeled dataset used to learn clinical signatures in an unsupervised manner."
588,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"additionally, image-ehr was a labeled dataset
with paired imaging and ehrs."
589,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"we labeled malignant cases as those with a lung
malignancy billing code occurring within three years after any scan and only
used data collected before the lung malignancy code."
590,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"all data within the
five-year period were used for controls."
591,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"cross-sectional embedded billing codes did not
significantly improve performance over images alone (cscode2vec vs csimage, p =
0.56), but adding clinical signatures did (cssig vs csimage, p < 0.01; tdsig vs
tdimage, p < 0.01) and the greatest improvement in longitudinal data over single
cross sections occurred when clinical signatures were included."
592,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"in this setting, we found that adding clinical context
increased the performance gap between longitudinal data and single
cross-sections."
593,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"we release our implementation at
https://github.com/masilab/lmsignatures.the lack of longitudinal multimodal
datasets has long been a limiting factor [24] in conducting studies such as
ours."
594,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"one of our contributions is demonstrating training strategies in a
small-dataset, incomplete-data regime."
595,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"we were able to overcome our small cohort
size (image-ehr-spn) by leveraging unsupervised learning on datasets without
imaging (ehr-pulmonary), pretraining on public datasets without ehrs (nlst), and
pretraining on paired multimodal data with noisy labels (image-ehr) within a
flexible transformer architecture.our approach of sampling cross-sections where
clinical decisions are likely to be made scales well with long, multi-year
observation windows, which may not be true for bert-based embeddings [20,25]."
596,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"however, they typically require a
large amount of expert-level accurate, densely-annotated data for training,
which is laborious and time consuming to collect."
597,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"therefore, existing fully
labeled datasets (termed as flds) are very few and often low in sample size [1]."
598,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"while there exist many publicly available partially labeled datasets (plds)
[2,3], each with one or a few out of the many organs annotated."
599,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"this has
motivated the development of various partially-supervised multi-organ
segmentation (psmos) methods that aim to learn a unified model from a union of
such datasets."
600,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"although consistency learning is frequently
used for leveraging unlabeled data in label-efficient learning [10][11][12], it
is mostly deployed in the label space [13][14][15], while little attention has
been paid to exploring consistency in the latent feature space."
601,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"by incorporating voxel-to-organ affinity in the
embedding space into consistency learning, our acl scheme is plug-and-play and
can capture rich context information in the embedding space.to tackle the data
discrepancy problem [17], based on the assumption that a well trained joint
model should generate consistent feature distributions across different sites,
we propose a novel cross-site feature alignment (csfa) module, where two terms
are introduced to attend to both the organ-specific and interorgan statistics in
the latent feature space."
602,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"to further reduce the
data discrepancy problem, we constrain the affinity relationships across
different organ-specific prototypes to be consistent among different sites."
603,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"-we design a novel cross site feature alignment module to
calibrate feature distributions of plds with distribution priors learned from a
small-sized fld, alleviating the cross-site data discrepancy."
604,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"-we demonstrate on
five datasets collected from different sites that our method can effectively
learn a unified mos model from multi-source datasets, achieving superior
performance over the state-of-the-art (sota) methods."
605,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"""aug"" refers to perturbations with data augmentations."
606,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"to learn a unified model from a small-sized fld and a number of plds, we propose
a novel framework to address the issues of label-scarcity and cross-site data
discrepancy."
607,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"each dataset can then be
formally defined as either, where i f j,i is the i-th pixel of the j-th image in
the fld d f , and y f j,i is its corresponding label."
608,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"since
foreground organ in one pld may be labeled as background in another dataset,
such a background ambiguity brings challenges to joint training on multiple
plds."
609,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"we adopt the teacher-student scheme [14] for consistency learning on the
unlabeled data."
610,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"given the prototypes generated on the
fldwhere < •, • > calculates the cosine similarity between the two
terms.similarly, in the student branch, denote ψ i as the i-th featuresince p
t,i , p s,i model the voxel-to-organ affinities in the embedding space,
constraining consistency on them introduces rich context information for
training on the unlabeled data, which is formulated as,where 1is the
normalization factor to get the mean kl-divergence in the feature embedding
space."
611,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,datasets and implementation details.
612,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"we use five abdominal ct datasets (malbcvwc
[1], decathlon spleen [3], kits [2], decathlon liver [3] and decathlon pancreas
[3] datasets respectively) to evaluate the effectiveness of our method
[1][2][3]."
613,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"the spatial resolution of all these datasets are resampled to (1 × 1
× 3)mm 3 ."
614,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"we randomly split each dataset into training (60%), validation (20%)
and testing (20%)."
615,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"we further study the effectiveness of the csfa module in alleviating
crosssite data discrepancy."
616,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"in the mmd calculation, for each dataset, we first
generate features from the penultimate layer."
617,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"as shown, by introducing the csfa module, the feature
distribution discrepancy in terms of mmd can be effectively alleviated across
all the ""full vs partial"" dataset pairs.comparison with the state-of-the-art
(sota) methods."
618,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"for fair comparison, all the sota methods
were trained/tested on our own dataset splits."
619,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"we reported the dsc values for each organ across test sets from all the
datasets."
620,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"therefore, we pay more attention to the performance on those hard organs (in our
datasets, pancreas and kidneys are deemed to be more difficult due to their
relatively small sizes)."
621,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"cafe module adjusts the input embedding
vectors with respect to the class centroids (i.e., training data distribution)."
622,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"assuming that the classes are well separated in the feature space, the centroid
embedding vectors can serve as reference points to represent the data
distribution of the training data."
623,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"during inference, we fix the centroid embedding vectors so that the
recalibrated embedding vectors do not vary much compared to the input embedding
vectors even though the data distribution substantially changes, leading to
improved stability and robustness of the feature representation."
624,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"the experimental results
demonstrate that cafenet achieves the state-of-the-art cancer grading
performance in colorectal cancer grading datasets."
625,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"3 experiments and
results two publicly available colorectal cancer datasets [9] were employed to
evaluate
the effectiveness of the proposed cafenet."
626,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"we conducted a series of comparative experiments to evaluate the effectiveness
of cafenet for cancer grading, in comparison to several existing methods: 1)
three dcnnbased models: resnet [13], densenet [14], efficientnet [12], 2) two
metric learningbased models: triplet loss (triplet) [15] and supervised
contrastive loss (sc) [16], 3) two transformer-based models: vision transformer
(vit) [17] and swin transformer (swin) [18], and 4) one (pathology)
domain-specific model (m mae-ce o ) [9], which demonstrates the state-of-the-art
performance on the two colorectal cancer datasets under consideration."
627,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"we initialized all models using the pre-trained weights on the imagenet dataset,
and then trained them using the adam optimizer with default parameter values (β
1 = 0.9, β 2 = 0.999, ε = 1.0e-8) for 50 epochs."
628,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"after data augmentation, all patches, except for those used in vit
[17] and swin [18] models, were resized to 512 × 512 pixels."
629,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"to increase the variability of the dataset during the training phase, we
applied several data augmentation techniques, including affine transformation,
random horizontal and vertical flip, image blurring, random gaussian noise,
dropout, random color saturation and contrast conversion, and random contrast
transformations."
630,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"this is ascribable to the difference between the test datasets (c testi and c
testii ) and the training and validation datasets (c train and c validation )."
631,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"these results suggest that
cafenet has the better generalizability so as to well adapt to unseen
histopathology image data.we conducted ablation experiments to investigate the
effect of the cafe module on cancer classification."
632,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"in the experiments on colorectal cancer datasets
against several competing models, the proposed network demonstrated that it has
a better learning capability as well as a generalizability in classifying
pathology images into different cancer grades."
633,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"however, the experiments were
only conducted on two public colorectal cancer datasets from a single institute."
634,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,shows the details of the datasets.
635,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"both datasets provide colorectal pathology
images with ground truth labels for cancer grading."
636,Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,"because of that, what researchers can do is only
to prepare enough data and spend time training a model to obtain a high
performance."
637,Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,"each slice of the ct volumes in
the dataset has a matrix size of 512 × 512 pixels, with in-plane pixel sizes of
0.60-1.00 mm and thicknesses of 0.20-0.70 mm."
638,Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,"the dataset consists of the
original hepatic ct image with the liver mask and the ""gold-standard"" liver
tumor region manually segmented by a radiologist, as illustrated in fig."
639,Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,"also, the patches were extracted from input images from
both channels: a 5 × 5 × 5 sized patch in the same spatial position was
extracted to form a training patch with a size of 2 × 5 × 5 × 5 pixels.seven
cases and 24 cases in the dynamic contrast-enhanced ct scans dataset were used
for training and testing, respectively."
640,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"however, training vit models typically requires
significantly more data than traditional convolutional neural network (cnn)
models [16], which limits their application in domains such as healthcare, where
data scarcity is a challenge."
641,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"one way to overcome this challenge is to train
such models in a collaborative and distributed manner, where large amounts of
data can be leveraged from different sites without the need for sharing private
data [9,11]."
642,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"since this can be accomplished without sharing raw data, fl mitigates risks
related to private data leakage."
643,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"since no participant in sl can access the complete model parameters, it has been
claimed that sl offers better data confidentiality compared to fl."
644,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"in
particular, the u-shaped sl configuration, where each client has its own feature
extraction head and a task-specific tail [27] can further improve client
privacy, as it circumvents the need to share the data or labels."
645,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"the work in [21] focuses on privacy and incorporates differential
privacy with mixed masked patches sent from the vit on the server to the clients
to prevent any potential data leakage.in this work, we build upon the festa
framework [22] for collaborative learning of vit."
646,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"despite its success, festa
requires pretraining the vit body on a large dataset prior to its utilization in
the sl and fl training process."
647,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"in the absence of pretraining, limited training
data availability (a common problem in medical imaging) leads to severe
overfitting and poor generalization."
648,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"we first describe the working of a typical split vision transformer before
proceeding to describe fesvibs., where n c is the number of training samples
available at client c, x represents the input data, and y is the class label."
649,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"the server
consists of a vit body (b φ ), which includes a stack of l transformer blocks
denoted ashere, φ l represents the parameters of the l th transformer block
anddenotes the complete set of parameters of the transformer body.during
training, the client performs a forward pass of the input data through the head
to produce an embedding h c = h θc (x c ) ∈ r 768×m of its local data, which is
typically organized as m patch tokens representing different patches of the
input image."
650,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,datasets.
651,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,we conduct our experiments on three medical imaging datasets.
652,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"the
first dataset is ham10000 [26], a multi-class dataset comprising of 10, 015
dermoscopic images from diverse populations."
653,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"the second dataset [2] termed ""bloodmnist""
is a multi-class dataset consisting of 17, 092 blood cell images for 8 different
imbalanced cell types."
654,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"we followed [29] and split the dataset into 70% training,
10% validation, and 20% testing."
655,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"finally, the fed-isic2019 dataset consists of
23, 247 dermoscopy images for 8 different melanoma classes."
656,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"this dataset was
prepared by flamby [25] from the original isic2019 dataset [6,7,26] and the data
was collected from 6 centers, with significant differences in population
characteristics and acquisition systems, representing real-world domain shifts."
657,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"the training
samples in all datasets are divided among 6 clients, whereas the testing set is
shared among them all."
658,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,the distribution of each dataset is depicted in fig.
659,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"following [25], we used balanced accuracy in all experiments to evaluate the
performance of the classification task across all datasets."
660,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"fesvibs consistently
outperforms other methods on the three datasets with both iid and non-iid
settings."
661,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"on the other hand, svibs shows dominant performance across datasets, where the
sampling of vit blocks provides augmented representations of the input images at
different rounds and improves the generalizability."
662,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"4
(left) show consistent performance for different sets of blocks across different
datasets."
663,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"differential privacy (dp) [1] is a widelyused
approach that aims to improve the privacy of local client's data by adding
noise."
664,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"for other methods, the communication cost
per client per collaboration round: (i) fedavg/fedprox: ∼ 97m, (ii) slvit/svibs:
∼ 197m values for ham10000 dataset, and (iii) festa/fesvibs: ∼ 197m values +12m
parameters per client per unifying round."
665,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"we evaluate fesvibs framework under
iid and non-iid settings on three real-world medical imaging datasets and
demonstrate consistent performance."
666,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"it is reported in many studies [9,18]
that using multi-phase data, like most professionals do in practice, can help
the network get a more accurate result, which also acts in liver lesion
classification [15,23,24]."
667,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"meanwhile, vision
transformers (vit) [4] have been shown to replace cnn with a transformer encoder
in computer vision tasks and can achieve obvious advantages on large-scale
datasets."
668,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"second, no complete open liver lesion classification
datasets exist."
669,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"most relevant studies are based on private datasets, which tend
to be small in size and cause overfitting in learning models.in this paper, we
construct a hybrid framework with vit backbone for liver lesion classification,
transliver."
670,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"while most
multi-phase liver lesion classification studies use datasets with no more than
three phases (without dl phase for its difficulty of collection) or no more than
six lesion classes, we validate the whole framework on an in-house dataset with
four phases of abdominal ct and seven classes of liver lesions."
671,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"considering the
disproportion of axial lesion slice number and the relatively small scale of the
dataset, we adopt a 2-d network in classification part instead of 3-d in
pre-processing part and achieve a 90.9% accuracy."
672,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"to reduce errors caused by unregistered data and address the situation that one
patient has multiple lesions of different types, we pre-process the multi-phase
liver cts registered and grouped by lesions.the registration network is based on
voxelmorph [1], with a u-net learning registration field and moving data
transformed by the field."
673,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"but in our work, we need to register
the original data in a cross-phase form."
674,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"vision transformers can get excellent performance on large-scale datasets such
as imagenet [4], but they are also prone to overfit on small datasets such as
private hospital datasets."
675,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,dataset.
676,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"the employed single-phase annotated dataset is collected from sir run
run shaw hospital (srrsh), affiliated with the zhejiang university school of
medicine, and has received the ethics approval of irb."
677,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"to
handle the imbalance of dataset, we randomly select 586 lesions as the training
and validation set with no more than 700 axial slices in each lesion type, and
the rest 175 lesions constitute the test set."
678,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"the data is augmented by flip, rotation, crop, shift, and scale."
679,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"in the results of our method, hm has a relatively low performance of 62.5%,
mainly due to its low proportion in our dataset."
680,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"the details can be found in
supplementary materials.because the sources of data are different among the
methods compared above and to the best of our knowledge, no relevant study based
on transformers was found, we further train some sota normal classification
models on our dataset."
681,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"most of lesions in our dataset having few slices weakens the
redundancy between slices in 2-d pipeline, while the number of slices is still
obviously larger than the number of lesions, alleviating the overfitting issue."
682,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"we report performance of an overall 90.9% classification
accuracy on a four-phase seven-class dataset through quantitative experiments
and show obvious improvement compared with sota classification methods."
683,Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,"for instance, in the
camelyon16 breast cancer metastases dataset [10], 49.5% of wsis contain
metastases that are smaller than 1% of the tissue, requiring a high level of
expertise and long inspection time to ensure exhaustive tumor localization;
whereas other wsis have large tumor lesions and require a substantial amount of
annotation time for boundary delineation [18]."
684,Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,"we test our method using a breast
cancer metastases segmentation task on the public camelyon16 dataset and
demonstrate that determining the selected regions individually provides greater
flexibility and efficiency than selecting regions with a uniform predefined
shape and size, given the variability in histological tissue structures."
685,Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,"data augmentation includes random flip, random rotation,
and stain augmentation [12]."
686,Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,"we used the publicly available camelyon16 challenge dataset [10] training
schedules."
687,Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,"since the camelyon16 dataset is fully annotated, we perform al by assuming all
wsis are unannotated and revealing the annotation of a region only after it is
selected during the al procedure."
688,Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,"to validate our segmentation framework, we first
train on the fully-annotated data (average performance of five repetitions
reported)."
689,Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,"future work will involve the development of a wsi dataset with comprehensive
documentation of annotation time to evaluate the proposed method and an
investigation of potential combination with self-supervised learning."
690,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"to date, data-driven deep
learning (dl) methods have shown prominent segmentation performance when trained
on fully-annotated datasets [8]."
691,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"however, data annotation is a significant
bottleneck for dataset creation."
692,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"active learning (al) is a
promising solution to improve annotation efficiency by iteratively selecting the
most important data to annotate with the goal of reducing the total number of
annotated samples required."
693,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"when the entire data pool is
unlabeled, which samples should one select as the initial set?"
694,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"this problem is
known as cold-start active learning , a low-budget paradigm of al that permits
only one chance to request annotations from experts without access to any
previously annotated data.cold-start al is highly relevant to many practical
scenarios."
695,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"first, cold-start al aims to study the general question of
constructing a training set for an organ that has not been labeled in public
datasets."
696,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"this is a very common scenario (whenever a dataset is collected for a
new application), especially when iterative al is not an option."
697,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"third, in low-budget
scenarios, cold-start al can achieve one-shot selection of the most informative
data without several cycles of annotation."
698,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"(2)
diversity sampling [7,10,19,22], where samples from diverse regions of the data
distribution are selected to avoid redundancy."
699,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"a recent
study on 3d medical segmentation shows the feasibility to use the uncertainty
estimated from a proxy task to rank the importance of the unlabeled data in the
cold-start scenario [14]."
700,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"we train and validate our models on five
3d medical image segmentation tasks from the publicly available medical
segmentation decathlon (msd) dataset [1], which covers two of the most common 3d
image modalities and the segmentation tasks for both healthy tissue and
tumor/pathology.our contributions are summarized as follows:• we offer the first
cold-start al benchmark for 3d medical image segmentation."
701,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"we make our code
repository, data partitions, and baseline results publicly available to
facilitate future cold-start al research."
702,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"formally, given an unlabeled data pool of size n , cold-start al aims to select
the optimal m samples (m ≪ n ) without access to any prior segmentation labels."
703,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"we use the medical segmentation decathlon (msd) collection [1] to define our
benchmark, due to its public accessibility and the standardized datasets
spanning across two common 3d image modalities, i.e., ct and mri."
704,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"with a low budget of 5
volumes (except for heart, where 3 volumes are used because of the smaller
dataset and easier segmentation task), we assess the performance of the
uncertainty-based and diversity-based approaches against the random selection."
705,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"as suggested by prior works [3,4,7,12,20,26], random selection is a
strong competitor in the cold-start setting, since it is independent and
identically distributed (i.i.d.) to the entire data pool."
706,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"[14] proposed a proxy task and then
utilized uncertainty generated from the proxy task to rank the unlabeled data."
707,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"we visually verify that the binary pseudo label includes the
coarse boundary of the target organ.as in [14], we compute the model uncertainty
for each unlabeled data using monte carlo dropout [6]: with dropout enabled
during inference, multiple predictions are generated with stochastic dropout
configurations."
708,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"finally, we rank all unlabeled data with
the overall uncertainty scores and select the most uncertain m
samples.diversity-based selection."
709,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"first, a
feature extraction network is trained using unsupervised/self-supervised tasks
to represent each unlabeled data as a latent feature."
710,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"second, clustering
algorithms are used to select the most diverse samples in latent space to reduce
data redundancy."
711,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"to address
this issue, we train a 3d auto-encoder on the unlabeled training data using a
self-supervised task, i.e., image reconstruction."
712,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"k -means clustering is used, followed
by selecting the most typical data from each cluster, which is similar to the
alps strategy but less sensitive to outliers."
713,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"the performance of different al strategies
is evaluated by training a 3d patch-based segmentation network using the
selected data, which is an important distinction from the earlier 2d variants in
the literature."
714,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"the only difference between different experiments is the
selected data."
715,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"1) follow the same trends.our results explain why random
selection remains a strong competitor for 3d segmentation tasks in cold-start
scenarios, as no strategy evaluated in our benchmark consistently outperforms
the random selection average performance.however, we observe that typiclust
(shown as orange) achieves comparable or superior performance compared to random
selection across all tasks in our benchmark, whereas other approaches can
significantly under-perform on certain tasks, especially challenging ones like
the liver dataset."
716,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"in this paper, we presented the colossal benchmark for cold-start al strategies
on 3d medical image segmentation using the public msd dataset."
717,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"we believe our findings and
the open-source benchmark will facilitate future cold-start al studies, such as
the exploration of different uncertainty estimation/feature extraction methods
and evaluation on multi-modality datasets."
718,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,"pa maps can be used
to visually highlight whether a model is poorly extracting semantic features
[32] and/or that the model is misinformed due to spurious correlations within
the data that it was trained on [16]."
719,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,"to efficiently process image data, these
methods mainly rely on convolutional neural networks (cnns) and achieve
state-of-the-art (sota) performance."
720,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,"in this work, we focus on the explainability of the
classification of brain tumours using probe-based confocal laser endomicroscopy
(pcle) data."
721,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,"performance evaluation on pcle data shows that our improved
explainability method outperforms the sota."
722,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,dataset.
723,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,"the developed explainability framework has been validated on an in vivo
and ex vivo pcle dataset of meningioma, glioblastoma and metastases of an
invasive ductal carcinoma (idc)."
724,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,"our dataset includes 38 meningioma
videos, 24 glioblastoma and 6 idc."
725,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,"the data has been curated to remove
noisy images and similar frames."
726,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,"this resulted in a training dataset of 2500
frames per class (7500 frames in total) and a testing dataset of the same size."
727,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,"the dataset is split into a training and testing subset, with the division done
on the patient level.implementation."
728,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"in contrast, deep learning models suffer
from catastrophic forgetting [10], where learning from new data can override
previously acquired knowledge."
729,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"[23], where new classes are
observed in different stages, restricting the model from accessing previous
data.the medical domain faces a similar problem: the ability to dynamically
extend a model to new classes is critical for multiple organ and tumor
segmentation, wherein the key obstacle lies in mitigating 'forgetting.' a
typical strategy involves retaining some previous data."
730,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"however, such methods, reliant on an account of data
and annotations, may face practical constraints as privacy regulations could
make accessing prior data and annotations difficult [9]."
731,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"an alternative strategy
is to use pseudo labels generated by previously trained models on new data."
732,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"q1: can we relieve the forgetting problem without needing previous data and
annotations?"
733,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"first, inspired by knowledge distillation methods in
continual learning [11,14,15,17], we propose to generate soft pseudo annotations
for the old classes on newly-arrived data."
734,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"this enables us to recall old
knowledge without saving the old data."
735,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"we
evaluate our continual learning method using three datasets: btcv [8], lits [1]
and jhh [25] (a private dataset at johns hopkins hospital) 1 ."
736,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"on the public
datasets, the learning trajectory is to first segment 13 organs in the btcv
dataset, then learn to fig."
737,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"these kernels, when applied to the decoder (dec) feature, yield the
mask for the respective class.segment liver tumors in the lits dataset."
738,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"on the
private dataset, the learning trajectory is to first segment 13 organs, followed
by continual segmentation of three gastrointestinal tracts and four
cardiovascular system structures."
739,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"we formulate the continual organ segmentation as follows: given a sequence of
partially annotated datasets {d 1 , d 2 , ."
740,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,", c n }, we learn a single multi-organ segmentation model
sequentially using one dataset at a time."
741,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"when training on the i-th dataset d t
, the previous datasets {d 1 , ."
742,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"the model is
required to predict the accumulated organ labels for all seen datasets {d 1 , ."
743,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"in the context of continual organ segmentation, the model's inability to access
the previous dataset presents a challenge as it often results in the model
forgetting the previously learned classes."
744,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"formally, the label lc t for class c in current
learning step t can be expressed as:where l c t represents the ground truth
label for class c in step t obtained from dataset d t ."
745,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"datasets: we empirically evaluate the proposed model under two data settings: in
one setting, both training and continual learning are conducted on the inhouse
jhh dataset."
746,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"in the other setting, we first train on the btcv dataset and then do
continual learning on the lits dataset."
747,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"the btcv dataset contains 47 abdominal
ct images delineating 13 organs."
748,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"the lits dataset contains 130 contrast-enhanced
abdominal ct scans for liver and liver tumor segmentation."
749,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"in each
learning step, we report the average dsc for the classes that are used at the
current step as well as the previous steps (e.g., in step 2 of the jhh dataset,
we report the average dice of the gastrointestinal tracts and the abdominal
organs)."
750,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"the initial the continual segmentation results using the jhh dataset and public
datasets are
shown in tables 1 and2, respectively."
751,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"notably, by simply using the pseudo
labeling technique (lwf), we are able to achieve reasonably good performance in
remembering the old classes (dice of 0.777 in step 2 and 0.767 in step 3 for
abdominal organs in the jhh dataset; dice of 0.770 in step 2 for btcv organs)."
752,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"specifically, the proposed method exhibits the least forgetting in old classes
and a far better ability to adapt to new data and new classes.to evaluate the
proposed model designs, we also conduct the ablation study on the jhh dataset,
shown in table 3."
753,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"finally, we show the qualitative segmentation results
of the proposed method together with the best baseline method ilt on the jhh
dataset."
754,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"numerical results on an in-house dataset and two public
datasets demonstrate that the proposed method outperforms the continual learning
baseline methods in the challenging multiple organ and tumor segmentation tasks."
755,Efficient Subclass Segmentation in Medical Images,"in recent years, the use of deep learning for automatic medical image
segmentation has led to many successful results based on large amounts of
annotated training data."
756,Efficient Subclass Segmentation in Medical Images,"however, the trend towards segmenting medical images
into finergrained classes (denoted as subclasses) using deep neural networks has
resulted in an increased demand for finely annotated training data [4,11,21]."
757,Efficient Subclass Segmentation in Medical Images,"moreover, in some
cases, a dataset may have already been annotated with superclass labels, but the
research focus has shifted towards finer-grained categories [9,24]."
758,Efficient Subclass Segmentation in Medical Images,"in such
cases, re-annotating an entire dataset may not be as cost-effective as
annotating only a small amount of data with subclass labels.here, the primary
challenge is to effectively leverage superclass annotations to facilitate the
learning of fine-grained subclasses."
759,Efficient Subclass Segmentation in Medical Images,"previous label-efficient learning
methods, such as semi-supervised learning [7,17,26], few-shot learning
[10,15,19] and weakly supervised learning [13,27], focus on either utilize
unlabeled data or enhance the model's generalization ability or use weaker
annotations for training."
760,Efficient Subclass Segmentation in Medical Images,"unlike previous works such as
[6,8,18,25], we assume that the target subclasses and their corresponding
limited annotations are available during the training process, which is more in
line with practical medical scenarios.our main approach is to utilize the
hierarchical structure of categories to design network architectures and data
generation methods that make it easier for the network to distinguish between
subclass categories."
761,Efficient Subclass Segmentation in Medical Images,"our experiments on the brats 2021 [3] and
acdc [5] datasets demonstrate that our model, with sufficient superclass and
very limited subclass annotations, achieves comparable accuracy to a model
trained with full subclass annotations."
762,Efficient Subclass Segmentation in Medical Images,"2, for each sample (x, y) in the dataset that does not
have subclass labels, we pair it with a randomly chosen fine-labeled sample (x ,
y , z )."
763,Efficient Subclass Segmentation in Medical Images,"this segmentation result is
supervised by the superposition of the pseudo label map z pse and subclass
labels z, with weighting factor α:the intuition behind this framework is to
simultaneously leverage the information from both unlabeled and labeled data by
incorporating a more robust supervision from transform-invariant pseudo labels."
764,Efficient Subclass Segmentation in Medical Images,dataset and preprocessing.
765,Efficient Subclass Segmentation in Medical Images,we conduct all experiments on two public datasets.
766,Efficient Subclass Segmentation in Medical Images,"the first one is the acdc1 dataset [5], which contains 200 mri images with
segmentation labels for left ventricle cavity (lv), right ventricle cavity (rv),
and myocardium (myo)."
767,Efficient Subclass Segmentation in Medical Images,"we adopt the processed data and the same data division
in [16], which uses 140 scans for training, 20 scans for validation and 40 scans
for evaluation."
768,Efficient Subclass Segmentation in Medical Images,"the second is the brats20212 dataset [3],
which consists of 1251 mpmri scans with an isotropic 1 mm 3 resolution."
769,Efficient Subclass Segmentation in Medical Images,"we randomly split the dataset into 876, 125, and 250
cases for training, validation, and testing, respectively."
770,Efficient Subclass Segmentation in Medical Images,"for both datasets,
image intensities are normalized to values in [0, 1] and the foreground
superclass is defined as the union of all foreground subclasses for both
datasets.implementation details and evaluation metrics."
771,Efficient Subclass Segmentation in Medical Images,"to augment the data
during training, we randomly cropped the images with a patch size of 256 × 256
for the acdc dataset and 96 × 96 × 96 for the brats2021 dataset."
772,Efficient Subclass Segmentation in Medical Images,"we used a
batch size of 24 for the acdc dataset and 4 for the brats2021 dataset, where
half of the samples are labeled with subclasses and the other half only labeled
with superclasses."
773,Efficient Subclass Segmentation in Medical Images,"the first u-net
was trained on the complete subclass dataset {(x l , y l , z l )} n l=1 , while
the second was trained on its subset {(x l , y l , z l )} n l=1 ."
774,Efficient Subclass Segmentation in Medical Images,"mean dice score (%, left) and hd95 (mm,
right) of different methods on acdc and brats2021 datasets."
775,Efficient Subclass Segmentation in Medical Images,"separately represents the number of data with superclass and subclass
annotations in the experiments."
776,Efficient Subclass Segmentation in Medical Images,"however, the
methods that were specifically designed to utilize superclass information or
explore the intrinsic structure of the subclass data, such as nl, cps, and uamt,
did not consistently outperform the simple modified u-net."
777,Efficient Subclass Segmentation in Medical Images,"in contrast, our
proposed method achieved the best performance among all compared methods on both
the acdc and brats2021 datasets."
778,Efficient Subclass Segmentation in Medical Images,"the effectiveness of the proposed hierarchicalmix is evident from
the comparisons made with models that use only image mixup or pseudo-labeling
for data augmentation, while the addition of separate normalization consistently
improves the model performance."
779,Efficient Subclass Segmentation in Medical Images,"our
approach leverages the hierarchical structure of categories to design network
architectures and data generation methods that enable the network to distinguish
between subclass categories more easily."
780,Efficient Subclass Segmentation in Medical Images,"our experiments on
the acdc and brats2021 datasets demonstrated that our proposed approach
outperformed other compared methods in improving the segmentation accuracy."
781,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"however, designing educational materials solely
based on real-world data poses several challenges."
782,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"five
expert physicians evaluated the edited images from a clinical perspective using
two datasets: a pelvic mri dataset and chest ct dataset."
783,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"ssim, and psnr were 1.41 × 10 -2 ± 1.04 × 10 -2 , 7.40 ×
10 -1 ± 0.57 × 10 -1 , and 22.5 ± 2.7 in the pelvic mri testing dataset and 5.03
× 10 -4 ± 3.03 × 10 -4 , 9.08 × 10 -1 ± 0.34 × 10 -1 , and 38.6 ± 1.7 in the
chest ct testing dataset."
784,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"the accuracies (i.e., the ratio of images correctly identified as real or
synthetic) were 0.69 ± 0.11 and 0.65 ± 0.11, for the pelvic mri and chest ct
testing datasets, respectively."
785,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"0.14 ± 0.13) for the pelvic mri testing dataset, and 0.74 ± 0.28 (vs."
786,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"0.12 ± 0.14) for the
chest ct testing dataset."
787,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"future challenges
include improving scalability with fewer manual operations, validating
segmentation maps from a more objective perspective, and comparing our proposed
algorithm with existing methods, such as those based on superpixels [10].data
use declaration and acknowledgment: the pelvic mri and chest ct datasets were
collected from the national cancer center hospital."
788,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"the study, data use, and
data protection procedures were approved by the ethics committee of the national
cancer center, tokyo, japan (protocol number 2016-496).our implementation and
all synthesized images will be available here: https://
github.com/kaz-k/medical-image-editing."
789,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"implementation and datasets: all neural networks were implemented in python 3.8
using the pytorch library 1.10.0 [12] on an nvidia tesla a100 gpu running cuda
10.2."
790,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"the pelvic mri dataset with
rectal cancer contained 289 image series for training and 100 image series for
testing."
791,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"the chest ct dataset with lung cancer contained 500 image
series for training and 100 image series for testing."
792,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"both were in-house datasets collected
from a single hospital."
793,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"by comparing different settings on
the pelvic mri training dataset (see supplementary information), the number of
segmentation classes of 10, the combination of t 1 , t 2 , and t 3 with moderate
magnitude, the weakly imposed reconstruction loss, and a certain value of the
margin parameter were considered suitable for self-supervised segmentation."
794,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"a similar configuration was
applied to the chest ct training dataset."
795,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"the anatomical substructures, including the histological
structure of the colorectal wall and subregions within the lung, corresponded
well with the segmentation maps in both the pelvic mri and chest ct testing
datasets."
796,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"since unlabeled medical images are comparatively easier to obtain in larger
quantities, an alternative strategy is to perform self-supervised learning and
generate pre-trained models from unlabeled datasets."
797,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"self-supervised learning
involves automatically generating a supervisory signal from the data itself and
learning a representation by solving a pretext task.in computer vision, current
self-supervised learning methods can be broadly divided into discriminative
modeling and generative modeling."
798,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"during the
fine-tuning stage, we simply concatenate the local and global contrast models
and fine-tune the resulting model on a small target dataset."
799,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"in the fine-tuning stage, we fine-tune the model with a
limited number of labelled images x f ∈ {x 1 , x 2 , ..., x f }, where f is the
size of the fine-tuning dataset.besides the two pre-trained encoders and one
decoder, a randomly initialized decoder d g is appended to the pre-trained e g
to ensure that the embeddings have the same dimensions prior to concatenation."
800,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"we combine local and global contrast models by concatenating the output of d g
and d l 's last convolutional layer, and fine-tune on the target dataset in an
end-to-end fashion."
801,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"during both global and local pre-training stages, we pre-train the encoders on
the abdomen-1k [17] dataset."
802,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"the ct images have been curated from 12
medical centers and include multi-phase, multi-vendor, and multi-disease
cases.although segmentation masks for liver, kidney, spleen, and pancreas are
provided in this dataset, we ignore these labels during pre-training since we
are following the self-supervised protocol."
803,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"during the fine-tuning stage, we perform extensive experiments on three datasets
with respect to different regions of the human body."
804,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"abd-110 is an abdomen
dataset from [25] that contains 110 ct images from patients with various
abdominal tumors and these ct images were taken during the treatment planning
stage."
805,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"we report the average dsc on 11 abdominal organs (large bowel, duodenum,
spinal cord, liver, spleen, small bowel, pancreas, left kidney, right kidney,
stomach and gallbladder).thorax-85 is a thorax dataset from [5] that contains 85
thoracic ct images."
806,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"through extensive experiments on 3 different datasets, we demonstrate lrc
is capable of enhancing these pre-training algorithms in a consistent way."
807,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"2, we show segmentation results on abd-110, thorax-85, and han datasets
respectively."
808,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"all the results are provided by models trained with target dataset
size |x t | = 10."
809,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"by comparing (c) with (g) and (d) with (h), our method shows
significant improvement, particularly on the challenging han dataset."
810,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"through extensive experiments on three multi-organ
segmentation datasets, we demonstrated that our approach consistently boosts
current supervised and unsupervised pre-training methods."
811,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"the demand for precise medical data analysis has led to the widespread use of
deep learning methods in the medical field."
812,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"however, accompanied by the
promulgation of data acts and the strengthening of data privacy, it has become
increasingly challenging to train models in large-scale centralized medical
datasets."
813,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"as one of the solutions, federated learning provides a new way out of
the dilemma and attracts significant attention from researchers.federated
learning (fl) [1,2] is a distributed machine learning paradigm in which all
clients train a global model collaboratively while preserving their data
locally."
814,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"as a crucial core of them, the aggregation algorithm plays an important
role in releasing data potential and improving global model performance."
815,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"fedavg
[1], as pioneering work, was a simple and effective aggregation algorithm, which
makes the proportions of local datasets size as the aggregation weights of local
models."
816,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"but in the real world, not only the numbers of datasets held by clients
is different, but also their data distribution may be diverse, which leads to
the fact that the data in the federated learning is non-independent identically
distribution (non-iid)."
817,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"the naive aggregation algorithms maybe have worse
performance because of the non-iid data [3][4][5][6][7][8]."
818,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"ida [15] introduced the inverse distance of local models and
the average model of all clients to handle non-iid data."
819,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"(3) we propose an aggregation algorithm that introduces the concept of
affinity and graph into federated learning, and the aggregation weights can be
adjusted adaptively; (4) the superior performance is achieved by the proposed
method, on the public cifar-10 and fets challenge datasets."
820,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"suppose k clients with private data cooperate to train a global model and share
the same neural network structure, 3d-unet [26], which is provided by the fets
challenge and kept unchanged."
821,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"the differences
in local models reflect the discrepancies in the distribution of client data to
a certain extent."
822,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"suppose the server has
received local models trained by local data, and we map them into the
topological graph."
823,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"these graphs contain all the information of local
models, including the part of universality and the part of characteristics of
the client data."
824,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"comparisons with other state-of-the-art methods on
the cifar-10 dataset."
825,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,the first dataset to verify the validity of our algorithm is cifar-10.
826,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"we partition the training set into 8 clients with heterogeneous data by sampling
from a dirichlet distribution (α = 0.5) as in [10] to simulate the non-iid
distribution, and the test set in cifar-10 is considered as the global test set
to evaluate the performance of different algorithms."
827,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,training data.
828,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"the real-world dataset used in experiments is provided by the
fets challenge organizer, which is the training set of the whole dataset about
brain tumor segmentation."
829,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"in order to evaluate the performance of fedgrav, we
partition the dataset composed of 341 data samples experiment results on the
cifar-10."
830,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"we first validate the proposed method on the
cifar-10 dataset."
831,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"as can be seen from the table, the proposed
fedgrav method outperforms the other competing fl aggregation methods including
auto-fedavg, a learning-based aggregation method, which indicates the potential
and superiority of fedgrav.experiment results on miccai fets2021 training
dataset."
832,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"in order to verify the robustness of our method and its performance in
real-world data, we conduct the experiment on the miccai fets2021 training
dataset."
833,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"to evaluate the effectiveness and find the better configuration of fedgrav, we
conduct the ablation study on the fets datasets, and the results are shown in
fig."
834,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"we evaluated our
method on cifar-10 and real-world miccai federated tumor segmentation challenge
(fets) datasets, and the superior results demonstrated the effectiveness and
robustness of our fedgrav."
835,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"as
with all deep learning-based approaches, the availability of large datasets is
essential, which is problematic in the considered case since the additional ce
low-dose scan is not acquired in clinical routine exams."
836,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"hence, there are no
public datasets to easily benchmark and compare different algorithms or evaluate
their performance."
837,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"in general, the enhancement behavior of pathological tissues
at various gbca dosages has barely been researched due to a lack of data [12].in
recent years, generative models have been used to overcome data scarcity in the
computer vision and medical imaging community."
838,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"the generator learns a non-linear transformation of a predefined
noise distribution to fit the distribution of a target dataset, while the
discriminator provides feedback by simultaneously approximating a distance or
divergence between the generated and the target distribution."
839,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"in particular, for
image-to-image translation tasks, these conditional gans have been successfully
applied using paired [14,25,27] and unpaired training data [35]."
840,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"using this dataset, we aim at the semantic
interpolation of the gbca signal at various fractional dose levels."
841,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"to this end,
we use gans to learn the contrast enhancement behavior from the dataset
collective and thereby enable the synthesis of contrast signals at various dose
levels for individual cases."
842,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"this novel loss
enables a faithful generation of noise, which is important for the
identification of enhancing pathologies and their usability as additional
training data.with this in mind, the contributions of this work are as
follows:-synthesis of gbca behavior at various doses using conditional gans,
-loss enabling interpolation of dose levels present in training data,
-noise-preserving content loss function to generate realistic synthetic images."
843,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"to focus the generation on the contrast agent signal,
our model predicts residual images ŷld ; the corresponding low-dose can be
obtained by xld = x na + ŷld .for training and evaluation, we consider samples
(x na , x sd , y ld , d, b) of a dataset ds, where y ld = x ld -x na is the
residual image of a real ce low-dose scan x ld with dose level d ∈ d and b ∈
{1.5, 3} is the field-strength in tesla of the used scanner."
844,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"further details of the dataset and the preprocessing are
in the supplementary material."
845,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"finally,
using a distance c , the content loss l c (θ) := e (xna,xsd,yld,d,b)∼u (ds),z∼n
(0,id) c g θ (z, c) , y ld guides the generator g θ towards residual images in
the dataset."
846,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"further
details of the dataset, model and training can be found in the supplementary."
847,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"moreover, the
gbca gadoterate was used, while our training data only consists of scans using
the gbca gadobutrol."
848,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"the central columns present metrics evaluated on the
synthesized low-dose images, whereas the right columns evaluate the effect of
purely synthesized data for training the standarddose prediction model [26]."
849,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"the
columns on the right of table 1 list the average psnr and ssim score on the real
33% ld subset of our test data from site 1 ."
850,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"both metrics show that the samples
synthesized using our np-loss model are superior to both 1 and vgg.to determine
the effectiveness of the ld synthesis models at different settings, we acquired
160 data samples from 1.5t and 3t philips ingenia scanners at site 2 ."
851,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"using the vcm solely trained on the real 33% ld data of site 1
yields an average psnr and mae ce on the test samples of site 2 of 40.04 and
0.092, respectively."
852,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"extending the training data for the vcm by synthesized ld
images from our model with np-loss, we get a significantly improvemed (p <
0.001) psnr score of 40.37 and mae ce of 0.075.finally, fig."
853,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"4 visualizes
synthesized ld images on the brats dataset [6] along with the associated vcm
outputs."
854,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"further, the performance of virtual contrast
models increases if training data is extended by synthesized images from our gan
model trained by the noise-preserving content loss."
855,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,"we conduct extensive
experiments on a public breast lesion ultrasound video dataset, named bluvd-186
[9]."
856,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,dataset.
857,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,"we conduct the experiments on the public bluvd-186 dataset [9],
comprising 186 videos including 112 malignant and 74 benign cases."
858,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,"the dataset
has totally 25,458 ultrasound frames, where the number of frames in a video
ranges from 28 to 413."
859,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,"we adopt the same dataset splits as in table
1."
860,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,"state-of-the-art quantitative comparison of our approach with existing
methods in literature on the bluvd-186 dataset."
861,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,"to enhance the diversity of training data, all
videos are randomly subjected to horizontal flipping, cropping, and resizing."
862,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,"the experiments conducted on a public
breast lesion ultrasound video dataset show the efficacy of our stnet, resulting
in a superior detection performance while operating at a fast inference speed."
863,DeDA: Deep Directed Accumulator,"despite
several efforts to tackle the issue [4,18,24], a clinically reliable solution
remains elusive.given the limited amount of data and high class imbalance, it's
more advantageous to explicitly incorporate domain knowledge into the network as
priors."
864,DeDA: Deep Directed Accumulator,"for fair and consistent comparison, the dataset applied in the previous work
[24] was asked for and used to demonstrate the performance of the proposed
dedabased rim parameterization da-tr."
865,DeDA: Deep Directed Accumulator,"a total of 172 subjects were included in
the dataset, and 177 lesions were identified as rim+ lesions and 3986 lesions
were identified as rim-lesions, please refer to [24] for more details about the
image acquisition and pre-processing."
866,DeDA: Deep Directed Accumulator,"transformer-based networks with fewer inductive biases rely heavily on the
use of a large training dataset or depends strongly on the feature reuse [19],
as a result, these networks as well as cnns with deeper structures are prone to
overfit small datasets.implementation details: a stratified five-fold
cross-validation procedure was applied to train and validate the performance,
and all experiments including ablation study were carried out within this
setting."
867,DeDA: Deep Directed Accumulator,"random flipping, random affine transformation and random gaussian
blurring were used to augment our data."
868,DeDA: Deep Directed Accumulator,"capturing these characteristics
poses a challenge for modern neural networks, especially given limited and
imbalanced training data."
869,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"deep learning techniques have achieved unprecedented success in the field of
medical image classification, but this is largely due to large amount of
annotated data [5,18,20]."
870,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"however, obtaining large amounts of high-quality
annotated data is usually expensive and time-consuming, especially in the field
of pathology image processing [5,[12][13][14]18]."
871,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"(color figure online)active learning (al) is an effective approach
to address this issue from a data selection perspective, which selects the most
informative samples from an unlabeled sample pool for experts to label and
improves the performance of the trained model with reduced labeling cost
[1,2,9,10,16,17,19]."
872,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"after that, we train the classifier again
using all the fine-grained labeled target class samples.we conducted two
experiments with different matching ratios (ratio of the number of target class
samples to the total number of samples) on a public 9-class colorectal cancer
pathology image dataset."
873,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"to validate the effectiveness of openal, we conducted two experiments with
different matching ratios (the ratio of the number of samples in the target
class to the total number of samples) on a 9-class public colorectal cancer
pathology image classification dataset (nct-crc-he-100k) [6]."
874,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"the dataset
contains a total of 100,000 patches of pathology images with fine-grained
labeling, with nine categories including adipose (adi 10%), background (back
11%), debris (deb 11%), lymphocytes (lym 12%), mucus (muc 9%), smooth muscle
(mus 14%), normal colon mucosa (norm 9%), cancer-associated stroma (str 10%),
and colorectal adenocarcinoma epithelium (tum, 14%)."
875,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"to construct the openset
datasets, we selected three classes, tum, lym and norm, as the target classes
and the remaining classes as the non-target classes."
876,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"we
visualize the cumulative sampling ratios of openal for the target classes in
each round on the original dataset with a 33% matching ratio, as shown in fig."
877,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"therefore, this framework can be applied to both datasets containing
only target class samples and datasets also containing a large number of
non-target class samples during testing."
878,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"data collected
from various sensors (e.g., microphones, cameras, motion controllers) are used
to identify human activity [4]."
879,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"satisfactory performances have
been achieved with these multimodal data.in practical application, however,
modality missing is a common scenario."
880,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"wirelessly connected sensors may
occasionally disconnect and temporarily be unable to send any data [3]."
881,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"but,
these approaches also require additional prediction networks for each missing
situation, and the quality of the recovered data directly affects the
performance, especially when there are only a few available modalities."
882,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"although the above two fusion strategies are
easily scalable to various data missing situations, their fusion operation is
hard-coded."
883,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"1(c), this
fusion strategy needs a constant number of data to meet the requirements of the
input channels in the convolutional network."
884,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"therefore, it has to simulate
missing data by crudely zero-padding or replacing it with similar modalities,
which inevitably introduces a bias in computation and causes performance
degradation [5,18,25].transformer has achieved success in the field of computer
vision, demonstrating that self-attention mechanism has the ability to capture
the latent correlation of image tokens."
885,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"furthermore, the calculation of
self-attention does not require a fixed number of tokens as input, which
represents a potential for handling missing data."
886,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"1(d), sfusion can handle any number of input
data instead of fixing its number."
887,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"finally, it builds a shared feature representation by fusing
the varying inputs with the weight maps.the contributions of this work are:-we
propose sfusion, which is a data-dependent fusion strategy without impersonating
missing modalities."
888,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"-we provide qualitative and quantitative performance
evaluations on activity recognition with the shl [22] dataset and brain tumor
segmentation with the brats2020 [1] dataset."
889,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"r f represents the shape of
feature representation extracted from the k-th modality of a sample data, which
can be 1d (l), 2d (h×w), 3d (d×h×w) or higher-dimensional."
890,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"it is crucial for variable
multimodal data analysis."
891,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"the shl (sussex-huawei locomotion) challenge 2019 [22] dataset provides
data from seven sensors of a smartphone to recognize eight modes of locomotion
and transportation (activities), including still, walking, run, bike, car, bus,
train, and subway."
892,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"the sensor data are collected from smartphones of a person
with four locations, including the bag, trousers front pocket, breast pocket and
hand."
893,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"data acquired from the locations except the ""hand"" are given in the train
subset, while the validation subset provides the data of all four locations."
894,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"in
the test subset, only unlabeled ""hand"" location data are available.brats2020."
895,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"the brats2020 [1] dataset provide four modality scans: t1ce, t1, t2, flair for
brain tumor segmentation."
896,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"we
select 70% data as training data, while 10% and 20% as validation and test data
respectively."
897,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"to prevent overfitting, two data augmentation techniques (randomly
flip the axes and rotate with a random angle in [-10 • , 10 • ]) are applied
during training."
898,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"during training, to simulate real missing modalities scenarios, each training
patient's data is fixed to one of 15 possible missing cases."
899,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"(1) in the early
fusion method, the data of seven sensors are concatenated along their c
dimension."
900,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"the results of different fusion methods on the validation data are presented in
table 1."
901,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"for a fair comparison, we conduct
experiments on brats2018, adopt the same data partition as [24], and cite the
results in [24]."
902,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"as a
data-dependent fusion strategy, sfusion can automatically learn the latent
correlations between different modalities and builds a shared feature
representation."
903,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"the entire fusion process is based on available data without
simulating missing modalities."
904,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"they can achieve their full potential
when trained on large amounts of fully annotated data, which is often
unavailable in the medical domain."
905,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"medical data annotation requires expert
knowledge, and exhaustive labor, especially for volumetric images [17]."
906,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"this can be
challenging as it may require a large amount of annotated data, which may not
always be available."
907,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"although some works on fss techniques focus on training
with fewer data [4,20,26], they require re-training before applying to unseen
classes."
908,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"to eliminate the need for annotated data during training and
re-training on unseen classes, some recent works have proposed self-supervised
fss methods for 3d medical images which use superpixel-based pseudo-labels as
supervision during training [8,19]."
909,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"in fss, a training dataset d tr = {(x i , y i (l))} ntr i=1 , l ∈ l tr , and a
testing dataset d te = {(x i , y i (l))} nte i=1 , l ∈ l te are available, where
(x i , y i (l)) denotes an imagemask pair of the binary class l."
910,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"during training,
a few-shot segmenter takes a support-query pair (s, q) as the input data, where
q = {(x i q , y i q (l))} ⊂ d tr , and s = {(x j s , y j s (l))} ⊂ d tr ."
911,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"in this way, the (s, q s ) pair is taken as the input data of the few-shot
segmenter, presenting a 1-way 1-shot segmentation problem."
912,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"to unify experiment results, we follow the evaluation protocol established by
[19], such as hyper-parameters, data preprocessing techniques, evaluation metric
(i.e., dice score), and compared methods."
913,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"however, the effect of this hyper-parameter is investigated in the supplementary
materials.dataset."
914,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"also, the performance of visa-fss was evaluated using hausedorff
distance and surface dice metrics on ct and mri datasets."
915,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"on the ct dataset,
visa-fss reduced sslalpnet's hausedorff distance from 30.07 to 23.62 effect of
task generation."
916,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"however, as data-hungry
approaches, deep learning models require large balanced and high-quality
datasets to meet the in scl, head classes are overtreated leading to
optimization concentrating on head classes."
917,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"long-tailed problem is
usually caused by differences in incidence rate and difficulties in data
collection."
918,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"some diseases are common while others are rare, making it difficult
to collect balanced data [13]."
919,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"thus,
existing public skin datasets usually suffer from imbalanced problems which then
results in class bias of classifier, for example, poor model performance
especially on tail lesion types.to tackle the challenge of learning unbiased
classifiers with imbalanced data, many previous works focus on three main ideas,
including re-sampling data [1,18], re-weighting loss [2,15,22] and re-balancing
training strategies [10,23]."
920,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"despite the
great results achieved, these methods either manually interfere with the
original data distribution or improve the accuracy of minority classes at the
cost of reducing that of majority classes [12,13].recently, contrastive learning
(cl) methods pose great potential for representation learning when trained on
imbalanced data [4,14]."
921,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"(3) most methods only
consider the impact of sample size (""imbalanced data"") on the classification
accuracy of skin diseases, while ignoring the diagnostic difficulty of the
diseases themselves (""imbalanced diagnosis difficulty"").to address the above
issues, we propose a class-enhancement contrastive learning (ecl) method for
skin lesion classification, differences between scl and ecl are illustrated in
fig."
922,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"for sufficiently utilizing the tail data information, we attempt to
address the solution from a proxy-based perspective."
923,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"these learnable proxies are optimized with a cycle
update strategy that captures original data distribution to mitigate the quality
degradation caused by the lack of minority samples in a mini-batch."
924,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"moreover, we design a balanced-weighted
crossentropy loss which follows a curriculum learning schedule by considering
both imbalanced data and diagnosis difficulty.our contributions can be
summarized as follows: (1) we propose an ecl framework for long-tailed skin
lesion classification."
925,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"(3) a new balancedweighted
cross-entropy loss is designed for an unbiased classifier, which considers both
""imbalanced data"" and ""imbalanced diagnosis difficulty""."
926,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"(4) experimental
results demonstrate that the proposed framework outperforms other
state-of-theart methods on two imbalanced dermoscopic image datasets and the
ablation study shows the effectiveness of each element."
927,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"since samples in a mini-batch follow
imbalanced data distribution, these proxies are designed to be generated in a
reversed imbalanced way by giving more representative proxies of tail classes
for enhancing the information of minority samples."
928,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"however, when dealing with an imbalanced
dataset, tail samples in a batch contribute little to the update of their
corresponding proxies due to the low probability of being sampled."
929,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"the proxies are updated only after a finished epoch that all data has been
processed by the framework with the gradients accumulated."
930,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"with such a strategy,
tail proxies can be optimized in a view of whole data distribution, thus playing
better roles in class information enhancement."
931,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"moreover, as the skin datasets are often small, richer
relations can effectively help form a high-quality distribution in the embedding
space and improve the separation of features."
932,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"taking both ""imbalanced data"" and ""imbalanced diagnosis difficulty"" into
consideration, we design a curriculum schedule and propose balanced-weighted
cross-entropy loss to train an unbiased classifier."
933,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"we first train a general classifier, then in the
second stage we assign larger weight to tail classes for ""imbalanced data""."
934,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,dataset and evaluation metrics.
935,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"we evaluate the ecl on two publicly available
dermoscopic datasets isic2018 [5,19] and isic2019 [5,6,19]."
936,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"we use the default data augmentation strategy on
imagenet in [9] as t 1 for classification branch."
937,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"and for cl branch, we add
random grayscale, rotation, and vertical flip in t 1 as t 2 to enrich the data
representations."
938,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"to ensure fairness, we re-train all methods by rerun their released codes
on our divided datasets with the same experimental settings."
939,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"it can be seen that ecl has a significant advantage with
the highest level in most metrics on two datasets."
940,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"noticeably, our ecl
outperforms other imbalanced methods by great gains, e.g., 2.56% in pre on
isic2018 compared with scl and 4.33% in f1 on isic2019 dataset compared with
tsc."
941,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"we can see from the results that adding cl branch can
significantly improve the network's data representation ability with better
performance than only adopting a classifier branch."
942,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"the overall performance of the network has declined compared with
training w/ the strategy, indicating that this strategy can better enhance
proxies learning through the whole data distribution."
943,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"furthermore, balanced-weighted
cross-entropy loss is designed to help train an unbiased classifier by
considering both ""imbalanced data"" and ""imbalanced diagnosis difficulty""."
944,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"extensive experiments on isic2018 and isic2019 datasets have demonstrated the
effectiveness and superiority of ecl over other compared methods."
945,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"deep learning has achieved promising performance in computer-aided diagnosis
[1,12,14,24], but it relies on large-scale labeled data to train, which is
challenging in medical imaging due to label scarcity and high annotation cost
[3,25]."
946,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"specifically, expert annotations are required for medical data, which
can be costly and time-consuming, especially in tasks such as 3d image
segmentation.transferring pre-trained models to downstream tasks is an effective
solution for addressing the label-limited problem [8], but fine-tuning the full
network with small downstream data is prone to overfitting [16]."
947,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"it avoids driving the entire model with few downstream data, which
enables it to outperform traditional fine-tuning in limited labeled data."
948,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"building on the recent success of prompt tuning in nlp [5], instead of designing
text prompts and transformer models, we explore visual prompts on convolutional
neural networks (cnns) and the potential to address data limitations in medical
imaging.however, previous prompt tuning research [18,28], whether on language or
visual models, has focused solely on the model-centric approach."
949,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"for instance,
coop [29] models a prompt's context using a set of learnable vectors and
optimizes it on a few downstream data, without discussing what kind of samples
are more suitable for learning prompts."
950,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"however, in
downstream tasks with limited labeled data, selective labeling as a data-centric
method is crucial for determining which samples are valuable for learning,
similar to active learning (al) [23]."
951,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"in al, given the initial labeled data, the
model actively selects a subset of valuable samples for labeling and improves
performance with minimum annotation effort."
952,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"first, unlike the task-specific
models trained with initial data in al, the task-agnostic pre-trained model
(e.g., trained by related but not identical supervised or self-supervised task)
is employed for data selection with prompt tuning."
953,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"however, previous al methods [27] did not consider the existence of prompts or
use prompts to estimate sample value.therefore, this paper proposes the first
framework for selective labeling and prompt tuning (slpt), combining
model-centric and data-centric methods to improve performance in medical
label-limited scenarios."
954,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"the results show that slpt outperforms
fine-tuning with just 6% of tunable parameters and achieves 94% of full-data
performance by selecting only 5% of labeled data."
955,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"given a task-agnostic pre-trained model and unlabeled data for an initial
medical task, we propose slpt to improve model performance."
956,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"specifically, with slpt, we can select valuable data to label and tune the model
via prompts, which helps the model overcome label-limited medical scenarios."
957,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"finetuning a large pre-trained model with limited data may be suboptimal and
prone to overfitting [16]."
958,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"to achieve this, we design k different data
augmentation, heads, and losses based on corresponding k prompts."
959,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"by varying
hyperparameters, we can achieve different data augmentation strengths,
increasing the model's diversity and generalization."
960,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"previous studies overlook the critical issue of data selection for downstream
tasks, especially when available labels are limited."
961,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"the first
step aims to maximize the diversity of the selected data, while the second step
aims to select the most uncertain samples based on diverse prompts.step 0:
unsupervised diversity selection."
962,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"since we do not have any labels in the initial
and our pre-trained model is task-agnostic, we select diverse samples to cover
the entire dataset."
963,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"to achieve this, we leverage the pre-trained model to obtain
feature representations for all unlabeled data."
964,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"we apply the k-center method from
coreset [22], which identifies the b samples that best represent the diversity
of the data based on these features."
965,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"these selected samples are then annotated
and serve as the initial dataset for downstream tasks.step 1: supervised
uncertainty selection."
966,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"after prompt tuning with the initial dataset, we obtain a
task-specific model that can be used to evaluate data value under supervised
training."
967,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"we sort the unlabeled data by their corresponding s values in
ascending order and select the top b data to annotate."
968,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,datasets and pre-trained model.
969,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"although there are publicly available liver
tumor datasets [1,24], they only contain major tumor types and differ in image
characteristics and label distribution from our hospital's data."
970,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"deploying a
model trained from public data to our hospital directly will be
problematic.collecting large-scale data from our hospital and training a new
model will be expensive."
971,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"we
collected a dataset from our in-house hospital comprising 941 ct scans with
eight categories: hepatocellular carcinoma, cholangioma, metastasis,
hepatoblastoma, hemangioma, focal nodular hyperplasia, cyst, and others."
972,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"we utilized a pre-trained model for liver segmentation using
supervised learning on two public datasets [24] with no data overlap with our
downstream task."
973,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"the nnunet [12] was used to preprocess and sample the data into
24 × 256 × 256 patches for training."
974,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"during training, we set k =
3 and employed diverse data augmentation techniques such as scale, elastic,
rotation, and mirror."
975,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"since we aim to evaluate the efficacy of prompt
tuning on limited labeled data in table 1, we create a sub-dataset of
approximately 5% (40/752) from the original dataset."
976,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"using this sub-dataset, we evaluated various tuning methods for limited
1."
977,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"in cases of limited data,
fine-tuning tends to overfit on a larger number of parameters, while prompt
tuning does not."
978,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"the pre-trained model is crucial for downstream tasks with
limited data, as it improves performance by 9.52% compared to
learn-from-scratch."
979,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"we
conducted steps 0 (unsupervised selection) and 1 (supervised selection) from the
unlabeled 752 data and compared our approach with other competing methods, as
shown in table 2."
980,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"in step 0, without any labeled data, our diversity selection
outperformed the random baseline by 1.86%."
981,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"building upon the 20 data points
selected by our method in step 0, we proceeded to step 1, where we compared our
method with eight other data selection strategies in supervised mode."
982,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"mc dropout and entropy underperformed in our
prompt tuning, likely due to the difficulty of learning such uncertain data with
only a few prompt parameters."
983,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"these results demonstrate the effectiveness of our data selection
approach in practical tasks.ablation studies."
984,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"moreover, we
presented a diversified visual prompt tuning and a tesla strategy that combines
unsupervised and supervised selection to build annotated datasets for downstream
tasks."
985,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"slpt pipeline is a promising solution for practical medical tasks with
limited data, providing good performance, few tunable parameters, and low
labeling costs."
986,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"(2) most segmentation tasks face the limitation of a small
labeled dataset, especially for 3d segmentation tasks, since pixel-wise 3d image
annotation is labor-intensive, time-consuming, and susceptible to operator bias."
987,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,train one model on n datasets using task-specific prompts.
988,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"although they benefit from the encoder parameter-sharing scheme and the rich
information provided by multiple training datasets, multi-head networks are
less-suitable for multi-task co-training, due to the structural redundancy
caused by the requirement of preparing a separate decoder for each task."
989,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"we collected 3237 volumetric data with three
modalities (ct, mr, and pet) and various targets (eight organs, vertebrae, and
tumors) from 11 datasets as the upstream dataset."
990,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"on this dataset, we evaluated
our uniseg model against other universal models, such as dodnet and the
clip-driven universal model."
991,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"we also compared uniseg to seven advanced
single-task models, such as cotr [26], nnformer [30], and nnunet [12], which are
trained independently on each dataset."
992,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"furthermore, to verify its generalization
ability on downstream tasks, we applied the trained uniseg to two downstream
datasets and compared it to other pre-trained models, such as mg [31], desd
[28], and unimiss [27]."
993,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"let {d 1 , d 2 , ..., d n } be n datasets."
994,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"here,j=1 represents that the i-th
dataset has a total of n i image-label pairs, and x ij and y ij are the image
and the corresponding ground truth, respectively."
995,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"straightforwardly, n tasks can
be completed by training n models on n datasets, respectively."
996,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"after training uniseg on upstream datasets, we transfer the pre-trained
encoderdecoder and randomly initialized segmentation heads to downstream tasks."
997,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"the model is fine-tuned in a fully supervised manner to minimize the sum of the
dice loss and cross-entropy loss.3 experiments and results datasets."
998,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"for this study, we collected 11 medical image segmentation datasets as
the upstream dataset to train our uniseg and single-task models."
999,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"the liver and
kidney datasets are from lits [3] and kits [11], respectively."
1000,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"the hepatic
vessel (hepav), pancreas, colon, lung, and spleen datasets are from medical
segmentation decathlon (msd) [1]."
1001,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"verse20 [19], prostate [18], brats21 [2], and
autopet [8] datasets have annotations of the vertebrae, prostate, brain tumors,
and whole-body tumors, respectively."
1002,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"we used the binary version of the verse20
dataset, where all foreground classes are regarded as one class."
1003,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"moreover, we
dropped the samples without tumors in the autopet dataset."
1004,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"meanwhile, we use
btcv [14] and vs datasets [20] as downstream datasets to verify the ability of
uniseg to generalize to other medical image segmentation tasks."
1005,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"the vs
dataset contains the annotations of the vestibular schwannoma."
1006,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"both pre-training on eleven upstream datasets and fine-tuning on two downstream
datasets were implemented based on the nnunet framework [12]."
1007,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"moreover, we
adopted a uniform sampling strategy to sample training data from upstream
datasets."
1008,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"during fine-tuning, we set the batch size to
2, the initial learning rate to 0.01, the default patch size to 48 × 192 × 192,
and the maximum training iterations to 25,000 for all downstream datasets."
1009,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"as shown in table 2, our uniseg achieves the highest
dice on eight datasets, beating the second-best models by 1.9%, 0.7%, 0.8%,
0.4%, 0.4%, 1.0%, 0.3%, 1.2% on the liver, kidney, hepav, pancreas, colon, lung,
prostate, and autopet datasets, respectively."
1010,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"moreover, uniseg also presents
superior performance with an average margin of 1.0% and 1.6% on eleven datasets
compared to the second-best universal model and single-task model, respectively,
demonstrating its superior performance.comparing to other pre-trained models."
1011,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"the former are officially
released with different backbones while the latter are trained using the
datasets and backbone used in our uniseg."
1012,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"to verify the benefit of training on
multiple datasets, we also report the performance of the models per-trained on
autopet and brats21, respectively."
1013,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"more important, thanks to the powerful baseline and small
gap between the pretext and downstream tasks, uniseg achieves the best
performance and competitive performance gains on downstream datasets,
demonstrating that it has learned a strong representations ability."
1014,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"thanks to both designs,
our uniseg achieves superior performance on 11 upstream datasets and two
downstream datasets, setting a new record."
1015,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"in our future work, we plan to design
a universal model that can effectively process multiple dimensional data."
1016,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,of single-task models and universal models on eleven datasets.
1017,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"we use dice (%)
on each dataset and mean dice (%) on all datasets as metrics."
1018,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"the best results
on each dataset are in bold."
1019,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"however, shortage of pathologists worldwide along with the complexity of
histopathological data make this task time consuming and challenging."
1020,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"in this work, we investigate the problem of diagnosing colorectal
cancer, which is one of the most common reason for cancer deaths around the
world and particularly in europe and america [23].existing deep learning-based
colorectal tissue classification methods [18,21,22] typically require large
amounts of annotated histopathological training data for all tissue types to be
categorized."
1021,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"however, obtaining large amount of training data is challenging,
especially for rare cancer tissues."
1022,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"to this end, it is desirable to develop a
few-shot colorectal tissue classification method, which can learn from seen
tissue classes having sufficient training data, and be able to transfer this
knowledge to unseen (novel) tissue classes having only a few exemplar training
images.while generative adversarial networks (gans) [6] have been utilized to
synthesize images, they typically need to be trained using large amount of real
images of the respective classes, which is not feasible in aforementioned
few-shot setting."
1023,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"furthermore, we evaluate the effectiveness
of our generated tissue images by using them as data augmentation during
training of fs colorectal tissue image classifier, leading to an absolute gain
of 4.4% in terms of mean classification accuracy over the vanilla fs classifier."
1024,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"the ability of generative models [6,15] to fit to a variety of data
distributions has enabled great strides of advancement in tasks, such as image
generation [3,12,13,19], and so on."
1025,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"despite their success, these generative
models typically require large amount of data to train and avoid overfitting."
1026,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"the transformation-based approach learns to perform
generalized data augmentations to generate intra-class images from a single
conditional image."
1027,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"the corresponding dataset [14] used in this study is widely employed
for multi-class texture classification in colorectal cancer histology and
comprises eight types of tissue: tumor epithelium, simple stroma, complex
stroma, immune cells, debris, normal mucosal glands, adipose tissue and
background (no tissue)."
1028,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,we conduct experiments on human colorectal cancer dataset [14].
1029,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"the dataset
consist of 8 categories of colorectal tissues, tumor, stroma, lymph, complex,
debris, mucosa, adipose, and empty with 625 per categories."
1030,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"1, we compare our xm-gan approach for fs
tissue image generation with state-of-the-art lofgan [7] on [14] dataset.our
proposed xm-gan that utilizes dense aggregation of relevant local information at
a global receptive field along with controllable feature modulation outperforms
lofgan with a significant margin of 30.1, achieving fid score of 55.8."
1031,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,table 3 shows the baseline comparison on the [14] dataset.
1032,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"however, dcnns require large-scale annotated data to explore
feature representations effectively."
1033,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"unfortunately, publicly available ct scans
with pixel-wise annotations are relatively limited due to high imaging and
annotation costs and data privacy concerns."
1034,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"this limited data scale currently
constrains the potential of dcnns for covid-19 segmentation using ct scans.in
comparison to ct scans, 2d chest x-rays are a more accessible and costeffective
option due to their fast imaging speed, low radiation, and low cost, especially
during the early stages of the pandemic [21]."
1035,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"for example, the chestxray dataset
[18] contains about 112,120 chest x-rays used to classify common thoracic
diseases."
1036,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"chestxr dataset [1] contains 17,955 chest x-rays used for covid-19
recognition."
1037,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"we advocate using chest x-ray datasets such as chestxray and
chestxr may benefit covid-19 segmentation using ct scans because of three
reasons: (1) supplement limited ct data and contribute to training a more
accurate segmentation model; (2) provide large-scale chest x-rays with labeled
features, including pneumonia, thus can help the segmentation model to recognize
patterns and features specific to covid-19 infections; and (3) help improve the
generalization of the segmentation model by enabling it to learn from different
populations and imaging facilities."
1038,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"second, the presence of unpaired
data, specifically ct and x-ray data, in the feature fusion/crossattention
interaction can potentially cause the model to learn incorrect or irrelevant
information due to the possible differences in their image distributions and
objectives, leading to reduced covid-19 segmentation accuracy."
1039,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"it's worth noting
that the method using paired multimodal data [2] is not suitable for our
application scenario, and the latest unpaired cross-modal [3] requires
pixel-level annotations for both modalities, while our method can use x-ray
images with image-level labels for training.this paper proposes a novel unpaired
cross-modal interaction (uci) learning framework for covid-19 segmentation,
which aims to learn strong representations from limited dense annotated ct scans
and abundant image-level annotated x-ray images."
1040,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"it includes
three main components: a multimodal encoder for image representations, a
knowledge condensation and interaction module for unpaired cross-modal data, and
task-specific networks."
1041,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"to address the challenge of information interaction
between unpaired cross-modal data, we introduce a momentum-updated prototype
learning strategy to condense modality-specific knowledge."
1042,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"the proposed uci
framework has significantly improved performance on the public covid-19
segmentation benchmark [15], thanks to the inclusion of chest x-rays.the main
contributions of this paper are three-fold: (1) we are the first to employ
abundant x-ray images with image-level annotations to improve covid-19
segmentation on limited ct scans, where the ct and x-ray data are unpaired and
have potential distributional differences; (2) we introduce the knowledge
condensation and interaction module, in which the momentum-updated prototype
learning is offered to concentrate modality-specific knowledge, and a
knowledgeguided interaction module is proposed to harness the learned knowledge
for boosting the representations of each modality; and (3) our experimental
results demonstrate our uci learning method's effectiveness and strong
generalizability in covid-19 segmentation and the potential for related disease
screening."
1043,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"it is difficult to directly learn cross-modal
dependencies using the features obtained by the encoder because ct and x-ray
data were collected from different patients."
1044,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"this means that the data may not
have a direct correspondence between two modalities, making it challenging to
capture their relationship."
1045,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"we also
used two chest x-ray-based classification datasets including chestx-ray14 [18]
and chestxr [1] to assist the uci training."
1046,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"the chestx-ray14 dataset comprises
112,120 x-ray images showing positive cases from 30,805 patients, encompassing
14 disease image labels pertaining to thoracic and lung ailments."
1047,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"the chestxr dataset consists of 21,390 samples,
with each sample classified as healthy, pneumonia, or covid-19."
1048,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"for ct data, we first truncated the hu values of each scan using the range of
[-958, 327] to filter irrelevant regions, and then normalized truncated voxel
values by subtracting 82.92 and dividing by 136.97."
1049,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"we randomly cropped
subvolumes of size 32 × 256 × 256 as the input and employed the online data
augmentation like [10] to diversify the ct training set."
1050,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"for chest x-ray data,
we set the size of input patches to 224 × 224."
1051,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"we employ the online data
argumentation, including random cropping and zooming, random rotation, and
horizontal/vertical flip, to enlarge the x-ray training dataset."
1052,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"notably, despite chestxr being more focused on covid-19
recognition, the uci model aided by the chestx-ray14 dataset containing 80k
images performs better than the uci model using the chestxr dataset with only
16k images."
1053,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"this suggests that having a larger auxiliary dataset can improve the
segmentation performance even if it is not directly related to the target task."
1054,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"we set the maximum iterations to 8w and use
chestx-ray14 as auxiliary data for all ablation experiments."
1055,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"to address the challenge of
information interaction between unpaired cross-modal data, uci further develops
a kc and ki module to condense modality-specific knowledge and facilitates
cross-modal interaction, thereby enhancing segmentation training."
1056,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,"some objects may touch the image border due to either a
restricted field of view (fov) of the imaging devices or spatial data
augmentation like the random crop."
1057,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,"4b) due to spatial data augmentation,
which causes ambiguity."
1058,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,"the diversity of
object appearance, size, and shape makes the task challenging.dataset and
evaluation metric."
1059,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,"we use the gland segmentation challenge dataset [17] that
contains colored light microscopy images of tissues with a wide range of
histological levels from benign to malignant."
1060,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,"we also compare with suggestive annotation (sa) [23], and sa with
model quantization (qsa) [20], which use multiple fcn models to select
informative training samples from the dataset."
1061,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,"since the training
data is relatively limited due to the challenges in collecting medical images,
we apply pixel-level and spatial-level augmentations, including random
brightness, contrast, rotation, crop, and elastic transformation, to alleviate
overfitting."
1062,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,"our sdt framework achieves state-of-the-art performance on 5 out of 6 evaluation
metrics on the gland segmentation dataset (table 1)."
1063,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"in addition to supervised tasks, transformer-based
models have also been shown to achieve superior performance in pre-training with
largescale (labeled/unlabeled) data sets [11].most existing neural architectures
are designed with strong human heuristics."
1064,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"we evaluated our proposed algorithm on two public data sets
with excellent performance."
1065,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"these approaches usually require
large-scale computing resources to train a large number of independent neural
networks, which makes them less practical when applied to large-scale data sets."
1066,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"we adopted large-scale data sets task07 pancreas from medical segmentation
decathlon (msd) [1] as used in [12] for architecture searching, and kits'19
[13,14] to validate the searched architectures."
1067,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"the pancreas data set has 3-class segmentation labels (background,
pancreas and tumor) for 282 ct volumes."
1068,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"we adopt entire labeled set for nas with
the same data split as [12]: 114 volumes for model training, 114 volumes for
architecture search, and 54 volumes for model validation."
1069,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"a different 4 : 1 data
split is used for experiments of training from scratch."
1070,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"the kits'19 data set has
3-class segmentation labels (background, kidney and tumor) for 210 ct volumes,
and an additional standalone 90 test volumes (with hidden ground truth) for the
public leaderboard."
1071,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"we train the searched models with 5-fold data split, and
verify the model performance on the test set using the public leaderboard."
1072,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"all
data sets are re-sampled into the isotropic voxel spacing 1.0 mm for both images
and labels."
1073,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"for both ct data sets, the voxel intensities of the images are
normalized to the range [0, 1] according to the 5 th and 95 th percentile of
overall foreground intensities.a combination of dice loss and cross-entropy loss
is adopted to minimize both global and pixel-wise distance between ground truth
and predictions."
1074,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"the training settings (like data augmentation, optimizer, etc.) are
very similar to the model searching."
1075,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"since dints is the closest work to dast, we directly compare the performance on
dints's searching tasks with the same data split."
1076,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,"to verify the effectiveness and generalization of our searched architectures
from dast, we validate the searched architecture (from pancreas data set) on
this challenging task."
1077,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"the success of deep neural networks heavily relies on the availability of large
and diverse annotated datasets across a range of computer vision tasks."
1078,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"to learn
a strong data representation for robust and performant medical image
segmentation, huge datasets with either many thousands of annotated data
structures or less specific self-supervised pretraining objectives with
unlabeled data are needed [29,33]."
1079,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"this results in a
situation where a zoo of partially labeled datasets is available to the
community."
1080,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"recent efforts have resulted in a large dataset of >1000 ct images
with >100 annotated classes each, thus providing more than 100,000 manual
annotations which can be used for pre-training [30]."
1081,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"focusing on such a dataset
prevents leveraging the potentially precious additional information of the above
mentioned other datasets that are only partially annotated."
1082,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"integrating
information across different datasets potentially yields a higher variety in
image acquisition protocols, more anatomical target structures or details about
them as well as information on different kinds of pathologies."
1083,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"consequently,
recent advances in the field allowed utilizing partially labeled datasets to
train one integrated model [21]."
1084,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"early approaches handled annotations that are
present in one dataset but missing in another by considering them as background
[5,27] and penalizing overlapping predictions by taking advantage of the fact
that organs are mutually exclusive [7,28]."
1085,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"trained
one network with a shared encoder and separate decoders for each dataset to
generate a generalized encoder for transfer learning [2]."
1086,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"so far, all previous methods do not convincingly leverage
cross-dataset synergies."
1087,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"pointed out, one common caveat is that
many methods force the resulting model to average between distinct annotation
protocol characteristics [22] by combining labels from different datasets for
the same target structure (visualized in fig."
1088,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"hence, they all fail to
reach segmentation performance on par with cutting-edge single dataset
segmentation methods."
1089,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"to this end, we introduce multitalent (multi dataset
learning and pre-training), a new, flexible, multi-dataset training method: 1)
multitalent can handle classes that are absent in one dataset but annotated in
another during training."
1090,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"overall, mul-titalent can include all kinds of new datasets
irrespective of their annotated target structures.multitalent can be used in two
scenarios: first, in a combined multi-dataset (md) training to generate one
foundation segmentation model that is able to predict all classes that are
present in any of the utilized partially annotated datasets, and second, for
pre-training to leverage the learned representation of this foundation model for
a new task."
1091,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"in experiments with a large collection of abdominal ct datasets, the
proposed model outperformed state-of-the-art segmentation networks that were
trained on each dataset individually as well as all previous methods that
incorporated multiple datasets for training."
1092,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"in comparison to an ensemble of single dataset solutions, multitalent comes with
shorter training and inference times."
1093,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"additionally, at the example of three
challenging datasets, we demonstrate that fine-tuning multitalent yields higher
segmentation performance than training from scratch or initializing the model
parameters using unsupervised pretraining strategies [29,33]."
1094,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"we introduce multitalent, a multi dataset learning and pre-training method, to
train a foundation medical image segmentation model."
1095,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"it comes with a novel
dataset and class adaptive loss function."
1096,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"furthermore, we introduce a
training schedule and dataset preprocessing which balances varying dataset size
and class characteristics."
1097,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"we begin with a dataset collection of k datasets, where c (k) ⊆ c is the label
set associated to dataset d (k) ."
1098,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"even if classes from different datasets refer
to the same target structure we consider them as unique, since the exact
annotation protocols and labeling characteristics of the annotations are unknown
and can vary between datasets: c (k) ∩ c (j) = ∅, ∀k = j."
1099,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"to solve the label contradiction problem we decouple the
segmentation outputs for each class by applying a sigmoid activation function
instead of the commonly used softmax activation function across the dataset."
1100,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"this modification allows the
network to assign multiple classes to one pixel and thus enables overlapping
classes and the conservation of all label properties from each dataset."
1101,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"we modify the loss function to be calculated only for
classes that were annotated in the corresponding partially labeled dataset
[5,27], in the following indicated by 1and 0 otherwise."
1102,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"this
compensates for the varying number of annotated classes in each dataset."
1103,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"however, the automatic pipeline configuration from nnu-net was not used in favor
of a manually defined configuration that aims to reflect the peculiarities of
each of the datasets, irrespective of the number of training cases they contain."
1104,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"for the swinunetr, we
adopted the default network topology.multi-dataset training setup."
1105,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"we trained
multitalent with 13 public abdominal ct datasets with a total of 1477 3d images,
including 47 classes (multi-dataset (md) collection)
[1,3,9,11,[18][19][20]25,26]."
1106,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"detailed information about the datasets, can be
found in the appendix in table 3 and fig."
1107,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"to compensate
for the varying number of training images in each dataset, we choose a sampling
probability per case that is inversely proportional to √ n, where n is the
number of training cases in the corresponding source dataset."
1108,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"we used the btcv
(small multi organ dataset [19]), amos (large multi organ dataset [16]) and
kits19 (pathology dataset [11]) datasets to evaluate the generalizability of the
multitalent features in a pre-training and fine tuning setting."
1109,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"naturally, the
target datasets were excluded from the respective pre-training."
1110,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"as a baseline for the multitalent, we applied the 3d u-net generated by the
nnu-net without manual intervention to each dataset individually."
1111,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"furthermore,
we trained a 3d u-net, a resenc u-net and a swinunetr with the same network
topology, patch and batch size as our multitalent for each dataset."
1112,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"as supervised baseline, we used the weights resulting
from training the three model architectures on the totalsegmentator dataset,
which consists of 1204 images and 104 classes [30], resulting in more than 10 5
annotated target structures."
1113,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"as unsupervised baseline
for the cnns, we pre-trained the networks on the multi-dataset collection based
on the work of zhou et al."
1114,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,multi-dataset training results are presented in fig.
1115,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"multitalent improves the performance of the purely convolutional
architectures (u-net and resenc u-net) and outperforms the corresponding
baseline models that were trained on each dataset individually."
1116,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"since a simple
average over all classes would introduce a biased perception due to the highly
varying numbers of images and classes, we additionally report an average over
all datasets."
1117,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"for example, dataset 7 consists of only 30 training images but has
13 classes, whereas 4 in the appendix provides all results for all classes."
1118,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"averaged over all datasets, the multitalent gains 1.26 dice points for the
resenc u-net architecture and 1.05 dice points for the u-net architecture."
1119,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"compared to the default nnu-net, configured without manual intervention for each
dataset, the improvements are 1.56 and 0.84 dice points."
1120,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"for the official btcv test set in table 1, multitalent outperforms all related
work that have also incorporated multiple datasets during training, proving that
multitalent is substantially superior to related approaches."
1121,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"the training is 6.5 times faster and the inference is around 13
times faster than an ensemble of models trained on 13 datasets.transfer learning
results are found in table 2, which compares the finetuned 5-fold
cross-validation results of different pre-training strategies for three
different models on three datasets."
1122,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"especially for the small multi-organ
dataset, which only has 30 training images (btcv), and for the kidney tumor
(kits19), the multi-talent pre-training boosts the segmentation results."
1123,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"in
general, the results show that supervised pre-training can be beneficial for the
swinunetr as well, but pre-training on the large totalsegmentator dataset works
better than the md pre-training."
1124,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"for the amos dataset, no pre-training scheme
has a substantial impact on the performance."
1125,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"we suspect that it is a result of
the dataset being saturated due to its large number of training cases."
1126,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"allows including any publicly available datasets
(e.g."
1127,Co-assistant Networks for Label Correction,"in particular,
obtaining high-quality labels needs professional experience so that corrupted
labels can often be found in medical datasets, which can seriously degrade the
effectiveness of medical image analysis."
1128,Co-assistant Networks for Label Correction,"based on this, given two
outputs of the gmm model for the i-th sample, its output with a larger mean
value and the output with a smaller mean value, respectively, are denoted as m
i,1 and m i,2 , so the following definition v i is used to determine if the i-th
samples is noise:hence, the noise rate r of training samples is calculated
by:where n represents the total number of samples in training dataset."
1129,Co-assistant Networks for Label Correction,"the used datasets are breakhis [13], isic [3], and nihcc [16]."
1130,Co-assistant Networks for Label Correction,"in particular, the
random selection in our experiments guarantees that three datasets (i.e., the
training set, the testing set, and the whole set) have the same ratio for each
class."
1131,Co-assistant Networks for Label Correction,"moreover, we assume that all labels in the used raw datasets are clean,
so we add corrupted labels with different noise rates = {0, 0.2, 0.4} into these
datasests, where = 0 means that all labels in the training set are clean.we
compare our proposed method with six popular methods, including one fundamental
baseline (i.e., cross-entropy (ce)), three robustness-based methods (i.e.,
co-teaching (ct) [6], nested co-teaching (nct) [2] and self-paced resistance
learning (sprl) [12]), and two label correction methods (i.e., co-correcting
(cc) [9] and self-ensemble label correction (selc) [11])."
1132,Co-assistant Networks for Label Correction,table 1 presents the classification results of all methods on three datasets.
1133,Co-assistant Networks for Label Correction,"first, our method obtains the best results, followed
by ct, nct, sprl, celc, cc, and ce, on all datasets in terms of four evalua-
this might be because our proposed method not only utilizes a robust method to
train a cnn for distinguishing corrupted labels from clean labels, but also
corrects them by considering their relationship among the samples within the
same class."
1134,Co-assistant Networks for Label Correction,"experiments on three
medical image datasets demonstrate the effectiveness of the proposed framework."
1135,Co-assistant Networks for Label Correction,"although our method has achieved promising performance, its accuracy might be
further boosted by using more powerful feature extractors, like pre-train models
on large-scale public datasets or some self-supervised methods, e.g.,
contrastive learning."
1136,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"however, acquiring such paired
data is challenging in real clinical scenarios."
1137,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"although it is possible to
simulate low-quality images from high-quality images, the models derived from
such data may have limited generalization ability when applied to real data
[9,14]."
1138,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"for example,
openai's improved diffusion models [21] took 1600-16000 a100 hours to be trained
on the imagenet dataset with one million images, which is prohibitively
expensive."
1139,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"notably, it eliminates the need for paired
data, enabling greater scalability and wider applicability than existing
paired-image dependent methods."
1140,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"image generation models aim to capture the intrinsic data distribution from a
set of training images and generate new images from the model itself."
1141,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"the main
objective is to recover x by solving the minimization problem:x * = arg minwhere
the first data-fidelity term keeps the data consistency and the second
dataregularization term r(x) imposes prior knowledge constraints on the
solution."
1142,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,2: for t = t to 1 do 3: dataset.
1143,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"to mimic the real-world setting, the diffusion models were
trained on a diverse dataset, including images from different centers and
scanners."
1144,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"specifically, the denoising task is based on
the aapm low dose ct grand challenge abdominal dataset [19], which can be also
used for sr [33]."
1145,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"the heart mr sr task is based on three datasets: acdc [1],
m&ms1-2 [3], and cmrxmotion [27]."
1146,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"notably, the presented framework eliminates
the requirement of paired data."
1147,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"for the ct image enhancement task, we trained a
diffusion model [21] based on the full-dose dataset that contains 5351 images,
and the hold-out quarter-dose images were used for testing."
1148,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"for the mr
enhancement task, we used the whole acdc [1] and m&ms1-2 [3] for training the
diffusion model and the cmrxmotion [27] dataset for testing."
1149,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"we also compared the present method with one commonly used
image enhancement method dip [10] and two recent diffusion model-based methods:
ivlr [6], which adopted low-frequency information from measurement y to guide
the generation process towards a narrow data manifold, and dps [7], which
addressed the intractability of posterior sampling through laplacian
approximation."
1150,Robust T-Loss for Medical Image Segmentation,"however, supervised training of cnns and vits
requires large amounts of annotated data, where each pixel in the image is
labeled with the category it belongs to."
1151,Robust T-Loss for Medical Image Segmentation,"despite efforts to obtain labels through
automated mining [31] and crowd-sourcing methods [11], the quality of datasets
gathered using these methods remains challenging due to often high levels of
label noise.for instance, the fitzpatrick 17k dataset, commonly used in
dermatology research, contains non-skin images and noisy annotations."
1152,Robust T-Loss for Medical Image Segmentation,"the dataset was scraped from online atlases, which makes it vulnerable to
inaccuracies and noise [10]."
1153,Robust T-Loss for Medical Image Segmentation,"noisy labels are and will continue to be, a problem
in medical datasets."
1154,Robust T-Loss for Medical Image Segmentation,"the t-loss, whose simplest formulation features a
single parameter, can adaptively learn an optimal tolerance level to label noise
directly during backpropagation, eliminating the need for additional
computations such as the expectation maximization (em) steps.to evaluate the
effectiveness of the t-loss as a robust loss function for medical semantic
segmentation, we conducted experiments on two widely-used benchmark datasets in
the field: one for skin lesion segmentation and the other for lung segmentation."
1155,Robust T-Loss for Medical Image Segmentation,"we injected different levels of noise into these datasets that simulate typical
human labeling errors and trained deep learning models using various robust loss
functions."
1156,Robust T-Loss for Medical Image Segmentation,"section 3 covers the datasets
used in our experiments, the implementation and training details of t-loss, and
the metrics used for comparison."
1157,Robust T-Loss for Medical Image Segmentation,"section 4 presents the main findings of our
study, including the results of the t-loss and the baselines on both datasets
and an ablation study on the parameter of t-loss."
1158,Robust T-Loss for Medical Image Segmentation,"for this reason, it is well known to
be robust to outliers [7,26].since the common mean squared error (mse) loss is
derived by minimizing the negative log-likelihood of the normal distribution, we
choose to apply the same transformation and getthe functional form of our loss
function for one image is then obtained with the identification y = y i and the
approximation µ = f w (x i ), and aggregated withequation ( 2) has d(d +1)/2
free parameters in the covariance matrix, which should be estimated from the
data."
1159,Robust T-Loss for Medical Image Segmentation,"the isic 2017 dataset [5] is a well-known public benchmark of dermoscopy images
for skin cancer detection."
1160,Robust T-Loss for Medical Image Segmentation,"the dataset also includes a list of
lesion attributes, such as size, shape, and color."
1161,Robust T-Loss for Medical Image Segmentation,"we resized the images to 256
× 256 pixels for our experiments.shenzhen [4,13,25] is a public dataset
containing 566 frontal chest radiographs with corresponding lung segmentation
masks for tuberculosis detection."
1162,Robust T-Loss for Medical Image Segmentation,"since there is not a predefined split for
shenzhen as in isic, to ensure representative training and testing sets, we
stratified the images by their tuberculosis and normal lung labels, with 70% of
the data for training and the remaining 30% for testing."
1163,Robust T-Loss for Medical Image Segmentation,"all images were resized to 256 × 256
pixels.without a public benchmark with real noisy and clean segmentation masks,
we artificially inject additional mask noise in these two datasets to test the
model's robustness to low annotation quality."
1164,Robust T-Loss for Medical Image Segmentation,"in particular, we follow [15], randomly sample a portion of the
training data with probability α ∈ {0.3, 0.5, 0.7}, and apply morphological
transformations with noise levels controlled by β ∈ {0.5, 0.7}1 ."
1165,Robust T-Loss for Medical Image Segmentation,"to increase
variations in the training data, we augment them with random mirroring,
flipping, and gamma transformations."
1166,Robust T-Loss for Medical Image Segmentation,"the nnu-net was trained for 100 epochs using the adam optimizer with a
learning rate of 10 -3 and a batch size of 16 for the isic dataset and 8 for the
shenzhen dataset."
1167,Robust T-Loss for Medical Image Segmentation,"we present experimental results for the skin lesion segmentation task on the
isic dataset in table 1."
1168,Robust T-Loss for Medical Image Segmentation,"1, where
traditional robust losses overfit data in later stages of learning while metrics
for the t-loss do not deteriorate."
1169,Robust T-Loss for Medical Image Segmentation,"similar to the isic dataset, all considered robust losses perform well at low
noise levels."
1170,Robust T-Loss for Medical Image Segmentation,"to shed light on this mechanism, we study the
behavior of ν during training for different label noise levels and
initializations on the isic dataset."
1171,Robust T-Loss for Medical Image Segmentation,"our evaluation on public medical
datasets for skin lesion and lung segmentation demonstrates that the t-loss
outperforms other robust losses by a statistically significant margin."
1172,Robust T-Loss for Medical Image Segmentation,"our
loss function also features remarkable independence to different noise types and
levels.it should be noted that other methods, such as [15] offer better
performance for segmentation on the isic dataset with the same synthetic noisy
labels, while the t-loss offers a simple alternative."
1173,Robust T-Loss for Medical Image Segmentation,"we declare that we have used the isic dataset [5] under the apache license 2.0,
publicly available, and the shenzhen dataset [4,13,25] public available under
the cc by-nc-sa 4.0 license."
1174,Multi-Head Multi-Loss Model Calibration,"briefly speaking, epistemic
uncertainty arises from imperfect knowledge of the model about the problem it is
trained to solve, whereas aleatoric uncertainty describes ignorance regarding
the data used for learning and making predictions."
1175,Multi-Head Multi-Loss Model Calibration,"a hard-todiagnose image, then it could express aleatoric
uncertainty, as it may not know how to solve the problem, but the ambiguity
comes from the data."
1176,Multi-Head Multi-Loss Model Calibration,"post-hoc calibration techniques like temperature scaling [10] and
its variants [6,15] have been proposed to correct over or underconfident
predictions by applying simple monotone mappings (fitted on a heldout subset of
the training data) on the output probabilities of the model."
1177,Multi-Head Multi-Loss Model Calibration,"assumption implicitly made when
using validation data to learn the mapping: these approaches suffer to
generalize to unseen data [28]."
1178,Multi-Head Multi-Loss Model Calibration,"in this sense, our approach is
closest to some recent works on multi-output architectures like [21], where a
multi-branch cnn is trained on histopathological data, enforcing specialization
of the different heads by backpropagating gradients through branches with the
lowest loss."
1179,Multi-Head Multi-Loss Model Calibration,"compared to our approach, ensuring correct gradient flow to avoid
dead heads requires ad-hoc computational tricks [21]; in addition, no analysis
on model calibration on in-domain data or aleatoric uncertainty was developed,
focusing instead on anomaly detection."
1180,Multi-Head Multi-Loss Model Calibration,"our main contribution is a multi-head
model that i) exploits multi-loss diversity to achieve greater confidence
calibration than other learning-based methods, while ii) avoiding the use of
training data to learn post-processing mappings as most post-hoc calibration
methods do, and iii) sidesteping the computation overhead of deep ensembles."
1181,Multi-Head Multi-Loss Model Calibration,"a binary classifier in a
balanced dataset, randomly predicting always one class with c = 0.5 +
confidence, has a perfect calibration and 50% accuracy."
1182,Multi-Head Multi-Loss Model Calibration,"we now describe the data we used for experimentation, carefully analyze
performance for each dataset, and end up with a discussion of our findings."
1183,Multi-Head Multi-Loss Model Calibration,"we conducted experiments on two datasets: 1) the chaoyang dataset1 , which
contains colon histopathology images."
1184,Multi-Head Multi-Loss Model Calibration,"2) kvasir2 , a dataset for the task of endoscopic
image classification."
1185,Multi-Head Multi-Loss Model Calibration,"the annotated part of this dataset contains 10,662 images,
and it represents a challenging classification problem due a high amount of
classes (23) and highly imbalanced class frequencies [2]."
1186,Multi-Head Multi-Loss Model Calibration,"for the sake of
readability we do not show measures of dispersion, but we add them to the
supplementary material (appendix b), together with further experiments on other
datasets.we implement the proposed approach by optimizing several popular neural
network architectures, namely a common resnet50 and two more recent models: a
convnext [23] and a swin-transformer [23]."
1187,Multi-Head Multi-Loss Model Calibration,"finally we would ideally observe
improved performance as we increase the diversity (comparing 2hsl to 2hml) and
as we add heads (comparing 2hml to 4hml).chaoyang: in table 1 we report the
results on the chaoyang dataset."
1188,Multi-Head Multi-Loss Model Calibration,"overall, accuracy is relatively low, since this
dataset is challenging due to label ambiguity, and therefore calibration
analysis of aleatoric uncertainty becomes meaningful here."
1189,Multi-Head Multi-Loss Model Calibration,"kvasir: next, we show in table 2 results for the kvasir dataset."
1190,Multi-Head Multi-Loss Model Calibration,"weights
are complementary, ensuring that each branch is rewarded for becoming
specialized in a subset of the original data categories."
1191,Multi-Head Multi-Loss Model Calibration,"comprehensive
experiments on two challenging datasets with three different neural networks
show that multi-head multi-loss models consistently outperform other
learning-based calibration techniques, matching and sometimes surpassing the
calibration of deep ensembles."
1192,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"deep learning models have achieved remarkable success in segmenting anatomy and
lesions from medical images but often rely on large-scale manually annotated
datasets [1][2][3]."
1193,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"this is challenging when working with volumetric medical
data as voxelwise labeling requires a lot of time and expertise."
1194,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"we compare 5 existing guidance signals on the autopet [1] and
msd spleen [2] datasets and vary various hyperparameters."
1195,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"we
implemented our experiments with monai label [23] and will release our code.we
trained and evaluated all of our models on the openly available autopet [1] and
msd spleen [2] datasets."
1196,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"we also only use pet data
for our experiments."
1197,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"we apply the same data augmentation
transforms to all models and simulate clicks as proposed in sakinis et al."
1198,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"geodesic maps exhibit lower dice
scores for small σ < 5 and achieve the best performance for σ = 5 on both
datasets."
1199,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"differences in results for different σ values are more pronounced in autopet [1]
as it is a more challenging dataset [17][18][19]25].(h2) theta."
1200,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"1d) also confirm that θ = 10 is the optimal parameter for both
datasets and that not truncating values on msd spleen [2], i.e."
1201,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"this holds true for
both datasets and the difference in performance is substantial.(h4) probability
of interaction."
1202,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"although the concrete values for msd spleen [2] autopet [1] are
different, the five metrics follow the same trend on both datasets.(m1) initial
and (m2) final dice."
1203,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"moreover, geodesic-based
signals have lower initial scores on both datasets and require more
interactions.(m3) consistent improvement."
1204,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"the consistent improvement is ≈ 65%
for both datasets, but it is slightly worse for autopet [1] as it is more
challenging."
1205,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"the guidances
are ranked in the same order in (m3) and in (m4) for both datasets."
1206,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"distance
transform-based guidances are the slowest on both datasets due to their
complexity, but all guidance signals are computed in a reasonable time (<1
s).adaptive heatmaps: results."
1207,Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,"we tested our model using the synapse dataset [13], which comprises 30 cases of
contrast-enhanced abdominal clinical ct scans (a total of 3,779 axial slices)."
1208,Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,"we followed the same preferences for
data preparation analogous to [5]."
1209,Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,"we also followed [2] experiments to evaluate
our method on the isic 2018 skin lesion dataset [6] with 2,694 images."
1210,Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,"our approach outperforms other competitors across most evaluation
metrics, indicating its excellent generalization ability across different
datasets."
1211,Understanding Silent Failures in Medical Image Classification,"failure cases produced by the system, which predominantly occur when deployment
data differs from the data it was trained on, a phenomenon known as distribution
shifts."
1212,Understanding Silent Failures in Medical Image Classification,"[3] studied failure detection on several biomedical
datasets, but only assessed the performance of csfs in isolation without
considering the classifier's ability to prevent failures."
1213,Understanding Silent Failures in Medical Image Classification,"we compare various csfs under a wide range
of distribution shifts on four biomedical datasets."
1214,Understanding Silent Failures in Medical Image Classification,"2) since the benchmark reveals that none of
the predominant csfs can reliably prevent silent failures in biomedical tasks,
we argue that a deeper understanding of the root causes in the data itself is
required."
1215,Understanding Silent Failures in Medical Image Classification,"to this end, we present sf-visuals, a visualization tool that
facilitates identifying silent failures in a dataset and investigating their
causes (see fig."
1216,Understanding Silent Failures in Medical Image Classification,"we follow the
spirit of recent robustness benchmarks, where existing datasets have been
enhanced by various distribution shifts to evaluate methods under a wide range
of failure sources and thus simulate real-world application [19,27]."
1217,Understanding Silent Failures in Medical Image Classification,"specifically, we introduce corruptions of various intensity levels to
the images in four datasets in the form of brightness, motion blur, elastic
transformations and gaussian noise."
1218,Understanding Silent Failures in Medical Image Classification,"we further simulate acquisition shifts and
manifestation shifts by splitting the data into ""source domain"" (development
data) and ""target domain"" (deployment data) according to sub-class information
from the meta-data such as lesion subtypes or clinical sites."
1219,Understanding Silent Failures in Medical Image Classification,"dermoscopy
dataset: we combine data from isic 2020 [26], derma 7 point [17], ph2 [24] and
ham10000 [30] and map all lesion sub-types to the super-classes ""benign"" or
""malignant""."
1220,Understanding Silent Failures in Medical Image Classification,"chest x-ray dataset: we pool the data from chexpert [14], nih14 [31] and
mimic [16], while only retaining the classes common to all three."
1221,Understanding Silent Failures in Medical Image Classification,"next, we
emulate two acquisition shifts by defining either the nih14 or the chexpert data
as the target domain."
1222,Understanding Silent Failures in Medical Image Classification,"fc-microscopy dataset: the rxrx1 dataset [28] represents
the fluorescence cell microscopy domain."
1223,Understanding Silent Failures in Medical Image Classification,"lung nodule ct dataset: we create a simple 2d
binary nodule classification task based on the 3d lidc-idri data [1] by
selecting the slice with the largest annotation per nodule (±two slices
resulting in 5 slices per nodule)."
1224,Understanding Silent Failures in Medical Image Classification,"we emulate two manifestation shifts by defining nodules with high
spiculation (rating > 2), and low texture (rating < 3) as target domains.the
datasets consist only of publicly available data, our benchmark provides scripts
to automatically generate the combined datasets and distribution shifts.the
sf-visuals tool: visualizing silent failures."
1225,Understanding Silent Failures in Medical Image Classification,"the proposed tool is based on
three simple operations, that enable effective and intuitive analysis of silent
failures in datasets across various csfs: 1) interactive scatter plots: see
example in fig."
1226,Understanding Silent Failures in Medical Image Classification,"to abstract
away from individual points in the scatter plot, concepts of interest, such as
classes or distribution shifts can be defined and visualized to identify
conceptual commonalities and differences in the data as perceived by the model."
1227,Understanding Silent Failures in Medical Image Classification,"for corruption shifts, we further allow
investigating the predictions on a fixed input image over varying intensity
levels.based on these visualizations, the functionality of sf-visuals is
three-fold: 1) visual analysis of the dataset including distribution shifts."
1228,Understanding Silent Failures in Medical Image Classification,"2)
visual analysis of the general behavior of various csfs on a given task 3)
visual analysis of individual silent failures in the dataset for various csfs."
1229,Understanding Silent Failures in Medical Image Classification,"training settings: on each dataset, we
employ the classifier behind the respective leading results in literature: for
chest xray data we use densenet121 [12], for dermoscopy data we use
efficientnet-b4 [29] and for fluorescence cell microscopy and lung nodule ct
data we us densenet161 [12]."
1230,Understanding Silent Failures in Medical Image Classification,"however, the method is
not reliable across all settings, falling short on manifestation shifts and
corruptions on the lung nodule ct dataset."
1231,Understanding Silent Failures in Medical Image Classification,"when looking beyond the averages
displayed in table 1 and analyzing the results of individual clinical centers,
corruptions and manifestation shifts, one remarkable pattern can be observed: in
various cases, the same csf showed opposing behavior between two variants of the
same shift on the same dataset."
1232,Understanding Silent Failures in Medical Image Classification,"outperforms all
other csfs for one clinical site (mskcc) as target domain, but falls short on
the other one (hcb).on the chest x-ray dataset, mcd worsens the performance for
darkening corruptions across all csfs and intensity levels, whereas the opposite
is observed for brightening corruptions."
1233,Understanding Silent Failures in Medical Image Classification,"further, on the lung nodule ct dataset,
dg-mcd-res performs best on bright/dark corruptions and the spiculation
manifestation shift, but worst on noise corruption and falls behind on the
texture manifestation shift."
1234,Understanding Silent Failures in Medical Image Classification,"1b, left) provides an overview of the mskcc acquisition shift
on the dermoscopy dataset and reveals a severe change of the data distribution."
1235,Understanding Silent Failures in Medical Image Classification,"figure 1b in both examples, the brightening of the image
leads to a malignant lesion taking on benign characteristics (brighter and
smoother skin on the dermoscopy data, decreased contrast between lesion and
background on the lung nodule ct data)."
1236,Understanding Silent Failures in Medical Image Classification,"2e shows how the proposed tool visualizes an acquisition
shift on the chest x-ray data."
1237,Understanding Silent Failures in Medical Image Classification,manifestation shift: on the dermoscopy data (fig.
1238,Understanding Silent Failures in Medical Image Classification,on the lung nodule ct data (fig.
1239,Understanding Silent Failures in Medical Image Classification,"2) this
study shows that in order to progress towards reliable ml systems, a deeper
understanding of the data itself is required."
1240,Understanding Silent Failures in Medical Image Classification,"on the lung nodule
ct data (fig.2a), we see how the classifier and csf break down when a malignant
sample (typically: small bright, round) exhibits characteristics typical to
benign lesions (larger, less cohesive contour, darker) and vice versa."
1241,Understanding Silent Failures in Medical Image Classification,"this
pattern of contrary class characteristics is also observed on the dermoscopy
dataset (2c)."
1242,Understanding Silent Failures in Medical Image Classification,"the failure example at the top is particularly severe, and
localization in the scatter plot reveals a position deep inside the 'benign'
cluster indicating either a severe sampling error in the dataset (e.g."
1243,Understanding Silent Failures in Medical Image Classification,"corruption shift:
figs.2b and 2dshow for the lung nodule ct data and the dermoscopy data,
respectively, how corruptions can lead to silent failures in low-confident
predictions."
1244,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"nevertheless, all of these
approaches are susceptible to the quality of the training set, which cannot
always be guaranteed in clinical applications.to address this limitation, we
propose an evidence reconciled neural network (ernn), which aims to reliably
detect those samples that are similar to the training data but still with
different distributions (near ood), while maintain accuracy for in-distribution
(id) classification."
1245,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"extensive experiments on both
isic2019 dataset and in-house pancreas tumor dataset demonstrate that the
proposed ernn significantly improves the reliability and accuracy of ood
detection for clinical applications."
1246,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,datasets.
1247,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"we conduct experiments on isic 2019 dataset [3,4,19] and an inhouse
dataset."
1248,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"the in-house pancreas
tumor dataset collected from a cooperative hospital is composed of eight
classes: pdac (302), ipmn (71), net (43), scn (37), asc (33), cp (6), mcn (3),
and panin (1)."
1249,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"• posterior network described in
[2], where density estimators are used for generating the parameters of
dirichlet distributions.inspired by [14], we further compare the proposed method
with mixup-based methods:• mixup: as described in [23], mix up is applied to all
samples.• mt-mixup: mix up is only applied to mid-class and tail-class samples.•
mtmx-prototype: on the basis of mt mixup, prototype network is also applied to
estimate uncertainty.the results on two datasets are shown in table 1."
1250,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"we can
clearly observe that ernn consistently achieves better ood detection performance
than other uncertainty-based methods without additional data augmentation."
1251,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"even
with using mixup, ernn exhibits near performance with the best method
(mtmx-prototype) on isic 2019 and outperforms the other methods on in-house
datasets."
1252,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"in this section, we conduct a detailed ablation study to clearly demonstrate the
effectiveness of our major technical components, which consist of evaluation of
evidential head, evaluation of the proposed evidence reconcile block on both
isic 2019 dataset and our in-house pancreas tumor dataset."
1253,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"as shown in
table 2, it is clear that a network with an evidential head can improve the ood
detection capability by 6% and 1% on isic dataset and in-house pancreas tumor
dataset respectively."
1254,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"furthermore, introducing erb further improves the ood
detection performance of ernn by 1% on isic dataset."
1255,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"and on the more challenging
inhouse dataset, which has more similarities in samples, the proposed method
improves the auroc by 2.3%, demonstrating the effectiveness and robustness of
our model on more challenging tasks."
1256,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"(b): statistics of the number of nodules at different scales in three
datasets."
1257,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"the main reason is that the lesion scale in the
two public datasets are relatively small, which matches the fact few patients
have very large nodule or mass."
1258,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"note that we do not need to exploit
any data from the training set."
1259,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"experimental results on two public datasets and one
in-house dataset demonstrate that the proposed method outperforms existing
methods with different backbones."
1260,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"the stochastic gradient descent (sgd) and the automatic data
acquisition module weight decay (adamw) optimizers are usually used to optimize
the weighted parameters.for each roi input, the center point c of the lesion,
which is represented as2 ) in cartesian coordinate system, can be used as a
reference point to assist the network in improving segmentation performance."
1261,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"due to differences in the statistical distribution of
pulmonary nodule scale in image data from different medical centers, the
segmentation results of some images, especially for large nodules, are worse
than expected."
1262,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"to determine the super parametric values for the mapping function
r, we perform cross-validation on three datasets."
1263,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,we experiment on two public datasets and one in-house dataset.
1264,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"all three
datasets are divided into training, validation, and test sets using a 7:1:2
ratio."
1265,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"the lidc dataset is a publicly available lung ct image database containing 1018
scans, developed by the lung image database consortium (lidc)."
1266,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"all pulmonary
nodules and masses in the dataset have been annotated by multiple raters."
1267,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"overall, we selected a total of 1625 nodules
and masses that were annotated by more than three raters from the lidc dataset
for the experiment."
1268,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"the lndb dataset published in 2019, comprises 294 ct scans collected between
2016 and 2018."
1269,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"each ct scan in the dataset has been segmented by at least one
radiologist."
1270,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,the nodules included in this dataset are larger than 3 mm.
1271,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"the mean
scale of the lesion in lndb dataset is the shortest among the three datasets."
1272,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"we
adopt 1968 nodules and masses from the lndb dataset."
1273,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,the in-house data (ours) contains 4055 ct scans and 6864 nodules and masses.
1274,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"we
exclude nodules and masses with diameters larger than 64 mm or smaller than 2
mm, as the diameter of the largest mass in the public dataset is no more than 64
mm."
1275,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"the experimental results presented in table 1, consistently
demonstrate that the cnn-based network can achieve better results in multiscale
pulmonary nodule and mass segmentation tasks across all three datasets."
1276,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"in datasets such as lidc and in-house, where the number imbalance of
multi-scale lesion phenomena is more notable, the multi-input method
consistently outperforms the other two baselines."
1277,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"firstly, we present the quantitative comparison in table 2, where we
group the nodules and masses in each dataset at 10 mm intervals and calculate
the average segmentation performance differences of the nodules in each scale
group."
1278,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"extensive experiments on two public datasets and one in-house dataset
demonstrate that though sattca increases inference time for each sample by about
one second, it outperforms the corresponding baseline and click-based methods
with different backbones."
1279,WeakPolyp: You only Look Bounding Box for Polyp Segmentation,"more meaningfully, weakpolyp can take existing large-scale polyp detection
datasets to assist the polyp segmentation task."
1280,WeakPolyp: You only Look Bounding Box for Polyp Segmentation,datasets.
1281,WeakPolyp: You only Look Bounding Box for Polyp Segmentation,"two large polyp datasets are adopted to evaluate the model
performance, including sun-seg [9] and polyp-seg."
1282,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"the work presented in [10]
proposed the first pixel-wise annotated benchmark dataset for breast lesion
segmentation in us videos, but it has some limitations."
1283,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"although their efforts
were commendable, this dataset is private and contains only 63 videos with 4,619
annotated frames."
1284,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"the small dataset size increases the risk of overfitting and
limits the generalizability capability."
1285,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"in this work, we collected a
larger-scale us video breast lesion segmentation dataset with 572 videos and
34,300 annotated frames, of which 222 videos contain aln metastasis, covering a
wide range of realistic clinical scenarios."
1286,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"please refer to table 1 for a
detailed comparison between our dataset and existing datasets.although the
existing benchmark method dpstt [10] has shown promising results for breast
lesion segmentation in us videos, it only uses the ultrasound image to read
memory for learning temporal features."
1287,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"the experimental results
unequivocally showcase that our network surpasses state-of-the-art techniques in
the realm of both breast lesion segmentation in us videos and two video polyp
segmentation benchmark datasets (fig."
1288,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"to support advancements in breast lesion segmentation and aln metastasis
prediction, we collected a dataset containing 572 breast lesion ultrasound
videos with 34,300 annotated frames."
1289,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"table 1 summarizes the statistics of
existing breast lesion us video datasets."
1290,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"unlike previous datasets [10,12], our
dataset has a reserved validation set to avoid model overfitting."
1291,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"the entire
dataset is partitioned into training, validation, and test sets in a proportion
of 4:2:4, yielding a total of 230 training videos, 112 validation videos, and
230 test videos for comprehensive benchmarking purposes."
1292,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"moreover, apart from
the segmentation annotation, our dataset also includes lesion bounding box
labels, which enables benchmarking breast lesion detection in ultrasound videos."
1293,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,more dataset statistics are available in the supplementary.
1294,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"to initialize the backbone of our network, we pretrained
res2net-50 [6] on the imagenet dataset, while the remaining components of our
network were trained from scratch."
1295,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"additionally, we retrain these networks on our dataset and fine-tune their
network parameters to attain their optimal segmentation performance, enabling
accurate and meaningful comparisons.quantitative comparisons."
1296,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"following the experimental protocol
employed in a recent study on video polyp segmentation [8], we retrain our
network and present quantitative results on two benchmark datasets, namely
cvc-300-tv [2] and cvc-612-v [3]."
1297,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"table 4 showcases the dice, iou, s α , e φ ,
and mae results achieved by our network in comparison to state-of-the-art
methods on these two datasets."
1298,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"our method demonstrates clear superiority over
state-ofthe-art methods in terms of dice, iou, e φ , and mae on both the
cvc-300-tv and cvc-612-v datasets."
1299,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"specifically, our method enhances the dice
score from 0.840 to 0.874, the iou score from 0.745 to 0.789, the e φ score from
0.921 to 0.969, and reduces the mae score from 0.013 to 0.010 for the cvc-300-tv
dataset."
1300,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"similarly, for the cvc-612-v dataset, our method achieves improvements
of 0.012, 0.014, 0.019, and 0 in dice, iou, e φ , and mae scores, respectively."
1301,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"in this study, we introduce a novel approach for segmenting breast lesions in
ultrasound videos, leveraging a larger dataset consisting of 572 videos
containing a total of 34,300 annotated frames."
1302,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"our proposed method surpasses existing
state-of-the-art techniques in terms of performance on our annotated dataset as
well as two publicly available video polyp segmentation datasets."
1303,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"extensive experiments on 5 benchmark datasets suggest that our proposed
modifications have the potential to improve unet models."
1304,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"in order to evaluate acc-unet, we conducted experiments on 5 public datasets
across different tasks and modalities."
1305,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"for the glas dataset, we considered the original test split as the test
data, for the other datasets we randomly selected 20% of images as test data."
1306,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"we performed online
data augmentations in the form of random flipping and rotating [22]."
1307,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"apparently, for the comparatively
larger datasets (isic-18) transformer-based swin-unet was the 2nd best method,
as transformers require more data for proper training [2]."
1308,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"on the other end of
the spectrum, lightweight convolutional model (multiresunet) achieved the 2nd
best score for small datasets (glas)."
1309,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"for the remaining datasets, hybrid model
(uctransnet) seemed to perform as the 2 nd best method."
1310,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"smeswin-unet fell behind
in all the cases, despite having such a large number of parameters, which in
turn probably makes it difficult to be trained on small-scale datasets.however,
our model combining the design principles of transformers with the inductive
bias of cnns seemed to perform best in all the different categories with much
lower parameters."
1311,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"compared to much larger state-of-the-art models, for the 5
datasets, we achieved 0.13%, 0.10%, 0.63%, 0.90%, 0.27% improvements in dice
score, respectively."
1312,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"each row of the figure comprises one example
from each of the datasets and the segmentation predicted by acc-unet and the
ground truth mask are presented in the rightmost two columns."
1313,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"for the 1 st
example from the isic-18 dataset, our model did not oversegment but rather
followed the lesion boundary."
1314,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"similarly, in the 4 th sample from the covid dataset, we were capable to model
the gaps in the consolidation of the left lung visually better, which in turn
resulted in 2.9% higher dice score than the 2 nd best method."
1315,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"again, in the
final example from the glas dataset, we not only successfully predicted the
gland at the bottom right corner but also identified the glands at the top left
individually, which were mostly missed or merged by the other models,
respectively."
1316,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"we performed an ablation study on the cvc-clinicdb dataset to analyze the
contributions of the different design choices in our roadmap (fig."
1317,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"as reported by [20], even pre-trained
with a massive amount of medical data using self-supervised learning, the
performance of prostate segmentation task using high-resolution and better soft
tissue contrast mri images has not been completely satisfactory, not to mention
the lower-quality ct images."
1318,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"third, our methodology
advances state-of-the-art performance via extensive experiments on both
realworld and benchmark datasets."
1319,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"to evaluate our method, we use a large private dataset with 400 ct scans and a
large public dataset with 300 ct scans (amos [9])."
1320,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"as far as we know, the amos
dataset is the only publicly available ct dataset including prostate ground
truth."
1321,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"we randomly split the private dataset with 280 scans for training, 40 for
validation, and 80 for testing."
1322,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"the amos dataset has 200 scans for training and
100 for testing [9]."
1323,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"although the amos dataset includes the prostate class, it
mixes the prostate (in males) and the uterus (in females) into one single class
labeled pro/ute."
1324,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"we interpolate
all ct scans into an isotropic voxel spacing of [1.0 × 1.0 × 1.5] mm for both
datasets."
1325,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"for the private
dataset, we train models for 200 epochs using the adamw optimizer with an
initial learning rate of 5e -4 ."
1326,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"we also tried using 10% percent of amos training set as validation
data to find a better training parameter setting and re-trained the model with
the full training set."
1327,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"however, we did not get improved performance compared
with directly applying the training parameters learned from tuning the private
dataset."
1328,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"detailed information regarding the number of parameters, flops, and average
inference time can be found in the supplementary materials.quantitative results
are presented in table 1, which shows that the proposed focalunetr, even without
co-training, outperforms other fcn and transformer baselines (2d and 3d) in both
datasets for most of the metrics."
1329,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"the amos dataset mixes the
prostate(males)/uterus(females, a relatively small portion)."
1330,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"thus, the overall performance of focalunetr is overshadowed by this
challenge, resulting in only moderate improvement over the baselines on the amos
dataset."
1331,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"however, the performance margin significantly improves when using the
real-world (private) dataset."
1332,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"to better examine the efficacy of the auxiliary task for
focalunetr, we selected different settings of λ 1 and λ 2 for the overall loss
function l tol on the private dataset."
1333,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"extensive experiments on two large ct
datasets have shown that the focalunetr outperforms state-ofthe-art methods for
the prostate segmentation task."
1334,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"lungs, heart) while predicting the organ outline similarly
to how experts label data [10,13]."
1335,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,aleatoric uncertainty is the uncertainty in the data.
1336,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"the latter
however requires a dataset containing multiple annotations per image to obtain
optimal results.previous methods provide pixel-wise uncertainty estimates."
1337,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"let's consider a dataset made of n pairs {x i , y k i } n i=1 , each pair
consisting of an image x i ∈ r h×w of height h and width w , and a series of k
ordered points y k i , drawn by an expert."
1338,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"the camus dataset [20] contains cardiac ultrasounds from 500 patients,
for which two-chamber and four-chamber sequences were acquired.manual
annotations for the endocardium and epicardium borders of the left ventricle
(lv) and the left atrium were obtained from a cardiologist for the end-diastolic
(ed) and end-systolic (es) frames."
1339,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"the dataset is split into 400 training
patients, 50 validation patients, and 50 testing patients."
1340,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"this is a proprietary multi-site multi-vendor dataset
containing 2d echocardiograms of apical two and four chambers from 890 patients."
1341,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"data comes from patients diagnosed with coronary artery disease, covid, or
healthy volunteers."
1342,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"the dataset is split into a training/validation set (80/20)
and an independent test set from different sites, comprised of 994
echocardiograms from 684 patients and 368 echocardiograms from 206 patients,
respectively."
1343,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"the japanese society of radiological technology (jsrt) dataset
consists of 247 chest x-rays [26]."
1344,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"we used a network based on enet [24] for the ultrasound data and on deeplabv3
[7] for the jsrt dataset to derive both the segmentation maps and regress the
per-landmark heatmaps."
1345,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"images were all reshaped to 256 × 256 and b-splines were
fit on the predicted landmarks to represent the contours.training was carried
out with the adam optimizer [17] with a learning rate of 1 × 10 -3 and with
ample data augmentation (random rotation and translations, brightness and
contrast changes, and gamma corrections)."
1346,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"reliability diagrams [22]
for the 3 datasets."
1347,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"as can be seen, our uncertainty
estimation method is globally better than the other approaches except for the
correlation score on the camus dataset which is slightly larger for tta."
1348,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"4 show that our method is systematically better
aligned to perfect calibration (dashed line) for all datasets, which explains
why our method has a lower mce."
1349,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"with the exception of the private cardiac us
dataset, the skewed normal distribution model shows very similar or improved
results for both correlation and mutual information compared to the univariate
and bivariate models."
1350,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"it can be noted, however, that in specific instances, the
asymmetric model performs better on private cardiac us dataset (c.f."
1351,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"for instance, structures such as the left ventricle and the myocardium wall in
the ultrasound datasets have large components of their contour oriented along
the vertical direction which allows the univariate and bivariate models to
perform as well, if not better, than the asymmetric model."
1352,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"the contrast between the left ventricle and myocardium in the images of
the private cardiac us dataset is small, which explains why the simpler
univariate and bivariate models perform well."
1353,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"this is why on very noisy and
poorly contrasted data, the univariate or the bivariate model might be
preferable to using the asymmetric model.while our method works well on the
tasks presented, it is worth noting that it may not be applicable to all
segmentation problems like tumour segmentation."
1354,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"the shortage of labeled data is a significant challenge in medical image
segmentation, as acquiring large amounts of labeled data is expensive and
requires specialized knowledge."
1355,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"to address this issue, researchers have proposed various
semi-supervised learning (ssl) techniques that incorporate both labeled and
unlabeled data to train models for both natural [2,4,12,13,15,16] and medical
images [10,11,14,[18][19][20][21]."
1356,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"however, most of these methods do not
consider the class imbalance issue, which is common in medical image datasets."
1357,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"however,
the proposed indicators can not model the difficulty well, and the benefits may
be overestimated due to the non-representative datasets used (fig."
1358,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"[10] proposed cld to address the data bias by weighting the overall loss
function based on the voxel number of each class."
1359,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"however, this method fails due
to the easily over-fitted cps (cross pseudo supervision) [4] baseline, ignoring
unlabeled data in weight estimation and the fixed class-aware weights.in this
work, we explore the importance of heterogeneity in solving the over-fitting
problem of cps (fig."
1360,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"2) and propose a novel dhc (dual-debiased heterogeneous
co-training) framework with two distinct dynamic weighting strategies leveraging
both labeled and unlabeled data, to tackle the class imbalance issues and
drawbacks of the cps baseline model."
1361,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"specifically, distdw solves the data bias by calculating the imbalance ratio
with the unlabeled data and forcing the model to focus on extreme minority
classes through careful function design."
1362,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"1(c)), which satisfies the design ethos of a heterogeneous framework.the
key contributions of our work can be summarized as follows: 1) we first state
the homogeneity issue of cps and improve it with a novel dual-debiased
heterogeneous co-training framework targeting the class imbalance issue; 2) we
propose two novel weighting strategies, distdw and diffdw, which effectively
solve two critical issues of ssl: data and learning biases; 3) we introduce two
public datasets, synapse [9] and amos [7], as new benchmarks for
class-imbalanced semi-supervised medical image segmentation."
1363,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"these datasets
include sufficient classes and significant imbalance ratios (> 500 : 1), making
them ideal for evaluating the effectiveness of class-imbalance-targeted
algorithm designs."
1364,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"dhc
leverages the benefits of combining two diverse and accurate sub-models with two
distinct learning objectives: alleviating data bias and learning bias."
1365,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"assume that the whole dataset consists of n l labeled samples {(x l i , y i )}
nl i=1 and n u unlabeled samples {x u i } nu i=1 , where x i ∈ r d×h×w is the
input volume and y i ∈ r k×d×h×w is the ground-truth annotation with k classes
(including background)."
1366,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"the two sub-models of dhc complement each other by
minimizing the following objective functions with two diverse and accurate
weighting strategies:where pis the output probability map and ŷ(• y) is the
supervised cross entropy loss function to supervise the output of labeled data,
andis the unsupervised loss function to measure the prediction consistency of
two models by taking the same input volume x i ."
1367,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"note that both labeled and
unlabeled data are used to compute the unsupervised loss."
1368,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"to mitigate the data distribution bias, we propose a simple yet efficient
reweighing strategy, distdw."
1369,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"the difficulty-aware weights
for all classes are dataset and implementation details."
1370,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"we introduce two new benchmarks on the
synapse [9] and amos [7] datasets for class-imbalanced semi-supervised medical
image segmentation."
1371,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"the synapse dataset has 13 foreground classes, including
spleen (sp), right kidney (rk), left kidney (lk), gallbladder (ga), esophagus
(es), liver(li), stomach(st), aorta (ao), inferior vena cava (ivc), portal &
splenic veins (psv), pancreas (pa), right adrenal gland (rag), left adrenal
gland (lag) with one background and 30 axial contrast-enhanced abdominal ct
scans."
1372,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"compared with synapse, the amos dataset excludes psv but
adds three new classes: duodenum(du), bladder(bl) and prostate/uterus(p/u)."
1373,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"furthermore,
our method outperforms sota methods on synapse by larger margins than the amos
dataset, demonstrating the more prominent stability and effectiveness of the
proposed dhc framework in scenarios with a severe lack of data."
1374,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"more results on datasets with different labeled
ratios can be found in the supplementary material."
1375,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"to
achieve it, we propose two diverse and accurate weighting strategies: distdw for
eliminating the data bias of majority classes and diffdw for eliminating the
learning bias of well-performed classes."
1376,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"extensive experiments on two publicly available datasets named
3dircadb [20] and lits [2] demonstrate that our proposed framework achieves
state-of-the-art (sota) performance, outperforming cutting-edge methods
quantitatively and qualitatively."
1377,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"2, including the liver segmentation, vessel attention map
generation, point data sampling and multi-scale point-voxel fusion network."
1378,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"based on the above work, we first use the m and the l to sample get point data,
which can convert into a voxel grid through re-voxelization."
1379,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"inspired by [12], a novel multi-scale point-voxel fusion network then is
proposed to simultaneously process point and voxel data through point-based
branch and voxel-based branch, respectively, aiming to accurately perform
couinaud segmentation."
1380,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"in order to obtain the topological relationship between couinaud segments,
a direct strategy is to sample the coordinate point data with 3d spatial
information from liver ct and perform point-wise classification."
1381,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"however, directly feeding the transformed point data as input into the
point-based branch undoubtedly ignores the vessel structure, which is crucial
for couinaud segmentation."
1382,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"moreover, due to the previously mentioned
point sampling strategy, the converted voxel grid also inherits the vessel
structure from the point data and dilutes the unimportant information in the ct
images.multi-scale point-voxel fusion network."
1383,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"specifically, in the point-based branch, the
input point data {(p t , f t )} passes through an mlp, denoted as e p , which
aims to extract fine-grained features with topological relationships."
1384,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"we re-transform
the features extracted from the voxel-based branch to point representation
through trilinear interpolation, to combine them with fine-grained features
extracted from the point-based branch, which provide complementary
information:where the superscript 1 of (p t , f 1 t ) indicates that the fused
point data and corresponding features f 1 t are obtained after the first round
of point-voxel operation."
1385,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"then, the point data (p t , f 1 t ) is voxelized again
and extracted point features and voxel features through two branches."
1386,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"we evaluated the proposed framework on two publicly available datasets, 3dircadb
[20] and lits [2]."
1387,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"the 3dircadb dataset [20] contains 20 ct images with spacing
ranging from 0.56 mm to 0.87 mm, and slice thickness ranging from 1 mm to 4 mm
with liver and liver vessel segmentation labels."
1388,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"the lits dataset [2] consists
of 200 ct images, with a spacing of 0.56 mm to 1.0 mm and slice thickness of
0.45 mm to 6.0 mm, and has liver and liver tumour labels, but without vessels."
1389,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"we annotated the 20 subjects of the 3dircadb dataset [20] with the couinaud
segments and randomly divided 10 subjects for training and another 10 subjects
for testing."
1390,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"for lits dataset [2], we observed the vessel structure on ct
images, annotated the couinaud segments of 131 subjects, and randomly selected
66 subjects for training and 65 for testing.we have used three widely used
metrics, i.e., accuracy (acc, in %), dice similarity metric (dice, in %), and
average surface distance (asd, in mm) to evaluate the performance of the
couinaud segmentation."
1391,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"based on
the liver mask has been extracted, we train a 3d unet [6] on the 3diradb dataset
[20] to generate the vessel attention map of two datasets."
1392,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"we perform scaling within the range of 0.9 to 1.1,
arbitrary axis flipping, and rotation in the range of 0 to 5 • c on the input
point data as an augmentation strategy."
1393,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"by comparing the first two rows, we can see that point-net2plus [16]
and 3d unet [6] have achieved close performance in the lits dataset [2], which
demonstrates the potential of the point-based methods in the couinaud
segmentation task."
1394,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"especially on the 3diradb dataset [20] with only 10 training subjects, the acc
and dice achieved by our method exceed pointnet2plus [16] and 3d unet [6] by
nearly 10 points, and the asd is also greatly reduced, which demonstrates the
effectiveness of the combining point-based and voxel-based methods.qualitative
comparison."
1395,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"figure 4 shows the ablation experimental results obtained on all the
couinaud segments of two datasets, under the dice and the asd metrics."
1396,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"it can be
seen that our full method is significantly better than the cnn branch joint
decoder method on both metrics of two datasets, which demonstrates the
performance gain by the combined point-based branch."
1397,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"this is because to the vessel
structure-guided sampling strategy can increase the important data access
between the boundaries of the couinaud segments."
1398,HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,"2, the csnb works as
the information bridge between encoders and decoders while also ensuring the
feasibility of experiments involving large 3d data."
1399,HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,dataset and evaluation metrics.
1400,HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,"we pre-process the data by
truncating the hu value to the range of [-900, 900] and then linearly scaling it
to [-1, 1]."
1401,HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,"in particular, we use the same data augmentation, no
post-processing scheme, adam optimizer with an initial learning rate of 10 -4 ,
and train for 800 epochs with a batch size of 4."
1402,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,"soft labels can be the result of data augmentation techniques such as label
smoothing (ls) [21,43] and are integral to regularization methods such as
knowledge distillation (kd) [17,36]."
1403,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,"following [23,41],
we use qubiq 2020, which contains 7 segmentation tasks in 4 different ct and mr
datasets: prostate (55 cases, 2 tasks, 6 raters), brain growth (39 cases, 1
task, 7 raters), brain tumor (32 cases, 3 tasks, 3 raters), and kidney (24
cases, 1 task, 3 raters)."
1404,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,"for each dataset, we calculate the average dice score
between each rater and the majority votes in table 1."
1405,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,"in some datasets, such as
brain tumor t2, the inter-rater disagreement can be quite substantial."
1406,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,"in our method, we average annotations
with uniform weights for brain tumor t2 and with each rater's dice score for all
other datasets."
1407,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"[15] proposed a margin-preserving
constraint along with a self-paced cl framework, gradually increasing the
training data difficulty."
1408,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"datasets: we evaluate our work on two different da tasks to evaluate its
generalizability: (1) polyp segmentation from colonoscopy images in kvasir-seg
[11] and cvc-endoscene still [20], and (2) brain tumor segmentation in mri
images from brats2018 [16]."
1409,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"the data was split into 4 : 1 train-test ratio, following [14]."
1410,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"act [14] simply
ignores the domain gap and only learns content semantics, resulting in
substandard performance on the brats dataset that has a significant domain gap."
1411,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"ssda results are shown for 10%-labeled
(10%l) and 50%-labeled (50%l) data in the target domain."
1412,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"no da: the
encoder-decoder model trained only using labeled data from the source domain is
applied to the target domain without adaptation."
1413,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"supervised: model is trained
using all labeled data from source and target domains."
1414,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"the
margins are even higher for less labeled data (1l) on the brats dataset, which
is promising considering the difficulty of the task."
1415,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"unlike ssda methods, uda fully relies on unlabeled data for domain-invariant
representation learning."
1416,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"methods like
[10,19] rely on adversarial learning for aligning multi-level feature space,
which is not effective for small-sized medical data."
1417,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"similar results are observed for the brats dataset in
table 2, where our work achieved a margin of upto 2.4% dsc than its closest
performer."
1418,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"we have
similar ablation study observations on the brats2018 dataset, which is provided
in the supplementary file, along with some qualitative examples along with
available ground truth."
1419,BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,"3) experimental results on lidc-idri and brats 2021
datasets demonstrate that our berdiff outperforms other state-of-the-art
methods."
1420,BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,"additionally, let denote elementwise
product, and norm(•) denote normalizing the input data along the channel
dimension and then returning the second channel."
1421,BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,"during the training phase, given an image and mask data pair
{x, y 0 }, we sample a random timestep t from a uniform distribution {1, ."
1422,BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,dataset and preprocessing.
1423,BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,"the data used in this experiment are obtained from
lidc-idri [2,7] and brats 2021 [4] datasets."
1424,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"to obtain
the final prediction, we average the obtained maps and obtain one soft map.we
evaluate the performance of the proposed method on a dataset of medical images
annotated by multiple annotators."
1425,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,gaussian) to data sampled from a complex distribution.
1426,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"the total number of diffusion steps t is set by the user, and c is
the number of different annotators in the dataset."
1427,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"our experiments were carried out on
datasets of the qubiq benchmark1 ."
1428,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,datasets.
1429,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"the quantification of
uncertainties in biomedical image quantification challenge (qubiq), is a
recently available challenge dataset specifically for the evaluation of
inter-rater variability."
1430,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"qubiq comprises four different segmentation datasets
with ct and mri modalities, including brain growth (one task, mri, seven raters,
34 cases for training and 5 cases for testing), brain tumor (one task, mri,
three raters, 28 cases for training and 4 cases for testing), prostate (two
subtasks, mri, six raters, 33 cases for training and 15 cases for testing), and
kidney (one task, ct, three raters, 20 cases for training and 4 cases for
testing)."
1431,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"based on the intuition that the
more rrdb blocks, the better the results, we used as many blocks as we could fit
on the gpu without overly reducing batch size.following [13], for all datasets
of the qubiq benchmark the input image resolution, as well as the test image
resolution, was 256 × 256."
1432,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"as can be seen in table
1, our method outperforms all other methods across all datasets of qubiq
benchmark.ablation study."
1433,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"we
also note that our ""no annotator"" variant outperforms the analog amis model in
four out of five datasets, indicating that our architecture is somewhat
preferable."
1434,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"for
example, for the brain and the prostate 1 datasets, optimal performance is
achieved using 5 generated images, while on prostate 2 the optimal performance
is achieved using 25 gen- erated images."
1435,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"figure 4 depicts samples from multiple
datasets and presents the progression as the number of generated images
increases."
1436,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"in order to investigate the relationship between the annotator agreement and the
performance of our model, we conducted an analysis by calculating the average
dice score between each pair of annotators across the entire dataset."
1437,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"we observed that
our proposed method demonstrated improved performance on datasets with higher
agreement among annotators, specifically the kidney and prostate 1 datasets."
1438,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"conversely, the performance of the other methods significantly deteriorated on
the kidney dataset, leading to a lower correlation between the dice score and
the overall performance."
1439,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"training a deep neural network (dnn) for such a task
is known to be data-hungry, as labeling dense pixel-level annotations requires
laborious and expensive human efforts in practice [23,32]."
1440,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"furthermore, semantic
segmentation in medical imaging suffers from privacy and data sharing issues
[13,35] and a lack of experts to secure accurate and clinically meaningful
regions of interest (rois)."
1441,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"this data shortage problem causes overfitting for
training dnns, resulting in the networks being biased by outliers and ignorant
of unseen data.to alleviate the sample size and overfitting issues, diverse data
augmentations have been recently developed."
1442,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"alternatively, feature perturbation methods augment
data by perturbing data in feature space [7,22] and logit space [9].although
these augmentation approaches have been successful for natural images, their
usage for medical image semantic segmentation is quite restricted as objects in
medical images contain non-rigid morphological characteristics that should be
sensitively preserved."
1443,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"with this regularization, the easier samples provide
adaptive guidance to the misclassified data such that the difficult (but
object-relevant) pixels can be gradually integrated into the correct prediction."
1444,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"from active learning perspective [12,19], as vague samples near the decision
boundary are augmented and trained, improvement on a downstream prediction task
is highly expected.we summarize our main contributions as follows: 1) we propose
a novel online data augmentation method for semantic segmentation by imposing
objectspecific consistency regularization between anti-adversarial and
adversarial data."
1445,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"our method provides a flexible regularization between differently perturbed data
such that a vulnerable network is effectively trained on challenging samples
considering their ambiguities."
1446,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"3) our method preserves underlying morphological
characteristics of medical images by augmenting data with quasiimperceptible
perturbation."
1447,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"to increase
a classification score, the anti-adversarial noises move data away from the
decision boundary, which is the opposite direction of the adversarial
perturbations."
1448,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"figure 2 shows the overall training scheme
with three phases: 1) online data augmentation, 2) computing adaptive aac
between differently perturbed samples, and 3) updating the segmentation model
using the loss from the augmented and original data."
1449,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"we separate the roles of perturbed data: adversaries are used as training
samples and anti-adversaries are used to provide guidance (i.e., pseudo-labels)
to learn the adversaries."
1450,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"specifically, consistency regularization is imposed
between these contrasting data by adaptively controlling the regularization
magnitude in the next phase."
1451,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"lastly, considering each sample's ambiguity, the
network parameters θ are updated for learning the adversaries along with the
given data so that discriminative regions are robustly expanded for challenging
samples.data augmentation with object-targeted adversarial attack."
1452,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"given x i and {x - i,k } k k=1 as training data, the
supervised segmentation loss l sup and the consistency regularization r con are
defined asusing the pseudo-label from anti-adversary as a perturbation of the
ground truth, the network is supervised by diverse and realistic labels that
contain auxiliary information that the originally given labels do not provide."
1453,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,dataset.
1454,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"we conducted experiments on two representative public polyp
segmentation datasets: kvasir-seg [11] and etis-larib polyp db [25] (etis)."
1455,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"4, we visualize data augmentation results with (anti-) adversarial
perturbations on kvasir-seg dataset."
1456,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"the perturbed data (c and e) are the
addition of noise (d and f) to the given data (a), respectively."
1457,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"5b and 5c demonstrate that the anti-adversaries
(blue) are consistently easier to predict than the given data (grey) and
adversaries (red) during the training and their differences get larger as the
perturbations are iterated."
1458,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"we present a novel data augmentation method for semantic segmentation using a
flexible anti-adversarial consistency regularization."
1459,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"extensive
experiments with various backbones and datasets confirm the effectiveness of our
method."
1460,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"several studies indicated that such an approach is
feasible, for example, for improving video classification by training a model on
an auxiliary audio reconstruction task [12], or for audio-based detection by
using the multimodal knowledge distillation concept, where teacher networks
trained on rgb, depth and thermal images improve a student network trained only
on audio data [20]."
1461,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"the advantages of the proposed mfm are the
following: 1) it enables the spatial alignment of fms from one with fms from the
other modality to further reduce errors that persist after deformable
registration of input images, and enrich the fms to improve the final oar
segmentation, 2) it significantly improves the performance of the missing
modality scenario compared to other baseline fusion approaches, and 3) it
performs well also on single modality out-of-distribution data, therefore
facilitating cross-modality learning and contributing to better model
generalizability."
1462,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"the fundamental idea is that stn can learn
meaningful features that are spatially invariant to characteristics of the input
data, without the need for extra supervision, thereby enhancing task
performance."
1463,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,image datasets.
1464,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"the proposed methodology was evaluated on two publicly available
datasets: our recently released han-seg dataset [14] and the pddca dataset [15]."
1465,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"the han-seg dataset comprises ct and t1-weighted mr images of 56 patients, which
were deformably registered with the simpleelastix registration tool, and
corresponding curated manual delineations of 30 oars (for details, please refer
to [14])."
1466,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"on the other hand, to evaluate the generalization ability of our
method, we also conducted experiments on the ct-only pddca dataset (for details,
please refer to [15]), from which we collected 15 images from the offand on-site
test sets of the corresponding challenge for our evaluation."
1467,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"as this dataset is
widely used for evaluating the performance of automatic han oar segmentation
methods, it serves as a valuable benchmark for comparison with other
state-of-the-art methods."
1468,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"note that none of the images from the ct-only pddca
dataset were used for training, and as our model expects two inputs, we
substituted the missing mr modality with an empty matrix (i.e."
1469,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"to address the challenge of a relatively small dataset, we adopted a
4-fold cross-validation strategy without using any external training images."
1470,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"the overall good
performance on the han-seg dataset suggests that all models are close to the
maximal performance, which is bounded by the quality of reference segmentations."
1471,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"however, the performance on the pddca dataset that consists only of ct images
allows us to test how the models handle the missing modality scenario and
perform on an out-ofdistribution dataset, as images from this dataset were not
used for training."
1472,Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,"-we conducted extensive
experiments using the brats 2021 [12] dataset for brain tumor segmentation and a
liver mri dataset for liver tumor segmentation."
1473,Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,"background of ddpm: as an unconditional generative method, ddpm [7] has the form
of p θ (x 0 ) := p θ (x 0:t )dx 1:t , where x 1 , ..., x t represent latents
with the same dimensionality as the data x 0 ∼ q(x 0 )."
1474,Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,"the forward process
of diffusion that the approximate posterior q(x 1:t |x 0 ), it is a markov chain
by gradually adding gaussian noise for converting the noise distribution to the
data distribution according to the variance schedule β 1 , ..., β t :the reverse
process of denoising that the joint distribution p θ (x 0:t ), it can be defined
as a markov chain with learnt gaussian transitions starting from p(x t ) = n (x
t ; 0, i):where α t := 1β t , ᾱt := t s=1 α s , and multi-modality medical
images conditioned ddpm: in eq."
1475,Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,dataset.
1476,Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,"we used two mri datasets that brats 2021 [1,2,12] and a liver mri
dataset."
1477,Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,"for the number of ddpm paths, brats 2021 dataset is
equal to 4 corresponding to the input 4 mri modalities and the liver mri dataset
is equal to 3 corresponding to the input 3 mri modalities."
1478,Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,"table 3 and table 4 show the learned reliability
coefficients η on brats 2021 dataset and liver mri dataset."
1479,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"however, the effectiveness
of vit-based approaches heavily relies on access to large datasets for learning
meaningful representations of input images."
1480,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"moreover,
multitask learning acts as a regularizer by introducing inductive bias and
prevents overfitting [25] (particularly with vits), and with that, can mitigate
the challenges posed by small bus dataset sizes."
1481,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"we evaluated the performance of hybrid-mt-estan using four public datasets, hmss
[9], busi [10], busis [20], and dataset b [6]."
1482,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"we combined all four datasets to
build a large and diverse dataset with a total of 3,320 b-mode bus images, of
which 1,664 contain benign tumors and 1,656 have malignant tumors."
1483,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"table 1 shows
the detailed information for each dataset."
1484,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"hmss dataset does not provide the
segmentation ground-truth masks, and for this study we arranged with a group of
experienced radiologists to prepare the masks for hmss."
1485,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"refer to the original
publications of the datasets for more details."
1486,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"all bus images in the
dataset were zero-padded and reshaped to form square images."
1487,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"to avoid data
leakage and bias, we selected the train, test, and validation sets based on the
cases, i.e., the images from one case (patient) were assigned to only one of the
training, validation, and test sets."
1488,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"furthermore, we employed horizontal flip,
height shift (20%), width shift (20%), and rotation (20 • c) for data
augmentation."
1489,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"the proposed approach utilizes the building blocks of resnet50 and
swin-transformer-v2, pretrained on imagenet dataset."
1490,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"among them, swinunetr [23] has
achieved the new top performance in the msd challenge and beyond the cranial
vault (btcv) segmentation challenge by pretraining on large datasets."
1491,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"it has a
u-shaped structure where the encoder is a swin-transformer [16].although
transformers have achieved certain success in medical imaging, the lack of
inductive bias makes them harder to be trained and requires much more training
data to avoid overfitting."
1492,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"moreover,
the number of training data is also limited."
1493,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"besides lacking
inductive bias and enough training data, one extra reason could be that
transformers are computationally much expensive and harder to tune."
1494,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"more
improvements and empirical evidence are needed before we say transformers are
ready to replace cnns for medical image segmentation.in this paper, we try to
develop a new ""to-go"" transformer for 3d medical image segmentation, which is
expected to exhibit strong performance under different data situations and does
not require extensive hyperparameter tuning."
1495,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"swinunetr reaches top performances
on several large benchmarks, making itself the current sota, but without
effective pretraining and excessive tuning, its performance on new datasets and
challenges is not as high-performing as expected.a straightforward direction to
improve transformers is to combine the merits of both convolutions and
self-attentions."
1496,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"the network is evaluated extensively on a
variety of benchmarks and achieved top performances on the word [17], flare2021
[18], msd prostate, msd lung cancer, and msd pancreas cancer datasets [1]."
1497,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"compared to the original swin-unetr which needs extensive recipe tuning on a new
dataset, we utilized the same training recipe with minimum changes across all
benchmarks, showcasing the straightforward applicability of swinunetr-v2 to
reach state-of-the-art without extensive hyperparameter tuning or pretraining."
1498,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"to make fair comparisons with baselines, we
did not use any pre-trained weights.datasets."
1499,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"the network is validated on five
datasets of different sizes, targets and modalities:1) the word dataset [17] the
challenge comes from segmenting small tumors from large full 3d ct images."
1500,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"the
pancreas dataset contains 281 3d ct scans with annotated pancreas and tumors (or
cysts)."
1501,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"these 20%
test data will not overlap with other folds and cover all data by 5 folds."
1502,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"random gaussian smooth,
gaussian noise, and random gamma correction are also added as additional data
augmentation."
1503,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,there are differences in data preprocessing across tasks.
1504,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"msd data
are resampled to 1 × 1x1 mm resolution and normalized to zero mean and standard
deviation (ct images are firstly clipped by .5% and 99.5% foreground intensity
percentile)."
1505,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,we follow the data split in [17] and report the test scores.
1506,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"we use the 5-fold cross-validation data split and baseline scores
from [14]."
1507,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"for msd datasets, we perform 5-fold cross-validation and ran the baseline
experiments with our codebase using exactly the same hyperparameters as
mentioned."
1508,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"we didn't compare
with leaderboard results because the purpose of the experiments is to make fair
comparisons, while not resorting to additional training data/pretraining,
postprocessing, or model ensembling."
1509,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"we perform
the study on the word dataset, and the mean test dice and hd95 scores are shown
in table 5."
1510,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"extensive experiments are performed on a variety of challenging datasets, and
swinunetr-v2 achieved promising improvements."
1511,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,"these
methods may encounter the issue of high computational complexity when analyzing
volumetric data, and most of them require manual interactions."
1512,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,"the method obtained a dice value
of 83% using the interval-slice annotation, on a testing dataset containing only
28 patients.in this study, we propose a simple yet effective weakly-supervised
strategy, by using extreme points as annotations (see fig."
1513,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,"we
evaluate our method on a collected dce-mri dataset containing 206 subjects."
1514,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,"to reduce the influence of possible incorrect label propagation, pseudo labels
for unlabeled voxels are valid only for the current iteration when they are
generated.after the fine-tune completed, the network generates binary
pseudo-masks for every training data, which are expected to be similar to the
ground-truths provided by radiologists."
1515,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,dataset.
1516,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,"we evaluated our method on an in-house breast dce-mri dataset collected
from the cancer center of sun yat-sen university."
1517,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,"we randomly divided
the dataset into 21 scans for training and the remaining scans for testing 1 ."
1518,Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,dataset and implementation.
1519,Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,"we evaluate the our uml network on two datasets
refuge [14] and ispy-1 [13]."
1520,Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,"a total of 157 patients who suffer
the breast cancer are considered -43 achieve pcr and 114 non-pcr.for each case,
we cut out the slices in the 3d image and totally got 1,570 2d images, which are
randomly divided into the train, validation, and test datasets with 1,230, 170,
and 170 slices, respectively."
1521,Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,comparison under noisy data.
1522,Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,"as can be observed that, the accuracy
of classification and segmentation significantly decreases after adding noise to
the raw data."
1523,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"our approach
achieves state-of-the-art performance on both datasets, demonstrating the
effectiveness of the proposed framework."
1524,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"this task is conducted on
dataset from isbi 2019 chaos challenge [12], which contains 20 volumes of
t2-spir mr abdominal scans."
1525,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"for both datasets, we repeat the evaluation protocols for four times and
report the average metrics and their standard deviation on test set."
1526,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"the batch sizes for both
datasets are 2."
1527,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"we benchmark our methods against previous
wsss works on two datasets in table 1 & 2, in terms of dice score, mean
intersection over union (miou), and hausdorff distance (hd95)."
1528,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"firstly, our proposed method, even without classifier guidance,
outperform all other wsss methods including the classifier guided diffusion
model cg-diff on both datasets for all three metrics."
1529,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"secondly, all wsss methods have performance
drop on kidney dataset compared with brats dataset."
1530,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"this demonstrates that the
kidney segmentation task is a more challenging task for wsss than brain tumor
task, which may be caused by the small training size and diverse appearance
across slices in the chaos dataset."
1531,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"the default setting is cg-cdm on brats dataset
with q = 400, r = 10, τ = 0.95, and s = 10."
1532,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"when compared with
latest wsss methods on two public medical image segmentation datasets, our
method shows superior performance regarding both segmentation accuracy and
inference efficiency."
1533,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,"and for semantic segmentation tasks, many methods, such as setr
[20] and segformer [21], use vit as the direct backbone network and combine it
with a taskspecific segmentation head for prediction results, reaching excellent
performance on some 2d natural image datasets."
1534,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,"however, the data in each slice is related to three views, discarding
any of them may lead to the loss of local information, which may cause the
degradation of performance [29]."
1535,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,"notably, dbtrans is designed for 3d medical images, avoiding the information
loss caused by data slicing.the contributions of our proposed method can be
described as follows: 1) based on transformer, we construct dual-branch encoder
and decoder layers that assemble two attention mechanisms, being able to model
close-window and distant-window dependencies without any extra computational
cost."
1536,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,"3) for the multi-modal data adopt in the
task of samm-bts, we improve the channel attention mechanism in se-net by
applying se-weights to features from both branches in the encoder and decoder
layers."
1537,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,"the model takes mri data of d×h ×w ×c with
four modalities stacked along channel dimensions as the input."
1538,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,"the 3d patch
embedding converts the input data to feature embedding e 1 ∈ r d 1 ×h 1 ×w 1 ×c
1 which will be further processed by encoder layers."
1539,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,datasets.
1540,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,"the
brats 2021 dataset reflects real clinical diagnostic species and has four
spatially aligned mri modality data, namely t1, t1ce, t2, and flair, which are
obtained from different devices or according to different imaging protocols."
1541,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,"the
dataset contains three distinct sub-regions of brain tumors, namely peritumoral
edema, enhancing tumor, and tumor core."
1542,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,"the data augmentation includes random
flipping, intensity scaling and intensity shifting on each axis with
probabilities set to 0.5, 0.1 and 0.1, respectively.comparative experiments."
1543,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,"finally, for the multi-modal superimposed data, we modify the channel
attention mechanism in se-net, focusing on exploring the contribution of
different modalities and branches to the effective information of feature maps."
1544,HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,"this provides useful insights that are usually
unavailable in other studies.experimental results on the brats'19 dataset
[2,3,22] show that the proposed models have superior robustness to training
image resolution than other tested models with less than 1% of their model
parameters."
1545,HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,"the dataset of brats'19 with 335 cases of gliomas was used, each with four
modalities of t1, post-contrast t1, t2, and t2-flair images with 240 × 240 × 155
voxels [3]."
1546,HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,"there is also an official validation dataset of 125 cases in the
same format without given annotations."
1547,HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,"in training, we split the training dataset (335 cases) into
90% for training and 10% for validation."
1548,HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,"in testing, each model was tested on
the official validation dataset (125 cases) with 240 × 240 × 155 voxels
regardless of the downsampling factor."
1549,HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,"although only the results of a dataset are shown because of
the page limit, the characteristics of the proposed models can be demonstrated
through this challenging multi-modal brain tumor segmentation problem."
1550,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"however, due to the lack of inductive biases, such as
weight sharing and locality, vits are more data-hungry than cnns, i.e., require
more data to train [31]."
1551,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"meanwhile, it is common to have access to multiple,
diverse, yet small-sized datasets (100 s to 1000 ss of images per dataset) for
the same mis task, e.g., ph2 [25] and isic 2018 [11] in dermatology, lits [6]
and chaos [18] in liver ct, or oasis [24] and adni [17] in brain mri."
1552,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"as each
dataset alone is too small to properly train a vit, the challenge becomes how to
effectively leverage the different datasets."
1553,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"various strategies have been
proposed to address vits' data-hunger (table 1), mainly: adding inductive bias
by constructing a hybrid network that fuses a cnn with a vit [39], imitating
cnns' shifted filters and convolutional operations [7], or enhancing spatial
information learning [22]; sharing knowledge by transferring knowledge from a
cnn [31] or pertaining vits on multiple related tasks and then fine-tuning on a
down-stream task [37]; increasing data via augmentation [34]; and non-supervised
pre-training [8]."
1554,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"nevertheless, one notable limitation in these approaches is
that they are not universal, i.e., they rely on separate training for each
dataset rather than incorporate valuable knowledge from related domains."
1555,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"as a
result, they can incur additional training, inference, and memory costs, which
is especially challenging when dealing with multiple small datasets in the
context of mis tasks."
1556,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"multi-domain learning, which trains a single universal
model to tackle all the datasets simultaneously, has been found promising for
reducing computational demands while still leveraging information from multiple
domains [1,21]."
1557,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"to the best of our knowledge, multi-domain universal models have
not yet been investigated for alleviating vits' data-hunger.given the
inter-domain heterogeneity resulting from variations in imaging protocols,
scanner manufacturers, etc."
1558,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"[4,21], directly mixing all the datasets for
training, i.e., joint training, may improve a model's performance on one dataset
while degrading performance on other datasets with non-negligible unrelated
domain-specific information, a phenomenon referred to as negative knowledge
transfer (nkt) [1,38]."
1559,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"however, those mat
techniques are built based on cnn rather than vit or are scalable, i.e., the
models' size at the inference time increases linearly with the number of
domains.to address vits' data-hunger, in this work, we propose mdvit, a novel
fixedsize multi-domain vit trained to adaptively aggregate valuable knowledge
from multiple datasets (domains) for improved segmentation."
1560,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"besides, for better representation learning across domains, we propose a novel
mutual knowledge distillation approach that transfers knowledge between a
universal network (spanning all the domains) and additional domain-specific
network branches.we summarize our contributions as follows: (1) to the best of
our knowledge, we are the first to introduce multi-domain learning to alleviate
vits' data-hunger when facing limited samples per dataset."
1561,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"(3) the experiments on 4 skin
lesion segmentation datasets show that our multi-domain adaptive training
outperforms separate and joint training (st and jt), especially a 10.16%
improvement in iou on the skin cancer detection dataset compared to st and that
mdvit outperforms state-of-the-art data-efficient vits and multi-domain learning
strategies."
1562,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"training samples {(x, y )} come from m datasets, each
representing a domain."
1563,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"we aim to build and train a single vit that performs well
on all domain data and addresses the insufficiency of samples in any of the
datasets."
1564,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"we do not employ integrated and hierarchical cnn
backbones, e.g., resnet, in base as data-efficient hybrid vits [33,39], to
clearly evaluate the efficacy of multi-domain learning in mitigating vits'
data-hunger."
1565,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"the universal
network experiences all the domains and grasps the domain-shared knowledge,
which is beneficial for peer learning.each auxiliary peer is trained on a small,
individual dataset specific to that peer (fig."
1566,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"to achieve a rapid training
process and prevent overfitting, particularly when working with numerous
training datasets, we adapt a lightweight multilayer perception (mlp) decoder
designed for vit encoders [36] to our peers' architecture."
1567,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"we study 4 skin lesion segmentation datasets collected from varied sources: isic
2018 (isic) [11], dermofit image library (dmf) [3], skin cancer detection (scd)
[14], and ph2 [25], which contain 2594, 1300, 206, and 200 samples,
respectively."
1568,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"to facilitate a fairer performance comparison across datasets, as
in [4], we only use the 1212 images from dmf that exhibited similar lesion
conditions as those in other datasets."
1569,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"1, to train all the models from
scratch on the skin datasets."
1570,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"we deploy
models on a single titan v gpu and train them for 200 epochs with the adamw [23]
optimizer, a batch size of 16, ensuring 4 samples from each dataset, and an
initial learning rate of 1×10 -4 , which changes through a linear decay
scheduler whose step size is 50 and decay factor γ = 0.5."
1571,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"in table 2-a,b, compared with base in st, base in jt improves the segmentation
performance on small datasets (ph2 and scd) but at the expense of diminished
performance on larger datasets (isic and dmf)."
1572,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"this is expected given the
non-negligible inter-domain heterogeneity between skin lesion datasets, as found
by bayasi et al."
1573,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"the above results demonstrate that shared knowledge in
related domains facilitates training a vit on small datasets while, without a
well-designed multi-domain algorithm, causing negative knowledge transfer (nkt)
due to inter-domain heterogeneity, i.e., the model's performance decreases on
other datasets."
1574,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"meanwhile, mdvit fits all the domains without nkt and
outperforms base in st by a large margin; significantly increasing dice and iou
on scd by 6.4% and 10.16%, showing that mdvit smartly selects valuable knowledge
when given data from a certain domain."
1575,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,this is expected since they are designed to reduce data requirements.
1576,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"nevertheless, in jt, these models also suffer from nkt: they perform better than
models in st on some datasets, like scd, and worse on others, like isic."
1577,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"though bat and transfuse in st have better results on
some datasets like isic, they require extra compute resources to train m models
as well as an m -fold increase in memory requirements."
1578,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"the above results
indicate that domain-shared knowledge is especially beneficial for training
relatively small datasets such as scd.we employ the two fixed-size (i.e.,
independent of m ) multi-domain algorithms proposed by rundo et al."
1579,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"we propose a new algorithm to alleviate vision transformers (vits)' datahunger
in small datasets by aggregating valuable knowledge from multiple related
domains."
1580,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"the experiments on 4 skin lesion segmentation datasets show that mdvit
outperformed sota data-efficient medical image segmentation vits and
multi-domain learning methods."
1581,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"† 28.6(.02×) mat 90.24 90.71 93.38 95.90 92.56 ± 0.52 82.97 83.31 88.06 92.19
86.64 ± 0.76 comparing against state-of-the-art (sota) methods: we conduct
experiments on
sota data-efficient mis vits and multi-domain learning methods."
1582,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"previous mis
vits mitigated the data-hunger in one dataset by adding inductive bias, e.g.,
swinunet the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8 43."
1583,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"therefore, cross-modality
generalization is needed when one imaging modality has insufficient training
data."
1584,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"these methods
perform well but do not explicitly use the unannotated target modality data to
further improve the segmentation.in this paper, we propose m-genseg, a novel
training strategy for crossmodality domain adaptation, as illustrated in fig."
1585,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"we evaluate m-genseg on a modified version of the brats
2020 dataset, in which each type of sequence (t1, t2, t1ce and flair) is
considered as a distinct modality."
1586,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"furthermore, (ii) training a second
genseg module on the target modality allows to further close the domain gap by
extending the segmentation objective to unannotated target data."
1587,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"in other words, the residual is the disentangled tumor that can
be added to the generated healthy image to create a reconstruction s pp of the
initial diseased image: like approaches in [18][19][20] we therefore generate
diseased samples from healthy ones for data augmentation."
1588,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"furthermore, note that these methods are limited to
data augmentation and do not incorporate any unannotated diseased samples when
training the segmentation network, as achieved by our model with the p→a
translation.modality translation."
1589,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"the same on-the-fly
data augmentation as in [14] was applied for all runs."
1590,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"experiments were performed on the brats 2020 challenge dataset [23][24][25],
adapted for the cross-modality tumor segmentation problem where images are known
to be diseased or healthy."
1591,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"datasets were then assembled from each distinct pair of the four
mri contrasts available (t1, t2, t1ce and flair)."
1592,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"to constitute unpaired
training data, we used only one modality (source or target) per training volume."
1593,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"all the images are provided with healthy/diseased weak labels, distinct from the
pixel-level annotations that we provide only to a subset of the data."
1594,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"however, this modified
version of the dataset provides an excellent study case for the evaluation of
any modality adaptation method for tumor segmentation."
1595,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"we used available github code for
the two baselines and performed fine-tuning on our data."
1596,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"3 the dice performance on the target modality
for (i) supervised segmentation on source data without domain adaptation, (ii)
domain adaptation methods and (iii) uagan [26], a model designed for unpaired
multi-modal datasets, trained on all source and target data."
1597,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"also, we showed that training modality
translation only on diseased data is sufficient ."
1598,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"however, doing it for healthy
data as well provides additional training examples for this task."
1599,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"likewise,
performing translation from absence to presence domain is not necessary but
makes more efficient use of the data."
1600,Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,"for the first time, eamtnet has achieved high performance with the dice
similarity coefficient (dsc) up to 90.01 ± 1.23%, and the mean absolute error
(mae) of the md, x o , y o and area are down to 2.72 ± 0.58 mm,1.87±0.76 mm,
2.14 ± 0.93 mm and 15.76 ± 8.02 cm 2 , respectively.dataset and configuration."
1601,Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,"an axial dataset includes 250 distinct subjects, each underwent initial standard
clinical liver mri protocol examinations with corresponding pre-contrast images
(t2fs [4mm]) and dwi [4mm]) was collected."
1602,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,"when channel shuffle is used, input and output
features are fully related where one convolution group takes data from other
groups, enabling more efficient feature information communication between
different groups."
1603,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,"to evaluate the proposed rcs-yolo model, we used the brain tumor detection 2020
dataset (br35h) [3], with a total of 701 images in the 'train' and 'val' two
folders, 500 images of which are the 'train' folder were selected as the
training set, while the other 201 images in the 'val' folder as the testing set."
1604,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,"the
small object is defined as the object whose pixel size is less than 32 × 32
defined by the ms coco dataset [18], so there are no small objects in the brain
tumor medical image data sets, and the scale change of the target boxes is
smooth, almost square."
1605,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,"to highlight the accuracy and rapidity of the proposed model for the detection
of brain tumor medical image data set, table 1 shows the performance comparison
between our proposed detector and other state-of-the-art object detectors."
1606,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,"the
time duration of fps includes data preprocessing, forward model inference, and
post-processing."
1607,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,"evaluation of the brain mri dataset
shows superior performance for brain tumor detection in terms of both speed and
precision, as compared to yolov6, yolov7, and yolov8 models."
1608,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,"evaluation on a publicly
available brain tumor detection annotated dataset shows superior detection
accuracy and speed compared to other state-of-the-art yolo architectures."
1609,Certification of Deep Learning Models for Medical Image Segmentation,"by extension, we
show that current diffusion models, trained on 'classical images' generalize
well to medical datasets for denoising tasks."
1610,Certification of Deep Learning Models for Medical Image Segmentation,"extensive experiments on five
public medical datasets of chest x-rays [21,31], skin lesions [10], and
colonoscopies [6], and different popular segmentation models, prove the
potential of our method."
1611,Certification of Deep Learning Models for Medical Image Segmentation,"in this paper, we propose to leverage randomized smoothing and
diffusion models for certified segmentation on medical datasets, setting the
first baseline for this challenging problem and certifying popular segmentation
architectures."
1612,Certification of Deep Learning Models for Medical Image Segmentation,"this method consists in adding random noise (e.g., noise
generated from a gaussian distribution) to the input data and then classifying
the perturbed data using the neural network."
1613,Certification of Deep Learning Models for Medical Image Segmentation,"let d = x × y denote the data
distribution where x ⊂ r d and y = {1, ."
1614,Certification of Deep Learning Models for Medical Image Segmentation,"first, a network, h : x → x , is trained to denoise
the data such that for η ∼ n (0, σ 2 i), we have h(x + η) ≈ x, then, the output
of the denoiser is given to the classifier.in this paper, we leverage randomized
smoothing and diffusion probabilistic models to obtain state-of-the-art results
on certified segmentation for medical imaging."
1615,Certification of Deep Learning Models for Medical Image Segmentation,"the reverse process then starts
from random noise and generates a new image that conforms to the data
distribution."
1616,Certification of Deep Learning Models for Medical Image Segmentation,"randomized smoothing needs a data point that is enhanced with
gaussian noise added to it, given by x rs = x + δ with δ ∼ n (x, σ 2 i)."
1617,Certification of Deep Learning Models for Medical Image Segmentation,datasets: we perform experiments on 5 different publicly available datasets.
1618,Certification of Deep Learning Models for Medical Image Segmentation,"all
datasets were divided to 70% for training, 10% for validation, and 20% for
testing."
1619,Certification of Deep Learning Models for Medical Image Segmentation,"the testing set is the one used to compute certified results.chest
x-rays datasets: jsrt dataset [31] with annotations of lung, heart, and
clavicles provided by [35] is used."
1620,Certification of Deep Learning Models for Medical Image Segmentation,this dataset contains 247 images.
1621,Certification of Deep Learning Models for Medical Image Segmentation,"for lung
segmentation only, we use both the montgomery and shenzen datasets [21]."
1622,Certification of Deep Learning Models for Medical Image Segmentation,this dataset consists of 2694 rgb dermatoscopy images.
1623,Certification of Deep Learning Models for Medical Image Segmentation,"colonoscopy images: cvc-clinicdb dataset [6] containing 612 colonoscopy images
in rgb together with their annotations were utilized.implementation details: we
train three different segmentation models namely, a unet [28], a resunet++ [22],
and a deeplabv2 [9] with and without noise."
1624,Certification of Deep Learning Models for Medical Image Segmentation,"our code is made publicly available at:
https://github.com/othmanela/medical_cert_seg.results and discussion: for all
five datasets, we compute a certified dice score and certified mean intersection
over union (iou)."
1625,Certification of Deep Learning Models for Medical Image Segmentation,"we also report the percentage of abstentions (% ) representing
the mean number of pixels on which the model's prediction confidence was
insufficient with respect to the radius r.the lower the percentage of
abstentions the better the segmentation model is.in table 1, we compare our
method using 3 different and popular architectures (unet, resunet++, and
deeplabv2) on the chest x-rays datasets."
1626,Certification of Deep Learning Models for Medical Image Segmentation,"a comparison of our method and segcertify using the
resunet++ architecture is presented in table 2 for the three chest x-ray
datasets."
1627,Certification of Deep Learning Models for Medical Image Segmentation,"1 for our proposed
method and segcertify for the different datasets and different levels of noise."
1628,Certification of Deep Learning Models for Medical Image Segmentation,"we train
three unet models (one for each noise level) on the jsrt dataset."
1629,Certification of Deep Learning Models for Medical Image Segmentation,"in this paper, we present the first work on certified segmentation for medical
imaging, and extensively evaluate it on five different datasets and three deep
learning segmentation models."
1630,Certification of Deep Learning Models for Medical Image Segmentation,"our technique leverages off-the-shelf denoising
and segmentation models and provides the highest certified dice and miou on
multi-class and binary segmentation of five different datasets."
1631,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"the unlabeled image pool can be quickly enriched via the support from partner
clinical centers with low barriers of entry (only unlabeled images are required)
data heterogeneity due to different scanners, scanning protocols and subject
groups, which violate the typical ssl assumption of i."
1632,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"yet, their success usually requires a
large amount of labeled medical data, which is expensive and expertise-demanding
in practice."
1633,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"in this regard, semi-supervised learning (ssl) has emerged as an
attractive option as it can leverage both limited labeled data and abundant
unlabeled data [3,[9][10][11]15,16,[21][22][23][24][25][26]28]."
1634,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"nevertheless,
the effectiveness of ssl is heavily dependent on the quantity and quality of the
unlabeled data.regarding quantity , the abundance of unlabeled data serves as a
way to regularize the model and alleviate overfitting to the limited labeled
data."
1635,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"taking c1 as a case
study, if the amount of local unlabeled data is limited, existing ssl methods
may still suffer from inferior performance when generalizing to unseen test data
(fig."
1636,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"yet, due to differences
in imaging protocols and variations in patient demographics, this solution
usually introduces data heterogeneity, lead-ing to a quality problem."
1637,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"such
heterogeneity may impede the performance of ssl which typically assumes that the
distributions of labeled data and unlabeled data are independent and identically
distributed (i.i.d.) [16]."
1638,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"however, it only deals with additional unlabeled data from a specific source
rather than multiple arbitrary sources."
1639,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"specifically,
cu2l is built upon the teacher-student architecture with customized learning
strategies for local and external unlabeled data: (i) recognizing the importance
of supervised learning in data distribution fitting (which leads to the failure
of cps [3] in ms-ssl as elaborated in sec."
1640,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"3), the local unlabeled data is
involved into pseudolabel supervised-like learning to reinforce fitting of the
local data distribution; (ii) considering that intra-class variance hinders
effective ms-ssl, we introduce a non-parametric unlabeled-to-labeled learning
scheme, which takes advantage of the scarce expert labels to explicitly
constrain the prototype-propagated predictions, to help the model exploit
discriminative and domain-insensitive features from heterogeneous multi-site
data to support the local center."
1641,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"yet, observing that such scheme is challenging
when significant shifts and various distributions are present, we further
propose category-level regularization, which advocates prototype alignment, to
regularize the distribution of intra-class features from arbitrary external data
to be closer to the local distribution; (iii) based on the fact that
perturbations (e.g., gaussian noises [15]) can be regarded as a simulation of
heterogeneity, perturbed stability learning is incorporated to enhance the
robustness of the model."
1642,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"our method is evaluated on prostate mri data from six
different clinical centers and shows promising performance on tackling ms-ssl
compared to other semi-supervised methods."
1643,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"in our scenario of ms-ssl, we have access to a local target dataset d local
(consisted of a labeled sub-set d l local and an unlabeled sub-set d u local )
and the external unlabeled support datasets d u e = m j=1 d u,j e , where m is
the number of support centers."
1644,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"as such, our task of ms-ssl can be formulated as
optimizing the following loss:where l l sup is the supervised guidance from
local labeled data and l u denotes the additional guidance from the unlabeled
data."
1645,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"the key challenge of ms-ssl is the proper design of l u for
robustly exploiting multi-site unlabeled data {d u local , d u e } to support
the local center."
1646,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"as mentioned above, supervised-like learning is advocated for local unlabeled
data to help the model fit local distribution better."
1647,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"yet, with limited local labeled data for
training, it is difficult to generate high-quality pseudo labels."
1648,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"with the argmax pseudo label ŷ u,t e and the
predicted probability map p u,t e , the object prototype from the external
unlabeled data can be computed via confidence-weighted masked average pooling:
c.likewise, the background prototype c u(bg) e can also be obtained."
1649,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"note that a similar
procedure can also be applied to the local unlabeled data x u local , and thus
we can obtain another prototype-propagated unlabeledto-labeled prediction p u2l
local for x l local ."
1650,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"specifically, we introduce category-level regularization, which advocates class
prototype alignment between local and external data, to regularize the
distribution of intra-class features from arbitrary external data to be closer
to the local one, thus reducing the difficulty of u2l learning."
1651,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"in u2l, we have
obtained prototypes from local unlabeled data {c where mean squared error is
adopted as the distance function d(•, •)."
1652,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"specifically, for the same unlabeled input x u ∈ {d u local ∪ d u e } with
different perturbations ξ and ξ (using the same gaussian noises as in [26]), we
encourage consistent pre-softmax predictions between the teacher and student
models, formulated aswhere mean squared error is also adopted as the distance
function d(•, •).overall, the final loss for the multi-site unlabeled data is
summarized as: materials."
1653,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"rizes the
characteristics of the six data sources, following [7,8], where [7,8] also
reveal the severity of inter-center heterogeneity here through extensive
experiments."
1654,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"data augmentation is applied, including random flip and rotation."
1655,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"as observed,
compared to the supervised-only baselines, our cu2l with {6, 8} local labeled
scans achieves {19.15%, 17.42%} and {9.1%, 6.44%} dsc improvements in {c1, c2},
showing its effectiveness in leveraging multi-site unlabeled data."
1656,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"data, existing ssl methods can still
benefit from the external unlabeled data to some extent compared to the results
using local data only as shown in fig."
1657,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"1, revealing that the quantity of
unlabeled data has a significant impact."
1658,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"however, due to the lack of proper
mechanisms for learning from heterogeneous data, limited improvement can be
achieved by them, especially for cps [3] and fixmatch [14] in c2."
1659,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"particularly,
cps relies on cross-modal pseudo labeling which exploits all the unlabeled data
in a supervised-like fashion."
1660,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"we attribute its degradation to the fact that
supervised learning is crucial for distribution fitting, which supports our
motivation of performing pseudo-label learning on local unlabeled data only."
1661,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"in
contrast, with specialized mechanisms for simultaneously learning informative
representations from multi-site data and handling heterogeneity, our cu2l
obtains the best performance over the recent ssl methods."
1662,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"cu2l-2 represents the removal of both l u2l and l cr ,
and it can be observed that such an unlabeled-to-labeled learning approach
combined with class-level regularization is crucial for exploring multi-site
data."
1663,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"in this work, we presented a novel category-level regularized
unlabeled-to-labeled (cu2l) learning framework for semi-supervised prostate
segmentation with multi-site unlabeled mri data."
1664,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"cu2l robustly exploits
multi-site unlabeled data via three tailored schemes: local pseudo-label
learning for better local distribution fitting, category-level regularized
unlabeled-to-labeled learning for exploiting the external data in a
distribution-insensitive manner and stability learning for further enhancing
robustness to heterogeneity."
1665,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"we evaluated our method on prostate mri data from
six different clinical centers and demonstrated its superior performance
compared to other semi-supervised methods."
1666,A Sheaf Theoretic Perspective for Robust Prostate Segmentation,"deep learning models are
susceptible to textural shifts and artefacts which is often seen in mri due to
variations in the complex acquisition protocols across multiple sites [12].the
most common approach to tackle domain shifts is with data augmentation
[16,33,35] and adversarial training [11,30]."
1667,A Sheaf Theoretic Perspective for Robust Prostate Segmentation,"topological data analysis is a field which extracts topological features from
complex data structures embedded in a topological space."
1668,A Sheaf Theoretic Perspective for Robust Prostate Segmentation,"the cubical complex c is naturally
equipped to deal with topological spaces represented as volumetric grid
structured data such as images [32]."
1669,A Sheaf Theoretic Perspective for Robust Prostate Segmentation,"sheaf theory provides a way of composing or 'gluing' local
data together to build a global object (new data) that is consistent with the
local information [8]."
1670,A Sheaf Theoretic Perspective for Robust Prostate Segmentation,"formally,
a sheaf is a mathematical object which attaches to each open subset or subspace,
u in a topological space, y an algebraic object like a vector space or set
(local data) such that it is well-behaved under restriction to smaller open sets
[8].we can consider a topological space, y such as a segmentation output divided
into a finite number of subspaces, {∅, y 1 , y 2 ...y n } which are the base
spaces for y or equivalently the patches in a segmentation map."
1671,A Sheaf Theoretic Perspective for Robust Prostate Segmentation,"effective data augmentation
techniques, such as cutout [16], mixup [34] and bigaug [35] offer a
straightforward approach to enhance the generalisability of segmentation models
across different domains."
1672,A Sheaf Theoretic Perspective for Robust Prostate Segmentation,"the novel approach of topological auto-encoders [27] marks the first instance of
incorporating persistent homology to maintain the topological structure of the
data manifold within the latent representation."
1673,A Sheaf Theoretic Perspective for Robust Prostate Segmentation,"the total loss to train our entire
framework is: in the task of anatomical segmentation, the first two columns of
table 1 show
the results for the domain shift from runmc in the decathlon dataset to
bmc.here, we demonstrate that our method improves segmentation performance in
all evaluation metrics compared to the baseline, nn-unet and the other sdg
methods."
1674,A Sheaf Theoretic Perspective for Robust Prostate Segmentation,"similar findings are noted for the domain shift from the internal
dataset to the runmc data in the prostatex2 dataset (second two columns of table
1).in table 2, we note our method significantly improves tumour segmentation and
localisation performance."
1675,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"however, transformers are plagued by
the necessity of large annotated datasets to maximize performance benefits owing
to their limited inductive bias."
1676,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"while such datasets are common to natural
images (imagenet-1k [6], imagenet-21k [26]), medical image datasets usually
suffer from the lack of abundant high quality annotations [19]."
1677,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"the authors
paired large kernel con-vnext networks with enormous datasets to outperform
erstwhile state-of-the-art transformer-based networks."
1678,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"out-of-the-box
data-efficient solutions such as nnunet [13], using variants of a standard unet
[5], have still remained effective across a wide range of tasks.the convnext
architecture marries the scalability and long-range spatial representation
learning capabilities of vision [7] and swin transformers [21] with the inherent
inductive bias of convnets."
1679,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"to achieve this would require
techniques to combat the tendency of large networks to overfit on limited
training data."
1680,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"however, 3d-ux-net only uses these blocks partially in a
standard convolutional encoder, limiting their possible benefits.in this work,
we maximize the potential of a convnext design while uniquely addressing
challenges of limited datasets in medical image segmentation."
1681,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"compression layer: convolution
layer with 1 × 1 × 1 kernel and c output channels performing channel-wise
compression of the feature maps.mednext is convolutional and retains the
inductive bias inherent to conv-nets that allows easier training on sparse
medical datasets."
1682,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"convnext architectures
in classification of natural images, despite the benefit of large datasets such
as imagenet-1k and imagenet-21k, are seen to saturate at kernels of size 7 × 7 ×
7 [22]."
1683,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"medical image segmentation tasks have significantly less data and
performance saturation can be a problem in large kernel networks."
1684,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"this leads to a simple but effective
initialization technique for med-next which helps large kernel networks overcome
performance saturation in the comparatively limited data scenarios common to
medical image segmentation."
1685,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"our experimental framework
uses the nnunet [13] as a backbone -where the training schedule (epochs = 1000,
batches per epoch = 250), inference (50% patch overlap) and data augmentation
remain unchanged."
1686,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"the data is resampled to 1.0 mm isotropic spacing during training and
inference (with results on original spacing), using input patch size of 128 ×
128 × 128 and 512 × 512, and batch size 2 and 14, for 3d and 2d networks
respectively."
1687,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"we use 4 popular tasks, encompassing organ as well as tumor segmentation tasks,
to comprehensively demonstrate the benefits of the mednext architecture -1)
beyond-the-cranial-vault (btcv) abdominal ct organ segmentation [16], 2) amos22
abdominal ct organ segmentation [14] 3) kidney tumor segmentation challenge 2019
dataset (kits19) [11], 4) brain tumor segmentation challenge 2021 (brats21) [1]."
1688,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"btcv, amos22 and kits19 datasets contain 30, 200 and 210 ct volumes with 13, 15
and 2 classes respectively, while the brats21 dataset contains 1251 mri volumes
with 3 classes."
1689,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"we ablate the mednext-b configuration on amos22 and btcv datasets to highlight
the efficacy of our improvements and demonstrate that a vanilla convnext is
unable to compete with existing segmentation baselines such as nnunet."
1690,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"we further
establish the performance of the mednext architecture against our baselines
-comprising of convolutional, transformer-based and large kernel baselines -on
all 4 datasets."
1691,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"in 5-fold cv scores in table 2,
med-next, with 3 × 3 × 3 kernels, takes advantage of depth and width scaling to
provide state-of-the-art segmentation performance against every baseline on all
4 datasets with no additional training data."
1692,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"1c), 5-fold ensembles for
mednext-l (kernel: 5 × 5 × 5) and nnunet, its strongest competitor are compared
-1) btcv: mednext beats nnunet and, to the best of our knowledge, is one of the
leading methods with only supervised training and no extra training data (dsc:
88.76, hd95: 15.34), 2) amos22: mednext not only surpasses nnunet, but is also
rank 1 (date: 09.03.23) currently on the leaderboard (dsc: 91.77, nsd: 84.00),
3) kits19: mednext exceeds nnunet performance (dsc: 91.02), 4) brats21: mednext
surpasses nnunet in both volumetric and surface accuracy (dsc: 88.01, hd95:
10.69)."
1693,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"in comparison to natural image analysis, medical image segmentation lacks
architectures that benefit from scaling networks due to inherent domain
challenges such as limited training data."
1694,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"in this work, mednext is presented as
a scalable transformer-inspired fully-convnext 3d segmentation architecture
customized for high performance on limited medical image datasets."
1695,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"this is primarily due to their ability to
learn informative hierarchical features directly from data."
1696,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"in the proposed kspc-net, a cnn is employed
to learn directly from the data to produce the pixel-wise bandwidth feature map
and initial segmentation map, which are used to define the tuning parameters in
the kspc module."
1697,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"more
specifically, we use the classic unet [17] as the cnn backbone and evaluate our
kspc-net on the publicly available miccai hecktor (head and neck tumor
segmentation) challenge 2021 dataset."
1698,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"1 can be
further simplified asa scaled kernel is positioned so that its mode coincides
with each data point x i which is expressed mathematically as k h (x-x i )."
1699,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"( 2) as the probability mass of the data point x which is
estimated by smoothing the suvs of the local neighbourhood using the gaussian
kernel."
1700,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"this
provides a robust definition of probability under the perturbation of the input
data."
1701,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"first, the initial
segmentation map and pixel-level bandwidth parameter map h(x i1 , x i2 ) of kspc
are learned from data by the cnn backbone."
1702,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"the dataset is from the hecktor challenge in miccai 2021 (head and neck tumor
segmentation challenge)."
1703,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"the hecktor training dataset consists of 224 patients
diagnosed with oropharyngeal cancer [1]."
1704,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"table 1 shows the quantitative
comparison of different approaches on hecktor dataset."
1705,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"promising performance was achieved by
our proposed kspc-net compared to the state-of-the-art approaches on the miccai
2021 challenge dataset (hecktor)."
1706,A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,"although our fusion idea is quite simple, a2fseg achieves
state-of-the-art (sota) performance in the incomplete multimodal brain tumor
image segmentation task on the brats2020 dataset."
1707,A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,"-we conduct
experiments on the brats 2020 dataset and achieve the sota segmentation
performance, having a mean dice core of 89.79% for the whole tumor, 82.72% for
the tumor core, and 66.71% for the enhancing tumor."
1708,A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,"the dataset is randomly
split into 70% for training, 10% for validation, and 20% for testing, and all
methods are evaluated on the same dataset and data splitting."
1709,A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,"figure 2 visualizes the segmentation results of samples from the brats2020
dataset."
1710,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"concerning the inherent issue of imbalanced tumor types in the training data
collected in clinic, a novel ordinal manifold mixup based feature augmentation
is presented and applied in the training stage of the tumor subtyping network."
1711,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"in this way, inconsistency between the augmented features and the corresponding
labels can be effectively reduced.our method is evaluated using pre-operative
multimodal mr brain images of 1726 diffuse glioma patients collected from
cooperation hospitals and a public dataset brats2019 [12] containing multimodal
mr brain images of 210 patients."
1712,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"to solve the inherent issue of
imbalanced tumor type in the training data collected in clinic, a novel ordinal
manifold mixup based feature augmentation is applied in the training of the
tumor subtyping network."
1713,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"it is worth noting that the ground truth of tumor
types, which is determined after craniotomy, is available in the training data,
while for the testing data, tumor types are not required, because
tumor-type-related features can be learned from the pre-operative multimodal mr
brain images."
1714,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"assume that d = {x 1 , ...,
x n } is the dataset containing pre-operative multimodal mr brain images of
diffuse glioma patients, and n is the number of patients."
1715,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"in the in-house dataset, the proportions of the three tumor types are
20.9% (oligodendroglioma), 28.7% (astrocytoma), and 50.4% (glioblastoma), which
is consistent with the statistical report in [13]."
1716,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"in our experiment, both in-house and public datasets are used to evaluate our
method."
1717,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"specifically, the in-house dataset collected in cooperation hospitals
contains pre-operative multimodal mr images, including t1, t1 contrast enhanced
(t1c), t2, and flair, of 1726 patients (age 49.7 ± 13.1) with confirmed diffuse
glioma types."
1718,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"besides the inhouse
dataset, a public dataset brats2019, including pre-operative multimodal mr
images of 210 non-censored patients (age 61.4 ± 12.2), is adopted as the
external independent testing dataset."
1719,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"all images of the in-house and brats2019
datasets go through the same pre-processing stage, including image normalization
and affine transformation to mni152 [17]."
1720,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"concordance index (c-index) is adopted to quantify the
prediction accuracy:where d = {x 1 , ..., x n } is the dataset containing all
patients, t i and t j are ground truth of survival times of the i-th and j-th
patients, r i and r j are the days predicted by rf, mcsp, and pgsp or risks
predicted by the deep cox proportional hazard models (i.e., deepconvsurv and our
method), 1 x<y = 1 if x < y, else 0, and δ i = 0 or 1 when the i-th patient is
censored or non-censored."
1721,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"as rf, mcsp, and pgsp cannot use the censored data in
the in-house dataset, 80% of the non-censored data (594 patients) are randomly
selected as the training data, and the rest 20% non-censored data (149 patients)
are for testing."
1722,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"so besides the 80%
non-censored patients, all censored data (983 patients) are also included in the
training data.table 1 shows the evaluation results of the in-house and the
external independent (brats2019) testing datasets using all methods under
evaluation."
1723,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"for the in-house dataset, the
resulting c-indices are 0.744 (baseline-1) and 0.735 (baseline-2)."
1724,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"for the
external independent testing dataset brats2019, the resulting c-indices are
0.738 (baseline-1) and 0.714 (baseline-2), and our method still has more than 6%
improvement comparing with baseline-2.figure 3 shows the distributions of
tumor-type-related features (after the gap) of the in-house testing data in
baseline-2, and our method."
1725,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"both in-house and public
datasets containing 1936 patients were used in the experiment."
1726,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"while some approaches aim to optimize the model architecture
by incorporating local and global contexts and multi-task supervision, and
others seek to improve performance by collecting more labeled data and building
larger models, both strategies are costly and can be limited by the inherent
complexity of skin lesion boundaries."
1727,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"we evaluate our
model on two popular skin lesion segmentation datasets, isic-2016 and ph 2
datasets, and find that it performs significantly better than existing models."
1728,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"finally, the segmentation map is generated as3 experiment datasets: we use two
publicly available skin lesion segmentation datasets from
different institutions in our experiments: the isic-2016 dataset and the ph 2
dataset."
1729,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"the isic-2016 dataset [8] is provided by the international skin imaging
collaboration (isic) archive and consists of 900 samples in the public training
set and 379 samples in the public validation set."
1730,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"as the annotation for its
public test set is not currently available, we additionally collect the ph 2
dataset [13], which contains 200 labeled samples and is used to evaluate the
generalization performance of our methods.evaluation metrics: to comprehensively
compare the segmentation results, particularly the boundary delineations, we
employ four commonly used metrics to quantitatively evaluate the performance of
our segmentation methods."
1731,Medical Boundary Diffusion Model for Skin Lesion Segmentation,regarding dataset.
1732,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"we use a set of random
augmentations, including vertical flipping, horizontal flipping, and random
scale change (limited to 0.9 ∼ 1.1), to augment the training data."
1733,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"the quantitative results are shown in table
1, which reports four evaluation scores for two datasets."
1734,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"moreover, our method shows a larger improvement
in generalization performance on the ph 2 dataset, indicating its better ability
to handle new data."
1735,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"2, including three samples from the isic-2016 validation set and three from
the ph 2 dataset."
1736,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"our
method is evaluated on two well-known skin lesion segmentation datasets, and the
results demonstrate superior performance and generalization ability in unseen
domains."
1737,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,"with the emergence of multimodal datasets (e.g., brats [25]
and hecktor [1]), various deep-learning-based multimodal image segmentation
methods have been proposed [3,10,13,27,29,31]."
1738,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,"as a typical approach,
input-level fusion [8,20,26,31,34] refers to concatenating multimodal images in
the channel dimension as network input during the data processing or
augmentation stage."
1739,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,"the core idea is to train an independent segmentation
network for each data modality and fuse the results in a specific way."
1740,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,"extensive experimental results on two publicly available datasets
demonstrate the effectiveness of our proposed method."
1741,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,"hecktor21 is a dualmodality dataset
for head and neck tumor segmentation, containing 224 pet-ct image pairs."
1742,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,"we randomly select 180 samples
for each dataset as the training set and the rest as the independent test set
(44 cases for hecktor21 and 40 cases for pi-cai22)."
1743,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,"online data augmentation,
including random rotation and flipping, is performed to alleviate the
overfitting problem."
1744,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,"it is worth noting that
the performance of hybrid models such as unetr is not as good as expected, even
worse than 3d u-net, perhaps due to the small size of the dataset."
1745,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,dataset.
1746,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"we evaluated the applicability of our approach across multiple
modalities by conducting evaluations on microscopy and histology datasets."
1747,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"the
private dataset contains 300 images sized at 512 × 512 tessellated from 50 wsis
scanned at 20×, and meticulously labeled by five pathologists according to the
labeling guidelines of the monuseg [10]."
1748,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"for both datasets, we randomly split
80% of the samples on the patient level as the training set and the remaining
20% as the test set."
1749,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"more
importantly, our method can outperform swinunet and the previous methods on both
datasets."
1750,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"for example, in the histology image dataset, transnuseg improves the
dice score, f1 score, accuracy, and iou by 2.08%, 3.41%, 1.25%, and 2.70%
respectively, over the second-best models."
1751,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"similarly, in the fluorescence
microscopy image dataset, our proposed model improves dsc by 0.96%, while also
leading to 1.65%, 1.03% and 1.91% increment in f1 score, accuracy, and iou to
the second-best performance."
1752,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"to further show the effectiveness of these schemes, as well as
consistency self distillation, we conduct a comprehensive ablation study on both
datasets."
1753,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"experimental results on two datasets demonstrate the excellence of our
transnuseg against state-of-the-art counterparts for potential real-world
clinical deployment."
1754,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"multi-modal learning has become a popular research area in computer vision and
medical image analysis, with modalities spanning across various media types,
including texts, audio, images, videos and multiple sensor data."
1755,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"in fact, variational auto-encoder (vae) has been adopted to generate data from
other modalities in the image or feature domains [3,11]."
1756,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"[20] aimed
to learn a unified subspace for incomplete and unlabelled multi-view data."
1757,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"let us represent the n -modality data with m l = {x∈ x denotes the l th data
sample and the superscript (i) indexes the modality."
1758,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"1.multi-modal segmentation is composed not only of multiple modalities, but also
of multiple tasks, such as the three types of tumours in brats2018 dataset that
represent the three tasks."
1759,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"our model and competing methods are evaluated on the brats2018 segmentation
challenge dataset [1,14]."
1760,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"the dataset consists of 3d multi-modal brain mris, including flair, t1, t1
contrast-enhanced (t1c), and t2, with ground-truth annotations."
1761,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"the dataset
comprises 285 cases for training, and 66 cases for evaluation."
1762,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"we trained the lckd model for 115,000 iterations and use 20% of the
training data as the validation task for teacher election."
1763,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"we believe that our
proposed lckd has the potential to allow the use of multimodal data for training
and missingmodality data per testing."
1764,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"in this framework, the test
image is registered to a preselected reference dataset with known ground-truth
segmentation."
1765,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"the quality of a query segmentation is assessed by warping the
query image to the reference dataset."
1766,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"we maintained the main structure
of the vanilla 2d resnet-34 [4] but made the following modifications, which were
necessary to account for the 3d nature of the input data (see fig."
1767,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"the
two parts are combined using a weight parameter λ to balance the different loss
components:3 experimentsfor this study, pre-operative multimodal mri scans of
varying grades of glioma were obtained from the 2021 brain tumor segmentation
(brats) challenge [1] training dataset (n = 1251)."
1768,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"pre-contrast t1-weighted (t1), t2-weighted (t2), post-contrast
t1-weighted (t1c), and fluid attenuated inversion recovery (flair) are included
in the dataset."
1769,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"all data were already registered to a standard anatomical atlas and
skull-stripped."
1770,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"all the data was first cropped to non-zero value regions, and
then zero-padded to a size of 160 × 192 × 160 to be fed into the network."
1771,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"the initial dataset was expanded by producing segmentation results at different
levels of quality to provide an unbiased estimation of segmentation quality."
1772,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"second, to further
enrich our dataset with segmentations of diverse quality, we sampled
segmentations along the training routines at different iterations."
1773,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"third, we devised a method called seggen that applied
image transformations, including random rotation (angle = [-15 • , 15 • ]),
random scaling (scale = [0.85, 1.25]), random translation (moves = [-20, 20]),
and random elastic deformation (displacement = [0, 20]), to the ground-truth
segmentations with a probability of 0.5, resulting in three segmentations for
each subject.the original brats 2021 training dataset was split into training (n
= 800), validation (n = 200), and testing (n = 251) sets."
1774,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"however, this generated dataset suffered from
imbalance (fig."
1775,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"training using such an imbalanced dataset is prone to
producing biased models that do not generalize well."
1776,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"using the quantile transform,
the data generator first transformed the distribution of the generated dsc to a
uniform distribution."
1777,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"next, the generated samples closest to the transformed
uniform distribution in terms of euclidean distance were chosen to form the
resampled dataset."
1778,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"data augmentation, including random rotation, random scaling,
random mirroring, random gaussian noise, and gamma intensity correction, was
applied to prevent overfitting during training."
1779,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,"b) respectively show the visualization of comparative experimental
results on the isic2017 and isic2018 datasets."
1780,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,datasets and implementation details.
1781,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,"to assess the efficacy of our model, we
select two public skin lesion segmentation datasets, namely isic2017 [1,3] and
isic2018 [2,6], containing 2150 and 2694 dermoscopy images, respectively."
1782,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,"consistent with prior research [19], we randomly partition the datasets into
training and testing sets at a 7:3 ratio."
1783,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,"we apply various data
augmentation, including horizontal flipping, vertical flipping, and random
rotation."
1784,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,"to evaluate our method, we
employ mean intersection over union (miou), dice similarity score (dsc) as
metrics, and we conduct 5 times and report the mean and standard deviation of
the results for each dataset."
1785,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,"table 1 reveal that our ege-unet exhibits a comprehensive state-of-the-art
performance on the isic2017 dataset."
1786,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,"for the isic2018 dataset, the performance of our
model also outperforms that of the best-performing model."
1787,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"in this manner, our latent kinetic code can be interpreted to
provide tic information and hemodynamic characteristics for accurate cancer
segmentation.we verify the effectiveness of our proposed diffusion kinetic model
(dkm) on dce-mri-based breast cancer segmentation using breast-mri-nact-pilot
dataset [13]."
1788,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"based on the consideration from nonequilibrium thermodynamics, ddpm
approximates the data distribution by learning a markov chain process which
originates from the gaussian distribution."
1789,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"the forward diffusion process
gradually adds gaussian noise to the data x 0 according to a variance schedule β
1 , ..., β t [8]:particularly, a noisy image x t can be directly obtained from
the data x 0 :where α t := 1β t and ᾱt := t s=1 α s ."
1790,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"next, we employ the
reverse diffusion process to transform the noisy sample x t to the post-contrast
data x k ."
1791,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"in particular, the diffusion loss for the
reverse diffusion process can be formulated as follows:where θ represents the
denoising model that employs an u-net structure, x 0 and x k are the
pre-contrast and post-contrast images, respectively, is gaussian distribution
data ∼ n (0, i), and t is a timestep."
1792,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"dataset: to demonstrate the effectiveness of our proposed dkm, we evaluate our
method on 4d dce-mri breast cancer segmentation using the breast-mri-nact-pilot
dataset [13], which contains a total of 64 patients with the contrastenhanced
mri protocol: a pre-contrast scan, followed by 2 consecutive postcontrast time
points (as shown in fig."
1793,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"we divided the
original dataset into training (70%) and test set (30%) based on the scans."
1794,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"ground truth segmentations of the data are provided in the dataset for tumor
annotation."
1795,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,no data augmentation techniques are used to ensure fairness.
1796,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"no data augmentation techniques are used to ensure
fairness.comparison with sota methods: the quantitative comparison of the
proposed method to recent state-of-the-art methdos is reported in table 1."
1797,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"this is useful when post-contrast data is
limited.ablation study: to explore the effectiveness of the latent kinetic code,
we first conduct ablation studies to select the optimal setting."
1798,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"however, such methods typically rely on large-scale annotated
image datasets, which usually require significant effort and expertise from
pathologists and can be prohibitively expensive [28].to reduce the annotation
cost, developing annotation-efficient methods for semantic-level gland
segmentation has attracted much attention [10,18,23,37]."
1799,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"(3) we validate the efficacy of our mssg on two public glandular
datasets (i.e., the glas dataset [27] and the crag dataset [13]), and the
experiment results demonstrate the effectiveness of our mssg in unsupervised
gland segmentation."
1800,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"we evaluate our mssg on the gland segmentation challenge (glas) dataset [27] and
the colorectal adenocarcinoma gland (crag) dataset [13]."
1801,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"the glas dataset
contains 165 h&e-stained histopathology patches extracted from 16 wsis."
1802,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"the crag
dataset owns 213 h&e-stained histopathology patches extracted from 38 wsis."
1803,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"the
crag dataset has more irregular malignant glands, which makes it more difficult
than glas, and we would like to emphasize that the results on crag are from the
model trained on glas without retraining."
1804,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"on the glas dataset, the end-to-end clustering methods (denoted by ""
* "") end up with limited improvement over a randomly initialized network."
1805,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"on crag dataset, even in
the absence of any hints, mssg still outperforms all unsupervised methods and
even some of the fully-supervised methods."
1806,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"on
both datasets, mssg obtains more accurate and complete results."
1807,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"our model has been evaluated on both the publicly brats 2020 dataset and
a private medulloblastoma segmentation dataset."
1808,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"our approach combines the strengths of cnn
and transformer to create a more powerful encoder that can extract both local
and global information from input data.we address the computational and memory
complexity issues that arise from 3d input by replacing the vanilla attention
with our extended 3d efficient attention."
1809,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"in the inference stage, the output feature f is produced
by a normal 3 × 3 × 3 convolution as follows: in order to validate the
performance of eoformer, we conduct extensive
experiments on both the publicly available brats 2020 dataset and a private
medulloblastoma segmentation dataset (medseg)."
1810,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"the brats 2020 dataset [14]
consists of mri image data from 369 patients, with each patient having four
modalities (t1, t1ce, t2 and t2-flair) of skull-striped mri, which are aligned
to a standard brain template."
1811,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"the training/validation/test split follows
315/16/37 according to recent works [10,23].the medseg dataset includes mri
images of t1, t1ce, t2, and t2 flair modalities from 255 patients with
medulloblastoma."
1812,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"the dataset includes manual annotations of the wt and et
regions."
1813,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,four-fold cross-validation is performed on this dataset.
1814,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"for data
augmentation, we apply image croping, flipping, identity scaling and shifting."
1815,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"the results are reproduced
on our data split.table 1 displays the performance comparison of eoformer
against other methods on the brats 2020 dataset."
1816,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"first, there lacks of a well-segmented
dataset with manual labels on lyme disease."
1817,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"on one hand, some datasets-such as
ham10000 [10] and isbi challenges [11]-have manual annotated segmentations for
diseases like melanoma, but they do not have lyme disease lesions."
1818,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"on the other
hand, some datasets-such as groh et al."
1819,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"furthermore, clinical data collected for training is
usually imbalanced in some properties, e.g., more samples with light skins
compared with dark skins."
1820,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"therefore, existing skin disease segmentation [13] as
well as existing general segmentation works, such as u-net [14], polar training
[15], vit-adapter [16], and mfsnet [17], usually suffer from relatively low
performance and reduced fairness [2,18,19].in this paper, we present the first
lyme disease dataset that contains labeled segmentation and skin tones."
1821,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"our lyme
disease dataset contains two parts: (i) a classification dataset, composed of
more than 3,000 diseased skin images that are either obtained from public
resources or clinicians with patient-informed consent, and (ii) a segmentation
dataset containing 185 samples that are manually annotated for three
regions-i.e., background, skin (light vs."
1822,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"our
dataset with manual labels is available at this url [20].secondly, we design a
simple yet novel data preprocessing and alternation method, called edgemixup, to
improve lyme disease segmentation and diagnosis fairness on samples with
different skin-tones."
1823,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"note that not all skin disease datasets are
carefully processed either due to the large amount of work required or the
scarcity of data samples collected, e.g., sd-198 [23] contains samples that are
taken under variant environments."
1824,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"specifically, we train two resnet-34 models
using the same dataset with and without edgemixup for a classification task of
skin disease."
1825,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"2
edgemixup improves model fairness on light and dark skin samples in both
segmentation and classification tasks, and it has two major components: (i) edge
detection using mixup, and (ii) data preprocessing and alteration for downstream
tasks."
1826,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"that is, even if the original
dataset is imbalanced, as long as one sample from a subpopulation exists, the
color range of the sample's lesion is considered in the initial detection."
1827,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"we present two datasets: (i) a dataset collected and annotated by us (called
skin), and (ii) a subset of sd-198 [23] with our annotation (called sd-sub)."
1828,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"first, we collect and annotate a dataset with 3,027 images containing three
types of disease/lesions, i.e., tinea corporis (tc), herpes zoster (hz), and
erythema migrans (em)."
1829,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"second, we select five classes from sd-198
[23], a benchmark dataset for skin disease classification, as another dataset
for both segmentation and classification tasks."
1830,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"we choose 30 samples in each class for segmentation task, and we
split them into 0.7, 0.1, and 0.2 ratio for training, validation, and testing,
respectively.table 1 show the characteristics of these two datasets for both
classification and segmentation tasks broken down by the disease type and skin
tone, as calculated by the individual typology angle (ita) [24]."
1831,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"edgemixup, a data preprocessing method, improves the
utility of lesion segmentation in terms of jaccard index compared with all
existing baselines."
1832,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,take our skin-seg dataset for example.
1833,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"our classification evaluation involves: (i)
adversarial debiasing (ad) [21], (ii) dexined-avg, the average version of
dexined [26] as an boundary detector used by edgemixup, and (iii) st-debias
[22], a debiasing method augmenting data with conflicting shape and texture
information."
1834,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"as for skin lesion segmentation tasks, few works has been
proposed due to the lack of datasets with ground-truth segmentation masks."
1835,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"however, official datasets released, e.g., ham10000 [10] only
contains melanoma samples and all of the samples are with light skins according
to our inspection using ita scores.bias mitigation: researchers have addressed
bias and heterogeneity in deep learning models [18,29]."
1836,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"the key insight is a novel data
preprocessing method that utilizes edge detection and mixup to isolate and
highlight skin lesions and reduce bias."
1837,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,"ablation study (in ""mean (std)"") on mdc dataset using
regnetx40 [26] as the backbone."
1838,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,"the feature maps obtained from the ensemble can be decoded using lightweight 2d
decoders to generate segmentation masks.3 experimental results we conducted
experiments on the public multi-dimensional choledoch (mdc) dataset
[31] with 538 scenes and hyperspectral gastric carcinoma (hgc) dataset [33]
(data provided by the author) with 414 scenes, both with highquality labels for
binary mhsi segmentation tasks."
1839,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,"following [23,27], we partition the datasets into training, validation, and
test sets using a patient-centric hard split approach with a ratio of 3:1:1."
1840,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,"specifically, each patient's data is allocated entirely to one of the three
sets, ensuring that the same patient's data do not appear in multiple sets.we
use data augmentation techniques such as rotation and flipping, and train with
an adam optimizer using a combination of dice loss and cross-entropy loss for 8
batch size and 100 epochs."
1841,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,"69.89) lower dsc, possibly because transformers are difficult
to optimize on small datasets."
1842,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,"table 3 shows
comparisons on mdc and hgc datasets."
1843,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,"we evaluate our approach on two
mhsi datasets."
1844,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"experimental results on lung (83 cts, 19 patients) and liver (77 cects, 18
patients) datasets show that our method yields high classification accuracy.to
the best of our knowledge, ours is the first method to perform longitudinal
lesion matching and lesion changes pattern detection."
1845,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"similarity-based methods pair two lesions
with similar features, e.g., intensity, shape, location [13][14][15][16] with an
84-96% accuracy on the deeplesion dataset [14]."
1846,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"we evaluated our method with two studies on retrospectively collected patient
datasets that were manually annotated by an expert radiologist.dataset: lung and
liver ct studies were retrospectively obtained from two medical centers
(hadassah univ hosp jerusalem israel) during the routine clinical examination of
patients with metastatic disease."
1847,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"dliver consists of
77 abdominal cect scans from 18 patients with a mean 4.3 ± 2.0 scans/patient, a
mean time interval between consecutive scans of 109.7 ± 93.5 days, and voxel
sizes of 0.6-1.0 × 0.6-1.0 × 0.8-5.0 mm 3 .lesions in both datasets were
annotated by an expert radiologist, yielding a total of 1,178 lung and 800 liver
lesions, with a mean of 14.2 ± 19.1 and 10.4 ± 7.9 lesions/scan (lesions with
<20 voxels were excluded)."
1848,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"ground-truth lesion matching graphs and lesion
changes labeling were produced by running the method on the datasets and then
having the radiologist review and correct the resulting node labels and
edges.study 1: lesion changes labeling, lesion matching, evaluation of patterns
of lesion changes."
1849,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"for the dlungs dataset, 25 visible
and 5 faintly visible or surmised to be present unmarked lesions were found for
27 nonconsecutive edges."
1850,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"for the dliver dataset, 20 visible and 21 faintly
visible or surmised to be present unmarked lesions were found for 25
non-consecutive edges.after reviewing the 42 and 37 lesions labeled as lone in
dlungs and dliver with > 5mm diameter, the radiologist determined that 1 and 8
of them had been wrongly identified as a cancerous lesion."
1851,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"in total, 45 and 62 missing lesions were added to the
ground truth dlungs and dliver datasets, respectively."
1852,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,"however, measurement-induced
non-gaussian noise in 1 h-mrs data undoubtedly limits the performance of
mmse-based machine learning methods.on the other hand, for the discovery task of
potential biomarkers, sparse codingbased methods (e.g., 2,1 norm, 2,0 norm,
etc.) force row elements to zero that remove some valuable features [12,21]."
1853,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,"specifically, we first extend our feature learning
through generalized correntropic loss to handle data with complex non-gaussian
noise and outliers."
1854,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,"the experimental results on a benchmark npsle dataset demonstrate the
proposed method outperforms comparing methods in terms of early noninvasive
biomarker discovery and early diagnosis."
1855,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,"dataset and preprocessing: the t2-weighted mr images of 39 participants
including 23 patients with npsle and 16 hcs were gathered from our affiliated
hospital."
1856,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,"the
collected spectroscopy data were preprocessed by a sage software package to
correct the phase and frequency."
1857,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,"given a data matrix x = [x 1 ; • • • ; x n ] ∈ r n×d with n sample, the i-th row
is represented by x i , and the corresponding label matrix is denoted aswhere y
i is one-hot vector."
1858,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,"the
classification performances of the npsle dataset contaminated by attribute noise
are shown in fig."
1859,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,"in this paper, we develop reasfs, a robust flexible feature selection that can
identify metabolic biomarkers and detect npsle at its early stage from noisy 1
h-mrs data."
1860,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,"additionally, carl learns view-specific representations as well which
complement the view-invariant ones, providing a comprehensive picture of the ct
volume data for histological subtype prediction."
1861,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,"we validate our approach by
using a publicly available nsclc dataset from the cancer imaging archive (tcia)."
1862,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,"-we conduct experiments on a
publicly available dataset and achieve superior performance compared to the most
advanced methods currently available."
1863,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,"our dataset nsclc-tcia for lung cancer histological subtype classification is
sourced from two online resources of the cancer imaging archive (tcia) [5]:
nsclc radiomics [1] and nsclc radiogenomics [2]."
1864,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,"we evaluate the performance of nsclc classification in
five-fold cross validation on the nsclc-tcia dataset, and measure accuracy
(acc), sensitivity (sen), specificity (spe), and the area under the receiver
operating characteristic (roc) curve (auc) as evaluation metrics."
1865,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,"we also
conduct analysis including standard deviations and 95% ci, and delong
statistical test for further auc comparison.for preprocessing, given that the ct
data from nsclc-tcia has an in-plane resolution of 1 mm × 1 mm and a slice
thickness of 0.7-3.0 mm, we resample the ct images using trilinear interpolation
to a common resolution of 1mm × 1mm × 1mm."
1866,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"although deep neural network-based object detectors
achieve tremendous success within the domain of natural images, directly
training generic object detectors on gld datasets performs below expectations
for two reasons: 1) the scale of labeled data in gld datasets is limited in
comparison to natural images due to the annotation costs."
1867,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"generic self-supervised
backbone pre-training or semi-supervised detector training methods can solve the
first challenge for natural images but its effectiveness is undermined for
gastroscopic images due to the second challenge.self-supervised backbone
pre-training methods enhance object detection performance by learning
high-quality feature representations from massive unlabelled data for the
backbone."
1868,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"therefore, both types of methods have their own weakness for gld
tasks.semi-supervised object detection methods [12,14,16,17,20,22,23] first use
detectors trained with labeled data to generate pseudo-labels for unlabeled data
and then enhance object detection performance by regarding these unlabeled data
with pseudo-labels as labeled data to train the detector."
1869,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"current pseudolabel
generation methods rely on the objectiveness score threshold to generate
pseudo-labels, which makes them perform below expectations on gld, because the
characteristic of gastroscopic lesions makes it difficult to set a suitable
threshold to discover potential lesions meanwhile avoiding introducing much
noise.the motivation of this paper is to explore how to enhance gld performance
using massive unlabeled gastroscopic images to overcome the labeled data
shortage problem."
1870,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"enlightened by this, we propose the self-and semi-supervised
learning (ssl) framework tailored to address challenges in daily clinical
practice and use massive unlabeled data to enhance gld performance."
1871,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"the
ppg generates pseudo-labels based on the similarity to the prototype feature
vectors (formulated from the feature vectors in its memory module) to discover
potential lesions from unlabeled data, and avoid introducing much noise at the
same time."
1872,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"moreover, we propose the first large-scale gld datasets (lgldd),
which contains 10,083 gastroscopic images with 12,292 well-annotated lesion
bounding boxes of four categories of lesions (polyp, ulcer, cancer, and
sub-mucosal tumor)."
1873,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"in summary, our contributions include:-a
self-and semi-supervise learning (ssl) framework to leverage massive unlabeled
data to enhance gld performance."
1874,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"-a large-scale gastroscopic lesion detection
datasets (lgldd) -experiments on lgldd demonstrate that ssl can bring
significant enhancement compared with baseline methods."
1875,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"we propose the prototype-based pseudo-label generation method (ppg) to discover
potential lesions from unlabeled gastroscopic data meanwhile avoid introducing
much noise to further enhance gld performance."
1876,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"in semi-supervised
learning, ppg generates pseudo-labels for unlabeled data relying on the
similarity to the prototype feature vectors, which achieves a better balance
between lesion discovery and noise avoidance.memory module."
1877,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"we
set τ u = 0.5 and τ s = 0.5 we contribute the first large-scale gstroscopic
lesion detection datasets
(lgldd) in the literature."
1878,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"the other data serves as unlabeled
data.evaluation metrics : we use standard object detection metrics to evaluate
the gld performance, which computes the average precision (ap) under multiple
intersection-of-union (iou) thresholds and then evaluate the performance using
the mean of aps (map) and the ap of some specific iou threshold."
1879,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"for map, we
follow the popular object detection datasets coco [11] and calculate the mean of
11 aps of iou from 0.5 to 0.95 with stepsize 0.05 (map @[.5:.05:.95]).we also
report ap under some specific iou threshold (ap 50 for .5, ap 75 for .75) and ap
of different scale lesions (ap s , ap m , ap l ) like coco [11]."
1880,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"experimental results in table 2.d show that ssl can bring
significant improvements to publicly available datasets."
1881,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"moreover, we contribute the first
large-scale gld datasets (lgldd)."
1882,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"since annotation cost always limits
of datasets scale of such tasks, we hope ssl and lgldd could fully realize its
potential, as well as kindle further research in this direction."
1883,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"(3) the proposed method achieves
state-of-the-art results in five polyp segmentation datasets and outperforms the
previous cutting-edge approach by a large margin (3%) on cvc-colondb and etis
datasets."
1884,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,datasets.
1885,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"we conduct extensive experiments on five polyp segmentation datasets,
including kvasir [6], cvc-clinicdb [1], cvc-colondb [13], etis [12] and cvc-t
[14]."
1886,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"we adopt the same data augmentation
techniques as uacanet [8], including random flip, random rotation, and color
jittering."
1887,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"according to the experimental settings, the results on
cvc-clinicdb and kvasir demonstrate the learning ability of the proposed model,
while the results on cvc-t, cvc-colondb, and etis demonstrate the model's
ability for cross-dataset generalization."
1888,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"furthermore, our proposed
method demonstrates strong cross-dataset generalization capability on cvc-t,
cvc-colondb, and etis datasets, with particularly good performance on the latter
two due to their larger and more representative datasets."
1889,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"specifically, across the five datasets, our proposed model improves
the mdice score by at least 1.4% and up to 3.4% on cvc-t, compared to the
baseline."
1890,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"for miou, the improvements are 1.5% and 3.5% on the corresponding
datasets."
1891,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"experimental results on
five popular polyp datasets demonstrate the effectiveness and superiority of our
proposed method."
1892,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"specifically, it outperforms the previous cutting-edge approach
by a large margin (3%) on cvc-colondb and etis datasets."
1893,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","the dino framework [4] uses a
self-distillation training approach, using data augmentation to locally crop the
patches and train with a local-global student-teacher approach."
1894,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","each graph-level prediction is derived from the corresponding branch
node predictions, by applying pooling and dropout.data."
1895,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","we train and validate
our methods on two retrospective rectal cancer datasets, grampian and aristotle."
1896,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","pathological complete response, which we use as a target outcome here,
was derived from histopathological assessment from posttreatment resections.the
cms labels for this data are derived from three different transcriptomic
versions (single cohort, combined cohort correcting batch effects and combined
cohort including 2036 cases run with the same platform), in order to generate
robust classifications."
1897,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","despite our
efforts to minimise the noise from rna sequencing, we still expect a certain
level of noise in our ground truth data, which we discuss in the results
section.the epithelial labels for each graph node are calculated from epithelial
masks for each wsi."
1898,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","grampian and aristotle are used in both
training and validation, with a 70/30% training-validation split, keeping any
wsis from a single patient in the same dataset."
1899,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","the datasets are unbalanced, since in grampian only 61/244 slides have
complete response, and in aristotle only 24/121 slides have complete response."
1900,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","there are 365 slides total in our dataset, from 249
patients."
1901,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","prior to
fitting the graph model we normalize the node features relative to the whole
dataset."
1902,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","we run the whole pipeline on four folds with different
random data splits for training and validation."
1903,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","for each fold, we take the mean metrics
for the three branch predictions from the best model on our validation data,
with the best epoch chosen based on mean auc for the three predictions."
1904,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","we use
weighted metrics due to the class imbalance in our dataset."
1905,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","despite the noise in our reference data used for training, our
model achieves good performance in terms of mean auc scores on all three
prediction branches of our model, predicting complete response to radiotherapy
(rt) with 0.819 auc, cms4 with 0.819 auc and epithelial tissue at the node level
with 0.760 auc across folds."
1906,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","the
prediction performance of the model could be improved by utilising a larger
training dataset and performing more exhaustive parameter searches, however the
current performance of the model is sufficient to demonstrate the impact of this
approach.the predicted response to radiotherapy can now be viewed in the context
of disease biology as captured by cms4."
1907,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","the focus of this research is not to achieve the best possible
metrics, but to develop robust methods which can add context and explanation to
clinical black box deep learning model predictions, with the view to ease
clinical translation of such models.to explore the effects of the noisy cms4
ground truth labels, we remove from our dataset any wsis classified as
'unmatched' for the cms call, which for the main results of this paper we
defined as 'not cms4'."
1908,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","removing this data and rerunning our analysis improved
our predictions for cms4 by +0.06 auc, and reduced our response to radiotherapy
and epithelial predictions by -0.02 and -0.01 respectively."
1909,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","these small changes indicate that the
noise in our data does not degrade the performance of our classifier,
reinforcing it as a robust and accurate model."
1910,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","these results do not
only enhance the interpretability, they also provide new ways to utilise large
retrospective clinical trial cohorts for which no additional molecular data is
available."
1911,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","extending the amount of training data and improving model training
will improve model performance, which is already impressive.we argue that this
work also advances the state of the art in feature representation and analysis."
1912,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","by cross-referencing these prediction maps with our prior
understanding of cancer biology, this approach can help to establish trust in
the prediction model and also help to identify potential failure cases.this work
relies on access to well annotated clinical trial samples which will limit our
ability to include more data for training and testing."
1913,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","financial support: rw -epsrc center
for doctoral training in health data science (ep/s02428x/1), oxford cruk cancer
centre; vhk -promedica foundation (f-87701-41-01) and swiss national science
foundation (p2skp3_168322/1, p2skp3_168322/2); tsm -s:cort (see above); jr, ks
-oxford nihr national oxford biomedical research centre and the pathlake
consortium (innovateuk)."
1914,Automatic Bleeding Risk Rating System of Gastric Varices,"in
cram, the varices features are extracted using the segmentation results and
combined with an attention mechanism to learn the intra-class correlation and
cross-region correlation between the target area and the context.to learn from
experienced endoscopists, gv datasets with bleeding risks annotation is needed."
1915,Automatic Bleeding Risk Rating System of Gastric Varices,"while most works and public datasets focus on colonoscopy [13,15] and esophagus
[5,9], with a lack of study on gastroscopy images."
1916,Automatic Bleeding Risk Rating System of Gastric Varices,"in the public dataset of
endocv challenge [2], the majority are colonoscopies while only few are
gastroscopy images."
1917,Automatic Bleeding Risk Rating System of Gastric Varices,"in this work, we collect a gv bleeding risks rating dataset
(gvbleed) that contains 1678 gastroscopy images from 411 patients with different
levels of gv bleeding risks."
1918,Automatic Bleeding Risk Rating System of Gastric Varices,"three senior clinical endoscopists are invited to
grade the bleeding risk of the retrospective data in three levels and annotated
the corresponding segmentation masks of gv areas.in sum, the contributions of
this paper are: 1) a novel gv bleeding risk rating framework that constructively
introduces segmentation to enhance the robustness of representation learning; 2)
a region-constraint module for better feature localization and a cross-region
attention module to learn the correlation of target gv with its context; 3) a gv
bleeding risk rating dataset (gvbleed) with high-quality annotation from
multiple experienced endoscopists."
1919,Automatic Bleeding Risk Rating System of Gastric Varices,"baseline methods have been evaluated on the
newly collected gvbleed dataset."
1920,Automatic Bleeding Risk Rating System of Gastric Varices,data collection and annotation.
1921,Automatic Bleeding Risk Rating System of Gastric Varices,"the gvbleed dataset contains 1678 endoscopic
images with gastric varices from 527 cases."
1922,Automatic Bleeding Risk Rating System of Gastric Varices,"to ensure the quality of our dataset, senior endoscopists are
invited to remove duplicates, blurs, active bleeding, chromoendoscopy, and nbi
pictures.criterion of gv bleeding risk level rating."
1923,Automatic Bleeding Risk Rating System of Gastric Varices,"based on the clinical
experience in practice, the gv bleeding risks in our dataset are rated into
three levels, i.e., mild, moderate, and severe."
1924,Automatic Bleeding Risk Rating System of Gastric Varices,"note that the diameter is only one reference for the final risk rating since the
gv is with 1 please refer to the supplementary material for more detailed
information about our dataset."
1925,Automatic Bleeding Risk Rating System of Gastric Varices,"to
ensure the accuracy of our annotation, three senior endoscopists with more than
10 years of clinical experience are invited to jointly label each sample in our
dataset."
1926,Automatic Bleeding Risk Rating System of Gastric Varices,"the gvbleed dataset is partitioned into training and testing sets for
evaluation, where the training set contains 1337 images and the testing set has
341 images."
1927,Automatic Bleeding Risk Rating System of Gastric Varices,"the dataset is planned to be released in the
future."
1928,Automatic Bleeding Risk Rating System of Gastric Varices,"in addition, common data augmentation techniques such
as rotation and flipping were adopted here."
1929,Automatic Bleeding Risk Rating System of Gastric Varices,"however, the
transformer-based models achieves much worse performances since they always
require more training data, which is not available in our task."
1930,Automatic Bleeding Risk Rating System of Gastric Varices,"in addition, we collected the gvbleed
dataset with high-quality annotation of three-level of gv bleeding risks."
1931,Automatic Bleeding Risk Rating System of Gastric Varices,"the
experiments on our dataset demonstrated the effectiveness and superiority of our
framework."
1932,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"based on the data
with racial bias, the unfairness presents in developing evaluative algorithms."
1933,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"using biased data for ai models reinforces racial inequities, worsening
disparities among minorities in healthcare decision-making [22].within the
radiology arm of ai research, there have been significant advances in
diagnostics and decision making [19]."
1934,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"ai model quality relies on input
data and addressing bias is a crucial research area."
1935,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"survival
analysis is often used in pe to assess how survival is affected by different
variables, using a statistical method like kaplan-meier method and cox
proportional-hazards regression model [7,12,14].however, one issue with
traditional survival analysis is bias from single modal data that gets
compounded when curating multimodal datasets, as different combinations of modes
and datasets create with a unified structure."
1936,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"multimodal data sets are useful
for fair ai model development as the bias complementary from different sources
can make de-biased decisions and assessments."
1937,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"in that process, the biases of
each individual data set will get pooled together, creating a multimodal data
set that inherits multiple biases, such as racial bias [1,15,23]."
1938,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"in addition,
it has been found that creating multimodal datasets without any debiasing
techniques does not improve performance significantly and does increase bias and
reduce fairness [5]."
1939,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"overall, a holistic approach to model development would be
beneficial in reducing bias aggregation in multimodal datasets."
1940,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"we then implemented methods
to remove racial bias in our dataset and model and output unbiased pe outcomes
as a result."
1941,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"we will first introduce our pulmonary embolism multimodal
datasets, including survival and race labels."
1942,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"then, we evaluate the baseline
survival learning framework without de-biasing in the various racial
groups.dataset."
1943,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"the pulmonary embolism dataset used in this study from 918
patients (163 deceased, median age 64 years, range 13-99 years, 52% female),
including 3978 ctpa images and 918 clinical reports, which were identified via
retrospective review across three institutions."
1944,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"for each patient, the race labels, survival time-to-event labels and
pesi variables are collected from clinical data, and the 11 pesi variables are
used to calculate the pesi scores, which include age, sex, comorbid illnesses
(cancer, heart failure, chronic lung disease), pulse, systolic blood pressure,
respiratory rate, temperature, altered mental status, and arterial oxygen
saturation at the time of diagnosis [2].diverse bias of multimodal survival
prediction model."
1945,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"we designed a deep survival prediction (sp) baseline framework
for multimodal data as shown in fig."
1946,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"first, we use
two large-scale data-trained models as backbones to respectively extract
features from preprocessed images and cleaned clinical reports."
1947,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"besides, clinical data in the form of text
reports and pesi variables objectively reflect the patient's physiological
information and the physician's diagnosis, exhibiting smaller race biases in
correlation with survival across different races."
1948,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"we validate the proposed de-biased survival prediction frameworks on the
collected multi-modality pe data."
1949,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"the data from 3 institutions are randomly
split the lung region of cpta images is extracted with a slice thickness of 1.25
mm and scaled to n × 512 × 512 pixels [10]."
1950,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"the penet is
pre-trained on large-scale ctpa studies and shows excellent pe detection
performance with an auroc of 0.85 on our entire dataset."
1951,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"a fully connected layer with sigmoid activation acts as a risk
classifier c m sur (z m sur ) for survival prediction, where z m sur is the
feature encoded from single modal data."
1952,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"for training the biased and de-biased sp
modules, we collect data from one modality as a batch with synchronized batch
normalization."
1953,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"the swapping augmentation provides a strong bias
correction effect for image data with obvious bias."
1954,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"for clinical data, the
resampling generally improves performance in most cases."
1955,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"we detected indications of racial bias in our dataset and
conducted an analysis of the multimodal diversity."
1956,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"the research in our paper demonstrates and proves that eliminating
racial biases from data improves performance, and yields a more precise and
robust survival prediction tool."
1957,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"however, these models do not
perform well enough to be utilized in practical screening settings [3] and
require the collection of data that is not always available."
1958,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"our
method is based on a transformer model that uses attention [30], similar to how
radiologists would compare current and prior mammograms.the method is trained
and evaluated on a large and diverse dataset of over 9,000 patients and shown to
outperform a model based on state-of-the art risk prediction techniques for
mammography [33]."
1959,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"the data comprises three main elements: features x, time of the event t,
and the occurrence of the event e [18]."
1960,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"if the event has not yet occurred
by the end of the study or observation period, the data is referred to as
right-censored (fig."
1961,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"the
predicted cumulative hazard is obtained by adding the base hazard and
time-dependent hazard, according to:when dealing with right-censored data, we
use an indicator function δ i (t) to determine whether the information for
sample i at time t should be included in the loss calculation or not."
1962,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"we compiled an in-house mammography dataset comprising 16,113 exams (64,452
images) from 9,113 patients across institutions from the united states, gathered
between 2010 and 2021."
1963,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"the
dataset has 3,625 biopsy-proven cancer exams, 5,394 biopsyproven benign exams,
and 7,094 normal exams."
1964,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"we partitioned the dataset by patient to create
training, validation, and test sets."
1965,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"all data was
de-identified according to the u.s hhs safe harbor method."
1966,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"therefore, the data
has no phi (protected health information) and irb (institutional review board)
approval is not required."
1967,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"the cindex
measures the performance of a model by evaluating how well it correctly predicts
the relative order of survival times for pairs of individuals in the dataset."
1968,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"to augment the training data, we
apply geometric transformations such as vertical flipping, rotation and
photometric transformations such as brightness/contrast adjustment, gaussian
noise, sharpen, clahe, and solarize."
1969,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"the addition of mammographic breast density has
improved the performance traditional breast cancer risk models [4] and can
therefore help us understand why the addition of prior images works.mammographic
breast density was determined using the breast imaging reporting and data system
(bi-rads) composition classification."
1970,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"our extensive
experiments on a dataset of 16,113 exams show that prime+ outperformed a model
based on the state-of-the-art for breast cancer risk prediction [33]."
1971,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"in pare, a nodule is diagnosed from two levels: first parsing the contextual
information contained in the nodule itself, and then recalling the previously
learned nodules to look for related clues.one of the major challenges of lung
nodule malignancy prediction is the quality of datasets [6]."
1972,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"recent works have focused on collecting pathologically
labeled data to develop reliable malignancy prediction models [16,17,19]."
1973,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"[16] collated a pathological gold standard dataset of 990
ct scans."
1974,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"to fulfill both ldct and ncct screening needs, we curate a large-scale
lung nodule dataset with pathology-or follow-up-confirmed benign/malignant
labels."
1975,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"for the ldct, we annotate more than 12,852 nodules from 8,271 patients
from the nlst dataset [14]."
1976,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"experimental results on
several datasets demonstrate that our method achieves outstanding performance on
both ldct and ncct screening scenarios.our contributions are summarized as
follows: (1) we propose context parsing to extract and aggregate rich contextual
information for each nodule."
1977,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"(3) we curate the largest-scale lung nodule dataset with
high-quality benign/malignant labels to fulfill both ldct and ncct screening
needs."
1978,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"as for the nodule embedding q
of the data (x, y), its nearest prototype is singled out and then updated by the
following momentum rules,where λ is the momentum factor, set to 0.95 by default."
1979,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"data collection and curation: nlst is the first large-scale ldct dataset for
low-dose ct lung cancer screening purpose [14]."
1980,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"unlike nlst, this dataset
is noncontrast chest ct, which is used for routine clinical care."
1981,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"segmentation
annotation: we provide the segmentation mask for our in-house data, but not for
the nlst data considering its high cost of pixel-level labeling."
1982,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"the nodule mask
of each in-house data was manually annotated with the assistance of ct labeler
[20] by our radiologists, while other contextual masks such as lung, vessel, and
trachea were generated using the totalsegmentator [21]."
1983,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"we additionally evaluate our method on the lungx [2] challenge
dataset, which is usually used for external validation in previous work
[6,11,24]."
1984,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"segmentation: we also evaluate the
segmentation performance of our method on the public nodule segmentation dataset
lidc-idri [3], which has 2,630 nodules with nodule segmentation mask."
1985,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"due to the lack of manual annotation of nodule masks for the nlst dataset, we
can only optimize the segmentation task using our in-house dataset, which has
manual nodule masks."
1986,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"our model is trained on a mix of ldct and ncct datasets, which can perform
robustly across low-dose and regular-dose applications."
1987,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"we compare the
generalization performance of the models obtained under three training data
configurations (ldct, ncct, and a combination of them)."
1988,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"we find that the models
trained on either ldct or ncct dataset alone cannot generalize well to other
modalities, with at least a 6% auc drop."
1989,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"besides, we curate a large-scale pathological-confirmed
dataset with up to 13,000 nodules to fulfill the needs of both ldct and ncct
screening scenarios."
1990,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"with the support of a high-quality dataset, our pare
achieves outstanding malignancy prediction performance in both scenarios and
demonstrates a strong generalization ability on the external validation."
1991,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"we utilize
optimam [7], a large dataset with a significant proportion of negatives (table
1), for training and evaluation."
1992,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"concretely, the false
positive rate is low for a typical evaluation data distribution but much higher
for a clinicallyrepresentative data distribution, as shown in fig."
1993,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"ultimately, each
component contributes to our goal of reducing false positives.we validate m&m
through evaluation on five datasets: two in-house datasets, two public datasets
-ddsm [8] and cbis-ddsm [9], and optimam [7]."
1994,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"furthermore, m&m can
provide breast-level classification predictions, achieving aucs of more than
0.88 on four different datasets (table 3)."
1995,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"we employ a 1:1 sampling
ratio between unannotated and annotated images.datasets."
1996,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"we utilize three 2d
digital mammography datasets: (1) optimam : a development dataset derived from
the optimam database [7], which is funded by cancer research uk."
1997,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"we split the
data into train/val/test with an 80:10:10 ratio at the patient level; (2)
inhouse-a: an evaluation dataset collected from a u.s."
1998,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"multi-site mammography
operator; (3) inhouse-b : an evaluation dataset collected from a u.s."
1999,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,2.2 for more details on the inhouse datasets).
2000,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"we also
utilize two film mammography datasets: (4) ddsm: a dataset maintained at the
university of south florida [8]."
2001,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"dataset statistics are reported
in table 1.metrics."
2002,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"ap denotes average precision when all data is
included."
2003,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"table 3a reports
m&m's breast-level and exam-level classification results on optimam and the two
inhouse datasets."
2004,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"both baseline models suffer large
generalization drops of approximately 3b compares m&m with recent literature
reporting on the public cbis-ddsm dataset."
2005,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"finally,
our mil formulation allows for training with representative data distribution in
an endto-end one stage pipeline."
2006,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"this is more advantageous than previous
pipelines that require additional stages or classifiers to reduce false
positives [15,22,29].as a classifier, m&m establishes strong performance on
several datasets (table 3)."
2007,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"our method can use both expert
annotatedor unsupervised generated masks to reverse and segment anomalies
annotated datasets for training and tend to generalize poorly beyond the learned
labels [21]."
2008,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"on the other hand, unsupervised methods focus on detecting patterns
that significantly deviate from the norm by training only on normal data.one
widely used category of unsupervised methods is latent restoration methods."
2009,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"they
involve autoencoders (aes) that learn low-dimensional representations of data
and detect anomalies through inaccurate reconstructions of abnormal samples
[17]."
2010,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"nevertheless,
latent methods still face difficulties in accurately reconstructing data from
their low-dimensional representations, causing false positive detections on
healthy tissues.several techniques have been proposed that make use of the
inherent spatial information in the data rather than relying on constrained
latent representations [12,25,30]."
2011,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"adapting the noise distribution to the diversity
and heterogeneity of pathology is inherently difficult, and even if achieved,
the noising process disrupts the structure of both healthy and anomalous regions
throughout the entire image.in related computer vision areas, such as industrial
inspection [3], the topperforming methods do not focus on reversing anomalies,
but rather on detecting them by using large nominal banks [7,20], or pre-trained
features from large natural imaging datasets like imagenet [4,22]."
2012,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"they include the variability
and complexity of normal data, subtlety of anomalies, limited size of datasets,
and domain shifts.this work aims to combine the advantages of constrained latent
restoration for understanding healthy data distribution with generative
in-painting networks."
2013,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"in summary our main
contributions are:• we investigate and measure the ability of sota methods to
reverse synthetic anomalies on real brain t1w mri data."
2014,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"on the other hand, the
decoder is trained to deceive the encoder by reconstructing real data samples
using the standard elbo and minimizing the kl divergence of generated samples
compressed by the encoder:where e l φ is the l-th embedding of the l encoder
layers,, and l sim is the cosine similarity."
2015,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"we compute
the final anomaly maps based on residual and perceptual differences: datasets."
2016,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"we trained our model using two publicly available brain t1w mri
datasets, including fastmri+ (131 train, 15 val, 30 test) and ixi (581 train
samples), to capture the healthy distribution."
2017,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"performance evaluation was done
on a large stroke t1-weighted mri dataset, atlas v2.0 [14], containing 655
images with manually segmented lesion masks for training and 355 test images
with hidden lesion masks."
2018,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"the lack of healthy data from the same
scanner and the limited size of the healthy datasets limit the successful
application of such methods, with a maximum achievable dice score of just under
6%."
2019,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"generally, unsupervised methods tend to have
lower dice scores partly due to unlabeled artefacts in the dataset."
2020,SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,"we use f ocal loss [16] to alleviate the class imbalanced data problem
of the hr and sr images' classification."
2021,SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,"dataset: this work uses the breast cancer histopathological image database
(breakhis)1 [20]."
2022,SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,"the images in the dataset have four magnification factors the
model is trained using the adam optimizer [25] with the learning rate set to
1x10 -3 ."
2023,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"in parallel, foundation models [4] have surged in computer
vision [5,6] and natural language processing [7,8] with growing model capacity
and data size, opening up perspectives in utilizing foundation models and
large-scale clinical data for diagnostic tasks."
2024,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"however, pure imaging data can
be insufficient to adapt foundation models with large model capacity to the
medical field."
2025,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"given the complex tissue characteristics of pathological whole
slide images (wsi), it is crucial to develop adaptation strategies allowing (1)
training data efficiency, and (2) data fusion flexibility for pathological image
analysis."
2026,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"language
models prove to be effective in capturing semantic characteristics with a lower
data acquisition and annotation cost in medical areas [12]."
2027,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"thus, connecting visual representations with text information from
biomedical language models becomes increasingly critical to adapting foundation
models for medical image classification, particularly in the challenging setting
of data deficiency.in this study, we propose cite, a data-efficient adaptation
framework that connects image and text embeddings from foundation models to
perform pathological image classification with limited training samples (see
fig."
2028,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"to enable language comprehension, cite makes use of large language
models pretrained on biomedical text datasets [10,11] with rich and professional
biomedical knowledge."
2029,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"however, those methods are supported by sufficient high-quality
data expensive to collect and curate [19]."
2030,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"to tackle this issue,
we emphasize the adaptation of foundation models in a data-efficient
manner.vision-language pre-training."
2031,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"instead, we combine
pre-trained unimodal models on downstream tasks and build a multi-modal
classifier with only a few data.model adaptation via prompt tuning."
2032,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"figure 2 depicts an overview of our approach cite for data-efficient
pathological image classification."
2033,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"during model adaptation, we freeze the
pre-trained encoders and only tune the introduced parameters, which not only
saves remarkable training data and computational resources but also makes our
approach favorable with various foundation model architectures."
2034,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"visual prompt tuning (vpt [23]) is a
lightweight adaptation method that can alleviate such an inherent difference by
only tuning prompt tokens added to the visual inputs of a fixed vision
transformer [24], showing impressive performance especially under data
deficiency."
2035,Text-Guided Foundation Model Adaptation for Pathological Image Classification,dataset.
2036,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"we adopt the patchgastric [25] dataset, which includes
histopathological image patches extracted from h&e stained whole slide images
(wsi) of stomach adenocarcinoma endoscopic biopsy specimens."
2037,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"the
dataset contains 9 subtypes of gastric adenocarcinoma."
2038,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"to extend our
evaluation into the real-world setting with insufficient data, we additionally
choose 1, 2, 4, 8, or 16 wsis with the largest numbers of patches from each
class as the training set.the evaluation metric is patient-wise accuracy, where
the prediction of a wsi is obtained by a soft vote over the patches, and
accuracy is averaged class-wise.implementation."
2039,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"we resize the
images to 224×224 to fit the model and follow the original data pipeline in
patchgastric [25]."
2040,Text-Guided Foundation Model Adaptation for Pathological Image Classification,cite consistently outperforms all baselines under all data scales.
2041,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"figure 3
shows the classification accuracy on the patchgastric dataset of our approach
compared with baseline methods and related works, including (1) r50-21k:
fine-tune the whole resnet50 [27] backbone pre-trained on imagenet-21k [26].(2)
linear probe: train a classification head while freezing the backbone
encoder.(3) fine-tune: train a classification head together with the backbone
encoder."
2042,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"(6) few-shot
[28]: cluster image features of the training data and classify images to the
nearest class center."
2043,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"adapting powerful foundation models into medical imaging constantly faces
data-limited challenges."
2044,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"in this study, we propose cite, a data-efficient and
model-agnostic approach to adapt foundation models for pathological image
classification."
2045,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"also, foundation training on multi-modal medical images is of
substantial interest to enhance model robustness under data-limited conditions
[30]."
2046,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"compared with erm, drl is an optimization method
minimizing the worst-case loss over an ambiguity set, therefore, can tolerate
outliers in the data [5]."
2047,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"our results show that on a real-world dataset, drl can
significantly improve the stroke classification performance of erm and other
baseline defensive training methods, when the signal sparsity and noise in
accelerated mri are generated by the cartesian undersampling (cu) method [20]
and white gaussian noise (wgn)."
2048,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"logistic regression solves this problem by
minimizing the following expected true riskwhereis the coefficient matrix, p *
is the true distribution of the data (x, y), h b (x, y) log 1 e b x -y b x is
the loss function to be minimized, and e p * denotes the expectation under the
distribution p * ."
2049,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"to obtain
robust estimators that can hedge against noise in the training data and
generalize well out-of-sample, [4] proposed the drl framework under the
wasserstein metric."
2050,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"specifically, it minimizes the worst-case expected loss over
a set of probability distributionswhere ω contains a set of probability
distributions that are close to the empirical distribution pn measured by the
wasserstein metric, ω {q ∈ p(z) : w 1 (q, pn ) ≤ }, where z is the set of
possible values for (x, y), p(z) is the space of all probability distributions
supported on z, is a pre-specified radius of the ambiguity set ω, pn is the
empirical distribution that assigns an equal probability 1/n to each observed
sample (x i , y i ), and w 1 (q, pn ) is the order-1 wasserstein distance
between q and pn defined aswhere π is the joint distribution of z 1 (x 1 , y 1 )
and z 2 (x 2 , y 2 ) with marginals q and pn , respectively, and l is a distance
metric on the data space.an equivalent reformulation (4) of (3) was developed by
[4] when, where w is a positive semidefinite weight matrix to account for any
transformation on the input feature x and can be estimated from data using
metric learning (see sec."
2051,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"our dataset included mri brain scans from 226 patients performed at an urban
tertiary referral academic medical center that is a comprehensive stroke center."
2052,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"while the whole dataset includes 4,883 (74.7%) normal slices and 1,650 (25.3%)
stroke slices, we further randomly split them into training/validation/test sets
using the ratio 80%/10%/10%."
2053,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"for the training set, we implemented data
augmentation strategies by rotating or flipping each slice."
2054,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"for the cnn model, we used a
resnet-18 [9] architecture, while for the vit model, we first pre-trained a
4-layer vit using a self-supervised pre-training method called masked
autoencoder (mae) [8], using the t1/t2-weighted brain mr images in the ixi
dataset [1]."
2055,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"as our
dataset is unbalanced, we also considered the area under precision-recall curve
(auprc)."
2056,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"computer-aided diagnosis utilizes machine learning techniques to conduct a
pathological diagnosis concerning biomedical imaging data collected from various
pathological modalities, such as computed tomography [19], magnetic resonance
imaging [11], ultrasound [23], and angiography [9]."
2057,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"however, a reliable machine
learning-based cad method usually relies on the supervision of abundant
annotated training data."
2058,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"yet diseased pathological data are rare and diverse,
and acquiring reliable pathological annotations are labor-intensive and
expertise-required."
2059,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"as a result, the difficulty of data collection restricts the
development of the supervised cad.due to the difficulty of acquiring the
abundant annotated training data, the current sota method, i.e., csm [14],
proposes a mil-based wvad manner to specifically tackle one specific disease
detection task, i.e., colorectal cancer diagnosis via colonoscopy."
2060,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"considering
the case of colonoscopy, the csm's anomaly detection setting is used to handle
the rare and diverse diseased pathological data by commonly assuming that only
video-level annotations are available for training."
2061,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"furthermore, its video
setting concerns the temporal correlation within data."
2062,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"the setting of such
mil-based weakly supervision prevents the need for abundant annotated training
data by assuming that merely the video-level annotations, including normal and
diseased ones, are available for training.similar to the previous mil-based wvad
methods [4,13,14], our model assumes all training snippets (consecutive video
frames) within a non-diseased video are all normal snippets, yet each diseased
video has at least one abnormal snippet."
2063,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"with
the decoupled snippet-level feature ingredients, our cfd employs both the normal
and abnormal feature ingredients via a contrastive learning paradigm to
concurrently optimize video-level and snippetlevel disease scores for pursuing
more accurate detection.to assess the proposed contrastive feature decoupling
network, we conduct experiments on two datasets, i.e., polyp and panda-mil."
2064,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"given dataset d comprising normal sub-dataset d 0 and abnormal sub-dataset d 1 ,
we first encode all instances per bag b ∈ d into instance-level feature set f =
{f t } t t=1 ∈ r t ×c via a pre-trained feature extractor e."
2065,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"we then collect all normal instance-level features f ∈ r 1×c from d 0
to learn the memory bank m by using the dictionary learning technique [7] where
d 0 is the normal sub-dataset collected from the training split, w t is the
learned weights within the memory bank learning process, and λ is a
hyperparameter to constrain the memory bank sparsity."
2066,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"we evaluate our model against sotas on the existing polyp [14] dataset and the
panda-mil dataset introduced in this work."
2067,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"please refer to the
supplementary material for the statistics of the two datasets.polyp."
2068,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"this
dataset collects colonoscopy videos from hyper-kvasir [1] and ldpolypvideo [10]."
2069,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"each bag/video is encoded into t = 32 snippets among both datasets
via linear interpolation."
2070,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"the results in table 1
demonstrate that our cfd consistently outperforms all the other methods on two
datasets."
2071,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"precisely, our model achieves the new sota by 1.1% auc and 1.5% ap
improvements on the polyp dataset and 1.09% auc and 2.45% ap improvements on the
panda-mil dataset."
2072,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"figure 2 visualizes one
disease detection result of our cfd model on the panda-mil dataset."
2073,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"the ablation study in
table 2 is conducted on the panda-mil dataset to evaluate the effectiveness of
the memory bank and loss functions in our model."
2074,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"besides, we introduce a new
dataset of prostate cancer detection, i.e., panda-mil, to provide a biomedical
imaging dataset concerning a different pathological modality."
2075,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"experiments
demonstrate that our cfd network achieves new sota performance on the polyp and
panda-mil datasets, indicating that our method effectively addresses the disease
detection task across different pathological modalities."
2076,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"the vlms are first pre-trained to learn
universal representations via large-scale unlabelled data and can be effectively
transferred to downstream tasks."
2077,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"in addition, we also examine the language syntax based prompt
fusion approach as a comparison, and explore several fusion strategies by first
grouping the prompts either with described attributes or categories and then
repeating the fusion process.we evaluate the proposed approach on a broad range
of public medical datasets across different modalities including photography
images for skin lesion detection isic 2016 [2], endoscopy images for polyp
detection cvc-300 [21], and cytology images for blood cell detection bccd."
2078,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"undeed [23], a
semi-supervised classification method, is presented to increase the classifier
accuracy on labeled data and diversity on unlabeled data simultaneously."
2079,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"we collect three public medical image datasets across various modalities
including skin lesion detection dataset isic 2016 [2], polyp detection dataset
cvc-300 [21], and blood cell detection dataset bccd to validate our proposed
approach for zero-shot medical lesion detection."
2080,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"more details on
the dataset and implementation are described in the appendix."
2081,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"as illustrated in table 1, our ensemble guided fusion rivals the
glip [11] with single prompt and other fusion baselines across all datasets."
2082,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"table 2 shows that our method outperforms yolov5, which indicates
fullysupervised models such as yolo may not be suitable for medical scenarios
where a large labeled dataset is often not available."
2083,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"the misclassification problem in some of the single prompts is
corrected (i.e., malignant to benign) on the first dataset."
2084,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"for all datasets,
the candidate boxes are more precise and associated with higher confidence
scores.fine-tuned models can further improve the detection performance."
2085,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"figure 3 shows the
visualization of the zero-shot results across three datasets."
2086,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"fusing prompts by category is
specifically for multi-category datasets to first gather the prompts belonging
to the same category and make further fusion."
2087,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"as
shown in table 5, we perform ablation studies on three datasets."
2088,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"thus, the utilization of domain knowledge from weakly supervised
data becomes a crucial factor in achieving high-quality anomaly detection."
2089,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"similarly, the
reverse process incorporates information from previous reverse states.moreover,
traditional generative models, such as gans [1,12] and normalizing flows [11],
commonly used for pixel-wise anomaly detection, are constrained by one-step data
projection in handling complex data distributions."
2090,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"this dilemma can be overcome
by employing probabilistic diffusion models [13,25] that capture data knowledge
through a series of step-by-step markovian processes [5]."
2091,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"we validate our
framework on two brain medical datasets, demonstrating the effectiveness of the
framework components and showing more accurate detection results of anomaly
regions."
2092,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"overall, we train our non-markovian
diffusion model depending on the current states, the coarse segmentation maps,
image labels, and the original data during the diffusion process with healthy
and diseased images."
2093,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"y denotes the corresponding binary
label for each data."
2094,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"then, we apply hybrid
conditional sampling to noisy data xt to reproduce a healthy one with the same
anatomy structure by conditional data prediction network and the binary
classifier [10]."
2095,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"brats 2020 [2] is a brain tumor segmentation dataset containing the mr sequences
of t1, t1gd, t2, and flair."
2096,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"isles 2022 [19] is an mr image dataset for stroke
lesions segmentation."
2097,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"we train them on
brats2020 and isles datasets."
2098,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"our fndm
outperforms the existing methods in all metrics on both datasets."
2099,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"fndm also
outperforms the previous state-of-theart diffusion method, diffano, by a large
gap of +9.56% dice, -0.98% hdis, and +0.54% vsim on brats dataset, and +19.98%
dice, -1.39% hdis, and +26.73% vsim on isles dataset, revealing that our fndm is
effective to reconstruct the healthy image from diseased to detect the anomaly
regions in brain mr images."
2100,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"thanks to non-markovian procedure and pixel-wise
hybrid guidance, the performance improvement of our method is larger on the
isles dataset where stroke lesions are more challenging due to smaller sizes and
irregular shapes."
2101,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"from table 2, we decompose the overall pixel-wise hybrid
condition into classifier gradient (cg), non-markovian (nm), and memory bank
(mb), comparing all possible combinations on brats2020 dataset."
2102,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"4, we further evaluate the
segmentation performance of diffano [27] and ours across diverse steps on
brats2020 dataset."
2103,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"extensive experiments on two brain datasets reveal the effectiveness and
superiority of our approach for anomaly detection."
2104,MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,"we first perform a strong data augmentation to train the
network for tumor detection and classification on individual frames."
2105,MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,"using complementary information between multi-modal data can greatly improve the
precision of detection."
2106,MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,"then, we utilize scaled dot-product to compute the attention weights
of v cls as:after temporal feature aggregation, f temp is fed into a multilayer
perceptron head to predict the class of tumor.3 experimental results we collect
a renal tumor us dataset of 179 cases from two medical centers, which
is split into the training and validation sets."
2107,MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,"data augmentation strategies are applied synchronously to b-mode and ceus-mode
images for all experiments, including random rotation, mosaic, mixup, and so on."
2108,MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,"this may be because the
generated weights are biased to make similar decisions to the source domain,
thereby reducing model generalization in the external data."
2109,MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,"in this paper, we create the first multi-modal ceus video dataset and propose a
novel attention-based multi-modal video fusion framework for renal tumor
diagnosis using b-mode and ceus-mode us videos."
2110,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"although this method seems reasonable, the number of patients is usually
relatively small, and deep learning models usually require a large amount of
data for training."
2111,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"we conducted rigorous experiments on two
datasets and demonstrated the effectiveness of our method."
2112,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"this innovative approach not only improves the classification
performance at the patient level but also at the slide level, showcasing its
effectiveness and versatility; 3) conducting extensive experiments on two
separate datasets."
2113,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,cd-itb dataset.
2114,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"cd-itb is a private dataset consisting of 853 slides from 163
patients, with binary patient-level labels of cd or itb in a ratio of 103:60 and
tri-class slide-level labels of cd, itb, and normal slides in a ratio of
436:121:296, respectively."
2115,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"the dataset comprises an average
of 2.3k instances per bag, with the largest bag containing over 16k
instances.camelyon17 dataset."
2116,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"camelyon17 [1] is a publicly dataset, and its
training set comprises 500 slides from 100 breast cancer patients with lymph
node metastases."
2117,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"the data folding method is the
same as the cd-itb dataset."
2118,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"abmil with p&sre improves the f1 score from 0.565 to 0.579 for the
cd-itb dataset and from 0.529 to 0.571 for the camelyon17 dataset at the
slide-level, and improves the f1 score from 0.522 to 0.599 for the cd-itb
dataset and from 0.842 to 0.861 for the camelyon17 dataset at the patient-level."
2119,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"by
introducing a transformer, the framework enables iterative interaction and
correction of information between patients and slides, resulting in better
performance at both the patient level and slide level compared to existing
state-of-the-art algorithms on two validation datasets."
2120,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"deep neural networks have recently shown impressive performance on lesion
quantification in positron emission tomography (pet) images [6]; however, they
usually rely on a large amount of well-annotated, diverse data for model
training."
2121,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"to address the data shortage issue, we propose to train a deep model
for lesion detection with synthesized pet images generated from list mode pet
data, which is low-cost and does not require human effort for manual data
annotation.synthesized pet images may exhibit a different data distribution from
real clinical images (see fig."
2122,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"to address domain shifts, domain
adaptation requires access to target data for model training [5,29], while
domain generalization (dg) trains a model with only source data [39] and has
recently attracted increasing attention in medical imaging [1,13,15,18]."
2123,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"most of
current dg methods rely on multiple sources of data to learn a generalizable
model, i.e., multisource dg (mdg); however, multi-source data collection is
often difficult in real practice due to privacy concerns or budget deficits."
2124,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"although single-source dg (sdg) using only one source dataset has been applied
to medical images [12,14,32], very few studies focus on sdg with pet imaging and
the current sdg methods may not be suitable for lesion identification on pet
data."
2125,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"several other sdg
approaches [26,31,34] leverage unique characteristics of the imaging modalities,
e.g., color spectrum of histological stained images, which are not applicable to
pet data.in this paper, we propose a novel single-stage sdg framework, which
learns with human annotation-free, list mode-synthesized pet images for
generalizable lesion detection in real clinical data."
2126,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"compared with domain
adaptation and mdg, the proposed method, while more challenging, is quite
practical for real applications due to the relatively cheaper net data
collection and annotation."
2127,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"specifically, we design a new data augmentation
module, which generates out-of-domain samples from single-source data with
multi-scale random convolutions."
2128,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"trained with a
single-source synthesized dataset, the proposed method provides superior
performance of hepatic lesion detection in multiple cross-scanner real clinical
pet image datasets, compared with the reference baseline and recent
state-of-the-art sdg methods."
2129,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"given a source-domain dataset of
list mode-synthesized 3d pet images and corresponding lesion labels (x s , y s
), the goal of the framework is to learn a lesion detection model h , composed
of e and d, which generalizes to real clinical pet image data."
2130,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"the framework
first feeds synthesized images x s into a random-convolution data augmentation
module a and generates out-of-domain samples x a = a(x s )."
2131,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"in the synthesized pet image dataset, each subject have multiple simulated
lesions of varying size with known boundaries [11], and thus no human annotation
is required."
2132,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"however, this synthesized dataset presents a significant domain
shift from real clinical data, as they have markedly different image textures
and voxel intensity values (see fig."
2133,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"inspired by previous domain
generalization work [39], we introduce a specific data augmentation module to
generate out-of-domain samples from this single-source synthesized dataset for
generalizable model learning (see fig."
2134,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"this module can
preserve global shapes or the structure of objects (e.g., lesions and livers) in
images but distorts local textures, so that the lesion detection model learned
with these augmented images can generalize to unseen real-world pet image data,
which typically have high lesion heterogeneity and divergent texture
styles.given a synthesized input image x s ∈ x s , our data augmentation module
a first performs a random convolution operation r(x s ) with a k × k kernel r,
where the kernel size k and the convolutional weights are randomly sampled from
a multi-scale set k = {1, 3, 5, 7} and a normal distribution n (0, 1/k 2 ),
respectively."
2135,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"this data mixing strategy allows
continuous interpolation between the source domain and a randomly generated
out-of-distribution domain to improve model generalizability."
2136,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"this intensity inversion operation is to ensure
the lesion region has higher intensity values than other regions, mimicking the
image characteristics of real-world pet data in our experiments."
2137,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"because of random convolution weights, the original synthesized x s and
augmented x a data can have substantially different image appearances."
2138,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"specifically, we
incorporate a domain classifier c on top of the encoder e to perform a pretext
task of domain discrimination, i.e., predict whether each input image is from
the original synthesized data x s or augmented data x a ."
2139,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"with the mse loss, the patch-based gradient reversal penalizes image structures
and enhances feature robustness and invariance to style shifts at the
local-patch level, so that the lesion detection model h (i.e., e followed by d)
learned with source data annotations is directly applicable to unseen domains
[4,24], based on the covariate shift assumption [20].formally, let x = {x s , x
a } denote the input data for the encoder e and z = {z s , z a } represent the
corresponding domain category labels, with z s and z a for the original source
images x s and corresponding random convolutionaugmented image x a ,
respectively."
2140,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"for source-domain
data (x s , y s ), the augmented images x a have the same gold-standard lesion
labels y a = y s , each of which is a 3d binary image with 1 s for lesion voxels
and 0 s for non-lesion regions."
2141,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"the combo loss l det can further help address the data
imbalance issue [22], i.e., lesions have significantly fewer voxels than the
non-lesion regions including the background."
2142,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,datasets.
2143,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"we evaluate the proposed method with multiple 68 ga-dotatate pet liver
net image datasets that are acquired using different pet/ct scanners and/or
imaging protocols."
2144,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"the synthesized source-domain dataset contains 103 simulated
subjects, with an average of 5 lesions and 153 transverse slices per subject."
2145,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"this dataset is synthesized using list mode data from a single real, healthy
subject acquired on a ge discovery mi pet/ct scanner with list mode
reconstruction [11,37]."
2146,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"we collect two additional real 68 ga-dotatate pet liver
net image datasets that serve as unseen domains."
2147,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"the first dataset (real1) has
123 real subjects with about 230 hepatic lesions in total and is acquired using
clinical reconstructions with a photomultiplier tube-based pet/ct scanner (ge
discovery ste)."
2148,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"the second real-world dataset (real2) consists of 65 cases with
around 113 lesions and is acquired from clinical reconstructions using a digital
pet/ct scanner (ge discovery mi)."
2149,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"following [28,38], we randomly split the
synthesized dataset and the real1 dataset into 60%, 20% and 20% for training,
validation and testing, respectively."
2150,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"due to the relatively small size of real2,
we use a two-fold cross-validation for model evaluation on this dataset."
2151,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"here we
split the real datasets to learn fully supervised models for a comparison with
the proposed method."
2152,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,table 1 presents the comparison results on the two unseen-domain datasets.
2153,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"our
method significantly outperforms the state-of-the-art approaches in terms of f 1
score, with p-value < 0.05 in student's t-test for almost all cases on both
datasets."
2154,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"this indicates that compared with the competitor approaches, our method
is relatively more effective and stable in learning generalizable
representations for lesion detection in a very challenging situation, i.e.,
learning with a single-source synthesized pet image dataset to generalize to
real clinical data.the qualitative results are provided in the supplementary
material.ablation study."
2155,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"in table 1, the baseline represents a lesion detection
model trained with the source data but without the data augmentation module a, l
con or l cls ."
2156,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"the u pper-bound means training with real-world images
and gold-standard labels from the testing datasets."
2157,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"we note that using the data
augmentation module a can significantly improve the lesion detection performance
compared with the baseline on the real1 dataset, and combining data augmentation
and patch gradient reversal can further close the gap to the u pper-bound model."
2158,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"our method also outperforms the baseline model by a large margin on the real2
dataset, suggesting the effectiveness of our method."
2159,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"in addition, we observe a similar trend of the f 1 curve for the λ cls ,
especially for the real1 dataset, and this indicates the necessity of the domain
classification pretext task."
2160,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"we propose a novel sdg framework that uses only a single dataset for hepatic
lesion detection in real clinical pet images, without any human data
annotations."
2161,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"with a specific data augmentation module and a new patch-based
gradient reversal, the framework can learn domain-invariant representations and
generalize to unseen domains."
2162,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"the experiments show that our method outperforms
the reference baseline and recent state-of-the-art sdg approaches on
cross-scanner or -protocol real pet image datasets."
2163,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"identifying unusual patterns in data is of great interest in many applications
such as medical diagnosis, industrial defect inspection, or financial fraud
detection."
2164,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"however, since it is more
feasible to obtain large data sets with normal samples, it is common to detect
outliers by detecting patterns that deviate from the expected normative
distribution.reconstruction-based aes have emerged as a very popular framework
for unsupervised anomaly detection and are widely adopted in medical imaging
[2]."
2165,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"the widely held popular belief is that aes can learn the distribution of the
training data and identify outliers from inaccurate reconstructions of abnormal
samples [31]."
2166,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"let x ⊂ r n be the data space that describes normal instances for a given
task.the manifold hypothesis implies that there exists a low-dimensional
manifold m ⊂ r d ⊂ x where all the points x ∈ x lie, with d n [9]."
2167,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"for example,
a set of images in pixel space x could have a compact representation describing
features like structure, shape, or orientation in m.given a set of unlabeled
data x 1 , .., x n ∈ x the objective of unsupervised representation learning is
to find a function f : r n → r d and its inverse g : r d → r n , such that x ≈
g(f (x)), with the mapping f defining the lowdimensional manifold m."
2168,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"the core
assumption of unsupervised anomaly detection is that once such functions f and g
are found, the learned manifold m would best describe the normal data samples in
x and results in high reconstruction errors for data-points x / ∈ x , that we
call anomalous."
2169,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"in medical imaging, the set x
describes the healthy anatomy and the data set x usually contains images with
both healthy and pathological regions."
2170,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"aes aim to extract meaningful representations from data, by learning to compress
inputs to a lower-dimensional manifold and reconstruct them with minimal error."
2171,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"given
a dataset x = {x 1 , .., x n } we optimize the encoder and decoder with
parameters θ, φ to minimize the mse loss between the input and its
reconstruction."
2172,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"in contrast, the detection of pathology on chest
radiographs is much more difficult due to the high variability and complexity of
nominal features and the diversity and irregularity of abnormalities.datasets."
2173,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"we used the rsna
dataset [35], which contains 10k cxr images of normal subjects and 6k lung
opacity cases."
2174,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"for the detection of covid-19, we used the padchest dataset [3]
containing cxr images manually annotated by trained radiologists."
2175,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"spatial aes and daes have a tendency
to reproduce the input and produce reconstructions of structures that are not
included in the training distribution, such as medical devices and pathologies,
despite not being trained on ood data."
2176,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,dataset.
2177,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,"in this study, we used data from shengjing hospital to train our method
with 892 patients, and data from three other centers, including guangdong
provincial people's hospital, tianjin medical university and sun yatsen
university cancer center for independent testing with 178 patients."
2178,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,"we also reported the survival auc, which
estimates the cumulative area under the roc curve for the first 36
months.implementation details: we used nested 5-fold cross-validation and
augmented the training data by rotating volumetric tumors in the axial direction
and randomly selecting cropped regions with random shifts."
2179,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"therefore, a domain and content
simultaneously aware data augmentation method is urgently needed to eliminate
and avoid the distortion challenges during tumor generation."
2180,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"1 experimental results on two public tumor
segmentation datasets show that dcaug improves the tumor segmentation accuracy
compared with state-of-theart tumor augmentation methods."
2181,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"-experimental results on two public tumor segmentation
datasets demonstrate that dcaug improves the diversity and quality of synthetic
tumor images."
2182,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"atlas dataset [11]: the atlas dataset consists of 229 t1-weighted mr images from
220 subjects with chronic stroke lesions."
2183,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"kits19 dataset [4]: the kits19 consists of 210 3d abdominal ct
images with kidney tumor subtypes and segmentation of kidney and kidney tumors."
2184,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"the default hyperparameters and default traditional data
augmentation (tda) including rotation, scaling, mirroring, elastic deformation,
intensity perturbation are used when model training."
2185,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"the maximum number of
training epochs was set to 500 for the two datasets."
2186,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"4 show that compared with other
stateof-the-art methods, including mixup [16], cutmix [15], carvemix [17],
selfmix [19], stylemix [5], nnunet combined with dcaug achieves the highest
improvement on the two datasets, which convincingly demonstrates the innovations
and contribution of dcaug in generating higher quality tumor."
2187,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"what's more, the potential of dcaug
in an extremely low-data regime is also demonstrated."
2188,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"we randomly select 25% and
50% of data from the training set same as training data."
2189,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"dataset num means and standard deviations of the dice coefficients (%) tda mixup
cutmix carvemix selfmix stylemix dcaug atlas 25% 49.87 ± 32.19 49.18 ± 32.72
41.19 ± 33.98 55.16 ± 32.16 57.89 ± 31.05 52.84 ± 34.36 56.43 ± 32.33 50% 56.72
± 30.74 58.40 ± 29.35 54.25 ± 30.24 58.34 ± 31.32 58.81 ± 31.75 58.04 ± 30.39
59.75 ± 31.41 100% 59.39 ± 32.45 59.33 ± 33.06 56.11 ± 32.44 62.32 ± 31.10 63.5
± 31.06 64.00 ± 28.89 64.64 ± 29.91 cross-domain consistency learning (cdcl)
strategy can preserve the
domaininvariant content information of tumor in the synthesized images x b→a a ,
x a→b b for avoiding content distortion."
2190,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,"the entire dataset is denoted by, where x i is the
i-th non-contrast ct volume, with y i being the voxel-wise label map of the same
size as x i and k channels."
2191,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,dataset and ground truth.
2192,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,"our study analyzed a dataset of ct scans collected
from guangdong province people's hospital between years 2018 and 2020, with
2,139 patients consisting of 787 gastric cancer and 1,352 normal cases."
2193,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,"we randomly selected 20% of the
training data as an internal validation set."
2194,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,"we
followed [8] to augment data."
2195,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,"readers were
informed that the dataset might contain more tumor cases than the standard
prevalence observed in screening, but the proportion of case types was not
disclosed."
2196,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,"we obtain the 95% confidence interval of auc,
sensitivity, and specificity values from 1000 bootstrap replicas of the test
dataset for statistical analysis."
2197,Self-supervised Polyp Re-identification in Colonoscopy,"many of those new cad applications require
aggregation of all available data on a polyp into a single unified entity."
2198,Self-supervised Polyp Re-identification in Colonoscopy,"we use a tracking by detection method [27]), in this paper we focus on the
second step.to avoid manual data annotation, which is extremely ineffective in
our case, we turn to self-supervision and adapt the widely used contrastive
learning approach [5] to video input and object tracking scenario.as tracklet
re-identification is a sequence-to-sequence matching problem, the standard
solution is comparing sequences element-wise and then aggregating the
per-element comparisons, e.g."
2199,Self-supervised Polyp Re-identification in Colonoscopy,"moreover,
self-supervised techniques using extensive unannotated datasets has exhibited
substantial advantages within the medical domain [12].hence, we turn to simclr
[5], a contrastive self-supervised learning technique, which requires no manual
labeling."
2200,Self-supervised Polyp Re-identification in Colonoscopy,dataset.
2201,Self-supervised Polyp Re-identification in Colonoscopy,"longer tracklets provide more
information for polyp classification.here, we investigate if the proposed reid
model, used to group disjoint tracklets of the same polyp, can increase the
accuracy of cadx.data."
2202,Self-supervised Polyp Re-identification in Colonoscopy,"the result on the
manually annotated data is the accuracy upper-bound and is brought as a
reference point."
2203,Self-supervised Polyp Re-identification in Colonoscopy,"its applicability to medical contexts is of
particular relevance, as medical data annotation often requires specific
expertise and may be costly and time consuming."
2204,Self-supervised Polyp Re-identification in Colonoscopy,"we use this method to train a
polyp re-identification model (reid) from large unlabeled data, and show that
using the reid model as part of a cadx system enhances the performance of polyp
classification."
2205,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"1(a), we show the target
motion speed [26] 1 on imagenetvid [14] (natural) and ldpolypvideo [9]
(colonoscopy) dataset."
2206,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"figure 1(b) shows the performance of fgfa [26] on two datasets with increasing
reference frames."
2207,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"(3) extensive experiments demonstrate that our yona
achieves new state-of-the-art performance on three large-scale public video
polyp detection datasets."
2208,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"for the fairness of the experiments, we
keep the same dataset settings for yona and all other methods."
2209,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"random rotation and flip with probability p = 0.5 are used
for data augmentation."
2210,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"besides,
yona achieves the best trade-off between accuracy and speed compared with all
other image-based sotas across all datasets."
2211,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"second, for video-based
competitors, previous video object detectors with multiple frame collaborations
lack the ability for accurate detection on challenging datasets."
2212,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"specifically,
yona surpasses the second-best stft [19] by 2.2%, 3.0%, and 1.3% on f1 score on
three datasets and 33.8 on fps."
2213,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"obtaining ultrasound video data with pathology gold standard results
poses a major challenge."
2214,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"consequently, while there are many breast ultrasound image datasets [1,28],
breast ultrasound video datasets remain scarce, with only one relatively small
dataset [15] containing 188 videos available currently.given the difficulties in
collecting ultrasound video data, we investigate the feasibility of enhancing
the performance of ultrasound video classification using a static image dataset."
2215,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"the images in the ultrasound dataset are keyframes of a lesion that
exhibit the clearest appearance and most typical symptoms, making them more
discriminative for diagnosis."
2216,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"our approach leverages both
image (keyframes) and video datasets to train the network."
2217,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"due
to the feature centers being generated by the larger scale image dataset, it
provides more accurate and discriminative feature centers which can guide the
video frame attention to focus on important frames, and finally leads to better
video classification.our experimental results on the public busv dataset [15]
show that our kga-net significantly outperforms other video classification
models by using an external ultrasound image dataset."
2218,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"we
analyze the relationship between ultrasound video data and image data, and
propose the coherence loss to use image feature centers to guide the training of
frame attention."
2219,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"we propose kga-net, which adopts a static image dataset to
boost the performance of ultrasound video classification."
2220,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"kga-net significantly
outperforms other video baselines on the busv dataset."
2221,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"however, all of them are based on
image datasets, such as busi [1], while few works focus on the video modality."
2222,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"fortunately, the recent publicly available dataset busv [15] has
made the research on the task of bus video-based classification possible."
2223,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"in
this paper, we build our model based on this dataset.video recognition based on
neural networks."
2224,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"empirically,
we set λ = 1 in our experiments.during inference, to perform classification on
video data, the video classification network can be utilized individually for
prediction."
2225,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,datasets.
2226,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"we use the public busv dataset [15] for video classification and the
busi dataset [1] as the image dataset."
2227,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"for the busv dataset, we use the official data split in [15]."
2228,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,all images of the busi dataset are adopted to train our kga-net.
2229,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"for fairness comparison, we train these models
using both video and image data, treating images as static videos."
2230,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"evaluation
metrics are reported on the busv test set for performance assessment.as shown in
table 1, by leveraging the guidance of the image dataset, our kga-net
significantly surpasses all other models on all of the metrics."
2231,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"the feature centers
formed by the image dataset with larger data size and clear appearance
effectively improve the accuracy of frame attention hence boosting the video
classification performance."
2232,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"to portray the effect of using the
image dataset, we train the kga-net using busv dataset alone in the first row of
table 2."
2233,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"without the image dataset, we generate the feature centers from the
video frames."
2234,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"as a result, the performance significantly drops due to the
decrease in dataset scale."
2235,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"it also shows that the feature centers generated by
the image dataset are more discriminative than that of the video dataset."
2236,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"our kga-net takes as input both the video data and image data to
train the network."
2237,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"in medical field, automatic analysis of medical image data has
actively been studied."
2238,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"to train our proposed model, we utilize a dataset consisting of
image and text pairs."
2239,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"as transformer has demonstrated its effectiveness in handling the long-range
dependency in sequential data through self-attention [1], it performs well in
various fields requiring nlp or contextual information analysis of data."
2240,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"this architecture is particularly suitable for our purpose
because it can be successfully trained on a small amount of data."
2241,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"2, we multiply the learnable parameter (l ∈ r 1×(hw )
) by the global text representation (v t ) to match the dimension of the text
feature with that of the image feature map as the text feature map f t is used
as key and value, and the image feature map f i is used as a query to perform
self-attention aswhere h q , h k , and h v are convolution layers with a kernel
size of 1, and q, k, and v are queries, keys, and values for
self-attention.finally, by upsampling the low-dimensional cp am t g obtained
through crossattention of text and image together with skip-connection, more
accurate segmentation prediction can express the detailed information of an
object.3 experiments medical datasets."
2242,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"we evaluated cp am t g using three datasets: monuseg [8]
dataset, qata-cov19 [6] dataset, and sacroiliac joint (sij) dataset."
2243,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"the first
two datasets are the same benchmark datasets used in [10]."
2244,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"sij is the dataset privately prepared for this study
which consists of 804 mri slices of nineteen healthy subjects and sixty patients
diagnosed with axial spondyloarthritis."
2245,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"for a better
training, data augmentation was used."
2246,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"we randomly rotated images by -20 • ∼ +20
• and conducted a horizontal flip with 0.5 probability for only the monuseg and
qata-cov19 datasets."
2247,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"furthermore, cp am
t g achieves a better performance by 1 to 3% than lvit [10] on all datasets."
2248,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"figure 3 also shows that even on the qata-cov19 and
monuseg datasets, cp am t g predicted the most accurate segmentation masks (see
the red box areas)."
2249,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"specifically, for the sij dataset, we
examined the effect of attention in extracting feature maps through comparison
with backbone networks (u-net) and pam."
2250,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"the proposed model has a
composite structure of position attention and cross-attention in that the key
and value are from text data, and the query is created from the image."
2251,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"mil
approaches have proven to work well in academic research on histopathology data
[1,17,29] as well as in commercial applications [26]."
2252,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"most mil methods for
digital pathology employ an attention mechanism as it increases the reliability
of the algorithms, which is essential for successful clinical adoption
[14].domain shift in dl occurs when the data distributions of testing and
training differs [20,34]."
2253,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"to address this problem previous
work either use domain adaptation when data from the target domain is available
[32], or domain generalisation when the target data is unavailable [34]."
2254,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"hence, it is important
to provide indications of the expected performance on a target dataset without
requiring annotations [5,25]."
2255,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"another related topic is out-of-distribution (ood)
detection [33] which aims to detect individual samples that are ood, in contrast
to our objective of estimating a difference of expected performances between
some datasets."
2256,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"alternatively, a drop in performance can be estimated by comparing the model's
softmax outputs [8] or some hidden features [24,28] acquired on in-domain and
domain shift datasets."
2257,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"hence, it is not clear how well
they will work in such a scenario.in this work, we evaluate an attention-based
mil model on unseen data from a new hospital and propose a way to quantify the
domain shift severity."
2258,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"we split the data from the new
hospital into several subsets to investigate clinically realistic scenarios
triggering different levels of domain shift."
2259,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"showing how fdd can help to identify subsets of patient
cases for which mil performance is worse than reported on the in-domain test
data; 3."
2260,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"fréchet inception distance (fid) [10] is commonly used to measure similarity
between real and synthetically generated data."
2261,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"inspired by fid, we propose a
metric named fréchet domain distance (fdd) for evaluating if a model is
experiencing a drop in performance on some new dataset."
2262,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"the fréchet distance
(fd) between two multivariate gaussian variables with means µ 1 , µ 2 and
covariance matrices c 1 , c 2 is defined as [3]:we are interested in using the
fd for measuring the domain shift between different wsi datasets x d ."
2263,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"grand challenge camelyon data [16] potentially large shift as some patients have
already started neoadjuvant treatment as well as the tissue may be affected from
the procedure of sentinel lymph node removal."
2264,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"68 wsis with lobular carcinoma
(28 wsis with metastases): potentially large shift as it is a rare type of
carcinoma and relatively difficult to diagnose.the datasets of lobular and
ductal carcinomas each contain 50 % of wsis from sentinel and axillary lymph
node procedures."
2265,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"all datasets are
publicly available to be used in legal and ethical medical diagnostics research."
2266,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"we trained, with default settings, 10 clam models to classify wsis of breast
cancer metastases using a 10-fold cross-validation (cv) on the training data."
2267,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,the test data was kept the same for all 10 models.
2268,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"whereas extremely large
variations in label prevalence could reduce the reliability of the mcc metric,
this is not the case here as label prevalence is similar (35-45%) in our test
datasets."
2269,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"as there is no related work on domain shift detection in the mil setting, we
selected methods developed for supervised algorithms as baselines:-the model's
accumulated uncertainty between two datasets."
2270,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"deep ensemble [15] (de) and
difference in confidence with entropy [8] (doc) compare the mean entropy over
all data points."
2271,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"-the accumulated confidence of a model across two
datasets."
2272,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,doc [8] can be measured on the mean softmax scores of two datasets.
2273,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"2.2) with both methods.for all possible pairs of camelyon and the other
test datasets, and for the 10 cv models, we compute the domain shift measures
and compare them to the observed drop in performance."
2274,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"while showing similar trends, there
is some discrepancy in the level of domain shift represented by the datasets due
to the differences between the mcc and roc-auc measures.as we deemed mcc to
better represent the clinical use situation (see sect."
2275,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"we observe the largest domain shift in terms of mcc on
axillary nodes followed by lobular carcinoma and full brln datasets."
2276,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"there seems
to be no negative effect from processing the sentinel nodes data."
2277,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"clam models
achieved better performance on ductal carcinoma compared to the in-domain
camelyon test data.table 2 summarises the pearson correlation between the change
in performance, i.e., the mcc difference between camelyon and other test
datasets, and the domain shift measures for the same pairs."
2278,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"figure 1
shows how individual drop in performance of model-dataset combinations are
related to the f dd 64 metric."
2279,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"for most models detecting larger drop in
performance (> 0.05) is easier on axillary lymph nodes data than on any other
analysed dataset."
2280,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"some previous work claim that mil is more
robust to domain shift as it is trained on more data due to the reduced costs of
data annotation [1,17]."
2281,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"we argue that domain shift will still be a factor to
consider as an algorithm deployed in clinical practice is likely to encounter
unseen varieties of data."
2282,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"however, it may require more effort to determine what
type of changes in data distribution are critical."
2283,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"our results show that domain
shift is present between the wsis from the same hospital (camelyon data) and
another medical centre (brln data)."
2284,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"however, as clinically relevant subsets of
brln data are analysed, stark differences in performance and reliability
(indicated by the standard deviation) are revealed."
2285,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"however, the drop is easier to detect on axillary and lobular datasets
compared to others."
2286,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,"for
example, public datasets such as the liver tumor segmentation benchmark (lits)
[1] fostered a series of works aiming to segment liver tumors with improved
convolutional neural network (cnn) backbones [9,13] and lesion edge information
[15]."
2287,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,"we collect a large-scale dataset with both tumor and
non-tumor subjects, where the non-tumor subjects includes not only healthy ones,
but also patients with various diffuse liver diseases such as steatosis and
hepatitis to improve the robustness of the algorithm."
2288,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,"it contains three branches with bottomup cooperation: the segmentation map from
the pixel branch helps to initialize the lesion branch, which is an improved
mask transformer aiming to segment and classify each lesion; the patient branch
aggregates information from the whole image and predicts image-level labels of
each lesion type, with regularization terms to encourage consistency with the
lesion branch.we collected a large-scale multi-phase dataset containing 810
non-tumor subjects and 939 tumor patients."
2289,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,data.
2290,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,"our dataset contains 810 normal subjects and 939 patients with liver
tumors."
2291,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,"in the former setting, both
normal and patient data are used and randomly split into 1149 training, 100
validation, and 500 testing."
2292,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,"in the latter one, only patient data are used with
641 training, 100 validation, and 200 testing."
2293,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,"we first train an nnu-net on public datasets to segment
liver and surrounding organs (gallbladder, hepatic vein, spleen, stomach, and
pancreas), and then crop the liver region to train plan."
2294,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,"extensive data
augmentation is applied including random cropping, scaling, flipping, elastic
deformation, and brightness adjustment [8].during training, we first pretrain
the backbone and the pixel branch for 500 epochs, and then train the whole
network for another 500 epochs.patient-level results."
2295,Self-supervised Learning for Endoscopic Video Analysis,"yet the success of
such ai systems heavily relies on acquiring annotated data which requires
experts of specific knowledge, leading to an expensive, prolonged process."
2296,Self-supervised Learning for Endoscopic Video Analysis,"in
the last few years, self-supervised learning (ssl [5][6][7][8]) has been shown
to be a revolutionary strategy for unsupervised representation learning,
eliminating the need to manually annotate vast quantities of data."
2297,Self-supervised Learning for Endoscopic Video Analysis,"training
large models on sizable unlabeled data via ssl leads to powerful representations
which are effective for downstream tasks with few labels."
2298,Self-supervised Learning for Endoscopic Video Analysis,"we first experiment solely on public datasets,
cholec80 [32] and polypsset [33], demonstrating performance on-par with the top
results reported in the literature."
2299,Self-supervised Learning for Endoscopic Video Analysis,"yet, the power of ssl lies in large data
regimes."
2300,Self-supervised Learning for Endoscopic Video Analysis,"therefore, to exploit msns to their full extent, we collect and build
two sizable unlabeled datasets for laparoscopy and colonoscopy with 7, 700
videos (>23m frames) and 14, 000 videos (>2m frames) respectively."
2301,Self-supervised Learning for Endoscopic Video Analysis,"through
extensive experiments, we find that scaling the data size necessitates scaling
the model architecture, leading to state-of-the-art performance in surgical
phase recognition of laparoscopic procedures, as well as in polyp
characterization of colonoscopic videos."
2302,Self-supervised Learning for Endoscopic Video Analysis,"furthermore, the proposed approach
exhibits robust generalization, yielding better performance with only 50% of the
annotated data, compared with standard supervised learning using the complete
labeled dataset."
2303,Self-supervised Learning for Endoscopic Video Analysis,"this shows the potential to reduce significantly the need for
expensive annotated medical data."
2304,Self-supervised Learning for Endoscopic Video Analysis,"ai solutions have shown remarkable
performance in recognizing surgical phases of cholecystectomy procedures
[17,18,32]; however, they typically require large labelled training datasets."
2305,Self-supervised Learning for Endoscopic Video Analysis,"a recent work [27] presented an extensive
analysis of modern ssl techniques for surgical computer vision, yet on
relatively small laparoscopic datasets.optical polyp characterization."
2306,Self-supervised Learning for Endoscopic Video Analysis,"however,
training such automatic optical biopsy systems relies on a large body of
annotated data, while ssl has not been investigated in this context, to the best
of our knowledge.3 self-supervised learning for endoscopy ssl approaches have
produced impressive results recently [5][6][7][8], relying on two key factors:
(i) effective algorithms for unsupervised learning and (ii) training on
large-scale datasets."
2307,Self-supervised Learning for Endoscopic Video Analysis,"additionally, we present our large-scale data collection
(see fig."
2308,Self-supervised Learning for Endoscopic Video Analysis,"4, we show that training
msns on these substantial datasets unlocks their potential, yielding effective
representations that transfer well to public laparoscopy and colonoscopy
datasets."
2309,Self-supervised Learning for Endoscopic Video Analysis,"recently, masked siamese
networks [2] have set a new state-of-the-art among ssl methods on the imagenet
benchmark [29], with a particular focus on the low data regime."
2310,Self-supervised Learning for Endoscopic Video Analysis,"this is of great
interest for us since our downstream datasets are typically of small size
[32,33]."
2311,Self-supervised Learning for Endoscopic Video Analysis,"applying msns on the large datasets described
below, generates representations that serve as a strong basis for various
downstream tasks, as shown in the next section."
2312,Self-supervised Learning for Endoscopic Video Analysis,"we compiled a dataset of laparoscopic procedures videos exclusively
performed on patients aged 18 years or older."
2313,Self-supervised Learning for Endoscopic Video Analysis,"the dataset consists of 7,877
videos recorded at eight different medical centers in israel."
2314,Self-supervised Learning for Endoscopic Video Analysis,"the dataset
predominantly consists of the following procedures: cholecystectomy (35%),
appendectomy (20%), herniorrhaphy (12%), colectomy (6%), and bariatric surgery
(5%)."
2315,Self-supervised Learning for Endoscopic Video Analysis,"the remaining 21% of the dataset encompasses various standard laparoscopic
operations."
2316,Self-supervised Learning for Endoscopic Video Analysis,"each video recording was sampled at a rate of 1 frame
per second (fps), resulting in an extensive dataset containing 23.3 million
images."
2317,Self-supervised Learning for Endoscopic Video Analysis,"we
have curated a dataset comprising 13,979 colonoscopy videos of patients aged 18
years or older."
2318,Self-supervised Learning for Endoscopic Video Analysis,"to ensure high-quality data, we
filtered out detections with confidence scores below 0.5."
2319,Self-supervised Learning for Endoscopic Video Analysis,"our experimental protocol is the following: (i) first, we perform ssl
pretraining with msns over our unlabeled private dataset to learn informative
and generic representations, (ii) second we probe these representations by
utilizing them for different public downstream tasks."
2320,Self-supervised Learning for Endoscopic Video Analysis,"(b) polypsset [33]: a unified dataset of
155 colonoscopy videos (37,899 frames) with labeled polyp classes (hyperplastic
or adenoma) and bounding boxes."
2321,Self-supervised Learning for Endoscopic Video Analysis,"table 1 compares the results of pretraining with different datasets
(public and private) and model sizes."
2322,Self-supervised Learning for Endoscopic Video Analysis,"we present results for the cholecystectomy
phase recognition task based on fine-tuned models and for the optical polyp
characterization task based on linear evaluation, due to the small size of the
public dataset."
2323,Self-supervised Learning for Endoscopic Video Analysis,"as baselines, we report fully-supervised resnet50 results,
trained on public datasets."
2324,Self-supervised Learning for Endoscopic Video Analysis,"ssl pretraining
on public datasets (without labels) provides comparable or better results than
fully supervised baselines."
2325,Self-supervised Learning for Endoscopic Video Analysis,"importantly, we see that the performance gap becomes prominent when
using the large scale private datasets for ssl pretraining."
2326,Self-supervised Learning for Endoscopic Video Analysis,"when using the private colonoscopy dataset the macro f1 improves by 11.5%
compared to the fully supervised baseline."
2327,Self-supervised Learning for Endoscopic Video Analysis,"notice that the performance improves
with scaling both model and private data sizes, demonstrating that both factors
are crucial to achieve optimal performance."
2328,Self-supervised Learning for Endoscopic Video Analysis,"note that msns have originally been found to
produce excellent features for low data regime [2]."
2329,Self-supervised Learning for Endoscopic Video Analysis,in table 2d) we study the effect of data augmentation.
2330,Self-supervised Learning for Endoscopic Video Analysis,"we observe that
excellent performance is achieved with only 10 epochs of finetuning on medical
data when starting from a strong dino checkpoint [6]."
2331,Self-supervised Learning for Endoscopic Video Analysis,"this study showcases the use of masked siamese networks to learn informative
representations from large, unlabeled endoscopic datasets."
2332,Self-supervised Learning for Endoscopic Video Analysis,"moreover, this methodology displays strong generalization, achieving comparable
performance with just 50% of labeled data compared to standard supervised
training on the complete labeled datasets."
2333,Self-supervised Learning for Endoscopic Video Analysis,"this dramatically reduces the need
for annotated medical data, thereby facilitating the development of ai methods
for healthcare."
2334,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,"extensive
experiments on the public dataset of hecktor 2022 [7] demonstrate that our xsurv
outperforms state-of-the-art survival prediction methods, including the
top-performing methods in hecktor 2022."
2335,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,"we adopted the training dataset of hecktor 2022 (refer to
https://hecktor.grand-cha llenge.org/), including 488 h&n cancer patients
acquired from seven medical centers [7], while the testing dataset was excluded
as its ground-truth labels are not released."
2336,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,"the patients from two centers (chum and chuv) were used for
testing and other patients for training, which split the data into 386/102
patients in training/testing sets."
2337,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,"data
augmentation was applied in real-time during training to minimize overfitting,
including random affine transformations and random cropping to 112 × 112 × 112
voxels."
2338,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,"validation was performed after every 200 training iterations and the
model achieving the highest validation result was preserved.in our experiments,
one training iteration (including data augmentation) took roughly 4.2 s, and one
inference iteration took roughly 0.61 s."
2339,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,"extensive
experiments have shown that the proposed framework and blocks enable our xsurv
to outperform state-of-the-art survival prediction methods on the
well-benchmarked hecktor 2022 dataset."
2340,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,"therefore, the
contributions of this work can be summarized as: 1) a novel graph-based model
for predicting survival that extracts both local and global properties by
identifying morphological super-nodes; 2) introducing a fine-coarse feature
distillation module with 3 various strategies to aggregate interactions at
different scales; 3) outperforming sota approaches in both risk prediction and
patient stratification scenarios on two datasets; 4) publishing two large and
rare prostate cancer datasets containing more than 220 graphs for active
surveillance and 240 graphs for brachytherapy cases."
2341,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,"we utilize two prostate cancer (pca) datasets to evaluate the performance of our
proposed model."
2342,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,"although majority of patients in our cohort are classified as low-risk based on
nccn guidelines [21], a significant subset of them experienced disease upgrade
that triggered definitive therapy (range: 6.2 to 224 months after diagnosis).the
second dataset (pca-bt) includes 105 pca patients with low to high risk disease
who went through brachytherapy."
2343,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,"we also utilized the prostate
cancer grade assessment (panda) challenge dataset [7] that includes more than
10,000 pca needle biopsy slides (no outcome data) as an external dataset for
training the encoder of our model."
2344,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,"while none of the baselines are capable of assigning patients into
risk groups with statistical significance, our distillation policies achieve
significant separation in both pca-as and pca-bt datasets; suggesting that
global histo-morphological properties improve patient stratification
performance."
2345,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,"however, the best baseline with vit still has poorer performance
compared to our model in both datasets, while the number of parameters (reported
for vit embeddings' size in table 1) in our full-model is about half of this
baseline."
2346,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,"most existing dg works
are proposed for classification tasks [12,13] and they can be roughly grouped
into data augmentation-, representation learning-, and optimization-based
methods."
2347,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,"the first category of methods [14][15][16][17] focus on the way to
diversify training data styles and expect the enriched styles cover those
appeared in target domains."
2348,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,"the proposed method is evaluated on four datasets, including two h&e stained
image datasets consep [3] and cpm17 [28] and two ihc stained datasets deepliif
[29] and bc-deepliif [29,32]."
2349,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,"in
the experiments, models trained on one of the datasets will be evaluated on the
three unseen ones."
2350,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,"to avoid the influence of the different sample numbers of the
datasets, we calculate the average scores within each unseen domain respectively
and then average them across domains.in this paper, we re-implement some
existing popular domain generalization algorithms for comparisons under the same
training conditions."
2351,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"this is also a
big cause of concern for publicly available h&e/ihc cell segmentation datasets
with immune cell annotations from single pathologists."
2352,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"this requires only affine registration to align the digitized
restained images to obtain non-occluded signal intensity profiles for all the
markers, similar to mif staining/scanning.in this paper, we introduce a new
dataset that can be readily used out-ofthe-box with any artificial intelligence
(ai)/deep learning algorithms for spatial characterization of tumor immune
microenvironment and several other use cases.to date, only two denovo stained
datasets have been released publicly: bci h&e and singleplex ihc her2 dataset
[7] and deepliif singleplex ihc ki67 and mif dataset [2], both without any
immune or tumor markers."
2353,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"in contrast, we release the first denovo mif/mihc
stained dataset with tumor and immune markers for more accurate characterization
of tumor immune microenvironment."
2354,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"the complete staining protocols for this dataset are given in the accompanying
supplementary material."
2355,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"it provides a standardized dataset to demonstrate the
equivalence of the two methods and a source that can be used to calibrate other
methods."
2356,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"in this section, we demonstrate some of the use cases enabled by this
high-quality ai-ready dataset."
2357,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"specifically, the model consists of two
sub-networks:(a) marker generation: this sub-network is used for generating mif
marker data from the generated stylized image."
2358,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"the cgan
network consists of a generator, responsible for generating mif marker images
given an ihc image, and a discriminator, responsible for distinguishing the
output of the generator from ground truth data."
2359,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"we
extracted 268 tiles of size 512×512 from this final segmented and co-registered
dataset."
2360,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"we randomly extracted tiles from
the lyon19 challenge dataset [14] to use as style ihc images."
2361,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"using these
images, we created a dataset of synthetically generated ihc images from the
hematoxylin and its marker image as shown in fig."
2362,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"3.we evaluated the
effectiveness of our synthetically generated dataset (stylized ihc images and
corresponding segmented/classified masks) using our generated dataset with the
nuclick training dataset (containing manually segmented cd3/cd8 cells) [6]."
2363,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"we
randomly selected 840 and 230 patches of size 256 × 256 from the created dataset
for training and validation, respectively."
2364,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"nuclick training and validation sets
[6] comprise 671 and 200 patches, respectively, of size 256 × 256 extracted from
lyon19 dataset [14]."
2365,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"as shown in
table 2, models trained with our synthetic training set outperform those trained
solely with nuclick data in all metrics.we also tested the trained models on
1,500 randomly selected images from the training set of the lymphocyte
assessment hackathon (lysto) [1], containing image patches of size 299 × 299
obtained at a magnification of 40× from breast, prostate, and colon cancer whole
slide images stained with cd3 and cd8 markers."
2366,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"only the total number of
lymphocytes in each image patch are reported in this dataset."
2367,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"to evaluate the
performance of trained models on this dataset, we counted the total number of
marked lymphocytes in a predicted mask and calculated the difference between the
reported number of lymphocytes in each image with the total number of
lymphocytes in the predicted mask by the model."
2368,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"in table 2, the average
difference value (diffcount) of lymphocyte number for the whole dataset is
reported for each model."
2369,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"as seen, the trained models on our dataset outperform
the models trained solely on nuclick data."
2370,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"there are several public h&e/ihc cell segmentation datasets with manual immune
cell annotations from single pathologists."
2371,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"sample images/results taken from the
testing dataset are shown in fig."
2372,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"we have released the first ai-ready restained and co-registered mif and mihc
dataset for head-and-neck squamous cell carcinoma patients."
2373,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"this dataset can be
used for virtual phenotyping given standard clinical hematoxylin images, virtual
clinical ihc dab generation with ground truth segmentations (to train
highquality segmentation models across multiple cancer types) created from
cleaner mif images, as well as for generating standardized clean mif images from
neighboring h&e and ihc sections for registration and 3d reconstruction of
tissue specimens."
2374,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"in the future, we will release similar datasets for additional
cancer types as well as release for this dataset corresponding whole-cell
segmentations via impartial https://github.com/nadeemlab/impartial."
2375,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"it is also supported in part by the moffitt's
total cancer care initiative, collaborative data services, biostatistics and
bioinformatics, and tissue core facilities at the h."
2376,Detection of Basal Cell Carcinoma in Whole Slide Images,"existing skin cancer detection methods [7][8][9] typically employs
models like inception net and resnet, designed for natural images like those in
the imagenet dataset."
2377,Detection of Basal Cell Carcinoma in Whole Slide Images,"a balanced evolutionary algorithm
is then used to select the optimal structure from the search space, with the
candidate structures' performance evaluated using mini-batch patch data."
2378,Detection of Basal Cell Carcinoma in Whole Slide Images,"we
evaluate the searched architectures on the skin cancer dataset."
2379,Detection of Basal Cell Carcinoma in Whole Slide Images,"the original dataset is typically
split into training d t and validation datasets dv."
2380,Detection of Basal Cell Carcinoma in Whole Slide Images,"classification accuracy) on the validation dataset, i.e.,where
f p is the resource budget of flops."
2381,Detection of Basal Cell Carcinoma in Whole Slide Images,"3 experiments the dataset, comprised of 194 skin slides acquired from the
southern sun pathology laboratory, includes 148 bcc cases and 46 other types
(common nevus, scc), all manually annotated by a dermatopathologist."
2382,Detection of Basal Cell Carcinoma in Whole Slide Images,the patient data were separated between training and testing to prevent overlap.
2383,Detection of Basal Cell Carcinoma in Whole Slide Images,"we validated our algorithm using the curated skin cancer dataset and sc-net as a
supernet, testing both heavy and light models."
2384,Detection of Basal Cell Carcinoma in Whole Slide Images,"to
ensure a fair comparison on our dataset, we selected several papers in the field
of pathological image analysis, such as [9,22,23], as well as others using the
ua principle, such as [18,24].evaluation metrics."
2385,Detection of Basal Cell Carcinoma in Whole Slide Images,"with scnet and
evolutionary search, we obtained optimal architectures, achieving 96.2% top-1
and 96.5% accuracy on a skin cancer dataset, improvements of 4.8% and 4.7% over
baselines."
2386,Detection of Basal Cell Carcinoma in Whole Slide Images,"future work will apply our approach to larger datasets for
wider-scale validation."
2387,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,"2) we develop a label-disambiguation module that leverages
prototypes and confidence bank to tackle the weakly supervised nature of
instance-level supervision and reduce the impact of assigned noisy labels.3) the
proposed framework outperforms state-of-the-art (sota) methods on public
datasets and in a practical clinical task, demonstrating its superiors in wsi
analysis."
2388,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,we evaluate our model with three datasets.
2389,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,"(1) luad-gm dataset: the objective is
to predict the epidermal growth factor receptor (egfr) gene mutations in
patients with lung adenocarcinoma (luad) using 723 whole slide image (wsi)
slices, where 47% of cases have egfr mutations."
2390,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,"(2) tcga-nsclc and tcga-rcc
datasets: cancer type classification is performed using the cancer genome atlas
(tcga) dataset."
2391,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,"the tcga-nsclc dataset comprised two subtypes, lung squamous
cell carcinoma (lusc) and lung adenocarcinoma (luad), while the tcga-rcc dataset
included three subtypes: renal chromophobe cell carcinoma (kich), renal clear
cell carcinoma (kirc), and renal papillary cell carcinoma (kirp)."
2392,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,"the dataset was randomly split into three parts: training, validation, and
testing, with 60%, 20%, and 20% of the samples, respectively."
2393,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,"experimental
results demonstrate that iib-mil surpasses current sota techniques on publicly
available datasets, and holds significant potential for addressing more complex
clinical applications, such as predicting gene mutations."
2394,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"the mlp-mixer adopts two types of mlp layers to
allow information communication in different dimensions of data.the
contributions of this work are summarized as follows:1) a novel prototypical
transformer (pt) is proposed to learn superior prototype representation for wsi
classification by integrating prototypical learning into the transformer
architecture."
2395,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"mil is a typical weakly supervised learning method, where the training data
consists of a set of bags, and each bag contains multiple instances."
2396,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"a wsi dataset t can be defined as:where x i denotes a
patient, y i the label of x i , i j i is the j-th instance of x i , n is the
number of patients and n is the number of instances."
2397,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"however, the wsi dataset generally has
a long sequence of instances, which makes the clustering algorithms
computationally expensive and slow down as the size of the bag increases.to
solve the issue above, we propose to apply the self-attention (sa) mechanism in
transformer to re-calibrate these cluster prototypes."
2398,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"to evaluate the effectiveness of mspt, we conducted experiments on two public
dataset, namely camelyon16 [24] and tcga-nsclc."
2399,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"camelyon16 is a wsi dataset for
the automated detection of metastases in lymph node tissue slides."
2400,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"the tcga-nsclc dataset includes two
sub-types of lung cancer, i.e., lung squamous cell carcinoma (tgca-lusc) and
lung adenocarcinoma (tcga-luad)."
2401,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"we collected a total of 854 diagnostic slides
from the national cancer institute data portal (https:// portal.gdc.cancer.gov)."
2402,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"the dataset yields 4.3 million patches at 20× magnification, 1.1 million patches
at 10× magnification, and 0.30 million patches at 5× magnification with an
average of about 5000, 1200, and 350 patches per bag."
2403,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"for camelyon16 dataset,
we reported the results of the official testing set."
2404,Multi-scale Prototypical Transformer for Whole Slide Image Classification,[9] for the camelyon16 and tcga datasets.
2405,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"but [9] only trained simclr encoders
at 20× and 5× magnification, to align with that setting, we used the same
settings to train the simclr encoder at 10× magnification on both datasets."
2406,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"1 shows the comparison results on the camelyon16 and
tcga-nsclc datasets."
2407,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"to evaluate the effectiveness of the pt, we first changed the number of
prototypes k in the range of {1, 2, 4, 8, 16, 32} to get the optimal k for each
dataset."
2408,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"in the camelyon16 dataset, the performance
of both pt and prototype-bag increases with the increase of k value, and
achieves the best results with k = 16."
2409,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"in the tcga-nsclc dataset, pt always
outperforms the fullbag and prototype-bag."
2410,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"(3)
ms-attention: this variant used attention-pooling [8] on the cluster prototypes
for each magnification, and then added them.table 2 gives the results on the
camelyon16 and tcga-nsclc datasets."
2411,Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,dataset.
2412,Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,"our dataset contained 282 consecutive patients who underwent thyroid
nodule examination at nanjing drum tower hospital."
2413,Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,"all data were approved by the institutional review board of
nanjing drum tower hospital, and all patients signed the informed consent before
enrollment into the study.implementation details."
2414,Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,"in addition, we carried out
data augmentation, including random rotation and cropping, and we resize the
resolution of input frames to (224 × 224)."
2415,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"in recent
advances, with a large amount of labeled data, fully-supervised learning methods
can easily achieve reasonable results [1][2][3][4][5]."
2416,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"methods to relieve
the high dependency on the accurate annotations of nuclei are highly
needed.unsupervised learning (ul) methods achieved great success in the data
dependency problem for nuclei segmentation, which learns from the structural
properties in the data without any manual annotations."
2417,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"finally, we apply the learned ssimnet to produce the final
nuclei segmentation.to validate the effectiveness of our method, we conduct
extensive experiments on the monuseg dataset [16,17] based on ten existing
unsupervised segmentation methods [9,[11][12][13][14][15][18][19][20]."
2418,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"our
method outperforms all comparison methods with an average dice score of 0.792
and aggregated jaccard index of 0.498 on the monuseg dataset which is close to
the supervised method."
2419,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"to
this, we conduct data purification to build a reliable training set for
subsequent learning.data purification."
2420,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"sorting these pairs by usmi from the smallest to
largest, only maintain the first α%(0 < α < 100) of data pairs as)."
2421,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"the monuseg dataset consists of 44 h&e stained histopathology
images with 28,846 manually annotated nuclei."
2422,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,the cpm17 dataset [24] is also derived from tcga repository.
2423,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"our experiment repeated ten times on
monuseg dataset and only once on cpm17 dataset for an augmented convenience."
2424,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"specially for our ssimnet training, we set α = 70% for data purification and λ =
0.9 for loss in training."
2425,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"to evaluate the effectiveness of ssimnet, we compare it with several deep
learning based and conventional unsupervised segmentation methods on the
mentioned datasets, including minibatch k-means (termed as mkmeans), gaussian
mixture model [9] (termed as gmm), invariant information clustering [12] (termed
as iic), double dip [18], deep clustering via adaptive gmm model [19] (termed as
dcagmm), deep image clustering [13] (termed as dic), kim's work [20], kanezaki's
work [11], deep conditional gmm [14] (termed as dcgmm), and deep constrained
gaussian network [15] (termed as dcgn).for the methods without public codes, we
report the results from the original publications for a fair comparison."
2426,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"it also conforms the effectiveness of our
method on eliminating the model confusion in the region between adjacent nuclei
and the ability in capturing nuclei shape.besides, we conduct an additional
comparison experiment based on cpm17 dataset to demonstrate the generalization
of our method."
2427,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"moreover, as the image size of cpm17 is smaller than that of
monuseg, the performance gain is not as big as on the monuseg dataset."
2428,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"as shown in table 3, each component in our
ssimnet can bring different degrees of improvement, which shows that all of the
label softening, data purification and finetuning process are significant parts
of our ssimnet and play an indispensable role in achieving superior performance."
2429,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"comprehensive
experimental results demonstrate that ssimnet achieves the best performances on
the benchmark monuseg and cpm17 datasets, outperforming other unsupervised
segmentation methods."
2430,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,"the graphs are defined by g =
(v, e), where v is the set of vertices (nodes) v = {v 1 , ...v n } with τ n
vertex types, and e is the collection of pairs of vertices from v, e = {e 1 ,
...e m }, which are called edges and φ n is the mapping function that maps every
vertex to one of n differential marker expressions in this dataset φ n : v → τ n
."
2431,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"as such, generative models can be sampled to emphasize each
disease subtype equally and generate more balanced datasets, thus preventing
dataset biases getting amplified by the models [7]."
2432,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"synthetic datasets can also tackle privacy
concerns surrounding medical data sharing."
2433,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"additionally, conditional generation
of annotated data adds even further value to the proposition as labeling medical
images involves tremendous time, labor, and training costs."
2434,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"in this work, (1) we leverage recently
discovered capabilities of ddpms to design a first-of-its-kind nuclei-aware
semantic diffusion model (nasdm) that can generate realistic tissue patches
given a semantic mask comprising of multiple nuclei types, (2) we train our
framework on the lizard dataset [5] consisting of colon histology images and
achieve state-of-the-art generation capabilities, and (3) we perform extensive
ablative, qualitative, and quantitative analyses to establish the proficiency of
our framework on this tissue generation task."
2435,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"it is also challenging to capture long-tailed distributions
and synthesize rare samples from imbalanced datasets using gans."
2436,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"in this section, we (1) describe
our data preparation, (2) detail our stain-normalization strategy, (3) review
conditional denoising diffusion probabilistic models, (4) outline the network
architecture used to condition on semantic label map, and (5) highlight the
classifier-free guidance mechanism that we employ at sampling time."
2437,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,we use the lizard dataset [5] to demonstrate our framework.
2438,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"this dataset
consists of histology image regions of colon tissue from six different data
sources at 20× objective magnification."
2439,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"a generative model trained on this dataset can be used to
effectively synthesize the colonic tumor micro-environments."
2440,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"the dataset
contains 238 image regions, with an average size of 1055 × 934 pixels."
2441,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"to use the data exhaustively, patching is
performed with a 50% overlap in neighboring patches."
2442,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"[26] to transform all the slides to match the stain distribution of an
empirically chosen slide from the training dataset."
2443,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"a
conditional diffusion model aims to maximize the likelihood p θ (x 0 | y), where
data x 0 is sampled from the conditional data distribution, x 0 ∼ q(x 0 | y),
and y represents the conditioning signal."
2444,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"the forward process is defined as a markov chain, where
gaussian noise is gradually added to the data over t timesteps aswhere {β} t=1:t
are constants defined based on the noise schedule."
2445,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"in all
following experiments, we synthesize images using the semantic masks of the
held-out dataset at the concerned objective magnification."
2446,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,to keep the size of the training data constant.
2447,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"however, the standard generative metric fréchet inception distance (fid)
measures the distance between distributions of generated and real images in the
inception-v3 [14] latent space, where a lower fid indicates that the model is
able to generate images that are very similar to real data."
2448,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,we demonstrate the model on a colon dataset and qualitatively fig.
2449,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,"unlike other dl models, the
diffusion model is trained without any extra assumption about target data
distribution, thus evading the average effect and alleviating the over-smoothing
problem [24]."
2450,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,"(3) the
proposed diffdp is extensively evaluated on a clinical dataset consisting of 130
rectum cancer patients, and the results demonstrate that our approach
outperforms other state-of-the-art methods."
2451,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,"by utilizing both processes, the diffdp model can progressively
transform the gaussian noise into complex data distribution.forward process."
2452,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,dataset and evaluations.
2453,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,"we measure the performance of our model on an in-house
rectum cancer dataset which contains 130 patients who underwent volumetric
modulated arc therapy (vmat) treatment at west china hospital."
2454,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,"extensive experiments on
an in-house dataset with 130 rectum cancer patients demonstrate the superiority
of our method."
2455,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"the proposed approach was evaluated on a public tcga-lung dataset and an
in-house endometrial dataset and compared with 6 state-of-the-art methods."
2456,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"(3) the
experiments on two datasets show our pama can achieve competitive performance
compared with sota mil methods and ssl methods."
2457,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"we evaluated the proposed method on two datasets, the public tcga-lung and the
in-house endometrial dataset, which are introduced as follows.algorithm 1:
kernel reorientation algorithm.input: p (n) ∈ n h×n k ×np : the relative polar
angle matrix of n-th block, where h is the head number of multi-head attention,
n k is the number of anchors in the wsi, np is the number of patches in the wsi;
a (n) ∈ r h×n k ×np : the attention matrix from anchors to patches, defined asd
score : a dictionary taking the angle as key for storing attention scores;
output: p (n+1) ∈ r h×n k ×np : the updated polar angle matrix.h,i,max = arg max
d score ; // find the orientation that has the highest attention score."
2458,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,tcga-lung dataset is collected from the cancer genome atlas (tcga) data portal.
2459,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"the dataset includes a total of 3,064 wsis, which consist of three categories,
namely tumor-free (normal), lung adenocarcinoma (luad), and lung squamous cancer
(lusc), endometrial dataset includes 3,654 wsis of endometrial pathology, which
includes 8 categories, namely well/moderately/low-differentiated endometrioid
adenocarcinoma, squamous differentiation carcinoma, plasmacytoid carcinoma,
clear cell carcinoma, mixed-cell adenocarcinoma, and benign tumor.each dataset
was randomly divided into training, validation and test sets according to 6:1:3
while keeping each category of data proportionally."
2460,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"we conducted wsi multi-type
classification experiments on the two datasets."
2461,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"the wsi representation pre-training stage uses all training data and does not
involve any supervised information."
2462,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"we first conducted experiments on the endometrial dataset to verify the
effectiveness of self-supervised learning for wsi analysis under label-limited
conditions."
2463,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"in
comparison with the second-best methods, pama achieves an increase of
0.015/0.011 and 0.025/0.009 in aucs on tcga and endometrial datasets,
respectively, by using 35%/100% labeled wsis."
2464,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"moreover, pama reveals the most
robust capacity when reducing the training data from 100% to 35%, with auc
decreasing slightly from 0.988 to 0.982 and from 0.851 to 0.829 on the two
datasets."
2465,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"the experiments on two large-scale datasets have
demonstrated the effectiveness of pama in the condition of limited-label."
2466,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"future work will focus on training the wsi
representation model based on datasets across multiple organs, thus promoting
the generalization ability of the model for different downstream tasks."
2467,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"first, they are not able to get
rid of their reliance on detection models, which means they have a high need for
expensive detection data labeling to train the detection model."
2468,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"cervical cancer
cell detection datasets involve labeling individual and small bounding boxes in
a large number of cells."
2469,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"a lot of data would be wasted if only a small part of annotated images
(e.g., corresponding to positive cells and bounding boxes) was used as training
data."
2470,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"our experiments show
that our method becomes more effective when increasing the data size for
training."
2471,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"to make full use of wsi data and provide a better feature encoder, inspired by
moco [4,11] and other contrastive learning methods [25,26] pre-training on
imagenet [6], we also perform pretraining for fine-grained encoder on a large
scale of pathology images."
2472,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"generally, large-scale pre-training usually requires
a massive dataset and a suitable loss function."
2473,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"for data, wsi naturally has the
advantage of having a large amount of training data."
2474,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"therefore, we only need
2,000-3,000 wsi samples to obtain a dataset that can even be compared to
imagenet in quantity."
2475,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,dataset and experimental setup.
2476,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"first, we label a
dataset with cell-level bounding boxes to train a detection model."
2477,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"the detection
dataset has 3761 images and 7623 cell-level annotations."
2478,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"thus, we conclude that a detection model trained with
an expensive annotated dataset is not necessary to build a cad pipeline for
cervical abnormality.ablation study."
2479,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"3, the traditional detection-based
method has quickly encountered a saturation bottleneck as the amount of data
increases."
2480,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"and at our current maximum data number (5384), the
proposed pipeline has already exceeded the performance of the detection-based
method."
2481,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"our method does not rely on detection
models and eliminates the need for expensive cell-level data annotation."
2482,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"importantly, our method offers even greater advantages
with increasing amounts of data."
2483,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"accelerating the training of
massive data can be our next optimization direction."
2484,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"our proposed
method was trained and tested on the 2015 mic-cai gland segmentation (glas)
challenge dataset [20] and colorectal adenocarcinoma gland (crag) dataset [6]
(as shown in fig."
2485,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"the components of diffusion model are a learning
reverse process called p θ (z t-1 |z t ) that creates samples by converting
noise into samples from q(z 0 ) and a forward diffusion process called q(z t |z
t-1 ) that gradually corrupts data from some target distribution into a normal
distribution."
2486,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"in this
setting, the data samples consist of a set of bounding boxes represented as z 0
, where z 0 is a set of n boxes.the neural network f θ (z t , t) is trained to
predict z 0 from the z t based on the corresponding image x."
2487,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"we evaluated the effectiveness of the proposed model on two datasets: the glas
dataset and the crag dataset."
2488,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"the glas dataset comprises 85 training and 80
testing images, divided into 60 images in test a and 20 images in test b."
2489,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"the
crag dataset consists of 173 training and 40 testing images."
2490,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"furthermore, to enhance the
training dataset and mitigate the risk of overfitting, we employed random
combinations of image flipping, translation, gaussian blur, brightness
variation, and other augmentation techniques.we assessed the segmentation
results using three metrics from the glas challenge: (1) object f1, which
measures the accuracy of detecting individual glands, (2) object dice, which
evaluates the volume-based accuracy of gland segmentation, and (3) object
hausdorff, which assesses the shape similarity between the segmentation result
and the ground truth."
2491,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"we trained on the glas and crag
datasets in a python 3.8.3 environment on ubuntu 18.04, using pytorch 1.10 and
cuda 11.4."
2492,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"table 1 provides an overview of the average performance of these
models.our proposed model demonstrated a enhancement in performance, surpassing
the second-best method on both test a and test b datasets."
2493,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"the proposed model was additionally evaluated on the crag dataset by comparing
it against the gcsba-net, doubleu-net, dse model, mild-net, and dcan."
2494,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"these results
demonstrate the effectiveness of our method in segmenting different
datasets.ablation studies: our network utilizes the mask branch and conditional
encoding to enhance performance and segmentation quality."
2495,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"ablation studies on
the glas and crag datasets confirm the effectiveness of these modules (table 3)."
2496,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"experimental results on the glas dataset and
crag dataset show that our method surpasses state-of-the-art approach,
demonstrating its effectiveness."
2497,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,"high inconsistencies, our proposed asp loss helps the network
learn more robustly.lastly, to support further research in virtual
ihc-restaining, we present the multi-ihc stain translation (mist) as a new
public dataset."
2498,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,"the mist dataset contains 4k+ training and 1k testing aligned
h&e-ihc patches for each of the following ihc stains that are critical for
breast cancer diagnostics: her2, ki67, er (estrogen receptor) and pr
(progesterone receptor)."
2499,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,datasets.
2500,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,"the following datasets are used in our experiments: the breast cancer
immunohistochemical (bci) challenge dataset [7] and our own mist dataset that is
now in the public domain."
2501,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,"due to the page limit, from the mist dataset, here we only present
detailed results on her2 and er."
2502,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,"overall, it can be observed that the
proposed framework with the asp loss consistently outperforms existing methods
across all three datasets."
2503,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,"finally, we have made public our multi-ihc stain translation dataset
with the hope to assist further research towards accurate h&e-to-ihc stain
translation."
2504,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"the
extensive experiments with promising results on two public wsi datasets from
tcga projects, i.e., kidney carcinoma (kica) and esophageal carcinoma (esca),
validate the effectiveness and efficiency of our framework on both tumor
subtyping and staging tasks."
2505,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,datasets and evaluation metrics.
2506,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"we assess the efficacy of the proposed higt
framework by testing it on two publicly available datasets (kica and esca) from
the cancer genome atlas (tcga) repository."
2507,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"the datasets are described below in
more detail:-kica dataset."
2508,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"the kica dataset consists of 371 cases of kidney
carcinoma, of which 279 are classified as early-stage and 92 as late-stage."
2509,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,-esca dataset.
2510,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"the esca dataset comprises 161 cases of esophageal carcinoma, with 96 cases
classified as early-stage and 65 as late-stage."
2511,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"the results for esca and kica datasets
are summarized in table 1 and table 2, respectively."
2512,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"even for the non-hierarchical graph-transformer baseline la-mil and
hierarchical transformer model hipt, our model approaches at least around 3% and
2% improvement on auc and acc in the classification of the staging of the kica
dataset."
2513,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"firstly, the
constructed hierarchical data structure of the multi-resolution wsi is able to
offer multi-scale information to the later model."
2514,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"extensive experimentation on two public wsi datasets demonstrates the
effectiveness and efficiency of our designed framework, yielding promising
results."
2515,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"although some classification methods achieve promising performance on balanced
and clean medical datasets, balanced datasets with high-accuracy annotations are
time-consuming and expensive."
2516,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"besides, pruning clean and balanced datasets
require a large amount of crucial clinical data, which is insufficient for
large-scale deep learning."
2517,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"therefore, we focus on a more practical yet
unexplored setting for handling imbalanced medical data with noisy labels,
utilizing all available lowcost data with possible noisy annotations."
2518,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"noisy
imbalanced datasets arise due to the lack of high-quality annotations [11] and
skewed data distributions [18] where the number of instances largely varies
across different classes."
2519,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"therefore, noisy-labeled, imbalanced datasets with various class
hardness remain a persistent challenge in medical classification.existing
approaches for non-ideal medical image classification can be summarized into
noisy classification, imbalanced recognition, and noisy imbalanced
identification."
2520,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"noisy classification approaches [3,7,23] conduct noise-invariant
learning depending on the big-loss hypothesis, where classifiers trained with
clean data with lower empirical loss aid with de-noising identification."
2521,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"however, imbalanced data creates different confidence distributions of clean and
noisy data in the majority class and minority class as shown in fig."
2522,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"imbalanced recognition approaches
[9,15,21] utilize augmented embeddings and imbalance-invariant training loss to
re-balance the long-tailed medical data artificially, but the disturbance from
noisy labels leads to uncasual feature learning, impeding the recognition of
tail classes."
2523,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"the main
contributions of our work include: 1) we decompose the negative effects in
practical medical image classification, 2) we minimize the invariant risk to
tackle noise identification influenced by multiple factors, enabling the
classifier to learn causal features and be distribution-invariant, 3) a
re-scaling class-aware gaussian mixture modeling (cgmm) approach is proposed to
distinguish noise labels under various class hardness, 4) we evaluate our method
on two medical image datasets, and conduct thorough ablation studies to
demonstrate our approach's effectiveness."
2524,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"in the noisy imbalanced classification setting, we denote a medical dataset as
{(x i , y i )} n i=1 where y i is the corresponding label of data x i and n is
the total amount of instances."
2525,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"further, we split the
dataset according to class categories."
2526,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"in each subset
containing n j samples, the data pairs are expressed as {(x j i , y j i )} nj
i=1 ."
2527,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"further, we denote the backbone as h(•; θ), x → z mapping data
manifold to the latent manifold, the classifier head as g(•; γ), z → c linking
latent space to the category logit space, and the identifier as f(•; φ), z → c."
2528,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"we aim to train a robust medical image classification model composed of a
representation backbone and a classifier head on label noise and imbalance
distribution, resulting in a minimized loss on the testing dataset: we decompose
the non-linear mapping p(y = c|x) as a product of two space
mappings p g (y = c|z) • p h (z|x)."
2529,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"in essence, the fundamental idea of noisy
classification involves utilizing clean data for classifier training, which
determines the importance of noise identification and removal."
2530,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"traditional learning with noisy label methods mainly minimize empirical risk on
training data."
2531,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"by assuming that the robust
classifier performs well on every data distribution, we solve the optimizing
object by finding the optima to reduce the averaged distance for gradient shift:
minwhere ε represents an environment (distribution) for classifier f φ and
backbone h θ ; and l denotes the empirical loss for classification."
2532,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"by transferring the constraints into a penalty in
the optimizing object, we solve this problem by learning the constraint scale ω
[2]:ideally, the noise removal process is distribution-invariant if data is
uniformly distributed w.r.t."
2533,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"to simplify this assumption, we
construct three different data distributions [25] composed of one uniform
distribution and two symmetric skewed distributions instead of theoretical
settings."
2534,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"to resolve the challenge, we propose a novel method called rescaling
class-aware gaussian mixture modeling (rcgm) which clusters each category data
independently by fitting confidence scores q ij from ith class into two gaussian
distributions as p n i (x n |μ n , σ n ) and p c i (x c |μ c , σ c )."
2535,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"this overcomes the limitations of global clustering methods and
significantly enhances the accuracy of noise identification even when class
hardness varies.instead of assigning a hard label to the potential noisy data as
[8] which also employs a class-specific gmm to cluster the uncertainty, we
further re-scale the confidence score of class-wise noisy data."
2536,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"finally, in the fine-tuning phases, we
apply mixup technique [13,25,26] to rebuild a hybrid distribution from noisy
pairs and clean pairs by:where α kl := v(x k ) v(x l ) denotes the balanced
scale; and {(x kl , ŷkl )} are the mixed clean data for classifier fine-tuning."
2537,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"sqrt sampler is applied to re-balance the data, and cross-stage kl [12] and ce
loss are the fine-tuning loss functions."
2538,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"we evaluated our approach on two medical image datasets with imbalanced class
distributions and noisy labels."
2539,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"the first dataset, ham10000 [22], is a
dermatoscopic image dataset for skin-lesion classification with 10,015 images
divided into seven categories."
2540,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"the second dataset, chaoyang [29], is a histopathology
image dataset manually annotated into four cancer categories by three
pathological experts, with 40% of training samples having inconsistent
annotations from the experts."
2541,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"consequently, chaoyang dataset consists of a training set with 2,181 images, a
validation set with 713 images, and a testing set with 1,426 images, where the
validation and testing sets have clean labels."
2542,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"we train all approaches under the same data augmentations and network
architecture."
2543,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"figure 3a and 3b show that only using mer or rcgm
achieves better performance than our strong baseline on both datasets."
2544,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"further, our multi-stage noise removal technique outperforms single mer and
rcgm, revealing that the decomposition for noise effect and hardness effect
works on noisy imbalanced datasets."
2545,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"we find that the combination of mer and rcgm
improves more on chaoyang dataset."
2546,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"it indicates the re-scaling process for noise weight deduction
contributes to balancing the feature learning and classification boundary
disturbance from the mixture of noisy and clean data."
2547,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"we address three practical adverse effects including data noise,
imbalanced distribution, and class hardness."
2548,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"to solve these difficulties, we
conduct multi-environment risk minimization (mer) and rescaling class-aware
gaussian mixture modeling (rcgm) together for robust feature learning.extensive
results on two public medical image datasets have verified that our framework
works on the noisy imbalanced classification problem."
2549,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"patients with colorectal cancer typically undergo contrast-enhanced computed
tomography (cect) scans multiple times during follow-up visits after surgery for
early detection of crlm, generating a 5d dataset."
2550,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"in addition to the axial,
sagittal, and coronal planes in 3d ct scans, the data comprises
contrast-enhanced multiple phases as its 4th dimension, along with different
timestamps as its 5th dimension."
2551,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"radiologists heavily rely on this data to
detect the crlm in the very early stage [15].extensive existing works have
demonstrated the power of deep learning on various spatial-temporal data, and
can potentially be applied towards the problem of crlm."
2552,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"for example, originally
designed for natural data, several mainstream models such as e3d-lstm [12],
convlstm [11] and predrnn [13] use convolutional neural networks (cnn) to
capture spatial features and long short-term memory (lstm) to process temporal
features."
2553,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"these models
can be adapted for classification tasks with the use of proper classification
head.however, all these methods have only demonstrated their effectiveness
towards 3d/4d data (i.e., time-series 2d/3d images), and it is not clear how to
best extend them to work with the 5d cect data."
2554,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"part of the reason is due to the
lack of public availability of such data."
2555,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"when extending these models towards 5d
cect data, some decisions need to be made, for example: 1) what is the most
effective way to incorporate the phase information?"
2556,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"e3d-lstm [12] shows
uni-directional lstm works well on natural videos while several other works show
bi-directional lstm is needed in certain medical image segmentation tasks
[2,7].in this paper, we investigate how state-of-art deep learning models can be
applied to the crlm prediction task using our 5d cect dataset."
2557,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"we evaluate the
effectiveness of bi-directional lstm and explore the possible method of
incorporating different phases in the cect dataset."
2558,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"our dataset follows specific inclusion criteria:-no tumor appears on the ct
scans."
2559,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"-patients have two or
more times of cect scans.-we already determined whether or not the patients had
liver metastases within 2 years after the surgery, and manually labeled the
dataset based on this."
2560,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"-no potential focal infection in the liver before the
colorectal radical surgery.-no metastases in other organs before the liver
metastases.-no other malignant tumors.our retrospective dataset includes two
cohorts from two hospitals."
2561,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"additional statistics on our dataset are presented in table 1 and examples of
representative images are shown in fig."
2562,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"the dataset is available upon
request."
2563,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"numerous state-of-the-art deep learning models are available to effectively
process 4d data."
2564,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"they used 3d-cnns to handle
the 3d data at each timestamp and lstms to compute information at different
timestamps."
2565,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"[13,14], uses
spatiotemporal lstm (st-lstm) by stacking multiple convlstm units and connecting
them in a zigzag pattern to handle spatiotemporal data of 4 dimensions."
2566,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"4) simvp
[4], introduced by gao et al., uses cnn as the translator instead of lstm.all of
these models need to be modified to handle 5d cect datasets."
2567,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"a straightforward
way to extend them is simply concatenating the a phase and v phase together,
thus collapsing the 5d dataset to 4d."
2568,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"below we explore an alternative modification multi-plane bi-directional lstm
(mpbd-lstm), based on e3d-lstm, to handle the 5d data."
2569,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"after this, the output y v,t0 is passed into the
bi-directional lstm module in the next layer and viewed as input for this
module.figure 2(a) illustrates how mpbd-lstm uses these 3d-lstm building blocks
to handle the multiple phases in our ct scan dataset."
2570,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"each encoder is followed by a 3d-lstm stack (the ""columns"") that processes the
spatiotemporal data for each timestamp."
2571,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"when the spatiotemporal dataset
enters the model, it is divided into smaller groups based on timestamps and
phases."
2572,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"we selected 170 patients who underwent three or more cect scans from our
original dataset, and cropped the images to only include the liver area, as
shown in fig."
2573,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"to handle the imbalanced training dataset, we selected and
duplicated 60% of positive cases and 20% of negative cases by applying standard
scale jittering (ssj) [5]."
2574,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"for data augmentation, we randomly rotated the images
from -30 • to 30 • and employed mixup [17]."
2575,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"we applied the same augmentation
technique consistently to all phases and timestamps of each patient's data."
2576,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"we used the a and v
phases of cect for our crlm prediction task since the p phase is only relevant
when tumors are significantly present, which is not the case in our dataset."
2577,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"as the data size is limited, 10-fold cross-validation is adopted, and the ratio
of training and testing dataset is 0.9 and 0.1, respectively."
2578,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"table 2 shows the auc scores of all
models tested on our dataset."
2579,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"additional data on accuracy, sensitivity
specificity, etc."
2580,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"by replacing the bi-directional connection with a uni-directional
connection, the mpbd-lstm model's performance decreased to 0.768 on the original
dataset."
2581,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"we conducted ablation studies using ct images
from different timestamps and phases to evaluate the effectiveness of
time-series data and multi-phase data."
2582,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"how to effectively address
inter-patient variability in the dataset, perhaps by better fusing the 5d
features, requires further research from the community in the future."
2583,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"in this paper, we put forward a 5d cect dataset for crlm prediction."
2584,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"based on
the popular e3d-lstm model, we established mpbd-lstm model by replacing the
uni-directional connection with the bi-directional connection to better capture
the temporal information in the cect dataset."
2585,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"although cell-level analysis has the potential
to produce more detailed and explainable data, it can be limited by the
unavailability of sufficiently annotated training data."
2586,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"transfer learning using backbones pretrained on natural images is a common
method that addresses the challenge of using data sets that largely lack
annotation."
2587,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"therefore, to enable the use of
large unlabeled clinical imaging data sets, as the backbone of our neural
network we used a resnet50 model [12]."
2588,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"the backbone was trained with the
bootstrap your own latent (byol) method [13] using four publicly available data
sets from the cancer genome atlas (tcga) and three data sets from private
vendors that included healthy and malignant tissue from a range of organs [14]."
2589,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"we normalized the tiles for stain color using a u-net model for stain
normalization [15] that was trained on a subset of data from one of the medical
centers in the camelyon17 data set to ensure homogeneity of staining [16].to
create the tile-level embeddings, we used the method proposed by [17] to
summarize the convolutional neural network (cnn) features with nonnegative
matrix factorization (nmf) for k = 2 factors."
2590,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"the models were selected using a validation
set, that was a random sample of 20% of the training data."
2591,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"for breast cancer human
epidermal growth factor receptor 2 (her2) prediction, we used data from the
herohe challenge data set [26]."
2592,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"to enable comparison with previous results we
used the same test data set that was used in the challenge [27]."
2593,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"for prediction
of estrogen receptor (er) status, we used images from the tcga-breast invasive
carcinoma (tcga-brca) data set [28] for which the er status was known.for these
two tasks we used artifact-free tiles from tumor regions detected with an
in-house tumor detection model.for breast cancer metastasis detection in lymph
node tissue, we used wsis of h&estained healthy lymph node tissue and lymph node
tissue with breast cancer metastases from the publicly available camelyon16
challenge data set [16,29]."
2594,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"all artifact-free tissue tiles were used.for cell of
origin (coo) prediction of activated b-cell like (abc) or germinal center b-cell
like (gcb) tumors in diffuse large b-cell lymphoma (dlbcl), we used data from
the phase 3 goya (nct01287741) and phase 2 cavalli (nct02055820) clinical
trials, hereafter referred to as ct1 and ct2, respectively."
2595,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"ct1
was used for training and testing the classifier and ct2 was used only as an
independent holdout data set."
2596,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"for these data sets we used artifact-free tiles
from regions annotated by expert pathologists to contain tumor tissue."
2597,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"in fact, for the her2
classification task, combined embeddings obtained using the xformer architecture
achieved, to our knowledge, the best performance yet reported on the herohe
challenge data set (area under the receiver operating characteristic curve
[auc], 90%; f1 score, 82%).for coo classification in dlbcl, not only did the
combined embeddings achieve better performance than the tile-level only
embeddings with both the xformer and a-mil architectures (fig."
2598,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"5) on the ct1
test set and ct2 holdout data set, but they also had a significant advantage
versus tile-only level embeddings in respect of the additional insights they
provided through cell-level model explainability (sect."
2599,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,"examples of our cellular explainability method
applied to weakly supervised tumor detection on wsis from the camelyon16 data
set using a-mil are shown in fig."
2600,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"however, partially due to the
limited data and annotation, screening for candidiasis is mostly
understudied.computer-aided diagnosis for candidiasis through wsi is highly
challenging (see examples in fig."
2601,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"while collecting more candida data may contribute to a more robust
network, such efforts are dwarfed by the inhomogeneity of wsis, which adds to
the risk of overfitting."
2602,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"nevertheless, such a transformer can be
hard to train for our task, due to the large image size, huge network
parameters, and huge demand for training data."
2603,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,datasets and experimental setup.
2604,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"all images used to pre-train the
detector are categorized as training data here."
2605,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"at the wsi
level, we use two datasets."
2606,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"dataset-small is balanced with 100 positive wsis and
100 negative wsis."
2607,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"we further validate upon an
imbalanced dataset-large of 7654 wsis."
2608,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"there are only 140 positive wsis in this
dataset, which is closer to real world."
2609,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"these two wsi-level datasets have no
overlay with the data used to train the above detection and classification
tasks.for implementation details, the models are implemented by pytorch and
trained on 4 nvidia tesla v100s gpus."
2610,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"to
save computation, we did not verify the performance of the methods that
performed too poorly on dataset-small."
2611,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"our attention-based method brings 6% improvement
of accuracy on data-small compared to other methods with the same wsi-level
method 'threshold'."
2612,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"transformer shows a better capacity of feature aggregation
than other wsi-level classifiers, raising the auc on dataset-large to 84.18%."
2613,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"as the wsi data per sample has a huge size, the idea of
identifying abnormal cells in a hierarchical manner has been proposed and
investigated by several studies using deep learning [3,27,31]."
2614,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"however, such a patchlevel
classification task requires a large number of annotated training data."
2615,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"and the
efforts in collecting reliably annotated data can hardly be negligible, which
requires high expertise due to the intrinsic difficulty of visually reading
wsis.to alleviate the shortage of sufficient data to supervise classification,
one may adopt traditional data augmentation techniques, which yet may bring
little improvement due to scarcely expanded data diversity [26]."
2616,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"thus,
synthesizing cytopathological images for cervical cells is highly desired to
effectively augment training data."
2617,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"we adopt an adversarial learning scheme, where the discriminator
is modified in a projection-based way [20] for matching condi- tional data
distribution."
2618,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"the experimental results validate the
visual plausibility of cellgan synthesized images, as well as demonstrate their
data augmentation effectiveness on patch-level cell classification."
2619,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"the dilemma of medical image synthesis lies in the conflict between the limited
availability of medical image data and the high demand for data amount to train
reliable generative models."
2620,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"in an adversarial training setting, the discriminator forces the generator to
faithfully match the conditional data distribution of real cervical
cytopathological images, thus prompting the generator to produce visually and
semantically realistic images."
2621,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"to align the class-conditional fake and real data distributions in the
adversarial setting, the discriminator directly incorporates class labels as
additional inputs in the manner of projection discriminator [20]."
2622,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"combining all the loss functions above, the total objective l
total to train the proposed cellgan in an adversarial manner can be expressed
as:where λ reg is empirically set to 0.01 in our experiments.3 experimental
results dataset."
2623,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"all the 256×256
images with their class labels are selected as the training data.implementation
details."
2624,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"to validate the data augmentation capacity of the proposed cellgan, we conduct
5-fold cross-validations on the cell classification performances of two classi-
in each fold, one group is selected as the testing data while the other four are
used for training."
2625,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"for different data settings, we synthesize 2,000 images for
each cell type using the corresponding generative method, and add them to the
training data of each fold."
2626,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"random
flip is applied to all data settings since it is reasonable to use traditional
data augmentation techniques simultaneously in practice.the experimental
accuracy, precision, recall, and f1 score are listed in table 3."
2627,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"it is shown
that both the classifiers achieve the best scores in all metrics using the
additional synthesized data from cellgan."
2628,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"meanwhile, the scores of other metrics are all improved by more
than 4%, indicating that our synthesized data can significantly enhance the
overall classification performance."
2629,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"thanks to the visually plausible and
semantically realistic synthesized data, cellgan is conducive to the improvement
of cell classification, thus serving as an efficient tool for augmenting
automatic abnormal cervical cell screening."
2630,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"qualitative and quantitative experiments validate the semantic
realism as well as the data augmentation effectiveness of the synthesized images
from cellgan.meanwhile, our current cellgan still has several limitations."
2631,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"such distinct
morphological differences can be characterized by the tbsrtc category, which
thus inspires us to utilize the handy image-wise grading labels to guide the
nuclei segmentation model learning from unbalanced datasets."
2632,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"(3) we establish a dataset of
thyroid cytopathology image patches of 224 × 224, where 4,965 image labels are
provided following tbsrtc, and 1,473 of them are densely annotated [3] (to be on
github upon acceptance)."
2633,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"to the best of our knowledge, it is the first
publicized thyroid cytopathology dataset of both image-wise and pixel-wise
labels."
2634,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"the annotated dataset well alleviates the insufficiency of an open
cytopathology dataset for computer-assisted analysis (fig."
2635,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"we propose a novel tbsrtc-category aware segmentation network
(tcsegnet) to segment nuclei boundaries in cytopathology images, which is guided
by tbsrtc-category label to learn from unbalanced data."
2636,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"in tcsegnet, we introduce a tbsrtc-category label guidance block to
address the learning issue from unbalanced routine datasets."
2637,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"correspondingly,
to train this block, we use a cross-entropy loss function (ce) that provides an
extra supervision signal to help the network learn from unbalanced datasets,
defined as follows:where y cls is the image-wise tbsrtc-category label, and the
balancing coefficient γ cls is set to 3, as the global feature captured by the
transformer branch is tightly correlated with the image-level classification
tag."
2638,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,image dataset.
2639,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"we construct a clinical thyroid cytopathology dataset with images
of both image-wise and pixel-wise labels as a benchmark (appear in github upon
acceptance) some representative images are presented in fig."
2640,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"2, together with
the profile of the dataset."
2641,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"the dataset comprises 4,965 h&e stained image
patches and labels of tbsrtc, where a subset of 1,473 images was densely
annotated for nuclei boundaries by three experienced cytopathologists and
reached a total number of 31,064 elaborately annotated nuclei."
2642,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"we divided the dataset with image-wise labels into 80%
training samples and the remaining 20% testing samples."
2643,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"moreover, with the semi-supervised learning, semi-tcsegnet
can further boost the performance to an 88.9% dice score, and 80.5% iou, by
leveraging additional data with image-wise tbsrtccategory labels solely."
2644,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"4 (c,d), as a demonstration of the
advantage using full data resources with semi-tcsegnet."
2645,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"the results indicate that performance improvement is accumulated with
increasing data size."
2646,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"importantly, it addresses the challenge of
distinguishing nuclei across different cell scales in an unbalanced dataset."
2647,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"moreover, we construct the first
thyroid cytopathology dataset with both image-wise and pixel-wise labels, which
we believe can it facilitate future research in this field."
2648,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"as the spatial
distribution, shape, and area information from nuclear segmentation is
supportive of diagnostic decisions, we will further leverage the segmentation
result for malignancy analysis and also explore the potential of spatial
information for unlabeled data exploration in the future."
2649,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"the pathological process
is usually the golden standard approach for bc diagnosis, which relies on
leveraging diverse complementary information from multi-modal data."
2650,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"according to the data format, there are two main multi-modal pre-training
approaches, as shown in fig."
2651,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"one is based on isomorphic data, such as
vision-language pre-training [5] and vision-speech-text pre-training [3]."
2652,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"the
other is based on heterogeneous data."
2653,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"in
the field of medical image analysis, it is widely recognized that using
multi-modal data can produce more accurate diagnoses than using single-modal
data."
2654,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"however, the development of multi-modal pre-training methods has been
limited due to the scarcity of paired multi-modal data."
2655,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"to our best knowledge, there is
no work for multi-modal pre-training based on pathological heterogeneous data.in
this paper, we propose a multi-modal pre-training method based on masked
autoencoders for bc downstream tasks."
2656,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"to
our best knowledge, this is the first pre-training work based on multi-modal
pathological data."
2657,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"we evaluate the proposed method on two public datasets as
herohe challenge and bci challenge, which shows that our method achieves
state-of-theart performance."
2658,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"we pre-train our mmp-mae on the acrobat dataset with adamw [17] and the
learning rate of 1e -4 ."
2659,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"three methods on bci datasets are compared in
our experiments, as shown in table 1."
2660,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"pix2pix and pyramid pix2pix use paired data, which obtain better
results than cyclegan."
2661,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"the visualization on the
acrobat dataset also shows our model could learn the modality-related
information, as shown in fig."
2662,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"most of these
methods use the multinetwork ensemble strategy and extra datasets."
2663,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"team macaroon
uses the came-lyon dataset [4] for tumor classification."
2664,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"team mitel uses bach
dataset [1] for tumor classification."
2665,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"both
the experiment results on bci and herohe datasets show our pretrained mmp-mae
demonstrates strong transfer ability."
2666,Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,"recently, deep learning has achieved remarkable performance in pathological
image segmentation when trained with a large and well-annotated dataset
[6,13,20]."
2667,Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,"however, obtaining dense annotations for pathological images is
challenging and time-consuming, due to the extremely large image size (e.g.,
10000 × 10000 pixels), scattered spatial distribution, and complex shape of
lesions.semi-supervised learning (ssl) is a potential technique to reduce the
annotation cost via learning from a limited number of labeled data along with a
large amount of unlabeled data."
2668,Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,"in addition, we apply an
uncertainty minimization-based regularization to the average probability
prediction across the decoders, which not only increases the network's
confidence, but also improves the inter-decoder consistency for leveraging
labeled images.the contribution of this work is three-fold: 1) a novel framework
named cdma based on mtnet is introduced for semi-supervised pathological image
segmentation, which leverages different attention mechanisms for generating
diverse and complementary predictions for unlabeled images; 2) a cross decoder
knowledge distillation method is proposed for robust and efficient learning from
noisy pseudo labels, which is combined with an average prediction-based
uncertainty minimization to improve the model's performance; 3) experimental
results show that the proposed cdma outperforms eight state-of-the-art ssl
methods on the public digestpath dataset [3]."
2669,Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,dataset and implementation details.
2670,Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,"we used the public digestpath dataset [3]
for binary segmentation of colonoscopy tumor lesions from whole slide images
(wsi) in the experiment."
2671,Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,"for data augmentation, we adopted random
flipping, random rotation, and random gaussian noise."
2672,Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,"experimental results on a colonoscopy tissue
segmentation dataset demonstrated that our cdma outperformed eight
state-of-the-art ssl methods."
2673,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"however, this comes at the
cost of a large amount of labeled data that needs to be collected for each
class."
2674,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"to alleviate the labeling burden, semi-supervised learning has been
proposed to exploit a large amount of unlabeled data to improve performance in
the case of limited labeled data [10,15,19]."
2675,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"however, it still requires a small
amount of labeled data for each class, which is often impossible in real
practice."
2676,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"for example, there are roughly more than 2000 named dermatological
diseases today, of which more than 200 are common, and new dermatological
diseases are still emerging, making it impractical to annotate data from scratch
for each new disease category [20]."
2677,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"most ncd
methods follow a two-stage scheme: 1) a stage of fully supervised training on
known category data and 2) a stage of clustering on unknown categories [7,9,24]."
2678,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"specifically, we
first use contrastive learning to pretrain the model based on all data from
known and unknown categories to learn a robust and general semantic feature
representation."
2679,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"we conducted extensive experiments on the dermoscopy dataset isic
2019, and the experimental results show that our method outperforms other
state-of-the-art comparison algorithms by a large margin."
2680,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"given an unlabeled dataset {x u i } n u i=1 with n u images, where x u i is the
ith unlabeled image."
2681,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"our goal is to automatically cluster the unlabeled data
into c u clusters."
2682,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"in addition, we also have access to a labeled dataset {x l i
, y l i } n l i=1 with n l images, where x l i is the ith labeled image and y l
i ∈ y = 1, ."
2683,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"then, the uncertainty-aware multi-view
cross-pseudo-supervision strategy is used for joint training on all category
data."
2684,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"1 is the indicator
function.in addition, to help the feature extractor learn semantically
meaningful feature representations, we introduce supervised contrastive learning
[12] for labeled known category data, which can be denoted as:where n (i)
represents the sample set with the same label as x i in a mini-batch data."
2685,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"b l is the labeled
subset of mini-batch data.uncertainty-aware multi-view cross-pseudo-supervision."
2686,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"then, we can compute the ensemble
predicted output of m 1 and m 2 :next, we need to obtain training targets for
all data."
2687,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"; p u bu ∈ r bu×c u denotes the ensemble prediction of data of unknown
categories in a mini-batch, where b u represents the number of samples."
2688,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,dataset.
2689,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"to validate the effectiveness of the proposed algorithm, we conduct
experiments on the widely used public dermoscopy challenge dataset isic 2019
[4,5]."
2690,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"the dataset contains a total of 25,331 dermoscopic images from eight
categories: melanoma (mel), melanocytic nevus (nv), basal cell carcinoma (bcc),
actinic keratosis (ak), benign keratosis (bkl), dermatofibroma (df), vascular
lesion (vasc), and squamous cell carcinoma (scc)."
2691,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"since the dataset suffers from
severe category imbalance, we randomly sampled 500 samples from those major
categories (mel, nv, bcc, bkl) to maintain category balance."
2692,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"for data augmentation, we use random horizontal/vertical flipping,
color jitter, and gaussian blurring following [7]."
2693,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"following [9,23,24], we report the clustering performance on
the unlabeled unknown category dataset."
2694,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"we also compare with the
benchmark method (baseline), which first trains a model using known category
data and then performs clustering on unknown category data."
2695,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"it
can be seen that the clustering performance of the benchmark method is poor,
which indicates that the model pre-trained using only the known category data
does not provide a good clustering of the unknown category."
2696,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"recall
that the contrastive learning strategy includes supervised contrastive learning
for the labeled known category data and unsupervised contrastive learning for
all data."
2697,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"second,
uncertainty-aware multi-view cross-pseudo-supervision strategy is trained
uniformly on data from all categories, while prediction uncertainty is used to
alleviate the effect of noisy pseudo labels."
2698,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"meanwhile,
genomics data (e.g., mrna-sequence) display a high relevance to regulate cancer
progression [3,29]."
2699,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"therefore, synergizing multimodal data could
deepen a crossscale understanding towards improved patient prognostication.the
major goal of multimodal data learning is to extract complementary contextual
information across modalities [4]."
2700,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"supervised studies [5][6][7] have allowed
multimodal data fusion among image and non-image biomarkers."
2701,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"yet these supervised approaches are limited by feature
generalizability and have a high dependency on data labeling."
2702,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"to alleviate label
requirement, unsupervised learning evaluates the intrinsic similarity among
multimodal representations for data fusion."
2703,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"to broaden the data utility, the study [28] leverages
the pathology and genomic knowledge from the teacher model to guide the
pathology-only student model for glioma grading."
2704,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"from these analyses, it is
increasingly recognized that the lack of flexibility on model finetuning limits
the data utility of multimodal learning."
2705,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"meanwhile, the size of multimodal
medical datasets is not as large as natural vision-language datasets, which
necessitates the need for data-efficient analytics to address the training
difficulty.to tackle above challenges, we propose a pathology-and-genomics
multimodal framework (i.e., pathomics) for survival prediction (fig."
2706,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"(1) unsupervised multimodal data
fusion."
2707,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"a key contribution of our multimodal framework is
that it combines benefits from both unsupervised pretraining and supervised
finetuning data fusion (fig."
2708,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"as a result, the task-specific finetuning
broadens the dataset usage (fig 1b andc), which is not limited by data modality
(e.g., both singleand multi-modal data)."
2709,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"(3) data efficiency with limited data
size."
2710,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"our approach could achieve comparable performance even with fewer
finetuned data (e.g., only use 50% of the finetuned data) when compared with
using the entire finetuning dataset."
2711,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"our method
includes an unsupervised multimodal data fusion pretraining and a supervised
flexible-modal finetuning."
2712,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"1a, in the pretraining, our unsupervised
data fusion aims to capture the interaction pattern of image and genomics
features."
2713,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"overall, we formulate the objective of multimodal feature learning by
converting image patches and tabular genomics data into groupwise embeddings,
and then extracting multimodal patient-wise embeddings."
2714,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"for
image feature representation, we randomly divide image patches into groups;
meanwhile, for each type of genomics data, we construct groups of genes
depending on their clinical relevance [22]."
2715,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"1b andc, our
approach enables three types of finetuning modal modes (i.e., multimodal,
image-only, and genomics-only) towards prognostic prediction, expanding the
downstream data utility from the pretrained model."
2716,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"1a, we propose a pathology-and-genomics multimodal model
containing two model streams, including a pathological image and a genomics data
stream.in each stream, we use the same architecture with different weights,
which is updated separately in each modality stream."
2717,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"in the pretraining stage, we develop an
unsupervised data fusion strategy by decreasing the mean square error (mse) loss
to map images and genomics embeddings into the same space."
2718,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"in this way, the pretrained model is trained to
map the paired image and genomics embeddings to be closer in the latent space,
leading to strengthen the interaction between different modalities.in the single
modality finetuning, even if we use image-only data, the model is able to
produce genomic-related image feature embedding due to the multimodal knowledge
aggregation already obtained from the model pretraining."
2719,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,datasets.
2720,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,all image and genomics data are publicly available.
2721,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"we collected wsis
from the cancer genome atlas colon adenocarcinoma (tcga-coad) dataset
(cc-by-3.0) [8,21] and rectum adenocarcinoma (tcga-read) dataset (cc-by-3.0)
[8,20], which contain 440 and 153 patients."
2722,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"we also collected the corresponding tabular genomics
data (e.g., mrna sequence, copy number alteration, and methylation) with overall
survival (os) times and censorship statuses from cbioportal [2,14]."
2723,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"we removed
the samples without the corresponding genomics data or ground truth of survival
outcomes."
2724,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"we implement two types of
settings that involve internal and external datasets for model pretraining and
finetuning."
2725,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"as shown in fig 2a, we pretrain and finetune the model on the same
dataset (i.e., internal setting)."
2726,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"for the external setting, we implement pretraining and
finetuning on the different datasets, as shown in fig 2b ; we use tcga-coad for
pretraining; then, we only use tcga-read for finetuning and final evaluation."
2727,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"we follow the same data split and processing, as well as the
identical training hyperparameters and supervised fusion as above."
2728,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"in table 1, our approach shows improved survival prediction performance on both
tcga-coad and tcga-read datasets."
2729,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"compared with supervised baselines, our
unsupervised data fusion is able to extract the phenotype-genotype interaction
features, leading to achieving a flexible finetuning for different data
settings."
2730,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"we recognize
that the combination of image and mrna sequencing data leads to reflecting
distinguishing survival outcomes."
2731,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"in the meantime, on the tcga-read, our single-modality
finetuned model achieves a better performance than multimodal finetuned baseline
models (e.g., with model pretraining via image and methylation data, we have
only used the image data for finetuning and achieved a c-index of 74.85%, which
is about 4% higher than the best baseline models)."
2732,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"in addition, our model reflects
its efficiency on the limited finetuning data (e.g., 75 patients are used for
finetuning on tcga-read, which are only 22% of tcga-coad finetuning data)."
2733,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"in
table 1, our method could yield better performance compared with baselines on
the small dataset across the combination of images and multiple types of
genomics data."
2734,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"approach broadens the scope of dataset inclusion, particularly
for model finetuning and evaluation, while enhancing model efficiency on
analyzing multimodal clinical data in real-world settings."
2735,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"in addition, the use
of synthetic data and developing a foundation model training will be helpful to
improve the robustness of multimodal data fusion [11,15]."
2736,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"we verify the
model efficiency by using fewer amounts of finetuning data in finetuning."
2737,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"for
tcga-coad dataset, we include 50%, 25%, and 10% of the finetuning data."
2738,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"for the
tcga-read dataset, as the number of uncensored patients is limited, we use 75%,
50%, and 25% of the finetuning data to allow at least one uncensored patient to
be included for finetuning."
2739,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"3a, by using 50% of tcga-coad
finetuning data, our approach achieves the c-index of 64.80%, which is higher
than the average performance of baselines in several modalities."
2740,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"3b, our model retains a good performance by using 50% or 75% of tcga-read
finetuning data compared with the average of c-index across baselines (e.g.,
72.32% versus 64.23%)."
2741,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"the performance
is lower 2%-10% than ours on multi-and single-modality data."
2742,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"for evaluating the
genomics data usage, we designed two settings: (1) combining all types of
genomics data and categorizing them by groups; (2) removing category information
while keeping using different types of genomics data separately."
2743,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"developing data-efficient multimodal learning is crucial to advance the survival
assessment of cancer patients in a variety of clinical data scenarios."
2744,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"importantly, our
approach opens up perspectives for exploring the key insights of intrinsic
genotypephenotype interactions in complex cancer data across modalities."
2745,Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,"[24] developed an artificial intelligence assistive diagnostic solution,
which integrated yolov3 [16] for detection, xception, and patch-based models to
boost classification.although the above-mentioned attempts can improve the
screening performance significantly, there are several issues that need to be
addressed: 1) object detection methods often require accurate annotated data to
guarantee performance with robustness and generalization."
2746,Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,"during inference, only the optimized detector is used
to output the final detection results without any additional modules.3
experimental results dataset."
2747,Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,"for cervical cell detection, our dataset includes 3761 images of 1024 ×
1024 pixels cropped from wsis."
2748,Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,"our private dataset was collected and
qualitycontrolled according to a standard protocol involving three pathologists:
a, b, and c."
2749,Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,"we also collect a new dataset of
5000 positive and negative 224 × 224 cell patches to train the
pcn.implementation details."
2750,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,dataset-a: mitos-atypia 14 challenge1 .
2751,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,"this dataset aims to measure the style
transfer performance on 284 histology frames."
2752,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,"( 2) dataset-b: the cancer
genome atlas (tcga)."
2753,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,"this dataset evaluates the performance of stain
normalization quantified by the downstream nine-category tissue structure
classification accuracy [27]."
2754,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,"to further investigate the effect of ensemble number m, we
conduct ablation on dataset-a."
2755,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"in spite of the large amount of data, the number of labeled samples in
mil (represented by the number of individual, globally labelled wsis) is often
small and/or imbalanced [6]."
2756,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"general data augmentation strategies, such as
rotations, flipping, stain augmentation and normalization and affine
transformations, are applicable to increase the amount of data [15]."
2757,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"all of
these methods are performed in the image domain.here, we consider feature-level
data augmentation directly applied to the representation extracted using a
convolutional neural network."
2758,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"this method was originally proposed as data agnostic approach which also
shows good results if applied to image data [2,4,16]."
2759,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"variations were proposed,
to be applied to latent representations [17] as well as to balance data sets
[6]."
2760,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"due to the structure of mil training data, we identified several options to
perform interpolation-based data augmentation.the main contribution of this work
is a set of novel data augmentation strategies for mil, based on the
interpolation of patch descriptors."
2761,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"for evaluation, a large experimental study was conducted,
including 2 histological data sets, 5 deep learning configurations for mil, 3
common data augmentation strategies and 4 mixup settings."
2762,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"image-based
data augmentation strategies (such as stain-augmentation, rotations or
deformations) can be combined easily with the feature-based approaches but
require individual feature extraction during training."
2763,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"we fix this number to the number of wsis in the training data set, in
order to keep the number of training iterations per epoch consistent.two
different configurations are considered."
2764,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"besides
performing combinations for each wsi during training, selective interpolation
can be useful to keep real samples within the training data."
2765,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"specifically,
we applied a resnet18 pre-trained on the image-net challenge data, due to the
high performance in previous work on similar data [5]."
2766,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"we actively decided not to use a self-supervised contrastive
learning approach [10] as feature extraction stage since invariant features
could interfere with the effect of data augmentation."
2767,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"thereby the amount of investigated data per wsi is reduced with the benefit of
increasing the variability of the data."
2768,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"the data set
utilized in the experiments consists of 80 wsis overall."
2769,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"one half (40) of the
data set consists of frozen and the other half (40) of paraffin sections [5]),
representing the different modalities."
2770,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"the
data set comprised 13 male and 27 female patients, corresponding to a slight
gender imbalance."
2771,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"q q q q q q q q 0 25 % 50 % 75 % 100 %
0.4 the data set was randomly separated into training (80 %) and test data (20
%)."
2772,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"data and source code
are publicly accessible via https://gitlab.com/mgadermayr/mixupmil."
2773,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"the columns represent the frozen
(left) and paraffin data set (right)."
2774,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"subfigure (b) show
the scores obtained with baseline data augmentation for embedding-based and
dual-stream mil."
2775,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"without
data augmentation, scores between 0.49 and 0.72 were obtained for frozen and
scores between 0.41 and 0.81 for the paraffin data set."
2776,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"with baseline data augmentation, scores between 0.69 and 0.73 were achieved for
the frozen and between 0.78 and 0.83 for the paraffin data set."
2777,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"inter-mixup
exhibited scores up to 0.71 for the frozen and up to 0.79 for the paraffin data
set."
2778,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"intra-mixup showed average accuracy up to 0.78 for the frozen and up to
0.84 for the paraffin data set."
2779,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"based on the used common box plot
variation (whiskers length is less than 1.5× the interquartile range), a large
number of data points was identified as outliers."
2780,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"however, these points are not
considered as real outliers, but occur due to the asymmetrical data distribution
(as indicated by the violin plot in the background)."
2781,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"in this work, we proposed and examined novel data augmentation strategies based
on the idea of interpolations of feature vectors in the mil setting."
2782,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"with the baseline data
augmentation approaches, the maximum improvements were 0.03, and 0.02 for the
frozen, and 0.01, and 0.05 for the paraffin data set."
2783,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"the multilinear intra-mixup method, however,
exhibited the best scores for 3 out of 4 combinations and the best overall mean
accuracy for both, the frozen and the paraffin data set."
2784,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"also a clear trend with
increasing scores in the case of an increasing ratio of augmented data (β) is
visible."
2785,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"with regard to the different data
sets, we noticed a stronger, positive effect in case of the frozen section data
set."
2786,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"this is supposed to be due to the clearly higher variability of the frozen
sections corresponding with a need for a higher variability in the training
data."
2787,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"we suppose that this is due
to the fact that the additional loss of the dual-stream architecture exhibits a
valuable regularization tool to reduce the amount of needed training data."
2788,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"with
the proposed intra-mixup augmentation strategy, this effect diminishes, since
the amount and quality of training data is increased.to conclude, we proposed
novel data augmentation strategies based on the idea of interpolations of image
descriptors in the mil setting."
2789,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,"in the future, additional
experiments will be conducted including stain normalization methods and larger
benchmark data sets to provide further insights."
2790,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,"however, due to the high-cost of
collecting survival information from the patients, it is still a challenge to
build effective machine learning models for specific bc subtypes with limited
annotation data.to deal with the above challenges, several researchers began to
design domain adaption algorithms, which utilize the labeled data from a related
cancer subtype to help predict the patients' survival in the target domain."
2791,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,data pre-processing.
2792,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,"next, we aligned the aggregated
tumor or tils features from the two domains separately using maximum mean
discrepancy(mmd) [15].here, we adopted mmd for feature alignment due to its
ability to measure the distance between two distributions without explicit
assumptions on the data distribution, we showed the objective function of mmd in
our method as follows:where h is a hilbert space, f represents the features from
the source, f represents the feature from the target, r represents the layer
number, k ∈ {l, t } referred to tils or tumor node."
2793,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,datasets.
2794,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,"we conducted our experiments on the breast invasive carcinoma (brca)
dataset from the cancer genome atlas (tcga)."
2795,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,"specifically, the brca dataset
includes 661 patients with hematoxylin and eosin (he)-stained pathological
imaging and corresponding survival information."
2796,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,"in this study, we compared the performance of our proposed model with several
existing domain adaptation methods, including 1) ddc [17]: utilize the maximum
mean discrepancy (mmd) to calculate the domain difference loss between source
and target data and optimize both classification loss and disparity loss."
2797,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"(2) learning features from genomics data which have tens of
thousands of genes make models such as transformer [16] impractical to use due
to its quadratic computation complexity."
2798,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"recently, the
literature corpus has proposed some methods for accomplishing specific
image-omic tasks via kronecker product fusion [2] or co-attention mapping
between wsis and genomics data [3]."
2799,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"as for the co-attention module, it is unidirectional and
cannot localize significant regions from genetic data with a large amount of
information.in this paper, we propose a task-specific framework dubbed
gene-induced multimodal pre-training (gimp) for image-omic classification."
2800,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"furthermore, to model the high-order relevance of the two modalities, we combine
cls tokens of paired image and genomic data to form unified representations and
propose a triplet learning module to differentiate patient-level positive and
negative samples in a mini-batch."
2801,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"given a multimodal dataset d consisting of pairs of wsi pathological images and
genomic data (x i , x g ), our gimp learns feature representations via
accomplishing masked patch modeling and triplets learning."
2802,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"in this section, we propose group multi-head self attention (groupmsa), a
specialized gene encoder to capture structured features in genomic data cohorts."
2803,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"finally, groupmsa could learn dense semantics from the genomic data
cohort."
2804,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"to overcome
these issues, we further propose a gene-induced triplet learning module, which
uses pathological images and genomic data as input and extracts high-order and
discriminative features via cls tokens."
2805,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"we use a simple multi-layer perceptron (mlp) head to map
cls pat to the final class predictions p , which can be written as p =
softmax(mlp(cls pat )).3 experiments datasets."
2806,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"we verify the effectiveness of our method on the caner genome atlas
(tcga) non-small cell lung cancer (nsclc) dataset, which contains two cancer
subtypes, i.e., lung squamous cell carcinoma (lusc) and lung adenocarcinoma
(luad)."
2807,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"we collect corresponding rna-seq fpkm
data for each patient and the length of the input genomic sequence is 60,480."
2808,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"we randomly split the data into 567 for training, 189 for validation and 190 for
testing.implementation details."
2809,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"the pre-training process of all algorithms is
conducted on the training set, without any extra data augmentation."
2810,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"note that
our genetic encoder, groupmsa, is fully supervised pre-trained on unimodal
genetic data to accelerate convergence and it is frozen during gimp training
process."
2811,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"firstly, we compare our proposed patch aggregator with the current
state-of-the-art deep mil models on unimodal tcga-nsclc dataset, i.e., only
pathological wsis are included as input."
2812,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"we can
observe in the table that, our gimp raises acc from 91.05% to 99.47% on
tcga-nsclc dataset."
2813,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"2 (a)
and (b) indicates that the addition of the genomic data is indispensable in
increasing the inter-class distance and reducing the intra-class distance, which
confirms our motivation that gene-induced multimodal fusion could model
high-order relevance and yield more discriminative representations."
2814,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"in this paper, we propose a novel multimodal pre-training method to exploit the
complementary relationship of genomic data and pathological images."
2815,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"simclr [3] introduced the utilization of data augmentation and a
learnable nonlinear transformation between the feature embedding and the
contrastive loss to generally improve the quality of feature embedding."
2816,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"pcl [9] and hcsc [5] integrated the
k-means clustering and contrastive learning model by introducing prototypes as
latent variables and assigning each sample to multiple prototypes to learn the
hierarchical semantic structure of the dataset."
2817,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"owing to the manifold distribution hypothesis [8], the relative
distance between high-dimensional data is preserved on a low-dimensional
manifold."
2818,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"however, this method captured
the nonlinear data manifold structure on the original data (not on the feature
vectors) only once at the beginning stage, which is not updated in the further
training process.in this study, we propose a hybrid method that combines
manifold learning and contrastive learning to generate a good feature extractor
(encoder) for histopathology image classification."
2819,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"the dataset for the former task was collected from 168 patients
with 332 wsis from seoul national university hospital."
2820,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"the liver cancer dataset for the latter task was composed of 323 wsis,
in which the wsis can be further classified into hepatocellular carcinomas
(hccs) (collected from pathology ai platform [1]) and ihccs."
2821,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"the performance of different models from two different datasets is reported in
this section."
2822,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"red dots represent sdt samples and blue dots represent ldt samples
from the ihccs dataset (corresponding histology thumbnail images are shown on
the right)."
2823,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"in
the future, we plan to optimize the algorithm and apply our method to other
datasets and tasks, such as multi-class classification problems and natural
image datasets."
2824,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"-we developed a comprehensive
pipeline for constructing tumor-associated stroma datasets across multiple data
sources, and employed adversarial training and neighborhood consistency
regularization techniques to learn robust multimodal-invariant image
representations."
2825,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"however, using data from multiple
modalities can introduce systematic shifts, which can impact the performance of
a deep learning model."
2826,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"the optimization process aims to achieve a
balance between these two goals, resulting in an embedding space that encodes as
much information as possible about tumor-associated stroma identification while
not encoding any information on the data source."
2827,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"in our study, we utilized three datasets for tumor-associated stroma
analysis.(1) dataset a comprises 513 tiles extracted from the whole mount slides
of 40 patients, sourced from the archives of the pathology department at
cedars-sinai medical center (irb# pro00029960)."
2828,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"the tiles were
annotated at the pixel-level by expert pathologists to generate stroma tissue
segmentation masks and were cross-evaluated and normalized to account for stain
variability.(2) dataset b included 97 whole mount slides with an average size of
over 174,000×142,000 pixels at 40x magnification."
2829,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"(3) dataset c comprised 6134 negative biopsy slides obtained from 262
patients' biopsy procedures, where all samples were diagnosed as negative."
2830,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"dataset a was utilized for
training the stroma segmentation model."
2831,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"extensive data augmentation techniques,
such as image scaling and staining perturbation, were employed during the
training process."
2832,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"this model was then applied to generate stroma
masks for all slides in datasets b and c."
2833,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"to precisely isolate stroma tissues
and avoid data bleeding from epithelial tissues, we only extracted patches where
over 99.5% of the regions were identified as stroma at 40x magnification to
construct the stroma classification dataset.for positive tumor-associated stroma
patches, we sampled patches near tumor glands within annotated tumor region
boundaries, as we presumed that tumor regions represent zones in which the
greatest amount of damage has progressed."
2834,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"to
incorporate multi-modal information, we randomly sampled negative stroma patches
from all biopsy slides in dataset c."
2835,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"future research can focus on validating our
approach on larger and more diverse datasets and expanding the method to a
patient-level prediction system, ultimately improving prostate cancer diagnosis
and treatment."
2836,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,"for
example, patch-gcn [5] treated the wsi as point cloud data, and the patch-level
adjacent relationship of wsi is learned by a graph convolutional network (gcn)."
2837,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,"furthermore, due to
the large size of crc dataset and relatively high model complexity, patch-gcn
and transmil encountered a memory overflow when processing the crc dataset,
which limits their clinical application."
2838,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,"in addition, the feature aggregation of the lower levels (i.e., patch
and tissue) are guided by the priors, and the mhsa is only executed on
pathological components, resulting in high efficiency even on the crc dataset."
2839,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,"[13] 0.580 ± 0.005 0.634 ± 0.005 0.617 ± 0.094 deepattnmisl [26] 0.570 ± 0.001
0.644 ± 0.009 0.584 ± 0.019 clam [18] 0.575 ± 0.010 0.641 ± 0.002 0.635 ± 0.006
dsmil [16] 0.550 ± 0.016 0.626 ± 0.005 0.603 ± 0.022 patchgcn [ we selected the
crc dataset for further interpretable analysis, as it is one of
the leading causes of mortality in industrialized countries, and its
prognosis-related factors have been widely studied [3,8]."
2840,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,"we trained an encoded
feature based classification model (i.e., a mlp) on a open-source colorectal
cancer dataset (i.e., nct-crc-he-100k [14]), which is annotated with 9 classes,
including: adipose tissue (adi); background (back); debris (deb); lymphocytes
(lym); mucus (muc); muscle (mus); normal colon mucosa (norm); stroma (str);
tumor (tum)."
2841,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"experiments on cva-bus dataset [9] demonstrate that ultra-det,
with real-time inference speed, significantly outperforms previous works,
reducing about 50% fps at a recall rate of 0.90.our contributions are four-fold."
2842,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"(4) we release high-quality labels of the cva-bus
dataset [9] to facilitate future research."
2843,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"we use the open source cva-bus dataset that consists of 186
valid videos, which is proposed in cva-net [9]."
2844,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"we split the dataset into
train-val (154 videos) and test (32 videos) sets."
2845,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"we focus on the lesion detection task and do not utilize the
benign/malignant classification labels provided in the original
dataset.high-quality labels."
2846,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"the bounding box labels provided in the original
cva-bus dataset are unsteady and sometimes inaccurate, leading to jiggling and
inaccurate model predictions."
2847,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"we use flownets [3] as the fixed flownet in iof align and
share the same finding with previous works [4,12,13] that the flownet trained on
natural datasets generalizes well on ultrasound datasets."
2848,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"as a result,
many methods focus solely on improving a(•) or f (•), leaving g(•) untrained on
the wsi dataset (as shown in fig."
2849,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"(3) we conduct extensive experiments on two
datasets using three different backbones and demonstrate the effectiveness of
our proposed framework."
2850,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"to begin with, we first employ a traditional approach to
train a bag-level classifier f (•) on a given dataset, with patch embeddings
generated by a fixed resnet50 [6] pre-trained on imagenet [19] (step 1 in fig."
2851,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"after this, g(•) is fine-tuned for the specific wsi
dataset, which allows it to generate improved representations for each instance,
thereby enhancing the performance of f (•)."
2852,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"our experiments utilized two datasets, with the first being the publicly
available breast cancer dataset, camelyon16 [1]."
2853,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"this dataset consists of a
total of 399 wsis, with 159 normal and 111 metastasis wsis for the training set,
and the remaining 129 for test."
2854,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"although patch-level labels are officially
provided in camelyon16, they were not used in our experiments.the second dataset
is a private hepatocellular carcinoma (hcc) dataset collected from sir run run
shaw hospital, hangzhou, china."
2855,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"this dataset comprises a total of 1140 valid
tumor wsis scanned at 40× magnification, and the objective is to identify the
severity of each case based on the edmondson-steiner (es) grading."
2856,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"for camelyon16, we tiled the wsis into 256×256 patches on 20× magnification
using the official code of [25], while for the hcc dataset the patches are
384×384 on 40× magnification following the pathologists' advice."
2857,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"for both
datasets, we used an imagenet pre-trained resnet50 to initialize g(•)."
2858,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"camelyon16 results
are reported on the official test split, while the hcc dataset used a 7:1:2
split for training, validation and test."
2859,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"since the number of
instances is very large in wsi datasets, we empirically recommend to choose to
run icmil one iteration for fine-tuning g(•) to achieve the balance between
performance gain and time consumption."
2860,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"results on the hcc dataset also proves the effectiveness of icmil, despite
the minor difference on the relative performance of baseline methods."
2861,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"mean
pooling performs better on this dataset due to the large area of tumor in the
wsis (about 60% patches are tumor patches), which mitigates the impact of
average pooling on instances."
2862,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"also, the performance differences among different
vanilla mil methods tends to be smaller on this dataset since risk grading is a
harder task than camelyon16."
2863,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"as a result, after applying icmil on the mil baselines, these
methods all gain great performance boost on the hcc dataset."
2864,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"5
displays the instance-level and bag-level representations of camelyon16 dataset
before and after applying icmil on ab-mil backbone.the results indicate that one
iteration of g(•) fine-tuning in icmil significantly improves the instance-level
representations, leading to a better aggregated baglevel representation
naturally."
2865,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,"furthermore, our approach is trained solely with artifact-free images, which
reduces the difficulty in data collection.the major contributions are two-fold."
2866,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,"extensive evaluations
on real-world histology datasets and downstream tasks demonstrate the
superiority of our framework in artifact removal performance, which can generate
reliable restored images while preserving the stain style."
2867,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,dataset.
2868,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,"the test set uses another public histology image dataset [6]
with 462 artifact-free images 2 , where we obtain the paired artifact images by
the manually-synthesized artifacts [18]."
2869,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,"consequently, we leverage the prevalent cycle-gan [19] as the baseline for
comparison, because of its excellent performance in the image transfer, and also
its nature that requires no paired data can fit our circumstance."
2870,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,"for a fair compaison, we train the
cyclegan with two configurations, namely (#1) using the entire dataset, and (#2)
using only half the dataset, where the latter uses the same number of the
training samples as artifusion."
2871,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,"to this end, we use the public dataset nct-crc-he-100k for training and
crc-val-he-7k for testing, which together contains 100, 000 training samples and
7, 180 test samples."
2872,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,"we consider the performance on the original unprocessed
data, denoted as 'clean', as the upper bound."
2873,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,"experimental results on a
public histological dataset demonstrate the superiority of our proposed method
over the state-of-the-art gan counterpart."
2874,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,"[3,4] developed the sononet [1] model, which integrates
eye-gaze data of sonographers and used generative adversarial networks to
address the lack of eye-gaze data."
2875,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,"[11] proposed the use of a
teacher-student knowledge transfer framework for us image analysis, which
combines doctor's eye-gaze data with us images as input to a large teacher
model, whose outputs and intermediate feature maps are used to condition a
student model."
2876,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,"although these methods have led to promising results, they can be
difficult to implement due to the need to collect doctors' eye movement data for
each image, along with certain restrictions on the network structure.different
from the existing studies, we propose a novel framework to adjust the general
cnns to ""think like sonographers"" from three different levels."
2877,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,"gaze
data collection."
2878,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,"we collected the eye movement data with the tobii 4c
eye-tracker operating at 90 hz."
2879,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,"binary maps of
the same size as the corresponding mskus images were generated using the gaze
data, with the pixel corresponding to the point of gaze marked with a'1' and the
other pixels marked with a'0'."
2880,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,"this removed the need to collect eye movement
maps during the training and testing phases, significantly lightening the
workload of data collection."
2881,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,"3)
experiments results show our proposed method outperforms five existing
scribble-supervised methods on the public dataset word [17] for multiple
abdominal organ segmentation."
2882,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,"we used the publicly available abdomen ct dataset word [17] for experiments,
which consists of 150 abdominal ct volumes from patients with rectal cancer,
prostate cancer or cervical cancer before radiotherapy."
2883,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,"following
the default settings in [17], the dataset was split into 100 for training, 20
for validation and 30 for testing, respectively, where the scribble annotations
for foreground organs and background in the axial view of the training volumes
had been provided and were used in model training."
2884,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,"figure 2 shows a visual comparison between our method
and the other weakly supervised methods on the word dataset (word 0014.nii)."
2885,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,"experiments on a public abdominal ct dataset
word demonstrated the effectiveness of the proposed method, which outperforms
five existing scribble-based methods and narrows the performance gap between
weakly-supervised and fully-supervised segmentation methods."
2886,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,"in the future, we
will explore the effect of our method on sparser labels, such as a volumetric
data with scribble annotations on one or few slices."
2887,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"by
using gaze data to guide model training, we can improve model interpretability
and performance [24].radiologists' eye movements can be automatically and
unobtrusively recorded during the process of reading mammograms, providing a
valuable source of data without the need for manual labeling."
2888,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"the gaze data can guide the model's attention towards the
malignant masses."
2889,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"leveraging gaze data can
guide the model to locate malignant calcifications.in this work, we propose a
novel diagnostic model, namely mammo-net, which integrates radiologists' gaze
data and interactive information between cc-view and mlo-view to enhance
diagnostic performance."
2890,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"to the best of our knowledge, this is the first work to
integrate gaze data into multi-view mammography classification."
2891,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"• we demonstrate the effectiveness of
our approach through experiments using mammography datasets, which show the
superiority of mammo-net.2 proposed method the pipeline of mammo-net is
illustrated in fig."
2892,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"additionally, by integrating gaze data from radiologists,
our proposed model is able to generate more precise attention maps.the fusion
network combines multi-view feature representations using a stack of
linear-activation layers and a fully connected layer, resulting in a
classification output."
2893,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"layernorm is
also employed to address the issue of imprecise gaze data."
2894,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"the overall loss function is defined as the sum of these three loss
functions, with coefficients λ and μ used to adjust their relative weights:3
experiments and results mammogram dataset."
2895,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"the cbis-ddsm dataset contains 1249 exams that have been divided based on
the presence or absence of masses, which we used to perform mass classification."
2896,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"the inbreast dataset contains 115 exams with both masses and
micro-calcifications, on which we performed benign and malignant classification."
2897,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,we split the inbreast dataset into training and testing sets in a 7:3 ratio.
2898,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"it
is worth noting that the official inbreast dataset does not provide image-level
labels, so we obtained these labels following shen et al."
2899,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,[20].eye gaze dataset.
2900,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"eye movement data was collected by reviewing all cases in inbreast using a tobii
pro nano eye tracker."
2901,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"to overcome the problem of limited data, we
employed various data augmentation techniques, including translation, rotation,
and flipping."
2902,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"considering the
relatively small size of our dataset, we used resnet-18 as the backbone of our
network."
2903,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"the ga-net [23] proposed
a resnet-based model with class activation mapping guided by eye gaze data."
2904,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"we believe that one possible reason for
the inferior performance of ga-net compared to mammo-net might be the use of a
simple mse loss by ga-net, which neglects the coarse nature of the gaze data."
2905,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"figure 2 illustrates the visualization of our proposed model on
three representative exams from the inbreast dataset that includes masses,
calcifications, and a combination of both."
2906,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"for each exam, we present gaze heat
maps generated from eye movement data."
2907,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"the
pyramid loss improves the model's robustness even when the radiologist's gaze
data is not entirely focused on the breast."
2908,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"to achieve this, we integrate gaze data
as a form of weak supervision for both lesion positioning and interpretability
of the model."
2909,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"our experimental results on mammography datasets demonstrate the
superiority of our proposed model."
2910,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"the increased availability of radiological data and rapid advances in medical
image analysis has led to an exponential growth in prediction models that
utilize features extracted from clinical imaging scans to detect and diagnose
diseases and predict response to treatment [1][2][3][4]."
2911,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"like gans or variational autoencoders, normalizing flows
is a method for learning complex data representations but with an explicit
ability to infer the output as a probability distribution and with the added
benefit of more stable training [12]."
2912,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"employed a normalizing flow
model conditioned on ldct reconstruction by filtered backprojection to improve
reconstruction quality from the raw sinogram data [14]."
2913,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"this study used two unique datasets: (1) the ucla low-dose chest ct dataset, a
collection of 186 exams acquired using siemens ct scanners at an equivalent dose
of 2 mgy following an institutional review board-approved protocol."
2914,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"the raw
projection data of scans were exported, and poisson noise was introduced, as
described in zabic et al."
2915,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"projection data were then reconstructed into an image size of 512 × 512
using three reconstruction kernels (smooth, medium, sharp) at 1.0 mm slice
thickness."
2916,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"the dataset was split into 80 scans for training, 20 for validation,
and 86 for testing."
2917,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"(2) aapm-mayo clinic low-dose ct ""grand challenge"" dataset,
a publicly available grand challenge dataset consisting of 5,936 abdominal ct
images from 10 patient cases reconstructed at 1.0 mm slice thickness."
2918,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"this dataset was only
used for evaluating image quality against other harmonization techniques."
2919,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"in practice,
a multilayer flow operation is preferred because a single-layer flow cannot
represent complex non-linear relationships within the data."
2920,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"using the grand challenge dataset, we assessed image
quality and compared it with other previously published low-dose ct denoising
techniques."
2921,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"the model was trained and validated on the
lidc-idri dataset, a public de-identified dataset of diagnostic and low-dose ct
scans with annotations from four experienced thoracic radiologists."
2922,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"as part of
the training process, we only considered nodules annotated by at least three
readers in the lidc dataset."
2923,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"on the grand challenge dataset, ctflow took 3 days to train on an nvidia rtx
8000 gpu."
2924,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"[28] applied an approximate bayesian inference scheme based on posterior
regularization to improve uncertainty quantification on covariate-shifted data
sets, resulting in improved prognostic models for prostate cancer."
2925,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"hoffman, nastaran emaminejad, and
michael mcnitt-gray for providing access to the ucla low-dose ct dataset."
2926,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"our contributions are threefold:• we propose a novel
gaussian-probabilistic guided semantic fusion method for polyp segmentation,
which improves the decoder's global perception of polyp locations and
discrimination capability for polyps in complex scenarios.• we evaluate the
performance of petnet on five widely adopted datasets, demonstrating its
superior ability to identify polyp camouflage and small polyp scenes, achieving
state-of-the-art performance in locating polyps with high precision."
2927,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"in our
evaluation, we also examine the decoder cfm utilized in [3], which shares the
same input features (excluding the first level) as the fus.3 experiments to
evaluate models fairly, we completely follow p ranet [4] and use five public
datasets, including 548 and 900 images from clinicdb [2] and kvasir-seg [5] as
training sets, and the remaining images as validation sets."
2928,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"we also test the
generalization capability of all models on three unseen datasets (etis [13] with
196 images, cvc-colondb [8] with 380 images, and endoscene [15] with 60 images)."
2929,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"our model achieves comparable performance to
the sota model on the kvasir-seg and clinicdb datasets."
2930,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"we conduct three
unseen datasets to test models' generalizability."
2931,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"we selected images from two unseen datasets with 0∼2% polyp
labeled area to perform the test."
2932,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"as shown, p et net demonstrates great strength
in both datasets, which indicates that one of the major advantages of our model
lies in detecting small polyps with lower false-positive rates.ablation
analysis."
2933,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"furthermore, the git method
significantly enhances instance-level evaluation without incurring performance
penalty in pixel-level evaluation, especially in unseen datasets."
2934,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"experiments show that p et net consistently outperforms
most current cutting-edge models on five challenging datasets, demonstrating its
solid robustness in distinguishing other intestinal analogs."
2935,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"besides using the
data-driven manner, to achieve accurate diagnosis and interpretation of the
ai-assisted system output, it is essential to consider mammogram domain
knowledge in a model-driven fashion.authenticated by the bi-rads lexicon [12],
the asymmetry of bilateral breasts is a crucial clinical factor for identifying
abnormalities."
2936,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the tumor set t is
collected from real-world datasets."
2937,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"specifically, to maintain the rule of
weaklysupervised learning of segmentation and localization tasks, we collect the
tumors from the ddsm dataset as t and train the model on the inbreast
dataset.when training the model on other datasets, we use the tumor set
collected from the inbreast dataset."
2938,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,this study reports experiments on four mammography datasets.
2939,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the inbreast
dataset [7] consists of 115 exams with bi-rads labels and pixel-wise
anno-tations, comprising a total of 87 normal (bi-rads = 1) and 342 abnormal
(bi-rads = 1) images."
2940,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the ddsm dataset [3] consists of 2,620 cases, encompassing
6,406 normal and 4,042 (benign and malignant) images with outlines generated by
an experienced mammographer."
2941,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the vindr-mammo dataset [8] includes 5,000 cases
with bi-rads assessments and bounding box annotations, consisting of 13,404
normal (bi-rads = 1) and 6,580 abnormal (bi-rads = 1) images."
2942,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the in-house
dataset comprises 43,258 mammography exams from 10,670 women between 2004-2020,
collected from a hospital with irb approvals."
2943,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"in this study, we randomly select
20% women of the full dataset, comprising 6,000 normal (bi-rads = 1) and 28,732
abnormal (bi-rads = 1) images."
2944,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"due to a lack of annotations, the in-house
dataset is only utilized for classification tasks."
2945,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"each dataset is randomly
split into training, validation, and testing sets at the patient level in an
8:1:1 ratio, respectively (except for that inbreast which is split with a ratio
of 6:2:2, to keep enough normal samples for the test).table 1."
2946,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"comparison of
asymmetric and abnormal classification tasks on four mammogram datasets."
2947,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"for training
models, we employ random zooming and random cropping for data augmentation."
2948,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the training process on the
inbreast dataset is conducted for 50 epochs with a lr decay of 0.1 every 20
epochs."
2949,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"for the other three datasets, the training is conducted separately on
each one with 20 epochs and a lr decay of 0.1 per 10 epochs."
2950,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the
training takes 3-24 h (related to the size of the dataset) on each dataset.to
assess the performance of different models in classification tasks, we calculate
the area under the receiver operating characteristic curve (auc) metric."
2951,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"our method outperforms all the single-based and mv-based
methods in these classification tasks across all datasets."
2952,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the extensive
experiments on four datasets demonstrate the robustness of our disasymnet
framework for improving performance in classification, segmentation, and
localization tasks."
2953,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"to achieve this goal, we
create a synthetic dataset, which has separate annotations for normal kidneys
and protruded regions, and train a segmentation network to separate the
protruded regions from the normal kidney regions."
2954,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"verify
that the proposed framework achieves a higher dice score compared to the
standard 3d u-net using a publicly available dataset."
2955,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"the release of two public ct image datasets with kidney and tumor masks from the
2019/2021 kidney and kidney tumor segmentation challenge [8] (kits19, kits21)
attracted researchers to develop various methods for segmentation.looking at the
top 3 teams from each challenge [6,11,13,17,21], all teams utilized 3d u-net [3]
or v-net [16], which bears a similar architecture."
2956,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,we train this network using synthetic datasets.
2957,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"the details of the
dataset and training procedures are described in sect."
2958,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,synthetic dataset.
2959,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"however, annotating such
areas is time-consuming and preparing a large number of data is challenging."
2960,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"alternatively, we create a synthetic dataset that mimics a kidney with
protrusions."
2961,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,the synthetic dataset is created through the following steps:1.
2962,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"if both of the following conditions are met, append to
the dataset.where k i is a voxel value (0 or 1) in the kidney mask and t i is a
voxel value in the tumor mask."
2963,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"although our network
is fully differentiable, since there is no separate annotation for protruded
regions other from the synthetic dataset, we freeze the parameters in
protuberance detection network.the output of the protuberance detection network
will likely have more false positives than the base network since it has no
access to the input image."
2964,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,we used a dataset from kits19 [8] which contains both cect and ncct images.
2965,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"cysts are not
annotated separately and included in the kidney label on this dataset."
2966,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"the data
can be downloaded from the cancer imaging archive (tcia) [4,9].the images were
first clipped to the intensity value range of [-90, 210] and normalized from -1
to 1."
2967,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"we applied
random rotation, random scaling and random noise addition as data
augmentation.during the step2 phase of the training, where we used the synthetic
dataset, we created 10,000 masks using the method from sect."
2968,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"to cope with isodensity tumors, which have similar intensity values
to their surrounding tissues, we created a synthetic dataset to train a network
that extracts protuberance from the kidney masks."
2969,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"we evaluated our method using the publicly
available kits19 dataset, and showed that the proposed method can achieve a
higher sensitivity than existing approach."
2970,Skin Lesion Correspondence Localization in Total Body Photography,"given this mapping, we create an
initial dense correspondence between the source and target vertices, φ l, : v 0
→ v 1 by mapping a source vertex v ∈ v 0 to the target vertex with the most
similar feature descriptor (with similarity measured in terms of the normalized
cross-correlation): while feature descriptors of corresponding vertices on the
source and target
mesh are identical when 1) the landmarks are in perfect correspondence, and 2)
the source and target differ by an isometry, neither of these assumptions holds
in realworld data."
2971,Skin Lesion Correspondence Localization in Total Body Photography,we evaluated our methods on two datasets.
2972,Skin Lesion Correspondence Localization in Total Body Photography,"the first dataset is from skin3d [26]
(annotated 3dbodytex [18,19])."
2973,Skin Lesion Correspondence Localization in Total Body Photography,"the second dataset comes from a 2d imaging-rich
total body photography system (irtbp), from which the 3d textured meshes are
derived from photogrammetry 3d reconstruction."
2974,Skin Lesion Correspondence Localization in Total Body Photography,"the number of vertices is on
average 300k and 600k for skin3d and irtbp datasets respectively."
2975,Skin Lesion Correspondence Localization in Total Body Photography,example data of the two datasets can be found in the supplement.
2976,Skin Lesion Correspondence Localization in Total Body Photography,"to interpret cle in a clinical application,
the localized correspondence is successful if its cle is less than a threshold
criterion.we measured the success rate as the percentage of the correctly
localized skin lesions over the total number of skin lesion pairs in the
dataset.to compare our result to the existing method [26], we compute our
success rates with the threshold criterion at 10 mm."
2977,Skin Lesion Correspondence Localization in Total Body Photography,"the qualitative result of the localized correspondence in the skin3d
dataset is shown in fig."
2978,Skin Lesion Correspondence Localization in Total Body Photography,"comparison of the success rate on skin3d
dataset."
2979,Skin Lesion Correspondence Localization in Total Body Photography,"the framework is evaluated on a private dataset and a public dataset with
success rates that are comparable to those of the state-of-the-art method.the
proposed method assumes that the local texture enclosing the lesion and its
surroundings should be similar from scan to scan."
2980,Skin Lesion Correspondence Localization in Total Body Photography,"in addition, the method may not work well with
longitudinal data that has non-isometric deformation due to huge variations in
body shape, inconsistent 3d reconstruction, or a dramatic change in pose and,
therefore, topology, such as an open armpit versus a closed one.in the future,
the method needs to be evaluated on longitudinal data with longer duration and
new lesions absent in the target."
2981,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,"data augmentation (da) is a key factor in the success of deep neural networks
(dnn) as it artificially enlarges the training set to increase their
generalization ability as well as robustness [22]."
2982,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,"it plays a crucial role in
medical image analysis [8] where annotated datasets are only available with
limited size."
2983,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,"however, the da scheme received less attention, despite its
potential to leverage the data characteristic and address overfitting as the
root of generalization problems.state-of-the-art approaches still rely on
simplistic spatial transformations, like translation, rotation, cropping, and
scaling by globally augmenting the mri sequences [12,20]."
2984,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,"however, soft tissue deformations, which are currently missing from the da
schemes, are known to significantly affect the image morphology and therefore
play a critical role in accurate diagnosis [6].both lesion and prostate shape
geometrical appearance influence the clinical assessment of prostate
imaging-reporting and data system (pi-rads) [24]."
2985,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,"motion models have not been integrated into
any deep learning framework as an online data augmentation yet, thereby leaving
the high potential of inducing applicationspecific knowledge into the training
procedure unexploited.in this work we propose an anatomy-informed spatial
augmentation, which leverages information from adjacent organs to mimic typical
deformations of the prostate."
2986,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,"this technique allows us
to simulate different physiological states during the training and enrich our
dataset with a wider range of organ and lesion shapes."
2987,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,"all
experiments were performed in accordance with the declaration of helsinki [2]
and relevant data privacy regulations."
2988,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,"a possible explanation for this result is that less
than 30% of the lesions are located close to the bladder, and our dataset did
not contain enough training examples for more improvements.realistic modeling of
organ deformation."
2989,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,"the success of
anatomy-informed da opens the research question of whether it enhances
performance across diverse datasets and model backbones."
2990,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"previously,
learning models have been used in combination with iknife data for ex-vivo
tissue characterization and real-time margin detection [16,17].the success of
clinical deployment of learning models heavily relies on approaches that are not
only accurate but also interpretable."
2991,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"studies
suggest that one way to improve these factors is through data centric approaches
i.e."
2992,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,to focus on appropriate representation of data.
2993,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"specifically,
representation of data as graphs has been shown to be effective for medical
diagnosis and analysis [1]."
2994,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"particularly, graph transformer networks (gtn) has have shown to
further enhance the transparency of underlying relation between the graph nodes
and decision making via attention mechanism [11].biological data, specially
those acquired intra-opertively, are heterogeneous by nature."
2995,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"while the use of
ex-vivo data collected under specific protocols are beneficial to develop
baseline models, intra-operative deployment of these models is challenging."
2996,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"for
iknife, the ex-vivo data is usually collected from homogeneous regions of
resected specimens under the guidance of a trained pathologist, versus the
intra-operative data is recorded continuously while the surgeon cutting through
tissues with different heterogeneity and pathology."
2997,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"to demonstrate the state-of-theart performance of the proposed
approach on mass spectrometry data, the model is compared with different
baselines in both cross-validation and prospective schemes on ex-vivo data."
2998,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"in
addition to the proposed model, we present a new visualization approach to
better correlate the graph nodes with the spectral content of the data, which
improves interpretability."
2999,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"following data
collection and curation, each burn (spectrum) is converted to a single graph
structure."
3000,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"ex-vivo: data is collected from fresh breast tissue samples from the patients
referred to bcs at kingston health sciences center over two years."
3001,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"in addition to spectral data, clinicopathological
details such as the status of hormone receptors is also provided
post-surgically."
3002,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"a stream of iknife data is collected during a bcs case (27 min) at kingston
health sciences center."
3003,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"therefore, there are two
mechanisms embedded in egt: i) node-level attention calculation -via aggregation
of neighboring nodes according to their relevance to the predictions, and ii)
graph-level uncertainty estimation -via fitting the dirichlet distribution to
the predictions.in the context of surgical margin assessment, the attentions
reveal the relevant metabolic ranges to cancerous tissue, while uncertainty
helps identify and filter data with unseen pathology."
3004,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"additionally, we run ablation studies on the graph structure themselves to
show the importance of presenting the data as graphs."
3005,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"this information can be provided during deployment to further
augment surgical decision making for uncertain data instances."
3006,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"it can be seen that by not using the
network prediction for up to 10% of most uncertain test data, the auc increases
to 1."
3007,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"multi-level graphs were shown to
outperform other structures for masspect data [akbarifar 2021] as they preserve
the receptive field in the neighborhood of subbands (metabolites)."
3008,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,"the raw intra-operative iknife data (y-axis is m/z spectral range and x-axis is
the surgery timeline) along with the temporal reference labels extracted from
surgeon's call-outs and pathology report are shown in fig."
3009,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,"although dnn's are more effective at capturing local
variations, they can overfit without sufficient data for training [17]."
3010,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,the overall loss function is then:3 experiments dataset.
3011,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,"we use a dataset of 172 patients containing 94 paaf and 78 peaf cases
collected from the sun yat-sen memorial hospital in china."
3012,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,"cross-validation is implemented by
splitting the dataset into five equal subsets and using three subsets for
training, one subset for validation, and one subset for testing."
3013,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,"data acquisition procedures and statistics are given in the
supplementary materials.setup."
3014,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,"experiments on larger
datasets or alternative tasks can also be done to provide more empirical
support, since current results show only slight improvements over baseline."
3015,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,"however, mri source data of too few sequences (only t1 and t2) may not
provide enough valuable informative to effectively synthesize ce-mri."
3016,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,"investigated the feasibility of using deep learning (a
simple u-net structure) to simulate contrast-enhanced breast mri of invasive
breast cancer, using source data including t1-weighted non-fatsuppressed mri,
t1-weighted fat-suppressed mri, t2-weighted fat-suppressed mri, dwi, and
apparent diffusion coefficient [5]."
3017,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,"the generator's objective function is as
follows:and the discriminator's objective function is as follows:where pro data
(d 1 , d 2 , d 3 , d 4 , t 1 ) represents the empirical joint distribution of
inputs d 1 (dw i b0 ), d 2 (dw i b150 ), d 3 (dw i b800 ), d 4 (dw i b1500 ) and
t 1 (t1weighted mri), λ 1 is a non-negative trade-off parameter, and l 1 -norm
is used to measure the difference between the generated image and the
corresponding ground truth."
3018,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,"based on the ratio of 8:2, the training set and independent test set of the
in-house dataset have 612 and 153 cases, respectively."
3019,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,"first, we compare the performance of different existing methods on synthetic
ce-mri using our source data, the quantitative indicators used include psnr,
ssim and nmse."
3020,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,"[5]
used full-sequence mri to synthesize ce-mri, it would be advantageous to obtain
synthetic ce-mri images using as little data as possible, taking advantage of
the most contributing sequences."
3021,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,"we have developed a multi-sequence fusion network based on multi-b-value dwi to
synthesize ce-mri, using source data including dwis and t1-weighted
fatsuppressed mri."
3022,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,"compared to existing methods, we avoid the challenges of
using full-sequence mri and aim to be selective on valuable source data dwi."
3023,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"this study has two main
novelties: (i) to the best of our knowledge, this is the first clinical
evaluation study of the vce-mri technique in rt; and (ii) multiinstitutional mri
data were included in this study to obtain more reliable results."
3024,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"patient data was retrospectively collected from three oncology centers in hong
kong."
3025,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"this dataset included 303 biopsy-proven (stage i-ivb) npc patients who
received radiation treatment during 2012-2016."
3026,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"the use of this dataset was approved by the institutional review board
of the university of hong kong/hospital authority hong kong west cluster (hku/ha
hkw irb) with reference number uw21-412, and the research ethics committee
(kowloon central/kowloon east) with reference number kc/ke-18-0085/er-1."
3027,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"the details of patient
characteristics and the number split for training and testing of each dataset
were illustrated in table 1."
3028,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"prior to model training, mri images were resampled
to 256*224 by bilinear interpolation [14] due to the inconsistent matrix sizes
of the three datasets."
3029,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"different from the original
study, which used single institutional data for model development and utilized
min-max value of the whole dataset for data normalization, in this work, we used
mean and standard deviation of each individual patient to normalize mri
intensities due to the heterogeneity of the mri intensities across institutions
[15]."
3030,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"ji measures
similarity of two datasets, which ranges from 0% to 100%."
3031,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"due to both real patients and synthetic
patients were involved in delineation, to erase the delineation memory of the
same patient, we separated the patients to two datasets, each with the same
number of patients, both two datasets with mixed real patients and synthetic
patients without overlaps (i.e., the ce-mri and vce-mri from the same patient
are not in the same dataset).when finished the first dataset delineation, there
was a one-month interval before the delineation of the second dataset."
3032,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"the average ji obtained from institution-1,
institution-2, and institution-3 dataset were similar with a result of 71.54%,
74.78% and 75.85%, respectively."
3033,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"for the institution-2 data, all synthetic patients
observed the same stages as real patients."
3034,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"we present a method for automatically aborting the scan if the predicted wed
deviates from real-time acquired data beyond the clinical limit."
3035,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"finally, we
propose a novel method to refine the prediction using real-time scan data."
3036,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"the training consists of a joint optimization
of the autodecoder and the latent vector: the autodecoder is learning a
realistic representation of the wed function while the latent vector is updated
to fit the data.during training, we initialize our latent space to a unit
gaussian distribution as we want it to be compact and continuous."
3037,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"additionally, since the encoder is trained
on a smaller data collection, it may not be able to perfectly project the depth
image to the wed manifold."
3038,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"to meet the strict safety criteria defined by the
iec, we propose to dynamically update the predicted wed profiles at inference
time using real-time scan data."
3039,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"as the
table moves and the patient gets scanned, ct data is being acquired and ground
truth wed can be computed for portion of the body that has been scanned, along
with the corresponding craniocaudal coordinate."
3040,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"we can then use this data to
optimize the latent vector by freezing the autodecoder and minimizing the l1
loss between the predicted and ground truth wed profiles through gradient
descent."
3041,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"we can then feed the updated latent vector to our autodecoder to
estimate the wed for the remaining portions of the body that have not yet been
scanned and repeat the process.in addition to improving the accuracy of the wed
profiles prediction, this approach can also help detect deviation from real
data."
3042,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"after the latent vector has been optimized to fit the previously scanned
data, a large deviation between the optimized prediction and the ground truth
profiles may indicate that our approach is not able to find a point in the
manifold that is close to the data."
3043,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"our ct scan dataset consists of 62, 420 patients from 16 different sites across
north america, asia and europe."
3044,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"our 3d camera dataset consists of 2, 742 pairs
of depth image and ct scan from 2, 742 patients from 6 different sites across
north america and europe acquired using a ceiling-mounted kinect 2 camera."
3045,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"we trained our autodecoder model on our unpaired ct scan dataset of 62, 420
patients with a latent vector of size 32."
3046,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"the encoder was trained on our paired
ct scan and depth image dataset of 2, 742 patients."
3047,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"our method finds common patterns of disease progression in datasets of
longitudinal images."
3048,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"clinicians
suspect that this is due to the grading system's reliance on static biomarkers
that are unable to capture temporal dynamics which contain critical information
for assessing progression risk.in their search for new biomarkers, clinicians
have annotated known biomarkers in longitudinal datasets that monitor patients
over time and mapped them against disease progression [2,16,19]."
3049,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"however, these
approaches neglect temporal relationships between images and the obtained
biomarkers are by definition static and cannot capture the dynamic nature of the
disease.our contribution: in this work, we present a method to automatically
propose biomarkers that capture temporal dynamics of disease progression in
longitudinal datasets (see fig."
3050,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,we use two retinal oct datasets curated in the scope of the pinnacle study [20].
3051,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"we first design and test our method on a development dataset, which was
collected from the southampton eye unit."
3052,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"afterwards, we test our method on a
second independent unseen dataset, which was obtained from moorfields eye
hospital."
3053,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"after strict quality control, the development
dataset consists of 46,496 scans of 6,236 eyes from 3,456 patients."
3054,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"the
unseen dataset is larger, containing 114,062 scans of 7,253 eyes from 3,819
patients."
3055,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"models were trained on the entire dataset for
120,000 steps using the adam optimiser with a momentum of 0.9, weight decay of
1.5 • 10 -6 and a learning rate of 5 • 10 -4 ."
3056,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"naively clustering whole time series of patients ignores two characteristics of
longitudinal data."
3057,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"initially, we tune the hyperparameters, λ, φ and k, on the development dataset
by heuristically selecting values that result in higher uniformity between
subtrajectories within each cluster."
3058,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"next, using the
same hyperparameters we apply the method directly to the unseen dataset."
3059,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"the
ophthalmologists then review these clusters and confirm whether they capture the
same temporal biomarkers observed in the development dataset.in addition to the
qualitative evaluation, we also validate the utility of our clusters as
biomarkers that stratify risk of disease progression."
3060,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"sub-trajectory clusters are candidate temporal biomarkers: by first applying our
method to the development dataset we found that using λ = 0.75, φ = 0.75 and k =
30 resulted in the most uniform and homogeneous clusters while still limiting
the total number of clusters to a reasonable amount."
3061,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"using the same
hyperparameters our method generalised to the unseen dataset which yielded
clusters with equivalent dynamics and quality (see fig."
3062,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"ophthalmologists
identified clusters capturing the same variants of temporal progression in both
datasets."
3063,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"we applied our method to two large longitudinal datasets, cataloguing 3,218
total years of disease progression."
3064,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"as late stage patients were overrepresented in our datasets, we
also intend to apply our method to datasets with greater numbers of patients
progressing from earlier disease stages."
3065,Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,database: we use publicly available tcga gbm-lgg dataset [6].
3066,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"moreover, the automatic delineation of the gtv in the esophagus poses a
significant difficulty, primarily attributable to the low contrast between the
esophageal gtv and the neighboring tissue, as well as the limited
datasets.recently, advances in deep learning [21] have promoted research in
automatic esophageal gtv segmentation from computed tomography (ct) [18,19]."
3067,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"meanwhile, an ideal method
for automatic esophageal gtv segmentation in the second course of rt should
consider three key aspects: 1) changes in tumor volume after the first course of
rt, 2) the proliferation of cancerous cells from a tumor to neighboring healthy
cells, and 3) the anatomical-dependent our training approach leverages
multi-center datasets containing relevant annotations, that challenges the
network to retrieve information from e1 using the features from e2."
3068,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"our training strategy leverages three datasets that introduce prior
knowledge to the network of the following three key aspects: 1) tumor volume
variation, 2) cancer cell proliferation, and 3) reliance of gtv on esophageal
anatomy.nature of gtv on esophageal locations."
3069,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"to achieve this, we efficiently
exploit knowledge from multi-center datasets that are not tailored for
second-course gtv segmentation."
3070,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"in this study, we use a paired first-second course gtv dataset
s p , an unpaired gtv dataset s v , and a public esophagus dataset s e .in order
to fully leverage both public and private datasets, the training objective
should not be specific to any tasks."
3071,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"1, our strategy is to challenge the network to retrieve information from
augmented inputs in e 1 using the features from e 2 , which can incorporate a
wide range of datasets that are not tailored for second-course gtv segmentation."
3072,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"to adequately
monitor changes in tumor volume and integrate information from the initial
course into the subsequent course, a paired first-second courses dataset s p =
{i 1 p , i 2 p , g 1 p ; g 2 p } is necessary for training."
3073,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"the paired dataset s p for the first and second courses is limited, whereas an
unpaired gtv dataset s v = {i v ; g v } can be easily obtained in a standard
clinical workflow with a substantial amount."
3074,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"the transformed data is feed into the encoders e 1/2 as shown
in the following equations:, p 1 (g e ), p 2 (i e ), p 2 (g e ), when i e , g e
∈ s e .(4)the esophageal tumor can proliferate with varying morphologies into
the surrounding tissues."
3075,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"to make full use of the datasets of relevant tasks, we incorporate a public
esophagus segmentation dataset, denoted as s e = {i e ; g e }, where i e /g e
represent the ct images and corresponding annotations of the esophagus
structure."
3076,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,by augmenting the data as described in eq.
3077,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"similarly, data from the paired s p is also augmented by p 1/2 to increase the
network's robustness.in summary, our training strategy is not dataset-specific
or target-specific, thus allowing the integration of prior knowledge from
multi-center esophageal gtv-related datasets, which effectively improves the
network's ability to retrieve information for the second course from the three
key aspects stated in sect."
3078,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,datasets.
3079,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"the paired first-second course dataset, s p , is collected from sun
yat-sen university cancer center (ethics approval number: b2023-107-01),
comprising paired ct scans of 69 distinct patients from south china."
3080,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"we
collected the gtv dataset s v from medmind technology co., ltd., which has ct
scans from 179 patients."
3081,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"to demonstrate the presence of a domain gap between the first and second
courses, we train sota methods with datasets s train p and s v , by feeding the
data sequentially into the network."
3082,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"notably, the paired first-second course dataset s test p pertains
to the same group of patients, thereby ensuring that any performance drop can be
attributed solely to differences in courses of rt, rather than variations across
different patients.figure 2 illustrates the reduction in the gtv area after the
initial course of rt, where the transverse plane is taken from the same location
relative to the vertebrae (yellow lines)."
3083,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,combination of various datasets.
3084,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"table 2 presents the information gain derived
from multi-center datasets using quantified metrics for segmentation
performance."
3085,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"our proposed training strategy fully exploits the datasets s p ,
s v , and s e , and further improve the dsc to 74.54% by utilizing comprehensive
knowledge of both the tumor morphology and esophageal
structures.region-preserving attention module."
3086,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"besides, to efficiently leverage prior
knowledge contained in various medical ct datasets, we train the network in an
information-querying manner."
3087,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"nevertheless, it is ineffective to directly transfer clip to medical tasks due
to the data covariate shift."
3088,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"experimental results on
lidc-idri [1] dataset demonstrate the effectiveness of learning with textual
knowledge for improving lung nodule malignancy prediction.the contributions of
this paper are summarized as follows.1) we propose clip-lung for lung nodule
malignancy prediction, which leverages clinical textual knowledge to enhance the
image encoder and classifier."
3089,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"for the attribute annotations, all the lung nodules in the lidc-idri dataset are
annotated with the same eight attributes: ""subtlety"", ""internal structure"",
""calcification"", ""sphericity"", ""margin"", ""lobulation"", ""spiculation"", and
""texture"" [4,8], and the annotated value for each attribute ranges from 1 to 5
except for ""calcification"" that is ranged from 1 to 6."
3090,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,dataset.
3091,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"lidc-idri [1] is a dataset for pulmonary nodule classification or
detection based on low-dose ct, which involves 1,010 patients."
3092,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"in this paper, we construct
three sub-datasets: lidc-a contains three classes of nodules both in training
and test sets; according to [11], we construct the lidc-b, which contains three
classes of nodules only in the training set, and the test set contains benign
and malignant nodules; lidc-c includes benign and malignant nodules both in
training and test sets.experimental settings."
3093,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"in this paper, we apply the clip
pre-trained vit-b/16 as the text encoder for clip-lung, and the image encoder we
used is resnet-18 [6] due to the relatively smaller scale of training data."
3094,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"in table 1, we compare the classification performances
on the lidc-a dataset, where we regard the benign-unsure-malignant we argue that
this is due to the indistinguishable textual annotations, such as similar
attributes of different nodules."
3095,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"in addition, we verify the effect of textual
branch of clip-lung using mv-dar [12] on lidc-a dataset."
3096,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"table 2
presents a performance comparison of clip-lung on the lidc-b and lidc-c
datasets."
3097,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"in table 3, we
verify the effectiveness of different loss components on the three constructed
datasets."
3098,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"additionally, we devise a domain
mixup strategy to resolve the problem of co-occurring artifacts in dermoscopic
images and mitigate the resulting noisy domain label assignments.our
contributions can be summarized as: (1) we resolve an artifacts-derived biasing
problem in skin cancer diagnosis using a novel environment-aware prompt
learning-based dg algorithm, epvt; (2) epvt takes advantage of a vitbased
domain-aware prompt learning and a novel domain prompt generator to improve
domain-specific and cross-domain knowledge learning simultaneously;(3) a domain
mixup strategy is devised to reduce the co-artifacts specific to dermoscopic
images; (4) extensive experiments on four out-of-distribution skin datasets and
six biased isic datasets demonstrate the outperforming generalization ability
and robustness of epvt under heterogeneous distribution shifts."
3099,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"in domain generalization (dg), the training dataset d train consists of m source
domains, denoted as d train = {d k |k = 1, ..., m }."
3100,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"we
train and validate all algorithms on isic2019 [6] dataset, following the split
of [3]."
3101,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"we evaluate models on four
out-of-distribution (ood) datasets, including derm7pt-dermoscopic [14],
derm7pt-clinical [14], ph2 [18], and pad-ufes-20 [21]."
3102,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"(2) trap set debiasing: we train
and test our epvt with its baseline on six trap sets [3] with increasing bias
levels, ranging from 0 (randomly split training and testing sets from the
isic2019 dataset) to 1 (the highest bias level where the correlation between
artifacts and class label is in the opposite direction in the dataset splits)."
3103,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"more details about these datasets and splits are provided in the complementary
material.implementation details: for a fair comparison, we train all models
using vit-base/16 [8] backbone pre-trained on imagenet and report the roc-auc
with five random seeds."
3104,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"we resize the input image to a size of 224 × 224 and
adopt the standard data augmentation like random flip, crop, rotation, and color
jitter."
3105,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"the results clearly demonstrate the superiority
of our approach, with the best performance on three out of four ood datasets and
remarkable improvements over the erm algorithm, especially achieving 4.1% and
8.9% improvement on the pad and ph2 datasets, respectively."
3106,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"although some
algorithms may perform similarly to our model on one of the four datasets, none
can consistently match the performance of our method across all four datasets."
3107,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"particularly, our approach showcases the highest average performance, with a
2.05% improvement over the second-best algorithm across all four datasets."
3108,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"when we
combine the adapter, the model's average performance improves by 1.37%, but it
performs worse than erm on pad dataset."
3109,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"the consistently better performance than the baseline on all four
datasets also highlights the importance of addressing coartifacts and
cross-domain learning for dg in skin lesion recognition.trap set debiasing: in
fig."
3110,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"3a, we present the performance of the erm baseline and our epvt on six
biased isic2019 datasets."
3111,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"notably, our epvt outperforms the erm baseline by 9.4% on the bias 1
dataset.prompt weights analysis: to verify whether our model has learned the
correct domain prompts for target domain prediction, we analyze and plot the
results in fig."
3112,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"firstly, we extract the features of each domain from
our training set and extract the feature from one target dataset,
derm7pt-clin.we then calculate the frechet distance [9] between each domain and
the target dataset using the extracted feature, representing the domain distance
between them."
3113,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"compared to other competitive domain generalization algorithms, our
method achieves outstanding results on three out of four ood datasets and the
second-best on the remaining one."
3114,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"most cad studies were developed on regular and selected
datasets in the laboratory environment, which avoided the problems (data noise,
missing data, etc.) in the clinical scenarios [3,6,9,13,18]."
3115,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"real-world studies have received increasing
attention [11,16], and it is challenging for the cad in the real-world scenarios
as: 1) consistent with the clinical workflow, cad needs to consider
multidisciplinary information to obtain multidimensional diagnosis; 2) due to
information collection, storage and manual evaluation, there are missing and
noisy medical data."
3116,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"in addition, none of the current multi-label cad studies have
considered the problem of missing labels and noisy labels.considering these
real-world challenges, we propose a multi-label model named self-feedback
transformer (sft), and validate our method on a realworld pnens dataset."
3117,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,real-world pnens dataset.
3118,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"we validated our method on a real-world pnens dataset
from two centers."
3119,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"the dataset contained 264 and 28 patients in center 1 and
center 2, and a senior radiologist annotated the bounding boxes for all 408 and
28 lesions."
3120,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"it is
obvious that the real-world dataset has a large number of labels with randomly
missing data, thus, we used an adjusted 5-fold cross-validation."
3121,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"taking a
patient as a sample, we chose the dataset from center 1 as the internal dataset,
of which the samples with most of the main labels were used as dataset 1 (219
lesions) and was split into 5 folds, and the remaining samples are randomly
divided into the training set dataset 2 (138 lesions) and the validation set
dataset 3 (51 lesions), the training set and the validation set of the
corresponding folds were added during cross-validation, respectively."
3122,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"details of each dataset are in
the supplementary material."
3123,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,dataset evaluation metrics.
3124,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"we evaluate the
performance of our method on the 10 main tasks for internal dataset, and due to
missing labels and too few sstr2 labels, only the performance of predicting rt,
pfs, os, gd, mtf are evaluated for external dataset."
3125,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"both sf and ea perform better than regular mode,
and emc outperforms other modes with a mauc of 0.72 (0.82 on external dataset)."
3126,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"when using 100 percent labels, the mauc of the internal
dataset decreased from 0.71 (noise ratio = 0.0) to 0.53 (noise ratio = 1.0), a
decrease of 0."
3127,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"we proposed a novel model sft for multi-label prediction on real-world pnens
data."
3128,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"the main contributions of our
work are as follows: (1) we propose a well-pruned simple but effective network
for breast tumor segmentation, which shows remarkable and solid performance on
large clinical dataset; (2) our large pretrained model is evaluated on two
additional public datasets without fine-tuning and shows extremely stabilized
improvement, indicating that our model has outstanding generalizability and good
robustness against multi-site data data."
3129,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"cn θcn (g) represents the probability for the
input of cn coming from the original dataset.in the implementation, we update
the segmentation network and all the discriminators alternatingly in each
iteration until both the generator and discriminators are converged."
3130,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"five-fold cross
validation is performed on the dataset in all experiments to verify our proposed
network."
3131,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"for external validation, we further test our model on two independent
publicly-available datasets collected by stu-hospital (dataset 1) [22] and
syu-university (dataset 2) [23]."
3132,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"the
comparison experiments are carried on a large-scale clinical breast ultrasound
dataset, and the quantitative results are reported in table 1."
3133,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"four groups of frameworks (stage i, stage ii, stage iii and stage iv)
are designed, with the numerals denoting the level of deep supervision counting
from the last deconvolutional layer.we test these four frameworks on the
in-house breast ultrasound dataset, and verify their segmentation performance
using the same five evaluation criteria."
3134,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"using a large clinical
dataset, our proposed model demonstrates not only state-of-the-art segmentation
performance, but also the outstanding generalizability to new ultrasound data
from different sites."
3135,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"therefore, automatic
instance segmentation of mitochondria is desired, since manually segmenting from
a large amount of data is particularly laborious and demanding."
3136,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"however,
mask r-cnn based approaches struggle due to undefined bounding-box scale in em
data volume.when designing a attention-based framework for 3d mitochondria
instance segmentation, a straightforward way is to compute joint spatio-temporal
selfattention where all pairwise interactions are modelled between all
spatiotemporal tokens."
3137,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"here,
we present the corresponding segmentation predictions of the baseline and our
approach along with the ground truth.our stt-unet approach achieves superior
segmentation performance by accurately segmenting 16% more cell instances in
these examples, compared to res-unet-r.segmentation performance on all three
datasets."
3138,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"set, stt-unet achieves ap-75 score of 0.842 and outperforms the recent 3d
res-unet [16] by 3.0%.figure 1 shows a qualitative comparison between our
stt-unet and 3d res-unet [16] on examples from mitoem-r and mitoem-h datasets."
3139,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"most recent approaches for 3d mitochondria instance segmentation utilize
convolution based designs within the ""u-shaped"" 3d encoder-decoder
architecture.in such an architecture, the encoder aims to generate a
low-dimensional representation of the 3d data by gradually performing the
downsampling of the extracted features."
3140,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"the other attention
such as [1,8,10,29,35] have demonstrated remarkable efficacy in effectively
managing volumetric data.inspired by vits [10,19] and based on the observation
that attention-based vision transformers architectures are an intuitive design
choice for modelling long-range global contextual relationships in volume data,
we investigate designing a cnntransformers based framework for the task of 3d
mitochondria instance segmentation.3 method we base our approach on the recent
res-unet [16], which utilizes encoderdecoder
structure of 3d unet [34] with skip-connections between encoder and decoder."
3141,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"we refer to [16] for more details.limitations: as discussed above, the
recent res-unet approach utilizes 3d convolutions to handle the volumetric input
data."
3142,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"dataset: we evaluate our approach on three datasets: mitoem-r [36], mitoem-h
[36] and lucchi [22]."
3143,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"the mitoem [36] is a dense mitochondria instance
segmentation dataset from isbi 2021 challenge."
3144,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"the dataset consists of 2 em
image volumes (30 μm 3 ) of resolution of 8 × 8 × 30 nm, from rat tissues
(mitoem-r) and human tissue (mitoem-h) samples, respectively."
3145,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"lucchi [22] is a sparse mitochondria semantic segmentation dataset with training
and test volume size of 165 × 1024 × 768.implementation details: we implement
our approach using pytorch1.9 [27] (rcom env) and models are trained using 2 amd
mi250x gpus."
3146,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"during training of mitoem, for the fair comparison, we adopt same
data augmentation technique from [36]."
3147,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"for fair comparison with previous works, we use the same
evaluation metrics as in the literature for both datasets."
3148,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"we use 3d ap-75
metric [36] for mitoem-r and mitoem-h datasets."
3149,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"the best
results are obtained with deformable-conv on both datasets."
3150,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"experiments on three datasets demonstrate the effectiveness of
our approach, leading to state-of-the-art segmentation performance."
3151,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"our method sets a new
state-of-the-art on this dataset in terms of both jaccard and dsc."
3152,Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,"however, the previous work on generating the b 1
-robust cest contrasts rely on valid target data and the underlying assumptions
to generate it, and can only create cest maps at one particular b 1 level.in
this work, we developed a conditional autoencoder (cae) [13] to generate b 1
-homogeneous cest-spectra at arbitrary b 1 levels, and a physics-informed
autoencoder (piae) to fit the 5-pool lorentzian model to the b 1 corrected
cest-spectra."
3153,Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,data measurements.
3154,Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,"[9] was used to acquire cest data on a 7t whole-body mri system
(magne-tom terra, siemens healthcare gmbh, erlangen, germany)."
3155,Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,"to further evaluate the
performance of piae and cae, we b1-corrected the data using cae and fitted it
with the least squares method (cae-lorentzian fit)."
3156,Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,"however, deepcest was trained on data fitted using a
conventional pipeline [9,14] which has suboptimal b 1 correction (cf."
3157,Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,"the optimal b 1 can often only be selected
at post-processing during the analysis of clinical data, as some clinically
important features appear better at certain b 1 levels (cf."
3158,Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,"4 also show the generalization capability of picae as it was trained
without the tumor data."
3159,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"a question
is raised naturally: can we expand the training dataset with a small proportion
of images labeled to reach or even exceed the segmentation performance of the
fully-supervised baseline?"
3160,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"intuitively, since the labeled images are samples
from the population of histopathology images, if the underlying distribution of
histopathology images is learned, one can generate infinite images and their
pixel-level labels to augment the original dataset."
3161,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"therefore, it is demanded to
develop a tool that is capable of learning distributions and generating new
paired samples for segmentation.generative adversarial network (gans)
[2,4,12,16,20] have been widely used in data augmentation [11,22,27,31]."
3162,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"as likelihood-based models,
diffusion models do not require adversarial training and outperform gans on the
diversity of generated images [3], which are naturally more suitable for data
augmentation."
3163,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"as far as our knowledge, we are the first to apply diffusion
models on histopathology image augmentation for nuclei segmentation.our
contributions are: (1) a diffusion-based data augmentation framework that can
generate histopathology images and their segmentation labels from scratch; (2)
an unconditional nuclei structure synthesis model and a conditional
histopathology image synthesis model; (3) experiments show that with our method,
by augmenting only 10% labeled training data, one can obtain segmentation
results comparable to the fully-supervised baseline."
3164,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"our goal is to augment a dataset containing a limited number of labeled images
with more samples to improve the segmentation performance."
3165,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"to maximize data likelihood, the diffusion model defines a
forward and a reverse process."
3166,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"specifically, p θ (x|y) is trained on paired data (x 0 , y 0 )
and p θ (x) can be trained by randomly discarding y (i.e."
3167,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,datasets.
3168,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,we conduct experiments on two datasets: monuseg [13] and kumar [14].
3169,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"the monuseg dataset has 44 labeled images of size 1000 × 1000, 30 for training
and 14 for testing."
3170,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"the kumar dataset consists of 30 1000×1000 labeled images
from seven organs of the cancer genome atlas (tcga) database."
3171,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"the dataset is
splited into 16 training images and 14 testing images."
3172,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"to validate the effectiveness of the proposed augmentation method, we create 4
subsets of each training dataset with 10%, 20%, 50% and 100% nuclei instance
labels."
3173,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"precisely, we first crop all images of each dataset into 256 × 256
patches with stride 128, then obtain the features of all patches with pretrained
resnet50 [6] and cluster the patches into 6 classes by kmeans."
3174,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"for the diffusion process of both steps, we set the total diffusion timestep
t to 1000 with a linear variance schedule {β 1 , ..., β t } following [8].for
monuseg dataset, we generate 512/512/512/1024 synthetic samples for
10%/20%/50%/100% labeled subsets; for kumar dataset, 256/256/256/512 synthetic
samples are generated for 10%/20%/50%/100% labeled subsets."
3175,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"third, the synthetic nuclei structures and images show great diversity:
the synthetic samples resemble different styles of the real ones but with
apparent differences.we then train segmentation models on the four labeled
subsets of monuseg and kumar dataset and corresponding augmented subsets with
both real and synthetic labeled images."
3176,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"for monuseg
dataset, it is clear that the segmentation metrics drop with fewer labeled
images."
3177,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"interestingly,
augmenting the full dataset also helps: dice increases by 1.3% and aji increases
by 1.6% compared with the original full dataset."
3178,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"for kumar dataset, by augmenting 10% labeled subset, aji
increases to a level comparable with that using 100% labeled images; by
augmenting 20% and 50% labeled subset, ajis exceed the fully-supervised
baseline."
3179,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"these results demonstrate the effectiveness of the proposed
augmentation method that we can achieve the same or higher level segmentation
performance of the fully-supervised baseline by augmenting a dataset with a
small amount of labeled images.generalization of the proposed data augmentation."
3180,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"for both monuseg and
kumar datasets, all the four labeling proportions metrics notably improve with
synthetic samples."
3181,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"in this paper, we propose a novel diffusion-based data augmentation method for
nuclei segmentation in histopathology images."
3182,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"by augmenting datasets
with a small amount of labeled images, we achieved even better segmentation
results than the fully-supervised baseline on some benchmarks."
3183,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"due to model complexity and limited training data, ml
performance often varies across data subgroups or domains, such as different
patient subpopulations or varied data acquisition scenarios."
3184,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"one solution to this issue is to apply a clustering
algorithm to the data, with the goal of identifying the unannotated subgroups."
3185,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"the main objective of unsupervised clustering is to group data points into
distinct classes of similar traits."
3186,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"however, due to the complexity and high
dimensionality of the medical imaging data and the resulting difficulty in
establishing a concrete notion of similarity, extracting low-dimensional
characteristics becomes the key to establishing the best criteria for grouping."
3187,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"deep unsupervised clustering
algorithms could map the medical imaging data back to their causal factors or
underlying domains, such as image acquisition equipment, patient subpopulations,
or other meaningful data subgroups."
3188,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"however, there is a practical need to be
able to guide the deep clustering model towards the identification of grouping
structures in a given dataset that have not been already annotated."
3189,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"to that end,
we propose a mechanism that is intended to constrain the model towards
identifying clusters in the data that are not associated with given variables of
choice (already known class labels or subgroup structures)."
3190,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"using a mixture-of-gaussians (mog) prior distribution
for the latent representations z, we examine subgroups or domains within the
dataset, revealed by the individual gaussians within the learned latent space,
and how z affects the generation of x."
3191,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"we denote the true data
distribution by p(z, x, c) and the variational posterior distribution by q(z,
c|x)."
3192,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"the decoder then translates this data representation in the form of (z, y)
to the input space (i.e., p(x|z, y))."
3193,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"given that the underlying vae architecture
seeks to efficiently compress the input data x into a learned representation,
this incentivizes the model to exclude information about y from the learned
variables z and c.the elbo of cdvade can be derived as follows,where we use the
fact that by the generative process of cdvade it holds that p(x, z, c|y) =
p(x|z, y)p(z|c, y)p(c|y) = p(x|z, y)p(z|c)p(c),and we adopt from vade the
assumption that q(z, c|x) = q(z|x)q(c|x) holds."
3194,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"a number of studies have been conducted with several approaches of deep
clustering for medical imaging data."
3195,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"the colored mnist is an extension to the classic mnist dataset [3], which
contains binary images of handwritten digits."
3196,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"this simple dataset can be used to investigate whether a given
clustering algorithm will categorize the images by color or by the digit label
and whether the proposed conditioning mechanism of cdvade can successfully guide
the clustering away from the categorization we want to avoid (e.g., condition
the model to avoid clustering by color, in order to distinguish the digits in an
unsupervised fashion)."
3197,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"2 a summary of the results for the
experiments on the colored mnist dataset is presented."
3198,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"notably, both vade and dec end up clustering
the data by color, as it is the most striking distinguishing characteristic of
these images."
3199,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"on the other hand, the predicted domains of cdvade have no
association with color, and the data are separated by the shapes in the images,
distinguishing some of the digit labels (albeit imperfectly)."
3200,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,her2 dataset.
3201,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"the dataset consists of 241
patches extracted from 64 digitized slides of breast cancer tissue which were
stained with her2 antibody."
3202,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"we use a subset of this dataset consisting of 672 images (the remainder is
held out for future research)."
3203,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"because the intended purpose is finding subgroups
in the given dataset only, a separate test set is not used."
3204,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"the dimensions of
the images vary from 600 to 826 pixels, and we scale all data to a uniform size
of 128 × 128 pixels before further processing."
3205,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"we refer to [4,8] for more
details about this dataset."
3206,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"this retrospective human subject dataset has been
made available to us by the authors of the prior studies [4,8], who are not
associated with this paper."
3207,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"appropriate ethical approval for the use of this
material in research has been obtained.deep clustering models applied to the
her2 dataset."
3208,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"we evaluate the performance and behavior of the dec, vade, and
cdvade models on the her2 dataset."
3209,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"we investigate whether the models will learn
to distinguish the her2 class labels, the scanner labels, or other potentially
meaningful data subgroups in a fully unsupervised fashion."
3210,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"to investigate the
clustering abilities of cdvade on the her2 dataset, we inject the her2 class
labels into the latent embedding space."
3211,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"we investigate the proposed cdvade model with the goal of identifying
meaningful data subgroups which are not associated with the already known her2
class labels."
3212,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"we investigated deep clustering models for the identification of meaningful
subgroups within medical image datasets."
3213,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"our experimental
findings on the her2 digital pathology dataset surmise that vade and dec are
capable of finding, in an unsupervised fashion, image subgroups related to the
her2 class labels, while cdvade (conditioned on the her2 labels) identifies
visually distinct subgroups that have a weaker association to the her2 labels."
3214,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"while cdvade can be used as an exploratory tool to unveil unknown subgroups in a
given dataset, developing specialized quantitative evaluation metrics for this
unsupervised task is inherently difficult and will also be a focus in our future
work."
3215,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"this paper constructed a bp dataset with the most commonly used bp
mris in our clinical practice."
3216,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"the major contributions of this study include 1) directed
triangle construction idea for tpp, 2) huge number of tpp matrices as the
heterogeneity representations of bp, 3) tppnet with 15 layers and huge number of
channels, 4) the bp dataset containing mr images and their corresponding roi
masks."
3217,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"the final dataset for radiomic analysis was
constructed by merging the datasets for each sequence type."
3218,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"only patients that
had all three sequences segmented (t2, t1 and post-gadolinium) were included in
the dataset."
3219,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,table 1 shows a breakdown of the final dataset.
3220,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"therefore,
data augmentation could be omitted when we combine tpp with deep learning for
this study."
3221,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"the whole dataset is divided into three subsets according to the mr
sequence, i.e."
3222,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"since all images in our dataset are 3d images, therefore, the initial channel is
set 325 which is equal to the tpp number."
3223,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"we evaluated our proposed tppnet by comparing it to the recent state-of-the-art
approaches over our bp dataset including vgg16 [28], inceptionnet [29],
mobilenet [30], glcm-cnn [13], vit [31]."
3224,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"to testify our proposed tppnet, a bp
dataset is constructed with 452 series including three most commonly used mr
sequences in clinical practice, i.e."
3225,CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,"we evaluate our circleformer on the
public monuseg dataset for nuclei detection in whole slide images."
3226,CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,monuseg dataset.
3227,CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,"monuseg dataset is a public dataset from the 2018 multi-organ
nuclei segmentation challenge [6]."
3228,CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,"since the maximum number of objects per image in the dataset is
close to 1000, we set the number of queries to 1000."
3229,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"the resulting datasets can consist of terabytes
of raw videos that require automatic methods for downstream tasks such as
classification, segmentation, and tracking of objects (e.g."
3230,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"concretely our contributions are: i) we
introduce the time arrow prediction pretext task to the domain of live-cell
microscopy and propose the tap pre-training scheme, which learns dense
representations (in contrast to only image-level representations) from raw,
unlabeled live-cell microscopy videos, ii) we propose a custom
(permutation-equivariant) time arrow prediction head that enables robust
training, iii) we show via attribution maps that the representations learned by
tap capture biologically relevant processes such as cell divisions, and finally
iv) we demonstrate that tap representations are beneficial for common
image-level and pixel-level downstream tasks in live-cell microscopy, especially
in the low training data regime."
3231,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"note that instead of creating image pairs from consecutive video frames we
can as well choose a custom time step δt ∈ n and sample x 1 ⊂ i t and x 2 ⊂ i
t+δt , which we empirically found to work better for datasets with high frame
rate."
3232,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"to demonstrate the utility of tap for a diverse set of specimen and microscopy
modalities we use the following four different datasets:hela."
3233,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"the dataset consists of four videos with overall 368 frames of
size 1100 × 700."
3234,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"the dataset consists of a single video with 1200
frames of size 1600×1200."
3235,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"the dataset consists of three videos with overall
410 frames of size 3900 × 1900.we use δt = 1.yeast."
3236,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"the dataset
consists of five videos with overall 600 frames of size 1024 × 1024."
3237,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"we use δt ∈
{1, 2, 3}.for each dataset we heuristically choose δt to roughly correspond to
the time scale of observable biological processes (i.e."
3238,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"we first study how well the time arrow prediction pretext task can be solved
depending on different image structures and used data augmentations."
3239,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"50% irrespective of the used augmentations, suggesting the absence of
predictive cues in the background for this dataset."
3240,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"when using
more data augmentations the accuracy decreases by roughly 12% points, suggesting
that data augmentation is key to avoid overfitting on confounding cues.next we
investigate which regions in full-sized videos are most discriminative for tap."
3241,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"3 we show example
attribution maps on top of single raw frames for three different datasets."
3242,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"we next investigate whether the learned tap representations are useful for
common supervised downstream tasks, where we especially focus on their utility
in the low training data regime."
3243,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"to that end, we generate a dataset of 97k crops
of size 2 × 96 × 96 from flywing and label them as mitotic/nonmitotic (16k/81k)
based on available tracking data [20]."
3244,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"we use the same dataset as for flywing mitosis
classification, but now densely label post-mitotic cells."
3245,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"training a
u-net on fixed tap representations always outperforms the baseline, and when
only using 3% of the training data it reaches similar performance as the
baseline trained on all available labels (0.67 vs."
3246,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"interestingly, fine-tuning tap only slightly outperforms the supervised baseline
for this task even for moderate amounts of training data, suggesting that fixed
tap representations generalize better for limited-size datasets.emerging bud
detection on yeast: finally, we test tap on the challenging task of segmenting
emerging buds in phase contrast images of yeast colonies."
3247,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"we train tap networks
on yeast and generate a dataset of 1205 crops of size 5 × 192 × 192 where we
densely label yeast buds in the central frame (defined as buds that appeared
less than 13 frames ago) based on available segmentation data [17]."
3248,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"surprisingly,
training with fixed tap representations performs slightly worse than the
baseline for this dataset (fig."
3249,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"0.39 for 120 frames)
across the full training data regime, yielding already with 15% labels the same
f1 score as the baseline using all labels."
3250,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"furthermore, we
demonstrate on a variety of datasets that the learned features can substantially
reduce the required amount of annotations for downstream tasks."
3251,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"although in this
work we focus on 2d+t image sequences, the principle of tap should generalize to
3d+t datasets, for which dense ground truth creation is often prohibitively
expensive and therefore the benefits of modern deep learning are not fully
tapped into."
3252,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"more recently, self-supervised learning (ssl), using a
large amount of unlabeled histopathology data, has become quite popular for wsi
classification [5,13] as it outperforms imagenet feature encoders.most existing
mil methods do not fine-tune their feature extractor together with their
classification task; this stems from the requirement for far larger gpu memory
than is available currently due to the gigapixel nature of wsis, e.g."
3253,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"for example, on the bright dataset [2], the accuracy drops more
than 5% compared to the conventional mil approaches."
3254,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"the poor performance is
probably caused by the large network over-fitted to the limited downstream
training data, leading to suboptimal feature representation."
3255,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"indeed, especially
for weakly supervised wsi classification, where annotated data for downstream
tasks is significantly less compared to natural image datasets, conventional
finetuning schemes can prove to be quite challenging.to address the subpar
performance of ssl-pretrained vision transformers, we utilize the prompt tuning
techniques."
3256,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"this approach is parameter efficient [12,15] and has been
shown to better inject task-specific information and reduce the overfitting in
downstream tasks, particularly in limited data scenarios [8,23]."
3257,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"prompt tuning
performs well even when only limited labeled data is available for training,
making it particularly attractive in computational pathology."
3258,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"this avoids
potential overfitting while still injecting task-specific knowledge into the
learned representations.extensive experiments on three public wsi datasets,
tcga-brca, tcga-crc, and bright demonstrate the superiority of prompt-mil over
conventional mil methods, achieving a relative improvement of 1.49%-4.03% in
accuracy and 0.25%-8.97% in auroc by using only less than 0.3% additional
parameters."
3259,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"we assessed prompt-mil using three histopathological wsi datasets: tcga-brca
[14], tcga-crc [19], and bright [2]."
3260,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"these datasets were utilized for both the
self-supervised feature extractor pretraining and the end-to-end finetuning
(with or without prompts), including the mil component."
3261,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"note that the testing
data were not used in the ssl pretraining."
3262,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"following the common 4-fold data split [1,16], we used the first three folds for
training (236 gs, 89 cin), and the fourth for testing (77 gs, 28 cin)."
3263,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"we
further split 20% (65 slides) training data as a validation set."
3264,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"we pretrained separate vit models on the tcga-crc datasets for 50
epochs, on the bright dataset for 50 epochs, and on the brca dataset for 30
epochs."
3265,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,this training strategy is optimized using the validation datasets.
3266,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"compared to the full fine-tuning method, our method achieved
a relative improvement of 1.29% to 13.61% in accuracy and 3.22% to 27.18% in
auroc on the three datasets."
3267,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"we evaluated the training speed and memory consumption
of our method and compared to the full fine-tuning baseline on four different
sized wsis in the bright dataset."
3268,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"foundational models refer to those trained on large-scale
pathology datasets (e.g."
3269,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,the entire tcga pan-cancer dataset [28]).
3270,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"table 4 shows the accuracy and auroc of our prompt-mil model
with 1, 2 and 3 trainable prompt tokens (k = 1, 2, 3) on the tcga-brca and the
bright datasets."
3271,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"on the tcga-brca dataset, our prompt-mil model with 1 to 3
prompt tokens reported similar performance."
3272,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"on the bright dataset, the
performance of our model dropped with the increased number of prompt tokens."
3273,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"we applied our proposed method to three
publicly available datasets."
3274,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"however, the transformer framework has a
relatively large number of parameters, which could cause high costs in
fine-tuning the whole model on large datasets."
3275,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"in
the prompt-tuning phase, grouping prompts are added to the input of the backbone
and gtc, while the backbone parameters are frozen.3 experiments and results
consep 1 [10] is a colorectal nuclear dataset with three types, consisting of 41
h&e stained image tiles from 16 colorectal adenocarcinoma whole-slide images
(wsis)."
3276,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"we split them following the official partition [1,10].is a breast cancer
dataset with three types and consists of 120 image tiles from 113 patients."
3277,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"we follow the work [1] to apply the slic [2] algorithm to generate
superpixels as instances and split them into 80/10/30 slides for
training/validation/testing.lizard 3 [9] has 291 histology images of colon
tissue from six datasets, containing nearly half a million labeled nuclei in h&e
stained colon tissue."
3278,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"random crop,
flipping, and scaling are used for data augmentation."
3279,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"our method is trained with
pytorch on a 48 gb gpu (nvidia a100) for 12-24 h (depending on the dataset
size)."
3280,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"as shown in table 1, our method exceeds
all 1 https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/."
3281,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"2
https://github.com/topoxlab/dataset-brca-m2c/."
3282,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"specifically, on the
consep dataset, our approach achieves 1.6% higher f-score on the detection (f d
) and 1.8% higher f-score on the classification (f c ) than the second best
methods mcspatnet [1] and upernet [22]."
3283,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"on brca-m2c dataset, our method has 0.5%
higher f d and 3.9% higher f c , compared with the second best models mcspatnet
[1] and dab-detr [19]."
3284,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"besides, on lizard dataset, our method outperforms
upernet [22] by more than 1.5% and 6.4% on f d and f c , respectively."
3285,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"meanwhile, we conduct t-test on consep dataset for statistical significance
test."
3286,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"the strengths of the grouping transformer based classifier and the grouping
prompts are verified on consep dataset, as shown in table 2.prompt-based
grouping transformer (pgt) is our proposed detection and classification
architecture with grouping prompts and the gtc (in fig."
3287,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"table 3 shows the effect of different numbers of grouping
prompts on consep dataset."
3288,Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,"but these methods are developed using dmri data acquired with a fixed
echo time (te)."
3289,Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,"me estimation is also a
standard approach for high-resolution power spectral estimation of time series
data which involves a similar trigonometric moment problem [7,21]."
3290,Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,"the performance of
these methods is compared with results based on basis functions using
simulations and in vivo data."
3291,Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,the rdd functions that satisfy the rdmri data may not be unique.
3292,Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,"based on
the three representations of rdmri data in eqs."
3293,Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,"the code
and data used in this work are available at
https://github.com/lipengning/me-rdd."
3294,Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,"the proposed algorithms were examined using synthetic rdmri data with an rdd
function consisting of three gaussian components with the mean at (1.5 µm 2 /ms,
10 ms -1 ), (0.5 µm 2 /ms, 40 ms -1 ), and (1.5 µm 2 /ms, 40 ms -1 ) with the
volume fraction being 0.2, 0.5 and 0.3, respectively."
3295,Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,"the proposed me algorithms were applied to an in vivo rdmri dataset acquired in
our previous work [16]."
3296,Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,"the data was acquired from a healthy volunteer on a 3t
siemens prisma scanner with the following parameters: voxel size = 2.5×2.5×
2.5mm 3 , matrix size = 96×96×54, te = 71, 101, 131, 161 and 191 ms, tr=5.9 s, b
= 700, 1400, 2100, 2800, 3500 s/mm 2 along 30 gradient directions together with
6 volumes at b = 0, simultaneous multi-slice (sms) factor = 2, and ipat = 2."
3297,Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,"then the data were further processed using the
unring [22] tool."
3298,Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,"moreover,
results based on in vivo data have shown that the proposed me-rdd can resolve
multiple components that cannot be distinguished by the basis function approach."
3299,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,"acquiring such a dataset requires
modification of the standard imaging protocol and involves additional training
of the mr technicians."
3300,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,"the performance of these dl models heavily depend on the
availability of high quality data."
3301,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,"there is a dearth of datadriven approaches to
mri dose-simulation given the lack of diverse ground truth data of the different
dose levels."
3302,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,"to this effect, we introduce a vision transformer based dl model1
that can synthesize brain 2 mri images that correspond to arbitrary dose levels,
by training on a highly imbalanced dataset with only t1w pre-contrast, t1w 10%
low-dose, and t1w ce standard dose images."
3303,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,"iterative learning design: dl based models tend to perform poorly when the
training data is highly imbalanced [11]."
3304,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,"furthermore, the problem of arbitrary
dose simulation requires the interpolation of intermediate dose-levels using a
minimum number of data points."
3305,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,"we
first utilize this design paradigm for the dose simulation task and train an
end-to-end model on a highly imbalanced dataset where only t1w pre-contrast, t1w
low-dose, and t1w post-contrast are available.as shown in fig."
3306,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,"1 rotational shift: image rotation has been widely used as a data augmentation
technique in preprocessing and model training."
3307,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,"dataset: with irb approval and informed consent, we retrospectively used 126
clinical cases (113 training, 13 testing) from a internal private dataset3 using
gadoterate meglumine contrast agent (site a)."
3308,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,for this analysis we used the data from site b.
3309,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"however, multiple
challenges exist when using streamline data as deep network input."
3310,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"finally, computational cost can pose a challenge for
the parcellation of large tractography datasets that can include thousands of
subjects with millions of streamlines per subject.in this work, we propose a
novel point-cloud-based strategy that leverages neighboring and whole-brain
streamline information to learn local-global streamline representations."
3311,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"avoiding
image registration can also reduce computational time and cost when processing
very large tractography datasets with thousands of subjects."
3312,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"while other
registration-free tractography parcellation techniques require freesurfer input
[29] or work with rigidly mni-aligned human connectome project data [19], our
method can directly parcellate tractography in individual subject space.in this
study, we propose tractcloud, a registration-free tractography parcellation
framework, as illustrated in fig."
3313,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"second, we leverage a
training strategy using synthetic transformations of labeled tractography data
to enable registration-free parcellation at the inference stage."
3314,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"we utilized a high-quality and large-scale dataset of 1 million labeled
streamlines for model training and validation."
3315,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"the dataset was obtained from a
wm tractography atlas [42] that was curated and annotated by a neuroanatomist."
3316,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"the training data includes 43 tract
classes: 42 anatomically meaningful tracts from the whole brain and one tract
category of ""other streamlines,"" including, most importantly, anatomically
implausible outlier streamlines."
3317,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"on average, the 42 anatomical tracts have 2539
streamlines with a standard deviation of 2693 streamlines.for evaluation, we
used a total of 120 subjects from four public datasets and one private dataset."
3318,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"these five datasets were independently acquired with different imaging protocols
across ages and health conditions."
3319,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"the
twotensor unscented kalman filter (ukf) [22,25,27] method, which is consistent
across ages, health conditions, and image acquisitions [42], was utilized to
create whole-brain tractography for all subjects across the datasets mentioned
above."
3320,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,synthetic transform data augmentation.
3321,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"to enable tractography parcellation
without registration, we augmented the training data by applying synthetic
transform-based augmentation (sta) including rotation, scaling, and
translations."
3322,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"in detail, we applied 30 random transformations
to each subject tractography in the training dataset to obtain 3000 transformed
subjects and 30 million streamlines."
3323,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"many methods are capable of
tractography parcellation after affine registration [12,42]; therefore, with sta
applied to the training dataset, our framework has the potential for
registration-free parcellation.module for local-global streamline representation
learning."
3324,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"training of
our registrationfree framework (tractcloud reg-free ) with the large sta dataset
took about 22 h and 10.9 gb gpu memory with pytorch (v1.13) on an nvidia rtx
a5000 3 experiments and results we evaluated our method on the original labeled
training dataset (registered and
aligned) and its synthetic transform augmented (sta) data (unregistered and
unaligned)."
3325,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"we divided both the original and sta data into train/validation/test
sets with the distribution of 70%/10%/20% by subjects (such that all streamlines
from an individual subject were placed into only one set, either train or
validation or test)."
3326,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"table 1 shows that the tractcloud framework achieves the best
performance on data with and without synthetic transformations (sta)."
3327,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"especially
on sta data, tractcloud yields a large improvement in accuracy (up to 9.9%) and
f1 (up to 13.8%), compared to pointnet and dgcnn baselines as well as sota
methods."
3328,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"we performed experiments on five independently acquired, unlabeled testing
datasets (dhcp, abcd, hcp, ppmi, btp) to evaluate the robustness and
generalization ability of our tractcloud reg-free framework on unseen and
unregistered data."
3329,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"furthermore, we also provide a visualization of identified tracts in an
example individual subject for every dataset across methods (fig."
3330,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"2).as shown in
table 2, all methods achieve high tirs on all datasets, and the tir metric does
not have significant differences across methods."
3331,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"however, our registration-free
framework (tractcloud reg-free ) obtains significantly lower tda values (better
quality of identified tracts) than all compared methods on abcd, hcp, and ppmi
datasets, where ages of test subjects are from 9 to 75 years old."
3332,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"on the very
challenging dhcp (baby brain) dataset, tractcloud reg-free still significantly
outperforms two sota methods."
3333,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"results of tract identification rate (tir) and tract distance to atlas (tda) on
five independently acquired testing datasets as well as computation time on a
randomly selected subject."
3334,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"the tract spatial overlap (wdice) is over 0.965
on all datasets, except for the challenging dhcp (wdice is 0.932) (table 3)."
3335,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,all methods can successfully identify these tracts across datasets.
3336,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"it
is visually apparent that the tractcloud reg-free framework obtains results with
fewer outlier streamlines, especially on the challenging dhcp dataset."
3337,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"the fast inference speed and robust ability to
parcellate data in original subject space will allow tractcloud to be useful for
analysis of large-scale tractography datasets."
3338,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"future work can investigate
additional data augmentation using local deformations to potentially increase
robustness to pathology."
3339,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"in the challenging btp (tumor patients) dataset, tractcloud reg-free
obtains significantly lower tda values than sota methods and comparable
performance to tractcloud regist ."
3340,Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,"however,
existing models are typically constrained to data acquired with a single te
(ste) and do not account for compartment-specific t 2 relaxation."
3341,Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,"several
studies have shown that multi-te (mte) data can account better for intravoxel
architectures and fiber orientation distribution functions (fodfs)
[1,6,16,17,19].here, we propose a unified strategy to estimate using mte
diffusion data (i) compartment specific t 2 relaxation times; (ii) non-t 2
-weighted (non-t 2 w) parameters of multi-scale microstructure; and (iii) non-t
2 w multi-scale fodfs."
3342,Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,"we evaluate rdsi using both ex vivo
monkey and in vivo human brain mte data, acquired with fixed diffusion times
across multiple b-values."
3343,Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,"the apparent relaxation rates at different b-values, r(b) = 1/t 2 (b), can be
estimated using single-shell data acquired with two or more tes [14]."
3344,Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,ex vivo data.
3345,Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,"using mte data, we
demonstrated that rdsi can delineate heterogeneous tissue microstructure elusive
to ste data."
3346,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"however, their satisfactory performance
depends on the appropriate annotated source dataset."
3347,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"to evaluate the effectiveness of psm, we evaluated our method on
two datasets."
3348,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,dataset.
3349,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"we validated the proposed method on the public dataset of multi-organ
nuclei segmentation (monuseg) [13] and breast tumor cell dataset (bcdata) [11]."
3350,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"bcdata is a public large-scale breast tumor dataset
containing 1338 immunohistochemically ki-67 stained images of size 640 × 640."
3351,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"in monuseg dataset, four fully-supervised methods unet [20], medt
[24], cdnet [8], and the competition winner [13] are adopted to estimate the
upper limit as shown in the first four rows of table 1."
3352,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"we can see that relying on the pretrained models
with external data can not improve the results of subsequent segmentation."
3353,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"unlike
natural image datasets containing diverse samples, the minor inter class
differences in biomedical images may not fully exploit the superiority of
contrastive learning."
3354,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"our unsupervised method
was evaluated on two publicly available datasets and obtained competitive
results compared to the methods with annotations."
3355,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"many
learning-based methods have been developed for automated mri analysis and brain
disorder prognosis, which usually heavily rely on labeled training data
[7][8][9][10]."
3356,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"considering that there are a large number of unlabeled mris in
existing large-scale datasets [12,13], several deep learning methods propose to
extract brain anatomical features from mri without requiring specific category
labels."
3357,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,data and preprocessing.
3358,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"while it
is often challenging to annotate mris in practice, there are a large number of
mris (without task-specific category labels) in existing large-scale datasets."
3359,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"to learn
brain anatomical features from mris in a data-driven manner, we propose to
employ a segmentation task for pretext model training."
3360,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"the
training data is duplicated and augmented using random affine transform."
3361,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"this implies
that the brain anatomical mri features learned by our pretext model on
large-scale datasets would be more discriminative, compared with those used in
the competing methods."
3362,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"second, among 10 deep models, our bar produces the lowest
standard deviation in most cases (especially on sen and spe), suggesting its
robustness to bias introduced by random data partition in the downstream task."
3363,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"cnd classification is more
challenging, which could be due to the more imbalanced training data in this
task (as shown in table sii of supplementary materials)."
3364,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"these results imply that data
imbalance may be an important issue affecting the performance of deep learning
models when the number of training samples is limited.segmentation results."
3365,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"even
for the lld study with significant inter-site data heterogeneity, the boundary
of wm and gm produced by bar is more continuous and smoother, which is in line
with the brain anatomy prior."
3366,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"specifically, the bar-b is trained
from scratch as a baseline on target data without any pre-trained encoder."
3367,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"also, bar and bar-r outperform bar-b in most cases,
implying that brain anatomy prior derived from tissue segmentation or mri
reconstruction can help improve discriminative ability of mri features and boost
prediction performance.influence of training data size."
3368,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"we also study the
influence of training data size on bar in cnd vs."
3369,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"with
fixed test data, we randomly select a part of mris (i.e., [20%, 40%, • • • ,
100%]) from target training data to fine-tune the downstream prediction model."
3370,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"3(b) that the overall performance in terms of auc
and acc of our bar increases with the increase of training data, and it produces
the best results when using all training data for model fine-tuning."
3371,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"this
suggests that using more data for downstream model fine-tuning helps promote
learning performance."
3372,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"there is significant intra-and inter-site data heterogeneity in lld
with two sites."
3373,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"it can be used to generate tractography data consisting of millions of synthetic
fibers or streamlines for a single subject stored in a tractogram that
approximate groups of biological axons [1]."
3374,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"those are trained using various features, either directly from
diffusion data in voxel space or from tractography data."
3375,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"additionally, supervised techniques are
restricted to fixed sets of predetermined tracts and are trained on substantial
volumes of hard-to-generate pre-annotated reference data.manual methods are
still frequently used for all cases not yet covered by automatic methods, such
as certain populations like children, animal species, new acquisition schemes or
special tracts of interests."
3376,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"clustering approaches were developed to reduce complexity of
large amounts of streamlines in the input data [4,6]."
3377,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"the method is implemented as the tool attractive in
mitk diffusion1 , enabling researchers to quickly and intuitively segment tracts
in pathological datasets or other situations not covered by automatic
techniques, simply by annotating a few but informative streamlines."
3378,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"to create a segmentation of a white matter tract from an individual wholebrain
tractogram t , streamlines which not belong to this tract must be excluded from
the tractography data."
3379,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"1 for a brief summary of the nomenclature of this work)to perform the
classification, supervised models have been trained on various features
representing the data."
3380,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"we choose the dissimilarity representation proposed by
olivetti to classify streamlines, which has shown well performance and can be
computed quickly for arbitrary data [3,13]."
3381,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"commonly, for training classifiers, large amounts of annotated and potentially
redundant data are used, leading to high annotation efforts and long training
times."
3382,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"active learning reduces both by training machine learning models with
only small and iteratively updated labeled subsets of the originally unlabeled
data."
3383,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"furthermore, the class probabilities p(s) determined by the random
forest are used to estimate its uncertainty with respect to each sample by
calculating the entropy enext, a subset s emax of streamlines with the highest
entropy or uncertainty is selected to be labeled by the expert and is added to
the training data (fig."
3384,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"the proposed technique was tested on a healthy-subject dataset and on a dataset
containing tumor cases."
3385,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"we focused on the left optic radiation (or), the left
cortico-spinal tract (cst), and the left arcuate-fasciculus (af), representing a
variety of established tracts.to test the proposed method on pathological data,
we used an in-house dataset containing ten presurgical scans of patients with
brain tumors."
3386,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"manual
segmentation experiments using an interactive prototype of attractive were
initiated on the tumor data (holistic evaluation)."
3387,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"additionally, reproducible
simulations on the freely available hcp and the internal tumor dataset were
created (algorithmic evaluation)."
3388,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"the code used for these
experiments is publicly available2 .for the algorithmic evaluation, the initial
training dataset was created with 20 randomly selected streamlines from the
whole-brain tractogram, which have been shown to be a decent number to start
training."
3389,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"since some tracts contain only a fraction of streamlines from the
entire tractogram, it might be unlikely that the training dataset will contain
any streamline belonging to the target tract."
3390,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"therefore, two streamlines of the
specific tract were further added to the training dataset, and class weights
were used to compensate for the class unbalance."
3391,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"in each iteration, the ten streamlines with the
highest entropy are added to the training dataset, which has been determined to
be a good trade-off between annotation effort and prediction improvement."
3392,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"the
process was terminated after 20 iterations, increasing the size of the training
data from 22 to 222 out of one million streamlines.the holistic evaluation was
conducted with equal settings, except that the workflow was terminated when the
prediction matched the expectation of the expert."
3393,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"to ensure that the initial
dataset s rand contained streamlines from the target tract, the expert initiated
the active learning workflow by defining a small roi that included fibers of the
tract."
3394,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"to allow comparison between the proposed and traditional
roi-based techniques, the or of subjects from the tumor dataset were segmented
using both approaches by an expert familiar with the respective tool, and the
time required was reported to measure efficiency.note, in all experiments, the
classifier is trained from scratch every iteration, prototypes are generated for
each subject individually, and the classifier predicts on data from the same
subject it is trained with, as it performs subject-individual tract segmentation
and is not used as a fully automated method."
3395,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"to ensure a stable active learning
setup that generalizes across different datasets, the whole method was developed
on the hcp and applied with fixed settings to the tumor data [10]."
3396,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"in table 1, the dice score of the active learning simulation on the hcp and
tumor data after the fifth, tenth, and twentieth iterations are shown and
compared with outcomes of classifyber and tractseg."
3397,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"results for the hcp data
were already on par with the benchmark of automatic methods between the fifth
and tenth iterations."
3398,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"on the tumor data, the performance of the proposed method
remains above 0.7 while the performance of tractseg drops substantially."
3399,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"figure 3 depicts the quantitative gain of active learning on the three
tracts of the hcp data and compares it to pure random sampling by displaying the
dice score depending on annotated streamlines."
3400,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"qualitative results of the algorithmic evaluation of the af of a randomly chosen
subject of the hcp dataset are shown in fig."
3401,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"initially, the randomly
sampled streamlines in the training data are distributed throughout the brain,
while entropy-based selected streamlines from subsequent iterations cluster
around the af."
3402,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"when accessing qualitative results of the pathological dataset
visual inspection revealed particularly poorly performance of tractseg in cases
where or fibers were in close proximity to tumor tissue, leading to fragmented
segmentations, while complete segmentations were reached with active learning
even for these challenging tracts after a few iterations, as shown in fig."
3403,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"the algorithmic evaluation
yielded consistent results from the fifth to the tenth iterations on both the
hcp and tumor datasets."
3404,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"as expected, outcomes obtained from the tumor dataset
were not quite as good as those of the hcp dataset."
3405,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"this trend is generally
observed in clinical datasets, which tend to exhibit lower performance levels
compared to high-quality datasets, which could be responsible for the decline in
the results."
3406,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"for selected
scenarios, the ability of the classifier to generalize by learning from
previously annotated subjects will be investigated, which may even allow to
train a fully automatic classifier for new tracts once enough data is annotated."
3407,B-Cos Aligned Transformers Learn Human-Interpretable Features,"• we
extensively evaluate both models on three public datasets: nct-crc-he-100k [18],
tcga-coad-20x [19], munich-aml-morphology [25]."
3408,B-Cos Aligned Transformers Learn Human-Interpretable Features,"for example, it has been shown that some saliency maps are independent of both
the data on which the model was trained and the model parameters [2]."
3409,B-Cos Aligned Transformers Learn Human-Interpretable Features,"we classify image patches from the public
colorectal cancer dataset nct-crc-he-100k [18]."
3410,B-Cos Aligned Transformers Learn Human-Interpretable Features,this dataset is highly unbalanced and not color normalized compared fig.
3411,B-Cos Aligned Transformers Learn Human-Interpretable Features,to the first dataset.
3412,B-Cos Aligned Transformers Learn Human-Interpretable Features,"additionally, we demonstrate that the
b-cos vision transformer is adaptable to domains beyond histopathology by
training the model on the single white blood cell dataset munich-aml-morphology
[25], which is also highly unbalanced and also publicly available.domain-expert
evaluation: our primary objective is to develop an extension of the vision
transformer that is more transparent and trusted by medical professionals."
3413,B-Cos Aligned Transformers Learn Human-Interpretable Features,"additional details on training,
optimization, and datasets can be found in the appendix."
3414,B-Cos Aligned Transformers Learn Human-Interpretable Features,"a third expert
points out that vit might overfit certain patterns in this dataset, which could
aid the model in improving its performance."
3415,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"to the best
of our knowledge, our work is the first to enable subject-specific
multi-contrast super-resolution from low-resolution scans without needing any
high-resolution training data."
3416,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"we extensively evaluate our method on multiple
brain mri datasets and show that it achieves high visual quality for different
contrasts and views and preserves pathological details, highlighting its
potential clinical usage.related work."
3417,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"however, all current mcsr approaches are limited by their need
for a large training dataset."
3418,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"consequently, this constrains their usage to
specific resolutions and further harbors the danger of hallucination of features
(e.g., lesions, artifacts) present in the training set and does not generalize
well to unseen data.originating from shape reconstruction [18] and multi-view
scene representations [17], implicit neural representations (inr) have achieved
state-of-the-art results by modeling a continuous function on a space from
discrete measurements."
3419,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"we empirically show that mutual information (mi) [26] is a good
candidate to capture such an equilibrium point without the need for ground truth
data in its computation."
3420,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,datasets.
3421,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"we
conduct experiments on two public datasets, brats [16], and msseg [4], and an
in-house clinical ms dataset (cms)."
3422,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"in each dataset, we select 25 patients that
fulfill the isotropic acquisition criteria for both ground truth hr scans."
3423,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"to the best of our knowledge, there are no prior data-driven methods
that can perform mcsr on a single-subject basis."
3424,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"for mcsr from single-subject scans, we achieve
encouraging results across all metrics for all datasets, contrasts, and views."
3425,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"lastly, given their similar physical acquisition
and lesion sensitivity, we note that dir/flair benefit to the same degree in the
cms dataset.qualitative analysis."
3426,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"figure 2 shows the typical behavior of our
models on cms dataset, where one can qualitatively observe that the split-head
inr pre-serves the lesions and anatomical structures shown in the yellow boxes,
which other models fail to capture."
3427,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"given the supervision of only single subject data and trained
within minutes on a single gpu, we believe our framework to be potentially
suited for broad clinical applications."
3428,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"future research will focus on
prospectively acquired data, including other anatomies."
3429,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"although these factors, particularly molecular
information, have usually proved to be strong predictors of survival in gbm,
there remain substantial challenges and unmet clinical needs to exploit easily
accessible, noninvasive neuroimaging data acquired preoperatively to predict
overall survival time of gbm patients, which can benefit treatment planning.to
do so, magnetic resonance imaging (mri) and its derived radiomics have been
widely used to study gbm preoperative prognosis over the last few decades."
3430,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"second, resting-state fmri data are not routinely collected for gbm clinical
practices, which restricts the size of annotated datasets such that it is
infeasible to train a reliable prediction model based on deep learning for
survival prediction."
3431,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"similar to data augmentation schemes, we can artificially
boost data volume (i.e., fln maps) up to m times through producing m fln maps
for each patient in the a-lnm, which helps to mitigate the risk of over-fitting
and improve the performance of overall survival time prediction when learning a
deep neural network from a small sized dataset.for this reason, we propose the
name ""augmented lnm (a-lnm)"", compared to the traditional lnm where only one fln
map is generated per patient by averaging all the n fdc maps."
3432,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"to evaluate the predictive
power of the fln maps generated by our a-lnm, we conduct extensive experiments
on 235 gbm patients in the training dataset of brats 2020 [18] to classify the
patients into three overall survival time groups viz."
3433,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"it publicly released preprocessed
restingstate fmri data of 1000 healthy right-handed subjects with an average age
21.5 ± 2.9 years and approximately equal numbers of males and females from the
brain genomics superstruct project (gsp) [5], where the concrete image
acquisition parameters and preprocessing procedures can be found as well."
3434,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"it provided an open-access pre-operative imaging training dataset to
segment brain tumors of glioblastoma (gbm, belonging to high grade glioma) and
low grade glioma (lgg) patients, as well as to predict overall survival time of
gbm patients [18]."
3435,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"this training dataset contained 133 lgg and 236 gbm patients,
and each patient had four mri modalities, including t1, post-contrast
t1-weighted, t2-weighted, and t2 fluid attenuated inversion recovery."
3436,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"in this paper, we propose to investigate the feasibility of the novel
neuroimaging features, i.e., fln maps, for overall survival time prediction of
gbm patients in the training dataset of the brats 2020, in which one patient
alive was excluded, and the remaining 235 patients consisted of 89 short-term
survivors (less than 10 months), 59 mid-term survivors (between 10 and 15
months), and 87 long-term survivors (more than 15 months)."
3437,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"one can clearly see that
similar to data augmentation schemes, we artificially boost the number of
training samples (i.e., fln maps) by our a-lnm, which helps to mitigate the risk
of over-fitting and improve the performance of overall survival time prediction
when learning a deep neural network from such a small sized training set used in
this paper."
3438,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"we evaluated
the classification performance of our proposed method using 235 gbm patients in
the brats 2020 training dataset, because only these 235 patients had both
overall survival time and manual expert segmentation labels of lesions."
3439,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"moreover, the a-lnm was performed ten times
randomly to avoid particular data distribution and obtain more reliable results."
3440,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"experimental results on the brats 2020 training dataset validated the
effectiveness of the a-lnm derived fln maps for gbm survival prediction."
3441,A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,"training on 18-videos (367-images), the model achieved
statistically significant results (p < 0.001) on the hold-out testing dataset of
5-videos (182-images) when compared to a location prior baseline model [12]."
3442,A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,"an extension of logits
cross-entropy, logits focal loss, was used instead as it accounts for data
imbalance between classes.centroid detection: 5-models were trialed: 3-models
consisted of encoders with a convolution layer and linear activation; and
2-models consisted of encoderdecoders with an average pooling layer and sigmoid
activation with 0.3 dropout."
3443,A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,"to account for structure data imbalance, images were randomly
split such that the number of structures in each fold is approximately even."
3444,A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,"images from a singular video were present in either the training or validation
dataset.each model was run for with a batch size of 5 for 20 epochs, where the
epoch with the best primary evaluation metric on the validation dataset was
kept."
3445,A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,"this model, resnet18 with mse loss, outperforms the
more sophisticated models, as these models over-learn image features in the
training dataset."
3446,A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,"collecting data from more pituitary surgeries will support
incorporating anatomy variations and achieving generalisability."
3447,Intraoperative CT Augmentation for Needle-Based Liver Interventions,"image fusion typically relies on the estimation
of rigid or non-rigid transformations between 2 images, to bring into the
intraoperative image structures of interest only visible in the preoperative
data."
3448,Intraoperative CT Augmentation for Needle-Based Liver Interventions,"therefore, we augment the data set by applying multiple random deformations to
the original images."
3449,Intraoperative CT Augmentation for Needle-Based Liver Interventions,"once the network has been trained on the patient-specific preoperative data, the
next step is to augment and visualize the intraoperative ncct."
3450,Intraoperative CT Augmentation for Needle-Based Liver Interventions,"to achieve this objective, we compute a graph data
structure from the preoperative segmentation."
3451,Intraoperative CT Augmentation for Needle-Based Liver Interventions,"finally, we extract the vm from each mpcect sample and apply
3 dilation operations, which demonstrated the best performance in terms of
prediction accuracy and robustness on our data."
3452,Intraoperative CT Augmentation for Needle-Based Liver Interventions,"we note that public data sets
such as deeplesion [24], 3dircadb-01 [25] and others do not fit our problem
since they do not include the ncct images."
3453,Intraoperative CT Augmentation for Needle-Based Liver Interventions,"for a given subject, we
generate 100 displacement fields using the data augmentation strategy explained
above with 50 voxels for the control points spacing in the three spatial
directions and a standard deviation of 5 voxels for the normal distributions."
3454,Intraoperative CT Augmentation for Needle-Based Liver Interventions,"we ha performed tests on 4 different (porcine)
data sets."
3455,Intraoperative CT Augmentation for Needle-Based Liver Interventions,"qualitative assessment: to further demonstrate the value of our
method, we have asked two clinicians to manually segment the ncct images in the
intraoperative mpcect data."
3456,Intraoperative CT Augmentation for Needle-Based Liver Interventions,"using the data of the subject 1, a u-net was trained to
segment the vessel tree of the intraoperative ncct image."
3457,Intraoperative CT Augmentation for Needle-Based Liver Interventions,"we have seen, on our experimental data, that 3
dilation operations were sufficient to compensate for the possible motion
between ncct and cct acquisitions."
3458,Intraoperative CT Augmentation for Needle-Based Liver Interventions,"to
assess this assumption, a voxelmorph1 network was trained on the subject 1 of
our porcine data sets."
3459,Intraoperative CT Augmentation for Needle-Based Liver Interventions,"our future steps will essentially involve applying this method to
patient data and perform a small user study to evaluate the usefulness and
limitations of our approach.aknowledgments."
3460,Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,"in the following, we first present our oce needle probe and outline data
processing for elasticity estimates."
3461,Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,"1,
using force and position sensor data (see supplementary material)."
3462,Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,"for each indentation, we place the
needle in front of the surface or deep tissue interface and acquire oct data
while driving the needle for 3 mm (fig."
3463,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"(3) we established a comprehensive pelvic
fracture ct dataset and provided ground-truth annotations."
3464,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"our dataset and
source code are publicly available at https://github.com/yzzliu/fracsegnet."
3465,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"with
a cascaded 3d nn-unet architecture, the network is pre-trained on a set of
healthy pelvic ct images [5,13] and further refined on our fractured dataset."
3466,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"the fdm is computed on the ground-truth
segmentation of each data sample before training."
3467,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"although large-scale datasets on pelvic segmentation have been studied in some
research [13], to the best of our knowledge, currently there is no
well-annotated fractured pelvic dataset publicly available."
3468,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"therefore, we
curated a dataset of 100 preoperative ct scans covering all common types of
pelvic fractures."
3469,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"these data is collected from 100 patients (aged 18-74 years,
41 females) who were to undergo pelvic reduction surgery at beijing jishuitan
hospital between 2018 and 2022, under irb approval (202009-04)."
3470,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"we evaluated our method on 100 pelvic fracture ct scans and
made our dataset and ground truth publicly available."
3471,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,"we employed the publicly available easy-resect (retrospective evaluation of
cerebral tumors) dataset [10] (https://archive.sigma2.no/pages/ public/dataset
detail.jsf?id=10.11582/2020.00025) to train and evaluate our proposed method."
3472,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,"this dataset is a deep-learning-ready version of the original resect database,
and was released as part of the 2020 learn2reg challenge [24]."
3473,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,"to train our dl model, we made subject-wise division of the entire dataset into
70%:15%:15% as the training, validation, and testing sets, respectively."
3474,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,"also,
to improve the robustness of the network, we used data augmentation for the
training data by random rotation, random horizontal flip, and random vertical
flip."
3475,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,"table 1 lists the mean and standard deviation of landmark identification errors
(in mm) between the predicted position and the ground truth in intra-operative
us for each patient of the resect dataset."
3476,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,"however, due to
limited clinical data, 3d approaches caused overfitting in our network training."
3477,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,"in the resect dataset, eligible anatomical landmarks were defined
as deep grooves and corners of sulci, convex points of gyri, and vanishing
points of sulci."
3478,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,"in this project, we proposed a cl framework for mri-us landmark detection for
neurosurgery for the first time by leveraging real clinical data, and achieved
state-of-the-art results."
3479,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,"our final model is similar to that presented in [15] but differs in its use of a
weight shared image encoder, software simulated defocusing for training data,
and small input patch size."
3480,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,"with a
sensor resolution of 2048 × 1088 pixels, hyperspectral data is acquired with a
spatial resolution of 512 × 272 pixels per spectral band."
3481,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,"video-rate imaging of
snapshot data is achieved with a speed of up to 50 fps depending on acquisition
parameters."
3482,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,"in order to assemble a large and diverse focal-time scan
dataset, we choose to simulate focal-time scans using existing in-focus video
data."
3483,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,"we use this
technique to create a training and testing dataset consisting of 1000 and 200
simulated focal-time scans based on 200 10-second video clips sampled from
cholec80 [13], a popular endoscopic dataset."
3484,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,"these act as a validation dataset to help prevent over fitting and aid
generalisation."
3485,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,"while gaussian blur is a reasonable approximation, we note that
more rigours methods exist to simulate defocus blur that may produce better
simulated data [9].robotic focal-time scan."
3486,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,"as a testing dataset similar to our
intended use case, we chose to approximate a real focal-time scan by controlling
conditions during capture of the individual focal stacks."
3487,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,"we developed a novel cnn-based autofocus policy
suitable for video data."
3488,Surgical Video Captioning with Mutual-Modal Concept Alignment,"extensive experiments
are performed on neurosurgery video and nephrectomy image datasets, and
demonstrate the effectiveness of our sca-net by remarkably outperforming the
state-of-the-art captioning works."
3489,Surgical Video Captioning with Mutual-Modal Concept Alignment,neurosurgery video captioning dataset.
3490,Surgical Video Captioning with Mutual-Modal Concept Alignment,"to evaluate the effectiveness of surgical
video captioning, we collect a large-scale dataset with 41 surgical videos of
endonasal skull base neurosurgery."
3491,Surgical Video Captioning with Mutual-Modal Concept Alignment,"after
necessary data cleaning, we divide these surgical videos with resolution of 1,
920× 1, 080 into 11, 004 thirty-second video clips with clear surgical purposes."
3492,Surgical Video Captioning with Mutual-Modal Concept Alignment,"we split these video
clips at patientlevel, where the video clips of 31 patients are used for
training and the rest of 10 patients are utilized for test.endovis image
captioning dataset."
3493,Surgical Video Captioning with Mutual-Modal Concept Alignment,"we further compare our method with state-of-the-arts on the
public endovis-2018 image captioning dataset [1,23]."
3494,Surgical Video Captioning with Mutual-Modal Concept Alignment,"this dataset reveals
robotic nephrectomy procedures acquired by the da vinci x or xi system, and is
annotated with surgical actions between 9 possible tools and surgical targets
[23]."
3495,Surgical Video Captioning with Mutual-Modal Concept Alignment,"in this way, these two datasets can comprehensively evaluate
the captioning tasks under both surgical videos and images.implementation
details."
3496,Surgical Video Captioning with Mutual-Modal Concept Alignment,"we optimize the sca-net and compared captioning
methods using adam with the batch size of 12 for both captioning datasets."
3497,Surgical Video Captioning with Mutual-Modal Concept Alignment,"all
models are trained for 20 and 50 epochs in neurosurgery and endovis datasets,
respectively."
3498,Surgical Video Captioning with Mutual-Modal Concept Alignment,"to further confirm the effectiveness of surgical captioning, we perform the
comparison on the public endovis image captioning dataset."
3499,Surgical Video Captioning with Mutual-Modal Concept Alignment,"moreover, we
propose the mc-align to mutually coordinate visual and text representations with
surgical concept representations of the other modality for multi-modal decoding,
thereby generating more accurate captions with aligned multi-modal
knowledge.extensive experiments on neurosurgery and nephrectomy datasets confirm
the advantage of our sca-net over state-of-the-arts on the surgical captioning."
3500,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"despite that deep learning models have shown success in surgical data science to
improve the quality of surgical intervention [20][21][22], such as intelligent
workflow analysis [7,13] and scene understanding [1,28], research on
higher-level cognitive assistance for surgery still remains underexplored."
3501,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"to date, there is still no work on
data-driven solutions to predict such dissection trajectories, but we argue that
it is possible to reasonably learn this skill from expert demonstrations based
on video data.imitation learning has been widely studied in various domains
[11,16,18] with its good ability to learn complex skills, but it still needs
adaptation and improvement when being applied to learn dissection trajectory
from surgical data."
3502,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"in addition, the
model performance can be sensitive to data distribution and the noise in
training data would result in unstable trajectory predictions.in this paper, we
explore an interesting task of predicting dissection trajectories in esd surgery
via imitation learning on expert video data."
3503,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"to address the limitations of inefficient
training and unstable performance associated with ebm-based implicit policies,
we formulate the implicit policy using an unconditional diffusion model, which
demonstrates remarkable ability in representing complex high-dimensional data
distribution for videos."
3504,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"for
experimental evaluation, we collected a surgical video dataset of esd
procedures, and preprocessed 1032 short clips with dissection trajectories
labelled."
3505,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"in this section, we describe our approach idiff-il, which learns to predict the
dissection trajectory from expert video data using the implicit diffusion
policy."
3506,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,", i t }, i t ∈ r h×w ×3 and the output is an action distribution of a
sequence of 2d coordinates a = {y t+1 , y t+2 , ..., y t+n }, y t ∈ r 2
indicating the future dissection trajectory projected to the image space.in
order to obtain the demonstrated dissection trajectories from the expert video
data, we first manually annotate the dissection trajectories on the video frame
according to the moving trend of the instruments observed from future frames,
then create a dataset d = {(s, a) i } m i=0 containing m pairs of video clip
(state) and dissection trajectory (action).to precisely predict the expert
dissection behaviors and effectively learn generalizable features from the
expert demonstrations, we use the implicit model as our imitation policy."
3507,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"1 from the video
demonstration data is challenging for previous ebm-based methods."
3508,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"by
representing the data using a continuous thermodynamics diffusion process, which
can be discretized into a series of gaussian transitions, the diffusion model is
able to express complex high-dimensional distribution with simple parameterized
functions."
3509,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"in addition, the diffusion process also serves as a form of data
augmentation by adding a range of levels of noise to the data, which guarantees
a better generalization in high-dimensional state space.as shown in fig."
3510,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"the forward process gradually diffuses the
original data x 0 = (s, a), to a series of noised data {x 0 , x 1 , • • • , x t
} with a gaussian kernel q(x t |x t-1 ), where t denotes the diffusion step."
3511,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"in
the reverse process, the data is recovered via a parameterized gaussian p θ (x
t-1 |x t ) iteratively."
3512,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"with the reverse process, the joint state-action
distribution in the implicit policy can be expressed as:the probability of the
noised data x t in forward diffusion process is a gaussian distribution
expressed as q(x t |x 0 ) = n (x t , √ α t x 0 , (1α t )i ), where α t is a
scheduled variance parameter, which can be referred from [10], and i is an
identity matrix."
3513,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"2.2 is for unconditional
generation, the conventional sampling strategy through the reverse process will
predict random trajectories in expert data."
3514,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,dataset.
3515,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"we evaluated the proposed approach on a dataset assembled from 22
videos of esd surgery cases, which are collected from the endoscopy centre of
the prince of wales hospital in hong kong."
3516,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"first, to
study how the model performs on data within the same surgical context as the
training data, we define a subset, referred as to the ""in-the-context"" testing
set, which consists of consecutive frames selected from the same cases as
included in the training data."
3517,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"the
superior results achieved by our method demonstrate the effectiveness of the
diffusion model in learning the implicit policy from the expert video data."
3518,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"in
addition, our method can learn generalizable dissection skills by exhibiting a
lower standard deviation of the prediction errors compared to the bc, which
severely suffers from over-fitting to the training data."
3519,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"while our method
improves marginally compared with the explicit form on the ""out-of-the-context""
data, exhibiting a slighter over-fitting with a lower standard
deviation.forward-diffusion guidance."
3520,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"when encountered with the
unseen scenarios in ""out-of-the-context"" data, the performance improvement of
such inference strategy is marginal.value of synthetic data."
3521,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"since the learned
implicit diffusion policy is capable of generating synthetic expert dissection
trajectory data, which can potentially reduce the expensive annotation cost."
3522,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"to
better explore the value of such synthetic expert data for downstream tasks, we
train the baseline model with the generated expert demonstrations."
3523,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"then, we train the bc model with different data, the pure
expert data (real), synthetic data only (synt) and the mixed data with the real
and the synthetic (mix).the table in fig."
3524,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"3 shows the synthetic data is useful
as the augmented data for downstream task learning."
3525,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"this paper presents a novel approach on imitation learning from expert video
data, in order to achieve dissection trajectory prediction in endoscopic
surgical procedure."
3526,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"experimental results show that our method outperforms
state-of-the-art approaches on the evaluation dataset, demonstrating the
effectiveness of our approach for learning dissection skills in various surgical
scenarios."
3527,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"despite seed information and large resections, reoperation rates are
still high (~17%) emphasizing the need for additional guidance technologies such
as computer-assisted surgery systems with nonrigid registration
[7].intraoperative data available for registration is often sparse and subject
to data collection noise."
3528,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"image-to-physical registration methods that accurately
model an elastic soft-tissue environment while also complying with
intraoperative data constraints is an active field of research."
3529,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"determining
correspondences between imaging space and geometric data is required for
image-to-physical registration, but it is often an inexact and ill-posed
problem."
3530,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"establishing point cloud correspondences using machine learning has
been demonstrated on liver and prostate datasets [8,9]."
3531,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"however, these methods require extensive training data and may struggle with
generalizability."
3532,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"the element-free
galerkin method is a mesh-free method that requires only nodal point data and
uses a moving least-squares approximation to solve for a solution [13]."
3533,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"regularized kelvinlet functions are analytical solutions to the equations for
linear elasticity that we superpose to compute a nonrigid deformation field
nearly instantaneously [15].we utilize ""grab"" and ""twist"" regularized kelvinlet
functions with a linearized iterative reconstruction approach (adapted from
[12]) that is well-suited for sparse data registration problems."
3534,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"sensitivity to
regularized kelvinlet function hyperparameters is explored on a supine mr breast
imaging dataset."
3535,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"then, the f grab and f twist vectors are optimized to solve for a
displacement field that minimizes distance error between geometric data inputs."
3536,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"by setting x 0
locations, ε grab , and ε twist as hyperparameters, deformation states can be
represented by various α vectors with the registration task being to solve for
the optimal α vector.an objective function is formulated to minimize
misalignment between the moving space x moving and fixed space x fixed through
geometric data constraints."
3537,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"for the breast imaging datasets in this work, we
used simulated intraoperative data features that realistically could be
collected in a surgical environment visualized in fig."
3538,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"the first data feature
is mr-visible skin fiducial points placed on the breast surface (fig."
3539,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"the other two data features are
an intra-fiducial point cloud of the skin surface (fig."
3540,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"these data
features are surfaces that do not have known correspondence."
3541,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"these data feature
designations are consistent with implementations in previous work [16,17].for a
given deformation state, each data feature contributes to the total error
measure."
3542,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"for the point data, the error e i point for each point i is simply the
distance magnitude between corresponding points in x fixed and x moving space."
3543,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"for the surface data, the error e i surface is calculated as the distance from
every point i in the x fixed point cloud surface to the closest point in the x
moving surface, projected onto the surface unit normal which allows for sliding
contact between surfaces."
3544,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"the first explores sensitivity
to regularized kelvinlet function hyperparameters k grab , k twist , ε grab ,
and ε twist and establishes optimal hyperparameters in a training dataset of 11
breast deformations."
3545,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"this dataset consists of supine breast mr images simulating surgical
deformations of 11 breasts from 7 healthy volunteers."
3546,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"a second mr
image in the deformed state was acquired to create simulated intraoperative
physical data and to use for validation."
3547,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"the skin fiducials and
intra-fiducial surface point clouds were labeled in both images as data
features."
3548,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"sparse tracked ultrasound data collection patterns were projected on
the posterior surface for use as the third data feature."
3549,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"three
hyperparameter sweeps were used: this dataset consists of supine breast mr
images simulating surgical
deformations from one breast cancer patient."
3550,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"however, it was included to demonstrate accuracy when
volumetric imaging data is available, as opposed to sparse geometric point data
as in the surgical application case."
3551,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"we address the important problem of intraoperative patient-to-image registration
in a new way by relying on preoperative data to synthesize plausible
transformations and appearances that are expected to be found intraoperatively."
3552,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"the idea of pre-computing data for registration was introduced by [26], who used
an atlas of pre-computed 3d shapes of the brain surface for registration."
3553,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"outside of neurosurgery, the concept of
pregenerating data for optimizing dnns for intraoperative registration has been
investigated for ct to x-ray registration in radiotherapy where x-ray images can
be efficiently simulated from cts as digital radiographic reconstructions
[12,27]."
3554,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"we present results on both
synthetic and clinical data and show that our approach outperformed
state-of-the-art methods."
3555,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"however, they require a large set of annotated data [3,24] or perform only on
modalities with similar sensors [12,27]."
3556,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"we choose to use a neural image analogy method that combines
the texture of a source image with a high-level content representation of a
target image without the need for a large dataset [1]."
3557,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,dataset.
3558,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"we tested our method retrospectively on 6 clinical datasets from 6
patients (cases) (see fig."
3559,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"we generated 100 poses for each 3d mesh (i.e.: each
case) and used a total of 15 unique textures from human brain surfaces
(different from our 6 clinical datasets) for synthesis using s θ ."
3560,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"we evaluated the pose regressor network on both synthetic
and real data."
3561,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"because a conventional train/validation/test split would lead to texture
contamination, we created our validation dataset so that at least one texture is
excluded from the training set."
3562,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"our
experiments using clinical data showed that our method provides accurate
registration without manual intervention, that it is computationally efficient,
and it is invariant to the visual appearance of the cortex."
3563,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"we present results on a clinical
dataset comprising fifty post-operative glioblastoma (gbm) patients."
3564,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"we remark that the dose predictor model was also trained
with data augmentation, so imperfect segmentation masks and corresponding dose
plans are included."
3565,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"due to the
limited data availability, we present results using 2d-based models but remark
that their extension to 3d is straightforward."
3566,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"we remark that
during training we use standard data augmentations including spatial
transformations, which are also subjected to dose predictions, so the model is
informed about relevant segmentation variations producing dosimetry changes."
3567,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"we divide the descriptions of the two separate datasets used for the dose
prediction and segmentation models."
3568,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"the dose prediction model was trained on an in-house dataset comprising a total
of 50 subjects diagnosed with post-operative gbm."
3569,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"this includes ct imaging data,
segmentation masks of 13 oars, and the gtv."
3570,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"we
divided the dataset into training (35 cases), validation (5 cases), and testing
(10 cases)."
3571,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"we refer the reader to [9] for further details.segmentation models:
to develop and test the proposed approach, we employed a separate in-house
dataset (i.e., different cases than those used to train the dose predictor
model) of 50 cases from post-operative gmb patients receiving standard rt
treatment."
3572,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"we divided the dataset into training (35 cases), validation (5
cases), and testing (10 cases)."
3573,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"3 the tumor presents a non-convex
shape alongside the skull's parietal lobe, which was not adequately modeled by
the training dataset used to train the segmentation models."
3574,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"in this case, the training data also lacked
characterization for such cases."
3575,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"we
also remark that the range of hd values is in range with values reported by
models trained using much more training data (see [1]), alluding to the
possibility that the problem of robustness might not be directly solvable with
more data."
3576,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"these first results on a dataset of post-operative gbm
patients show the ability of the proposed doselo to deliver improved
dosimetric-compliant segmentation results."
3577,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,"1, the proposed method uses a clinically-compatible flim
system coupled to the da vinci sp transoral robotic surgical platform to scan
the surgical cavity in vivo and acquire flim data."
3578,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,"we used the cumulative
distribution transform (cdt) of the fluorescence decay curves extracted from the
flim data as the input feature."
3579,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,"this study used a multispectral fluorescence lifetime imaging (flim) device to
acquire data [14]."
3580,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,"the resulting autofluorescence waveform measurements for each channel
are averaged four times, thus with a 480 hz excitation rate, resulting in 120
averaged measurements per second [15].the flim device includes a 440 nm
continuous wave laser that serves as an aiming beam; this aiming beam enables
real-time visualization of the locations where fluorescence (point measurements)
is collected by generating visible blue illumination at the location where data
is acquired."
3581,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,"segmentation of the 'aiming beam' allows for flim data points to be
localized as pixel coordinates within a surgical white light image (see fig."
3582,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,"localization of these coordinates is essential to link the regions where data is
obtained to histopathology, which is used as the ground truth to link flim
optical data to pathology status [16]."
3583,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,"flim data was acquired using the da vinci
sp robotic surgical platform."
3584,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,"the gods is a pairwise complimentary classifier defined by
two separating hyperplanes to minimize the distance between the two classifiers,
limiting the healthy flim data within the smallest volume and maximizing the
margin between the hyperplanes and the data, thereby avoiding overfitting while
improving classification robustness."
3585,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,"the testing data contained both healthy
and residual cancer labels."
3586,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,"s1).the gods uses
two separating hyperplanes to minimize the distance between the two classifiers
by learning a low-dimensional subspace containing flim data properties of
healthy labels."
3587,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,"residual tumor labels are detected by calculating the distance
between the projected data points and the learned subspace."
3588,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,"this is mainly due to the robustness of the model, the ability to
handle high-dimensional data, and the contrast in the flim decay curves."
3589,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,"we
observed a correlation between a larger spread of false positive predictions
associated with a zone of coagulation to a zone of hyperemia.the novelty
detection model generalizes to the healthy labels and considers data falling off
the healthy distribution as residual cancer."
3590,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"however, as the true
underlying tissue deformation is unknown due to the 3d nature of the surgical
data and the time constraint, real-time manual inspection of mri-ius
registration results is challenging and error-prone, especially for
precision-sensitive neurosurgery."
3591,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"therefore, algorithms that can detect and
quantify unreliable inter-modal medical image registration results are highly
beneficial.recently, automatic quality assessment for medical image registration
has attracted increasing attention [4] from the domains of big medical data
analysis and surgical interventions."
3592,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"lastly, we developed and thoroughly evaluated our
technique against a recent baseline model [12] using real clinical data and
showed excellent results."
3593,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"for methodological development and assessment, we used the resect
(retro-spective evaluation of cerebral tumors) dataset [16], which has
pre-operative mri, and ius scans at different surgical stages from 23 subjects
who underwent low-grade glioma resection surgeries."
3594,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"to create the silver registration ground truths, we
used the homologous landmarks between mri and ius in the resect dataset to
perform landmark-based 3d b-spline nonlinear registration to register ius to the
corresponding mri for all 22 cases."
3595,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"furthermore, in addition to the transformation augmentation, we also included
additional data augmentation, including random noise addition and random image
flipping on training sets to mitigate overfitting and increase the model's
generalizability."
3596,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"the two dl models were trained with the same dataset and
procedure, and their prediction accuracies, measured as the absolute error
between the predicted and ground truths mis-registration on the test set were
compared with two-sided pairedsamples t-tests to confirm the superiority of the
proposed method, in addition to correlations between their estimated and ground
truth errors."
3597,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"across all samples in the testing data, we
achieved an accuracy of 0.59 ± 0.57 mm, while the counterpart obtained a
prediction error of 1.69 ± 1.37 mm."
3598,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"compared with the vit and its variants, focal modulation allows a more
lightweight setup, which could be desirable for 3d data."
3599,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"although simulated ultrasound has been used to provide a perfect alignment with
mris, the fidelity of the simulated results is still suboptimal, and this may
explain the underperformance of the previous technique in real clinical data
[12]."
3600,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"one limitation of our work
lies in the limited patient data, as public ius datasets are scarce, while the
settings and properties of us scanners can vary, potentially affecting the dl
model designs."
3601,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"therefore, we created random deformations for patch-wise error
estimation, and will further explore data-efficient approaches for registration
error assessment."
3602,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"however, it is not trivial to
determine this using traditional methods due to poor textural definition of
tissues and lack of per-pixel ground truth depth data."
3603,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"[22]
proposed a deep learning framework for surgical scene depth estimation in
self-supervised mode and achieved scalable data acquisition by incorporating a
differentiable spatial transformer and an autoencoder into their framework."
3604,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"however, acquiring per-pixel ground truth depth
data is challenging, especially for laparoscopic images, which makes it
difficult for large-scale supervised training [8].laparoscopic segmentation is
another important task in computer-assisted surgery as it allows for accurate
and efficient identification of instrument position, anatomical structures, and
pathological tissue."
3605,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"hence, stereo image data was also adopted in this paper.if the
problem of inferring the intersection point is treated as a geometric problem,
both data collection and intra-operative registration would be difficult, which
inspired us to approach this problem differently."
3606,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"therefore, we can establish a dataset with an image pair (rgb image and laser
image) that shares the same intersection point ground truth with the laser image
(see fig."
3607,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"to validate our proposed solution for the newly formulated problem, we acquired
and publicly released two new datasets."
3608,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,2 shows a sample from our dataset.
3609,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,data collection.
3610,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"we acquired the dataset on a silicone
tissue phantom which was 30 × 21 × 8 cm and was rendered with tissue color
manually by hand to be visually realistic."
3611,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"the
ground truth probe axis-surface intersection.all data acquisition and devices
were controlled by python and labview programs, and complete data sets of the
above images were collected on visually realistic phantoms for multiple probe
and laparoscope positions."
3612,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"therefore, our first newly
acquired dataset, named jerry, contains 1200 sets of images."
3613,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"since it is
important to report errors in 3d and in millimeters, we recorded another dataset
similar to jerry but also including ground truth depth map for all frames by
using structured-lighting system [8]-namely the coffbee dataset.these datasets
have multiple uses such as:-intersection point detection: detecting intersection
points is an important problem that can bring accurate surgical cancer
visualization."
3614,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"we partitioned the jerry dataset into three subsets, the
training, validation, and test set, consisting of 800, 200, and 200 images,
respectively, and the same for the coffbee dataset."
3615,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"quantitative results on the released datasets are shown in table 1 and table 2
with different backbones for extracting image features, resnet and vit."
3616,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"for the
2d error on two datasets, among the different settings, the combination of
resnet and mlp gave the best performance with a mean error of 70.5 pixels and a
standard deviation of 56.8."
3617,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"we note that the 3d error subjected to the quality
of the acquired ground truth depth maps, which had limited resolution and
non-uniformly distributed valid data due to hardware constraints."
3618,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"both the hardware and software
design of the proposed solution were illustrated and two newly acquired datasets
were publicly released."
3619,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"we believe that our problem
reformulation and dataset release, together with the initial experimental
results, will establish a new benchmark for the surgical vision community."
3620,A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,"to the best of
our knowledge, this work shows the first study to continuously track the
flexible ureteroscope in preoperative data using a vision-based method."
3621,A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,"while some structural
features such as capillary texture and striations at the tip of the renal
pyramids are observed ureteroscopic images, they are not discernible in ct or
other preoperative data."
3622,A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,"vision transformers require large
datasets for training, so we initialize the encoder with weights pretrained on
imagenet and further train it on our in-house database."
3623,A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,"according to ctu characteristics [10], this work uses our
excretory-phase data to generate virtual images and set [t u , t v ] to [-1000,
120], where -1000 represents air and 120 was determined by the physician's
experience and characteristics of contrast agents.unfortunately, the accuracy of
thresholded structural regions suffers from inaccurate depth maps caused by
renal stones and contrast agents."
3624,A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,"we validate our method on clinical ureteroscopic lithotripsy data with video
sequences and ctu volumes."
3625,A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,"while we manually annotated ureteroscopic video images for dpt-base
segmentation, three experts also manually generated ureteroscope pose
groundtruth data by our developed software, which can manually adjust position
and direction parameters of the virtual camera to visually align endoscopic real
images to virtual images, evaluating the navigation accuracy of the different
methods."
3626,A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,"we will improve the segmentation of ureteroscopic
video images, while generating more ground-truth data for training and testing."
3627,Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,"additionally, four public datasets including
kvasir, etis-laribpolypdb, cvc-colondb, and cvc-clinicdb were also used to
evaluate our network model."
3628,Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,"furthermore, we also summarizes the average three metrics computed from
all the five databases (the in-house dataset and four public datasets)."
3629,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"however, as noted in
[30], multi-modal data is expensive and sparse, typically leading to incomplete
sets of images."
3630,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"however, these operations do not force
the network to learn a shared latent representation of multi-modal data and lack
theoretical foundations."
3631,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"isotropic normal distribution) over latent variables z ∈ r h and where p θ
(x|z) is a decoder parameterized by θ that reconstructs data x ∈ r n given z."
3632,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"the training goal with respect to θ is to maximize the
marginal likelihood of the data p θ (x) (the ""evidence""); however since the true
posterior p θ (z|x) is in general intractable, the variational evidence lower
bound (elbo) is instead optimized."
3633,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"multi-modal vaes [8,25,30] introduced a principled probabilistic formulation to
support missing data at training and inference time."
3634,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"the joint reconstruction and synthesis optimization goal is to maximize the
expected evidence e x∼p data [log(p(x))]."
3635,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"5 is valid
for any approximate distribution q, the evidence, log(p θ (x)), is in particular
lowerbounded by the following subset-specific elbo for any subset of images
π:hence, the expected evidence e x∼p data [log(p(x))] is lower-bounded by the
average of the subset-specific elbo, i.e.:consequently, we propose to average
all the subset-specific losses at each training iteration."
3636,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"in this section, we report experiments conducted on the challenging problem of
mr and ius image synthesis.data."
3637,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"we evaluated our method on a dataset of 66
consecutive adult patients with brain gliomas who were surgically treated at the
brigham and women's hospital, boston usa, where both pre-operative 3d t2-space
and pre-dural opening intraoperative us (ius) reconstructed from a tracked
handheld 2d probe were acquired."
3638,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,the data will be released on tcia in 2023.
3639,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"the dataset was randomly split into a
training set (n = 56) and a testing set (n = 10)."
3640,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"since paired data was available for evaluation, standard
supervised evaluation metrics are employed: psnr (peak signal-to-noise ratio),
ssim (structural similarity), and lpips [31] (learned perceptual image patch
similarity)."
3641,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"the current framework enables the generation of
ius data using t 2 mri data."
3642,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"since image delineation is much more efficient on
mri than on us, annotations performed on mri could be used to train a
segmentation network on pseudo-ius data, as performed by the top-performing
teams in the crossmoda challenge [9]."
3643,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"for example, synthetic ultrasound images
could be generated from the brats dataset [1], the largest collection of
annotated brain tumor mr scans."
3644,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,"similarly to [14], we adopt a loss l rigid masked on
empty slices to avoid the introduction of noise at slices within the gradient
where no data is provided, and directly train on the dice similarity coefficient
(dsc) between rigid areas:additionally, r also warps ct without gradient
backpropagation and is the input with sct for plane selection."
3645,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,dataset and preprocessing.
3646,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,"our clinical dataset consists of 108 patients for
whom were acquired both a pre-operative h&n ct scan and 4 to 11 wsis after
laryngectomy (with a total amount of 849 wsis)."
3647,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,"we split the dataset patient-wise into three
groups for training (64), validation (20), and testing (24)."
3648,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,"to demonstrate the
performance of our model on another application, we also retrieved the datasets
from [14] for pelvis 3d ct/2d mr."
3649,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,"we drew our code from cyclegan and voxelmorph implementations with modifications
explained above, and we thank the authors of msv-regsynnet for making their code
and data available to us [1,14,25]."
3650,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,"according to the mr/ct application in rt, we compared our model against
the state-of-the-art results of msv-regsynnet which were computed on the same
dataset."
3651,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,"we also compared against
msv-regsynnet on its own validation dataset for generalization assessment: we
yielded comparable results for the first cohort and significantly better ones
for the second, which proves that structuregnet behaves well on other modalities
and that the structure awareness is an essential asset for better registration,
as pelvis is a location where organs are moving."
3652,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,"we implement adaptive discriminator
augmentation from stylegan-ada [14] to improve learning of the model's manifold
with limited medical imaging data."
3653,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,"we trained our model with a large dataset of 3500 cts of
patients with head-and-neck cancer, more exactly 2297 patients from the publicly
available the cancer imaging archive (tcia) [1,6,16,17,28,32] and 1203 from
private internal data, after obtention of ethical approbations."
3654,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,"we split this
data into 3000 cases for training, 250 for validation, and 250 for testing."
3655,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,"for
reference, compared to 3dstylegan [10], our model achieved half their fid score
on another brain mri dataset, with comparable ms-ssim."
3656,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,"this approach may provide coarse
reconstructions for patients with rare abnormalities, as most learning methods,
but a larger dataset or developing a prior including tissue abnormalities could
improve robustness."
3657,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,"given these issues, there are clear
advantages for synthesizing anatomically accurate ct data from mri.most
synthesis methods adopt supervised learning paradigms and train generative
models to synthesize ct [1][2][3]6,17]."
3658,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,"despite the superior performance,
supervised methods require a large amount of paired data, which is prohibitively
expensive to acquire."
3659,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,"several unsupervised mri-to-ct synthesis methods [4,6,14],
leverage cyclegan with cycle consistency supervision to eliminate the need for
paired data."
3660,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,"the
structural distortion in synthetic results exacerbates when data from the two
modalities are heavily misaligned, which usually occurs in pediatric scanning
due to the rapid growth in children.recent unsupervised methods impose
structural constraints on the synthesized ct through pixel-wise or shape-wise
consistency."
3661,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,data collection.
3662,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,the dataset comprises brain mr and ct volumes from 262 subjects.
3663,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,"the dataset is divided into
249, 1 and 12 subjects for training, validating and testing set."
3664,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,"unlike adult datasets [4,14], pediatric datasets are
easily misaligned due to children's rapid growth between scans."
3665,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,"to alter object shapes, we employ random elastic
deformation, a standard data augmentation technique [10] that applies random
displacement vectors to objects."
3666,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,"experimental results on a clinical dataset show that maskgan
significantly outperforms existing methods and produces synthetic ct with more
consistent mappings of anatomical structures."
3667,FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,"sparse-view ct is one of the effective solutions, which
reduces the radiation by only sampling part of the projection data for image
reconstruction."
3668,FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,"freeseed achieves promising results with only image data and can be
further enhanced once the sinogram is available.our contributions can be
summarized as follows: 1) a novel frequency-bandaware network is introduced to
efficiently capture the pattern of global artifacts in the fourier domain among
different sparse-view scenarios; 2) to promote the restoration of heavily
corrupted image detail, we propose a self-guided artifact refinement network
that ensures targeted refinement of the reconstructed image and consistently
improves the model performance across different scenarios; and 3) quantitative
and qualitative results demonstrate the superiority of freeseed over the
state-of-the-art sparse-view ct reconstruction methods."
3669,FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,"dual-domain methods are effective in the task of sparse-view ct reconstruction
when the sinogram data are available."
3670,FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,"we conduct experiments on the dataset of ""the 2016 nih-aapm mayo clinic low dose
ct grand challenge"" [8], which contains 5,936 ct slices in 1 mm image thickness
from 10 anonymous patients, where a total of 5,410 slices from 9 patients,
resized to 256 × 256 resolution, are randomly selected for training and the 526
slices from the remaining one patient for testing without patient overlap."
3671,FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,"note that when the sinogram data are available, dual-domain counterpart
freeseed dudo gains further improvements, showing the great flexibility of our
model."
3672,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"the non-correspondence
detection approach, which typically relies on a sophisticated designed loss
function, is very sensitive to the dataset [1] and difficult to find a set of
unified parameters.therefore, to effectively address the non-correspondence
problem in registering pathology images, it is necessary to incorporate both a
data-independent segmentation module and a modality-adaptive inpainting module
into the registration pipeline."
3673,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"importantly, our proposed training procedure is fully unsupervised
which does not require any labeled data for training the network."
3674,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"our experimental design focuses on two common clinical tasks: atlas-based
registration, which involves warping pathology images to a standard atlas
template, and longitudinal registration, which involves registering
pre-operative images to post-operative images for the purpose of tracking
changes over time.dataset and pre-processing."
3675,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"to evaluate our approach, we employed a
5-fold cross-validation method and divided our data into training and test sets
in an 8:2 ratio."
3676,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"to create
such a mapping, we created a pseudo dataset by utilizing images from the oasis-1
and brats2020."
3677,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"from the resulting t1 sequences, a pseudo dataset of 300 images
was randomly selected for further analysis."
3678,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"appendix b provides a detailed
process for creating the pseudo dataset.real data with landmarks."
3679,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"after creating the pseudo dataset, we warped brain mr images without tumors to
the atlas and used the resulting deformation field as the gold standard for
evaluation."
3680,Fast Reconstruction for Deep Learning PET Head Motion Correction,"currently, there are two main
categories of head motion tracking methods, hardware-based motion tracking (hmt)
and data-driven methods."
3681,Fast Reconstruction for Deep Learning PET Head Motion Correction,"in data-driven motion
tracking methods, head motion is estimated from pet reconstructions or raw data."
3682,Fast Reconstruction for Deep Learning PET Head Motion Correction,"with the development of commercial pet systems and technological advancements
such as time of flight (tof), data-driven head pet motion tracking has shown
promising results in reducing motion artifacts and improving image quality."
3683,Fast Reconstruction for Deep Learning PET Head Motion Correction,"for
instance, [12] developed a novel data-driven head motion detection method based
on the centroid of distribution (cod) of pet 3d point cloud image (pci)."
3684,Fast Reconstruction for Deep Learning PET Head Motion Correction,"image
registration methods that seek to align two or more images offer a data-driven
solution for correcting head motion."
3685,Fast Reconstruction for Deep Learning PET Head Motion Correction,"however, because of the dynamic change in pet
images, current registration-based methods need to split the data into several
discrete time frames, e.g., 5 min."
3686,Fast Reconstruction for Deep Learning PET Head Motion Correction,"this
study achieved accurate motion tracking on single subject testing data, but
showed less accurate motion predictions for multi-subject motion studies."
3687,Fast Reconstruction for Deep Learning PET Head Motion Correction,"a novel encoder and data augmentation
strategy was also applied to improve model performance."
3688,Fast Reconstruction for Deep Learning PET Head Motion Correction,"multi-subject studies were conducted on a dataset of 20 subject and its results
were quantitatively and qualitatively evaluated by molar reconstruction studies
and corresponding brain region of interest (roi) standard uptake values (suv)
evaluation."
3689,Fast Reconstruction for Deep Learning PET Head Motion Correction,"pet list-mode data and vicra
motion tracking information are available for each subject, as well as
t1-weighted mr images and mr-space to pet-space transformation matrices."
3690,Fast Reconstruction for Deep Learning PET Head Motion Correction,"we
consider data acquired between 60 and 90 min post injection (30 min total)."
3691,Fast Reconstruction for Deep Learning PET Head Motion Correction,"for
comparison purposes, we also computed the same resolution pci by tof
back-projection of the pet list-mode data along the line-of-response (lor) with
normalization for scanner sensitivity."
3692,Fast Reconstruction for Deep Learning PET Head Motion Correction,data augmentation.
3693,Fast Reconstruction for Deep Learning PET Head Motion Correction,"to improve
the performance and generalizability of our network, we use a task-specific data
augmentation strategy to expose it to more varied and diverse training data."
3694,Fast Reconstruction for Deep Learning PET Head Motion Correction,"to take this problem into account, we perform
data augmentation by simulating an additional relative motion that can be
concatenated with the true relative motion."
3695,Fast Reconstruction for Deep Learning PET Head Motion Correction,"the synthetic moving frame and the
synthetic relative motion will be used for training to increase the data
variability.network training and inference."
3696,Fast Reconstruction for Deep Learning PET Head Motion Correction,"we split our dataset of 20 subjects into distinct
subsets for training and testing with 14 and 6 subjects, respectively."
3697,Fast Reconstruction for Deep Learning PET Head Motion Correction,"an ablation study quantifies the effect of stochastic data
augmentation (da) and to standard intensity-based registration (bis)."
3698,Fast Reconstruction for Deep Learning PET Head Motion Correction,"we evaluate the following motion prediction methods
(table 1): (i) dl-hmc with pci as input (dl-hmc pci);(ii) dl-hmc with fri as
input (dl-hmc fri); (iii) proposed network with pci as input (proposed pci);
(iv) proposed network with fri as input (proposed fri); and (v) proposed method
with fri but without the data augmentation module (proposed w/o da); results
demonstrate that the proposed network with fri input provides the best motion
tracking performance in both validation and testing data."
3699,Fast Reconstruction for Deep Learning PET Head Motion Correction,"however, the proposed fri method exhibits higher variance than other
methods, which may be a result of the data augmentation distribution."
3700,Fast Reconstruction for Deep Learning PET Head Motion Correction,"in this
study, we showed that conventional intensity-based registration fails at
performing motion tracking on fri data."
3701,Fast Reconstruction for Deep Learning PET Head Motion Correction,"because of the limited vicra data, in the future, we will develop
semi-supervised deep learning methods for pet head motion correction."
3702,Fast Reconstruction for Deep Learning PET Head Motion Correction,"our study
used tof pet data because it can yield high signal to noise ratio (snr) for both
fri and pci due to the better location identification of photons, thus the
one-second fri still retains some essential brain structures."
3703,Fast Reconstruction for Deep Learning PET Head Motion Correction,"such pet will give data-driven pet motion correction a
revolutionary opportunity to have more accurate tracking and higher time
resolution."
3704,Fast Reconstruction for Deep Learning PET Head Motion Correction,"we plan to apply the proposed method to other datasets, developing a
generalized model for multi-tracer and multi-scanner pet data."
3705,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,"experiments
on four publicly collected pet/ct datasets demonstrate that our aseg outperforms
existing methods by preserving better anatomical structures in generated pseudo
ct images and achieving better visual similarity in corrected pet images."
3706,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,"the data used in our experiments are collected from the cancer image archive
(tcia) [4] (https://www.cancerimagingarchive.net/collections/), where a series
of public datasets with different types of lesions, patients, and scanners are
open-access."
3707,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,"we use
these samples in hnscc for training and in other three datasets for
evaluation.each sample contains co-registered (acquired with pet-ct scans) ct,
pet, and nac-pet whole-body scans."
3708,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,"because we cannot
access the original scatters [5], inspired by [11], we propose to resort cgan to
simulate the ac process, denoted as acgan and trained on hnscc dataset."
3709,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,"four metrics, including the peak signal to noise ratio
(psnr), mean absolute error (mae), normalized cross correlation (ncc), and ssim,
are used to measure acgan with pseudo ct images on test datasets (nsclc,
tcga-hnsc, and tcga-luda)."
3710,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,"experiments on a collection of public datasets demonstrate that
our aseg outperforms existing methods by achieving advanced performance in
anatomical consistency."
3711,An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,none
3712,Geometric Ultrasound Localization Microscopy,"for example, a recent study has shown that ultrasound
image segmentation can be learned from radiofrequency data and thus without
beamforming [13]."
3713,Geometric Ultrasound Localization Microscopy,"although the impact of
adaptive beamforming has been studiedfor ulm to investigate its potential to
refine mb localization [3], optimization of the point-spread function (psf)
poses high demands on the transducer array, data storage, and algorithm
complexity.to this end, we propose an alternative approach for ulm, outlined in
fig."
3714,Geometric Ultrasound Localization Microscopy,"dataset: we demonstrate the feasibility of our geometric ulm and present
benchmark comparison outcomes based on the pala dataset [11]."
3715,Geometric Ultrasound Localization Microscopy,"this dataset is
chosen as it is publicly available, allowing easy access and reproducibility of
our results."
3716,Geometric Ultrasound Localization Microscopy,"to date, it is the only public ulm dataset featuring radio
frequency (rf) data as required by our method."
3717,Geometric Ultrasound Localization Microscopy,"its third-party simulation data
makes it possible to perform a numerical quantification and direct comparison of
different baseline benchmarks for the first time, which is necessary to validate
the effectiveness of our proposed approach.metrics: for mb localization
assessment, the minimum root mean squared error (rmse) between the estimated p
and the nearest ground truth position is computed."
3718,Geometric Ultrasound Localization Microscopy,"to mimic the noise reduction
achieved through the use of sub-aperture beamforming with 16 transducer channels
[11], we multiplied the rf data noise by a factor of 4 for an equitable
comparison.baselines: we compare our approach against state-of-the-art methods
that utilize beamforming together with classical image filterings [8], spline
interpolation [17], radial symmetry (rs) [11] and a deep-learning-based u-net
[16] for mb localization."
3719,Geometric Ultrasound Localization Microscopy,"we
obtain the results for classical image processing approaches directly from the
open-source code provided by the authors of the pala dataset [11]."
3720,Geometric Ultrasound Localization Microscopy,"since the u-net-based
localization is a supervised learning approach, we split the pala dataset into
sequences 1-15 for testing and 16-20 for training and validation, with a split
ratio of 0.9, providing a sufficient number of 4500 training frames.results:
table 1 provides the benchmark comparison results with state-of-theart methods."
3721,Geometric Ultrasound Localization Microscopy,"we employed an energy-based model for feature
extraction in conjunction with ellipse intersections and clustering to pinpoint
contrast agent positions from rf data available in the pala dataset."
3722,Geometric Ultrasound Localization Microscopy,"the promising results from this study motivate us
to expand our research to more rf data scenarios."
3723,Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,"moreover, the data is acquired almost instantly by an
inexpensive (<100 euros) camera with flexible fiber-optics making it suitable
for endoscopic fgs in contrast to the standard slow in acquisition
frequency-based measurements obtained by expensive (usd100k+ range) stationary
cameras."
3724,Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,"the code and data produced for this work are released as an open source
at https://github.com/ibm/dot."
3725,Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,"in what follows we propose an algorithm that estimates target's indicator χ from
data y, i.e."
3726,Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,"we
obtain a sequence of updates χ n that converge into a vicinity of the true χ
provided data is ""representative enough""."
3727,Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,"this was achieved by scaling the data misfit and ptv
term to similar magnitude: we normalised the misfit term by the norm of the
observations vector and rescaled ptv term by the number of subdomains and each
local total variation weight by the number of nodes in that subdomain."
3728,Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,"additionally, iftr restricts the search space by a set
of regularizers promoting piece-wise constant structure of target's indicator
function which in turn allows to recover fluorescent targets from only the
reflectance mode cw measurements collected by a consumer grade camera.although
the scheme was tested using proof-of-concept experimental data and cubical shape
target the method is general and depending on mesh discretization level,
scalable to arbitrary domain and target shapes."
3729,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"some of these methods involve the utilization of
convolutional neural networks (cnn) to extract segmentation volumes from the
source data, transforming the problem into the registration of label maps
[13,24]."
3730,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"note that the network will be trained
only once, on a fixed dataset that is fully independent of the datasets that
will be used in the evaluation (see sect."
3731,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4).dataset.
3732,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"our neural network is
trained using patches from the ""gold atlas -male pelvis -gentle radiotherapy""
[14] dataset, which is comprised of 18 patients each with a ct, mr t1, and mr t2
volumes."
3733,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"we would like to report that, initially, we also made use of a
proprietary dataset including us volumes."
3734,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"however, as our investigation
progressed, we observed that the incorporation of us data did not significantly
contribute to the generalization capabilities of our model."
3735,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"consequently, for
the purpose of ensuring reproducibility, all evaluations presented in this paper
exclusively pertain to the model trained solely on the public mr-ct
dataset.patch sampling from unregistered datasets."
3736,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"running this procedure on our training data
results in a total of 510000 pairs of patches."
3737,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,we use the same feed-forward 3d cnn to process all data modalities.
3738,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"the architecture consists of ten layers and a total of
90,752 parameters, making it notably smaller than many commonly utilized neural
networks.augmentation on the training data is used to make the model as robust
as possible while leaving the target similarity unchanged."
3739,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"we make the training code and
preprocessed data openly available online1 ."
3740,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"notably, the experimental data utilized in our
analysis differs significantly from our model's training data in terms of both
anatomical structures and combination of modalities."
3741,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"in this experiment, we evaluate the performance of different methods for
estimating affine registration of the retrospective evaluation of cerebral
tumors (resect) miccai challenge dataset [23]."
3742,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"this dataset consists of 22 pairs
of pre-operative brain mrs and intra-operative ultrasound volumes."
3743,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"the dataset comprises 8 sets of mr and ct volumes, both depicting the
abdominal region of a single patient and exhibiting notable deformations."
3744,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"as the most challenging experiment, we finally use our method to achieve
deformable registration of abdominal 3d freehand us to a ct or mr volume.we are
using a heterogeneous dataset of 27 cases, comprising liver cancer patients and
healthy volunteers, different ultrasound machines, as well as optical vs."
3745,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"all 3d ultrasound data sets are accurately calibrated, with overall
system errors in the range of commercial ultrasound fusion options."
3746,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"the training is
unsupervised and merely requires unregistered data."
3747,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"to illustrate the efficiency of our proposed approach, we conduct
rigorous experiments on several real clinical datasets; the experimental results
reveal the advantages of our approach over several state-of-the-art ct
reconstruction methods."
3748,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,datasets.
3749,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"first, our proposed approaches are evaluated on the ""mayo-clinic
low-dose ct grand challenge"" (mayo-clinic) dataset of lung ct images [19].the
dataset contains 2250 two dimensional slices from 9 patients for training, and
the remaining 128 slices from 1 patient are reserved for testing."
3750,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"to evaluate the
generalization of our model, we also consider another dataset rider with
nonsmall cell lung cancer under two ct scans [36] for testing."
3751,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"we randomly
select 4 patients with 1827 slices from the dataset."
3752,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"to evaluate the stability and generalization of our model and the
baselines trained on mayo-clinic dataset, we also test them on the rider
dataset."
3753,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"due to the bias in the datasets
collected from different facilities, the performances of all the models are
declined to some extents."
3754,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"the experimental results on real datasets demonstrate the
advantages of our proposed network over several popular approaches."
3755,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"training deep learning models for medical
applications often needs new data."
3756,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"this was not the case for noise2aliasing, and
historical clinical data sufficed for training.we validated our method on
publicly available data [15] against a supervised approach [6] and applied it to
an internal clinical dataset of 30 lung cancer patients."
3757,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"we explore different
dataset sizes to understand their effects on the reconstructed images."
3758,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"let j 1 , j 2 be two random variables
that pick different subsets at random belonging to a partition of j , andbe the
input-target pairs in dataset d of reconstructions using disjoint subsets of
noisy projections."
3759,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"this means that, in our dataset,
we should have at our disposal reconstructions of the same underlying volume x
using disjoint subsets of projections."
3760,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"first, we used the spare varian dataset to study whether noise2aliasing can
match the performance of the supervised baseline and if it can outperform it
when adding noise to the projections."
3761,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"then, we use the internal dataset to
explore the requirements for the method to be applied to an existing clinical
dataset."
3762,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"given two volumes (x, y), the
training pairs (x i (k) , y i (k) ) are the same i-th slice along the k-th
dimension of each volume chosen to be the axial plane.the datasets used in this
study are two:1."
3763,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"the spare varian dataset was used to provide performance
results on publicly available patient data."
3764,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"to more closely resemble normal
respiratory motion per projection image, the 8 min scan has been used from each
patient (five such scans are available in the dataset)."
3765,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"the hyperparameters are
optimized over the training dataset.2."
3766,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"an internal dataset (irb approved) of 30
lung cancer patients' 4dcbcts from 2020 to 2022, originally used for igrt, with
25 patients for training and 5 patients for testing."
3767,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"the
data were anonymized prior to analysis.projection noise was added using the
poisson distribution to the spare varian dataset to evaluate the ability of the
unsupervised method to reduce it."
3768,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"for the spare varian dataset, we use the rois defined provided
[15] and used the 3d reconstruction using all the projections available as a
ground truth."
3769,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"for the internal dataset, we deformed the planning ct to each of
the phases reconstructed using the fdk algorithm and evaluate the metric over
only the 4dcbct volume boundaries."
3770,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"1,
noise2aliasing matches the visual quality of the supervised approach on the
low-noise dataset on both soft tissue and bones."
3771,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"1 and table 1, the supervised approach reproduces the noise
that was seen during training, while noise2aliasing manages to remove it
consistently, outperforming the supervised approach, especially in the soft
tissue area around the lungs, where the noise affects attenuation coefficients
the most.noise2aliasing is capable of reducing the artifacts present in
reconstructions caused by stochastic noise in the projections used,
outperforming the supervised baseline.internal dataset."
3772,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"reconstruction using
noise2aliasing with different-sized datasets."
3773,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"however, the model also tends to remove small
anatomical structures as high-frequency objects that cannot be distinguished
from the noise.when applied to a clinical dataset, noise2aliasing benefits from
more patients being included in the dataset, however, qualitatively good
performance is already achieved with 5 patients."
3774,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"no additional data collection
was required and the method can be applied without major changes to the current
clinical practice."
3775,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"we have empirically demonstrated its performance on a publicly available
dataset and on an internal clinical dataset."
3776,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"noise2aliasing can be trained on
existing historical datasets and does not require changing current clinical
practices."
3777,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"the method removes noise more reliably when the dataset size is
increased, however further analysis is required to establish a good quantitative
measurement of this phenomenon."
3778,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,"without any physical
constraints, these methods can be unstable and extremely data-hungry."
3779,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,"as a typical inverse problem, pet image reconstruction can be modeled in a
variational form and cast as an optimization task, as follows:where y is the
measured sinogram data, y is the mean of the measured sinogram.x is the pet
activity image to be reconstructed, l(y|x) is the poisson loglikelihood of
measured sinogram data."
3780,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,"a ∈ r i×j is the system response matrix, with a ij representing the
probabilities of detecting an emission from voxel j at detector i.we expect that
the parameter θ in penalty term p can be learned from the training data like
many other deep unrolling methods."
3781,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,"we choose to parameterize p as the l 2,1 norm with a feature extraction operator
g(x) to be learned in the training data."
3782,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,"subsequently, we perform a pet scan to obtain the sinogram data of x tr and x t
."
3783,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,"for sinogram domain loss l measure , the data argumentation with
random noise ξ is performed on y: we implemented dulda using pytorch 1.7 on a
nvidia geforce gtx titan x."
3784,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,"to test the robustness of proposed dulda, we forward-project one patient brain
image data with different dose level and reconstructed it with the trained dulda
model."
3785,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,"the real image has very
different cortex structure and some deflection compared with the training data."
3786,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,datasets.
3787,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,"two widely-used public ct image datasets, 3d-ircadb [5] and pancreas
[5] we augment the data by rotation and flipping first and then randomly crop
them to 128 × 128 patches."
3788,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,"we compare the performance of our proposed method with other state-of-theart
methods, including bicubic interpolation [16], dan [11], realsr [15], spsr [21],
aid-srgan [12] and jdnsr [10].figure 2 shows the qualitative comparison results
on the 3d-ircadb dataset with the scale factor of 2."
3789,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,"figure 3 shows the
qualitative comparison results on the pancreas dataset with the scale factor of
4."
3790,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,"therefore, our method reconstructs more detailed results than other
methods.table 2 shows the quantitative comparison results of different
state-of-theart methods with two scale factors on two datasets."
3791,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,"for the
3d-ircadb and pancreas datasets, our method outperforms the second-best methods
1.6896/0.0157 and 1.7325/0.0187 on psnr/ssim with the scale factor of 2
respectively."
3792,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,"the experiments compared with 6
state-ofthe-art methods on 2 public datasets demonstrate the superiority of our
method."
3793,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"deep registration methods learn a mapping from image pairs
to spatial transformations based on training data in an unsupervised manner,
which have shown advantages in registration accuracy and computational
efficiency [7][8][9][10][11][12][13][14][15][16][17][18].many deep registration
methods perform coarse-to-fine registration to improve registration accuracy,
where the registration is decoupled into multiple coarse-to-fine registration
steps that are iteratively performed by using multiple cascaded networks
[10][11][12][13] or repeatedly running a single network for multiple iterations
[14,15]."
3794,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"extensive experiments with seven public datasets show that our
nice-trans outperforms state-of-the-art registration methods on both
registration accuracy and runtime."
3795,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"we followed the dataset settings
in [18]: 2,656 brain mri images acquired from four public datasets (adni [27],
abide [28], adhd [29], and ixi [30]) were used for training; two public brain
mri datasets with anatomical segmentation (mindboggle [31] and buckner [32])
were used for validation and testing."
3796,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"the mindboggle dataset contains 100 mri
images and were randomly split into 50/50 images for validation/testing."
3797,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"the
buckner dataset contains 40 mri images and were used for testing only."
3798,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"in
addition to the original settings of [18], we adopted an additional public brain
mri dataset (lpba [33]) for testing, which contains 40 mri images.we performed
brain extraction and intensity normalization for each mri image with freesurfer
[32]."
3799,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"at each iteration, two
images were randomly picked from the training data as the fixed and moving
images."
3800,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"a total of 100 image pairs, randomly picked from the validation data,
were used to monitor the training process and to optimize hyper-parameters."
3801,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"bold: the best dsc and njd in each testing dataset and the shortest runtime of
completing both affine and deformable registration."
3802,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,bold: the best dsc and njd in each testing dataset.
3803,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,"3 presents our current results on ex
vivo porcine data, and finally, we conclude in sect."
3804,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,"clips were created by sliding a window of 7
frames (corresponding to a value of k = 2) with a stride of 1 over each
continuous sequence, yielding a data set that contains a total of 13734 clips."
3805,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,"it is clear that our data mostly contains
rotations, in particular over the axis x."
3806,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,"the
number of heatmaps m and the frame jump k were experimentally chosen among 0, 2,
4, 6.the data was split into train, validation and test sets by a ratio of
7:1.5:1.5."
3807,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,"the model with
the best performance on the validation data was selected and used for the
testing."
3808,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,"the test data was used to evaluate our method, it contains 2060 clips over which
our method achieved a translation error of translation of 0.449 ± 0.189 mm, and
an orientation error of orientation 1.3 ± 1.5 • ."
3809,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,"both state-of-the-art
methods use imu sensor data as additional input to estimate the relative
transformation between two relative frames."
3810,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,"as the table 1 shows, our method is comparable with
state-of-the-art methods in terms of drift errors without using any imu and with
non-linear probe motion as one may notice in our data distribution in the fig."
3811,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,"our method does not
use any additional sensor data and is based on a siamese architecture that
leverages the ultrasound image features and the optical flow to estimate
relative transformations."
3812,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,"our method was evaluated on ex vivo porcine data and
achieved translation and orientation errors of 0.449±0.189 mm and 1.3±1.5 •
respectively with a fair drift error."
3813,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,"recent generative adversarial networks (gans) and variants, e.g., mm-gan [23],
diamondgan [13] and provogan [30], have been successful based on multi-modal
mri, further improved by introducing multi-modal coding [31], enhanced
architecture [7], and novel learning strategies [29].despite the success,
gan-based models are challenged by the limited capability of adversarial
learning in modelling complex multi-modal data distributions [25] recent studies
have demonstrated that gans' performance can be limited to processing and
generating data with less variability [1]."
3814,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,datasets and baselines.
3815,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,"we evaluated cola-diff on two multi-contrast brain mri
datasets: brats 2018 and ixi datasets."
3816,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,"the ixi1 dataset consists of 200 multi-contrast mris from healthy
brains, plit them into (140:25:35) for training/validation/testing."
3817,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,"seven cases were tested
in two datasets (table 1)."
3818,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,"our results show that each component contributes to the
performance improvement, with auto-weight adaptation bringing a psnr increase of
1.9450db and ssim of 4.0808%.to test the generalizability of cola-diff under the
condition of varied inputs, we performed the task of generating t2 on two
datasets with progressively increasing input modalities (table 2 bottom)."
3819,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,"as a result, such methods are
unsuitable for mri sr, as it is challenging to obtain paired training data that
cover the variability in acquisition protocols and resolution of clinical brain
mri scans across institutions [14].building image priors through generative
models has recently become a popular approach in the field of image sr, for both
computer vision [1,2,7,17,19] as well as medical imaging [18,25], as they do not
require re-training in the presence of several types of input distribution
shifts."
3820,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,"while the ldm model was
trained on uk biobank, we demonstrate our methods on an external dataset (ixi)
which was inaccessible to the pre-trained generative model."
3821,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,"however, these methods require paired data to train, which is hard to
acquire because of the large variability present in clinical mris [14,23]."
3822,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,"an encoder e maps each
highresolution t1w brain mri x ∼ p data (x) into a latent vector z 0 = e(x) of
size 20 × 28 × 20."
3823,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,"for low sparsity mri sr, we directly find the optimal latent code z * t using
the decoder d: dataset for validation: we use 100 hr t1 mris from the ixi
dataset
(http://brain-development.org/ixi-dataset/) to validate our method, after
filtering out those scans where registration failed."
3824,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,"we note that subjects in
the ixi dataset are around 10 years younger on average than those in uk
biobank.the mri scans from uk biobank also had the faces masked out, while the
scans from ixi did not."
3825,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,"qualitative results of applying our method on tumour and lesion filling are
available in the supplementary material.table 1 shows quantitative results on
100 hr t1 scans from the ixi dataset, which the brain ldm did not have access to
during training."
3826,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,"we validated our method on 100 brain t1w mris from the
ixi dataset through slice imputation using input scans of 4 and 8 mm slice
thickness, and compared our method with cubic interpolation and unires
[3].experimental results have shown that our approach achieves superior
performance compared to the unsupervised baselines, and could create smooth hr
images with fine detail even on an external dataset (ixi)."
3827,Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,"furthermore, we constructed a new
ultra-high resolution ct scan dataset obtained with the most advanced ct
machines."
3828,Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,"the dataset contained 87 uhrct scans with a spatial resolution of
0.34×0.34 mm 2 and an image size of 1024×1024."
3829,Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,"to avoid non-derivative operations in image enhancement, we proposed a
novel enhancement module consisting of lightweight convolutional layers to
replace the filtering operation for faster and easier back-propagation in
structural domain optimization.3) we established an ultra-high-resolution ct
scan dataset with a spatial resolution of 0.34 × 0.34 mm 2 and an image size of
1024 × 1024 for training and testing the sr task."
3830,Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,"the loss function is then modified
asthe total objective function is the sum of two losses.3 experiments and
conclusion we constructed three datasets for framework training and evaluation;
two of them
were in-house data collected from two ct scanners(the ethics number is
20220359), and the other was the public luna16 dataset [22]."
3831,Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,"more details about
the two in-house datasets are described in the supplementary materials.we
evaluated our sr model on three ct datasets:• dataset 1: 2d super-resolution
from 256×256 to 1024×1024, with the spatial resolution from 1.36 × 1.36 mm 2 to
0.34 × 0.34 mm 2 .• dataset 2: 3d super-resolution from 256 × 256 × 1x to 512 ×
512 × 5x, with the spatial resolution from 1.60 × 1.60 × 5.00 mm 3 to 0.80 ×
0.80 × 1.00 mm 3 ."
3832,Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,"• dataset 3: 2d super-resolution from 256 × 256 to 512 × 512
on the luna16 dataset.we compare our model with other sota super-resolution
methods, including bicubic interpolation, srcnn [7], srresnet [6], cycle-gan
[2], and sr3 [12]."
3833,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"by contrast, we can handle much
longer ones and provide a broad evaluation in a real dataset (c3vd) over
multiple sequences."
3834,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"hence, we can write the color of the corresponding pixel as
[3]:where l e is the radiance emitted by the light source to the surface point,
that was modeled and calibrated in the endomapper dataset [1] according to the
sls model from [16]."
3835,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"thus, our
photometric loss is computed using a normalized image:(5) we validate our method
on the c3vd dataset [4], which covers all different
sections of the colon anatomy in 22 video sequences."
3836,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"this dataset contains
sequences recorded with a medical video colonoscope, olympus evis exera iii
cf-hq190l."
3837,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"the gain values were easily estimated
from the dataset itself."
3838,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"for vignetting, we use the calibration obtained from a
colonoscope of the same brand and series from the endomapper dataset [1].during
training, we follow the neus paper approach of using a few informative frames
per scene, as separated as possible, by sampling each video uniformly."
3839,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"since the c3vd dataset comprises a ground-truth
triangle mesh, we compute point-to-triangle distances from all the vertices in
the reconstruction to the closest ground-truth triangle.in the first rows of
table 1, we report median (medae), mean (mae), and root mean square (rmse)
values of these distances for all vertices seen in at least one image."
