Paper Title,Header Number,Header Title,Text,Volume
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,1.0,Introduction,"Recent years have seen a significant increase in the use of artificial intelligence (AI) in medical imaging, as evidenced by the rising number of academic publications and the accelerated approval of commercial AI applications for clinical use  Monitoring the performance of AI models in production systems is crucial to ensure safety and effectiveness in healthcare, particularly for medical imaging applications. The unrealistic expectation that input data and model performance will remain static indefinitely runs counter to decades of machine learning operations research, as outlined by extensive experience in AI model deployment for other verticals  In healthcare, the availability of real-time ground truth data is often limited, presenting a significant challenge to accurate and timely performance monitoring. This limitation renders many existing monitoring strategies inadequate, as they require access to contemporaneous ground truth labels. Moreover, existing solutions do not tackle the distinct challenges posed by monitoring medical imaging data, including both pixel and non-pixel data, as they are primarily designed for structured tabular data. Our challenge is then to develop a systematic approach to real-time monitoring of medical imaging AI models without contemporaneous ground truth labels. This gap in the current landscape of monitoring strategies is what our method aims to fill. In this manuscript, we present a solution that relies on only statistics of input data, deep-learning based pixel data representations, and output predictions. Our innovative approach goes beyond traditional methods and addresses this gap by not necessitating the use of up-to-date ground truth labels. Our framework is coupled with a novel multi-modal integration methodology for realtime monitoring of medical imaging AI systems for conditions which will likely have an adverse effect on performance. Through the solution proposed in this paper, we make a meaningful contribution to the medical imaging AI monitoring landscape, offering an approach specifically tailored to navigate the inherent constraints and challenges in the field.",vol3
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,2.1,Data and Deep Learning Model,"We test our medical imaging AI drift workflow using the CheXpert [9] and Pad-Chest  Our approach utilizes a Densely Connected Convolutional Neural Network [8], pretrained on frontal-only CheXpert data, then we fine-tuned only the final classifier layers of the model using PadChest frontal training data. To assess the performance of our classifier over a simulated production timeframe, we employ AUROC as an evaluation metric. This approach offers a definitive indication of any potential model drift, but it necessitates real-time, domain expert-labeled ground truth labels.",vol3
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,2.2,Data Stream Drift and Concordance,"Our approach differs from others in that we monitor for similarity or concordance of the datastream with respect to a reference dataset rather than highlighting differences. When the concordance metric decreases the degree to which the data has drifted has increased. Our method summarizes each exam in the datasteam into an embedding consisting of DICOM metadata, pixel features and model output which is sampled into temporal detection windows in order to compare distributions of individual features to a reference set using statistical tests. Our framework, though extensible, uses two statistical tests: 1) the Kolmogorov-Smirnov (K-S) test and 2) the chi-square (χ 2 ) goodness of fit test. The K-S test is a non-parametric test which measures distribution shift in a real-valued sample without assuming any specific distribution  Multi-Modal Embedding. To calculate statistics, each image must be embedded into a compressed representation suitable for our statistical tests. Our embedding is comprised of three categories: 1) DICOM metadata, 2) image appearance, and 2) model output (Fig.  The aim of live medical data stream monitoring is to ensure consistency, detect changes impacting model performance, and identify shifts in class distribution or visual representations. We utilize soft predictions (model raw score/activation) to monitor model output and detect subtle distribution changes that hard predictions may overlook, enhancing early detection capabilities.",vol3
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,,Metric Measurement and Unification,"Our framework constructs detection windows using a sliding window approach, where the temporal parameters dictate the duration and step size of each window. Specifically, the duration defines the length of time that each window covers, while the step size determines the amount of time between the start of one window and the start of the next. We use ψi (ω t ) = mt i to denote individual metrics calculated at time t from a ω t , and m[a,b] i to represent the collection of individual metric values from time a to b. To mitigate sample size sensitivity, we employ a bootstrap method. This involves repeatedly calculating metrics on fixed-size samples drawn with replacement from the detection window. We then average these repeated measures to yield a final, more robust metric value. Formally: where θ K collects K samples from ω with replacement and ψ i is the metric function calculated on the sample. There remains three main challenges to metric unification: 1) fluctuation normalization, 2) scale standardization, and 3) metric weighting. Fluctuation normalization and scale standardization are necessary to ensure that the metrics are compatible and can be meaningfully compared and aggregated. Without these steps, comparing or combining metrics could lead to misleading results due to the variations in the scale and distribution of different metrics. We address the first two challenges by utilizing a standardization function, Γ , which normalizes each individual metric into a numerical space with consistent upper and lower bounds across all metrics. This function serves to align the metric values so that they fall within a standard range, thereby eliminating the influence of extreme values or discrepancies in the original scales of the metrics. In our experiments, we apply a simple normalization function using scale (η) and offset factors (ζ), specifically: Γ (m) = m-ζ η . Metric weighting is used to reflect the relative importance or reliability of each metric in the final unified metric. The weights are determined through a separate process which takes into account factors such as the sensitivity and specificity of each metric. We then calculate our unified multi-modal concordance metric, MMC , on a detection window ω by aggregating individual metric values across L metrics using predefined weights, α i , for each metric, as follows: where ψi (ω) represents the ith metric calculated on detection window ω, Γ i represents the standardization function, and α i represents the weight used for the ith metric value. Each metric value is derived by a function that measures a specific property or characteristic of the detection window. For instance, one metric could measure the average intensity of the window, while another could measure the variability of intensities. By calculating MMC on a time-indexed detection window set Ω [a,b] , we obtain a robust multi-modal concordance measure that can monitor drift over the given time period from a to b, denoted as MMC [a,b] . This unified metric is advantageous as it provides a single, comprehensive measurement that takes into account multiple aspects of the data, making it easier to track and understand changes over time.",vol3
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,3.0,Experimentation,"Our framework is evaluated through three simulations, inspired by clinical scenarios, each involving a datastream modification to induce noticeable drift. All experiments share settings of a 30-day detection window, one-day stride, and parameters K = 2500 and N = 20 in Eq. 1. Windows with less than 150 exams are skipped. We use the reference set for generating Ω r and calculating η and ζ. Weights α i are calculated by augmenting Ω r with poor-performing samples. Scenario 1: Performance Degradation. We investigate if performance changes are detectable by inducing degradation through hard data mining. We compile a pool of difficult exams for the AI to classify by selecting exams with low model scores but positive ground truth for their label as well as high scoring negatives. Exams are chosen based on per-label quantiles of scores, with Q = 0.25 indicating the lowest 25% positives and highest 25% negatives are included. These difficult exams replace all other exams in each detection window at a given point in the datastream. Scenario 2: Metadata Filter Failure. In this scenario, we simulate a workflow failure, resulting in processing out-of-spec data, specifically lateral images, in contrast to model training on frontal images only. The datastream is modified at two points to include and then limit to lateral images. Scenario 3: No Metadata Available. The final experiment involves a nometadata scenario using the Pediatric Pneumonia Chest X-ray dataset [10], comprising of 5, 856 pediatric Chest X-rays. This simulates a drift scenario with a compliance boundary, relying solely on the input image. The stream is altered at two points to first include and then limit to out-of-spec data.",vol3
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,4.0,Results and Discussion,"The performance and concordance metrics of each experiment are visualized in Fig.  We start by discussing results in Fig.  Next, Fig.  Results of our final scenario, appear in Fig.  We demonstrate model monitoring for a medical imaging with CheXStray can achieve real-time drift metrics in the absence of contemporaneous ground truth in a chest X-ray model use case to inform potential change in model performance. This work will inform further development of automated medical imaging AI monitoring tools to ensure ongoing safety and quality in production to enable safe and effective AI adoption in medical practice. The important contributions include the use of VAE in reconstructing medical images for the purpose of detecting input data changes in the absence of ground truth labels, data-driven unsupervised drift detection statistical metrics that correlate with supervised drift detection approaches and ground truth performance, and open source code and datasets to optimize validation and reproducibility for the broader community.",vol3
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,1.0,Introduction,"Coronary computed tomographic angiography (CCTA) is a well-established noninvasive imaging modality to diagnose coronary artery disease (CAD) which is a common disease with increasing prevalence worldwide  Manual segmentation of coronary arteries is time-consuming and expensive. As a result, multiple traditional methods based on image processing techniques have been developed over the years to automatically or semi-automatically segment coronary arteries  In spite of reasonably good performance achieved by deep learning-based methods, there are still often some disconnected segments in segmentation results, which may be the mispredicted vessels, or may be caused by the presence of artifacts, stenosis, plaques, and occlusions  In this paper, we propose a topology-preserving scheme for the extraction of fully-connected coronary arteries, integrating image segmentation, centerline reconnection, and geometry reconstruction. Our major contributions are as follows: 1) We design a new centerline enhanced loss for coronary artery segmentation; 2) We propose the distance probability cosine (DP C) regularized walk algorithm to reconnect the broken centerlines; 3) We use a reconstruction method based on 2D level-set model and 3D implicit modeling technique to reconstruct vascular model along the stitched centerlines.",vol3
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,2.0,Method,As shown in Fig. ,vol3
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,2.1,Vascular Segmentation Based on NSDT Soft-ClDice Loss,"Shit et al.  where V L and V P represent true mask and real-valued probabilistic prediction, while S L , S P represent their skeletons, and NSDT L , NSDT P represent their normalized skeleton distance transform masks. • is the Hadamard product. The first and second parts of numerator in Eq. (  where γ is the balancing weight and set to 0.5 empirically.",vol3
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,2.2,Vascular Reconnection Based on DPC Walk,"To improve the connectivity of the segmented coronary arteries, we propose the DP C walk algorithm to reconnect the disconnected vascular centerlines. First, centerlines are extracted from predictions. We refer to the centerlines of the two biggest connected components as V i (i = 1, 2, ...) and other broken centerlines as CL j (j = 1, 2, ...). V i consists of a sequence of points (p 1 , p 2 , ..., p m ) where p 1 represents the head and p m represents the tail, while CL j is represented by (q 1 , q 2 , ..., q n ). Then we select candidate branches for CL j by distance and direction. To connect CL j to candidate V i , we iteratively make locally optimal decisions based on distance (D), centerline probability (P ) and cosine similarity (C). The goal is to identify a potential path from the head of CL j to the tail of V i . In general, if the current point of walker is A, then its 26-connected neighbors are denoted as Θ = {A k |k = 1, 2, ..26}, which form a cube sized 3 × 3 × 3 (see Fig.  where D is the inverse distance between A k and p m , P is the probability that A k is on the centerline, and C is the cosine similarities between A k 's offset vector (o k ) and the offset vectors of the last two stitched points (o -1 , o -2 ). To obtain P , we first train a centerline binary classifier on mini patch level using cascade forest classifier (CFC)  The global direction is a vector from q 1 to p m , denoted as v. And the cosine similarity of candidate o k and v should be greater than 0. We then select the next point (A next ) with the largest DP C from the current point A (see Fig.  The reconnection process ends abnormally by steps beyond the limit or continuous low probabilities. If A arrives at p m , we then analyse the probability sequences and filter out unstable reconnections. Finally, we remove the vascular segments reconnected unsuccessfully and obtain reconnected centerlines.",vol3
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,2.3,Vascular Reconstruction Based on Level-Set Segmentation and Implicit Modeling,"The task of coronary reconstruction stage is to reconstruct the vascular model along the stitched centerlines and obtain a coronary artery tree with full connectivity. Firstly, we construct cross-section profiles perpendicular to the stitched centerlines, and extract the corresponding vessel contours using level-set model. We propose a Self-adaptive Local Hybrid level-set model, which can automatically calculate the dynamic threshold using the local region information to act as the lower bound of target object. The energy function is defined as follows:  where K σ (x) is a truncated Gaussian window sized (4k + 1) × (4k + 1). k is the largest integer smaller than the standard deviation. Secondly, the extracted vessel contours are represented as 2D Partial Shape-Preserving Spline (PSPS) functions ",vol3
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,3.1,Setup,"Dataset. We validate our approach on two datasets. The first dataset is public available from the MICCAI 2020 Automated Segmentation of Coronary Arteries (ASOCA) challenge Evaluation Metric. The quantitative results are reported using Dice similarity coefficient (Dice), Hausdorff Distance (HD) and Overlap (OV). Besides, Reconnection Accuracy (RecAcc) is defined to evaluate how successful DPC Walk is to reconnect the broken vessels by calculating directly on centerlines involved in reconnection and removal: where b and s represent the broken and stitched centerlines respectively. For the points on these centerlines, we label them by comparing the reconnected centerlines with ground truth. Similarly, we obtain Reconnection Sensitivity (RecSen) and Reconnection Specificity (RecSpe):",vol3
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,,1),We trained 3d_fullres version of nnU-Net ,vol3
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,3.3,Comparison with State-of-the-Art Methods,"We performed quantitative comparisons with some advanced deep learning-based segmentation methods on ASOCA and PDSCA as presented in Table  For ASOCA, our CorSegRec has better performance in Dice and HD compared with other methods using vanilla 2D or 3D U-Net, and nnU-Net  For PDSCA, our approach shows significant improvements in Dice and HD compared with ResU-Net ",vol3
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,3.4,Ablation Study,"Ablation Study on Components of DPC Walk. We conducted several experiments by combining different subsets of D, P , and C. As shown in Table ",vol3
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,4.0,Conclusion,"In this paper, we present a new topology-preserving scheme called CorSegRec to extract fully-connected coronary arteries from CCTA. Our approach combines coronary artery segmentation, centerline reconnection, and coronary model reconstruction. Notably, the proposed DPC Walk algorithm can remove most false positive segments and effectively track and connect some arteries hardto-segment. Experiment results on two CCTA datasets demonstrate that our method can achieve better connected and more accurate coronary artery extraction compared to other methods.",vol3
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,1.0,Introduction,"Semantic segmentation is a fundamental task in medical imaging used to delineate regions of interest, and has been applied extensively in diagnostic radiology. Recently, deep learning methods that use a dense probability map to classify each pixel such as UNet  To address this problem in segmentation networks,  With this in mind, we return to HybridGNet which efficiently optimises points directly and theorise about the causes of the performance gap relative to dense segmentation models. We identify that describing segmentation contours using points is a sub-optimal approach because (1) points are an incomplete representation of the segmentation map; (2) the supervisory signal is usually weaker (n distances are calculated from n pairs of points, versus, h x w distances for pairs of dense probability maps); (3) the distance from the contour is more meaningful than the distance from the points representing the contour, hence minimising the point-wise distance can lead to predictions which fall on the contour being penalised.",vol3
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,,Contributions:,"We propose a novel joint architecture and contour loss to address this problem that leverages the benefits of both point and dense approaches. First, we combine image features from an encoder trained using a point-wise distance with image features from a decoder trained using a pixel-level objective. Our motivation is that contrasting training strategies enable diverse image features to be encoded which are highly detailed, discriminative and semantically rich when combined. Our joint learning strategy benefits from the segmentation accuracy of dense-based approaches, but without topological errors that regularly afflict models trained using a pixel-level loss. Second, we propose a novel hybrid contour distance (HCD) loss which biases the distance field towards pre-dictions that fall on the contour boundary using a sampled unsigned distance function which is fully differentiable and computationally efficient. To our knowledge this is the first time unsigned distance fields have been applied to graph segmentation tasks in this way. Our approach is able to generate highly plausible and accurate contour predictions with lower HD and higher DS/JC scores than a variety of dense and graph-based segmentation baselines.",vol3
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,2.1,Network Design,"We implement an architecture consisting of two networks, a Dense-Graph (DG) network and a Dense-Dense (DD) network, as shown in Fig. ",vol3
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,2.2,Graph Convolutional Network,"Our graph decoder passes features initialised from the VAE bottleneck through six Chebyshev spectral graph convolutional  where Θ (k) ∈ R fin × fout are learnable weights and σ is a ReLU activation function. Z (k)  is computed recursively such that Z (1) = X, Z (2) = L•Z (1) , 2) where X ∈ R N × fin are graph features, and L represents the scaled and normalized graph Laplacian ",vol3
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,2.3,Joint Dense-Point Learning,"As typical DG networks are trained with a point-wise distance loss and not a pixel-level loss, the image encoder is not directly optimised to learn clear and well-defined boundary features. This misalignment problem results in the DG encoder learning features pertinent to segmentation which are distinctively different from those learnt in DD encoders. This is characterised by activation peaks in different image regions such as the background and other non-boundary areas (see Fig. ",vol3
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,2.4,Hybrid Contour Distance,"Mean squared error (MSE) is a spatially symmetric loss which is agnostic to true contour borders. We alleviate this pitfall by designing an additional contouraware loss term that is sensitive to the border. To achieve this we precompute a 2D unsigned distance map S from the dense segmentation map for each class c (i.e. lungs, heart), where each position represents the normalised distance to the closest contour border of that class. Specifically, for a dense segmentation map M we use a Canny filter  During training, we sample S c as an additional supervisory signal using the predicted 2D point coordinates ŷi ∈ c, and combine with MSE with weight β. The effect of β is illustrated in Fig.  3 Experiments and Results",vol3
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,3.1,Datasets,We obtain four publicly available Chest X-ray segmentation datasets (JSRT ,vol3
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,3.2,Model Implementation and Training,"We implement our model in PyTorch and use PyTorch-Geometric for the graph layer. All models were trained for 2500 epochs using a NVIDIA A100 GPU from Queen Mary's Andrena HPC facility. For reliable performance estimates, all models and baselines were trained from scratch three times, the mean scores obtained for quantitative analysis and the median model used for qualitative analysis. Hyperparameters for all experiments were unchanged from ",vol3
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,3.3,Comparison to Existing Methods and Ablation Study,"We compare our approach to a variety of different dense-and point-based segmentation methods. First we validate our joint DD-DG learning approach by comparing to a DD-only segmentation network (UNet  To demonstrate the effectiveness of our HCD loss, we compare to our joint network trained with the contour term removed (MSE only). Our HCD loss is similar to differentiable polygon rasterization in BoundaryFormer  Tables ",vol3
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,4.0,Conclusion,"We proposed a novel segmentation architecture which leverage the benefits of both dense-and point-based algorithms to improve accuracy while reducing topological errors. Extensive experiments support our hypothesis that networks that utilise joint dense-point representations can encode more discriminative features which are both semantically rich and highly detailed. Limitations in segmentation methods using a point-wise distance were identified, and remedied with a new contour-aware loss function that offers an efficient alternative to differentiable rasterization methods. Our methodology can be applied to any graph segmentation network with a convolutional encoder that is optimised using a point-wise loss, and our experiments across four datasets demonstrate that our approach is generalizable to new data.",vol3
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,1.0,Introduction,"In digital pathology, nuclear segmentation and classification are crucial tasks in disease diagnosis. Because of the diverse nature (e.g., shape, size, and color) and large numbers of nuclei, nuclei analysis in whole-slide images (WSIs) is a challenging task for which computerized processing has become a de facto standard  For example, HoVer-Net  Data augmentation  The main motivation for this study stems from the recent advances in generative models. Recently, the denoising diffusion probabilistic model(DDPM)  In this study, we proposed a novel data augmentation technique using a conditioned diffusion model, DiffMix, for imbalanced nuclear pathology datasets. DiffMix consists of the following steps. First, we trained the SDM with semantic map guidance, which consists of instance and class-type maps. Next, we built custom label maps by modifying the existing imbalanced label maps. We changed the nuclei labels and randomly shifted the locations of the nuclei mask to ensure that the number of each class label was balanced and the data distribution expanded. Finally, we synthesized more diverse, semantically realistic, and well-balanced new pathology nuclei images using SDM conditioned on custom label maps. The main contributions of this study are summarized as follows: -We introduce a data augmentation framework for imbalanced pathology image datasets that can generate realistic samples using semantic diffusion model conditioned on two custom label maps, which can enlarge the data distribution. -We demonstrate the efficacy and generalization ability of our scheme with experimental results on two imbalanced pathology nuclei datasets, GLySAC ",vol3
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,2.0,Method,"In this section, the proposed method is described in detail. DiffMix operates through several steps. First, we trained the SDM first on the training data. Balancing label maps comprise several rare class labels, and enlarging label maps are composed of randomly shifted nuclei. Finally, using the pre-trained SDM and custom label maps, we synthesized realistic data to train on imbalanced datasets. Before discussing DiffMix, a brief introduction of SDM is presented. An overview of the proposed method is presented in Fig. ",vol3
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,2.1,Preliminaries,"The SDM is a conditional denoising diffusion probabilistic model (CDPM) conditioned on semantic label maps. Based on the CDPM, SDM follows two fundamental diffusion processes i.e., forward and reverse process. The reverse process was a Markov chain with Gaussian transitions. When the added noise is sufficiently large, the reverse process is approximated by a random variable y T ∼ N (0, I), defined as follows: The forward process implements Gaussian noise addition for T timesteps based on variance schedule {β 1 , ...β T } as follows: For α t := 1β t and ᾱt := t s=1 α s , we can write the marginal distribution as follows, q(y t |y 0 ) = N (y t ; The conditional DDPM was optimized to minimize the negative log-likelihood of the data for the particular input and condition information. If noise in the data follows Gaussian distribution with a diagonal covariance matrix Σ θ (y t , x, t) = σ t I, denoising can be the optimization target by removing the noise assumed to be present in data as follows,",vol3
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,2.2,Semantic Diffusion Model (SDM),"The SDM is a U-Net-based network that estimates noise from a noisy input image. Contrary to other conditional DDPMs, the denoising network of the SDM processes the semantic label map x and noisy input y t independently. When y t is fed into the encoder, x is injected into the decoder to fully leverage semantic information  During the sampling process, the disentangled component s is increased to improve the samples from the conditional diffusion models, formulated as follows:",vol3
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,2.3,Custom Semantic Label Maps Generation,Figure ,vol3
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,2.4,Image Synthesis,We synthesized the virtual data with a pretrained SDM conditioned on custom semantic label maps x. Figure  3 Experiments,vol3
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,3.1,Datasets,"In this study, we used two imbalanced nuclear segmentation and classification datasets. First, GLySAC ",vol3
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,3.2,Implementation Details,"We used an NVIDIA RTX A6000 to train the SDM for 10000epochs. For data synthesis, we implemented a DDIM-based diffusion process from 1000 to 100 and added noise to the input image to the SDM, setting T as 55. In our scheme, ∅ is defined as the all-zero vector, as in ",vol3
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,3.3,Results,Figure ,vol3
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,4.0,Conclusion,"In this study, we introduced DiffMix, a semantic diffusion model-based data augmentation framework for imbalanced pathology nuclei datasets. We experimentally demonstrated that our method can synthesize virtual data that can balance and enlarge imbalanced nuclear pathology datasets. Our method also outperformed the state-of-the-art GradMix in terms of qualitative and quantitative comparisons. Moreover, DiffMix enhances the segmentation and classification performance of two state-of-the-art networks, HoVer-Net and SONNET, even in imbalanced datasets, such as CoNSeP. Our results suggest that DiffMix can be used to improve the performance of medical image processing tasks in various applications. In the future, we plan to improve the performance of the diffusion model to generate various pathological tissue types.",vol3
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_33.,vol3
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,1.0,Introduction,"For the past several years, in skin disease diagnosis, deep learning (DL) techniques have been extensively studied, due to its effectiveness and outstanding diagnostic performance  We propose to develop a personalized diagnosis network, called APD-Net in the FL framework to target personalized diagnostics. Our APD-Net comprises two novel techniques, including (1) a genetic algorithm (GA)-based fine-tuning method and (2) a dual-pipeline (DP) architecture for the DL models. The GAbased fine-tuning method improves APD-Net on each edge device by adaptively customizing the optimized DL model. We validated our framework on three public datasets, including 7pt  Experimental results demonstrated that the APD-Net yielded outstanding performance, compared with other comparison methods, and achieved adaptively personalized diagnosis. The contributions of this paper are three-fold: • We developed a mobile-and FL-based learning (APD-Net) for skin disease diagnosis and achieved superior performance on skin disease diagnosis for public and custom datasets. • We introduce a customized GA for APD-Net, combined with a corresponding network architecture, resulting in improved personalized diagnostic performance as well as faster prediction time.  This work aims to develop a mobile-based FL system that can provide a personalized and customized diagnosis to patients across different clients. The APD-Net framework, shown in Fig. ",vol3
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,2.0,Methodology,"The detailed procedure to fine-tune APD-Net is described in Fig.  p k , here chromosome) is achieved, where i = 1, 2, 3, ..., N , and N is the number of populations. The evolutionary operations, including crossover and mutation, then offer a new 4N number of chromosomes. Here, the fitness scores are compared for all individual chromosomes, and the N number of chromosomes that achieve a high fitness score is selected to form a new population. Subsequently, the fitness score is calculated using the DP architecture, as illustrated in Fig. ",vol3
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,2.1,Dual-Pipeline (DP) Architecture for APD-Net,"Since the proposed FL system is implemented in a mobile-based environment, MobileNetV3 is employed as a baseline network of APD-Net. In particular, to achieve faster fine-tuning time, MobileNetV3-small is utilized. Here, the novel architecture of APD-Net is differentiated by the use of the DP architecture that allows (1) to diagnose patients adaptively, and (2) to evaluate the fitness function. Therefore, in the DP architecture, one pipeline employs the generalized parameters (P g ) from the FL server, whereas the other pipeline employs the personalized parameters (P * p k ) in the client.",vol3
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,2.2,Customized Genetic Algorithm,"In the FL environment, data privacy is achieved by transferring gradients without sharing data. Therefore, since the domain of one client is not recognizable by another client, the domain gap between two clients is not computable. Therefore, in the proposed FL system, to adaptively fine-tune the parameters transferred from the FL server to be personalized concerning the domain of one client, the GA is employed. The GA is the optimal solution for adaptively personalized diagnosis in the FL environment, since it heuristically searches for another local minimum point regardless of the domain gaps. The detailed procedures of the GA are illustrated in Algorithm I (Appendix).",vol3
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,Gene and Chromosome.,"A gene and a chromosome are modeled to represent fine-tuned parameters (P (i) f ) by jointly using P g and P * f . The gene indicates the internal division between P g and P * p . Figure  k ∈ [-1, 1] be a k th gene in the i th chromosome, and then the k th convolution weight (P where P g | k and P * p | k are the k th convolution parameter in P g and P * f , respectively. Here, is calculated by the internal division between P g and P * p . Crossover. Let crossover (P (i) , P (j) ) be a crossover function by jointly using two chromosomes, and then the k th genes of P (i) | k and P (j) | k are changed in the 50% probability when the constraint of Here, since the convolution parameters in a deep depth are rarely fine-tuned due to a gradient vanishing problem, l k exhibits a relatively larger value when k becomes larger. In addition, since we experimentally demonstrated that l k ≥ 0.15 provides a much longer time to fine-tune APD-Net, we constrained l k ≤ 0.15, and the fixed values of l k are randomly determined for each experiment. Mutation. mutation (P (i) ) represents a mutation function onto a chromosome, of which the k th gene is P (i) | k , such that it is defined as follows: ( where η is the randomly selected value in the constraint range for each individual gene. While training the DL model, we experimentally verified that the convolution weights are changed within the range of the maximum 0.2%. Therefore, here, μ is initially determined as 2e-3 (0.2%), but it depends on the variance of convolution weights in every epoch. Selection. As illustrated in Algorithm I, the newly generated chromosomes, which yield a large value of the fitness score, are contained in a new population. In APD-Net, the fitness score is evaluated by the fitness function that is jointly utilized in the architecture of APD-Net as illustrated in Fig.  where sim(x, y) is cosine similarity between x and y, and exp(x) is the exponential function. Note that APD-Net provides the fitness function related to its architecture, and the fitness function is more reliable than other fitness functions used in accuracy-based GA. Therefore, the APD-Net with our GA offers high accuracy in both the conventional diagnosis for overall patients and the personalized diagnosis for each patient at a specific client.",vol3
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,2.3,Training and Fine-Tuning APD-Net,"To summarize, (1) APD-Net is initially trained in the FL server using gradients from many clients. The cross-entropy loss function is utilized for training APD-Net, and the generally optimized parameter (P g ) is achieved in the initial training. (2) P g is then transferred to each client, and (3) the fine-tuned procedures are processed by jointly using P g and a personalized parameter (P * p ). By using the proposed GA, the new population of fine-tuned parameters (P (i) f where i = 1, 2, ..., 4N ) is generated, and the chromosome with the highest fitness score becomes a new personalized parameter. (4) After the diagnosis, the gradients are shared with the FL server, and P g is newly optimized.  3 Experimental Results",vol3
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,3.1,Experimental Setup,"Dataset. To evaluate the performance and feasibility of APD-Net, we used three public datasets, including 7pt, ISIC, and HAM, and detailed descriptions for datasets are illustrated in Table  To compensate for the limited number of images in the test set, a 4-fold cross-validation approach was employed. To assess the performance of the proposed network as well as compared networks, two distinct FL environments were considered: (1) an FL simulation environment to evaluate the performance of APD-Net and (2) a realistic FL environment to analyze the feasibility of APD-Net. For the FL simulation environment in Experiment I, public datasets were employed, and the distribution of samples was re-sampled using t-Distributed Stochastic Neighbor Embedding (t-SNE) ",vol3
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,3.2,Experiment I. FL Simulation,"Ablation Study. An ablation study was conducted to evaluate the impact of GA and DP on diagnostic performance. APD-GA and APD-GA-DP indicate APD-Net without GA and without GA and DP, respectively. APD-DP was not evaluated since GA could not be realized without the DP architecture. As illustrated in Table  Comparison Analysis. Performance of APD-Net was compared against those of the other DL models for adaptively personalized diagnosis. Table ",vol3
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,3.3,Experiment II. Realistic FL Environment,"To verify the feasibility of APD-Net, the performance of APD-Net was evaluated using the customized datasets acquired from our devices for adaptively personalized diagnosis. The performance of APD-Nets was compared against the other DL models. Since the prediction time is critical in the mobile-based environment, the prediction times of the DL models were compared in addition to the accuracy. As illustrated in Fig.  In addition, to verify the similarity as a fitness score, we examined the correlation between the similarity score and the prediction accuracy. The fitness score and accuracy were calculated corresponding to many input images and various versions of the fine-tuned parameters. Figure ",vol3
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,4.0,Conclusion,"In this work, we first carried out the adaptively personalized diagnosis task in the FL system, and developed a novel DL model, termed APD-Net, with the FL system. Our APD-Net yielded outstanding performance, by jointly using a novel DP architecture and a GA-based fine-tuning technique for adaptively personalized diagnosis. The DP architecture enabled extracting feature maps using generalized and personalized parameters, thereby providing high diagnostic accuracy. In addition, GA heuristically generated the best performance for the personalized parameters. Using the public skin datasets, including 7pt, ISIC, and HAM, our APD-Net was able to achieve an improved accuracy of 9.9% compared with other state-of-the-art DL models for the adaptively personalized diagnosis. The ablation study also demonstrated that the partial fine-tuning technique achieved higher accuracy as well as a faster fine-tuning time. Furthermore, the feasibility of APD-Net in FL was demonstrated by using the customized datasets acquired from our system, thus suggesting that the proposed system with APD-Net can be applied to the multiclass classification task of various skin diseases. However, achieving even faster prediction speeds is possible by incorporating computing performance enhancement methodologies from related fields or employing lightweight techniques like quantization, and it remains a future work.",vol3
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_37.,vol3
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,1.0,Introduction,"Domain shift (see Fig.  Unsupervised Domain Adaptation (UDA) refers to training the model with labeled source data and adapting it with target data without annotation  Test-Time Adaptation (TTA) adapts the target data or pre-trained models during testing  In this study, we propose a novel framework called Fourier TTA (FTTA) to enhance the model robustness. We believe that this is the first exploration of dual-adaptation design in TTA that jointly updates input and model for online refinement. Here, one assumption is that a well-adapted model will get consistent outputs for different transformations of the same image. Our contribution is twofold. First, we align the high-level features and attention regions of transformed paired images for complementary consistency at global and local dimensions. We adopt the Fourier -based input adaptation as the transformation strategy, which can reduce the distances between unseen testing images and the source domain, thus facilitating the model learning. We further propose to smooth the hard consistency via the weighted integration of features, thus reducing the adaptation difficulties of the model. Second, we employ self-consistency of frequency-based style interpolation to regularize the output logits. It can provide direct and effective hints to improve model robustness. Validated on three classification datasets, we demonstrate that FTTA is general in improving classification robustness, and achieves state-of-the-art results compared to other strong TTA methods. ",vol3
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,2.0,Methodology,"Figure  Fourier-Based Input Adaptation for Domain Transfer. Transferring unseen images to the known domain plays an important role in handling domain shift risks. In this study, instead of learning on multiple domains, we only have access to one single domain of data during training. Therefore, we need to utilize the limited information and find an effective way to realize the fast transfer from the unseen domain to the source domain. Inspired by  where M is the circular low-pass filtering with radius r to obtain the radialsymmetrical amplitude  Since one low-level amplitude represents one style, we have n style choices. n is the number of training data. The chosen styles for input adaptation should be representative of the source domain while having significant differences from each other. Hence, we use the validation set to select the styles by first turning the whole validation data into the n styles and calculating n accuracy. Then, styles for achieving top-k performance are considered representative, and L2 distances between the C 2 K pairs are computed to reflect the differences. Smooth Consistency for Global and Local Constraints. Building a reliable consistency measurement of paired inputs is the key to achieving TTA. In this study, we propose global and local alignments to provide a comprehensive consistency signal for tuning the model toward robustness. For global consistency, we compare the similarity between high-level features of paired inputs. These features encode rich semantic information and are therefore well-suited for assessing global consistency. Specifically, we utilize hard and soft feature alignments via pixel-level L2 loss and distribution-level cosine similarity loss, to accurately compute the global feature loss L f . To ensure local consistency, we compute the distances between the classification activation maps (CAMs) of the paired inputs. It is because CAMs (e.g., Grad-CAM  Despite global and local consistency using single paired images can provide effective self-supervised signals for TTA in most cases, they may be difficult or even fail in aligning the features with a serious gap during testing. This is because the representation ability of single-paired images is limited, and the hard consistency between them may cause learning and convergence difficulties. For example, the left-upper CAMs of c1 and c2 in Fig.  Style Consistency for Regularization on Logit Space. As described in the first half of Eq. 1, two low-level amplitudes (i.e., styles) can be linearly combined into a new one. We propose to use this frequency-based style consistency to regularize the model outputs in logit space, which is defined as the layer before softmax. Thus, it is directly related to the model prediction. A total of 8 logit pairs can be obtained (see Fig.  where x t and x ti,i∈1,2 are the testing image and two transformed images after input adaptation. x ij represents style-interpolated images controlled by λ j . y log (•) outputs the logits of the model.",vol3
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,3.0,Experimental Results,"Materials and Implementations. We validated the FTTA framework on three classification tasks, including one private dataset and two public datasets (see Fig.  We also perform ablation studies on the Fetal-17 dataset in the last 7 rows of Table  Figure ",vol3
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,4.0,Conclusion,"In this study, we proposed a novel and general FTTA framework to improve classification robustness. Based on Fourier-based input adaptation, FTTA is driven by the proposed multi-level consistency, including smooth global and local constraints, and also the self-consistency on logit space. Extensive experiments on three large datasets validate that FTTA is effective and efficient, achieving stateof-the-art results over strong TTA competitors. In the future, we will extend the FTTA to segmentation or object detection tasks.",vol3
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_22.,vol3
Performance Metrics for Probabilistic Ordinal Classifiers,1.0,Introduction and Related Work,"The output of predictive machine learning models is often presented as categorical values, i.e. ""hard"" class membership decisions. Nonetheless, understanding the faithfulness of the underlying probabilistic predictions giving rise to such hard class decisions can be essential in some critical applications. Meaningful probabilities enable not only high model accuracy, but also more reliable decisions: a doctor may choose to order further diagnostic tests if a binary classifier gives a p = 45% probability of disease, even if the hard prediction is ""healthy""  There is a large body of research around performance metrics for medical image analysis  How to measure the correctness of probabilistic predictions is a decades-old question, naturally connected to forecasting, i.e. predicting the future state of a complex system ",vol3
Performance Metrics for Probabilistic Ordinal Classifiers,,Relation to Calibration:,A popular approach to assess the quality of probabilistic predictions is measuring calibration. A model is well calibrated if its probabilistic predictions are aligned with its accuracy on average. PSRs and calibration are intertwined concepts: PSRs can be decomposed into a calibration and a resolution component  The two most widely adopted PSRs are the Brier and the Logarithmic Score ,vol3
Performance Metrics for Probabilistic Ordinal Classifiers,2.1,"Scoring Rules -Notation, Properties, Examples","We consider a K-class classification problem, and a classifier that takes an image x and maps it into a vector of probabilities p ∈ [0, 1] K . Typically, p is the result of applying a softmax operation on the output of a neural network. Suppose x belongs to class y ∈ {1, ..., K}, and denote by y its one-hot representation. A Scoring Rule (SR) S is any function taking the probabilistic prediction p and the label y and producing a number S(p, y) ∈ R (a score). Here we consider negatively oriented SRs, which assign lower values to better predictions. Of course, the above is an extremely generic definition, to which we must now attach additional properties in order to encode our understanding of what better predictions means for a particular problem. Property 1: A Scoring Rule (SR) is proper if its value is minimal when the probabilistic prediction coincides with the ground-truth in expectation. Example: The Brier Score  Since its value is always non-negative, and it decreases to 0 when p = y, we conclude that the Brier Score is indeed proper. Property 2: A Proper Scoring Rule (PSR) is local if its value only depends on the probability assigned to the correct category. Example: The Brier Score is non-local, as its value depends on the probability placed by the model on all classes. The Logarithmic Score  where c is the correct category of x, rewards the model by placing as much probability mass as possible in c, regardless of how the remaining probability is distributed. It is, therefore, a local PSR. The Logarithmic Score is also known, when taken on average over a dataset, as the Negative Log-Likelihood. Property 3: A PSR is sensitive to distance if its value takes into account the order of the categories, in such a way that probability placed in categories further away from the correct class is more heavily penalized. Example: Both the Brier and the Logarithmic scores are insensitive to distance (shuffling p and y won't affect the score). Sensitivity to distance is essential for assessing ordinal classifiers. Below we define the Ranked Probability Score (RPS) ",vol3
Performance Metrics for Probabilistic Ordinal Classifiers,2.2,The Ranked Probability Score for Ordinal Classification,"Consider a test sample (x, y) in a 3-class classification problem, with label y and two probabilistic predictions p 1 , p 2 : In this scenario, both the Brier and the Logarithmic scores produce the same penalty for each prediction, whereas a user might prefer p 1 over p 2 due to the latter assigning more probability to the second category. Indeed, if we use the arg-max operator to generate a hard-decision for this sample, we will obtain a prediction of class 2 and class 3 respectively, which could result in the second model declaring a patient as severely unhealthy with serious consequences. In this context, we would like to have a PSR that takes into account distance to the true category, such as the Ranked Probability Score (RPS,  The RPS is the squared 2 distance between the cumulative distributions Y of the target label y and P of the probabilistic prediction p, discounting their last component (as they are both always one) and normalizing so that it varies in the unit interval. In the above example, the RPS would give for each prediction a penalty of RPS(p 1 , y) = 1 /8, RPS(p 2 , y) = 1 /4, as shown in Fig. ",vol3
Performance Metrics for Probabilistic Ordinal Classifiers,2.3,The Squared Absolute RPS,"Our second goal in this paper is to identify and then fix certain failure modes of the RPS that might lead to counter-intuitive behaviors. First, in disease grading and other ordinal classification problems it is customary to assign penalties to mistakes that grow quadratically with the distance to the correct category. This is the reason why most works utilize the Quadratic-Weighted Kappa Score (QWK) instead of the linearly weighted version of this metric. However, the RPS increases the penalty linearly, as can be quickly seen with a simple 3-class problem and an example (x 1 , y 1 ) of class 1 (y 1 = [ 1, 0, 0 ]): Also, the RPS has a hidden preference for symmetric predictions. To see this, consider a second example (x 2 , y 2 ) in which the correct category is now the middle one (y 2 = [ 0, 1, 0 ]), and two probabilistic predictions: p sym = [ 3/10, 4/10, 3/10 ], p asym = [ 1/10, 5/10, 9/10 ]. In principle, there is no reason to prefer p sym over p asym , unless certain prior/domain knowledge tells us that symmetry is a desirable property. In this particular case, p asym is actually more confident on the correct class than p sym , which is however the preferred prediction for the RPS: RPS([ 0.30, 0.40, 0.30 ], y 2 ) = 0.09 < 0.1025 = RPS([ 0.45, 0.50, 0.05 ], y 2 ). (6) Fig.  In order to address these aspects of the conventional RPS, we propose to implement instead the Squared Absolute RPS (sa-RPS), given by: Replacing the inner square in Eq. ( ",vol3
Performance Metrics for Probabilistic Ordinal Classifiers,2.4,Evaluating Evaluation Metrics,"Our third goal is to demonstrate how the (sa-)RPS is useful for evaluating probabilistic ordinal predictions. In the next section we will show some illustrative examples that qualitatively demonstrate its superiority over the Brier and logarithmic score. However, it is hard to quantitatively make the case for one performance metric over another, since metrics themselves are what quantify modeling success. We proceed as follows: we first train a neural network to solve a biomedical image grading problem. We generate probabilistic predictions on the test set and apply distance sensitive metrics to (arg-maxed) hard predictions (QWK and EC, as recommended in  Here it is important to stress that, contrary to conventional metrics (like accuracy, QWK, or ECE) PSRs can act on an individual datum, without averaging over sets of samples. We exploit this property to design the following experiment: we sort the probabilistic predictions of the test set according to a score S, and then progressively remove samples that are of worst quality according to S. We take the arg-max on the remaining probabilistic predictions and compute QWK and EC. If S prefers better ordinal predictions, we must see a performance increase on that subset. We repeat this process, each time removing more of the worse samples, and graph the evolution of QWK and EC for different scores S: a better score should result in a faster QWK/EC-improving trend. Lastly, in order to derive a single number to measure performance, we compute the area under the remaining samples vs QWK/EC curve, which we call Area under the Retained Samples Curve (AURSC). In summary: What we expect to see: As we remove test set samples considered as worse classified by RPS, we expect to more quickly improve QWK/EC on the resulting subsets. We measure this with the Area under the Retained Samples Curve (AURSC)",vol3
Performance Metrics for Probabilistic Ordinal Classifiers,3.0,Experimental Results,"We now give a description of the data we used for experimentation, analyze performance for each considered problem, and close with a discussion of results.",vol3
Performance Metrics for Probabilistic Ordinal Classifiers,3.1,Datasets and Architecture,Our experiments are on two different medical image grading tasks: 1) the TMED-v2 dataset (  We train a ConvNeXt ,vol3
Performance Metrics for Probabilistic Ordinal Classifiers,3.2,How is RPS Useful? Qualitative Error Analysis,"The obvious application of RPS would be to train better ordinal classification models. But beyond this, RPS also enables improved, fine-grained error analysis. Let us see this through a simple experiment. Since PSRs assess samples individually, we can sort our test set using RPS, NLL, and Brier score. The worst-scored items are what the model considers the wrongest probabilistic predictions. The result of sorting predictions on the Eyepacs test set with the Brier, Neg-Log and RPS rules is show on Fig. ",vol3
Performance Metrics for Probabilistic Ordinal Classifiers,4.0,Conclusion and Future Work,"We have shown that Proper Scoring Rules are useful tools for diagnosing probabilistic predictions, but the standard Brier and Logarithmic scores should not be preferred in ordinal classification problems like medical image grading. Instead, the Ranked Probability Score, popular in the forecasting community, should be favoured. We have also proposed sa-RPS, an extension of the RPS that can better handle some pathological cases. Future work will involve using the RPS to learn ordinal classifiers, and investigating its impact in calibration problems.",vol3
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,1.0,Introduction,"Recent years have witnessed the remarkable success of deep learning in medical image segmentation. However, although the performance of deep learning models even surpasses the accuracy of human exports on some segmentation tasks, two challenges still persist. (1) Different segmentation tasks are usually tackled separately by specialized networks (see Fig.  Several strategies have been attempted to address both challenges. First, multi-head networks (see Fig.  In this paper, we propose a prompt-driven Universal Segmentation model (UniSeg) to segment multiple organs, tumors, and vertebrae on 3D medical images with diverse modalities and domains. UniSeg contains a vision encoder, a fusion and selection (FUSE) module, and a prompt-driven decoder. The FUSE module is devised to generate the task-specific prompt, which enables the model to be 'aware' of the ongoing task (see Fig.  Our contributions are three-fold: ",vol3
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,2.1,Problem Definition,"Let {D 1 , D 2 , ..., D N } be N datasets. Here, j=1 represents that the i-th dataset has a total of n i image-label pairs, and X ij and Y ij are the image and the corresponding ground truth, respectively. Straightforwardly, N tasks can be completed by training N models on N datasets, respectively. This solution faces the issues of ( ",vol3
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,2.2,Encoder-Decoder Backbone,"The main architecture of UniSeg is based on nnUNet  32 , where C is the number of channels and D, H, and W are the depth, height, and width of the input, respectively. Symmetrically, in each stage of the decoder, the upsampling operation implemented by a transposed convolution layer is applied to the input feature map to improve its resolution and reduce its channel number. The upsampled feature map is concatenated with the output of the corresponding encoder stage and then fed to a convolutional block. After the decoder process, the output of each decoder stage is passed through a segmentation head to predict segmentation maps for deep supervision, which is governed by the sum of the Dice loss and cross-entropy loss. Note that the channel number of multi-scale segmentation maps is set to the maximum number of classes among all tasks.",vol3
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,2.3,Universal Prompt,"Following the simple idea that everything is correlated, we believe that the correlations among different segmentation tasks must exist undoubtedly, though they are ignored by DoDNet which uses a set of orthogonal and one-hot task codes. Considering the correlations among tasks are extremely hard to handcraft, we propose a learnable prompt called universal prompt to describe them and use that prompt to generate task prompts for all tasks, aiming to encourage interaction and fusion among different task prompts. We define the shape of the universal prompt as 32 , where N is the number of tasks.",vol3
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,2.4,Dynamic Task Prompt,"Before building a universal network, figuring out a way to make the model 'aware' of the ongoing task is a must. DoDNet adopts a one-hot vector to encode each task, and the CLIP-driven universal model  where F taski denotes the prompt features belonging to the i-th task, cat(, ) is a concatenation operation, f (•) denotes the feed forward process, and Split(•) N means splitting features along the channel to obtain N features with the same shape. Then, we select the target features, called task-specific prompt F tp , from {F task1 , F task2 , ..., F taskN } according to the ongoing task. Finally, we concatenate F and selected F tp as the decoder input. In this way, we introduce task-related prior information into the model, aiming to boost the training of the whole decoder rather than only the last few convolution layers.",vol3
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,2.5,Transfer Learning,"After training UniSeg on upstream datasets, we transfer the pre-trained encoderdecoder and randomly initialized segmentation heads to downstream tasks. The model is fine-tuned in a fully supervised manner to minimize the sum of the Dice loss and cross-entropy loss. 3 Experiments and Results",vol3
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,3.1,Datasets and Evaluation Metric,"Datasets. For this study, we collected 11 medical image segmentation datasets as the upstream dataset to train our UniSeg and single-task models. The Liver and Kidney datasets are from LiTS  Evaluation Metric. The Dice similarity coefficient (Dice) that measures the overlap region of the segmentation prediction and ground truth is employed to evaluate the segmentation performance. ",vol3
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,3.2,Implementation Details,Both pre-training on eleven upstream datasets and fine-tuning on two downstream datasets were implemented based on the nnUNet framework ,vol3
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,3.3,Results,"Comparing to Single-Task and Universal Models. Our UniSeg was compared with advanced single-task models and universal models. The former includes UNETR  Comparing to Other Pre-trained Models. We compared our UniSeg with advanced unsupervised pre-trained models, such as MG  Comparison of Different Variants. We attempted three UniSeg variants, including Fixed Prompt, Multiple Prompts, and UniSeg-T, as shown in Fig. ",vol3
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,4.0,Conclusion,"This study proposes a universal model called UniSeg (a single model) to perform multiple organs, tumors, and vertebrae segmentation on images with multiple modalities and domains. To solve two limitations existing in preview universal models, we design the universal prompt to describe correlations among all tasks and make the model 'aware' of the ongoing task early, boosting the training of the whole decoder instead of just the last few layers. Thanks to both designs, our UniSeg achieves superior performance on 11 upstream datasets and two downstream datasets, setting a new record. In our future work, we plan to design a universal model that can effectively process multiple dimensional data.",vol3
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_49.,vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,1.0,Introduction,"Deep learning (DL)  Perturbation-based anonymization approaches  scale. Parameter is used to determine the privacy budget (smaller values indicate greater privacy), while the m-neighborhood represents a sensitivity factor. However, one major drawback of perturbation-based anonymization is the potential degradation of image quality and an associated reduction in data utility. In recent years, DL has emerged as a prominent tool for anonymizing medical images. In this context, synthetic image generation with privacy guarantees is currently actively explored, aimed at creating fully anonymous medical image datasets  In this work, we aim to resolve the privacy-utility trade-off by proposing theto the best of our knowledge -first adversarial image anonymization approach for chest radiography data. Our proposed model architecture (PriCheXy-Net) is a composition of three independent neural networks that collectively allow for the learning of targeted image deformations to deceive a well-trained patient verification model. We apply our method to the publicly available ChestX-ray14 dataset ",vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,2.1,Data,We use X-ray data from the ChestX-ray14 dataset ,vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,2.2,PriCheXy-Net: Adversarial Image Anonymization,The proposed adversarial image anonymization approach is depicted in Fig. ,vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,U-Net. The U-Net,"( Factor μ controls the degree of deformation, with larger values allowing for more deformation. Note that the exclusive use of F id would result in the original image, i. e., x = F (x), assuming the deformation factor is being set to μ = 0. The resulting flow field F is Gaussian filtered (kernel size 9, σ = 2) to ensure smooth deformations in the final image. The corresponding parameters were selected manually in preliminary experiments. Auxiliary Classifier. To ensure the preservation of underlying abnormality patterns and image utility during deformation, PriCheXy-Net integrates an auxiliary classifier using CheXNet ",vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Patient Verification Network.,The incorporated patient verification model is represented by the SNN architecture presented by Packhäuser et al. ,vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,2.3,Objective Functions,"Similar to most adversarial models, our system undergoes training through the use of dual loss functions that guide the model towards opposing directions. To enforce the U-Net not to eliminate important class information while deforming a chest radiograph, we introduce the auxiliary classifier loss L aux (θ G , θ aux ) realized by the class-wise binary cross entropy (BCE) loss according to Eq. 2 where i represents one out of 14 abnormality classes. Conversely, to guide the U-Net with deceiving the incorporated patient verification model, we utilize its output as an additional verification loss term L ver (θ G , θ ver ) (see Eq. 3): The total loss to be minimized (see Eq. 4) results from the sum of the two partial losses L aux (θ G , θ aux ) and L ver (θ G , θ ver ): Lastly, both the auxiliary classifier and the verification model are updated by minimizing the loss terms in Eq. 5 and Eq. 6, respectively. Note that the similarity labels for positive and negative pairs are encoded using y v = 1 and y v = 0. 3 Experiments and Results",vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,3.1,Experimental Setup,"For all experiments, we used PyTorch (1.10.2) ",vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Pre-training of the Flow Field,"Generator. The incorporated U-Net architecture was pre-trained on an autoencoder-like reconstruction task for 200 epochs using the mean squared error (MSE) loss, Adam  Training of PriCheXy-Net. After pre-training, PriCheXy-Net was trained in an end-to-end fashion for 250 epochs using the Adam optimizer ",vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Re-training and Evaluation of the Verification Model.,"To assess the anonymization capability of PriCheXy-Net and to determine if the anonymized images can reliably deceive the verification model, we re-trained the incorporated SNN for each model configuration by using deformed images only. We then simulated multiple linkage attacks by comparing deformed images with real ones. Training was conducted until early stopping (patience p = 5) using the BCE loss, the Adam optimizer  Evaluation of the Classification Model on Anonymized Data. To assess the extent to which underlying abnormalities, and thus data utility, were preserved during the anonymization process, each individually trained anonymization network was used to perturb the images of our test set. Then, the pre-trained auxiliary classifier was evaluated using the resulting images. We report the mean of the 14 class-wise AUC values. To quantify the uncertainty, the 95% confidence intervals (CIs) from 1,000 bootstrap runs were computed. Comparison with Other Obfuscation-Based Methods. To compare our proposed system with other obfuscation-based methods, we additionally analyzed the anonymization capability and utility preservation of Privacy-Net ",vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,3.2,Results,Baseline and Comparison Methods. The results of all conducted experiments are shown in Table ,vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,PriCheXy-Net.,The results of our proposed PriCheXy-Net (see Table ,vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,4.0,Discussion and Conclusion,"To the best of our knowledge, we presented the first adversarial approach to anonymize thoracic images while preserving data utility for diagnostic purposes. Our proposed anonymization approach -PriCheXy-Net -is a composition of three independent neural networks consisting of (1) a flow field generator, (2) an auxiliary classifier, and (3) a patient verification network. In this work, we were able to show that collective utilization of these three components enables learning of a flow field that targetedly deforms chest radiographs and thus reliably deceives a patient verification model, even after re-training was performed. For the best hyper-parameter configuration of PriCheXy-Net, the re-identification performance drops from 81.8% to 57.7% in AUC for a simulated linkage attack, whereas the abnormality classification performance only decreases from 80.5% to 76.2%, which indicates the effectiveness of the proposed approach. We strongly hypothesize that the promising performance of PriCheXy-Net can be largely attributed to the constraints imposed on the learned flow field F . The limited deviation of the flow field from the identity (cf. Eq. 1) ensures a realistic appearance of the resulting deformed image to a considerable extent, thereby avoiding its content from being completely destroyed. This idea has a positive impact on preserving relevant abnormality patterns in chest radiographs, while allowing adequate scope to obfuscate biometric information. Such domain-specific constraints are not integrated in examined comparison methods such as Privacy-Net (which directly predicts an anonymized image without ensuring realism) and DP-Pix (which does not contain any mechanism to maintain data utility). This is, as we hypothesize, the primary reason for their limited ability to preserve data utility and the overall superiority of our proposed system. Interestingly, PriCheXy-Net's deformation fields primarily focus on anatomical structures, including lungs and ribs, as demonstrated in Fig.  In future work, we aim to further improve the performance of PriCheXy-Net by incorporating additional components into its current architecture. For instance, we plan to integrate a discriminator loss into the model, which may positively contribute to achieving perceptual realism. Furthermore, we also consider implementing a region of interest segmentation step into the pipeline to ensure not to perturb diagnostically relevant image areas. Lastly, we hypothesize that our method is robust to variations in image size or compression rate, and posit its applicability beyond chest X-rays to other imaging modalities as well. However, confirmation of these hypotheses requires further exploration to be conducted in forthcoming studies.",vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Data Use,Declaration. This research study was conducted retrospectively using human subject data made available in open access by the National Institutes of Health (NIH) Clinical Center  Code Availability. The source code of this study has been made available at https://github.com/kaipackhaeuser/PriCheXy-Net.,vol3
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 26.,vol3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,1.0,Introduction,"Histopathological image analysis is an important step towards cancer diagnosis. However, shortage of pathologists worldwide along with the complexity of histopathological data make this task time consuming and challenging. Therefore, developing automatic and accurate histopathological image analysis methods that leverage recent progress in deep learning has received significant attention in recent years. In this work, we investigate the problem of diagnosing colorectal cancer, which is one of the most common reason for cancer deaths around the world and particularly in Europe and America  Existing deep learning-based colorectal tissue classification methods  While generative adversarial networks (GANs) ",vol3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,,Contributions:,"We propose a few-shot colorectal tissue image generation framework, named XM-GAN, which simultaneously focuses on generating highquality yet diverse images. Within our tissue image generation framework, we introduce a novel controllable fusion block (CFB) that enables a dense aggregation of local regions of the reference tissue images based on their congruence to those in the base tissue image. Our CFB employs a cross-attention based feature aggregation between the base (query) and reference (keys, values) tissue image features. Such a cross-attention mechanism enables the aggregation of reference features from a global receptive field, resulting in locally consistent features. Consequently, colorectal tissue images are generated with reduced artifacts. To further enhance the diversity and quality of the generated tissue images, we introduce a mapping network along with a controllable cross-modulated layer normalization (cLN) within our CFB. Our mapping network generates 'metaweights' that are a function of the global-level features of the reference tissue image and the control parameters. These meta-weights are then used to compute the modulation weights for feature re-weighting in our cLN. This enables the cross-attended tissue image features to be re-weighted and enriched in a controllable manner, based on the reference tissue image features and associated control parameters. Consequently, it results in improved diversity of the tissue images generated by our transformer-based framework (see Fig.  We validate our XM-GAN on the FS colorectral tissue image generation task by performing extensive qualitative, quantitative and subject specialist (pathologist) based evaluations. Our XM-GAN generates realistic and diverse colorectal tissue images (see Fig. ",vol3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,2.0,Related Work,"The ability of generative models  Our Approach: While the aforementioned works explore FS generation in natural images, to the best of our knowledge, we are the first to investigate FS generation in colorectal tissue images. In this work, we look into multi-class colorectal tissue analysis problem, with low and high-grade tumors included in the set. The corresponding dataset ",vol3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,3.0,Method,"Problem Formulation: In our few-shot colorectal tissue image generation framework, the goal is to generate diverse set of images from K input examples X of a unseen (novel) tissue classes. Let D s and D u be the set of seen and unseen classes, respectively, where D s ∩ D u = ∅. In the training stage, we sample images from D s and train the model to learn transferable generation ability to produce new tissue images for unseen classes. During inference, given K images from an unseen class in D u , the trained model strives to produce diverse yet plausible images for this unseen class without any further fine-tuning. Overall Architecture: Figure ",vol3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,3.1,Controllable Fusion Block,"Figure  Fig.  Here, a reference feature h ref i , noise z and control parameter αi are input to a mapping network for generating meta-weights wi. The resulting wi modulates the features via λ(wi) and β(wi) in our cLN. As a result of this controllable feature modulation, the output features fi enable the generation of tissue images that are diverse yet aligned with the semantics of the input tissue images. Next, we introduce a controllable feature modulation mechanism in our crosstransformer to further enhance the diversity and quality of generated images. Controllable Feature Modulation: The standard cross-attention mechanism, described above, computes locally consistent features that generate images with reduced artifacts. However, given the deterministic nature of the cross-attention and the limited set of reference images, simultaneously generating diverse and high-quality images in the few-shot setting is still a challenge. To this end, we introduce a controllable feature modulation mechanism within our CFB that aims at improving the diversity and quality of generated images. The proposed modulation incorporates stochasticity as well as enhanced control in the feature aggregation and refinement steps. This is achieved by utilizing the output of a mapping network for modulating the visual features in the layer normalization modules in our crosstransformer. Mapping Network: The meta-weights w i ∈ R D are obtained by the mapping network as, where ψ α (•) and ψ z (•) are linear transformations, z ∼ N (0, 1) is a Gaussian noise vector, and α i is control parameter. g ref i is global-level feature computed from the reference features h ref i through a linear transformation and a global average pooling operation. The meta-weights w i are then used for modulating the features in our cross-modulated layer normalization, as described below.",vol3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,,Controllable Cross-modulated Layer Normalization (cLN):,"Our cLN learns sample-dependent modulation weights for normalizing features since it is desired to generate images that are similar to the few-shot samples. Such a dynamic modulation of features enables our framework to generate images of high-quality and diversity. To this end, we utilize the meta-weights w i for computing the modulation parameters λ and β in our layer normalization modules. With the cross-attended feature c i as input, our cLN modulates the input to produce an output feature o i ∈ R n×D , given by where μ and σ 2 are the estimated mean and variance of the input c i . Here, λ(w i ) is computed as the element-wise multiplication between meta-weights w i and sample-independent learnable weights λ ∈ R D , as λ w i . A similar computation is performed for β(w i ). Consequently, our proposed normalization mechanism achieves a controllable modulation of the input features based on the reference image inputs and enables enhanced diversity and quality in the generated images. The resulting features o i are then passed through a feed-forward network (FFN) followed by another cLN for preforming point-wise feature refinement, as shown in Fig.  Finally, the decoder F D generates the final image x.",vol3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,3.2,Training and Inference,"Training: The whole framework is trained end-to-end following the hinge version GAN  and Additionally, to encourage the generated image x to be perceptually similar to the reference images based on the specified control parameters α, we use a parameterized formulation of the standard perceptual loss  where Moreover, a classification loss L cl enforces that the images generated by the decoder are classified into the corresponding class of the input few-shot samples. Our XM-GAN is then trained using the formulation: where η p and η cl are hyperparameters for weighting the loss terms. Inference: During inference, multiple high-quality and diverse images x are generated by varying the control parameter α i for a set of fixed K-shot samples. While a base image x b and α i can be randomly selected, our framework enables a user to have control over the generation based on the choice of α i values.",vol3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,4.0,Experiments,We conduct experiments on human colorectal cancer dataset ,vol3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,4.1,State-of-the-Art Comparison,"FS Tissue Image Generation: In Tab. 1, we compare our XM-GAN approach for FS tissue image generation with state-of-the-art LoFGAN  Our proposed XM-GAN that utilizes dense aggregation of relevant local information at a global receptive field along with controllable feature modulation outperforms LoFGAN with a significant margin of 30.1, achieving FID score of 55.8. Furthermore, our XM-GAN achieves a better LPIPS score. In Fig. ",vol3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,4.2,Ablation Study,"Here, we present our ablation study to validate the merits of the proposed contributions. Table  Baseline+PL refers to extending the Baseline by also integrating the standard perceptual loss. We conduct an additional experiment using random values of α i s.t. i α i = 1 for computing the fused feature f and parameterized perceptual loss (Eq. 5). We refer to this as Baseline+PPL. Our final XM-GAN referred here as Baseline+PPL+cLN contains the novel CFB. Within our CFB, we also validate the impact of the reference features for feature modulation by computing the meta-weights w i using only the Gaussian noise z in Eq. 2. This is denoted here as Baseline+PPL+cLN † . Our approach based on the novel CFB achieves the best performance amongst all baselines.",vol3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,4.3,Human Evaluation Study,"We conducted a study with a group of ten pathologists having an average subject experience of 8.5 years. Each pathologist is shown a random set of 20 images (10 real and 10 XM-GAN generated) and asked to identify whether they are real or generated. The study shows that pathologists could differentiate between the AI-generated and real images only 55% time, which is comparable with a random prediction in a binary classification problem, indicating the ability of our proposed generative framework to generate realistic colorectal images.",vol3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,5.0,Conclusions,"We proposed a few-shot colorectal tissue image generation approach that comprises a controllable fusion block (CFB) which generates locally consistent features by performing a dense aggregation of local regions from reference tissue images based on their similarity to those in the base tissue image. We introduced a mapping network together with a cross-modulated layer normalization, within our CFB, to enhance the quality and diversity of generated images. We extensively validated our XM-GAN by performing quantitative, qualitative and human-based evaluations, achieving state-of-the-art results.",vol3
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,1.0,Introduction,"The COVID-19 pneumonia pandemic has posed an unprecedented global health crisis, with lung imaging as a crucial tool for identifying and managing affected individuals  In comparison to CT scans, 2D chest X-rays are a more accessible and costeffective option due to their fast imaging speed, low radiation, and low cost, especially during the early stages of the pandemic  To achieve this, an intuitive solution is building independent networks to learn features from each modality initially. Afterward, late feature fusion, coattention or cross-attention modules are incorporated to transfer knowledge between CT and X-ray  This paper proposes a novel Unpaired Cross-modal Interaction (UCI) learning framework for COVID-19 segmentation, which aims to learn strong representations from limited dense annotated CT scans and abundant image-level annotated X-ray images. The UCI framework learns representations from both segmentation and classification tasks. It includes three main components: a multimodal encoder for image representations, a knowledge condensation and interaction module for unpaired cross-modal data, and task-specific networks. The encoder contains modality-specific patch embeddings and shared Transformer layers. This design enables the network to capture optimal feature representations for both CT and X-ray images while maintaining the ability to learn shared representations between the two modalities despite dimensional differences. To address the challenge of information interaction between unpaired cross-modal data, we introduce a momentum-updated prototype learning strategy to condense modality-specific knowledge. This strategy groups similar representations into the same prototype and iteratively updates the prototypes with a momentum term to capture essential information in each modality. Therewith, a knowledge-guided interaction module is developed that accepts the learned prototypes, enabling the UCI to better capture critical features and relationships between the two modalities. Finally, the task-specific networks, including the segmentation decoder and classification head, are presented to learn from all available labels. The proposed UCI framework has significantly improved performance on the public COVID-19 segmentation benchmark  The main contributions of this paper are three-fold: ",vol3
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,2.0,Approach,The proposed UCI aims to explore effective representations for COVID-19 segmentation by leveraging both limited dense annotated CT scans and abundant image-level annotated X-rays. Figure ,vol3
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,2.1,Multi-modal Encoder,"The multi-modal encoder F(•) consists of three stages of blocks, with modalityspecific patch embedding layers and shared Transformer layers in each block, capturing modality-specific and shared patterns, which can be more robust and discriminative across modalities. Notice that due to the dimensional gap between CT and X-ray, we use the 2D convolution block as patch embedding for X-rays and the 3D convolution block as patch embedding for CTs. In each stage, the patch embedding layers down-sample the inputs and generate the sequence of modality-specific embedded tokens. The resultant tokens, combined with the learnable positional embedding, are fed into the shared Transformer layers for long-term dependency modeling and learning the common patterns. More details about architecture can be found in the Appendix. Given a CT volume x ct , and a chest X-ray image x cxr , we denote the output feature sequence of the multi-modal encoder as where C ct and C cxr represent the channels of CT and X-ray feature sequence. N ct and N cxr means the length of CT and X-ray feature sequence.",vol3
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,2.2,Knowledge Condensation and Interaction,"Knowledge Condensation. It is difficult to directly learn cross-modal dependencies using the features obtained by the encoder because CT and X-ray data were collected from different patients. This means that the data may not have a direct correspondence between two modalities, making it challenging to capture their relationship. As shown in Fig.  where C cxr i suggests the feature points closing to the i-th prototype. σ(•) represents a linear projection to reduce the feature sequence length to relieve the computational burden. Then we introduce a momentum learning function to update the prototypes with C cxr i , which means that the updates at each iteration not only depend on the current C cxr i but also consider the direction and magnitude of the previous updates, defined as where λ is the momentum factor, which controls the influence of the previous update on the current update. Similarly, the prototypes P ct for CT modality can be calculated and updated with the feature set f ct . The prototypes effectively integrate the informative features of each modality and can be considered modality-specific knowledge to improve the subsequent cross-modal interaction learning. The momentum term allows prototypes to move more smoothly and consistently towards the optimal position, even in the presence of noise or other factors that might cause the prototypes to fluctuate. This can result in a more stable learning process and more accurate prototypes, thus contributing to condensate the knowledge of each modality better. Knowledge-Guided Interaction. The knowledge-guided interaction (KI) module is proposed for unpaired cross-modality learning, which accepts the learned prototypes from one modality and features from another modality as inputs. As shown in Fig. ",vol3
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,2.3,Task-Specific Networks,"The outputs of the KI module are fed into two multi-task heads -one decoder for segmentation and one prediction head for classification respectively. The segmentation decoder has a symmetric structure with the encoder, consisting of three stages. In each stage, the input feature map is first up-sampled by the 3D patch embedding layer, and then refined by the stacked Transformer layers. Besides, we also add skip connections between the encoder and decoder to keep more low-level but high-resolution information. The decoder includes a segmentation head for final prediction. This head includes a transposed convolutional layer, a Conv-IN-LeakyReLU, and a convolutional layer with a kernel size of 1 and the output channel as the number of classes. The classification head contains a linear layer with the output channel as the number of classes for prediction. We use the deep supervision strategy by adding auxiliary segmentation losses (i.e., the sum of the Dice loss and cross-entropy loss) to the decoder at different scales. The cross-entropy loss is used to optimize the classification task.",vol3
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3.1,Materials,We used the public COVID-19 segmentation benchmark ,vol3
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3.2,Implementation Details,"For CT data, we first truncated the HU values of each scan using the range of  To evaluate the COVID-19 segmentation performance, we utilized six metrics, including the Dice similarity coefficient (DSC), intersection over union (IoU), sensitivity (SEN), specificity (SPE), Hausdorff distance (HD), and average surface distance (ASD). These metrics provide a comprehensive assessment of the segmentation quality. The overlap-based metrics, namely DSC, IoU, SEN, and SPE, range from 0 to 1, with a higher score indicating better performance. On the other hand, HD and ASD are shape distance-based metrics that measure the dissimilarity between the surfaces or boundaries of the segmentation output and the ground truth. For HD and ASD, a lower value indicates better segmentation results.",vol3
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3.3,Compared with Advanced Segmentation Approaches,Table ,vol3
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3.4,Discussions,"Ablations. We perform ablation studies over each component of UCI, including the multi-modal encoder, Knowledge Condensation (KC) and Knowledge Interaction (KI) models, as listed in Fig.  (2) w/o shared encoder: replacing the multi-modal encoder with two independent encoders, each designed to learn features from a separate modality; (3) w/o KC: removing the prototype and using the features before KC for interaction; (4) w/o KC & KI: only with encoder to share multi-modal information; and (5) w/o warm-up: removing the prototype warm-up in KI. Figure  Hyper-Parameter Settings. To evaluate the impact of hyper-parameter settings on COVID-19 segmentation, we conducted an investigation of the number of prototypes (k) and the number of momentum factors (λ). Figure ",vol3
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,4.0,Conclusion,"Our study introduces UCI, a novel method for improving COVID-19 segmentation under limited CT images by leveraging unpaired X-ray images with imagelevel annotations. Especially, UCI includes a multi-modal shared encoder to capture optimal feature representations for CT and X-ray images while also learning shared representations between the two modalities. To address the challenge of information interaction between unpaired cross-modal data, UCI further develops a KC and KI module to condense modality-specific knowledge and facilitates cross-modal interaction, thereby enhancing segmentation training. Our experiments demonstrate that the UCI method outperforms existing segmentation models for COVID-19 segmentation.",vol3
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 58.,vol3
The Role of Subgroup Separability in Group-Fair Medical Image Classification,1.0,Introduction,"Medical image computing has seen great progress with the development of deep image classifiers, which can be trained to perform diagnostic tasks to the level of skilled professionals  An often overlooked aspect of this problem is subgroup separability: the ease with which individuals can be identified as subgroup members. Some medical images encode sensitive information that models may leverage to classify individuals into subgroups  We highlight how the separability of protected groups interacts in non-trivial ways with the training of deep neural networks. We show that the ability of models to detect which group an individual belongs to varies across modalities and groups in medical imaging and that this property has profound consequences for the performance and fairness of deep classifiers. To the best of our knowledge, ours is the first work which analyses group-fair image classification through the lens of subgroup separability. Our contributions are threefold: -We demonstrate empirically that subgroup separability varies across realworld modalities and protected characteristics. -We show theoretically that such differences in subgroup separability affect model bias in learned classifiers and that group fairness metrics may be inappropriate for datasets with low subgroup separability. -We corroborate our analysis with extensive testing on real-world medical datasets, finding that performance degradation and subgroup disparities are functions of subgroup separability when data is biased.",vol3
The Role of Subgroup Separability in Group-Fair Medical Image Classification,2.0,Related Work,"Group-fair image analysis seeks to mitigate performance disparities caused by models exploiting sensitive information. In medical imaging, Seyyed-Kalantari et al.  One step towards overcoming these challenges and developing fair and performant methods is understanding the circumstances under which deep classifiers learn to exploit sensitive information inappropriately. Today, our understanding of this topic is limited. Closely related to our work is Oakden-Rayner et al., who consider how 'hidden stratification' may affect learned classifiers ",vol3
The Role of Subgroup Separability in Group-Fair Medical Image Classification,3.0,The Role of Subgroup Separability,"Consider a binary disease classification problem where, for each image x ∈ X, we wish to predict a class label y ∈ Y : {y + , y -}. We denote P : [Y |X] → [0, 1] the underlying mapping between images and class labels. Suppose we have access to a (biased) training dataset, where P tr is the conditional distribution between training images and training labels; we say that such a dataset is biased if P tr = P . We focus on group fairness, where each individual belongs to a subgroup a ∈ A and aim to learn a fair model that maximises performance for all groups when deployed on an unbiased test dataset drawn from P . We assume that the groups are consistent across both datasets. The bias we consider in this work is underdiagnosis, a form of label noise  We may now use the law of total probability to express the overall mapping from image to label in terms of the subgroup-wise mappings in Eq. (  P tr (y|x) = a∈A P tr (y|x, a)P tr (a|x) (2) At training time, supervised learning with empirical risk minimisation aims to obtain a model p, mapping images to predicted labels ŷ = argmax y∈Y p(y|x) such that p(y|x) ≈ P tr (y|x), ∀(x, y). Since this model approximates the biased training distribution, we may expect underdiagnosis from the training data to be reflected by the learned model when evaluated on the unbiased test set. However, the distribution of errors from the learned model depends on subgroup separability. Revisiting Eq. (  and ∀a = a * , p(y|x + , a) ≈ P tr (y|x + , a) = P (y|x + , a) ( Equation (  Here, N +,a denotes the number of positive samples belonging to group a in the test set. Remember, in practice, we must train our model on the biased training distribution P tr . We thus derive test-time TPR for such a model, TPR (b)  a , from Eq. (  and In the case of high subgroup separability, Eq. (  Equations (  We have derived the effect of underdiagnosis bias on classifier performance for the two extreme cases of high and low subgroup separability. In practice, subgroup separability for real-world datasets may vary continuously between these extremes. In Sect. 4, we empirically investigate (i) how subgroup separability varies in the wild, (ii) how separability impacts performance for each group when underdiagnosis bias is added to the datasets, (iii) how models encode sensitive information in their representations.",vol3
The Role of Subgroup Separability in Group-Fair Medical Image Classification,4.0,Experiments and Results,We support our analysis with experiments on five datasets adapted from a subset of the MEDFAIR benchmark ,vol3
The Role of Subgroup Separability in Group-Fair Medical Image Classification,,Subgroup Separability in the Real World,"We begin by testing the premise of this article: subgroup separability varies across medical imaging settings. To measure subgroup separability, we train binary subgroup classifiers for each dataset-attribute combination. We use testset area under receiver operating characteristic curve (AUC) as a proxy for separability, reporting results over ten random seeds in Table ",vol3
The Role of Subgroup Separability in Group-Fair Medical Image Classification,,Performance Degradation Under Label Bias,We now test our theoretical finding: models are affected by underdiagnosis differently depending on subgroup separability. We inject underdiagnosis bias into each training dataset by randomly mislabelling 25% of positive individuals in Group 1 (see Table ,vol3
The Role of Subgroup Separability in Group-Fair Medical Image Classification,,Use of Sensitive Information in Biased Models,"Finally, we investigate how biased models use sensitive information. We apply the post hoc Supervised Prediction Layer Information Test (SPLIT) ",vol3
The Role of Subgroup Separability in Group-Fair Medical Image Classification,5.0,Discussion,"We investigated how subgroup separability affects the performance of deep neural networks for disease classification. We discuss four takeaways from our study: Subgroup Separability Varies Substantially in Medical Imaging. In fairness literature, data is often assumed to contain sufficient information to identify individuals as subgroup members. But what if this information is only partially encoded in the data? By testing eleven dataset-attribute combinations across three medical modalities, we found that the ability of classifiers to predict sensitive attributes varies substantially. Our results are not exhaustive -there are many modalities and sensitive attributes we did not consider -however, by demonstrating a wide range of separability results across different attributes and modalities, we highlight a rarely considered property of medical image datasets. Performance Degradation is a Function of Subgroup Separability. We showed, theoretically and empirically, that the performance and fairness of models trained on biased data depends on subgroup separability. When separability is high, models learn to exploit the sensitive information and the bias is reflected by stark subgroup differences. When separability is low, models cannot exploit sensitive information, so they perform similarly for all groups. This indicates that group fairness metrics may be insufficient for detecting bias when separability is low. Our analysis centred on bias in classifiers trained with the standard approach of empirical risk minimisation -future work may wish to investigate whether subgroup separability is a factor in the failure of bias mitigation methods and whether it remains relevant in further image analysis tasks (e.g. segmentation).",vol3
The Role of Subgroup Separability in Group-Fair Medical Image Classification,,Sources of Bias Matter.,"In our experiments, we injected underdiagnosis bias into the training set and treated the uncorrupted test set as an unbiased ground truth. However, this is not an endorsement of the quality of the data. At least some of the datasets may already contain an unknown amount of underdiagnosis bias (among other sources of bias)  Reproducibility and Impact. This work tackles social and technical problems in machine learning for medical imaging and is of interest to researchers and practitioners seeking to develop and deploy medical AI. Given the sensitive nature of this topic, and its potential impact, we have made considerable efforts to ensure full reproducibility of our results. All datasets used in this study are publicly available, with access links in Table ",vol3
The Role of Subgroup Separability in Group-Fair Medical Image Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 18.,vol3
Self-adaptive Adversarial Training for Robust Medical Segmentation,1.0,Introduction,"Medical image segmentation is a fundamental task in medical image analysis  Recent studies on the natural image domains show that adversarial training is one of the most successful strategies against adversarial attacks  In this paper, we consider how to effectively improve the adversarial robustness in 3D medical image segmentation tasks. Taking inspiration from the PAC-Bayes generalisation bounds on standard training  In summary, our contribution comes from three parts: i) Based on the PAC-Bayes generalisation framework, we show that the adversarial training effect on 3D segmentation tasks can be improved by reducing the norm of the trained models' gradient; ii) As existing methods do not work on 3D tasks, our empirical investigation demonstrates that dynamically adjusting the adversarial iteration can achieve a better regularising effect on the gradient norm than fixing the iteration; iii) We design a SElf-adaptive Adversarial Training strategy, SEAT for short, and empirically prove its effectiveness on the MSD dataset.",vol3
Self-adaptive Adversarial Training for Robust Medical Segmentation,2.0,Related Works,"The goal of adversarial attacks is to add malicious perturbations to the input examples, aiming to fool or deceive target neural networks while maintaining imperceptible to human or detection mechanisms  In the field of 3D medical imaging, due to the large volume of 3D examples and the shortage of training data, medical segmentation models are often prone to overfitting, resulting in poor generalisation and increased vulnerability to adversarial attacks ",vol3
Self-adaptive Adversarial Training for Robust Medical Segmentation,,Notations. Considering a segmentation task with input domain X,", where • is a norm constrain and C is the number of classes. We let D be a dataset containing N pairs of example and segmentation mask drawn i.i.d from the unknown distribution D. Denoted by f w : R n → R n , the segmentation results can be computed via a neural network parameterised over w = vec where d is the number of blocks.",vol3
Self-adaptive Adversarial Training for Robust Medical Segmentation,3.1,PAC-Bayes Generalisation Bounds,"Previous works, e.g.,  where γ > 0 is the margin term. Similarly, we can extend the expected margin loss for segmentation models as ( Based on Eq. (  where h is number of hidden units in each block and Φ is the complexity score given by . Farnia et al.  where ε is the perturbation ratio, and Φ adv is proportion to Φ, while the exact form of it depends on the adversarial attack method. The complexity scores Φ and Φ adv are both proportional to d i=1 W i 2 , which is the product of the spectral norm of all blocks  Definition 1 (Lipschitz constant). Let f w : R n → R n be a segmentation model, δ be a perturbation, and L be a Lipschitz continued loss function, K > 0 is said to be a Lipschitz constant of model f w if, for any x, x + δ ∈ X , we have (5)",vol3
Self-adaptive Adversarial Training for Robust Medical Segmentation,3.2,Narrowing down the Generalisation Gap,"Decreasing the expected risk requires reducing the Lipschitz constant of the trained model while maintaining satisfactory training performance. One approach to control the Lipschitz constant is regularising the gradient during training, where Farnia et al.  normalisation is theoretically inapplicable for high-dimensional tensors. Note that the Lipschitz constant is an upper bound on how fast the loss value changes when small perturbations are added to the network's input  On a small classification dataset, Moosavi-Dezfooli et al. ",vol3
Self-adaptive Adversarial Training for Robust Medical Segmentation,3.3,Self-adaptive Adversarial Training Schedule,Motivated by the empirical investigation in Fig. ,vol3
Self-adaptive Adversarial Training for Robust Medical Segmentation,4.0,Experiment,This section evaluates the proposed adversarial training strategy under both adversarial training and adversarial fine-tuning scenarios on the MSD datasets  Implementation Details. Following the benchmark on the MSD dataset built by Daza et al.  Robustness Performance. We first evaluate the adversarially trained models by using attacks with different ratios. The averaged dice scores over categories and attacks on each task are reported in Fig. ,vol3
Self-adaptive Adversarial Training for Robust Medical Segmentation,5.0,Conclusion,"In this paper, we first introduce the PAC-Bayes generalisation bounds by defining the expected margin loss for the segmentation task and show that the generalisation gap can be narrowed down by reducing the Lipschitz constant of the trained model. While existing techniques like spectral normalisation and penalising the gradient norm are impractical for 3D segmentation models, we empirically show that dynamically adjusting the adversarial iterations can achieve a better regularisation of the model's Lipschitz constant. Accordingly, we developed a self-adaptive adversarial training method, namely SEAT, and evaluated its performance on the MSD dataset. Our experiments demonstrate that SEAT can train segmentation models with considerable robustness and is much more efficient than its opponents. Please note that the observation in this paper is only made on the ROG model, and we plan to extend our investigation to other state-of-the-art segmentation models in the future.",vol3
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,1.0,Introduction,"Medical image segmentation is ruled by large machine learning models which require substantial infrastructure to be executed. These are variations of UNetstyle  Neural Cellular Automata (NCA)  With Med-NCA, Kalkhof et al.  Naively adapting Med-NCA for three-dimensional inputs exponentially increases VRAM usage and convergence becomes unstable. We address these challenges with M3D-NCA, which takes NCA medical image segmentation to the third dimension and is illustrated in Fig.  We compare our proposed M3D-NCA against the UNet ",vol3
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,2.0,Methodology,"Cellular Automata (CA) are sets of typically hand-designed rules that are iteratively applied to each cell of a grid. They have been actively researched for decades, Game Of Life  NCA segmentation in medical images faces the problem of high VRAM consumption during training. Our proposed M3D-NCA described in Sect. 2.1 solves this problem by performing segmentation on different scales of the image and using patches during training. In Sect. 2.3 we introduce a score that indicates segmentation quality by utilizing the variance of NCAs during inference.",vol3
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,2.1,M3D-NCA Training Pipeline,"Our core design principle for M3D-NCA is to minimize the VRAM requirements. Images larger than 100 × 100, can quickly exceed 40 GB of VRAM, using a naive implementation of NCA, especially for three-dimensional configurations. The training of M3D-NCA operates on different scales of the input image where the same model architecture m is applied, as illustrated in Fig. ",vol3
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Batch Duplication:,"Training NCA models is inherently more unstable than classical machine learning models like the UNet, due to two main factors. First, stochastic cell activation can result in significant jumps in the loss trajectory, especially in the beginning of the training. Second, patchification in M3D-NCA can cause serious fluctuations in the loss function, especially with three or more layers, thus it may never converge properly. The solution to this problem is to duplicate the batch input, meaning that the same input images are multiple times in each batch. While this limits the number of images per stack, it greatly improves convergence stability.",vol3
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Pseudo Ensemble:,"The stochasticity of NCAs, caused by the random activation of cells gives them an inherent way of predicting multiple valid segmentation masks. We utilize this property by executing the trained model 10 times on the same data sample and then averaging over the outputs. We visualize the variance between several predictions in Fig. ",vol3
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,2.2,M3D-NCA Core Architecture,"The core architecture of M3D-NCA is optimized for simplicity. First, a convolution with a kernel size k is performed, which is appended with the identity of the current cell state of depth c resulting in state vector v of length 2 * c. v thus contains information about the surrounding cells and the knowledge stored in the cell. v is then passed into a dense layer of size h, followed by a 3D BatchNorm layer and a ReLU. In the last step, another Dense layer is applied, which has the output size c, resulting in the output being of the same size as the input. Now the cell update can be performed, which adds the model's output to the previous state. Performing a full execution of the model requires it to be applied s times. In the standard configuration, the core NCA sets the hyperparameters to k = 7 for the first layer, and k = 3 for all the following ones. c = 16 and h = 64 results in a model size of 12480 parameters. The bigger k in the first level allows the model to detect low-frequency features, and c and h are chosen to limit VRAM requirements. The steps s are determined per level by s = max(width, height, depth)/((k -1)/2), allowing the model to communicate once across the whole image.",vol3
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,2.3,Inherent Quality Control,"The variance observed in the derived segmentation masks serves as a quantifiable indicator of the predicted segmentation. We expect that a higher variance value indicates data that is further away from our training domain and consequently may lead to poorer segmentation accuracy. Nevertheless, relying solely on this number is problematic, as the score obtained is affected by the size of the segmentation mask. To address this issue, we normalize the metric by dividing the sum of the standard deviation by the number of segmentation pixels. The NCA quality metric (NQM) where v is an image volume and v i are N = 10 different predictions of M3D-NCA for v is defined as follows: We calculate the relation between Dice and NQM by running a linear regression on the training dataset, which has been enriched with spike artifacts to extend the variance range. Using the regression, we derive the detection threshold for a given Dice value (e.g., Dice > 0.8). In clinical practice, this value would be based on the task and utility.",vol3
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,3.0,Experimental Results,"The evaluation of the proposed M3D-NCA and baselines is performed on hippocampus (198 patients, ∼ 35×50×35) and prostate (32 patients, ∼ 320×320× 20) datasets from the medical segmentation decathlon (medicaldecathlon.com) ",vol3
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,3.1,Comparison and Ablation,"Our results in Fig.  In contrast, our proposed M3D-NCA uses two orders of magnitude fewer parameters, reaching 90.5% and 82.9% Dice for hippocampus and prostate respectively. M3D-NCA outperforms all basic UNet-style models, falling short of the nnUNet by only 0.6% for hippocampus and 6.3% for prostate segmentation. Utilizing the 3D patient data enables M3D-NCA to outperform the 2D segmentation model Med-NCA in both cases by 2.4% and 1.1% Dice. The Seg-NCA ",vol3
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,3.2,Automatic Quality Control,"To evaluate how well M3D-NCA identifies failure cases through the NQM metric, we degrade the test data with artifacts using the TorchIO package ",vol3
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,4.0,Conclusion,"We introduce M3D-NCA, a Neural Cellular Automata-based training pipeline for achieving high-quality 3D segmentation. Due to the small model size with under 13k parameters, M3D-NCA can be run on a Raspberry Pi 4 Model B (2 GB RAM). M3D-NCA solves the VRAM requirements for 3D inputs and the training instability issues that come along. In addition, we propose an NCA quality metric (NQM) that leverages the stochasticity of M3D-NCA to detect 50-94.6% of failure cases without additional overhead. Despite its small size, M3D-NCA outperforms UNet-style models and the 2D Med-NCA by 2% Dice on both datasets. This highlights the potential of M3D-NCAs for utilization in primary care facilities and conflict zones as a viable lightweight alternative.",vol3
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 17.,vol3
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,1.0,Introduction,"Instances with complex shapes arise in many biomedical domains, and their morphology carries critical information. For example, the structure of gland tissues in microscopy images is essential in accessing the pathological stages for cancer diagnosis and treatment. These instances, however, are usually closely in touch with each other and have non-convex structures with parts of varying widths (Fig.  In the biomedical domain, most methods  To preserve the connectivity of instances while keeping the precise instance boundary, in this paper, we propose a novel representation named skeleton-aware distance transform (SDT). Our SDT incorporate object skeleton, a concise and connectivity-preserving representation of object structure, into the traditional boundary-based distance transform (DT) (Fig. ",vol3
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,1.1,Related Work,Instance Segmentation. Bottom-up instance segmentation approaches have become de facto for many biomedical applications due to the advantage in segmenting objects with arbitrary geometry. U-Net  Object Skeletonization. Object skeleton ,vol3
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,2.1,SDT Energy Function,"Given an image, we aim to design a new representation E for a model to learn, which is later decoded into instances with simple post-processing. Specifically, a good representation for capturing complex-structure masks should have two desired properties: precise geometric boundary and robust topological connectivity. Let Ω denote an instance mask, and Γ b be the boundary of the instance (pixels with other object indices in a small local neighborhood). The boundary (or affinity) map is a binary representation where E| Γ b = 0 and E| Ω\Γ b = 1. Taking the merits of DT in modeling the geometric arrangement and skeleton in preserving connectivity, we propose a new representation E that satisfies: Here E| Ω\Γs < E| Γs = 1 indicates that there is only one global maximum for each instance, and the value is assigned to a pixel if and only if the pixel is on the skeleton. This property avoids ambiguity in defining the object interior and preserve connectivity. Besides, E| Ω\Γ b > E| Γ b = 0 ensures that boundary is distinguishable as the standard DT, which produces precise geometric boundary. For the realization of E, let x be a pixel in the input image, and d be the metric, e.g., Euclidean distance. The energy function for distance transform (DT) is defined as E DT (x) = d(x, Γ b ), which starts from 0 at object boundary and increases monotonically when x is away from the boundary. Similarly, we can define an energy function d(x, Γ s ) representing the distance from the skeleton. It vanishes to 0 when the pixel approaches the object skeleton. Formally, we define the energy function of the skeleton-aware distance transform (Fig.  where α controls the curvature of the energy surface Besides, since common skeletonization algorithms can be sensitive to small perturbations on the object boundary and produce unwanted branches, we smooth the masks before computing the object skeleton by Gaussian filtering and thresholding to avoid complex branches. Learning Strategy. Given the ground-truth SDT energy map, there are two ways to learn it using a CNN model. The first way is to regress the energy using L 1 or L 2 loss. In the regression mode, the output is a single-channel image. The second way is to quantize the [0, 1] energy space into K bins and rephrase the regression task into a classification task ",vol3
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,2.2,SDT Network,Network Architecture. Directly learning the energy function with a fully convolutional network (FCN) can be challenging. Previous approaches either first regress an easier direction field representation and then use additional layers to predict the desired target ,vol3
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,,Target SDT Generation.,"There is an inconsistency problem in object skeleton generation: part of the complete instance skeleton can be different from the skeleton of the instance part (Fig.  Instance Extraction from SDT. In the SDT energy map, all boundary pixels share the same energy value and can be processed into segments by direct thresholding and connected component labeling, similar to DWT ",vol3
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,3.1,Histopathology Instance Segmentation,"Accurate instance segmentation of gland tissues in histopathology images is essential for clinical analysis, especially cancer diagnosis. The diversity of object appearance, size, and shape makes the task challenging. Dataset and Evaluation Metric. We use the gland segmentation challenge dataset ",vol3
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,,Methods in Comparison.,"We compare SDT with previous state-of-the-art segmentation methods, including DCAN ",vol3
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,,Results.,Our SDT framework achieves state-of-the-art performance on 5 out of 6 evaluation metrics on the gland segmentation dataset (Table ,vol3
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,3.2,Ablation Studies,"Loss Function. We compare the regression mode using L1 and L2 losses with the classification mode using cross-entropy loss. There is a separate channel for background under the classification mode where the energy values are quantized into bins. However, for regression mode, if the background value is 0, we need to use a threshold τ > 0 to decide the foreground region, which results in shrank masks. To separate the background region from the foreground objects, we assign an energy value of -b to the background pixels (b ≥ 0). To facilitate the regression, given the predicted value ŷi for pixel i, we apply a sigmoid function (σ) and affine transformation so that ŷ i = (1+b) • σ(ŷ i )b has a range of (-b, 1). We set b = 0.1 for the experiments. We show that under the same settings, the model trained with quantized energy reports the best results (Table  Curvature. We also compare different α in Eq. 2 that controls the curvature of the energy landscape. Table  Global/Local Skeleton. In Sect. 2.2 we show the inconsistency problem of global and local skeletons. In this study, we set α = 0.8 and let the model learn the pre-computed SDT energy for the training set. The results show that pre-computed SDT significantly degrades performance (Table ",vol3
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,4.0,Conclusion,"In this paper, we introduce the skeleton-aware distance transform (SDT) to capture both the geometry and topological connectivity of instance masks with complex shapes. For multi-class problems, we can use class-aware semantic segmentation to mask the SDT energy trained for all objects that is agnostic to their classes. We hope this work can inspire more research on not only better representations of object masks but also novel models that can better predict those representations with shape encoding. We will also explore the application of SDT in the more challenging 3D instance segmentation setting.",vol3
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,1.0,Introduction,"The human brain comprises millions of neurons that interconnect via intricate neural synapses, enabling efficient information exchange and transient, self-organized functional fluctuations. Regrettably, this rapid and efficient transport mechanism also facilitates the dissemination of toxic pathological proteins, including amyloid-beta (Aβ) plaques and neurofibrillary tangles (tau), which spread rapidly throughout the brain, exacerbating the progression of Alzheimer's disease (AD)  Aβ and tau represent pathological hallmarks of AD, which can be measured through PET (positron emission tomography) scan  A plethora of deep learning models  Fortunately, the partial differential equation (PDE)-based systems biology approach studies biological pathways and interactions between Aβ and tau from the mathematical perspective, allowing us to uncover the intrinsic mechanism that steers the spatiotemporal dynamics of tau propagation throughout the brain. In this context, reaction-diffusion model (RDM)  Following this cue, we sought to integrate the principle of systems biology and the power of machine learning in a unified mathematical framework, with a focus on an RDM-based deep model for uncovering the novel biological mechanism. Specifically, we introduce a novel framework for producing fresh PDE-based solutions from an application-specific constrained functional, known as Aβ influence. We formulate the evolving biological process of Aβ cascade and tau propagation into a closed-loop feedback system where the system dynamics are constrained by region-to-region white matter fiber tracts in the brain. This approach enables accurate prediction of the progression of the underlying neurobiological process, namely tau propagation. Additionally, we develop an explainable deep learning model that is based on the newly formulated RDM. The neural network is trained to clarify the Aβ-tau interaction while adhering to the principles of mathematics. We demonstrate promising results in both predicting AD progression and diagnosing the disease on the ADNI dataset.",vol3
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,,Suppose we have a brain network,", and the output is the clinical outcome η T . From the perspective of brain dynamics, we introduce an evolution state v i (t) for each brain region, which can be regarded as the intrinsic interaction trajectory of the features of each brain node. Herein, we investigate two prominent features on the basis of the brain region, namely tau-x i (t) and Aβ-u i (t), we then explore the interaction between tau propagation and amyloid cascade, which is believed to play a crucial role in the evolution dynamics i=1 of AD progression. In particular, we investigate how Aβ influences the spreading of tau in AD progression. Our study aims to shed light on the complex mechanisms underlying the progression of AD, a critical area of research in the field of neuroscience.",vol3
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,2.1,Reaction-Diffusion Model for Neuro-Dynamics,"RDM, a mathematical model with the capability to capture various dynamic evolutionary phenomena, is often employed to describe the reaction-diffusion process  A = -∇ • (∇) expresses the Laplacian operator, which is represented by the divergence ∇• of gradient ∇. In this context, the first term of Eq. (  In the deep neural network chiché, the nonlinear interaction is often defined as",vol3
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,2.2,Construction on the Interaction Between Tau and Amyloid,"Conventionally the connection between tau and amyloid has been established by treating them as an embedding (z i {x i , u i } ∈ R M ) on the graph in a cuttingedge graph-based learning approach  where the designed last term characterizes the interaction between the evolution state v and u with B denoting the interaction matrix. To reasonably and appropriately establish the interaction, we incorporate an interaction constraint that ensures a desirable evolution state. In the spirit of the linear quadratic regulator (LQR) in control theory  where the requirement that L ≥ 0 implies that both P and Q are positive definite. Minimizing the objective function Eq. (  where the terminal cost ψ(v T , η T ) = KL(v T , η T ) is measured by Kullback-Leibler (KL) divergence ",vol3
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,2.3,Neural Network Landscape of RDM-Based Dynamic Model,"In this section, we further design the explainable deep model based on the new PDE, and the designed deep model is trained to learn the mechanism of neurodynamics, i.e., tau propagation, which can predict disease progression and diagnosis accuracy. The overall network architecture of our physics-informed model is shown in Fig. ",vol3
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,3.1,Data Description and Experimental Setting,"We evaluate the performance of the proposed PDE-informed deep mode of neurodynamics on ADNI dataset  We (1) validate the performance of disease progression prediction (i.e., the future tau accumulation) of our proposed PDE-informed deep model, a PDEbased liquid time-constant network (LTC-Net)  (2) predict the diagnosis accuracy of AD (i.e., the recognition of 'non-convert' vs. 'converted'), and further uncover the interaction between tau and Aβ. To assess fairness, we perform one solely utilizing tau as input, and the other incorporating both amyloid and tau as input. In the latter scenario, we conduct a concatenation operation for LTC-Net and Neuro-RDM, with tau and Aβ serving as the graph embeddings based on GCN. To further verify the effectiveness of the proposed components of our deep model, we conduct an ablation study in terms of the presence/absence LQR constraint. Note, we report the testing results using 5fold cross-validation, the evaluation metrics involve (1) the mean absolute error (MAE) for predicting the level of tau burden (2) the prediction accuracy for recognizing the clinical outcomes.",vol3
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,3.2,Ablation Study in Prediction Disease Progression,"We design the ablation study in the scene of predicting the future tau burden x T using the baseline tau level x 0 , where we model the influence of amyloid build-up (u 0 ...u T ) in the time course of tau propagation dv dt . As shown in the first column of Fig.  It is clear that (1) the prediction error by the counterpart methods is much less reliable than our PDE-informed method since the machine learning model does not fully capture neurobiological mechanism, and (2) adding additional information (Aβ) does not contribute to the prediction, partially due to the lack of modeling Aβ-tau interaction, implying that the Aβ interacts with the propagation of tau to a certain extent.  ",vol3
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,3.3,Prognosis Accuracies on Forecasting AD Risk,"First, suppose we have the baseline amyloid and tau scans, we evaluate the prediction accuracy in forecasting the risk of developing AD by LTC-Net (in green), Neuro-RDM (in red), GCN (in purple), and OURS (in blue) in Fig.  ( In what mechanism that local or remote Aβ-tau interaction promotes the spreading of tau aggregates? Our explainable deep model aims to uncover the answer from the interaction matrix B in Eq. (2), which is the primary motivation behind our research. We visualize the interaction matrix B and the corresponding brain mapping (node size and link bandwidth are in proportion to the strength of local and remote interaction, respectively) in Fig. ",vol3
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,4.0,Conclusion,"In this endeavor, we have embarked on an explainable machine learning initiative to unearth the intrinsic mechanism of Aβ-tau interaction from the unprecedented amount of spatiotemporal data. Since RDM has been well studied in the neuroscience field, we formulate optimal constraint in vanilla RDM and dissect it into the deep model. We have applied our RDM-based deep model to investigate the prion-like propagation mechanism of tau aggregates as well as the downstream association with clinical manifestations in AD, our tailored deep model not only achieves significant improvement in the prediction accuracy of developing AD, but also sheds the new light to discover the latent pathophysiological mechanism of disease progression using a data-driven approach.",vol3
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,1.0,Introduction,"Image segmentation is one of the most fundamental and popular tasks in medical image analysis. It is widely applied to parse organs, bones, soft tissues, or lesions in N -D medical images. Conventional methods rely on statistics of image intensities or object shapes to infer the boundaries of the target regions  Inspired by the advancement from related domains, e.g., natural language processing (NLP), transformers  Most existing neural architectures are designed with strong human heuristics. Neural architecture search (NAS) has been proposed in an attempt to reduce dependency on such heuristics while optimizing model performance for given tasks. Given target constraints, it is capable of optimizing multiple objectives (e.g., accuracy, memory consumption, latency, etc.) of the neural network models at the same time. Nowadays, NAS has been widely applied for many applications in medical imaging including image classification and segmentation. Existing NAS works have been focusing on optimization with convolutional deep learning components ",vol3
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,2.0,Related Work,"Neural architecture search tries to find optimal global model structures and local operations from large search spaces for different applications. Searching algorithms, including reinforcement learning and genetic algorithms  Transformer based neural network has been recently introduced to medical imaging domain for various applications following the success of vision transformer (ViT) in computer vision ",vol3
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,3.0,Method,"Our NAS algorithm, DAST, is an intuitive extension of DiNTS  and n i+1,j+1 . Each edge e of connection is a weighted combination of outputs from operations o, and the pool of o includes skip-connection, convolution and transformer. For neighboring nodes at different levels, additional ×2 up-sampling or down-sampling is added in e, as well as convolutions to match feature channel numbers. There is no additional operation when passing features between nodes at the same level. To optimize the architecture, two different types of weights are introduced. Each edge has weight w e , and each operation has weight w o . Then the searched architecture can be defined when all w e and w o are binarized. The stem cells are concatenated at input and output of the search space. The input stem cells down-samples image toward different resolution scales and fit image features to the search space. The output stem cells up-samples multiscale features out of search space with necessary concatenation to produce multichannel probability maps. Transformer. We introduce transformer  The positional embedding P in Eq. 2 is initialized as a normalized 3D position map. It is efficient to compute with only 3 channels. The full transformer with an encoder and a decoder is adopted to process the output of the projector. The decoder takes additional learnable query embedding as input. The output of the transformer shares the same dimension/shape with the input. Next, we need to add another reversed projector to map the 1-D feature map back to the 3D shape of input. Segmentation Attention. To further understand the self-attention scheme, we embed an additional multi-head self-attention layer A after the transformer. A uses the feature maps from the transformer as the semantic query q, and the features from the transformer encoder as key k. Unlike multi-head self-attention layers inside the transformer, A does not have residual connection. Thus, A is enforced to learn meaningful attention weights for segmentation tasks. The attention weights are directly multiplied with intermediate features maps, which can be reshaped from 1-D to 3-D for visual interpretation. Memory Estimation. Like DiNTS  In practice, the number of token l is fixed as 512 to avoid potential memory explosion. Eight heads are used in each multi-head self-attention layer. Number of operations N is approximately estimated as 15 including convolutions, batch normalization, linear operations, layer normalization, and multi-head selfattention.",vol3
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,4.1,Data Sets and Implementation Details,We adopted large-scale data sets Task07 Pancreas from Medical Segmentation Decathlon (MSD)  A combination of dice loss and cross-entropy loss is adopted to minimize both global and pixel-wise distance between ground truth and predictions. The NAS is conducted through conventional bi-level optimization following implementation ,vol3
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,4.2,Comparison with DiNTS,"Since DiNTS is the closest work to DAST, we directly compare the performance on DiNTS's searching tasks with the same data split. Then we re-train the searched architectures from both methods from scratch. As shown in Fig. ",vol3
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,4.3,KiTS'19 Experiments,"To verify the effectiveness and generalization of our searched architectures from DAST, we validate the searched architecture (from pancreas data set) on this challenging task. Metrics for kidneys and tumors are the average Dice score per case. Finally, we evaluate our single-fold model as well as the ensemble from 5 cross-validation models on the public test leaderboard Table ",vol3
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,4.4,Ablation Studies,Memory Constraints. We provide the option to change the parameter controlling memory consumption budget in the loss function with different values shown in Fig. ,vol3
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,5.0,Discussion and Conclusion,"In this study, we observe that DAST is able to find effective and concise relationships between convolutions and transformers in a single neural network model. The optimized connections between operations improves the model effectiveness in various applications. Such models benefit from the different inductive biases introduced by these two operations. Adding a memory constraint loss as an additional objective can lower memory consumption for the searched architecture. Transformers will then benefit more from long-range dependencies of larger input patches. We hope this perspective will be helpful for different applications in medical imaging.",vol3
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,1.0,Introduction,"Recent advancements in applying machine learning techniques to MRI-based brain imaging studies have shown substantial progress in predicting neurodegenerative diseases (e.g., Alzheimer's Disease or AD) and clinical phenotypes (e.g., behavior measures), and in uncovering novel biomarkers that are closely related to them  To address this, we propose a novel bidirectional mapping framework, where the mapping from structural MRI data (i.e., diffusion MRI-derived brain structural network) to the functional counterpart (i.e., BOLD signals) and the inverse mapping are implemented simultaneously. Unlike previous studies  -We propose a novel bidirectional framework to yield multimodal brain MRI representations by modeling the interactions between brain structure and the functional counterpart. -We use contrastive learning to extract the intrinsic unity of both modalities. -The experimental results on two publicly available datasets demonstrate the superiority of our proposed method in predicting neurodegenerative diseases and clinical phenotypes. Furthermore, the interpretability analysis highlights that our method provides biologically meaningful insights. Afterward, ROI-level contrastive learning is applied to these extracted representations, facilitating their alignment in a common space. These derived representations are then utilized for downstream prediction tasks.",vol3
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,2.0,Method,The proposed bidirectional mapping framework (Fig. ,vol3
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,,Preliminaries.,"A structural brain network is an attributed and weighted graph G = (A, H) with N nodes, where H ∈ R N ×d is the node feature matrix, and A ∈ R N ×N is the adjacency matrix where a i,j ∈ R represents the edge weight between node i and node j. Meanwhile, we utilize X B ∈ R N ×T to represent the BOLD signal matrix derived from functional MRI data of each subject, where each brain ROI has a time series BOLD signal with T points. Reconstruction. For the reconstruction task, we deploy an encoder-decoder architecture and utilize the L 1 loss function. Particularly, we use a multi-layer feed-forward neural network as the encoder and decoder. Our method differs from previous studies ",vol3
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,,ROI-Level's Contrastive Representation Learning.,"With latent representation Z B ∈ R N ×dB generated from BOLD signal and Z S ∈ R N ×dS from structural networks, we then conduct ROI-level's contrastive learning to associate the static structural and dynamic functional patterns of multimodal brain measurements. The contrastive learning loss aims to minimize the distinctions between latent representations from two modalities. To this end, we first utilize linear layers to project Z B and Z S to the common space, where we obtain to denote representations from the same ROI, where z B i and z S i are elements of Z B and Z S , respectively. For the same brain ROI, the static structural representation and the dynamic functional counterpart are expected to share a maximum similarity. Conversely, for the pairs that do not match, represented as (z B i , z S j ) i =j , these are drawn from different ROIs and should share a minimum similarity. To formally build up the ROI-level's contrastive loss, it is intuitive to construct positive samples and negative ones based on the match of ROIs. Specifically, we construct (z B i , z S i ) i=1•••N as positive sample pair, and (z B i , z S j ) i =j as negative sample pair. And our contrastive loss can be formulated as follow: where Similarity(•) is substantiated as cosine similarity. Loss Functions. The loss functions within our proposed framework are summarized here. Besides the reconstruction loss (L rec ) and the ROI-level's contrastive loss (L contrast ), we utilize cross-entropy loss (L supervised = L cross-entropy ) for classification tasks, and L 1 loss (L supervised = L mean-absolute-error ) for regression tasks, respectively. In summary, the loss function can be described as: where η 1 , η 2 and η 3 are loss weights. ",vol3
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.1,Data Description and Preprocessing,"Two publicly available datasets were used to evaluate our framework. The first includes data from 1206 young healthy subjects (mean age 28.19 ± 7.15, 657 women) from the Human Connectome Project ",vol3
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.2,Experimental Setup and Evaluation Metrics,"We randomly split each dataset into 5 disjoint sets for 5-fold cross-validations, and all the results are reported in mean (s.t.d.) across 5 folds. To evaluate the performance of each model, we utilize accuracy, precision score, and F 1 score for classification tasks, and mean absolute error (MAE) for regression tasks. The learning rate is set as 1 × 10 -4 and 1 × 10 -3 for classification and regression tasks, respectively. The loss weights (i.e., η 1 ,η 2 , and η 3 ) are set equally as 1/3. To demonstrate the superiority of our method in cross-modal learning, bidirectional mapping, and ROI-level's contrastive learning, we select four baselines including 2 single-modal graph learning methods (i.e., DIFFPOOL ",vol3
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.3,BOLD Signal and Structural Network Reconstruction,"We train the model in a task-free manner where no task-specific supervised loss is involved. The MAE values between the edge weights in the ground-truth and reconstructed structural networks are 0.0413 ± 0.0009 and 0.0309 ± 0.0015 under 5-fold cross-validation on the HCP and OASIS, respectively. The MAE values between ground-truth and reconstructed BOLD signals are 0.0049 ± 0.0001 and 0.0734 ± 0.0016 on the HCP and OASIS, respectively. The reconstruction results on HCP are visualized in Fig. ",vol3
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.4,Disease and Sex Classification,"We conduct Alzheimer's disease (AD) classification on the OASIS dataset, and sex classification on the HCP dataset. As shown in Table ",vol3
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.5,ASR and MMSE Regression,"Mini-Mental State Exam (MMSE) is a quantitative measure of cognitive status in adults, and Adult Self-Report scale (ASR) ",vol3
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.6,Ablation Study,"To demonstrate the significance of bidirectional mapping, we remove a part of our proposed BMCL model to yield two unidirectional mappings (i.e., either mapping from structural network to BOLD signal, or mapping inversely). As shown in the bottom three rows in Table ",vol3
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.7,Interpretability,The 10 key brain regions (Fig. ,vol3
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,4.0,Conclusions,"We propose a new multimodal data mining framework, named BMCL, to learn the representation from two modality data through bidirectional mapping between them. The elaborated ROI-level contrastive learning in BMCL can reduce the distinction and eliminate biases between two one-way mappings. Our results on two publicly available datasets show that BMCL outperforms all baselines, which demonstrates the superiority of bidirectional mapping with ROI-level contrastive learning. Beyond these, our model can identify key brain regions highly related to different clinical phenotypes and brain diseases, which demonstrates that our framework is interpretable and the results are biologically meaningful. The contrastive learning method, while emphasizing the alignment of features from different modalities, may inadvertently neglect the unique characteristics inherent to each modality. Moving forward, we intend to refine our method by aiming for a balance between the alignment of modalities and the preservation of modality-specific information. Additionally, the pre-selection of important features or the consideration of subnetworks holds promising for further research.",vol3
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,1.0,Introduction,"Deep learning models have shown high prediction performance on many medical imaging tasks (e.g., ",vol3
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,2.0,Methodology,"We propose a two-stage training strategy, Cluster-Focal. The first stage consists of identifying different levels of poorly calibrated samples. In the second stage, we introduce a group-wise focal loss to mitigate the calibration bias. At test time, our model can mitigate biases for a variety of relevant subgroups of interest. We denote D = {(x i , y i )} N i=1 as a dataset, where x i represents multi-modal medical images and y i ∈ {1, 2, . . . } are the corresponding ground-truth class label. A neural network f produces pi,y = f (y|x i ), the predicted probability for a class y given x i . The predicted class for an x i is defined as ŷi = argmax y pi,y , with the corresponding prediction confidence pi = pi,ŷi .",vol3
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Stage 1: Identifying Poorly Calibrated Samples (Clustering).,"In this stage, we first train a model f id via ERM  where pi is the confidence score of the predicted class.  where Intuitively, the focal loss penalizes confident predictions with an exponential term (1 -f pred (y i |x i )) γ , thereby reducing the chances of poor calibration  Additionally, due to clustering based on gap(x i ), poorly calibrated samples will end up in the same cluster. The number of samples in this cluster will be small compared to other clusters for any model with good overall performance. As such, doing focal loss separately on each cluster instead of on all samples will implicitly increase the weight of poorly calibrated samples and help reduce bias.",vol3
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,2.2,Test Time Evaluation on Subgroups of Interest,"At test time, we aim to mitigate the calibration error for the worst-performing subgroup for various subgroups of interest  In practice, calibration performance cannot be considered in isolation, as there always exists a shortcut model that can mitigate calibration bias but have poor prediction performance, e.g., consider a purely random (under-confident) prediction with low accuracy. As such, there is an inherent trade-off between calibration bias and prediction error. When measuring the effectiveness of the proposed method, the objective is to ensure that calibration bias is mitigated without a substantial increase in the prediction error. ",vol3
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,3.0,Experiments and Results,"Experiments are performed on two different medical image analysis tasks. We evaluate the performance of the proposed method against popular debiasing methods. We examine whether these methods can mitigate calibration bias without severely sacrificing performance on the worst-performing subgroups. Task 1: Skin lesion multi-class (n = 7) classification. HAM10000 is a public skin lesion classification dataset containing 10,000 photographic 2D images of skin lesions. We utilize a recent MedFair pipeline ",vol3
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Task 2: Future new multiple sclerosis (MS) lesional activity prediction (binary classification).,"We leverage a large multi-centre, multi-scanner proprietary dataset comprised of MRI scans from 602 RRMS (Relapsing-Remitting MS) patients during clinical trials for new treatments ",vol3
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Implementation Details:,We adopt 2D/3D ResNet-18 ,vol3
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Comparisons and Evaluations:,"Macro-F1 is used to measure the performance for Task 1 (7 class), and F1-score is used for Task 2 (binary). Q-ECE ",vol3
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,3.1,"Results, Ablations, and Analysis","Results: The resulting performance vs. Q-ECE errors tradeoff plots for worstperforming subgroups are shown in Figs.  In addition to subgroups based on sensitive demographic attributes, we investigate how the methods perform on subgroups defined on medical image-derived features. In the context of MS, results based on subgroups, lesion load or Gadenhancing lesion count are shown in Fig. ",vol3
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Ablation Experiments:,Further experiments are performed to analyze the different components of our method. The following variant methods are considered: (1) Focal: Removing stage 1 and using regular focal loss for the entire training set; (2) Cluster-ERM: Group-wise focal loss in stage 2 is replaced by standard cross entropy; (3) Cluster-GroupDRO: Group-wise focal loss in stage 2 is replaced by GroupDRO  Calibration Curves: Figure ,vol3
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,4.0,Conclusions,"In this paper, we present a novel two stage calibration bias mitigation framework (Cluster-Focal) for medical image analysis that (1) successfully controls the trade-off between calibration error and prediction performance, and (2) flexibly overcomes calibration bias at test time without requiring pre-labeled subgroups during training. We further compared our proposed approach against different debiasing methods and under different subgroup splittings such as demographic subgroups and image-derived attributes. Our proposed framework demonstrates smaller calibration error in the worst-performing subgroups without a severe degradation in prediction performance.",vol3
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 19.,vol3
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,1.0,Introduction,"The U-Net  It has been proposed that anomalous activation patterns within the network, which differ from those that were observed during training, indicate problematic inputs  Our work introduces segmentation distortion, a novel and more specific measure of segmentation uncertainty under domain shift. It is motivated by the observation that latent space distances reliably detect images from a different scanner, but do not correlate strongly with segmentation errors within a given domain, as illustrated in Fig.  This yields a novel image-level uncertainty score, which is a better indicator of segmentation errors in out-of-distribution data than activation space distances or mean entropy. At the same time, it can be added to any existing U-Net, since it neither requires modification of its architecture nor its training.",vol3
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,2.0,Related Work,"The core of our method is to modify activation maps so that they become more similar to those that were observed during training, and to observe the effect of this after propagating the result through the remainder of the network. We use autoencoders for this, based on the observation that the difference r(x) -x between the reconstruction r(x) of a regularized autoencoder and its input x points towards regions of high density in the training data  This has previously motivated the use of autoencoders for unsupervised anomaly segmentation  Conditional variational autoencoders have been integrated into the U-Net to quantify uncertainty that arises from ambiguous labels ",vol3
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,3.1,"Autoencoder Architecture, Placement, and Loss","We adapted a U-shaped autoencoder architecture which was successfully used in a recent comparative study  Using autoencoders to make activations more similar to those observed in the training data requires regularization  Since we want to use the difference between propagating the reconstruction r(x) instead of x through the remainder of the network as an indicator of segmentation uncertainty due to domain shift, the autoencoder should reconstruct activations from the training set accurately enough so that it has a negligible effect on the segmentation. However, the autoencoder involves spatial subsampling and thus introduces a certain amount of blurring. This proved problematic when applying it to the activations that get passed through the U-Net's skip connections, whose purpose it is to preserve resolution. Therefore, we only place an autoencoder at the lowest resolution level, as indicated in Fig.  While autoencoders are often trained with an 1 or 2 (MSE) loss, we more reliably met our goal of preserving the segmentation on the training data by introducing a loss that explicitly accounts for it. Specifically, let U (I) denote the logits (class scores before softmax) obtained by applying the U-Net U to an input image I without the involvement of the autoencoder, while U d • r(x) indicates that we apply the U-Net's decoder U d after replacing bottleneck activations x with the reconstruction r(x). We define the segmentation preservation loss as and complement it with the established 2 loss to induce a degree of consistency with the underlying activation space. Since in our experiments, the optimization did not benefit from an additional balancing factor, we aggregate both terms into our training objective",vol3
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,3.2,Segmentation Distortion,"We train the autoencoder so that, on in distribution (ID) images, it has almost no effect on the segmentation. Out of distribution (OOD), reconstructions diverge from the original activation. It is the goal of our segmentation distortion measure to quantify how much this affects the segmentation. Therefore, we define segmentation distortion (SD) by averaging the squared differences of class probabilities P (C p |U ) that are estimated by the U-Net U at pixel p ∈ P with and without the autoencoder, over pixels and classes c ∈ C: SD is defined similarly as the multi-class Brier score ",vol3
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,3.3,Implementation Details,"Optimizing the autoencoder with respect to L seg requires gradient flow through the U-Net's decoder. Our implementation makes use of PyTorch's pre-forward hook functionality to compute it while keeping the weights of the U-Net intact. The U-Nets and the corresponding autoencoders were trained on identical training sets, with Adam and default parameters, until the loss converged on a respective validation set. We crop images to uniform shape to accommodate our AEs with fixed-size latent dimension. This facilitated some of our ablations, but is not a requirement of our method itself, and might be avoided by fully convolutional AE architectures in future work. Our AEs were trained on single TITAN X GPUs for approximately three hours and exhausted the 11GB of VRAM through appropriate batching. Our code is publicly available on github. 4 Experiments",vol3
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,4.1,Experimental Setup,"We show results for two segmentation tasks, which are illustrated in Fig.  For both tasks, we train U-Nets on one of the domains, with an architecture similar to previous work on domain shift in image segmentation  Figure ",vol3
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,4.2,Correlation with Segmentation Errors,"The goal of our proposed segmentation distortion (SD) is to identify images in which segmentation errors arise due to a domain shift. To quantify whether this goal has been met, we report rank correlations with (1-Dice), so that positive correlations will indicate a successful detection of errors. The use of rank correlation eliminates effects from any monotonic normalization or re-calibration of our uncertainty score. Figure  On both segmentation tasks, SD correlates much more strongly with segmentation errors than PM. The correlation of LM is usually in between the two, indicating that the benefit from our method is not just due to replacing the simpler pooling strategy with an autoencoder, but that passing its reconstruction through the remainder of the U-Net is crucial for our method's effectiveness. As another widely used uncertainty measure, Fig.  We note that ensembling affects not just the uncertainty estimates, but also the underlying segmentations, which are now obtained by averaging over all ensemble members. This leads to slight increases in Dice, and makes the results from ensembling less directly comparable to the others. We also investigated the effects of our autoencoder on downstream segmentation accuracy in out-of-distribution data, but found that it led to a slight reduction in Dice. Therefore, we keep the segmentation masks from the unmodified U-Net, and only use the autoencoder for uncertainty quantification.",vol3
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,4.3,Out-of-Distribution Detection,"The distance-based method PM was initially introduced for out-of-distribution (OOD) detection, i.e., detecting whether a given image has been taken with the same device as the images that were used for training  For this purpose, we report the AUROC for the five uncertainty scores based on their classification of images as whether they were drawn from a target domain or an in-domain validation set. As before, we evaluate each target domain separately for all independently trained U-Nets. For the M&MS dataset, results are displayed in Fig.  This experiment confirms the excellent results for OOD detection that were reported previously for the PM method  Of course, a method that successfully solves OOD detection can be used to reject OOD inputs, and thereby avoid silent failures that arise due to domain shifts. However, it can be seen from Fig.  It is a known limitation of the PM method, which our Segmentation Distortion seeks to overcome, that ""many OOD cases for which the model did produce adequate segmentation were deemed highly uncertain"" ",vol3
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,5.0,Discussion and Conclusion,"In this work, we introduced Segmentation Distortion as a novel approach for the quantification of segmentation uncertainty under domain shift. It is based on using an autoencoder to modify activations in a U-Net so that they become more similar to activations observed during training, and quantifying the effect of this on the final segmentation result. Experiments on two different datasets, which we re-ran multiple times to assess the variability in our results, confirm that our method more specifically detects erroneous segmentations than anomaly scores that are based on latent space distances  Finally, we observed that different techniques for uncertainty quantification under domain shift have different strengths, and we argue that they map to different use cases. If safety is a primary concern, reliable OOD detection should provide the strongest protection against the risk of silent failure, at the cost of excluding inputs that would be adequately processed. On the other hand, a stronger correlation with segmentation errors, as it is afforded by our approach, could be helpful to prioritize cases for proofreading, or to select cases that should be annotated to prepare training data for supervised domain adaptation.",vol3
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,1.0,Introduction,"The success of deep neural networks heavily relies on the availability of large and diverse annotated datasets across a range of computer vision tasks. To learn a strong data representation for robust and performant medical image segmentation, huge datasets with either many thousands of annotated data structures or less specific self-supervised pretraining objectives with unlabeled data are needed  MultiTalent can be used in two scenarios: First, in a combined multi-dataset (MD) training to generate one foundation segmentation model that is able to predict all classes that are present in any of the utilized partially annotated datasets, and second, for pre-training to leverage the learned representation of this foundation model for a new task. In experiments with a large collection of abdominal CT datasets, the proposed model outperformed state-of-the-art segmentation networks that were trained on each dataset individually as well as all previous methods that incorporated multiple datasets for training. Interestingly, the benefits of MultiTalent are particularly notable for more difficult classes and pathologies. In comparison to an ensemble of single dataset solutions, MultiTalent comes with shorter training and inference times. Additionally, at the example of three challenging datasets, we demonstrate that fine-tuning MultiTalent yields higher segmentation performance than training from scratch or initializing the model parameters using unsupervised pretraining strategies ",vol3
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,2.0,Methods,"We introduce MultiTalent, a multi dataset learning and pre-training method, to train a foundation medical image segmentation model. It comes with a novel dataset and class adaptive loss function. The proposed network architecture enables the preservation of all label properties, learning overlapping classes and the simultaneous prediction of all classes. Furthermore, we introduce a training schedule and dataset preprocessing which balances varying dataset size and class characteristics.",vol3
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,2.1,Problem Definition,"We begin with a dataset collection of K datasets , where C (k) ⊆ C is the label set associated to dataset D (k) . Even if classes from different datasets refer to the same target structure we consider them as unique, since the exact annotation protocols and labeling characteristics of the annotations are unknown and can vary between datasets: C (k) ∩ C (j) = ∅, ∀k = j. This implies that the network must be capable of predicting multiple classes for one voxel to account for the inconsistent class definitions.",vol3
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,2.2,MultiTalent,"Network Modifications. We employ three different network architectures, which are further described below, to demonstrate that our approach is applicable to any network topology. To solve the label contradiction problem we decouple the segmentation outputs for each class by applying a Sigmoid activation function instead of the commonly used Softmax activation function across the dataset. The network shares the same backbone parameters Θ but it has independent segmentation head parameters Θ c for each class. The Sigmoid probabilities for each class are defined as ŷc = f (x, Θ, Θ c ). This modification allows the network to assign multiple classes to one pixel and thus enables overlapping classes and the conservation of all label properties from each dataset. Consequently, the segmentation of each class can be thought of as a binary segmentation task.  While the regular dice loss is calculated for each image within a batch, we calculate the dice loss jointly for all images of the input batch. This regularizes the loss if only a few voxels of one class are present in one image and a larger area is present in another image of the same batch. Thus, an inaccurate prediction of a few pixels in the first image has a limited effect on the loss. In the following, we unite the sum over the image voxels i and the batch b to z . We modify the loss function to be calculated only for classes that were annotated in the corresponding partially labeled dataset  and 0 otherwise. Instead of averaging, we add up the loss over the classes. Hence, the loss signal for each class prediction does not depend on the number of other classes within the batch. This compensates for the varying number of annotated classes in each dataset. Otherwise, the magnitude of the loss e.g. for the liver head from D1 (2 classes) would be much larger as for D7  Network Architectures. To demonstrate the general applicability of this approach, we applied it to three segmentation networks. We employed a 3D U-Net  Multi-dataset Training Setup. We trained MultiTalent with 13 public abdominal CT datasets with a total of 1477 3D images, including 47 classes (Multi-dataset (MD) collection)  Transfer Learning Setup. We used the BTCV (small multi organ dataset ",vol3
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,2.3,Baselines,"As a baseline for the MultiTalent, we applied the 3D U-Net generated by the nnU-Net without manual intervention to each dataset individually. Furthermore, we trained a 3D U-Net, a Resenc U-Net and a SwinUNETR with the same network topology, patch and batch size as our MultiTalent for each dataset. All baseline networks were also implemented within the nnU-Net framework and follow the default training procedure. Additionally, we compare MultiTalent with related work on the public BTCV leaderboard in Table ",vol3
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,3.0,Results,Multi-dataset training results are presented in Fig.  Transfer learning results are found in Table ,vol3
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_62.,vol3
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,1.0,Introduction,"Supervised learning of deep neural networks is susceptible to overfitting when labelled training datasets are small, as is often the case in medical image analysis. Data augmentation (DA) tackles this issue by transforming (informed by knowledge of task-specific invariances and equivariances) labelled input-output pairs, thus simulating new input-output pairs to expand the training dataset. This idea is used in semi-supervised learning  While previous work has employed CR to leverage unlabelled images, we show that even in the absence of any additional unlabelled images, CR improves calibration  To answer this question, we note that boundaries between anatomical regions are often ambiguous in medical images due to absence of sufficient contrast or presence of image noise or partial volume effects. Annotations in labelled segmentation datasets, however, typically comprise of hard class assignments for each pixel, devoid of information regarding such ambiguity. Supervised learning approaches then insist on perfect agreement at every pixel between predictions and ground truth labels, which can be achieved by over-parameterized neural networks. For instance, using the cross-entropy loss function for training maximizes logit differences between the ground truth class and other classes for each pixel  This viewpoint suggests that reduced logit differences across classes for pixels with ambiguous labels may help counter such miscalibration. Based on this idea, we make two main contributions in this paper. First, we show that CR can automatically discover such pixels and prevent overfitting to their noisy labels. In doing so, CR induces a spatially varying pixel-wise regularization effect, leading to improved calibration. In contrast to previous use of CR in medical image segmentation ",vol3
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,2.0,Related Work,Label ambiguity in medical image segmentation is tackled either by generating multiple plausible segmentations for each image ,vol3
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,3.0,Methods,"Using a labelled dataset {(X i , Y i )}, i = 1, 2, . . . n, we wish to learn a function that maps images X ∈ R H×W to segmentation labelmaps Y ∈ {1, 2, ..., C} H×W , where C is the number of classes. Let f θ be a convolutional neural network that predicts Ŷ = σ(f θ (X)), where f θ (X) ∈ R H×W ×C are logits and σ is the softmax function. In supervised learning, optimal parameter values are obtained by minimizing an appropriate supervised loss, θ = argmin θ E X,Y L s (σ(f θ (X)), Y ). Data augmentation (DA) leverages knowledge that the segmentation function is invariant to intensity transformations S φ (e.g., contrast and brightness modifications, blurring, sharpening, Gaussian noise addition) and equivariant to geometric transformations T ψ (e.g., affine and elastic deformations).",vol3
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,,The optimization becomes θ = argmin,"In order to achieve equivariance with respect to T ψ , the loss is computed after applying the inverse transformation to the logits. Consistency regularization (CR) additionally constrains the logits predicted for similar images to be similar. This is achieved by minimizing a consistency loss L c between logits predicted for two transformed versions of the same image: θ The exact strategy for choosing arguments of L c can vary: as above, we use predictions of the same network θ for different transformations (φ, ψ) and (φ , ψ ) ",vol3
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,3.1,Consistency Regularization at Pixel-Level,"Here, we show how understanding the relative behaviours of the supervised and unsupervised losses used in CR help to improve calibration. Common choices for L s and L c are pixel-wise cross-entropy loss and pixel-wise sum-of-squares loss, respectively. For these choices, the total loss for pixel j can be written as follows: where z j and z j are C-dimensional logit vectors at pixel j in g(X; θ, φ, ψ) and g(X; θ, φ , ψ ) respectively, and the subscript c indexes classes. L s drives the predicted probability of the ground truth label class to 1, and those of all other classes to 0. Such low-entropy predictions are preferred by the loss function even for pixels whose predictions should be ambiguous due to insufficient image contrast, partial volume effect or annotator mistakes. Consistency loss L c encourages solutions with consistent logit predictions across stochastic transformations. This includes, but is not restricted to, the low-entropy solution preferred by L s . In fact, it turns out that due to the chosen formulation of L s in the probability space and L c in the logit space, deviations  from logit consistency are penalized more strongly than deviations from lowentropy predictions. Thus, L c permits high confidence predictions only for pixels where logit consistency across stochastic transformations can be achieved. Furthermore, variability in predictions across stochastic transformations has been shown to be indicative of aleatoric image uncertainty ",vol3
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,,Special Case of Binary Segmentation:,"To illustrate the pixel-wise regularization effect more clearly, let us consider binary segmentation. Here, we can fix z 1 = 0 and let z 2 = z, as only logit differences matter in the softmax function. Further, let us consider only one pixel, drop the pixel index and assume that its ground truth label is c = 2. Thus, y 1 = 0 and y 2 = 1. With these simplifications, L s =log(σ(z)) and L c = (zz ) 2 . Figure ",vol3
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,3.2,Spatially Varying Weight for Consistency Regularization,"Understanding consistency regularization as mitigation against overfitting to hard labels in ambiguous pixels points to a straightforward improvement of the method. Specifically, the regularization term in the overall loss should be weighed higher when higher pixel ambiguity, and thus, higher label noise, is expected. Natural candidates for higher ambiguity are pixels near label boundaries. Accordingly, we propose boundary-weighted consistency regularization (BWCR): where r j is the distance to the closest boundary from pixel j, λ(r j ) drops away from the label boundaries, and R is the width of the boundary region affected by the regularization. We compute r j = argmin c r j c , where r j c is the absolute value of the euclidean distance transform ",vol3
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,,Datasets:,"We investigate the effect of CR on two public datasets. The NCI  Data Splits: From the N subjects in each dataset, we select N ts test, N vl validation and N tr training subjects. {N ts , N vl } are set to {30, 4} for NCI and {50, 5} for ACDC. We have 3 settings for N tr : small, medium and large, with N tr as 6, 12 and 36 for NCI, and 5, 10 and 95 for ACDC, in the three settings, respectively. All experiments are run thrice, with test subjects fixed across runs, and training and validation subjects randomly sampled from remaining subjects. In each dataset, subjects in all subsets are evenly distributed over different scanners.",vol3
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,,Pre-processing:,We correct bias fields using the N4  Training Details: We use a 2D U-net  Evaluation Criteria: We evaluate segmentation accuracy using Dice similarity coefficient and calibration using Expected Calibration Error (ECE) ,vol3
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,4.1,Effect of CR,"First, we check if CR improves calibration of segmentation models. We perform this experiment in the small training dataset setting, and present results in Table  Table ",vol3
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,4.2,Effect of BWCR,"We compare CR and BWCR with the following baseline methods: (1) supervised learning without DA (Baseline), (2) data augmentation (DA) ",vol3
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,5.0,Conclusion,"We developed a method for improving calibration of segmentation neural networks by noting that consistency regularization mitigates overfitting to ambiguous labels, and building on this understanding to emphasize this regularization in pixels most likely to face label noise. Future work can extend this approach for lesion segmentation and/or 3D models, explore the effect of other consistency loss functions (e.g. cosine similarity or Jensen-Shannon divergence), develop other strategies to identify pixels that are more prone to ambiguity, or study the behaviour of improved calibration on out-of-distribution samples.",vol3
Co-assistant Networks for Label Correction,1.0,Introduction,"The success of deep neural networks (DNNs) mainly depends on the large number of samples and the high-quality labels. However, either of them is very difficult to be obtained for conducting medical image analysis with DNNs. In particular, obtaining high-quality labels needs professional experience so that corrupted labels can often be found in medical datasets, which can seriously degrade the effectiveness of medical image analysis. Moreover, sample annotation needs expensive cost. Hence, correcting corrupted labels might be one of effective solutions to solve the issues of high-quality labels. Numerous works have been proposed to tackle the issue of corrupted labels. Based on whether correcting corrupted labels, previous methods can be roughly divided into two categories, i.e., robustness-based methods  To address the aforementioned issues, in this paper, we propose a new co-assistant framework, namely Co-assistant Networks for Label Correction (CNLC) (shown in Fig. ",vol3
Co-assistant Networks for Label Correction,2.0,Methodology,"In this section, our proposed method first designs a noise detector to discriminate corrupted samples from all samples, and then investigates a noise cleaner to correct the detected corrupted labels.",vol3
Co-assistant Networks for Label Correction,2.1,Noise Detector,"Noise detector is used to distinguish corrupted samples from clean samples. The prevailing detection method is designed to first calculate the loss of DNNs on all training samples and then distinguish corrupted samples from clean ones based on their losses. Specifically, the samples with small losses are regarded as clean samples while the samples with large losses are regarded as corrupted samples. Different from previous literature  where b is the number of samples in each batch, p t i [j] represents the j-th class prediction of the i-th sample in the t-th epoch, ỹi ∈ {0, 1, ..., C -1} denotes the corrupted label of the i-th sample x i , C denotes the number of classes and λ(t) is a time-related hyper-parameter. In Eq. (  In label partition, based on the resistant loss in Eq. (  samples, and corrupted samples. Specifically, the samples with n 1 smallest losses are regarded as clean samples and the samples with n 1 largest loss values are regarded as corrupted samples, where n 1 is experimentally set as 5.0% of all training sample for each class. The rest of the training samples for each class are regarded as uncertain samples. In noise detector, our goal is to identify the real clean samples and real corrupted samples, which are corresponded to set as positive samples and negative samples in noise cleaner. If we select a large number of either clean samples or corrupted samples (e.g., larger than 5.0% of all training samples), they may contain false positive samples or false negative samples, so that the effectiveness of the noise cleaner will be influenced. As a result, our noise detector partitions all training samples for each class into three subgroups, including a small proportion of clean samples and corrupted samples, as well as uncertain samples.",vol3
Co-assistant Networks for Label Correction,2.2,Noise Cleaner,"Noise cleaner is designed to correct labels of samples with corrupted labels. Recent works often employ DNNs (such as CNN  The inputs of each class-based GCN include labeled samples and unlabeled samples. The labeled samples consist of positive samples (i.e., the clean samples of this class with the new label z ic = 1 for the i-th sample in the c-th class) and negative samples (i.e., the corrupted samples of this class with the new label z ic = 0). The unlabeled samples include a subset of the uncertain samples from all classes and corrupted samples of other classes. We follow the principle to select uncertain samples for each class, i.e., the higher the resistant loss in Eq. (  Given the resistant loss in Eq. (  Hence, the noise rate r of training samples is calculated by: where n represents the total number of samples in training dataset. Supposing the number of samples in the c-th class is s c , the number of uncertain samples of each class is s c × r -n 1 . Hence, the total number of unlabeled samples for each class is n × r -n 1 in noise cleaner. Given 2 × n 1 labeled samples and n × r -n 1 unlabeled samples, the classbased GCN for each class conducts semi-supervised learning to predict n × r samples, including n × r -n 1 unlabeled samples and n 1 corrupted samples for this class. The semi-supervised loss L ssl includes a binary cross-entropy loss L bce for labeled samples and an unsupervised loss L mse  where q t ic denotes the prediction of the i-th sample in the t-th epoch for the class c, qt ic is updated by qt ic = , where (t) is related to time  In corrupted label correction, given C well-trained GCNs and the similarity scores on each class for a subset of uncertain samples and all corrupted samples, their labels can be determined by: ỹi = argmax 0≤c≤C-1 (q ic ) . (6)",vol3
Co-assistant Networks for Label Correction,2.3,Objective Function,"The optimization of the noise detector is associated with the corrupted label set ỹ, which is determined by noise cleaner. Similarly, the embedding of all samples E is an essential input of the noise cleaner, which is generated by the noise detector. As the optimizations of two modules are nested, the objective function of our proposed method is the following bi-level optimization problem: In this paper, we construct a bi-level optimization algorithm to search optimal network parameters of the above objective function. Specifically, we optimize the noise detector to output an optimal feature matrix E * , which is used for the construction of the noise cleaner. Furthermore, the output ỹ * of the noise cleaner is used to optimize the noise detector. This optimization process alternatively optimize two modules until the noise cleaner converges. We list the optimization details of our proposed algorithm in the supplemental materials.",vol3
Co-assistant Networks for Label Correction,3.1,Experimental Settings,"The used datasets are BreakHis  We compare our proposed method with six popular methods, including one fundamental baseline (i.e., Cross-Entropy (CE)), three robustness-based methods (i.e., Co-teaching (CT) ",vol3
Co-assistant Networks for Label Correction,3.2,Results and Analysis,Table ,vol3
Co-assistant Networks for Label Correction,3.3,Ablation Study,"To verify the effectiveness of the noise cleaner, we compare our method with the following comparison methods: 1) W/O NC: without noise cleaner, and 2) MLP: replace GCN with Multi-Layer Perceptron, i.e., without considering the relationship among samples. Due to the space limitation, we only show results on ISIC, which is listed in the first and second rows of Table  To verify the effectiveness of the resistance loss in Eq. ( ",vol3
Co-assistant Networks for Label Correction,4.0,Conclusion,"In this paper, we proposed a novel co-assistant framework, to solve the problem of DNNs with corrupted labels for medical image analysis. Experiments on three medical image datasets demonstrate the effectiveness of the proposed framework. Although our method has achieved promising performance, its accuracy might be further boosted by using more powerful feature extractors, like pre-train models on large-scale public datasets or some self-supervised methods, e.g., contrastive learning. In the future, we will integrate these feature extractors into the proposed framework to further improve its effectiveness.",vol3
Co-assistant Networks for Label Correction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_16.,vol3
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,1.0,Introduction,"Retinal fundus images are widely used in diabetic retinopathy (DR) detection through a data-driven approach  Classic image processing methods for data augmentation include random rotation, vertical and horizontal flipping, cropping, random erasing and so on. In some cases, standard data augmentation methods can increase the size of dataset and prevent overfitting of the neural network  For the aforementioned problems, this paper proposes a data augmentation method for dealing with the class imbalanced problem by manipulating the lesions in DR images to increase the size and diversity of the pathologies while preserving the labels of the data. In this paper, we train a DR image generator based on StyleGAN3  The major contributions of this work can be summarized as follows. Firstly, we introduce LPIPS loss based on DR detection in the training phase of Style-GAN3 to make the reconstructed images have more realistic lesions. Secondly we apply channel locating method with gradient and backpropagation algorithms to identify the position of latent code with the highest contribution score to lesions in DR images. Finally, we perform manipulation on lesions for data augmentation. We also show the performance of proposed method for DR recognition, and explore the correlation between label preservation and editing strength.",vol3
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,2.0,Methods,Figure ,vol3
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,2.1,Part A: Training Phase of StyleGAN3,"StyleGAN3 has the ability to finely control the style of generated images. It achieves style variations by manipulating different directions in the latent vector space. In latent space of StyleGAN3, the latent code z ∈ Z is a stochastic normally distributed vector, always called Z space. After the fully connected mapping layers, z is transformed into a new latent space W  LPIPS Loss Based on DR Detection. Due to the good performance of LPIPS in generation tasks ",vol3
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,2.2,Part B: Detecting the Channel of Style Code Having Highest Contribution to Exudates,"As shown in Fig.  When we get output image from the generator of a well-trained StyleGAN3, we apply a pre-trained exudates segmentation network based on Unet  After y, the importance score of DR pathology is computed. We can calculate the contribution of each latent code to DR pathology in S space by computing the gradient of y with respect to each latent code in S space, based on backpropagation algorithm in neural network as shown in the Eq. 4. where R is the contribution score list of latent codes in S space, k ∈ K is the dimension of S space, R k is the contribution score of kth-dimension of S space. In our paper, S space has K dimensions in total including 6048 dimensions of feature maps and 3040 dimensions of tRGB blocks. So, in StyleGAN3, depending on the structure of the network, the K dimensions can be correspond to different layers l and channels c depending on the number of channels in each layers. L is the number of layers in StyleGAN3. Since we want to do image editing without affecting other features of the image as much as possible to avoid changes in image label. From the experiments in  To ensure that the collected style codes are not impacted by individual noise and are consistent across all images, we calculate the average of 1K images. Afterwards, the collected contribution list R are sorted from high to low based on contribution score of each latent code. To make it easier to understand we can convert k to (l, c) based on the layers and channels in StyleGAN3 network structure.",vol3
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,2.3,Part C: Manipulation on Real Images in Inference Phase,"We project the real images to style spaces using projector provided by Karras et al.  where S new denotes the S space after manipulation, I aug denotes the generated image after manipulation. α is the editing strength. μ is the mean vector of style space, which is obtained by projecting lots of real images to S space. m is a special vector with same dimension with S, the value of all elements in the vector is 0 except for the element at the (l, c) position, which is equal to 1. It means which layer and channel is selected.",vol3
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,3.1,Setup Details,Dataset. We carry out all experiments on publicly available datasets. We train StyleGAN3 on EyePACS  Evaluation Metric. We employ Frechet Inception Distance (FID) ,vol3
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,,Experimental Settings.,"All experiments are conducted on a single NVIDIA RTX A5000 GPU with 24GB memory using PyTorch implementation. For Style-GAN3, we initialize the learning rate with 0.0025 for G(•)and 0.002 for D(•). For optimization, we use the Adam optimizer with β 1 = 0.0, β 2 = 0.99. We empirically set λ lpips = 0.6, λ gan = 0.4. For evaluation of classification and detection tasks, we both initialize the learning rate with 0.001 for the training of VGG16. ",vol3
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,3.2,Evaluation and Results,"Results of Synthetic Images. For visual evaluation of generation of Style-GAN3, we show some synthetic images in Fig. ",vol3
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,4.0,Conclusions,"In this paper we propose a label-preserving data augmentation method to deal with the classes imbalanced problem in DR detection. The proposed approach computes the contribution score of latent code, and perform manipulation on the lesion of real images. In this way, the method can augment the dataset in a label-preserving manner. It is a targeted and effective label-preserving data augmentation approach. Although the paper mainly discusses exudates, the proposed method can be applied to other types of lesions as long as we have the lesion mask and the GAN model can generate realistic lesions. Data Declaration. Data underlying the results presented in this paper are available in APTOS ",vol3
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,1.0,Introduction,"Positron emission tomography (PET) image synthesis  However, the generalizability of existing models can still be suboptimal for a multi-center study due to domain shift, which results from non-identical data distribution among centers with different imaging systems/protocols (see Fig.  A recent trend, known as generalist models, is to request that a single unified model works for multiple tasks/domains, and even express generalizability to novel tasks/domains. By sharing architecture and parameters, generalist models can better utilize shared transferable knowledge across tasks/domains. Some pioneers  Nonetheless, recent studies  In this paper, inspired by the aforementioned studies, we innovatively propose a generalist model with Dynamic Routing for Multi-Center PET image synthesis, termed DRMC. To mitigate the center interference issue, we propose a novel dynamic routing strategy to route data from different centers to different experts. Compared with existing routing strategies, our strategy makes an improvement by building cross-layer connections for more accurate expert decisions. Extensive experiments show that DRMC achieves the best generalizability on both known and unknown centers. Our contribution can be summarized as:  ",vol3
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,2.1,Center Interference Issue,"Due to the non-identical data distribution across centers, different centers with shared parameters may conflict with each other in the optimization process. To verify this hypothesis, we train a baseline Transformer with 15 base blocks (Fig. ",vol3
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,2.2,Network Architecture,The overall architecture of our DRMC is shown in Fig. ,vol3
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,2.3,Dynamic Routing Strategy,"We aim at alleviating the center interference issue in deep feature extraction. Inspired by prior generalist models  Base Expert Foundation. As shown in Fig.  It consists of a 2-layer MLP with GELU activation in between. Expert Number Scaling. Center interference is observed on both attention experts and FFN experts at different layers (see Fig.  , both of which have M base experts. However, it does not mean that we prepare specific experts for each center. Although using center-specific experts can address the interference problem, it is hard for the model to exploit the shared knowledge across centers, and it is also difficult to generalize to new centers that did not emerge in the training stage  Expert Dynamic Routing. Given a bank of experts, we route data from different centers to different experts so as to avoid interference. Prior generalist models  where X denotes the input; GAP(•) represents the global average pooling operation to aggregate global context information of the current layer; H is the hidden representation of the previous MLP layer. ReLU activation generates sparsity by setting the negative weight to zero. W is a sparse weight used to assign weights to different experts. In short, DRM sparsely activates the model and selectively routes the input to different subsets of experts. This process maximizes collaboration and meanwhile mitigates the interference problem. On the one hand, the interference across centers can be alleviated by sparsely routing X to different experts (with positive weights). The combinations of selected experts can be thoroughly different across centers if violent conflicts appear. On the other hand, experts in the same bank still cooperate with each other, allowing the network to best utilize the shared knowledge across centers.",vol3
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,,Expert Sparse Fusion.,"The final output is a weighted sum of each expert's knowledge using the sparse weight W = [W 1 , W 2 , ..., W M ] generated by DRM. Given an input feature X, the output X of an expert bank can be obtained as: where E m (•) represents an operator of E m AT T (•) or E m F F N (•). ",vol3
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,2.4,Loss Function,We utilize the Charbonnier loss  3 Experiments and Results,vol3
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,3.1,Dataset and Evaluation,"Full-dose PET images are collected from 6 different centers (C 1 -C 6 ) at 6 different institutions 1 . The data of C 3 and C 4  To evaluate the model performance, we choose the PSNR metric for image quantitative evaluation. For clinical evaluation, to address the accuracy of the standard uptake value (SUV) that most radiologists care about, we follow the paper ",vol3
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,3.2,Implementation,"Unless specified otherwise, the intermediate channel number, expert number in a bank, and Transformer block number are 64, 3, and 5, respectively. We employ Adam optimizer with a learning rate of 10 -4 . We implement our method with Pytorch using a workstation with 4 NVIDIA A100 GPUs with 40GB memory (1 GPU per center). In each training iteration, each GPU independently samples data from a single center. After the loss calculation and the gradient backpropagation, the gradients of different GPUs are then synchronized. We train our model for 200 epochs in total as no significant improvement afterward.",vol3
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,3.3,Comparative Experiments,"We compare our method with five methods of two types. (i) 3D-cGAN  All methods are trained using data from C kn and tested over both C kn and C ukn . For methods in (i), we regard C kn as a single center and mix all data together for training. For federated learning methods in (ii), we follow the ""Mix"" mode (upper bound of FL-based methods) in the paper  Comparison Results for Known Centers. As can be seen in Table  Comparison Results for Unknown Centers. We also test the model generalization ability to unknown centers C 5 and C 6 . C 5 consists of normal brain data (without lesion) that is challenging for generalization. As the brain region only occupies a small portion of the whole-body data in the training dataset but has more sophisticated structure information. C 6 is a similar center to C 1 but has different working locations and imaging preferences. The quantitative results are shown in Table ",vol3
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,3.4,Ablation Study,Specialized Model vs. Generalist Model. As can be seen in Table  Ablation Study of Hyperparameters. In Fig. ,vol3
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,4.0,Conclusion,"In this paper, we innovatively propose a generalist model with dynamic routing (DRMC) for multi-center PET image synthesis. To address the center interference issue, DRMC sparsely routes data from different centers to different experts. Experiments show that DRMC achieves excellent generalizability.",vol3
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 4.,vol3
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,1.0,Introduction,"Computed Tomography (CT) and Magnetic Resonance (MR) are two widely used imaging techniques in clinical practice. CT imaging uses X-rays to produce detailed, cross-sectional images of the body, which is particularly useful for imaging bones and detecting certain types of cancers with fast imaging speed. However, CT imaging has relatively high radiation doses that can pose a risk of radiation exposure to patients. Low-dose CT techniques have been developed to address this concern by using lower doses of radiation, but the image quality is degraded with increased noise, which may compromise diagnostic accuracy  MR imaging, on the other hand, uses a strong magnetic field and radio waves to create detailed images of the body's internal structures, which can produce high-contrast images for soft tissues and does not involve ionizing radiation. This makes MR imaging safer for patients, particularly for those who require frequent or repeated scans. However, MR imaging typically has a lower resolution than CT  Motivated by the aforementioned, there is a pressing need to improve the quality of low-dose CT images and low-resolution MR images to ensure that they provide the necessary diagnostic information. Numerous algorithms have been developed for CT and MR image enhancement, with deep learning-based methods emerging as a prominent trend  These algorithms are capable of improving image quality, but they have two significant limitations. First, paired images are required for training, e.g., low-dose and full-dose CT images; low-resolution and high-resolution MR images). However, acquiring such paired data is challenging in real clinical scenarios. Although it is possible to simulate low-quality images from high-quality images, the models derived from such data may have limited generalization ability when applied to real data  Recently, pre-trained diffusion models  In this paper, we aim at addressing the limitations of existing image enhancement methods and the scarcity of pre-trained diffusion models for medical images. Specifically, we provide two well-trained diffusion models on full-dose CT images and high-resolution heart MR images, suitable for a range of applications including image generation, denoising, and super-resolution. Motivated by the existing plug-and-play image restoration methods ",vol3
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,2.0,Method,This section begins with a brief overview of diffusion models for image generation and the mathematical model and algorithm for general image enhancement. We then introduce a plug-and-play framework that harnesses the strengths of both approaches to enable unsupervised medical image enhancement.,vol3
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,2.1,Denoising Diffusion Probabilistic Models (DDPM) for Unconditional Image Generation,"Image generation models aim to capture the intrinsic data distribution from a set of training images and generate new images from the model itself. We use DDPM  where t ∈ {1, • • • , T } represents the current timestep, x t and x t-1 are adjacent image status, and is a predefined noise schedule. Furthermore, we can directly obtain x t based on x 0 at any timestep t by: where α t := 1-β t , ᾱt := Π t s=1 α s , and ∼ N (0, I). This property enables simple model training where the input is the noisy image x t and the timestep t and the output is the predicted noise θ (θ denotes model parameters). Intuitively, a denoising network is trained with the mean square loss E t,x ||θ (x t , t)|| 2 . The sampling process aims to generate a clean image from Gaussian noise x T ∼ N (0, I), and each reverse step is defined by:",vol3
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,2.2,Image Enhancement with Denoising Algorithm,"In general, image enhancement tasks can be formulated by: where y is the degraded image, H is a degradation matrix, x is the unknown original image, and n is the independent random noise. This model can represent various image restoration tasks. For instance, in the image denoising task, H is the identity matrix, and in the image super-resolution task, H is the downsampling operator. The main objective is to recover x by solving the minimization problem: x * = arg min where the first data-fidelity term keeps the data consistency and the second dataregularization term R(x) imposes prior knowledge constraints on the solution. This problem can be solved by the Iterative Denoising and Backward Projections (IDBP) algorithm  where is the pseudo inverse of the degradation matrix H and f H T H := f T H T Hf. Specifically, x * and ŷ * can be alternatively estimated by solving min x ŷx 2 2 + R(x) and min ŷ ŷx 2 2 s.t. H ŷ = y. Estimating x * is essentially a denoising problem that can be solved by a denoising operator and ŷ * has a closed-form solution: Intuitively, IDBP iteratively estimates the original image from the current degraded image and makes a projection by constraining it with prior knowledge. Although IDBP offers a flexible way to solve image enhancement problems, it still requires paired images to train the denoising operator ",vol3
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,2.3,Pre-Trained Diffusion Models for Plug-and-play Medical Image Enhancement,"We introduce a plug-and-play framework by leveraging the benefits of the diffusion model and IDBP algorithm. Here we highlight two benefits: (1) it removes the need for paired images; and (2) it can simply apply the single pre-trained diffusion model across multiple medical image enhancement tasks. First, we reformulate the DDPM sampling process  and Intuitively, each sampling iteration has two steps. The first step estimates the denoised image x 0|t based on the current noisy image x t and the trained denoising network θ (x t , t). The second step generates a rectified image x t-1 by taking a weighted sum of x 0|t and x t and adding a Gaussian noise perturbation. As mentioned in Eq. (  It can be easily proved that H x0|t = HH † y + Hx 0|t -HH † Hx 0|t = y. By replacing x 0|t with x0|t in Eq. (  Algorithm 1 shows the complete steps for image enhancement, which inherit the denoising operator from DDPM and the projection operator from IDBP. The former employs the strong denoising capability in the diffusion model and the latter can make sure that the generated results match the input image. Notably, the final algorithm is equivalent to DDNM ",vol3
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,,Algorithm 1. Pre-trained DDPM for plug-and-play medical image enhancement,"Require: Pre-trained DDPM θ , low-quality image y, degradation operator H 1: Initialize xT ∼ N (0, I). 2: for t = T to 1 do 3: ",vol3
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,3.0,Experiments,"Dataset. We conducted experiments on two common image enhancement tasks: denoising and SR. To mimic the real-world setting, the diffusion models were trained on a diverse dataset, including images from different centers and scanners. The testing set (e.g., MR images) is from a new medical center that has not appeared in the training set. Experiments show that our model can generalize to these unseen images. Specifically, the denoising task is based on the AAPM Low Dose CT Grand Challenge abdominal dataset  Evaluation Metrics. The image quality was quantitatively evaluated by the Peak Signal-to-Noise Ratio (PSNR), Structural SIMilarity index (SSIM)  Comparison with Other Methods. The pseudo-inverse operator H † was used as the baseline method, namely, x * = H † y. We also compared the present method with one commonly used image enhancement method DIP ",vol3
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,4.0,Results and Discussion,"Low-Dose CT Image Enhancement. The presented method outperformed all other methods on the denoising task in all metrics, as shown in Table ",vol3
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,5.0,Conclusion,"In summary, we have provided two well-trained diffusion models for abdomen CT and heart MR, and introduced a plug-and-play framework for image enhancement. Our experiments have demonstrated that a single pre-trained diffusion model could address different degradation levels without customized models. However, there are still some limitations to be solved. The degradation operator and its pseudo-inverse should be explicitly given, which limits its application in tasks such as heart MR motion deblurring. Although the present method is in general applicable for 3D images, training the 3D diffusion model still remains prohibitively expensive. Moreover, the sampling process currently requires multiple network inferences, but it could be solved with recent advances in one-step generation models ",vol3
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_1.,vol3
Robust T-Loss for Medical Image Segmentation,1.0,Introduction,"Convolutional Neural Networks (CNNs) and Visual Transformers (ViTs) have become the standard in semantic segmentation, achieving state-of-the-art results in many applications  For instance, the Fitzpatrick 17k dataset, commonly used in dermatology research, contains non-skin images and noisy annotations. In a random sample of 504 images, 5.4% were labeled incorrectly or as other classes  Previous literature has explored many methods to mitigate the problem of noisy labels in deep learning. These methods can be broadly categorized into label correction  Although previous methods have shown robustness in semantic segmentation, they often have limitations, such as more hyper-parameters, modifications to the network architecture, or complex training procedures. In contrast, robust loss functions offer a much simpler solution as they could be incorporated with a simple change in a single modeling component. However, their effectiveness has not been thoroughly investigated. In this work, we show that several traditional robust loss functions are vulnerable to memorizing noisy labels. To overcome this problem, we introduce a novel robust loss function, the T-Loss, which is inspired by the negative loglikelihood of the Student-t distribution. The T-Loss, whose simplest formulation features a single parameter, can adaptively learn an optimal tolerance level to label noise directly during backpropagation, eliminating the need for additional computations such as the Expectation Maximization (EM) steps. To evaluate the effectiveness of the T-Loss as a robust loss function for medical semantic segmentation, we conducted experiments on two widely-used benchmark datasets in the field: one for skin lesion segmentation and the other for lung segmentation. We injected different levels of noise into these datasets that simulate typical human labeling errors and trained deep learning models using various robust loss functions. Our experiments demonstrate that the T-Loss outperforms these robust state-of-the-art loss functions in terms of segmentation accuracy and robustness, particularly under conditions of high noise contamination. We also observed that the T-Loss could adaptively learn the optimal tolerance level to label noise which significantly reduces the risk of memorizing noisy labels. This research is divided as follows: Sect. 2 introduces the motivation behind our T-Loss and provides its mathematical derivation. Section 3 covers the datasets used in our experiments, the implementation and training details of T-Loss, and the metrics used for comparison. Section 4 presents the main findings of our study, including the results of the T-Loss and the baselines on both datasets and an ablation study on the parameter of T-Loss. Finally, in Sect. 5, we summarize our contributions and the significance of our study for the field.",vol3
Robust T-Loss for Medical Image Segmentation,2.0,Methodology,"Let x i ∈ R c×w×h be an input image and y i ∈ {0, 1} w×h be its noisy annotated binary segmentation mask, where c represents the number of channels, w the image's width, and h its height. Given a set of images {x 1 , . . . , x N } and corresponding masks {y 1 , . . . , y N }, our goal is to train a model f w with parameters w such that f w (x) approximates the accurate binary segmentation mask for any given image x. To this end we note that, heuristically, assuming error terms to follow a Student-t distribution (as suggested e.g. in  where µ and Σ are respectively the mean and the covariance matrix of the associated multivariate normal distribution, ν is the number of degrees of freedom, and | • | indicates the determinant (see e.g.  Since the common Mean Squared Error (MSE) loss is derived by minimizing the negative log-likelihood of the normal distribution, we choose to apply the same transformation and get The functional form of our loss function for one image is then obtained with the identification y = y i and the approximation µ = f w (x i ), and aggregated with Equation (  To clarify the relation with known loss functions, let δ = |y i -f w (x i )|, and fix the value of ν. For δ → 0, the functional dependence from δ reduces to a linear function of δ 2 , i.e. MSE. For large values of δ, though, Eq. (  We optimize the parameter ν jointly with w using gradient descent. To this end, we reparametrize ν = e ν + where is a safeguard for numerical stability. Loss functions with similar dynamic tolerance parameters were also studied in ",vol3
Robust T-Loss for Medical Image Segmentation,3.0,Experiments,"In this section, we demonstrate the robustness of the T-Loss for segmentation tasks on two public image collections from different medical modalities, namely ISIC ",vol3
Robust T-Loss for Medical Image Segmentation,3.1,Datasets,"The ISIC 2017 dataset  Shenzhen  Without a public benchmark with real noisy and clean segmentation masks, we artificially inject additional mask noise in these two datasets to test the model's robustness to low annotation quality. This simulates the real risk of errors due to factors like annotator fatigue and difficulty in annotating certain images. In particular, we follow ",vol3
Robust T-Loss for Medical Image Segmentation,3.2,Setup,"We train a nnU-Net  The model is trained using noisy masks. However, by using the ground truth for the corresponding noisy mask, we can evaluate the robustness of the model and measure noisy-label memorization. This is done by analyzing the dice score of the model's prediction compared to the actual ground truth. In addition to the T-Loss, we train several other losses for comparison. Our analysis includes some traditional robust losses, such as Mean Absolute Error (MAE), Reverse Cross Entropy (RCE), Normalized Cross Entropy (NCE), and Normalized Generalized Cross Entropy (NGCE), as well as more recent methods, such as Generalized Cross Entropy (GCE)  Finally, we complete our evaluation with statistical significance tests. We use the ANOVA test ",vol3
Robust T-Loss for Medical Image Segmentation,4.1,Results on the ISIC Dataset,We present experimental results for the skin lesion segmentation task on the ISIC dataset in Table ,vol3
Robust T-Loss for Medical Image Segmentation,4.2,Results on the Shenzhen Dataset,The results of lung segmentation for the Shenzhen test set are reported in Table ,vol3
Robust T-Loss for Medical Image Segmentation,4.3,Dynamic Tolerance to Noise,"The value of ν is crucial for the model's performance, as it controls the sensitivity to label noise. To shed light on this mechanism, we study the behavior of ν during training for different label noise levels and initializations on the ISIC dataset. As seen in Fig. ",vol3
Robust T-Loss for Medical Image Segmentation,5.0,Conclusions,"In this contribution, we introduced the T-Loss, a loss function based on the negative log-likelihood of the Student-t distribution. The T-Loss offers the great advantage of controlling sensitivity to outliers through a single parameter that is dynamically optimized. Our evaluation on public medical datasets for skin lesion and lung segmentation demonstrates that the T-Loss outperforms other robust losses by a statistically significant margin. While other robust losses are vulnerable to noise memorization for high noise levels, the T-Loss can reabsorb this form of overfitting into the tolerance level ν. Our loss function also features remarkable independence to different noise types and levels. It should be noted that other methods, such as  The T-Loss provides a robust solution for binary segmentation of medical images in the presence of high levels of annotation noise, as frequently met in practice e.g. due to annotator fatigue or inter-annotator disagreements. This may be a key feature in achieving good generalization in many medical image segmentation applications, such as clinical decision support systems. Our evaluations and analyses provide evidence that the T-Loss is a reliable and valuable tool in the field of medical image analysis, with the potential for broader application in other domains.",vol3
Robust T-Loss for Medical Image Segmentation,6.0,Data Use Declaration and Acknowledgment,We declare that we have used the ISIC dataset ,vol3
Robust T-Loss for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 68.,vol3
Multi-Head Multi-Loss Model Calibration,1.0,Introduction and Related Work,"When training supervised computer vision models, we typically focus on improving their predictive performance, yet equally important for safety-critical tasks is their ability to express meaningful uncertainties about their own predictions  Producing good uncertainty estimates can be useful, e.g. to identify test samples where the model predicts with little confidence and which should be reviewed  Training-Time Calibration. Popular training-time approaches consist of reducing the predictive entropy by means of regularization  Post-Hoc Calibration. Post-hoc calibration techniques like Temperature Scaling  Model Ensembling. A third approach to improve calibration is to aggregate the output of several models, which are trained beforehand so that they have some diversity in their predictions  In this work we achieve model calibration by means of multi-head models trained with diverse loss functions. In this sense, our approach is closest to some recent works on multi-output architectures like ",vol3
Multi-Head Multi-Loss Model Calibration,2.0,Calibrated Multi-Head Models,In this section we formally introduce multi-head models ,vol3
Multi-Head Multi-Loss Model Calibration,2.1,Multi-Head Ensemble Diversity,"Consider a K-class classification problem, and a neural network U θ taking an image x and mapping it onto a representation U θ (x) ∈ R N , which is linearly transformed by f into a logits vector z = f (U θ (x)) ∈ R K . This is then mapped into a vector of probabilities p ∈ [0, 1] K by a softmax operation p = σ(z), where p j = e zj / i e zi . If the label of x was y ∈ {1, ..., K}, we can measure the error associated to prediction p with the cross-entropy loss L CE (p, y) = -log(p y ). We now wish to implement a multi-head ensemble model like the one shown in Fig.  From Eq. (1) we see that the gradient in branch m will be scaled depending on how much probability mass p m y is placed by f m on the correct class relative to the total mass placed by all heads. In other words, if every head learned to produce a similar prediction (not necessarily correct) for a particular sample, then the optimization process of this network would result in the same updates for all of them. As a consequence, diversity in the predictions that make up the output p μ of the network would be damaged.",vol3
Multi-Head Multi-Loss Model Calibration,2.2,Multi-Head Multi Loss Models,"In view of the above, one way to obtain more diverse gradient updates in a multihead model during training could be to supervise each head with a different loss function. To this end, we will apply the weighted cross-entropy loss, given by L ω -CE (p, y) = -ω y log(p μ y ), where ω ∈ R K is a weight vector. In our case, we assign to each head a different weight vector ω m (as detailed below), in such a way that a different loss function L ω m -CE will supervise the intermediate output of each branch f m , similar to deep supervision strategies  where p = (p 1 , ..., p M ) is an array collecting all the predictions the network makes. Since L ω -CE results from just multiplying by a constant factor the conventional CE loss, we can readily calculate the gradient of L MH at each branch. Property 2: For the Multi-Loss Multi-Head classifier shown in Fig.  Note that having equal weight vectors in all branches fails to break the symmetry in the scenario of all heads making similar predictions. Indeed, if for any two given heads f mi , f mj we have ω mi = ω mj and p mi ≈ p mj , i.e. p m ≈ p μ ∀m, then the difference in norm of the gradients of two heads would be: It follows that we indeed require a different weight in each branch. In this work, we design a weighting scheme to enforce the specialization of each head into a particular subset of the categories {c 1 , ..., c K } in the training set. We first assume that the multi-head model has less branches than the number of classes in our problem, i.e. M ≤ K, as otherwise we would need to have different branches specializing in the same category. In order to construct the weight vector ω m , we associate to branch f m a subset of N/K categories, randomly selected, for specialization, and these are weighed with ω m j = K. Then, the remaining categories in ω m receive a weight of ω m j = 1/K. For example, in a problem with 4 categories and 2 branches, we could have If N is not divisible by K, the reminder categories are assigned for specialization to random branches.",vol3
Multi-Head Multi-Loss Model Calibration,2.3,Model Evaluation,"When measuring model calibration, the standard approach relies on observing the test set accuracy at different confidence bands B. For example, taking all test samples that are predicted with a confidence around c = 0.8, a well-calibrated classifier would show an accuracy of approximately 80% in this test subset. This can be quantified by the Expected Calibration Error (ECE), given by: where s B s form a uniform partition of the unit interval, and acc(B s ), conf(B s ) are accuracy and average confidence (maximum softmax value) for test samples predicted with confidence in B s . In practice, the ECE alone is not a good measure in terms of practical usability, as one can have a perfectly ECE-calibrated model with no predictive power ",vol3
Multi-Head Multi-Loss Model Calibration,3.0,Experimental Results,"We now describe the data we used for experimentation, carefully analyze performance for each dataset, and end up with a discussion of our findings.",vol3
Multi-Head Multi-Loss Model Calibration,3.1,Datasets and Architectures,"We conducted experiments on two datasets: 1) the Chaoyang dataset We implement the proposed approach by optimizing several popular neural network architectures, namely a common ResNet50 and two more recent models: a ConvNeXt ",vol3
Multi-Head Multi-Loss Model Calibration,3.2,Performance Analysis,"Notation: We train three different multi-head classifiers: 1) a 2-head model where each head optimizes for standard (unweighted) CE, referred to as 2HSL (2 Heads-Single Loss); 2) a 2-head model but with each head minimizing a differently weighed CE loss as described in Sect. 2.2. We call this model 2HML (2 Heads-Multi Loss)); 3) Finally, we increase the number of heads to four, and we refer to this model as 4HML. For comparison, we include a standard singleloss one-head classifier (SL1H), plus models trained with Label Smoothing (LS  What we expect to see: Multi-Head Multi-Loss models should achieve a better calibration (low ECE) than other learning-based methods, ideally approaching Deep Ensembles calibration. We also expect to achieve good calibration without sacrificing predictive performance (high accuracy). Both goals would be reflected jointly by a low NLL value, and by a better aggregated ranking. Finally we would ideally observe improved performance as we increase the diversity (comparing 2HSL to 2HML) and as we add heads (comparing 2HML to 4HML). Chaoyang: In Table ",vol3
Multi-Head Multi-Loss Model Calibration,4.0,Conclusion,"Multi-Head Multi-Loss networks are classifiers with enhanced calibration and no degradation of predictive performance when compared to their single-head counterparts. This is achieved by simultaneously optimizing several output branches, each one minimizing a differently weighted Cross-Entropy loss. Weights are complementary, ensuring that each branch is rewarded for becoming specialized in a subset of the original data categories. Comprehensive experiments on two challenging datasets with three different neural networks show that Multi-Head Multi-Loss models consistently outperform other learning-based calibration techniques, matching and sometimes surpassing the calibration of Deep Ensembles.",vol3
Multi-Head Multi-Loss Model Calibration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_11.,vol3
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,1.0,Introduction,"Federated learning (FL)  To this end, we propose to solve this challenging yet common scenario where each client holds different or even disjoint annotated label sets. We consider not only single-label classification but also multi-label classification where partial labels exist, making this problem setting more general and challenging. Specifically, each client has data of locally identified classes and locally unknown classes. Although locally identified classes differ among clients, the union of identified classes in all clients covers locally unknown classes in each client (Fig.  In this paper, we present FedLSM, a framework that aims to solve Label Set Mismatch and is designed for both single-label and multi-label classification tasks. FedLSM relies on pseudo labeling on unlabeled or partially labeled samples, but pseudo labeling methods could lead to incorrect pseudo labels and ignorance of samples with relatively lower confidence. To address these issues, we evaluate the uncertainty of each sample using entropy and conduct pseudo labeling only on data with relatively lower uncertainty. We also apply MixUp ",vol3
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,2.1,Problem Setting,"( Despite the fact that each client only identifies a subset of the class set C, the union of locally identified class sets equals C, which can be formulated as: ( The local dataset of client k is denoted as , where n k denotes the number of data, x i is the i -th input image, and For subsequent illustration, we denote the backbone model as , where f θ (•) refers to the feature extractor with parameters θ and f Ψ refers to the last classification layer with parameters Ψ = {ψ c } M c=1 . Adopting the terminology from previous studies ",vol3
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,2.2,Overview of FedLSM,Our proposed framework is presented in Fig. ,vol3
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,2.3,Local Model Training Uncertainty Estimation (UE).,"We use the global model to evaluate the uncertainty of each sample in the local dataset by calculating its entropy  where P (y = c|x i ) refers to the predicted probability for a given class c and input x i . The calculation of entropy in multi-label classification is similar to . We use the weakly-augmented version (i.e., slightly modified via rotations, shifts, or flips) of x i to generate pseudo labels on locally unknown classes by the teacher model. The loss L I k applied on locally identified class set I k is cross-entropy, and the loss applied on the locally unknown class set U k of k -th client in single-label classification can be formulated as: where  where τ p and τ n are the confidence threshold for positive and negative labels, N C is the number of data.",vol3
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,Uncertain Data Enhancing (UDE).,"The pseudo label filtering mechanism makes it difficult to acquire pseudo-labels for uncertain data, which results in their inability to contribute to the training process. To overcome this limitation, we propose to MixUp  , where x l ∈ D l k and x h ∈ D h k , and y l and y h are their corresponding labels or pseudo labels, respectively. We generate pseudo labels for uncertain data (x h , y h ) with a relatively smaller confidence threshold. The UDE loss function L UDE is cross-entropy loss.",vol3
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,Overall Loss Function. The complete loss function is defined as:,where λ is a hyperparameter to balance different objectives.,vol3
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,2.4,Server Model Aggregation,"After local training, the server will collect all the client models and aggregate them into a global model. In the r -th round, the aggregation of the feature extractors {θ k } K k=1 is given by: Adaptive Weighted Proxy Aggregation (AWPA). As analyzed in  The weighting number of each client is modulated in an adaptive way through the pseudo labeling process in each round.",vol3
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,3.1,Datasets,"We evaluated our method on two real-world medical image classification tasks, including the CXR diagnosis (multi-label) and skin lesion diagnosis (singlelabel). Task 1) NIH CXR Diagnosis. We conducted CXR diagnosis with NIH ChestX-ray14 dataset  Task 2) ISIC2018 Skin Lesion Diagnosis. We conducted skin lesion diagnosis with HAM10000  Training, validation and testing sets for both datasets were divided into 7:1:2.",vol3
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,3.2,Experiment Setup,"FL Setting. We randomly divided the training set into k client training sets and randomly select s classes as locally identified classes on each client. We set the number of clients k = 8, the number of classes s = 3 for Task 1 and k = 5, s = 3 for Task 2. Please find the detail of the datasets in the supplementary materials. Data Augmentation and Preprocessing. The images in Task 1 were resized to 320×320, while in Task 2, they were resized to 224×224. For all experiments, weak augmentation refered to horizontal flip, and strong augmentation included a combination of random flip, rotation, translation, scaling and one of the blur transformations in gaussian blur, gaussian noise and median blur. Evaluation Metrics. For Task 1, we adopted AUC to evaluate the performance of each disease. For task 2, we reported macro average of AUC, Accuracy, F1, Precision and Recall of each disease. All the results are averaged over 3 runs. Implementation Details. We used DenseNet121 ",vol3
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,3.3,Comparison with State-of-the-Arts and Ablation Study,"We compared our method with recent state-of-the-art (SOTA) non-IID FL methods, including FedProx  which introduced contrastive learning. We also compared with other SOTA non-IID FL methods that shared a similar setting with ours, including FedRS ",vol3
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,AUC Accuracy F1 Precision Recall,FedAvg with 100% labeled data 0.977 0.889 0.809 0.743 0.800 FedAvg ,vol3
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,4.0,Conclusion,"We present an effective framework FedLSM to tackle the issue of label set mismatch in FL. To alleviate the impact of missing labels, we leverage uncertainty estimation to partition the data into distinct uncertainty levels. Then, we apply pseudo labeling to confident data and uncertain data enhancing to uncertain data. In the server aggregation phase, we use adaptive weighted proxies averaging on the classification layer, where averaging weights are dynamically adjusted every round. Our FedLSM demonstrates notable effectiveness in both CXR diagnosis (multilabel classification) and ISIC2018 skin lesion diagnosis (single-label classification) tasks, holding promise in tackling the label set mismatch problem under federated learning.",vol3
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,1.0,Introduction,"Medical image segmentation is greatly helpful to diagnosis and auxiliary treatment of diseases. Recently, deep learning methods  To this end, many weakly-supervised segmentation (WSS) methods are developed to alleviate the annotation burden, including image level  A new annotation strategy has been proposed and investigated recently. It is typically referred as sparse annotation and it only requires a few slice of each volume to be labeled. With this annotation way, the exact boundaries of different classes are precisely kept. It shows great potential in reducing the amount of annotation. And its advantage over traditional weak annotations has been validated in previous work  Most existing methods solve the problem by generating pseudo label through registration.  Thus, we suggest to view this problem from the perspective of semisupervised segmentation (SSS). Traditional setting of 3D SSS is that there are several volumes with dense annotation and a large number of volumes without any annotation. And now there are voxels with annotation and voxels without annotation in every volume. This actually complies with the idea of SSS, as long as we view the labeled and unlabeled voxels as labeled and unlabeled samples, respectively. SSS methods  To sum up, our contributions are three folds: -A new perspective of solving sparse annotation problem, which is more versatile compared with recent methods using registration. -A novel cross-teaching paradigm which imposes consistency on the prediction of 3D and 2D networks. Our method enlarges the view difference of networks and boosts the performance. -A pseudo label selection strategy discriminating between reliable and unreliable predictions, which excludes error-prone voxels while keeping credible voxels though with low confidence. 2 Method",vol3
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,2.1,Cross Annotation,"Recent sparse annotation methods  The aim of the task is to train a segmentation model on dataset D consists of L volumes X 1 , X 2 , ..., X L with cross annotation Y 1 , Y 2 , ..., Y L .",vol3
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,2.2,3D-2D Cross Teaching,"Our framework consists of three networks, which are a 3D networks and two 2D networks. We leverage the unlabeled voxels through the cross teaching between 3D network and 2D networks. Specifically, the 3D network is trained on volumes and the 2D networks are trained with slices on transverse plane and coronal plane, respectively. The difference between 3D and 2D network is inherently in their network structure, and the difference between 2D networks comes from the different plane slices used to train the networks. For each sample, 3D network directly use it as input. Then it is cut into slices from two directions, resulting in transverse and coronal plane slices, which are used to train the 2D networks. And the prediction of each network, which is denoted as P , is used as pseudo label for the other network after selection. The selection strategy is detailedly introduced in the following part. To increase supervision signal for each training sample, we mix the selected pseudo label and ground truth sparse annotation together for supervision. And it is formulated as: where MIX(•, •) is a function that replaces the label in P with the label in Y for those voxels with ground truth annotation. Considering that the performance of 3D network is typically superior to 2D networks, we further introduce a label correction strategy. If the prediction of 3D network and the pseudo label from 2D networks differ, no loss on that particular voxel should be calculated as long as the confidence of 3D networks is higher than both 2D networks. We use M to indicate how much a voxel contribute to the loss calculation, and the value of position i is 0 if the loss of voxel i should not be calculated, otherwise 1 for ground truth annotation and w for pseudo label, where w is a value increasing from 0 to 0.1 according to ramp-up from  The total loss consists of cross-entropy loss and dice loss: and where p i , y i is the output and the label in Ŷ of voxel i, respectively. m i is the value of M at position i. H, W, D denote the height, width and depth of the input volume, respectively. And the total loss is denoted as:",vol3
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,2.3,Pseudo Label Selection,"Hard-Soft Confidence Threshold. Due to the limitation of supervision signal, the prediction of 3D model has lots of noisy label. If it is directly used as pseudo label for 2D networks, it will cause a performance degradation on 2D networks. So we set a confidence threshold to select voxels which are more likely to be correct. However, we find that this may also filter out correct prediction with lower confidence, which causes the waste of useful information. If we know the quality of the prediction, we can set a lower confidence threshold for the voxels in the prediction of high-quality in order to utilize more voxels. However, the real accuracy R acc of prediction is unknown, for the dense annotation is unavailable during training. What we can obtain is the pseudo accuracy P acc calculated with the prediction and the sparse annotation. And we find that R acc and P acc are completely related on the training samples. Thus, it is reasonable to estimate R acc using P acc : where I(•) is the indicator function and pi is the one-hot prediction of voxel i. Now we introduce our hard-soft confidence threshold strategy to select from 3D prediction. We divide all prediction into reliable prediction (i.e., with higher P acc ) and unreliable prediction (i.e., with lower P acc ) according to threshold t q . And we set different confidence thresholds for these two types of prediction, which are soft threshold t s with lower value and hard threshold t h with higher value. In reliable prediction, voxels with confidence higher than soft threshold can be selected as pseudo label. The soft threshold aims to keep the less confident voxels in reliable prediction and filter out those extremely uncertain voxels to reduce the influence of false supervision. And in unreliable prediction, only those voxels with confidence higher than hard threshold can be selected as pseudo label. The hard threshold is set to choose high-quality voxels from unreliable prediction. The hard-soft confidence threshold strategy achieves a balance between increasing supervision signals and reducing label noise. Consistent Prediction Fusion. Considering that 2D networks are not able to utilize inter-slice information, their performance is typically inferior to that of 3D network. Simply setting threshold or calculating uncertainty is either of limited use or involving large extra calculation cost. To this end, we provide a selection strategy which is useful and introduces no additional calculation. The 2D networks are trained on slices from different planes and they learn different  patterns to distinguish foreground from background. So they will produce predictions with large diversity for a same input sample and the consensus of the two networks are quite possible to be correct. Thus, we use the consistent part of prediction from the two networks as pseudo label for 3D network. 3 Experiments",vol3
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,3.1,Dataset and Implementation Details,MMWHS Dataset.  Implementation Details. We adopt Adam ,vol3
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,3.2,Comparison with SOTA Methods,As previous sparse annotation works  The quantitative results and qualitative results are shown in Table ,vol3
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,3.3,Ablation Study,"We also investigate how hyper-parameters t q , t h and t s affect the performance of the method. We conduct quantitative ablation study on the validation set. The results are shown in Table ",vol3
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,4.0,Conclusion,"In this paper, we extend sparse annotation to cross annotation to suit more general real clinical scenario. We label slices from two planes and it enlarges the diversity of annotation. To better leverage the cross annotation, we view the problem from the perspective of semi-supervised segmentation and we propose a novel cross-teaching paradigm which imposes consistency on the prediction of 3D and 2D networks. Furthermore, to achieve robust cross-supervision, we propose new strategies to select credible pseudo label, which are hard-soft threshold for 3D network and consistent prediction fusion for 2D networks. And the result on MMWHS dataset validates the effectiveness of our method.",vol3
Uncertainty and Shape-Aware Continual Test-Time Adaptation for Cross-Domain Segmentation of Medical Images,1.0,Introduction,"Deep neural networks (DNN) have demonstrated state-of-the-art performance in medical image segmentation in recent years  Many studies have attempted to address the domain shift. Earlier works adapt models to the target domain with access to the source domain  CTTA could be a useful technique to segment patient data acquired at different time points of longitudinal studies. However, we note that adaptation is possible only when the source model, typically trained with empirical risk minimization (ERM)  To address those issues, we propose a generalizable CTTA framework for the cross-domain segmentation task of medical images. We first incorporate shape-aware feature learning into existing models and train them on the source domain with DG techniques. This removes the need for carefully preprocessed target domain data and allows the source model to perform reasonably in most target domains regardless of the severity of the domain shift. Then, we use an uncertainty-weighted multi-task mean teacher network inspired by semisupervised literature to perform adaptation, producing results with improved accuracy and refined contours. In addition, a small portion of the model weight is stochastically reset to its initial, domain-generalized state at each adaptation step to prevent the model from overfitting to later test samples. We show the proposed framework works with ERM and DG-trained source models and (1) outperforms several state-of-the-art methods on three challenging cross-domain segmentation tasks and (2) is better suited for CTTA than its peers in various scenarios.",vol3
Uncertainty and Shape-Aware Continual Test-Time Adaptation for Cross-Domain Segmentation of Medical Images,2.0,Methodology,"Overview. The proposed framework is a synergy of three components (Fig.  Specifically, we train the modified model on the source domain (X S , Y S , Z S ) ∈ S, where X S denotes the input image, Y S the manual annotations, and Z S the ground-truth SDF calculated from Y S using  Here, seg and sdf represent loss functions used for optimizing the segmentation and SDF prediction tasks, respectively, N is the number of images in each batch, and ) indicate the predicted segmentation probability and SDF maps produced by the source model f from the augmented input g(x). We implement g with causality-inspired DG (CiDG)  Uncertainty and Shape-Aware Adaptation With Mean Teacher. The mean teacher network trains a student model and uses the exponential moving averages (EMA) of its weights to update an identical teacher model whose predictions further regularize the student model. Inspired by their rising popularity in semi-supervised studies  Specifically, both models are initialized with the weights of the source model. Then, at each time step t, the student model first predicts segmentation probability maps Ỹ T t and SDF predictions ZT t for the current test data x T t . Next, the teacher model performs K forward passes, producing K segmentation probability maps { Ŷ T tk } K k=1 and SDF predictions { ẐT tk } K k=1 from a set of noisy input images constructed by adding K random Gaussian noise vectors to x T t . The final segmentation map of the teacher model at time step t is obtained by aggregating all K segmentation probability maps through their uncertainties. The pixel-wise uncertainty of each of the K segmentation probability maps is measured as the entropy , where the log function has a base of C, the number of segmentation classes. Next, the confidence map of kth probability map is calculated as 1 -U tk , as higher values in U tk ∈ [0, 1] denote areas with higher uncertainties. Then, all confidence maps are stacked in the first dimension where we apply the softmax function, i.e., {W tk } K k=1 = softmax({1 -U tk } K k=1 ), to normalize the confidence value to [0, 1]. Lastly, the final segmentation probability map is constructed as a confidence-weighted combination of all K intermediate probability maps as The entropy of the final segmentation represents its uncertainty Entropy cannot be calculated on real-valued outputs such as SDF maps. As such, the final SDF prediction is obtained by averaging all K SDF maps, i.e., ẐT ẐT tk , and we follow  The student model is therefore guided by the teacher model by minimizing four loss terms: where sdf and seg are the MSE and the Dice loss  2 also penalize inconsistencies between the student and teacher models, but are weighted by the calculated uncertainty maps to encourage learning of confident predictions from the teacher model. The student model also performs self-regularization comprising two loss terms: where σ is the sigmoid function and κ is a multiplying factor approximating the inverse transformation from segmentation labels to SDF maps. The first loss term converts SDF maps into approximations of their corresponding segmentation labels and enforces a cross-task consistency  Let W t+1 denote the weights of a trainable conv layer after the gradient update at time step t. A small portion of W t+1 is reset to its initial weights as , where M ∼ Bernoulli(p) is a binary mask tensor, and W 0 denotes the initial domain-generalized weights of the conv layer.",vol3
Uncertainty and Shape-Aware Continual Test-Time Adaptation for Cross-Domain Segmentation of Medical Images,3.0,Experiments,"Setup. We implemented our method with PyTorch 1.10.0 and trained it on one Nvidia Tesla V100 GPU. We evaluated our method and other benchmarking methods on three cross-domain datasets with varying degrees of domain shifts: (1) cross-site binary prostate segmentation from T 2 -weighted MRI scans collected from six different sites (12-30 scans/site)  We treated each site as the source domain and adapted to all other sites in the first experiment. For other experiments, we first performed adaptation from modality A to B, then from B to A. All experiments were performed in an online manner: each test scan arrived randomly and was broken down into multiple batches if needed. The model adapted itself to each batch before making a prediction. U-Net with an EfficientNet-b2 backbone was used as the source model for all our experiments. The Adam optimizer  The proposed method substantially outperformed other methods on all three tasks and could consistently improve the CiDG-trained source model even in scenarios where other peer methods could not (Table  The proposed model also demonstrated an equal or higher final performance (in comparison to its running performance) in all experiments, whereas many of its peers demonstrated the opposite (Supplementary Table ",vol3
Uncertainty and Shape-Aware Continual Test-Time Adaptation for Cross-Domain Segmentation of Medical Images,4.0,Conclusion,"We proposed a generalizable framework for continual test-time adaption of medical images. Our approach first trains a model on the source domain with domain-invariant shape features before adapting it to the target domain with uncertainty-weighted pseudo-labels and SDF maps. Our method can work with ERM or DG-trained source models and outperformed its peers on three crosssite/cross-domain segmentation tasks without showing performance degradation as the adaptation progressed. Our framework can continuously adapt the source model to unknown test data online for the segmentation task, significantly reducing the cost and bias associated with manual labeling.",vol3
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,1.0,Introduction,Deep learning models have achieved remarkable success in segmenting anatomy and lesions from medical images but often rely on large-scale manually annotated datasets  1. We compare 5 existing guidance signals on the AutoPET ,vol3
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,,Related Work.,Previous work comparing guidance signals has mostly been limited to small ablation studies. Sofiiuk et al. ,vol3
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,2.1,Guidance Signals,"We define the five guidance signals over a set of N clicks C = {c 1 , c 2 , ..., c N } where c i = (x i , y i , z i ) is the i th click. As disks and heatmaps can be computed independently for each click, they are defined for a single click c i over 3D voxels v = (x, y, z) in the volume. The disk signal fills spheres with a radius σ centered around each click c i , which is represented by the equation in Eq.  The Gaussian heatmap applies Gaussian filters centered around each click to create softer edges with an exponential decrease away from the click (Eq. (  The Euclidean distance transform (EDT) is defined in Eq. (  The Geodesic distance transform (GDT) is defined in Eq. (4) as the shortest path distance between each voxel in the volume and the closest click in the set C  We also examine the exponentialized Geodesic distance (exp-GDT) proposed in MIDeepSeg  Note: We normalize signals to [0, 1] and invert intensity values for Euclidean and Geodesic distances d(x) by 1d(x) for better highlighting of small distances. We define our adaptive Gaussian heatmaps ad-heatmap(v, c i , σ i ) via: Here, N ci is the 9-neighborhood of c i , a = 13 limits the maximum radius to 13, and b = 0.15 is set empirically 1 (details in supplementary). The radius σ i is smaller for higher x, i.e., when the mean geodesic distance in the neighboring voxels is high, indicating large intensity changes such as edges. This leads to a more precise guidance with a smaller radius σ i near edges and a larger radius in homogeneous areas such as clicks in the center of the object of interest. An example of this process and each guidance signal can be seen in Fig. ",vol3
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,2.2,Model Backbone and Datasets,We use the DeepEdit  We trained and evaluated all of our models on the openly available AutoPET ,vol3
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,2.3,Hyperparameters: Experiments,"We keep these parameters constant for all models: learning rate = 10 -5 , #clicks N = 10, Dice Cross-Entropy Loss  We vary the following four hyperparameters (H1)-(H4): (H1) Sigma. We vary the radius σ of disks and heatmaps in Eq. (  (H2) Theta. We explore how truncating the values of distance-based signals in Eq. (  (H3) Input Adaptor. We test three methods for combining guidance signals with input volumes proposed by Sofiuuk et al. ",vol3
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,2.4,Additional Evaluation Metrics,We use 5 metrics (M1)-(M5) (Table ,vol3
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,3.1,Hyperparameters: Results,"We first train a DeepEdit  (H1) Sigma. Results in Fig.  (H2) Theta. We examine the impact of truncating large distance values for the EDT and GDT guidances from Eq. (  For our next experiments, we fix the optimal (σ, θ) pair for each of the five guidances (see Table  (H3) Input Adaptor. We look into different ways of combining guidance signals with input volumes using the input adaptors proposed by Sofiuuk et al.  (H4) Probability of Interaction. Figure ",vol3
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,3.2,Additional Evaluation Metrics: Results,"The comparison of the guidance signals using our five metrics (M1)-(M5) can be seen in Fig.  (M1) Initial and (M2) Final Dice. Overall, all guidance signals improve their initial-to-final Dice scores after N clicks, with AutoPET  (M3) Consistent Improvement. The consistent improvement is ≈ 65% for both datasets, but it is slightly worse for AutoPET  (M4) Overlap with Ground Truth. Heatmaps, disks, and EDT have a significantly higher overlap with the ground truth compared to geodesicbased signals, particularly on AutoPET  (M5) Efficiency. Efficiency is much higher on MSD Spleen [2] compared to AutoPET  Adaptive Heatmaps: Results. Varying (H1)-(H4) and examining (M1)-(M5), we find disks/heatmaps as the best signals, but with inflexibility near edges due to their fixed radius (Fig. ",vol3
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,4.0,Conclusion,"Our comparative experiments yield insights into tuning existing guiding signals and designing new ones. We find that smaller radiuses (σ ≤ 5), a small threshold (θ = 10%), more iterations with interactions (p ≥ 75%), and traditional concatenation should be used. Weaknesses in existing signals include overly large radiuses near edges and inconsistent improvement for geodesic-based signals that change with each click. This analysis inspires our adaptive heatmaps, which adapt the radiuses of the heatmaps according to the geodesic values around the clicks, mitigating the inflexibility and inconsistency of existing guidances. We emphasize the importance of guidance representation in clinical applications, where a consistent and robust model is critical. Our study provides an overview of potential pitfalls, important parameters to tune, and how to design future guidance signals, along with proposed metrics for systematic comparison.",vol3
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 61.,vol3
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,1.0,Introduction,The recent advancements in Transformer-based models have revolutionized the field of natural language processing and have also shown great promise in a wide range of computer vision tasks ,vol3
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,,New Attention Models:,"The redesign of the self-attention mechanism within pure Transformer models is another method aiming to augment feature repre-sentation to enhance the local feature representation ultimately. In this direction, Swin-Unet ",vol3
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,,Drawbacks of Transformers:,"Recent research has revealed that traditional self-attention mechanisms, while effective in addressing local feature discrepancies, have a tendency to overlook important high-frequency information such as texture and edge details  Our Contributions: ➊ We propose Laplacian-Former, a novel approach that includes new efficient attention (EF-ATT) consisting of two sub-attention mechanisms: efficient attention and frequency attention. The efficient attention mechanism reduces the complexity of self-attention to linear while producing the same output. The frequency attention mechanism is modeled using a Laplacian pyramid to emphasize each frequency information's contribution selectively. Then, a parametric frequency attention fusion strategy to balance the importance of shape and texture features by recalibrating the frequency features. These two attention mechanisms work in parallel. ➋ We also introduce a novel efficient enhancement multi-scale bridge that effectively transfers spatial information from the encoder to the decoder while preserving the fundamental features. ➌ Our method not only alleviates the problem of the traditional self-attention mechanism mentioned above, but also it surpasses all its counterparts in terms of different evaluation metrics for the tasks of medical image segmentation.",vol3
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,2.0,Methods,"In our proposed network, illustrated in Fig. ",vol3
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,2.1,Efficient Enhancement Transformer Block,"In medical imaging, it is important to distinguish different structures and tissues, especially when tissue boundaries are ill-defined. This is often the case for accurate segmentation of small abnormalities, where high-frequency information plays a critical role in defining boundaries by capturing both textures and edges. Inspired by this, we propose an Efficient Enhancement Transformer Block that incorporates an Efficient Frequency Attention (EF-ATT) mechanism to capture contextual information of an image while recalibrating the representation space within an attention mechanism and recovering high-frequency details. Our efficient enhancement Transformer block first takes a LayerNorm (LN) from the input x. Then it applies the EF-ATT mechanism to capture contextual information and selectively include various types of frequency information while using the Laplacian pyramid to balance the importance of shape and texture features. Next, x and diversity-enhanced shortcuts are added to the output of the attention mechanism to increase the diversity of features. It is proved in ",vol3
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,2.2,Efficient Frequency Attention (EF-ATT),"The traditional self-attention block computes the attention score S using query (Q) and key (K) values, normalizes the result using Softmax, and then multiplies the normalized attention map with value (V): where d k is the embedding dimension. One of the main limitations of the dotproduct mechanism is that it generates redundant information, resulting in unnecessary computational complexity. Shen et al.  Their approach involves applying the Softmax function (ρ) to the key and query vectors to obtain normalized scores and formulating the global context by multiplying the key and value matrix. They demonstrate that efficient attention E can provide an equivalent representation of self-attention while being computationally efficient. By adopting this approach, we can alleviate the issues of feature redundancy and computational complexity associated with self-attention. Wang et al.  where X refers to the input feature map, (i, j) corresponds to the spatial location within the encoded feature map, the variable σ l denotes the variance of the Gaussian function for the l-th scale, and the symbol * represents the convolution operator. The pyramid is then built by subtracting the l-th Gaussian function (G l ) output from the (l + 1)-th output (G l -G l+1 ) to encode frequency information at different scales. The Laplacian pyramid is composed of multiple levels, each level containing distinct types of information. To ensure a balanced distribution of low and high-frequency information in the model, it is necessary to efficiently aggregate the features from all levels of the frequency domain. Hence, we present frequency attention that involves multiplying the key and value of each level (X l ) to calculate the attention score and then fuses the resulting attention scores of all levels using a fusion module, which performs summation. The resulting attention score is multiplied by Query (Q) to obtain the final frequency attention result, which subsequently concatenates with the efficient attention result and applies the depth-wise convolution with the kernel size of 2×1×1 in order to aggregate both information and recalibrate the feature map, thus allowing for the retrieval of high-frequency information.",vol3
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,2.3,Efficient Enhancement Multi-scale Bridge,It is widely known that effectively integrating multi-scale information can lead to improved performance ,vol3
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,3.0,Results,"Our proposed technique was developed using the PyTorch library and executed on a single RTX 3090 GPU. A batch size of 24 and a stochastic gradient descent algorithm with a base learning rate of 0.05, a momentum of 0.9, and a weight decay of 0.0001 was utilized during the training process, which was carried out for 400 epochs. For the loss function, we used both cross-entropy and Dice losses (Loss = γ • L dice + (1γ) • L ce ), γ set to 0.6 empirically.",vol3
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,,Datasets:,"We tested our model using the Synapse dataset  Skin Lesion Segmentation: Table  Our method achieves superior performance by utilizing the frequency attention in a pyramid scale to model local textures. Specifically, our frequency attention emphasizes the fine details and texture characteristics that are indicative of skin lesion structures and amplifies regions with significant intensity variations, thus accentuating the texture patterns present in the image and resulting in better performance. In addition, we provided the spectral response of LaplacianFormer vs. Standard Transformer in identical layers in Table ",vol3
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,4.0,Conclusion,"In this paper, we introduce Laplacian-Former, a novel standalone transformerbased U-shaped architecture for medical image analysis. Specifically, we address the transformer's inability to capture local context as high-frequency details, e.g., edges and boundaries, by developing a new design within a scaled dot attention block. Our pipeline benefits the multi-resolution Laplacian module to compensate for the lack of frequency attention in transformers. Moreover, while our design takes advantage of the efficiency of transformer architectures, it keeps the parameter numbers low.",vol3
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,1.0,Introduction,"Despite raising concerns about the environmental impact of artificial intelligence  In this work, we investigate the case of subtle abnormality detection in medical images, in an unsupervised context usually referred to as Unsupervised Anomaly Detection (UAD). This formalism requires only the identification of normal data to construct a normative model. Anomalies are then detected as outliers, i.e. as samples deviating from this normative model. Artificial neural networks (ANN) have been extensively used for UAD  Our approach is illustrated with the MR imaging exploration of de novo (just diagnosed) Parkinson's Disease (PD) patients, where brain anomalies are subtle and hardly visible in standard T1-weighted or diffusion MR images. The anomalies detected by our method are consistent with the Hoehn and Yahr (HY) scale ",vol3
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,2.0,UAD with Mixture Models,"Recent studies have shown that, on subtle lesion detection tasks with limited data, alternative approaches to ANN, such as one class support vector machine or mixture models  Learning a Reference Model. We consider a set Y H of voxel-based features for a number of control (e.g. healthy) subjects, Y H = {y v , v ∈ V H } where V H represents the voxels of all control subjects and y v ∈ IR M is typically deduced from image modality maps at voxel v or from abstract representation features provided by some ANN performing a pre-text task  with π k ∈ [0, 1], k=1:KH π k = 1 and K H the number of components, each characterized by a distribution f (•; θ k ). The EM algorithm is usually used to estimate Θ H that best fits Y H while K H can be estimated using the slope heuristic  Designing a Proximity Measure. Given a reference model (1), a measure of proximity r(y v ; Θ H ) of voxel v (with value y v ) to f H needs to be chosen. To make use of the mixture structure, we propose to consider distances to the respective mixture components through some weights acting as inverse Mahalanobis distances. We specify below this new proximity measure for MST mixtures. MST distributions are generalizations of the multivariate t-distribution that extend its Gaussian scale mixture representation  where G(•, νm  2 ) denotes the gamma density with parameter ( νm 2 , νm 2 ) ∈ R 2 and N M the multivariate normal distribution with mean parameter μ ∈ R M and covariance matrix DΔ w AD T showing the scaling by the W m 's through a diagonal matrix Δ w = diag(w -1 1 , . . . , w -1 M ). The MST parametrization uses the spectral decomposition of the scaling matrix The scale variable W m for dimension m can be interpreted as accounting for the weight of this dimension and can be used to derive a measure of proximity. After fitting a mixture  The proximity r is typically larger when at least one dimension of y v is well explained by the model. A similar proximity measure can also be derived for Gaussian mixtures, see details in the Supplementary Material Sect. 1. Decision Rule. For an effective detection, a threshold τ α on proximity scores can be computed in a data-driven way by deciding on an acceptable false positive rate (FPR) α; τ α is the value such that P (r(Y; Θ H ) < τ α ) = α, when Y follows the f H reference distribution. All voxels v whose proximity r(y v ; Θ H ) is below τ α are then labeled as abnormal. In practice, while f H is known explicitly, the probability distribution of r(Y; Θ H ) is not. However, it is easy to simulate this distribution or to estimate τ α as an empirical α-quantile ",vol3
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,3.0,Online Mixture Learning for Large Data Volumes,"Online learning refers to procedures able to deal with data acquired sequentially. Online variants of EM, among others, are described in  Assume (Y i ) n i=1 is a sequence of n independent and identically distributed replicates of a random variable Y ∈ Y ⊂ IR M , observed one at a time. Extension to successive mini-batches of observations is straightforward  Using the sequence (Y i ) n i=1 , the method of  (A1) The complete-data likelihood for X is of the exponential family form:  Let (γ i ) n i=1 be a sequence of learning rates in (0, 1) and let θ (0) ∈ T be an initial estimate of θ 0 . For each i = 1 : n, the online EM of  and where s (0) = s(y 1 ; θ (0) ). It is shown in Thm. 1 of  In practice, the algorithm implementation requires two quantities, s in (4) and θ in  Online MST Mixture EM. As shown in  with s(y, w) = w 1 y, w 1 vec(yy ), w 1 , logw 1 , . . . , w M y, w M vec(yy ), w M , logw M , φ(μ, D, A, ν ) = [φ 1 , . . . , φ M ] T with φ m equal to: where d m denotes the m th column of D and vec(•) the vectorisation operator, which converts a matrix to a column vector. The exact form of h is not important for the algorithm. It follows that θ(s) is defined as the unique maximizer of function Q(s, θ) = s T φ(θ)ψ(θ) where s is a vector that matches the definition and dimension of φ(θ) and can be conveniently written as s = [s 11 , vec(S 21 ), s 31 , s 41 , . . . , s 1M , vec(S 2M ), s 3M , s 4M ] T , with for each m, s 1m is a M -dimensional vector, S 2m is a M ×M matrix, s 3m and s 4m are scalars. Solving for the roots of the Q gradients leads to θ(s) = (μ(s), Ā(s), D(s), ν(s)) whose expressions are detailed in Supplementary Material Sect.   , where . The update of s (i) in (  Online Gaussian Mixture EM. This case can be found in previous work e.g. ",vol3
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,4.0,Brain Abnormality Exploration in de novo PD Patients,Data Description and Preprocessing. The Parkinson's Progression Markers Initiative (PPMI) ,vol3
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,,Pipeline and Results,". We follow Sects. 2 and 3 using T1w, FA and MD volumes as features (M = 3) and a FPR α = 0.02. The pipeline is repeated 10 times for cross-validation. Each fold is composed of 64 randomly selected HC images for training (about 70M voxels), the remaining 44 HC and all the PD samples for testing. For the reference model, we test Gaussian and MST mixtures, with respectively K H = 14 and K H = 8, estimated with the slope heuristic. Abnormal voxels are then detected for all test subjects, on the basis of their proximity to the learned reference model, as detailed in Sect. 2. The PPMI does not provide ground truth information at the voxel level. This is a recurring issue in UAD, which limits validations to mainly qualitative ones. For a more quantitative evaluation, we propose to resort to an auxiliary task whose success is likely to be correlated with a good anomaly detection. We consider the classification of test subjects into healthy and Parkinsonian subjects based on their global (over all brain) percentages of abnormal voxels. We exploit the availability of HY values to divide the patients into two HY = 1 and HY = 2 groups, representing the two early stages of the disease's progression. Classification results yield a median g-mean, for stage 1 vs stage 2, respectively of 0.59 vs 0.63 for the Gaussian mixtures model and 0.63 vs 0.65 for the MST mixture. The ability of both mixtures to better differentiate stage 2 than stage 1 patients from HC is consistent with the progression of the disease. Note that the structural differences between these two PD stages remain subtle and difficult to detect, demonstrating the efficiency of the models. The MST mixture model appears better in identifying stage 2 PD patients based on their abnormal voxels. To gain further insights, we report, in Fig.  Regarding efficiency, energy consumption in kilojoules (kJ) is measured using the PowerAPI library ",vol3
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,5.0,Conclusion and Perspectives,"Despite a challenging medical problematic of PD progression at early stages, we have observed that energy and memory efficient methods could yield interesting and comparable results with other studies performed on the same database  We have investigated statistical mixture models for an UAD task and shown that their expressivity could account for multivariate reference models, and their much simpler structure made them more amenable to efficient learning than most ANN solutions. Although very preliminary, we hope this attempt will open the way to the development of more methods that can balance the environmental impact of growing energy cost with the obtained healthcare benefits.",vol3
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 40.,vol3
Understanding Silent Failures in Medical Image Classification,1.0,Introduction,"Although machine learning-based classification systems have achieved significant breakthroughs in various research and practical areas, their clinical application is still lacking. A primary reason is the lack of reliability, i.e. failure cases produced by the system, which predominantly occur when deployment data differs from the data it was trained on, a phenomenon known as distribution shifts. In medical applications, these shifts can be caused by image corruption (""corruption shift""), unseen variants of pathologies (""manifestation shift""), or deployment in new clinical sites with different scanners and protocols (""acquisition shift"")  We argue, that silent failures, which occur when test cases break both the classifier and the CSF, are a significant bottleneck in the clinical translation of ML systems and require further attention in the medical community. Note that the task of silent failure prevention is orthogonal to calibration, as, for example, a perfectly calibrated classifier can still yield substantial amounts of silent failures and vice versa  Bernhardt et al.  In this work, our contribution is twofold: 1) Building on the work of Jaeger et al. ",vol3
Understanding Silent Failures in Medical Image Classification,2.0,Methods,"Benchmark for Silent Failure Prevention under Distribution Shifts. We follow the spirit of recent robustness benchmarks, where existing datasets have been enhanced by various distribution shifts to evaluate methods under a wide range of failure sources and thus simulate real-world application  The datasets consist only of publicly available data, our benchmark provides scripts to automatically generate the combined datasets and distribution shifts. The SF-Visuals Tool: Visualizing Silent Failures. The proposed tool is based on three simple operations, that enable effective and intuitive analysis of silent failures in datasets across various CSFs: 1) Interactive Scatter Plots: See example in Fig.  Based on these visualizations, the functionality of SF-Visuals is three-fold: 1) Visual analysis of the dataset including distribution shifts. 2) Visual analysis of the general behavior of various CSFs on a given task 3) Visual analysis of individual silent failures in the dataset for various CSFs.",vol3
Understanding Silent Failures in Medical Image Classification,3.0,Experimental Setup,Evaluating Silent Failure Prevention: We follow Jaeger et al. ,vol3
Understanding Silent Failures in Medical Image Classification,4.1,Silent Failure Prevention Benchmark,Table ,vol3
Understanding Silent Failures in Medical Image Classification,,None of the Evaluated Methods from the Literature Beats the Maximum Softmax Response Baseline Across a Realistic Range of Failure,"Sources. This result is generally consistent with previous findings in Bernhard et al.  On the Chest X-ray dataset, MCD worsens the performance for darkening corruptions across all CSFs and intensity levels, whereas the opposite is observed for brightening corruptions. Further, on the lung nodule CT dataset, DG-MCD-RES performs best on bright/dark corruptions and the spiculation manifestation shift, but worst on noise corruption and falls behind on the texture manifestation shift. These observations indicate trade-offs, where, within one distribution shift, reliability against one domain might induce susceptibility to other domains. Current Systems are Not Generally Reliable Enough for Clinical Application. Although CSFs can mitigate the rate of silent failures (see Appendix Fig. ",vol3
Understanding Silent Failures in Medical Image Classification,4.2,Investigation of Silent Failure Sources,SF-Visuals Enables Comprehensive Analysis of Silent Failures. Figure ,vol3
Understanding Silent Failures in Medical Image Classification,5.0,Conclusion,"We see two major opportunities for this work to make an impact on the community. 1) We hope the revealed shortcomings of current systems on biomedical tasks in combination with the deeper understanding of CSF behaviors granted by SF-Visuals will catalyze research towards a new generation of more reliable CSFs. 2) This study shows that in order to progress towards reliable ML systems, a deeper understanding of the data itself is required. SF-Visuals can help to bridge this gap and equip researchers with a better intuition of when and how to employ ML systems for a particular task.",vol3
Understanding Silent Failures in Medical Image Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 39.,vol3
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,1.0,Introduction,"Detecting out-of-distribution (OOD) samples is crucial in real-world applications of machine learning, especially in medical imaging analysis where misdiagnosis can pose significant risks  Deep learning-based OOD detection methods with uncertainty estimation, such as Evidential Deep Learning (EDL)  To address this limitation, we propose an Evidence Reconciled Neural Network (ERNN), which aims to reliably detect those samples that are similar to the training data but still with different distributions (near OOD), while maintain accuracy for In-Distribution (ID) classification. Concretely, we introduce a module named Evidence Reconcile Block (ERB) based on evidence offset. This module cancels out the conflict evidences obtained from the evidential head, maximizes the uncertainty of derived opinions, thus minimizes the error of uncertainty calibration in OOD detection. With the proposed method, the decision boundary of the model is restricted, the capability of medical outlier detection is improved and the risk of misdiagnosis in medical images is mitigated. Extensive experiments on both ISIC2019 dataset and in-house pancreas tumor dataset demonstrate that the proposed ERNN significantly improves the reliability and accuracy of OOD detection for clinical applications. Code for ERNN can be found at https://github.com/KellaDoe/ERNN.",vol3
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,2.0,Method,"In this section, we introduce our proposed Evidence Reconciled Neural Network (ERNN) and analyze its theoretical effectiveness in near OOD detection. In our approach, the evidential head firstly generates the original evidence to support the classification of each sample into the corresponding class. And then, the proposed Evidence Reconcile Block (ERB) is introduced, which reforms the derived evidence representation to maximize the uncertainty in its relevant opinion and better restrict the model decision boundary. More details and theorical analysis of the model are described below.",vol3
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,2.1,Deep Evidence Generation,"Traditional classifiers typically employ a softmax layer on top of feature extractor to calculate a point estimation of the classification result. However, the point estimates of softmax only ensure the accuracy of the prediction but ignore the confidence of results. To address this problem, EDL utilizes the Dirichlet distribution as the conjugate prior of the categorical distribution and replaces the softmax layer with an evidential head which produces a non-negative output as evidence and formalizes an opinion based on evidence theory to explicitly express the uncertainty of generated evidence.   where Based on the fact that the parameters of the categorical distribution should obey Dirichlet distribution, the model prediction ŷ and the expected cross entropy loss L ece on Dirichlet distributioncan be inferred as: (2)",vol3
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,2.2,Evidence Reconcile Block,"In case of OOD detection, since the outliers are absent in the training set, the detection is a non-frequentist situation. Referring to the subjective logic  To tackle the problem mentioned above, we propose an Evidence Reconcile Block (ERB) that reformulates the representation of original evidence and minimizes the deviation of uncertainty in evidence generation. In the proposed ERB, different pieces of evidence that support different classes are canceled out by transforming them from subjective opinion to epistemic opinion and the theoretical maximum uncertainty mass is obtained. As shown in Fig.  Since a is a uniform distribution defined earlier, the transformed belief mass can be calculated as: b = bb min , where b min is the minimum value in the original belief mass b. Similarly, the evidence representation ë in our ERB, based on epistemic opinion ω, can be formulated as: ]a = e -min i e, f or i ∈ {1, . . . , K}. ( After the transformation by ERB, the parameters α = ë + 1 of Dirichlet distribution associate with the reconciled evidence can be determined, and the reconciled evidential cross entropy loss L rece can be inferred as  By reconciling the evidence through the transformation of epistemic opinion in subjective logic, this model can effectively reduce errors in evidence generation caused by statistical accumulation. As a result, it can mitigate the poor uncertainty calibration in EDL, leading to better error correction and lower empirical loss in near OOD detection, as analysized in Sect. 2.3. As shown in Fig. ",vol3
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,2.3,Theorical Analysis of ERB,"To further analyze the constraint of the proposed model in OOD detection, we theoretically analyze the difference between the loss functions before and after the evidence transformation, as well as why it can improve the ability of near OOD detection. Detailed provements of following propositons are provided in Supplements. Proposition 1. For a given sample in K-classification with the label c and The misclassified ID samples with p c = α c /S ≤ 1/K are often located at the decision boundary of the corresponding categories. Based on Proposition 1, the reconciled evidence can generate a larger loss, which helps the model focus more on the difficult-to-distinguish samples. Therefore, the module can help optimize the decision boundary of ID samples, and promote the ability to detect near OOD. Proposition 2. For a given sample in K-classification with the label c and K i=1 α i = S, for any α c > S K , L rece < L ece is satisfied. Due to the lower loss derived from the proposed method, we achieve better classification accuracy and reduce empirical loss, thus the decision boundary can be better represented for detecting outliers. Proposition 3. For a given sample in K-classification and Dirichlet distribution parameter α, when all values of α equal to const α, L rece ≥ L ece is satisfied. During the training process, if the prediction p of ID samples is identical to the ideal OOD outputs, the proposed method generates a greater loss to prevent such evidence from occurring. This increases the difference in predictions between ID and OOD samples, thereby enhancing the ability to detect OOD samples using prediction entropy. In summary, the proposed Evidence Reconciled Neural Network (ERNN) optimizes the decision boundary and enhances the ability to detect near OOD samples. Specifically, our method improves the error-correcting ability when the probability output of the true label is no more than 1/K, and reduces the empirical loss when the probability output of the true label is greater than 1/K. Furthermore, the proposed method prevents model from generating same evidence for each classes thus amplifying the difference between ID and OOD samples, resulting in a more effective near OOD detection.",vol3
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,3.1,Experimental Setup,"Datasets. We conduct experiments on ISIC 2019 dataset  Implementations and Evaluation Metrics. To ensure fairness, we used pretrained Resnet34 ",vol3
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,3.2,Comparison with the Methods,"In the experiment, we compare the OOD detection performance of ERNN to several uncertainty-based approaches: • Prototype Network described in  Inspired by  • Mixup: As described in  • MT-mixup: Mix up is only applied to mid-class and tail-class samples. • MTMX-Prototype: On the basis of MT mixup, prototype network is also applied to estimate uncertainty. The results on two datasets are shown in Table ",vol3
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,3.3,Ablation Study,"In this section, we conduct a detailed ablation study to clearly demonstrate the effectiveness of our major technical components, which consist of evaluation of evidential head, evaluation of the proposed Evidence Reconcile Block on both ISIC 2019 dataset and our in-house pancreas tumor dataset. Since the Evidence Reconcile Block is based on the evidential head, thus there are four combinations, but only three experimental results were obtained. As shown in Table ",vol3
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,4.0,Conclusion,"In this work, we propose a simple and effective network named Evidence Reconciled Nueral Network for medical OOD detection with uncertainty estimation, which can measure the confidence in model prediction. Our method addresses the failure in uncertainty calibration of existing methods due to the similarity of near OOD with ID samples. With the evidence reformation in the proposed Evidence Reconcile Block, the error brought by accumulative evidence generation can be mitigated. Compared to existing state-of-the-art methods, our method can achieve competitive performance in near OOD detection with less loss of accuracy in ID classification. Furthermore, the proposed plug-and-play method can be easily applied without any changes of network, resulting in less computation cost in identifying outliers. The experimental results validate the effectiveness and robustness of our method in the medical near OOD detection problem.",vol3
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_30.,vol3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,1.0,Introduction,"Lung cancer is the main cause of cancer death worldwide  benign or malignant tumors  Segmenting nodules is a tedious task that requires significant human labor. Computer aided diagnosis (CAD) systems can significantly reduce such heavy workloads. The accuracy of the existing nodule detection model reaches 96.1%  Several studies have proposed solutions to tackle the large scale span challenges at both the input and feature level. For instance, some approaches adopt multi-scale inputs  Recently, some click-based lesion segmentation methods  In this paper, we propose a scale-aware test-time click adaptation (SaTTCA) method, which simply utilizes easily obtainable lesion click (i.e., the center detected nodule) to adjust the parameters of the network normalization layers ",vol3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,2.1,Restatement of Image Segmentation Based on Click,"For pulmonary nodule and mass segmentation, existing methods mostly rely on regions of interest (ROI) obtained by lesion detection networks. A set of 3D ROI inputs I can be represented as I ∈ R D×H×W with size (D, H, W ), along with its corresponding segmentation ground truth of nodules and masses represented by S ∈ (0, 1) D×H×W . Typically, a neural network with weighted parameters θ is trained to predict the lesion area Ŝ = θ(I), with the goal of minimizing the loss function L (S, Ŝ). The stochastic gradient descent (SGD) and the automatic data acquisition module weight decay (AdamW) optimizers are usually used to optimize the weighted parameters. For each ROI input, the center point C of the lesion, which is represented as 2 ) in Cartesian coordinate system, can be used as a reference point to assist the network in improving segmentation performance. This can be achieved either through an artificial or automatic approach, for instance, by adding click channels directly to the input or by adding a prior encoder to the network as demonstrated by the methods ",vol3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,2.2,Network Architecture,"The network structure of the proposed method, as shown in Fig.  To achieve this, we employ a clipping strategy to adjust the proportion of foreground and background in the input image, producing a group of input images with dimensions of 64 × 96 × 96, 32 × 48 × 48, and 16 × 24 × 24. These images are then passed through three convolution paths. The feature maps are concatenated as they are down-sampled to the same scale. The subsequent modules can be based on either CNN or transformer structures. The multi-scale input encoder allows the network to capture more scale information of the nodules and masses, thus mitigating the problem of large lesion scale span.",vol3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,2.3,Scale-Aware Test-Time Click Adaptation,"In clinical scenarios, the neural network for assisted diagnosis is generally a pretraining model. Due to differences in the statistical distribution of pulmonary nodule scale in image data from different medical centers, the segmentation results of some images, especially for large nodules, are worse than expected. For such scenarios, we propose the Scale-aware Test-time Click Adaptation method, which can improve the performance of segmentation results for largescale nodules and masses by adjusting some of the network parameters during testing. The pipeline of the proposed method is shown in Fig.  where n is the number of samples in the test set. Then we make a projection on the main connected region of Ŝi along three coordinate axes to obtain the size of the bounding box B i = (d, w, h) of the pre-segmentation result, and generate an ellipsoid M i with three axes length proportional to the corresponding side length of the bounding box B i . More formally, the coordinates of any foreground voxel point V : (x, y, z) in M i meets the following requirement: where R represents the mapping function between the axis length of the ellipsoid and the side length of the bounding box B i . Taking the x-axis as an example, R(d) is given by: R (d) = min (0.02 To account for the introduction of error information at some voxels during adaptive click adjustment, we develop a mapping function to generate M i adaptively based on the size of nodules and masses. If the nodule's length and diameter are less than 7 mm, M i degenerates into a voxel. When the predicted nodule size ranges from 7 mm to 40 mm, the axial length of B i and the side length of the bounding box follow a quadratic nonlinear relationship. If the predicted nodule size is greater than 40 mm, the axial length of M i has a linear relationship with the side length. To determine the super parametric values for the mapping function R, we perform cross-validation on three datasets.",vol3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,2.4,Training Objective of SaTTCA,"We use the foreground range of adaptively adjusted ellipsoid M i to mask Ŝi to obtain a masked segmentation ŜM i . Then we use M i to adjust the normalization layer parameters in the network during testing  where σ and γ are hyper-parameters set to 0.5 and 1 in all experiments, respectively. The sum of the first two equations is referred to as click loss L Click .",vol3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,3.1,Datasets and Evaluation Protocols,"We experiment on two public datasets and one in-house dataset. All three datasets are divided into training, validation, and test sets using a 7:1:2 ratio. ",vol3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,LIDC [1]:,"The LIDC dataset is a publicly available lung CT image database containing 1018 scans, developed by the Lung Image Database Consortium (LIDC). All pulmonary nodules and masses in the dataset have been annotated by multiple raters. To generate the ground truth for each nodule and mass, we combined the segmentation annotations from different raters. Overall, we selected a total of 1625 nodules and masses that were annotated by more than three raters from the LIDC dataset for the experiment.",vol3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,LNDb [16]:,"The LNDb dataset published in 2019, comprises 294 CT scans collected between 2016 and 2018. Each CT scan in the dataset has been segmented by at least one radiologist. The nodules included in this dataset are larger than 3 mm. The mean scale of the lesion in LNDb dataset is the shortest among the three datasets. We adopt 1968 nodules and masses from the LNDb dataset.",vol3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,In-House Data (ours):,"The in-house data (ours) contains 4055 CT scans and 6864 nodules and masses. Every CT scans are annotated with voxel-level nodule masks by radiologists. We exclude nodules and masses with diameters larger than 64 mm or smaller than 2 mm, as the diameter of the largest mass in the public dataset is no more than 64 mm.",vol3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,Evaluation Metrics:,"The performance of the nodule segmentation is evaluated by three metrics: volume-based Dice Similarity Coefficient (DSC), surface-based Normalized Surface Dice (NSD) ",vol3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,3.2,Implementation Details,"The ROI of the lesion is a patch cropped around nodules or masses from the original CT scans with shape 64 × 96 × 96. During pre-processing, Hounsfield Units (HU) values in all patches are first clipped to the range of ",vol3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,3.3,Results,"We adopt nnUNet  We further analyze the performance of SaTTCA. Firstly, we present the quantitative comparison in Table ",vol3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,4.0,Conclusion,"This paper introduces a novel approach called the Scale-aware Test-time Click Adaptation for nodule and mass segmentation, which aims to address the issue of extremely imbalanced lesion scale and poor segmentation performance on largescale nodules and masses. The network parameters are adapted at the instance level according to the scale-aware click during testing without altering the model architecture. This allows the network to achieve high recall for large-scale lesions. Then, a multi-scale input encoder is also proposed to enhance the segmentation performance of multi-scale nodules and masses. Extensive experiments on two public datasets and one in-house dataset demonstrate that though SATTCA increases inference time for each sample by about one second, it outperforms the corresponding baseline and click-based methods with different backbones.",vol3
Chest X-ray Image Classification: A Causal Perspective,1.0,Introduction,"Chest X-ray (CXR) is a non-invasive diagnostic test frequently utilized by medical practitioners to identify thoracic diseases. In clinical practice, the interpretation of CXR results is typically performed by expert radiologists, which can be time-consuming and subject to individual medical abilities  Some examples are shown in Fig.  To effectively address the aforementioned challenges, we approach the task of CXR image classification from a causal perspective. Our approach involves elucidating the relationships among causal features, confounding features, and the classification outcomes. In essence, our fundamental idea revolves around the concept of ""borrowing from others."" To illustrate this concept, let us consider an example involving letters in an image. Suppose a portion of the image contains marked letters, which can impact the classification of the unmarked portion. We perceive these letters as confounders. By borrowing the mark from the marked portion and adding it to the unmarked part, we effectively eliminate the confounding effect: ""If everyone has it, it's as if no one has it."" The same principle applies to other confounding assumptions we have mentioned. Towards this end, we utilize causal inference to minimize the confounding effect and maximize the causal effect to achieve a stable and decent performance. Specifically, we utilize CNN-based modules to extract the feature from the input CXR images, and then apply Transformer based cross-attention mechanism  We evaluate our method on multiple datasets and the experimental results consistently demonstrate the superior performance of our approach. The contributions of our work can be summarized as follows: -We take a casual look at the chest X-ray multi-label classification problem and model the disordered or easily-confused part as the confounder. -We propose a framework based on the guideline of backdoor adjustment and presented a novel strategy for chest X-ray image classification. It allows our properly designed model to exploit real and stable causal features while removing the effects of filtrable confounding patterns. -Extensive experiments on two large-scale public datasets justify the effectiveness of our proposed method. More visualizations with detailed analysis demonstrate the interpretability and rationalization of our proposed method.",vol3
Chest X-ray Image Classification: A Causal Perspective,2.0,Methodology,"In this section, we first define the causal model, then identify the strategies to eliminate confounding effects.",vol3
Chest X-ray Image Classification: A Causal Perspective,2.1,A Causal View on CXR Images,"From the above discussion, we construct a Structural Causal Model (SCM)  • C ← D → X: X denotes the causal feature which really contributes to the diagnosis, C denotes the confounding feature which may mislead the diagnosis and is usually caused by data bias and other complex situations mentioned above. The arrows denote feature extraction process, C and X usually coexist in the medical data D, these causal effects are built naturally. Fig.  • C → Y ← X: We denote Y as the classification result which should have been caused only by X but inevitably disturbed by confounding features. The two arrows can be implemented by classifiers. The goal of the model should capture the true causality between X and Y , avoiding the influence of C. However, the conventional correlation P (Y |X) fails to achieve that because of the backdoor path  Therefore, we apply the causal intervention to cut off the backdoor path and use P (Y |do(X)) to replace P (Y |X), so the model is able to exploit causal features.",vol3
Chest X-ray Image Classification: A Causal Perspective,2.2,Causal Intervention via Backdoor Adjustment,"Here, we propose to use the backdoor adjustment  • P (c) = P b (c): the marginal probability is invariant under the intervention, because C will remain unchanged when cutting the link between D and X. Based on the conclusions, the backdoor adjustment for the SCM in Fig.  where C is the confounder set, P (c) is the prior probability of c. We approximate the formula by a random sample operation which will be detailed next.  ",vol3
Chest X-ray Image Classification: A Causal Perspective,2.3,Training Object,"Till now, we need to provide the implementations of Eq. (  Given x ∈ R H0×W0×3 as input, we extract its spatial feature F ∈ R H×W ×v using the backbone, where H 0 × W 0 , H × W represent the height and width of the CXR image and the feature map respectively, and v denotes the hidden dimension of the network. Then, we use zero-initialized Q 0 ∈ R k×v as the queries in the cross-attention module inside the transformer, where k is the number of categories, each decoder layer l updates the queries Q l-1 from its previous layer. Here, we denote Q as the causal features and Q as the confounding features: where the tilde means position encodings, the disentangled features yield two branches, which can be fed separately into a point-wise Multi-layer perceptron (MLP) network and get corresponding classification logits via a sigmoid function.",vol3
Chest X-ray Image Classification: A Causal Perspective,,Disentanglement.,"As shown in Fig.  where h ∈ R v×k , Φ(•) represents classifier, and z denotes logits. The causal part aims to estimate the really useful feature, so we apply the supervised classification loss in a cross-entropy format: where d is a sample and D is the training data, y is the corresponding label. The confounding part is undesirable for classification, so we follow the work in CAL  where KL is the KL-Divergence, and y unif orm denotes a predefined uniform distribution. Causal Intervention. The idea of the backdoor adjustment formula in Eq. (  where ẑc is the prediction from a classifier on the ""intervened graph"", ĥc is the stratification feature via Eq. (3), D is the estimated stratification set contains trivial features. The training objective of our framework can be defined as: where α 1 and α 2 are hyper-parameters, which decide how powerful disentanglement and backdoor adjustment are. It pushes the prediction stable because of the shared image features according to our detailed results in the next section.",vol3
Chest X-ray Image Classification: A Causal Perspective,3.1,Experimental Setup,"We evaluate the common thoracic diseases classification performance on the NIH ChestX-ray14  In our experiments, we adopt ResNet101 ",vol3
Chest X-ray Image Classification: A Causal Perspective,3.2,Results and Analysis,"Table  Ablation studies on the NIH data set are shown in Table . 2. Where ""+"" denotes utilizing the module whereas ""-"" denotes removing the module. We demonstrate the efficiency of our method from the ablation study, and we can find that our feature extraction and causal learning module play significant roles, respectively. Besides, during the training process, Fig.  The results on CheXpert also prove the superiority of our method, we achieve the mean AUC of 0.912 on the five challenging pathologies ",vol3
Chest X-ray Image Classification: A Causal Perspective,4.0,Conclusion,"In conclusion, we present a novel causal inference-based chest X-ray image multilabel classification framework from a causal perspective, which comprises a feature learning module and a backdoor adjustment-based causal inference module. We find that previous deep learning based strategies are prone to make the final prediction via some spurious correlation, which plays a confounder role then damages the performance of the model. We evaluate our proposed method on two public data sets, and experimental results indicate that our proposed framework and method are superior to previous state-of-the-art methods.",vol3
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,1.0,Introduction,"Colorectal Cancer (CRC) has become a major threat to health worldwide. Since most CRCs originate from colorectal polyps, early screening for polyps is necessary. Given its significance, automatic polyp segmentation models  All above models are fully supervised and require pixel-level annotations. However, pixel-by-pixel labeling is time-consuming and expensive, which hampers practical clinical usage. Besides, many polyps do not have welldefined boundaries. Pixel-level labeling inevitably introduces subjective noise. To address the above limitations, a generalized polyp segmentation model is urgently needed. In this paper, we achieve this goal by a weakly supervised polyp segmentation model (named WeakPolyp) that only uses coarse bounding box annotations. Figure  However, bounding box annotations are much coarser than pixel-level ones, which can not describe the shape of polyps. Simply adopting these box annotations as supervision introduces too much background noise, thereby leading to suboptimal models. As a solution, BoxPolyp  WeakPolyp is mainly enabled by two novel components: mask-to-box (M2B) transformation and scale consistency (SC) loss. In practice, M2B is applied to transform the predicted mask into a box-like mask by projection and backprojection. Then, this transformed mask is supervised by the bounding box annotation. This indirect supervision avoids the misleading of box-shape bias of annotations. However, many regions in the predicted mask are lost in the projection and therefore get no supervision. To fully explore these regions, we propose the SC loss to provide a pixel-level self-supervision while requiring no annotations at all. Specifically, the SC loss explicitly reduces the distance between predictions of the same image at different scales. By forcing feature alignment, it inhibits the excessive diversity of predictions, thus improving the model generalization. In summary, our contributions are three-fold: ",vol3
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,2.0,Method,"Model Components. Fig.  Considering the computational cost, only f 2 , f 3 and f 4 are utilized. To fuse them, we first apply a 1 × 1 convolutional layer to unify the channels of f 2 , f 3 , f 4 and then use the bilinear upsampling to unify their resolutions. After being transformed to the same size, f 2 , f 3 , f 4 are added together and fed into one 1 × 1 convolutional layer for final prediction. Instead of the segmentation phase, our contributions primarily lie in the supervision phase, including mask-to-box (M2B) transformation and scale consistency (SC) loss. Notably, both M2B and SC are independent of the specific model structure. Model Pipeline. For each input image I, we first resize it into two different scales: I 1 ∈ R s1×s1 and I 2 ∈ R s2×s2 . Then, I 1 and I 2 are sent to the segmentation model and get two predicted masks P 1 and P 2 , both of which have been resized to the same size. Next, an SC loss is proposed to reduce the distance between P 1 and P 2 , which helps suppress the variation of the prediction. Finally, to fit the bounding box annotations (B), P 1 and P 2 are sent to M2B and converted into box-like masks T 1 and T 2 . With T 1 /T 2 and B, we calculate the binary cross entropy (BCE) loss and Dice loss, without worrying about noise interference. Fig. ",vol3
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,2.1,Mask-to-Box (M2B) Transformation,"One naive method to achieve the weakly supervised polyp segmentation is to use the bounding box annotation B to supervise the predicted mask P 1 /P 2 . Unfortunately, models trained in this way show poor generalization. Because there is a strong box-shape bias in B. Training with this bias, the model is forced to predict the box-shape mask, unable to maintain the polyp's contours. To solve this, we innovatively use B to supervise the bounding box mask (i.e.,T 1 /T 2 ) of P 1 /P 2 , rather than P 1 /P 2 itself. This indirect supervision separates P 1 /P 2 from B so that P 1 /P 2 is not affected by the shape bias of B while obtaining the position and extent of polyps. But how to implement the transformation from P 1 /P 2 to T 1 /T 2 ? We design the M2B module, which consists of two steps: projection and back-projection, as shown in Fig.  Projection. As shown in Eq. 1, given a predicted mask P ∈ [0, 1] H×W , we project it horizontally and vertically into two vectors P w ∈ [0, 1] 1×W and P h ∈ [0, 1] H×1 . In this projection, instead of using mean pooling, we use max pooling to pick the maximum value for each row/column in P . Because max pooling can completely remove the shape information of the polyp. After projection, only the position and scope of the polyp are stored in P w and P h . Back-projection. Based on P w and P h , we construct the bounding box mask of the polyp by back-projection. As shown in Eq. 2, P w and P h are first repeated into P w and P h with the same size as P . Then, we element-wisely take the minimum of P w and P h to achieve the bounding box mask T . As shown in Fig.  Supervision. By M2B, P 1 and P 2 are transformed into T 1 and T 2 , respectively. Because both T 1 /T 2 and B are box-like masks, we directly calculate the supervision loss between them without worrying about the misguidance of box-shape bias. Specifically, we follow  Priority. By simple transformation, M2B turns the noisy supervision into a noise-free one, so that the predicted mask is able to preserve the contours of the polyp. Notably, M2B is differentiable, which can be easily implemented with PyTorch and plugged into the model to participate in gradient backpropagation.",vol3
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,2.2,Scale Consistency (SC) Loss,"In M2B, most pixels in P are ignored in the projection, thus only a few pixels with high response values are involved in the supervision loss. This sparse supervision may lead to non-unique predictions. As shown in Fig.  Method. As shown in Fig. ",vol3
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,2.3,Total Loss,"As shown in Eq. 5, combining L Sum and L SC together, we get WeakPolyp model. Note that WeakPolyp simply replaces the supervision loss without making any changes to the model structure. Therefore, it is general and can be ported to other models. Besides, L Sum and L SC are only used during training. In inference, they will be removed, thus having no effect on the speed of the model.",vol3
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,3.0,Experiments,"Datasets. Two large polyp datasets are adopted to evaluate the model performance, including SUN-SEG ",vol3
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,4.0,Conclusion,"Limited by expensive labeling cost, pixel-level annotations are not readily available, which hinders the development of the polyp segmentation field. In this paper, we propose the WeakPolyp model completely based on bounding box annotations. WeakPolyp requires no pixel-level annotations, thus avoiding the interference of subjective noise labels. More importantly, WeakPolyp even achieves a comparable performance to the fully supervised models, showing the great potential of weakly supervised learning in the polyp segmentation field. In future, we will introduce temporal information into weakly supervised polyp segmentation to further reduce the model's dependence on labeling.",vol3
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,1.0,Introduction,"Deep learning has achieved remarkable success in various computer vision tasks, such as image generation, translation, and semantic segmentation  Methods such as domain adaptation (DA) or domain generalization (DG) have been explored to address the aforementioned problems. These methods aim to leverage learning from domains where information such as annotations exists and apply it to domains where such information is absent. Unsupervised domain adaptation (UDA) aims to solve this problem by simultaneously utilizing learning from a source domain with annotations and a target domain without supervised knowledge. UDA methods are designed to mitigate the issue of domain shift between the source and target domains. Pixel-level approaches, as proposed in  In DG, unlike UDA, the model aims to directly generalize to a target domain without joint training or retraining. DG has been extensively studied recently, resulting in various proposed approaches to achieve generalization across domains. One common approach  DG in the medical field includes variations in imaging devices, protocols, clinical centers, and patient populations. Medical generalization is becoming increasingly important as the use of medical imaging data is growing rapidly. Compared to general fields, DG in medical fields is still in its early stages and faces many challenges. One major challenge is the limited amount of annotated data available. Additionally, medical imaging data vary significantly across domains, making it difficult to develop models that generalize well to unseen domains. Recently, researchers have made significant progress in developing domain generalization methods for medical image segmentation.  In recent years, Transformer has gained significant attention in computer vision. Unlike traditional convolutional neural networks (CNNs), which operate locally and hierarchically, transformers utilize a self-attention mechanism to weigh the importance of each element in the input sequence based on its relationship with other elements. Swin Transformer  In this paper, we present a novel approach for domain generalization in medical image segmentation that addresses the limitations of existing methods. To be specific, our method is based on the disentanglement training strategy to learn invariant features across different domains. We first propose a combination of recent vision transformer architectures and style-based generators. Our proposed method employs a hierarchical combination strategy to learn global and local information simultaneously. Furthermore, we introduce domain-invariant representations by swapping domain-specific features, facilitating the disentanglement of content (e.g., objects) and styles. By incorporating a patch-wise discriminator, our method effectively separates domain-related features from entangled ones, thereby improving the overall performance and interpretability of the model. Our model effectively disentangles both domain-invariant features and domainspecific features separately. Our proposed method is evaluated on a medical image segmentation task, namely retinal fundus image segmentation with four different clinical centers. It achieves superior performance compared to state-ofthe-art methods, demonstrating its effectiveness.",vol3
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,2.1,Framework,"Efficiently extracting information from input images is crucial for successful domain generalization, and the process of reconstructing images using this information is also important, as it allows meaningful information to be extracted and, in combination with the learning methods presented later, allows learning to discriminate between domain-relevant and domain-irrelevant information. To this end, we designed an encoder using a transformer structure, which is nowadays widely used in computer vision, and combined it with a StyleGAN-based image decoder. The overall framework of our approach comprises three primary architectures: an encoder denoted as E, a segmentor denoted as S, and an image generator denoted as G. Additionally, the framework includes two discriminators: D and P D, as shown in Fig.  The encoder E is constructed using hierarchical transformer architecture, which offers increased efficiency and flexibility through its consideration of multi-scale features, as documented in  where F in and F out represent the input and output features, respectively. This formulation enables the model to capture local continuity while preserving important information within the feature representation. To ensure the preservation of local continuity across overlapping feature patches, we employ the Patch Merging process. This process combines feature patches by considering patch size (K), stride (S), and padding size (P ). For instance, we design the parameters as K = 7, S = 4, P = 3, which govern the characteristics of the patch merging operation. E takes images I D from multiple domains D : {D 1 , D 2 , ...D N }, and outputs two separated features of F D C which is a domain-invariant feature and F D S , which are domain-related features as (F D C , F D S ) = E(I D ). By disentangling these two, we aim to effectively distinguish what to focus when conducting target task such as segmentation on an unseen domain. For an image generator G, we take the StyleGAN2-based decoder ",vol3
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,2.2,Loss Function,"The generator aims to synthesize realistic images such as Î(i,j) = G(F Di C , F Dj S ) that matches the distribution of the input style images, while maintaining the consistency of a content features. For this, reconstruction loss term is introduced first to maintain self-consistency: Also, adversarial loss used to train G to synthesize a realistic images. Finally, segmentor S tries to conduct a main task which should be work well on the unseen target domain. To enable this, segmentation decoder only focuses on the disentangled content feature rather than the style features, as in Fig.  To better separate domain-invariant contents from domain-specific features, a patch-wise adversarial discriminator P D is included in the training, in a similar manner as introduce in  L padv = -log(P D( Î(i,j) )) for i = j. ( Under an assumption that well-trained disentangled representation learning satisfies the identity on content features, we apply an identity loss on both contents and segmentation outputs for a translated images. In addition to a regularization effect, this leads to increased performance and a stability in the training. where ) = E( Î(i,j) ) , and ŷ * = S(F Di * C ). Therefore, overall loss function becomes as below. where λ 0 , λ 1 , λ 2 , λ 3 , and λ 4 are the weights of L seg , L rec , L identity , L adv , and λ padv , respectively.",vol3
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,3.0,Experiments and Results,"To evaluate the effectiveness of our proposed approach in addressing domain generalization for medical fields, we conduct experiments on a public dataset. The method is trained on three source domains and evaluates its performance on the remaining target domain. We compare our results to those of existing methods, including Fed-DG ",vol3
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,3.1,Setup,"Dataset. We evaluate our proposed method on a public dataset, called Fundus  Metrics. We adopt the Dice coefficient (Dice), a widely used metric, to assess the segmentation results. Dice is calculated for the entire object region. A higher Dice coefficient indicates better segmentation performance.",vol3
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,3.2,Implementation Details,"Our proposed network is implemented based on 2D. The encoder network is applied with four and three downsampling layers and with channel numbers of 32, 64, 160, and 256. Adam optimizer with a learning rate of 0.0002 and momentum of 0.9 and 0.99 is used. We set λ 0 , λ 1 , λ 2 , λ 3 , and λ 4 as 0.1, 1, 0.5, 0.5, and 0.1, respectively. We train the proposed method for 100 epochs on the Fundus dataset with a batch size of 6. Each batch consists of 2 slices from each of the three domains. We use data augmentation to expand the training samples, including random gamma correction, random rotation, and flip. The training is implemented on one NVIDIA RTX A6000 GPU. We evaluate the performance of our proposed method by comparing it to four existing methods, as mentioned earlier. Table ",vol3
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,3.3,Results,"To evaluate the effectiveness of our model in extracting variant and invariant features, we conducted t-SNE visualization on the style features of images synthesized using two different methods: the widely-used mixup method as in ",vol3
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,4.0,Conclusion,"Our work addresses the challenge of domain generalization in medical image segmentation by proposing a novel framework for retinal fundus image segmentation. The framework leverages disentanglement learning with adversarial and regularized training to extract invariant features, resulting in significant improvements over existing approaches. Our approach demonstrates the effectiveness of leveraging domain knowledge from multiple sources to enhance the generalization ability of deep neural networks, offering a promising direction for future research in this field.",vol3
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,2.2,Post-processing,The inspiratory CT scan was registered to the expiratory CT scan by applying 3-D deformable image registration using the image registration tool Elastix (version 5.0.1) ,vol3
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,2.3,DenseNet Architecture and Training,"A sketch of the DenseNet architecture is presented in Fig.  In the last layer, a 1 × 1 convolution and a softmax operation are performed to obtain the output probability map. The network was trained to minimize the Dice loss ",vol3
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,2.4,Model Evaluation,The DenseNet AT percentages used to compute the correlations presented in Table ,vol3
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,3.0,Results,"For each model of each parameter combination obtained from cross-validation, the DICE coefficient was computed over the patches from the test set. Resulting mean DICE coefficients and standard deviations over the 5-folds for the different parameter combinations are presented in Table  Here, the best model achieved a score of 0.82. It was trained on both, LD and ULD. The scores did not differ noticeably (third decimal) when training on LD only (mean DICE coefficient 0.806 ± 0.213) compared to including both scan protocols, LD and ULD (0.809 ± 0.216). Analyzing the correlations of the percentage of AT in the lungs detected by the best DenseNet model of the five folds between LD and ULD, strong correlations and small ULD-LD differences become apparent for all tested parameter Table ",vol3
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,4.0,Discussion,"In this study, we trained a densely connected convolutional neural network to segment AT using 2-D patches of the expiratory and corresponding registered inspiratory CT scan slices. We wanted to evaluate the best settings and the effect of a noticeable dose reduction on AT quantification. Using a smaller stride and respectively more patches only resulted in a slightly higher patch-based DICE coefficient evaluated on the test set (Table  It is important to note the limitations of this study. First of all, it should be mentioned that generating ground truth is a difficult task even for experienced radiologists, which is not always clearly solvable. In addition, the results presented are limited to the available number of patients. Children were scanned at inspiration and expiration, with two different scan protocols, without leaving the CT table. This results in four scans for each patient and explains the limited availability of patients to be included in the study. The particularity of the dataset clarifies why the model could not easily be tested on an independent test dataset since there is none available obtained in a comparable manner.",vol3
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,5.0,Conclusion,"We were able to show that similar QAT indices can be calculated on ULD CT images despite an 82% reduced dose. QAT values were comparable for ULD and LD across all parameter combinations. The relationship to the LCI was retained. AT is not only an early sign of incipient pulmonary dysfunction in patients with CF, but also in other diseases such as COPD or asthma. We want to investigate how our DenseNet performs on other data sets of patients with CF, COPD or asthma and, if necessary, expand the amount of training data.",vol3
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,1.0,Introduction,"The success of deep learning for medical data analysis has demonstrated its potential to become a core component of future diagnosis and treatment methodologies. However, in spite of the efforts devoted to improve data efficiency  To address this issue, we propose an approach, complementing k-same techniques, for generating an extended variant of a dataset by sampling a privacypreserving walk in the GAN latent space. Our method directly optimizes latent points, through the use of an auxiliary identity classifier, which informs on the similarity between training samples and synthetic images corresponding to candidate latent points. This optimized navigation meets three key properties of data synthesis for medical applications: 1) equidistance, encouraging the generation of diverse realistic samples suitable for model training; 2) privacy preservation, limiting the possibility of recovering original samples, and, 3) class-consistency, ensuring that synthesized samples contain meaningful clinical information. To demonstrate the generalization capabilities of our approach, we experimentally evaluate its performance on two medical image tasks, namely, tuberculosis classification using the Shenzhen Hospital X-ray dataseet  Contributions: 1) We present a latent space navigation approach that provides a large amount of diverse and meaningful images for model training; 2) We devise an optimization strategy of latent walks that enforces privacy; 3) We carry out several experiments on two medical tasks, demonstrating the effectiveness of our generative approach on model's training and its guarantees to privacy preservation.",vol3
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,2.0,Related Work,"Conventional methods to protect identity in private images have involved modifying pixels through techniques like masking, blurring, and pixelation  Recent approaches, based on the k-same framework  Our latent navigation strategy complements these approaches by synthesizing large and diverse samples, suitable for downstream tasks. In general, latent space navigation in GANs manipulates the latent vectors to create new images with specific characteristics. While many works have explored this concept to control semantic attributes of generated samples ",vol3
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,3.0,Method,"The proposed Privacy-preserving LAtent Navigation (PLAN) strategy envisages three separate stages: 1) GAN training using real samples; 2) latent privacy-preserving trajectory optimization in the GAN latent space; 3) privacypreserving dataset synthesis for downstream applications. Figure  Formally, given a GAN generator G : W → X , we aim to navigate its latent space W to generate samples in image space X in a privacy-preserving way, i.e., avoiding latent regions where real images might be embedded. The expected result is a synthetic dataset that is safe to share, while still including consistent clinical features to be used by downstream tasks (e.g., classification). Our objective is to find a set of latent points W ⊂ W from which it is safe to synthesize samples that are significantly different from training points: given the training set X ⊂ X and a metric d on X , we want to find W such that min x∈ X d (G ( w) , x) > δ, ∀ w ∈ W, for a sufficiently large δ. Manually searching for W, however, may be unfeasible: generating a large W is computationally expensive, as it requires at least | W| forward passes through G, and each synthesized image should be compared to all training images; moreover, randomly sampled latent points might not satisfy the above condition. To account for latent structure, one could explicitly sample away from latent vectors corresponding to real data. Let Ŵi ⊂ W be the set of latent vectors that produce near-duplicates of a training sample x i ∈ X , such that G( ŵi ) ≈ x i , ∀ ŵi ∈ Ŵi . We can thus define Ŵ = N i=1 Ŵi as the set of latent points corresponding to all N samples of the training set: knowledge of Ŵ can be used to move the above constraint from X to W, by finding W such that min ŵ∈ Ŵ d ( w, ŵ) > δ, ∀ w ∈ W. In practice, although Ŵi can be approximated through latent space projection  From these limitations, we pose the search of seeking privacy-preserving latent points as a trajectory optimization problem, constrained by a set of objectives that mitigate privacy risks and enforce sample variability and class consistency. Given two arbitrary latent points (e.g., provided by a k-same aggregation method), w a , w b ∈ W, we aim at finding a latent trajectory WT = [w a = w1 , w2 , . . . , wT -1 , w b = wT ] that traverses the latent space from w a to w b in T steps, such that none of its points can be mapped to any training sample. We design our navigation strategy to satisfy three requirements, which are then translated into optimization objectives: 1. Equidistance. The distance between consecutive points in the latent trajectory should be approximately constant, to ensure sample diversity and mitigate mode collapse. We define the equidistance loss, L dist , as follows: where • 2 is the L 2 norm. Note that without any additional constraint, L dist converges to the trivial solution of linear interpolation, which gives no guarantee that the path will not contain points belonging to Ŵ. 2. Privacy preservation. To navigate away from latent regions corresponding to real samples, we employ an auxiliary network φ id , trained on X to perform identity classification. We then set the privacy preservation constraint by imposing that a sampled trajectory must maximize the uncertainty of φ id , thus avoiding samples that could be recognizable from the training set. Assuming φ id to be a neural network with as many outputs as the number of identities in the original dataset, this constraint can be mapped to a privacypreserving loss, L id , defined as the Kullback-Leibler divergence between the softmax probabilities of φ id and the uniform distribution U: where n id is the number of identities. This loss converges towards points with enhanced privacy, on which a trained classifier is maximally uncertain. 3. Class consistency. The latent navigation strategy, besides being privacypreserving, needs to retain discriminative features to support training of downstream tasks on the synthetic dataset. In the case of a downstream classification task, given w a and w b belonging to the same class, all points along a trajectory between w a and w b should exhibit the visual features of that specific class. Moreover, optimizing the constraints in Eq. 1 and Eq. 2 does not guarantee good visual quality, leading to privacy-preserving but useless synthetic samples. Thus, we add a third objective that enforces class-consistency on trajectory points. We employ an additional auxiliary classification network φ class , trained to perform classification on the original dataset, to ensure that sampled latent points share the same visual properties (i.e., the same class) of w a and w b . The corresponding loss L class is as follows: where CE is the cross-entropy between the predicted label for each sample and the target class label y. Overall, the total loss for privacy-preserving latent navigation is obtained as: where λ 1 and λ 2 weigh the three contributions. In a practical application, we employ PLAN in conjunction with a privacypreserving method that produces synthetic samples (e.g., a k-same approach). We then navigate the latent space between random pairs of such samples, and increase the size of the dataset while retaining privacy preservation. The resulting extended set is then used to train a downstream classifier φ down on synthetic samples only. Overall, from an input set of N samples, we apply PLAN to N/2 random pairs, thus sampling T N/2 new points.",vol3
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,4.0,Experimental Results,"We demonstrate the effectiveness and privacy-preserving properties of our PLAN approach on two classification tasks, namely, tuberculosis classification and diabetic retinopathy (DR) classification.",vol3
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,4.1,Training and Evaluation Procedure,"Data Preparation. For tuberculosis classification, we employ the Shenzhen Hospital X-ray set Baseline Methods. We evaluate our approach from a privacy-preserving perspective and by its capability to support downstream classification tasks. For the former, given the lack of existing methods for privacy-preserving GAN latent navigation, we compare PLAN to standard linear interpolation. After assessing privacy-preserving performance, we measure the impact of our PLAN sampling strategy when combined to k-SALSA ",vol3
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,4.2,Results,"To measure the privacy-preserving properties of our approach, we employ the membership inference attack (MIA)  Results in Table  Figure ",vol3
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,5.0,Conclusion,"We presented PLAN, a latent space navigation strategy designed to reduce privacy risks when using GANs for training models on synthetic data. Experimental results, on two medical image analysis tasks, demonstrate how PLAN is robust to membership inference attacks while effectively supporting model training with performance comparable to training on real data. Furthermore, when PLAN is combined with state-of-the-art k-anonymity methods, we observe a mitigation of performance drop while maintaining privacy-preservation properties. Future research directions will address the scalability of the method to large datasets with a high number of identities, as well as learning latent trajectories with arbitrary length to maximize privacy-preserving and augmentation properties of the synthetic datasets.",vol3
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,1.0,Introduction,"Medical image segmentation is a critical task in computer-assisted diagnosis, treatment planning, and intervention. While large-scale transformers have demonstrated impressive performance in various computer vision tasks  Self-supervised learning, a technique for constructing feature embedding spaces by designing pretext tasks, has emerged as a promising solution for addressing the issue of label deficiency. One representative methodology for self-supervised learning is the masked autoencoder (MAE)  In this paper, we propose Masked Multi-view with Swin (SwinMM), the first comprehensive multi-view pipeline for self-supervised medical image segmentation. We draw inspiration from previous studies  -We present SwinMM, a unique and data-efficient pipeline for 3D medical image analysis, providing the first comprehensive multi-view, self-supervised approach in this field. -Our design includes a masked multi-view encoder and a novel mutual learningbased proxy task, facilitating effective self-supervised pretraining. -We incorporate a cross-view decoder for optimizing the utilization of multiview information via a cross-attention block. -SwinMM delivers superior performance with an average Dice score of 86.18% on the WORD dataset, outperforming other leading segmentation methods in both data efficiency and segmentation performance.",vol3
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,2.0,Method,Figure ,vol3
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,2.1,Pre-training,"Masked Multi-view Encoder. Following  As shown in Fig.  Pre-training Strategy. To incorporate multiple perspectives of a 3D volume, we generated views from different observation angles, including axial, coronal, and sagittal. Furthermore, we applied rotation operations aligned with each perspective, consisting of angles of 0 • , 90 • , 180 • , and 270 • along the corresponding direction. To facilitate self-supervised pre-training, we devised four proxy tasks. The reconstruction and rotation tasks measure the model's performance on each input individually, while the contrastive and mutual learning tasks enable the model to integrate information across multiple views. -The reconstruction task compares the difference between unmasked input X and the reconstructed image y rec . Following  -The contrastive learning task aims to assess the effectiveness of a model in representing input data by comparing high-level features of multiple views. Our working assumption is that although the representations of the same sample may vary at the local level when viewed from different perspectives, they should be consistent at the global level. To compute the contrastive loss, we use cosine similarity sim(•), where y con i and y con j represent the contrastive pair, t is a temperature constant, and 1 is the indicator function. -The mutual learning task assesses the consistency of reconstruction results from different views to enable the model to learn aligned information from multi-view inputs. Reconstruction results are transformed into a uniform perspective and used to compute a mutual loss L mul , which, like the reconstruction task, employs the MSE loss. Here, y rec i and y rec j represent the predicted reconstruction from views i and j, respectively. The total pre-training loss is as shown in Eq. 5. The weight coefficients α 1 , α 2 , α 3 and α 4 are set equal in our experiment (α 1 = α 2 = α 3 = α 4 = 1). (5)",vol3
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,2.2,Fine-Tuning,"Cross-View Decoder. The structure of the cross-view decoder, comprising Conv-Blocks for skip connection, Up-Blocks for up-sampling, and a Crossview Attention block for views interaction, is depicted in Fig. ",vol3
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,,Cross Attention(f,"Multi-view Consistency Loss. We assume consistent segmentation results should be achieved across different views of the same volume. To quantify the consistency of the multi-view results, we introduce a consistency loss L mc , calculated using KL divergence in the fine-tuning stage, as in previous work on mutual learning  We evaluate the effectiveness of different mutual loss functions in an ablation study (see supplementary). The KL divergence calculation is shown in Eq. 7: where V i (x m ) and V j (x m ) denote the different view prediction of m-th voxel. N represents the number of voxels of case x. V i (x) and V j (x) denote different view prediction of case x. We measure segmentation performance using L DiceCE , which combines Dice Loss and Cross Entropy Loss according to  where p m and y i respectively represent the predicted and ground truth labels for the m-th voxel, while N is the total number of voxels. We used L fin during the fine-tuning stage, as specified in Eq. 9, and added weight coefficients β DiceCE and β mc for different loss functions, both set to a default value of 1. ",vol3
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,2.3,Semi-supervised Learning with SwinMM,"As mentioned earlier, the multi-view nature of SwinMM can substantially enhance the reliability and accuracy of its final output while minimizing the need for large, high-quality labeled medical datasets, making it a promising candidate for semi-supervised learning. In this study, we propose a simple variant of SwinMM to handle semi-supervision. As depicted in Fig. ",vol3
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,3.0,Experiments,"Datasets and Evaluation. Our pre-training dataset includes 5833 volumes from 8 public datasets: AbdomenCT-1K  Implementation Details. Our SwinMM is trained on 8 A100 Nvidia GPUs with 80G gpu memory. In the pre-training process, we use a masking ratio of 50%, a batch size of 2 on each GPU, and an initial learning rate of 5e-4 and weight decay of 1e-1. In the finetuning process, we apply a learning rate of 3e-4 and a layer-wise learning rate decay of 0.75. We set 100K steps for pre-training and 2500 epochs for fine-tuning. We use the AdamW optimizer and the cosine learning rate scheduler in all experiments with a warm-up of 50 iterations to train our model. We follow the official data-splitting methods on both WORD and ACDC, and report the results on the test dataset. For inference on these datasets, we applied a double slicing window inference, where the window size is 64 × 64 × 64 and the overlapping between windows is 70%.",vol3
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,3.1,Results,"Comparing with SOTA Baselines. We compare the segmentation performance of SwinMM with several popular and prominent networks, comprising fully supervised networks, i.e., U-Net  Single View vs. Multiple Views. To evaluate the effectiveness of our proposed multi-view self-supervised pretraining pipeline, we compared it with the state-of-the-art self-supervised learning method SwinUNETR ",vol3
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,3.2,Ablation Study,"To fairly evaluate the benefits of our proposed multi-view design, we separately investigate its impact in the pre-training stage, the fine-tuning stage, as well as both stages. Additionally, we analyze the role of each pre-training loss functions. Pre-training Loss Functions. The multi-view pre-training is implemented by proxy tasks. The role of each task can be revealed by taking off other loss functions. For cheaper computations, we only pre-train our model on 2639 volumes from 5 datasets (AbdomenCT-1K, BTCV, MSD, TCIA-Covid19, and WORD) in these experiments, and we applied a 50% overlapping window ratio, during testing time. As shown in Table  Data Efficiency. The data efficiency is evaluated under various semi-supervised settings. Initially, a base model is trained from scratch with a proportion of supervised data from the WORD dataset for 100 epochs. Then, the base model finishes the remaining training procedure with unsupervised data. The proportion of supervised data (denoted as label ratio) varies from 10% to 100%. Table ",vol3
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,4.0,Conclusion,"This paper introduces SwinMM, a self-supervised multi-view pipeline for medical image analysis. SwinMM integrates a masked multi-view encoder in the pre-training phase and a cross-view decoder in the fine-tuning phase, enabling seamless integration of multi-view information, thus boosting model accuracy and data efficiency. Notably, it introduces a new proxy task employing a mutual learning paradigm, extracting hidden multi-view information from 3D medical data. The approach achieves competitive segmentation performance and higher data-efficiency than existing methods and underscores the potential and efficacy of multi-view learning within the domain of self-supervised learning.",vol3
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_47.,vol3
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,1.0,Introduction,"In recent years, machine learning-based medical diagnosis systems have been introduced by many institutions. Although these systems achieve high accuracy in predicting medical conditions, bias has been found in dermatological disease datasets as shown in  The discriminatory nature of these models can have a negative impact on society by limiting access to healthcare resources for different sensitive groups, such as those based on race or gender. Several methods are proposed to alleviate the bias in machine learning models, including pre-processing, in-processing, and post-processing strategies. Preprocessing strategies adjust training data before training  In this paper, we observe that although features from a deep layer of a neural network are discriminative for target groups (i.e., different dermatological diseases), they cause fairness conditions to deteriorate, and we will demonstrate this observation by analyzing the entanglement degree regarding sensitive information with the soft nearest neighbor loss  Through extensive experiments, we demonstrate that our proposed multi-exit convolutional neural network (ME-CNN) can achieve fairness without using sensitive attributes (unawareness) in the training process, which is suitable for dermatological disease diagnosis because the sensitive attributes information exists privacy issues and is not always available. We compare our approach to the current state-of-the-art method proposed in  The main contributions of the proposed method are as follows: -Our quantitative analysis shows that the features from a deep layer of a neural network are highly discriminative yet cause fairness to deteriorate. -We propose a fairness through unawareness framework and use multi-exit training to improve fairness in dermatological disease classification. -We demonstrate the extensibility of our framework, which can be applied to various state-of-the-art models and achieve further improvement. Through extensive experiments, we show that our approach can improve fairness while keeping competitive accuracy on both the dermatological disease dataset, ISIC 2019 ",vol3
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,2.0,Motivation,"In this section, we will discuss the motivation behind our work. The soft nearest neighbor loss (SNNL), as introduced in  To evaluate the SNNL, we analyzed the performance of ResNet18  A practical approach to avoiding using features that are distinguishable to sensitive attributes for prediction is to choose the result at a shallow layer for the final prediction. To the best of our knowledge, we are the first work that leverages the multi-exit network to improve fairness. In Sect. 5, we demonstrate that our framework can be applied to different network architectures and datasets.",vol3
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,3.1,Problem Formulation,"In the classification task, define input features x ∈ X = R d , target class, y ∈ Y = {1, 2, ..., N }, and sensitive attributes a ∈ A = {1, 2, ..., M }. In this paper, we focus on the sensitive attributes in binary case, that is a ∈ A = {0, 1}. The goal is to learn a classifier f : X → Y that predicts the target class y to achieve high accuracy while being unbiased to the sensitive attributes a.",vol3
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,3.2,Multi-Exit (ME) Training Framework,"Our approach is based on the observation that deep neural networks can exhibit bias against certain sensitive groups, despite achieving high accuracy in deeper layers. To address this issue, we propose a framework leveraging an early exit policy, which allows us to select a result at a shallower layer with high confidence while maintaining accuracy and mitigating fairness problems. We illustrate our multi-exit framework in Fig.  ), where α is determined by the depth of each CLF , similar to  Our framework can also be extended to other pruning-based fairness methods, such as FairPrune  During inference, we calculate the softmax score of each internal classifier's prediction, taking the maximum probability value as the confidence score. We use a confidence threshold θ to maintain fairness and accuracy. High-confidence instances exit early, and we select the earliest internal classifier with confidence above θ for an optimal prediction of accuracy and fairness. ",vol3
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,4.1,Dataset,"In this work, we evaluate our method on two dermatological disease datasets, including ISIC 2019 challenge ",vol3
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,4.2,Implementation Details and Evaluation Protocol,"We employ ResNet18 and VGG-11 as the backbone architectures for our models. The baseline CNN and the multi-exit models are trained for 200 epochs using an SGD optimizer with a batch size of 256 and a learning rate of 1e-2. Each backbone consisted of four internal classifiers (CLF s) and a final classifier (CLF f ). For ResNet18, the internal features are extracted from the end of each residual block, and for VGG-11 the features are extracted from the last four max pooling layers. The loss weight hyperparameter α is selected based on the approach of  To evaluate the fairness performance of our framework, we adopted the multiclass equalized opportunity (Eopp0 and Eopp1) and equalized odds (Eodd) metrics proposed in ",vol3
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,5.1,Comparison with State-of-the-Art,"In this section, we compare our framework with several baselines, including CNN (ResNet18 and VGG-11), AdvConf ",vol3
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,,ISIC 2019 Dataset.,In Table ,vol3
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,5.2,Multi-Exit Training on Different Method,"In this section, we evaluate the performance of our ME framework when applied to different methods. Table  We also applied our ME framework to MFD and HSIC, which initially exhibited better fairness performance than other baselines. With our framework, these models showed better fairness while maintaining similar levels of accuracy. These findings suggest that our ME framework can improve the fairness of existing models, making them more equitable without compromising accuracy.",vol3
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,5.3,Ablation Study,"Effect of Different Confidence Thresholds. To investigate the impact of varying confidence thresholds θ on accuracy and fairness, we apply the ME-FairPrune method to a pre-trained model from the ISIC 2019 dataset and test different thresholds. Our results, shown in Fig. ",vol3
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,6.0,Conclusion,We address the issue of deteriorating fairness in deeper layers of deep neural networks by proposing a multi-exit training framework. Our framework can be applied to various bias mitigation methods and uses a confidence-based exit strategy to simultaneously achieve high accuracy and fairness. Our results demonstrate that our framework achieves the best trade-off between accuracy and fairness compared to the state-of-the-art on two dermatological disease datasets.,vol3
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 10.,vol3
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,1.0,Introduction,"Magnetic Resonance Imaging (MRI) is a widely applied imaging technique for medical diagnosis since it is non-invasive and able to generate high-quality clinical images without exposing the subject to radiation. The imaging speed is of vital importance for MRI  However, measurements in MRI are often noisy due to the imaging hardware and thermal fluctuations of the subject  In this paper, we propose a framework with diffusion models for MRI reconstruction that is robust to measurement noise and distribution shifts. To achieve robustness, we perform test-time tuning when ground truth data is not available at test time, using Stein's Unbiased Risk Estimator (SURE)  We evaluate our framework on FastMRI  -We propose a test-time hyperparameter tuning algorithm that boosts the robustness of the pre-trained diffusion models against distribution shifts; -We propose to use SURE as a surrogate loss function for MSE and incorporate it into the sampling stage for test-time tuning and early stopping without access to ground truth data from the target distribution; -SMRD achieves state-of-the-art performance across different noise levels, acceleration rates, and anatomies.",vol3
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,2.0,Related Work,SURE for MRI Reconstruction. SURE has been used for tuning parameters of compressed sensing  Adaptation in MRI Reconstruction. Several proposals have been made for adaptation to a target distribution in MRI reconstruction using self-supervised losses for unrolled models ,vol3
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,3.1,Accelerated MRI Reconstruction Using Diffusion Models,"The sensing model for accelerated MRI can be expressed as where y is the measurements in the Fourier domain (k-space), x is the real image, S are coil sensitivity maps, F is the Fourier transform, Ω is the undersampling mask, ν is additive noise, and A = ΩF S denotes the forward model. Diffusion models are a recent class of generative models showing remarkable sample fidelity for computer vision tasks  break 10: return xt+1 from a noise distribution x 0 ∼ N (0, I), annealed Langevin Dynamics is run for T steps where η t is a sampling hyperparameter, and ζ t ∼ N (0, I). For MRI reconstruction, measurement consistency can be incorporated via sampling from the posterior distribution p(x t |y)  The form of ∇ xt log p(x t |y) depends on the specific inference algorithm ",vol3
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,3.2,Stein's Unbiased Risk Estimator (SURE),"SURE is a statistical technique which serves as a surrogate for the true mean squared error (MSE) when the ground truth is unknown. Given the ground truth image x, the zero-filled image can be formulated as x zf = x + z, where z is the noise due to undersampling. Then, SURE is an unbiased estimator of MSE = xx 2 2  where x zf is the input of a denoiser, x is the prediction of a denoiser, N is the dimensionality of x. In practical applications, the noise variance σ 2 is not known a priori. In such cases, it can be assumed that the reconstruction error is not large, and the sample variance between the zero-filled image and the reconstruction can be used to estimate the noise variance, where σ 2 ≈ xx zf 2 2 /N  A key assumption behind SURE is that the noise process that relates the zero-filled image to the ground truth is i.i.d. normal, namely z ∼ N(0, σ 2 I). However, this assumption does not always hold in the case of MRI reconstruction due to undersampling in k-space that leads to structured aliasing. In this case, density compensation can be applied to enforce zero-mean residuals and increase residual normality ",vol3
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,3.3,SURE-Based MRI Reconstruction with Diffusion Models,"Having access to SURE at test time enables us to monitor it as a proxy for the true MSE. Our goal is to optimize inference hyperparameters in order to minimize SURE. To do this, we first consider the following optimization problem at time-step t: where we introduce a time-dependent regularization parameter λ t . The problem in Eq. 6 can be solved using alternating minimization (which we call AM-Langevin): Equation 8 can be solved using the conjugate gradient (CG) algorithm, with the iterates where A H is the Hermitian transpose of A, x zf = A H y is the zero-filled image, and h denotes the full update including the Langevin Dynamics and CG. This allows us to explicitly control the balance between the prior through the score function and the regularization through λ t . Monte-Carlo SURE. Calculating SURE requires evaluating the trace of the Jacobian tr( ∂xt+1 ∂x zf ), which can be computationally intensive. Thus, we approximate this term using Monte-Carlo SURE  where h(x t , λ t ) is the prediction which depends on the input x zf , shown in Eq. 9. Tuning λ t . By allowing λ t to be a learnable, time-dependent variable, we can perform test-time tuning (TTT) for λ t by updating it in the direction that minimizes SURE. As both SURE(t) and λ t are time-dependent, the gradients can be calculated with backpropagation through time (BPTT). In SMRD, for the sake of computation, we apply truncated BPTT, and only consider gradients from the current time step t. Then, the λ t update rule is: where α is the learning rate for λ t . Early Stopping (ES). Under measurement noise, it is critical to prevent overfitting to the measurements. We employ early-stopping (ES) by monitoring the moving average of SURE loss with a window size w at test time. Intuitively, we perform early stopping when the SURE loss does not decrease over a certain window. We denote the early-stopping iteration as T ES . Our full method is shown in Algorithm 1.",vol3
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,4.0,Experiments,"Experiments were performed with PyTorch on a NVIDIA Tesla V100 GPU  For baselines and SMRD, we use the score function from  Zero-filled 24.5/0.63 24.5/0.61 the multi-coil fastMRI brain dataset  Noise Simulation. The noise source in MRI acquisition is modeled as additive complex-valued Gaussian noise added to each acquired k-space sample  To simulate measurement noise, we add masked complex-gaussian noise to the masked k-space ν ∼ N (0, σ) with standard deviation σ, similar to ",vol3
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,5.0,Results and Discussion,"We compare with three baselines: (1) Csgm-Langevin  (2) Csgm-Langevin with early stopping using SURE loss with window size w = 25; (3) AM-Langevin where λ t = λ 0 and fixed throughout the inference. For tuning AM-Langevin, we used a brain scan for validation at R = 4 in the noiseless case (σ = 0) similar to Csgm-Langevin, and the optimal value was λ 0 = 2. We evaluate the methods across different measurement noise levels, acceleration rates and anatomies using the same pretrained score function from ",vol3
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,6.0,Conclusion,"We presented SMRD, a SURE-based TTT method for diffusion models in MRI reconstruction. SMRD does not require ground truth data from the target distribution for tuning as it uses SURE for estimating the true MSE. SMRD surpassed baselines across different shifts including anatomy shift, measurement noise change, and acceleration rate change. SMRD could be helpful to improve the safety and robustness of diffusion models for MRI reconstruction used in clinical settings. While we applied SMRD to MRI reconstruction, future work could explore the application of SMRD to other inverse problems and diffusion sampling algorithms and can be used to tune their inference hyperparameters.",vol3
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_20.,vol3
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,1.0,Introduction,"Cone Beam Computed Tomography (CBCT) is widely used in dental clinics, as it provides volumetric views of tooth structures for diagnosis, treatment, and surgery. Despite the extensive research on teeth segmentation from CBCT images  In this paper, we propose an end-to-end framework for tooth segmentation including internal structures from CBCT which, to the best of our knowledge, is the first work to do so. As shown in Fig.  We take a hierarchical approach to tackle the challenges, and propose a 3stage process in order to accurately extract structures without compromising the original resolution of CBCT data. Each stage performs precise detection and segmentation for each level of hierarchy in the tooth structure. We propose a novel module called Dual-Hierarchy U-Net (DHU-Net) which is designed to extract and combine hierarchical features so as to effectively leverage hierarchy in the internal tooth structure. The segmentation performance of our model is evaluated for internal structures as well as the whole teeth. Experiments show that our method outperforms state-of-the-art (SOTA) baselines in both cases. Our contributions are summarized as follows: 1) a fully automated, end-toend model for internal tooth segmentation for the first time; 2) a novel 3-stage method with Dual-Hierarchy U-Net module leveraging the hierarchical structures of teeth; 3) the superiority of our model over SOTA baselines. Related Work. 3D tooth segmentation has been actively studied, including knowledge-based approaches, e.g., graph cut ",vol3
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,2.1,Three-Stage Segmentation Process,"We propose a 3-stage process for the internal tooth segmentation from CBCT images, as shown in Fig.  Each stage performs the task associated with each level of hierarchy. In Stage 1, a bounding box containing the set of teeth is extracted from CBCT. In Stage 2, 3D patches of individual tooth in 32 classes are extracted. In Stage 3, a tooth patch is segmented into enamel, dentin and pulp structures. We perform a binary segmentation of teeth (versus non-tooth) instead of a simple bounding box regression, considering the importance of extracting accurate bounding boxes. After segmentation, we find a tight bounding box around the teeth set which is then zero-padded for extra margins. We use 3D U-Net ",vol3
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,2.3,Hierarchical Feature Fusion (HFF) Module,"One of the properties that made U-Net successful is the combination of encoder and decoder features through skip connections. In the proposed Hierarchical Feature Fusion (HFF) module, the decoder layers at C-Net combines two encoder features from both hierarchies, i.e., P-Net and C-Net: see Fig.  As shown in Fig. ",vol3
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,2.4,Loss Function,"The loss function of DHU-Net is given by L P and L C are binary cross-entropy (CE) loss for P-Net and CE + DICE loss for C-Net, respectively. λ 1 and λ 2 are hyperparameters for balancing losses. The λ 1 and λ 2 are hyperparameters for balancing losses which are set to 2 and 5, respectively. L FTM is Focal Tree-Min Loss ",vol3
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,3.1,Dataset,"The dataset consisted of 70 anonymized cases of 3D dental CBCT images collected from the Korea University Anam Hospital. This study was approved by the Institutional Review Board of of the same hospital (IRB No. 2020AN0410). The dimension of CBCT images is 768 × 768 × 576 with the voxel size 0.3 × 0.3 × 0.3 mm 3 . We clipped the intensity values of CBCT images to [-1000, 2500] and applied intensity normalization. All the internal structures of teeth in CBCT images were individually labelled as enamel, dentin, and pulp. The labeling was performed by two experts and cross-checked, with a final inspection performed by a oral & maxillofacial surgeon. The dataset is split in 3:1:1 for train, validation and test with 5-fold nested cross-validation. We use the following metrics: Dice similarity coefficient (DSC), Jaccard index, and Hausdorff distance (HD95). We evaluate the accuracy of tooth identification (in 32 classes) during the patch extraction in Stage 2. We define the metric of detection precision (DP) as DP = |D ∩ G|/|D ∪ G|, where D represents the set of predicted tooth classes in Stage 2, and G represents the ground truth set. All the results are averaged over 10 repetitions of experiments.",vol3
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,3.2,Experimental Results,"We evaluated the performance of our model for internal tooth segmentation by comparing with two commonly used models in medical segmentation: U-Net  Table  We conducted a qualitative analysis of segmentation results as shown in Fig.  Next, we evaluate the segmentation performance of the whole tooth, which also is an important problem. Our model provides the prediction of the whole tooth, i.e., we can simply take a union of the predicted enamel, dentin and pulp. We selected state-of-the-art methods for tooth segmentation as baselines: C2FSeg  We observe that by comparing Table ",vol3
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,4.0,Conclusion,"In this work, we proposed a fully automated segmentation of internal tooth structures, which, to the best of our knowledge, is the first attempt. We proposed a 3-stage process to reduce detection error and overcome difficulties in segmentation and computational complexity. We introduced DHU-Net, a segmentation network capable of effectively learning hierarchical features of tooth structures, demonstrating improved segmentation performance for both the whole tooth and internal structures. Our future work include the segmentation of additional structures from CBCT, such as mandible or maxilla, simultaneously with teeth.",vol3
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_67.,vol3
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,1.0,Introduction,"Medical image segmentation is one of the most fundamental and challenging tasks in medical image analysis. It aims at classifying each pixel in the image into an anatomical category. With the success of deep neural networks (DNNs), medical image segmentation has achieved great progress in assisting radiologists in contributing to a better disease diagnosis. Until recently, the field of medical image segmentation has mainly been dominated by an encoder-decoder architecture, and the existing state-of-the-art (SOTA) medical segmentation models are roughly categorized into two groups: (1) convolutional neural networks (CNNs)  Orthogonally to the popular belief that the model architecture matters the most in medical segmentation (i.e., complex architectures generally perform better), this paper focuses on an under-explored and alternative direction: towards improving segmentation quality via rectifying uncertain coarse predictions. To this end, we propose a new INR-based framework, MORSE (iMplicit anatOmical Rendering with Stochastic Experts). The core of our approach is to formulate medical image segmentation as a rendering problem in an end-to-end manner. We think of building a generic implicit neural rendering framework to have finegrained control of segmentation quality, i.e., to adaptively compose coordinatewise point features and rectify uncertain anatomical regions. Specifically, we encode the sampled coordinate-wise point features into a continuous space, and then align position and features with respect to the continuous coordinate. We further hinge on the idea of mixture-of-experts (MoE) to improve segmentation quality. Considering our goal is to rectify uncertain coarse predictions, we regard multi-scale representations from the decoder as experts. During training, experts are randomly activated for features from multiple blocks of the decoder, and correspondingly the INRs of multi-scale representations are sepa- ",vol3
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,2.0,Method,"Let us assume a supervised medical segmentation dataset D = {(x, y)}, where each input x = x 1 , x 2 , ..., x T is a collection of T 2D/3D scans, and y refers to the ground-truth labels. Given an input scan x ∈ R H×W ×d , the goal of medical segmentation is to predict a segmentation map ŷ. Figure ",vol3
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,2.1,Stochastic Mixture-of-Experts (SMoE) Module,"Motivation. We want a module that encourages inter-and intra-associations across multi-block features. Intuitively, multi-block features should be specified by anatomical features across each block. We posit that due to the specializationfavored nature of MoE, the model will benefit from explicit use of its own anatomical features at each block by learning multi-scale anatomical contexts with adaptively selected experts. In implementation, our SMoE module follows an MoE design ",vol3
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,,Modulization.,"We first use multiple small MLPs with the same size to process different block features and then up-sample the features to the size of the input scans, i.e., H × W × d. With N as the total number of layers (experts) in the decoder, we treat these upsampled features [F 1 , F 2 , ..., F N ] as expert features. We then train a gating network G to re-weight the features from activated experts with the trainable weight matrices [W 1 , W 2 , ..., W N ], where W ∈ R H×W ×d . Specifically, the gating network or router G outputs these weight matrices satisfying i W i = 1 H×W ×d using a structure depicted as follows: The gating network first concatenates all the expert features along channels and uses several convolutional layers to get where C is the channel dimension. A softmax layer is applied over the last dimension (i.e., N -expert) to output the final weight maps. After that, we feed the resultant output x out to another MLP to fuse multi-block expert features. Finally, the resultant output x out (i.e. the coarse feature) is given as follows: where • denotes the pixel-wise multiplication, and Stochastic Routing. The prior MoE-based model  That is, a model needs to access all its parameters to process all inputs. One drawback of such design often comes at the prohibitive training cost. Moreover, the large model size suffers from the representation collapse issue ",vol3
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,2.2,Implicit Anatomical Rendering (IAR),"The existing methods generally assume that the semantically correlated information and fine anatomical details have been captured and can be used to obtain high-quality segmentation quality. However, CNNs inherently operate the discrete signals in a grid of pixels or voxels, which naturally blur the high-frequency anatomical regions, i.e., boundary regions. To address such issues, INRs in computer graphics are often used to replace standard discrete representations with continuous functions parameterized by MLPs  Point Selection. Given a coarse segmentation map, the rendering head aims at rectifying the uncertain boundary regions. A point selection mechanism is thus required to filter out those pixels where the rendering can achieve maximum segmentation quality improvement. Besides, point selection can significantly reduce computational cost compared to blindly rendering all boundary pixels. Therefore, our MORSE selects N p points for refinement given the coarse segmentation map using an uncertainty-based criterion. Specifically, MORSE first uniformly randomly samples k p N p candidates from all pixels where the hyper-parameter k p ≥ 1, following  Positional Encoding. It is well-known that neural networks can be cast as universal function approximators, but they are inferior to high-frequency signals due to their limited learning power  the positional encoding function is given as: where x = 2x/H -1 and ỹ = 2y/W -1 are the standardized coordinates with values in between [-1, 1]. The frequency {w i , v i } L i=1 are trainable parameters with Gaussian random initialization, where we set L = 128  Rendering Head. The fine-grained features are then fed to the rendering head whose goal is to rectify the uncertain predictions with respect to these selected points. Inspired by  Adaptive Weight Adjustment. Instead of directly leveraging pre-trained weights, it is more desirable to train the model from scratch in an end-to-end way. For instance, we empirically observe that directly using coarse masks by pretrained weights to modify unclear anatomical regions might lead to suboptimal results (See Sect. 3.1). Thus, we propose to modify the importance of L rend as: where t is the index of the iteration, T denotes the total number of iterations, and 1{•} denotes the indicator function. Training Objective. As such, the model is trained in an end-to-end manner using total loss L total = L sup + λ t × L rend .",vol3
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,3.0,Experiments,"Dataset. We evaluate the models on two important medical segmentation tasks. (2) Liver segmentation: Multi-phasic MRI (MP-MRI) dataset is an in-house dataset including 20 patients, each including T1 weighted DCE-MRI images at three-time phases (i.e., pre-contrast, arterial, and venous). Here, our evaluation is conducted via 5-fold cross-validation on the 60 scans. For each fold, the training and testing data includes 48 and 12 cases, respectively. Implementation Details. We use AdamW optimizer  λ rend are set to 0.1. We adopt four representative models, including UNet ",vol3
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,3.1,Comparison with State-of-the-Art Methods,"We adopt classical CNN-and transformer-based models, i.e., 2D-based {UNet  Main Results. The results for 2D synapse multi-organ segmentation and 3D liver segmentation are shown in Tables ",vol3
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,3.2,Ablation Study,"We first investigate our MORSE equipped with UNet by varying α (i.e., stochastic rate) and N (i.e., experts) on Synapse. The comparison results of α and N are reported in Table ",vol3
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,4.0,Conclusion,"In this paper, we proposed MORSE, a new implicit neural rendering framework that has fine-grained control of segmentation quality by adaptively composing coordinate-wise point features and rectifying uncertain anatomical regions. We also demonstrate the advantage of leveraging mixture-of-experts that enables the model with better specialization of features maps for improving the performance. Extensive empirical studies across various network backbones and datasets, consistently show the effectiveness of the proposed MORSE. Theoretical analysis further uncovers the expressiveness of our INR-based model.",vol3
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 54.,vol3
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,1.0,Introduction,"Axillary lymph node (ALN) metastasis is a severe complication of cancer that can have devastating consequences, including significant morbidity and mortality. Early detection and timely treatment are crucial for improving outcomes and reducing the risk of recurrence. In breast cancer diagnosis, accurately segmenting breast lesions in ultrasound (US) videos is an essential step for computer-aided diagnosis systems, as well as breast cancer diagnosis and treatment. However, this task is challenging due to several factors, including blurry lesion boundaries, inhomogeneous distributions, diverse motion patterns, and dynamic changes in lesion sizes over time  Although the existing benchmark method DPSTT ",vol3
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,2.0,Ultrasound Video Breast Lesion Segmentation Dataset,"To support advancements in breast lesion segmentation and ALN metastasis prediction, we collected a dataset containing 572 breast lesion ultrasound videos with 34,300 annotated frames. Table ",vol3
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,3.0,Proposed Method,Figure ,vol3
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,3.1,Frequency-Based Feature Aggregation (FFA) Module,"According to the spectral convolution theorem in Fourier theory, any modification made to a single value in the spectral domain has a global impact on all the original input features ",vol3
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,3.2,Two-Branch Decoder,"After obtaining the frequency features, we introduce a two-branch decoder consisting of a segmentation branch and a localization branch to incorporate temporal features from nearby frames into the current frame. Each branch is built based on the UNet decoder  Location Ground Truth. Instead of formulating it as a regression problem, we adopt a likelihood heatmap-based approach to encode the location of breast lesions, since it is more robust to occlusion and motion blur. To do so, we compute a bounding box of the annotated breast lesion segmentation result, and then take the center coordinates of the bounding box. After that, we apply a Gaussian kernel with a standard deviation of 5 on the center coordinates to generate a heatmap, which is taken as the ground truth of the breast lesion localization.",vol3
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,3.3,Location-Based Contrastive Loss,"Note that the breast lesion locations of neighboring ultrasound video frames are close, while the breast lesion location distance is large for different ultrasound videos, which are often obtained from different patients. Motivated by this, we further devise a location-based contrastive loss to make the breast lesion locations at the same video to be close, while pushing the lesion locations of frames from different videos away. By doing so, we can enhance the breast lesion location prediction in the localization branch. Hence, we devise a location-based contrastive loss based on a triplet loss  where α is a margin that is enforced between positive and negative pairs. H t and H t-1 are predicted heatmaps of neighboring frames from the same video. N t denotes the heatmap of the breast lesion from a frame from another ultrasound video. Hence, the total loss L total of our network is computed by: where G H t and G S t denote the ground truth of the breast lesion segmentation and the breast lesion localization. We empirically set weights",vol3
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,4.0,Experiments and Results,"Implementation Details. To initialize the backbone of our network, we pretrained Res2Net-50 ",vol3
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,4.1,Comparisons with State-of-the-Arts,"We conduct a comparative analysis between our network and nine state-of-theart methods, comprising four image-based methods and five video-based methods. Four image-based methods are UNet  Quantitative Comparisons. The quantitative results of our network and the nine compared breast lesion segmentation methods are summarized in Table ",vol3
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,4.2,Ablation Study,"To evaluate the effectiveness of the major components in our network, we constructed three baseline networks. The first one (denoted as ""Basic"") removed the localization encoder branch and replaced our FLA modules with a simple feature concatenation and a 1 × 1 convolutional layer. The second and third baseline networks (named ""Basic+FLA"" and ""Basic+LB"") incorporate the FLA module and the localization branch into the basic network, respectively. Table ",vol3
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,4.3,Generalizability of Our Network,"To further evaluate the effectiveness of our FLA-Net, we extend its application to the task of video polyp segmentation. Following the experimental protocol employed in a recent study on video polyp segmentation ",vol3
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,5.0,Conclusion,"In this study, we introduce a novel approach for segmenting breast lesions in ultrasound videos, leveraging a larger dataset consisting of 572 videos containing a total of 34,300 annotated frames. We introduce a frequency and location feature aggregation network that incorporates frequency-based temporal feature learning, an auxiliary prediction of breast lesion location, and a location-based contrastive loss. Our proposed method surpasses existing state-of-the-art techniques in terms of performance on our annotated dataset as well as two publicly available video polyp segmentation datasets. These outcomes serve as compelling evidence for the effectiveness of our approach in achieving accurate breast lesion segmentation in ultrasound videos.",vol3
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,1.0,Introduction,"Semantic segmentation, an essential component of computer-aided medical image analysis, identifies and highlights regions of interest in various diagnosis tasks. However, this often becomes complicated due to various factors involving image modality and acquisition along with pathological and biological variations  The original UNet model comprises a symmetric encoder-decoder architecture (Fig.  Till this point in the history of UNet, all the innovations were performed using CNNs. However, the decade of 2020 brought radical changes in the computer vision landscape. The long-standing dominance of CNNs in vision was disrupted by vision transformers  Very recently, studies have begun rediscovering the potential of CNNs in light of the advancements brought by transformers. The pioneering work in this regard is 'A ConvNet for the 20202020ss'  In this paper, we ask the same question but in the context of UNet models. We investigate if a UNet model solely based on convolution can compete with the transformer-based UNets. In doing so, we derive motivations from the transformer architecture and develop a purely convolutional UNet model. We propose a patch-based context aggregation contrary to window-based self-attention. In addition, we innovate the skip connections by fusing the feature maps from multiple levels of encoders. Extensive experiments on 5 benchmark datasets suggest that our proposed modifications have the potential to improve UNet models.",vol3
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,2.0,Method,"Firstly, we analyze the transformer-based UNet models from a high-level. Deriving motivation and insight from this, we design two convolutional blocks to simulate the operations performed in transformers. Finally, we integrate them in a vanilla UNet backbone and develop our proposed ACC-UNet architecture. Leveraging the Long-Range Dependency of Self-attention. Transformers can compute features from a much larger view of context through the use of (windowed) self-attention. In addition, they improve expressivity by adopting inverted bottlenecks, i.e., increasing the neurons in the MLP layer. Furthermore, they contain shortcut connections, which facilitate the learning  Adaptive Multi-level Feature Combination Through Channel Attention. Transformer-based UNets fuse the feature maps from multiple encoder levels adaptively using channel attention. This generates enriched features due to the combination of various regions of interest from different levels compared to simple skip-connection which is limited by the information at the current level  Based on these observations, we modify the convolutional blocks and skipconnections in a vanilla UNet model to induce the capabilities of long-range dependency and multi-level feature combinations.",vol3
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,2.2,Hierarchical Aggregation of Neighborhood Context (HANC),"We first explore the possibility of inducing long-range dependency along with improving expressivity in convolutional blocks. We only use pointwise and depthwise convolutions to reduce the computational complexity  In order to increase the expressive capability, we propose to include inverted bottlenecks in convolutional blocks  Next, we wish to emulate self-attention in our convolution block, which at its core is comparing a pixel with the other pixels in its neighborhood  patches. For k = 1, it would be the ordinary convolution operation, but as we increase the value of k, more contextual information will be provided, bypassing the need for larger convolutional kernels. Thus, our proposed hierarchical neighborhood context aggregation enriches feature map x 1 ∈ R cinv,n,m with contextual information as x 2 ∈ R cinv * (2k-1),n,m (Fig.  Next, similar to the transformer, we include a shortcut connection in the convolution block for better gradient propagation. Hence, we perform another pointwise convolution to reduce the number of channels to c in and add with the input feature map. Thus, x 2 ∈ R cinv * (2k-1),n,m becomes x 3 ∈ R cin,n,m (Fig.  Finally, we change the number of filters to c out , as the output, using pointwise convolution (Fig.  Thus, we propose a novel Hierarchical Aggregation of Neighborhood Context (HANC) block using convolution but bringing the benefits of transformers. The operation of this block is illustrated in Fig. ",vol3
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,2.3,Multi Level Feature Compilation (MLFC),"Next, we investigate the feasibility of multi-level feature combination, which is the other advantage of using transformer-based UNets. Transformer-based skip connections have demonstrated effective feature fusion of all the encoder levels and appropriate filtering from the compiled feature maps by the individual decoders  For the features, x 1 , x 2 , x 3 , x 4 from 4 different levels, the feature maps can be enriched with multilevel information as (Fig.  Here, resize i (x j ) is an operation that resizes x j to the size of x i and c tot = c 1 + c 2 + c 3 + c 4 . This operation is done individually for all the different levels. We thus propose another novel block named Multi Level Feature Compilation (MLFC), which aggregates information from multiple encoder levels and enriches the individual encoder feature maps. This block is illustrated in Fig. ",vol3
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,2.4,ACC-UNet,"Therefore, we propose fully convolutional ACC-UNet (Fig.  To summarize, in a UNet model, we replaced the classical convolutional blocks with our proposed HANC blocks that perform an approximate version of self-attention and modified the skip connection with MLFC blocks which consider the feature maps from different encoder levels. The proposed model has 16.77 M parameters, roughly a 2M increase than the vanilla UNet model.",vol3
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,3.1,Datasets,"In order to evaluate ACC-UNet, we conducted experiments on 5 public datasets across different tasks and modalities. We used ISIC-2018 ",vol3
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,3.2,Implementation Details,We implemented ACC-UNet model in PyTorch and used a workstation equipped with AMD EPYC 7443P 24-Core CPU and NVIDIA RTX A6000 (48G) GPU for our experiments. We designed our training protocol identical to previous works ,vol3
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,3.3,Comparisons with State-of-the-Art Methods,"We evaluated ACC-UNet against UNet, MultiResUNet, Swin-Unet, UCTransnet, SMESwin-Unet, i.e., one representative model from the 5 classes of UNet, respectively (Fig.  However, our model combining the design principles of transformers with the inductive bias of CNNs seemed to perform best in all the different categories with much lower parameters. Compared to much larger state-of-the-art models, for the 5 datasets, we achieved 0.13%, 0.10%, 0.63%, 0.90%, 0.27% improvements in dice score, respectively. Thus, our model is not only accurate, but it is also efficient in using the moderately small parameters it possesses. In terms of FLOPs, our model is comparable with convolutional UNets, the transformer-based UNets have smaller FLOPs due to the massive downsampling at patch partitioning.",vol3
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,3.4,Comparative Qualitative Results on the Five Datasets,"In addition to, achieving higher dice scores, apparently, ACC-UNet generated better qualitative results. Figure ",vol3
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,3.5,Ablation Study,We performed an ablation study on the CVC-ClinicDB dataset to analyze the contributions of the different design choices in our roadmap (Fig. ,vol3
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,4.0,Conclusions,"Acknowledging the benefits of various design paradigms in transformers, we investigate the suitability of similar ideas in convolutional UNets. The resultant ACC-UNet possesses the inductive bias of CNNs infused with long-range and multi-level feature accumulation of transformers. Our experiments reveals this amalgamation indeed has the potential to improve UNet models. One limitation of our model is the slowdown from concat operations (please see supplementary materials), which can be solved by replacing them. In addition, there are more innovations brought by transformers ",vol3
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_66.,vol3
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,1.0,Introduction,"Application of DNNs to critical applications like medical imaging requires that a model is not only accurate, but also well calibrated. Practitioners can trust Fig.  deployed models if they are certain that the model will give a highly confident answer when it is correct, and uncertain samples will be labeled as such. In case of uncertainty, the doctors can be asked for a second opinion instead of an automated system giving highly confident but incorrect and potentially disastrous predictions  Our key observation is that current metrics for calibration are highly unreliable for small datasets. For example, given a particular data distribution, if one measures calibration on various sample sets drawn from the distribution, an ideal metric should give an estimate with low bias and variance. We show that this does not hold true for popular metrics like ECE (c.f. Fig. ",vol3
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,Contributions: (1),"We demonstrate the ineffectiveness of common calibration metrics on medical image datasets with limited number of samples. (2) We propose a novel and robust metric to estimate calibration accurately on small datasets. The metric regularizes the probability of predicting a particular confidence value by estimating a parametric density model for each sample. The calibration estimates using the regularized probability estimates have significantly lower bias, and variance. (3) Finally, we also propose a train-time auxiliary loss for calibrating models trained on small datasets. We validate the proposed loss on several public medical datasets and achieve SOTA calibration results.",vol3
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,2.0,Related Work,Metrics for Estimating Calibration: Expected Calibration Error (ECE)  Calibrating DNNs: Calibration techniques typically reshape the output confidence vector so as to minimize a calibration loss. The techniques can be broadly categorized into post-hoc and train-time techniques. Whereas post-hoc techniques  Calibration in Medical Imaging: Liang et al. ,vol3
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,3.0,Proposed Methodology,"Model Calibration: Let D be a dataset with N samples: Hence, for a sample i, the model outputs a confidence vector, C i ∈ [0, 1] K , a probability vector denoting the confidence for each class. A prediction y i is made by selecting the class with maximum confidence in C. A model is said to be calibrated if: Expected Calibration Error (ECE): is computed by bin-wise addition of difference between the average accuracy A i and average confidence C i : Here, C j denotes the confidence vector, and y j predicted label of a sample j. The confidences, C[ y], of all the samples being evaluated are split into M equal sized bins with the i th bin having B i number of samples. C i represents the average confidence of samples in the i th bin. The basic idea is to compute the probability of outputting a particular confidence and the associated accuracy, and the expression merely substitutes sample mean in place of true probabilities.",vol3
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,Static Calibration Error (SCE):,"extends ECE to a multi-class setting as follows: Here B i,k denotes the number of samples of class k in the i th bin.",vol3
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,3.1,Proposed Metric: Robust Expected Calibration Error (RECE),"We propose a novel metric which gives an estimate of true ECE with low bias, and variance, even when the sample set is small. RECE incorporates the inherent uncertainty in the prediction of a confidence value, by considering the observed value as a sample from a latent distribution. This not only helps avoid overfitting on outliers, but also regularizes the confidence probability estimate corresponding to each confidence bin. We consider two versions of RECE based on the parameterization of the latent distribution.",vol3
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,RECE-G:,"Here, we assume a Gaussian distribution of fixed variance (σ) as the latent distribution for each confidence sample. We estimate the mean of the latent distribution as the observed sample itself. Formally: and To prevent notation clutter, we use denotes the probability of the interval [a, b] for a Gaussian distribution with mean μ, and variance σ. In the above expression, the range [ i-1 M , i M ] corresponds to the range of confidence values corresponding to i th bin. We also normalize the weight values over the set of bins. The value of standard deviation σ is taken as a fixed hyper-parameter. Note that the expression is equivalent to sampling infinitely many confidence values from the distribution N (•; c j , σ) for each sample j, and then computing the ECE value from thus computed large sampled dataset.",vol3
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,RECE-M:,"Note that RECE-G assumes fixed uncertainty in confidence prediction for all the samples as indicated by the choice of single σ for all the samples. To incorporate sample specific confidence uncertainty, we propose RECE-M in which we generate multiple confidence observations for a sample using test time augmentation. In our implementation, we generate 10 observations using random horizontal flip and rotation. We use the 10 observed values to estimate a Gaussian Mixture Model (denoted as G) with 3 components. We use θ j to denote the estimated parameters of mixture model for sample j. Note that, unlike RECE-G, computation of this metric requires additional inference passes through the model. Hence, the computation is more costly, but may lead to more reliable calibration estimates. Formally, RECE-M is computed as: and",vol3
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,3.2,Proposed Robust Calibration Regularization (RCR) Loss,"Most train time auxiliary loss functions minimize ECE over a mini-batch. When the mini-batches are smaller, the problem of unreliable probability estimation affects those losses as well. Armed with insights from the proposed RECE metric, we apply similar improvements in state of the art MDCA loss  The RCR loss can be used as a regularization term along side any application specific loss function as follows: Here, β is a hyper-parameter for the relative weightage of the calibration. As suggested in the MDCA, we also use focal loss ",vol3
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,4.0,Experiments and Results,"Datasets: We use following publicly available datasets for our experiments to demonstrate variety of input modalities, and disease focus. GBCU dataset  Experimental Setup: We use a ResNet-50 ",vol3
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,Evaluation of Calibration Methods:,We compare our RCR loss with other SOTA calibration techniques in Table ,vol3
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,5.0,Conclusion,We demonstrated the ineffectiveness of existing calibration metrics for medical datasets with limited samples and propose a robust calibration metric to accurately estimates calibration independent of dataset size. We also proposed a novel loss to calibrate models using proposed calibration metric.,vol3
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 15.,vol3
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,1.0,Introduction,"The human brain is a complex inter-wired system that emerges spontaneous functional fluctuations  A growing body of research studies the statistical association between SC and FC from the perspectives of connectivity  However, current state-of-the-art SC-FC coupling methods lack integrated neuroscience insight at a system level. Specifically, many SC-FC coupling methods are mainly designed to find a statistical association between SC and FC topology patterns, lacking a principled system-level integration to characterize the coupling mechanism of how neural population communicates and emerges remarkable brain functions on top of the structural connectomes. To address this limitation, we present a new approach to elucidate the complex SC-FC relationship by characterizing the dynamical behaviors underlying a dissected mechanism. As shown in Fig.  In this regard, we conceptualize the human brain as a complex system. With that being said, spontaneous functional fluctuation is not random. Instead, there is a coherent system-level mechanism that supports oscillatory neural activities throughout the brain anatomy. Therefore, we assume that each brain region is associated with a neural population, which manifests frequency-specific spontaneous neural oscillations. Inspired by the success of Kuramoto model  Fig.  In the neuroscience field, tremendous efforts have been made to elucidate the biological mechanism underlying the spatiotemporally organized low-frequency fluctuations in BOLD data during the resting state. In spite of the insightful mathematical formulation and physics principles, the tuning of model parameters heavily relies on neuroscience prior knowledge and thus affects the model replicability. On the flip side, machine learning is good at data fitting in a data-hungry manner, albeit through a ""black-box"" learning mechanism. Taking together, we have laid the foundation of our proposed deep model on the principle of the Kuramoto model, which allows us to characterize the SC-FC relationships with mathematical guarantees. Specifically, we first translate the Kuramoto model into a GNN architecture, where we jointly learn the neural oscillation process at each node (brain region) and allow the oscillation state (aka. graph embedding vector) to diffuse throughout the SC-constrained topological pathways. The driving force of our deep model is to dissect the non-linear mechanism of coupled synchronization which can replicate the self-organized patterns of slow functional fluctuations manifested in BOLD signals. Following the notion of complex system theory, we further propose to yield new SC-FC coupling biomarkers based on learned system dynamics. We have evaluated the statistical power and clinical value of our new biomarkers in recognizing the early sign of neurodegeneration using the ADNI dataset, where the promising result indicates great potential in other network neuroscience studies.",vol3
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,2.0,Method,"Suppose the brain network of SC G = (Ξ, W ) consists of N brain regions Ξ = {ξ i |i = 1, ..., N } and the region-to-region structural connectivities W = [w ij ] ∈ R N ×N measured from diffusion-weighted images. On the other hand, the mean time course of BOLD signal x i (t)(i = 1, ..., N, t = 1, ..., T ) at each brain region forms a data matrix X(t) = [x i (t)] T t=1 ∈ R N ×T , which characterizes whole-brain functional fluctuations. In our work, we conceptualize that the human brain is a complex system where distinct brain regions are physically wired (coupled) via neuronal fibers. On top of this, the status of neural oscillation at each brain region is determined by an intrinsic state variable of brain rhythm v i (t). Multiple oscillators in the brain, each with their own frequency and phase, align their oscillations over time, which gives rise to the ubiquitous self-organized patterns of spontaneous functional fluctuations. To test this hypothesis, we present a deep model to reproduce the topology of traditional FC matrix measured by Pearson's correlation ",vol3
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,2.1,Generalized Kuramoto Model for Coupled Neural Oscillations,"The Kuramoto family of coupled oscillators is a fundamental example of a nonlinear oscillator that exhibits various qualitative behaviors observed in physical systems. Each individual oscillator in the Kuramoto family has an inherent natural frequency denoted as ω and is subject to global coupling mediated by a sinusoidal interaction function. The dynamics of each oscillator are governed by the following partial differential equation (PDE), as described in  where θ i denotes the phase of the oscillator i for the Kuramoto model, K ij denotes the relative coupling strength from node i to node j, ω i is the natural frequency associated with node i. The Kuramoto model is a well-established tool for studying complex systems, with its primary application being the analysis of coupled oscillators through pairwise phase interaction. The model enables each oscillator to adjust its phase velocity based on inputs from other oscillators via a pairwise phase interaction function denoted as K ij . The Kuramoto model's versatility is due to its ability to generate interpretable models for various complex behaviors by modifying the network topology and coupling strength. However, capturing higher-order dynamics is challenging with the classic Kuramoto model due to its pre-defined dynamics (K ij sin(θ i, θ j ) in Eq. 1). To address this limitation, we propose a more general formulation to model a nonlinear dynamical system as: where the system dynamics is determined by the state variable of brain rhythm v i on each node. Compared to Eq. 1, we estimate the natural frequency ω i through a non-linear function f (•), which depends on the current state variable v i and the neural activity proxy x i . Since the Hilbert transform (H(•)) has been widely used in functional neuroimaging research to extract the phase and amplitude information from BOLD signals ",vol3
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,2.2,Deep Kuramoto Model for SC-FC Coupling Mechanism,"The overview of our deep Kuramoto model is shown in Fig.  The backbone of our deep Kuramoto model is a neuronal oscillation component where the evolution of state v(t) is governed by the PDE in Eq. 2. Under the hood, we discretize the continuous process by recursively applying the following operations at the current time point T : (  The driving force of our deep Kuramoto model to minimize the discrepancy between the observed BOLD signal x t and the reconstructed counterpart xt = f -1 (v t )) which is supposed to emerge from the intrinsic neural oscillation process. In training our deep Kuramoto model, we use a variant of stochastic gradient descent (SGD), Adma, with a learning rate of 0.001, to optimize the network parameters. As well the detailed parameter setting is as follows: epoch = 500, dropout = 0.5, hidden dimension = 64.",vol3
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,2.3,Novel SC-FC Coupling Biomarkers,"The valuable bi-product of our deep Kuramoto model of neural oscillation is a system-level explanation of how the neuro-system dynamics is associated with phenotypes such as clinical outcomes. In doing so, we introduce the Kuramoto order parameters φ t to quantify the synchronization level at time t as φ t = 1 N real{ N i=1 e iv(t) }, where real(•) denotes the real part of the complex number. In complex system area, φ is described as the synchronization level, aka. the metastability of the system  Empirical SC-FC Coupling Biomarkers. As a proof-of-concept approach, we propose a novel SC-FC coupling biomarker Φ = (φ t0 , φ t1 , ..., φ tT ) (bottom right corner in Fig. ",vol3
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,,SC-FC Coupling Network for Disease Diagnosis.,"To leverage the rich system-level heuristics from Kuramoto model, it is straightforward to integrate a classification branch on top of Φ which is trained to minimize the cross-entropy loss in classifying healthy and disease subjects. Thus, the tailored deep Kuramoto model for early diagnosis is called SC-FC-Net. ",vol3
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,3.0,Experiments,"In this study, we evaluate the statistical power and clinical value of our learningbased SC-FC coupling biomarkers in separating Alzheimer's Disease (AD) from cognitively normal (CN) subjects using ADNI dataset ",vol3
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,3.1,Validating the Neuroscience Insight of Deep Kuramoto Model,"The main hypothesis is that spontaneous functional fluctuations arise from the phase oscillations of coupled neural populations. In this regard, we spotlight the topological difference between the conventional FC by Pearson's correlation on BOLD signal and the reproduced FC based on the state variable of brain rhythm {v i (t)}. First, we display the population-average of conventional FC (Fig. ",vol3
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,3.2,Evaluation on Empirical Biomarker of SC-FC-META,"First, the CN vs. AD group comparison on SC-FC coupling biomarker is shown in Fig. ",vol3
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,3.3,Evaluation on SC-FC-Net in Diagnosing AD,"Herein, we first assess the accuracy of diagnosing AD using SC-FC-META and SC-FC-Net. The former method is a two-step approach where we first calculate the SC-FC-META biomarker for each subject and then train a SVM. While our SC-FC-Net is an end-to-end solution. The classification results are shown in Fig. ",vol3
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,4.0,Conclusion,"We introduce a novel approach that combines physics and deep learning to investigate the neuroscience hypothesis that spontaneous functional fluctuations arise from a dynamic system of neural oscillations. Our successful deep model has led to the discovery of new biomarkers that capture the synchronization level of neural oscillations over time, enabling us to identify the coupling between SC and FC in the brain. We have utilized these new SC-FC coupling biomarkers to identify brains at risk of AD, and have obtained promising classification results. This approach holds great promise for other neuroimaging applications.",vol3
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,1.0,Introduction,"Continual advancement of deep learning algorithms for medical image analysis has increased the potential for their adoption at scale. Across a wide range of medical applications including skin lesion classification  However, with these innovations has come increased scrutiny of the integrity of these models in safety critical applications. Prior work  As attention to these issues grows, recent legislation has been proposed that would require the algorithmic auditing and impact assessment of ML-based automated decision systems  Our method generates targeted hypotheses for model audits by assessing (1) how feasible it is for a downstream model to detect and exploit the presence of a given attribute from the image alone (detectability), and (2) how much information the model would gain about the task labels if said attribute were known (utility). Causally irrelevant attributes with high utility and detectability become top priorities when performing downstream model audits. We demonstrate high utility complicates attempts to draw conclusions about the detectability of attributes and show our approach succeeds where unconditioned approaches fail. We rigorously validate our approach using a range of synthetic artifacts which allow us to expedite the auditing of models via the use of true counterfactuals. We then apply our method to a popular skin lesion dataset where we identify a previously unreported potential shortcut.",vol3
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,2.0,Related Work,"Issues of bias and fairness are of increasing concern in the research community. Recent works such as  Of interest to this work,  In addition to model auditing methods, a number of metrics have been proposed to quantify bias  Lastly, ",vol3
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,3.0,Methods,"To audit at the dataset level, we perform a form of causal discovery to identify likely relationships between the task labels, dataset attributes represented as image metadata, and features of the images themselves (as illustrated in Fig.  We start from a set of labels {Y }, attributes {A}, and images {X}. We assume that Y (the disease) is the causal parent of X (the image) given that the disease affects the image appearance but not vice versa  Causal Discovery with Mutual Information. Considering attributes in isolation, we assess attribute utility and detectability from an information theoretic perspective. In particular, we recognize first that the presence of a relationship between A and Y can be measured via their Mutual Information: MI(A; Y ) = H(Y )-H(Y |A). MI measures the information gained (or reduction in uncertainty) about Y by observing A (or vice versa) and MI(A; Y ) = 0 occurs when A and Y are independent. We rely on the faithfulness assumption which implies that a causal relationship exists between A and Y when MI(A; Y ) > 0. From an auditing perspective, we aim to identify the presence and relative magnitude of the relationship but not necessarily the nature of it. Attributes identified as having a relationship with Y are then assessed for their detectability (i.e., condition (2)). We determine detectability by training a DNN on the data to predict attribute values. Because we wish to audit the entire dataset for bias, we cannot rely on a single train/val/test split. Instead, we partition the dataset into k folds (typically 3) and finetune a sufficiently expressive DNN on the train split of each fold to predict the given attribute A. We then generate unbiased predictions for the entire dataset by taking the output Â from each DNN evaluated on their respective test split. We measure the Conditional Mutual Information over all predictions: CM I( Â; A|Y ) = H( Â|Y )-H( Â|A, Y ). CM I(A, Â|Y ) measures information shared between attribute A and its prediction Â when controlling for information provided by Y . Since relationship A and Y was established via MI(A; Y ), we condition on label Y to understand the extent to which attribute A can be predicted from images when accounting for features associated with Y that may also improve the prediction of A. Similar to MI, CM I( Â; A|Y ) > 0 implies A → Â exists. To determine independence and account for bias and dataset specific effects, we include permutation-based shuffle tests from ",vol3
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,4.0,Experiments and Discussion,"To demonstrate the effectiveness of our method, we first conduct a series of experiments using synthetically-altered skin lesion data from the HAM10000 dataset where we precisely create, control, and assess biases in the dataset. After establishing the accuracy and sensitivity of our method on synthetic data, we apply our method to the natural attributes of HAM10000 in Experiment 5 (Sect. 4.5).",vol3
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,,Datasets:,"We use publicly available skin lesion data from the HAM10000  Training Protocol: For attribute prediction networks used by our detectability procedure, we finetune ResNet18 ",vol3
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,4.1,Experiment 1: Induced Bias Versus Relationship Strength,"For this experiment, we select an artifact that we are certain is visible (JPEG compression at quality 30 applied to 1000 images), and seek to understand how the relationship between attribute and task label influences the task model's reliance on the attribute. The artifact is introduced with increasing utility such that the probability of the artifact is higher for cases that are malignant. Then, we create a worst case counterfactual set, where each malignant case does not have the artifact, and each benign case does. In Fig. ",vol3
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,4.2,Experiment 2: Detectability of Known Invisible Artifacts,"In the previous section, we showed that the utility A ↔ Y directly impacts the task model bias, given A is visible in images. However, it is not always obvious whether an attribute is visible. In Reading Race, Gichoya et al. showed racial identity can be predicted with high AUC from medical images where this information is not expected to be preserved. Here, we show CMI represents a promising method for determining attribute detectability while controlling for attribute information communicated through labels and not through images. Specifically, we consider the case of an ""invisible artifact"". We make no changes to the images, but instead create a set of randomized labels for our nonexistent artifact that have varying correlation with the task labels (A ↔ Y ). As seen in Fig. ",vol3
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,4.3,Experiment 3: Conditioned Detectability Versus Ground Truth,"To verify that the conditional independence testing procedure does not substantially reduce our ability to correctly identify artifacts that truly are visible, we introduce Gaussian noise with standard deviation decreasing past human perceptible levels. In Experiment 2 (Fig 4A ) the performance of detecting artifact presence is artificially inflated because of a relationship between disease and artifact. Here the artifact is introduced at random so AUC is an unbiased measure of detectability. In Table ",vol3
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,4.4,Experiment 4: Relationship and Detectability vs Induced Bias,"Next, we consider how utility and detectability together relate to bias. We introduce a variety of synthetic artifacts and levels of bias and measure the drop in   AUC that occurs when evaluated on a test set with artifacts introduced in the same ratio as training versus the worst case ratio as defined in Experiment 1. Of 36 unique attribute-bias combinations trialed, 32/36 were correctly classified as visible via permutation test with 95% cutoff percentile. The remaining four cases were all compression at quality 90 and had negligible impact on task models (mean drop in AUC of -0.0003 ± .0006). In Fig. ",vol3
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,4.5,Experiment 5: HAM10000 Natural Attributes,"Last, we run our screening procedure over the natural attributes of HAM10000 and find that all pass the conditional independence tests of detectability. Based on our findings, we place the attributes in the following order of concern: (1) Data source, (2) Fitzpatrick Skin Scale, (3) Ruler Presence, (4) Gentian marking presence (we skip localization, age and sex due to clinical relevance ",vol3
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,5.0,Conclusions,"Our proposed method marks a positive step forward in anticipating and detecting unwanted bias in machine learning models. By focusing on dataset screening, we aim to prevent downstream models from inheriting biases already present and exploitable in the data. While our screening method naturally includes common auditing hypotheses (e.g., bias/fairness for vulnerable groups), it is capable of generating targeted hypotheses on a much broader set of attributes ranging from sensor information to clinical collection site. Future work could develop unsupervised methods for discovering additional high risk attributes without annotations. The ability to identify and investigate these hypotheses provides broad benefit for research, development, and regulatory efforts aimed at producing safe and reliable AI models.",vol3
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,1.0,Introduction,"Object detection and identification are key steps in many biomedical imaging applications  A precursor to fully automated workflows consists of accurate detection and recognition of dental structures  The identification of dental instances is a complex task due to the presence of anatomical variations such as crowding, missing teeth, and ""shark teeth"" (or double teeth)  The labeling of dental casts presents a non-trivial challenge of identifying objects of similar shapes that are geometrically connected and may have duplicated or missing elements. This study introduces an assignment theory-based approach for recognizing objects based on their positional inter-dependencies. We developed a distance-based dental model of jaw anatomy. The model was transformed into a cost function for a bipartite graph using a convolutional neural network. To compute the optimal labeling path in the graph, we introduced a novel loss term based on assignment theory into the objective function. The assignment theory-based framework was tested on a large database of dental casts and achieved almost perfect labeling of the teeth.",vol3
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,2.1,Generation of Candidate Labels,"The database utilized in the present study comprised meshes that represented dental casts, with the lower and upper jaws being depicted as separate entities. Each mesh vertex was associated with a label following the World Dental Federation (FDI) tooth numbering system. A large proportion of the samples in the dataset was associated with individuals who had healthy dentition. A subset of patients presented dental conditions, including misaligned, missing, and duplicated teeth. Furthermore, the dataset consisted of patients with both permanent and temporary dentition. The dental cast labeling task was divided into two stages: the detection of candidate teeth ",vol3
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,2.2,Dental Anatomical Model,"The difficulty of segmenting dental structures stems from the high inter-personal shape and position variability, artifacts (e.g. fillings, implants, braces), embedded, and missing teeth  To address these challenges and build an accurate instance segmentation pipeline, we first generated a dental anatomical model that provided a framework for understanding the expected positions of the teeth within the jaw. The dental model relied on the relative distances between teeth, instead of their actual positions, for robustness against translation and rotation transformations. The initial step in modeling the jaws was calculating the centroids of the dental instances by averaging the coordinates of their vertices. The spatial relationship between two teeth centroids, c 1 and c 2 , was described by the displacement vector, d = c 1 -c 2 . To evaluate the relative position of two instances of types t 1 and t 2 , we calculated the means (μ x , μ y , μ z ) and standard deviations (σ x , σ y , σ z ) for each dimension (x, y, z) of the displacement vectors corresponding to instances of types t 1 and t 2 in patients assigned to the training set. The displacement rating r for the two instances was computed as the average of the univariate Gaussian probability density function evaluated at each dimension (d x , d y , d z ) of the displacement vector d: (1) The dental model of a patient consisted of a 3D matrix A of shape m×m×4, where m corresponded to the total number of tooth types. For each pair of dental instances i and j, the elements a ij0 , a ij1 , a ij2 correspond to the x, y, and z coordinates of the displacement vector, respectively, while a ij3 represents the relative position score. The presence of anatomical anomalies, such as missing teeth and double teeth, did not impact the information embedded in the dental model. If tooth i was absent, the values for the displacement vectors and position ratings in both row i and column i of the dental model A were set to zero.",vol3
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,2.3,The Assignment Problem,"The next step was evaluating and correcting the candidate tooth labels generated by the U-net models using the dental anatomical model. We reformulated the task as finding the optimal label assignment. Assignment theory aims to solve the similar task of assigning m jobs to m workers in a way that minimizes expenses or maximizes productivity  Proof. Let's assume that the addition of one dummy object θ with maximum assignment cost q changes the optimal assignment to g ∈ F\{f * } on the set C ∪ {θ}. The assignment cost of the new candidate set considering that B θ,g(θ) = q. The definition of the optimal label assignment function f * states that its cumulative assignment cost is smaller than the cumulative assignment cost of any g ∈ F\{f * } on the set C ∪ {θ}, which indicates that This contradicts the assumption that g is the optimal assignment for C ∪ {θ}. This proof can be generalized to the case where multiple dummy teeth are added to the candidate set. In other words, Proposition 1 states that adding ""dummy"" instances to the candidate teeth sets does not affect the optimal assignment of the non-dummy objects. The dummy teeth played a dual role in our analysis. On one hand, they could be used to account for the presence of double teeth in patients with the ""shark teeth"" condition. On the other hand, they could be assigned to missing teeth in patients with missing dentition.",vol3
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,2.4,DentAssignNet,"The optimal assignment solution f * ensured that each candidate tooth would be assigned a unique label. We integrated the assignment solver into a convolutional neural network for labeling candidate teeth, entitled DentAssignNet (Fig.  The input to the convolutional neural network consisted of a matrix A of shape m × m × 4, which represented a dental model as introduced in Sect. 2.2. For each pair of dental instances i and j, the element a ij included the coordinates of the displacement vector and the relative position score. The architecture of the network was formed of 3 convolutional blocks. Each convolutional block in the model consisted of the following components: a convolutional layer, a rectified linear unit (ReLU) activation function, a max pooling layer, and batch normalization. The convolutional and pooling operations were applied exclusively along the rows of the input matrices because the neighboring elements in each row were positionally dependent. The output of the convolutional neural network was a cost matrix B of shape m × m, connecting candidate instances to potential labels. The assignment solver transformed the matrix B into the optimal label assignment C * . The loss function utilized during the training phase was a weighted sum of two binary cross entropy losses: between the convolutional layer's output Ŷ and the ground truth Y, and between Ŷ multiplied by the optimal assignment solution Ŷ and the ground truth Y. The direct application of the assignment solver on a dental model A with dimensions m × m × 4 was not possible, as the solver required the weights associated with the edges of the bipartite graph connecting candidate instances to labels. By integrating the optimal assignment solution in the loss function, Den-tAssignNet enhanced the signal corresponding to the most informative convolutional layer output cells in the task of labeling candidate teeth.",vol3
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,3.1,Database,The database employed in this study was introduced as part of the 3D Teeth Scan Segmentation and Labeling Challenge held at MICCAI 2022 ,vol3
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,3.2,Experiment Design,"To obtain the candidate tooth labels, we employed the U-net architecture on the volumetric equivalent of the dental mesh. Considering that the majority of the samples in the database featured individuals with healthy dentition, a series of transformations were applied to the U-net results to emulate the analysis of cases with abnormal dentition. To simulate the simultaneous occurrence of both permanent and temporary dental instances of the same type, we introduced artificial centroids with labels copied from existing teeth. They were placed at random displacements, ranging from d min to d max millimeters, from their corresponding true teeth, with an angulation that agreed with the shape of the patient's dental arch. The protocol for constructing the database utilized by our model involved the following steps. Firstly, we calculated the centroids of each tooth using the vertex labels generated by the U-Net model. Subsequently, we augmented the U-net results by duplicating, removing, and swapping teeth. The duplication procedure was performed with a probability of p d = 0.5, with the duplicate positioned at a random distance between d min = 5 millimeters and d max = 15 millimeters from the original. The teeth removal was executed with a probability of p e = 0.5, with a maximum of e = 4 teeth being removed. Lastly, the swapping transformation was applied with a probability of p w = 0.5, involving a maximum of w = 2 tooth pairs being swapped. Given that neighboring dental instances tend to share more similarities than those that are further apart, we restricted the label-swapping process to instances that were located within two positions of each other.",vol3
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,3.3,Results,"Each sample in the database underwent 10 augmentations, resulting in 9000 training samples, 1000 validation samples, and 2000 testing samples for both jaws. The total number of possible tooth labels was set to m = 17, consisting of t = 16 distinct tooth types and d = 1 double teeth. The training process involved 100 epochs, and the models were optimized using the RMSprop algorithm with a learning rate of 10 -4 and a weight decay of 10 -8 . The weighting coefficient in the assignment-based loss was λ = 0.8. The metrics reported in this section correspond to the detection and identification of teeth instances. The U-net models achieved detection accuracies of 0.989 and 0.99 for the lower and upper jaws, respectively. The metrics calculated in the ablation study take into account only the dental instances that were successfully detected by the candidate teeth proposing framework. Table ",vol3
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,4.0,Conclusion,"We proposed a novel framework utilizing principles of assignment theory for the recognition of objects within structured, multi-object environments with missing or duplicate instances. The multi-step pipeline consisted of detecting and assigning candidate labels to the objects using U-net (1), modeling the environment considering the positional inter-dependencies of the objects (2), and finding the optimal label assignment using DentAssignNet (3). Our model was able to effectively recover most teeth misclassifications, resulting in identification accuracies of 0.992 and 0.991 for the lower and upper jaws, respectively.",vol3
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,1.0,Introduction,"Prostate cancer is a leading cause of cancer-related deaths in adult males, as reported in studies, such as  Due to the relatively low spatial resolution and soft tissue contrast in CT images compared to MRI images, manual prostate segmentation in CT images can be time-consuming and may result in significant variations between operators  In spite of the improved performance for the aforementioned ViT-based networks, these methods utilize the standard or shifted-window-based SA, which is the fine-grained local SA and may overlook the local and global interactions  Recently, Focal Transformer ",vol3
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,2.1,FocalUNETR,"Our FocalUNETR architecture (Fig.  For window-wise focal SA  )×(s l w ×s l w ) . After obtaining the pooled feature maps x l L 1 , we calculate the query at the first level and key and value for all levels using three linear projection layers f q , f k , and f v : For the queries inside the i-th window Q i ∈ R d×sw×sw , we extract the s l r × s l r keys and values from K l and V l around the window where the query lies in and then gather the keys and values from all L to obtain Finally, a relative position bias is added to compute the focal SA for where B = {B l } L 1 is the learnable relative position bias ",vol3
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,2.2,The Auxiliary Task,"For the main task of mask prediction (as illustrated in Fig.  To address the challenge of unclear boundaries in CT-based prostate segmentation, an auxiliary task is introduced for the purpose of predicting boundaryaware contours to assist the main prostate segmentation task. This auxiliary task is achieved by attaching another convolution head after the extracted feature maps at the final stage (see Fig.  where N is the total number of images for each batch. This auxiliary task is trained concurrently with the main segmentation task. A multi-task learning approach is adopted to regularize the main segmentation task through the auxiliary boundary prediction task. The overall loss function is a combination of L seg and L reg : L tol = λ 1 L seg + λ 2 L reg , where λ 1 and λ 2 are hyper-parameters that weigh the contribution of the mask prediction loss and contour regression loss, respectively, to the overall loss. The optimal setting of λ 1 = λ 2 = 0.5 is determined by trying different settings.",vol3
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,3.1,Datasets and Implementation Details,"To evaluate our method, we use a large private dataset with 400 CT scans and a large public dataset with 300 CT scans (AMOS  Regarding the architecture, we follow the hyperparameter settings suggested in  For the implementation, we utilize a server equipped with 8 Nvidia A100 GPUs, each with 40 GB of memory. All experiments are conducted in PyTorch, and each model is trained on a single GPU. We interpolate all CT scans into an isotropic voxel spacing of [1.0 × 1.0 × 1.5] mm for both datasets. Houndsfield unit (HU) range of ",vol3
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,3.2,Experiments,"Comparison with State-of-the-Art Methods. To demonstrate the effectiveness of FocalUNETR, we compare the CT-based prostate segmentation performance with three 2D U-Net-based methods: U-Net  Quantitative results are presented in Table ",vol3
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,4.0,Conclusion,"In summary, the proposed FocalUNETR architecture has demonstrated the ability to effectively capture local visual features and global contexts in CT images by utilizing the focal self-attention mechanism. The auxiliary contour regression task has also been shown to improve the segmentation performance for unclear boundary issues in low-contrast CT images. Extensive experiments on two large CT datasets have shown that the FocalUNETR outperforms state-ofthe-art methods for the prostate segmentation task. Future work includes the evaluation of other organs and extending the focal self-attention mechanism for 3D inputs.",vol3
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,1.0,Introduction,"Although deep learning has achieved great success in various computer vision tasks  the field of medical image analysis. Annotated abnormal images are difficult to acquire, especially for rare or new diseases, such as the COVID-19, whereas normal images are much easier to obtain. Therefore, many efforts  Due to the absence of anomalous subjects, most previous studies adopted the encoder-decoder structure or generative adversarial network (GAN) as backbone to obtain image reconstruction error as the metric for recognition of outliers  To this end, we propose a novel model-agnostic framework, denoted as Multi-Anomaly Detection with Disentangled Representation (MADDR), for the simultaneous detection of anomalous images within different organs and modalities. As displayed in Fig.  -To the best of our knowledge, this is the first study to detect the anomalies of multiple organs and modalities with a single network. We show that introducing images from different organs and modalities with the proposed framework not only extends the generalization ability of the network towards the recognition of the anomalies within various data, but also improves its performance on each single kind. -We propose to disentangle the latent representation into three parts so that the categorical information can be fully exploited through the classification constraints, and the feature representation of normal images is tightly clustered with the center constraint for better identification of anomalous pattern. -Extensive experiments demonstrate the superiority of the proposed framework regarding the medical anomaly detection task, as well as its universal applicability to various baseline models. Moreover, the effectiveness of each component is evaluated and discussed with thorough ablation study.",vol3
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,2.0,Methodology,"Unlike previous approaches which trained a separate model to capture the anomalies for each individual organ and modality, in this study we aim to exploit the normal images of multiple organs and modalities to train a generic network towards better anomaly detection performance for each of them. The proposed framework is model-agnostic and can be readily applied to most standard anomaly detection methods. For demonstration, we adopt four state-of-the-art anomaly detection methods, i.e., deep perceptual autoencoder (DPA)  In this section, we present the proposed universal framework for the anomaly detection task of medical images in details.",vol3
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,2.1,Framework Overview,"We first formulate the anomaly detection task for images with various organs and modalities. For a dataset targeting a specific organ k (k ∈ [0, K]) and modality l (l ∈ [0, L]), there is a training dataset D k,l with only normal images and a test set D t k,l with both normal and abnormal images. In this study, we use N k to represent total number of images targeting organ k and N l to represent total number of images belong to modality l. The goal of our study is to train a generic model with these multi-organ and multi-modality normal images to capture the intrinsic normal distribution of training sets, such that the anomalies in test sets can be recognized as outliers. To better elaborate the process of applying our MADDR framework on standard anomaly detection methods, we first briefly introduce the workflow of a baseline method, DPA  , where x and x denote the input image and the reconstructed one, respectively. f (x) = f (x)-μ σ is the normalized feature with mean μ and standard deviation σ pre-calculated on a large dataset, where f (•) represents the mapping function of the pre-trained feature extractor. By comparing the features of original images and the reconstructed ones through the relative perceptual loss, the subjects with loss larger than the threshold are recognized as abnormal ones. To fully exploit the underlying patterns in the normal images of various organs and modalities, we incorporate additional constraints on the encoded latent representations. Specifically, our MADDR approach encourages the model to convert the input image x into a latent representation z, which consists of disentangled category and individuality information. To be more precise, the encoded latent representation z is decomposed into three parts, i.e., the organ category part z o , the modality category part z m and the continuous variable part z c . Here, z o and z m represent the categorical information (which is later converted into the probabilities of x belonging to each organ and modality through two separate fully-connected layers), and z c denotes the feature representation for characterizing each individual image (which should be trained to follow the distribution of normal images). Leveraging the recorded categorical information of the images, we impose two classification constraints on z o and z m , respectively, along with a center constraint on z c to compact the cluster of feature representation in addition to the original loss(es) of the baseline methods. In this study, we evaluate the proposed model-agnostic framework on four cutting-edge methods  where X and Y denote the set of images and labels, respectively, while λ 1 , λ 2 and λ 3 are the weights to balance different losses, and set to 1 in this study (results of exploratory experiment are displayed in the supplementary material).",vol3
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,2.2,Organ and Modality Classification Constraints,"Benefiting from the acquisition procedure of medical images, the target organ and modality of each scan should be recorded and can be readily used in our study. Based on the assumption that a related task could provide auxiliary guidance for network training towards superior performance regrading the original task, we introduce two additional classification constraints (organ and modality classification) to fully exploit such categorical information. Through an additional organ classifier (a fully-connected layer), the organ classification constraint is applied on the transformed category representation z o by distinguishing different organs. A similar constraint is also applied on the modality representation z m . Considering the potential data imbalance issue among images of different organs and modalities, we adopt the focal loss  where α k o denotes the weight to balance the impact of different organs and P k (z i o ) represents the probability of image i belonging to class k. The focusing parameter γ can reduce the contribution of easy samples to the loss function and extend the range of loss values for comparison. Similarity, with the modality category representation z i m , the modality classification loss can be written as:",vol3
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,2.3,Center Constraint,"Intuitively, the desired quality of a representation is to have similar feature embeddings for images of the same class. Because in the anomaly detection task, all the images used for training belong to the same normal group, we impose a center constraint so that the features from the normal images are tightly clustered to the center and the encoded features of abnormal images lying far from the normal cluster are easy to identify. However, directly compacting the latent representation into a cluster is potentially contradictory to the organ and modality classification tasks which aim to separate different organs and modalities. To avoid the contradiction, we propose to impose the center constraint only on the continuous variable part z c with Euclidean distance as the measurement of the compactness. Similar to  where L b represents the number of images in batch b, B denotes the number of batches and is the mean of the rest images in the same batch as image i.",vol3
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,2.4,Optimization and Inference,"To demonstrate the effectiveness of the proposed framework, we inherit the network architectures and most procedures of the baseline methods for a fair comparison. During the optimization stage, there are only two differences: 1) we introduce three additional losses as stated above; 2) considering the limited number of images, mixup ",vol3
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,3.1,Experimental Setting,"We evaluate our method on three benchmark datasets targeting various organs and modalities as stated below. -The LiTS-CT dataset  To ensure a sufficient amount of training data, we remove the 2D abnormal slices from some patients and use the rest normal slices for training. Therefore, 186 axial CT slices from 77 3D volumes are used as normal data for training, 192 normal and 105 abnormal slices from 27 3D volumes for validation, and 164 normal and 249 abnormal slices from the rest 27 3D volumes for testing. -The Lung-X-rays  To evaluate the performance of proposed framework, we adopt three widely used metrics, including the area under the curve (AUC) of the receiver operating characteristic, F1-score and accuracy (ACC). Grid search is performed to find the optimal threshold based on the F1-score of the validation set. For all three metrics, a higher score implies better performance. The experiments are repeated three times with different random seeds to verify the robustness of the framework and provide more reliable results. The framework is implemented with PyTorch 1.4 toolbox ",vol3
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,3.2,Comparison Study,The quantitative results of the proposed framework and the state-of-the-art methods are displayed in Table ,vol3
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,3.3,Ablation Study,"To further demonstrate the effectiveness of each component, we perform an ablation study with the following variants: 1) the baseline DPA method (the dimensions of z is 16) which trains networks for each organ and modality separately; 2) the baseline DPA method with the dimensions of z increased to 128; 3) using the images of LiTS, Lung-CT and Lung-X-rays datasets together to train the same DPA network; The results of all these variants are displayed in Table ",vol3
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,4.0,Conclusion,"Unlike previous studies which committed to train exclusive networks to recognize the anomalies of specific organs and modalities separately, in this work we hypothesized that normal images of various organs and modalities could be combined and utilized to train a generic network and superior performance could be achieved for the recognition of each type of anomaly with proper methodology. With the proposed model-agnostic framework, the organ/modality classification constraint and the center constraint were imposed on the disentangled latent representation to fully utilize the available information as well as improve the compactness of representation to facilitate the identification of outliers. Four state-of-the-art methods were adopted as baseline models for thorough evaluation, and the results on various organs and modalities demonstrated the validity of our hypothesis as well as the effectiveness of the proposed framework.",vol3
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_23.,vol3
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,1.0,Introduction,Supervised learning for medical image segmentation requires a large amount of per-voxel annotated data  Consistency regularization methods are widely studied in semi-supervised segmentation models. Consistent predictions are enforced by perturbing input images  Motivated by Knowledge Distillation (KD) ,vol3
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,2.1,Overview,As shown in Fig. ,vol3
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,2.2,Deep Mutual Learning,"The original DML  The critical part of mutual learning contains a 2-way KL mimicry loss: where L ml is obtained on both labelled and unlabelled sample points, p 1 , p 2 being posterior probability predictions of corresponding networks. Together with standard supervised loss obtained on labelled sample points, and the trade-off weight λ, we get the final DML  where λ is set to 1 in DML ",vol3
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,2.3,Deep Mutual Distillation,"The original p m j of DML  In the case of the binary segmentation task, we replace softmax with sigmoid to get the distilled per-pixel probability mask p j,T from f θj : However, using KL-divergence-based loss in KD  Together with standard supervised loss obtained on labelled sample points, and the trade-off weight λ, we get our final DMD objective: where L distill is obtained on both labelled and unlabelled sample points. Here, we also adopt Dice loss  With the temperature scaling, each network under DMD learns from each other through the distilled high-entropy probabilities, which are more informative, especially on ambiguous regions. The distillation ",vol3
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,3.1,Experimental Setup,Dataset: We evaluated our proposed DMD on the 2018 Atria Segmentation Challenge (LA) ,vol3
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,,Evaluation Metric:,"The performance of our method is quantitatively evaluated in terms of Dice, Jaccard, the average surface distance (ASD), and the 95% Hausdorff Distance (95HD) as previous methods  Implementation Details: We implement DMD using PyTorch ",vol3
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,3.2,Quantitative Evaluation,"We compare DMD with previous state-of-the-arts  To study how consistency-based methods with different entropy guidance affect performances, we implement CPS ",vol3
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,3.3,Parameter Analysis,"Here, we first demonstrate the effectiveness of temperature scaling T and the choice of KL-divergence-based and Dice ",vol3
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,4.0,Conclusions,"We revisit Knowledge Distillation and have presented a novel and simple semisupervised medical segmentation method through Deep Mutual Distillation. We rethink and analyze consistency regularization-based methods with the entropy minimization, and point out that cross guidance with low entropy on extremely ambiguous regions may be unreliable. We hereby propose to introduce a temperature scaling strategy into the network training and propose a Dice-based distillation loss to alleviate the influence of the background noise when the temperature T > 1. Our DMD works favorably for semi-supervised medical image segmentation, especially when the number of training data is small (e.g., 10% training data are labelled in LA). Compared with all prior arts, a significant improvement up to 1.15% in the Dice score is achieved in LA dataset. Ablation studies with the consistency-based methods of different entropy guidance further verify our assumption and design.",vol3
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 52.,vol3
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,1.0,Introduction,"Deep neural networks (DNNs) have achieved remarkable success in important areas of various domains, such as computer vision, machine learning and natural language processing. Nevertheless, there exists growing evidence that suggests that these models are poorly calibrated, leading to overconfident predictions that may assign high confidence to incorrect predictions  In light of the significance of this issue, there has been a surge in popularity for quantifying the predictive uncertainty in modern DNNs. A simple approach involves a post-processing step that modifies the softmax probability predictions of an already trained network  Indeed, the nature of structured predictions in segmentation, involves pixelwise classification based on spatial dependencies, which limits the effectiveness of these strategies to yield performances similar to those observed in classification tasks. In particular, this potentially suboptimal performance can be attributed to the uniform (or near-to-uniform) distribution enforced on the softmax/logits distributions, which disregards the spatial context information. To address this important issue, Spatially Varying Label Smoothing (SVLS)  The contributions of this work can be summarized as follows: -We provide a constrained-optimization perspective of Spatially Varying Label Smoothing (SVLS) ",vol3
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,2.0,Methodology,"Formulation. Let us denote the training dataset as Ωn representing the n th image, Ω n the spatial image domain, and y (n) ∈ Y ⊂ R K its corresponding ground-truth label with K classes, provided as a one-hot encoding vector. Given an input image x (n) , a neural network parameterized by θ generates a softmax probability vector, defined as where s is obtained after applying the softmax function over the logits l (n) ∈ R Ωn×K . To simplify the notations, we omit sample indices, as this does not lead to any ambiguity.",vol3
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,2.1,A Constrained Optimization Perspective of SVLS,"Spatially Varying Label Smoothing (SVLS)  Thus, once we replace the smoothed labels ỹk p in the standard cross-entropy (CE) loss, the new learning objective becomes: where s k p is the softmax probability for the class k at pixel p (the pixel in the center of the patch). Now, this loss can be decomposed into: with p denoting the index of the pixel in the center of the patch. Note that the term in the left is the cross-entropy between the posterior softmax probability and the hard label assignment for pixel p. Furthermore, let us denote y k i w i as the soft proportion of the class k inside the patch/mask y, weighted by the filter values w. By replacing τ k into the Eq. 3, and removing | d i w i | as it multiplies both terms, the loss becomes: As τ is constant, the second term in Eq. 4 can be replaced by a Kullback-Leibler (KL) divergence, leading to the following learning objective: where c = stands for equality up to additive and/or non-negative multiplicative constant. Thus, optimizing the loss in SVLS results in minimizing the crossentropy between the hard label and the softmax probability distribution on the pixel p, while imposing the equality constraint τ = s, where τ depends on the class distribution of surrounding pixels. Indeed, this term implicitly enforces the softmax predictions to match the soft-class proportions computed around p.",vol3
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,2.2,Proposed Constrained Calibration Approach,"Our previous analysis exposes two important limitations of SVLS: 1) the importance of the implicit constraint cannot be controlled explicitly, and 2) the prior τ is derived from the σ value in the Gaussian filter, making it difficult to model properly. To alleviate this issue, we propose a simple solution, which consists in minimizing the standard cross-entropy between the softmax predictions and the one-hot encoded masks coupled with an explicit and controllable constraint on the logits l. In particular, we propose to minimize the following constrained objective: where τ now represents a desirable prior, and τ = l is a hard constraint. Note that the reasoning behind working directly on the logit space is two-fold. First, observations in  Even though the constrained optimization problem presented in Eq. 6 could be solved by a standard Lagrangian-multiplier algorithm, we replace the hard constraint by a soft penalty of the form P(|τ -l|), transforming our constrained problem into an unconstrained one, which is easier to solve. In particular, the soft penalty P should be a continuous and differentiable function that reaches its minimum when it verifies P(|τ -l|) ≥ P(0), ∀ l ∈ R K , i.e., when the constraint is satisfied. Following this, when the constraint |τ -l| deviates from 0 the value of the penalty term increases. Thus, we can approximate the problem in Eq. 6 as the following simpler unconstrained problem: where the penalty is modeled here as a ReLU function, whose importance is controlled by the hyperparameter λ.",vol3
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,3.1,Setup,"Datasets. FLARE Challenge  Implementation Details . We benchmark the proposed model against several losses, including state-of-the-art calibration losses. These models include the compounded CE + Dice loss (CE+DSC), FL  For training, we fixed the batch size to 16, epochs to 100, and used ADAM ",vol3
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,3.2,Results,"Comparison to State-of-the-Art. Table  To have a better overview of the performance of the different methods, we follow the evaluation strategies adopted in several MICCAI Challenges, i.e., sumrank  Ablation Studies. 1-Constraint over logits vs softmax. Recent evidence  Impact of the Prior. A benefit of the proposed formulation is that diverse priors can be enforced on the logit distributions. Thus, we now assess the impact of different priors τ in our formulation (See Supplemental Material for a detailed explanation). The results presented in Table ",vol3
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,4.0,Conclusion,"We have presented a constrained-optimization perspective of SVLS, which has revealed two important limitations of this method. First, the implicit constraint enforced by SVLS cannot be controlled explicitly. And second, the prior imposed in the constraint is directly derived from the Gaussian kernel used, which makes it hard to model. In light of these observations, we have proposed a simple alternative based on equality constraints on the logits, which allows to control the importance of the penalty explicitly, and the inclusion of any desirable prior in the constraint. Our results suggest that the proposed method improves the quality of the uncertainty estimates, while enhancing the segmentation performance.",vol3
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 55.,vol3
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,1.0,Introduction,"Segmentation is key in medical image analysis and is primarily achieved with pixel-wise classification neural networks  Other methods estimate the aleatoric uncertainty from multiple forward passes of test-time augmentation  Previous methods provide pixel-wise uncertainty estimates. These estimates are beneficial when segmenting abnormal structures that may or may not be present. However, they are less suited for measuring uncertainty on organ delineation because their presence in the image are not uncertain. In this work, we propose a novel method to estimate aleatoric uncertainty of point-wise defined contours, independent on the model's architecture, without compromising the contour estimation performance. We extend state-of-the-art point-regression networks ",vol3
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,2.0,Method,"Let's consider a dataset made of N pairs {x i , y k i } N i=1 , each pair consisting of an image x i ∈ R H×W of height H and width W , and a series of K ordered points y k i , drawn by an expert. Each point series defines the contour of one or more organs depending on the task. A simple way of predicting these K points is to regress 2K values (x-y coordinates) with a CNN, but doing so is sub-optimal due to the loss of spatial coherence in the output flatten layer  Inspired by this work, our method extends to the notion of heatmaps to regress univariate, bivariate, and skew-bivariate uncertainty models.",vol3
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,2.1,Contouring Uncertainty,"Univariate Model -In this approach, a neural network f θ (•) is trained to generate K heatmaps Z k ∈ R H×W which are normalized by a softmax function so that their content represents the probability of presence of the center c k of each landmark point. Two coordinate maps I ∈ R H×W and J ∈ R H×W , where",vol3
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,,H,", are then combined to these heatmaps to regress the final position μ k and the corresponding variance (σ k x , σ k y ) of each landmark point through the following two equations: where •, • F is the Frobenius inner product, corresponds to the Hadamard product, and (σ ky ) 2 is computed similarly. Thus, for each image x i , the neural network f θ (x i ) predicts a tuple (μ i , σ i ) with μ i ∈ R 2K and σ i ∈ R 2K through the generation of K heatmaps. The network is finally trained using the following univariate aleatoric loss adapted from  where y k i is the k th reference landmark point of image x i . Bivariate Model -One of the limitations of the univariate model is that it assumes no x-y covariance on the regressed uncertainty. This does not hold true in many cases, because the uncertainty can be oblique and thus involve a nonzero x-y covariance. To address this, one can model the uncertainty of each point with a 2 × 2 covariance matrix, Σ, where the variances are expressed with Eq. 2 and the covariance is computed as follows: The network f θ (x i ) thus predicts a tuple (μ i , Σ i ) for each image x i , with μ i ∈ R K×2 and Σ i ∈ R K×2×2 . We propose to train f θ using a new loss function L N2 : Asymmetric Model -One limitation of the bivariate method is that it models a symmetric uncertainty, an assumption that may not hold in some cases as illustrated on the right side of Fig.  where φ n is a multivariate normal, Φ 1 is the cumulative distribution function of a unit normal, Σ = ω Σω and α ∈ R n is the skewness parameter. Note that this is a direct extension of the multivariate normal as the skew-normal distribution is equal to the normal distribution when α = 0. The corresponding network predicts a tuple (μ, Σ, α) with μ ∈ R K×2 , Σ ∈ R K×2×2 and α ∈ R K×2 . The skewness output α is predicted using a sub-network whose input is the latent space of the main network (refer to the supplementary material for an illustration). This model is trained using a new loss function derived from the maximum likelihood estimate of the skew-normal distribution:",vol3
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,2.2,Visualization of Uncertainty,"As shown in Fig.  From these distributions, we draw isolines of equal uncertainty on the inside and outside of the predicted contour. By aggregating multiple isolines, we construct a smooth uncertainty map along the contours (illustrated by the white-shaded areas). Please refer to the supp. material for further details on this procedure. 3 Experimental Setup",vol3
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,3.1,Data,"CAMUS. The CAMUS dataset  Manual annotations for the endocardium and epicardium borders of the left ventricle (LV) and the left atrium were obtained from a cardiologist for the end-diastolic (ED) and end-systolic (ES) frames. The dataset is split into 400 training patients, 50 validation patients, and 50 testing patients. Contour points were extracted by finding the basal points of the endocardium and epicardium and then the apex as the farthest points along the edge. Each contour contains 21 points. Private Cardiac US. This is a proprietary multi-site multi-vendor dataset containing 2D echocardiograms of apical two and four chambers from 890 patients. Data comes from patients diagnosed with coronary artery disease, COVID, or healthy volunteers. The dataset is split into a training/validation set (80/20) and an independent test set from different sites, comprised of 994 echocardiograms from 684 patients and 368 echocardiograms from 206 patients, respectively. The endocardium contour was labeled by experts who labeled a minimum of 7 points based on anatomical landmarks and add as many other points as necessary to define the contour. We resampled 21 points equally along the contour. JSRT. The Japanese Society of Radiological Technology (JSRT) dataset consists of 247 chest X-Rays ",vol3
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,3.2,Implementation Details,We used a network based on ENet  Training was carried out with the Adam optimizer ,vol3
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,3.3,Evaluation Metrics,To assess quality of the uncertainty estimates at image and pixel level we use: Correlation. The correlation between image uncertainty and Dice was computed using the absolute value of the Pearson correlation score. We obtained image uncertainty be taking the sum of the uncertainty map and dividing it by the number of foreground pixels.,vol3
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,,Maximum Calibration Error (MCE),. This common uncertainty metric represents the probability if a classifier (here a segmentation method) of being correct by computing the worst case difference between its predicted confidence and its actual accuracy  Uncertainty Error Mutual-Information. As proposed in ,vol3
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,4.0,Results,"We computed uncertainty estimates for both pixel-wise segmentation and contour regression methods to validate the hypothesis that uncertainty prediction is better suited to per-landmark segmentation than per-pixel segmentation methods. For a fair comparison, we made sure the segmentation models achieve similar segmentation performance, with the average Dice being .90 ± .02 for CAMUS, .86 ± .02 for Private US., and .94 ± .02 for JSRT. For the pixel-wise segmentations, we report results of a classical aleatoric uncertainty segmentation method  As for the landmark prediction, since no uncertainty estimation methods have been proposed in the literature, we adapted the MC-Dropout method to it. We also report results for our method using univariate, (N 1 ), bivariate, (N 2 ) and bivariate skew-normal distributions (SN 2 ). The uncertainty maps for TTA and MC-Dropout (i.e. those generating multiple samples) were constructed by computing the pixel-wise entropy of multiple Fig.  Quantitative results are presented in Table ",vol3
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,5.0,Discussion and Conclusion,"The results reported before reveal that approaching the problem of segmentation uncertainty prediction via a regression task, where the uncertainty is expressed in terms of landmark location, is globally better than via pixel-based segmentation methods. It also shows that our method (N 1 , N 2 and SN 2 ) is better than the commonly-used MC-Dropout. It can also be said that our method is more interpretable as is detailed in Sect. 2.2 and shown in Fig.  The choice of distribution has an impact when considering the shape of the predicted contour. For instance, structures such as the left ventricle and the myocardium wall in the ultrasound datasets have large components of their contour oriented along the vertical direction which allows the univariate and bivariate models to perform as well, if not better, than the asymmetric model. However, the lungs and heart in chest X-Rays have contours in more directions and therefore the uncertainty is better modeled with the asymmetric model. Furthermore, it has been demonstrated that skewed uncertainty is more prevalent when tissue separation is clear, for instance, along the septum border (CAMUS) and along the lung contours (JSRT). The contrast between the left ventricle and myocardium in the images of the Private Cardiac US dataset is small, which explains why the simpler univariate and bivariate models perform well. This is why on very noisy and poorly contrasted data, the univariate or the bivariate model might be preferable to using the asymmetric model. While our method works well on the tasks presented, it is worth noting that it may not be applicable to all segmentation problems like tumour segmentation. Nevertheless, our approach is broad enough to cover many applications, especially related to segmentation that is later used for downstream tasks such as clinical metric estimation. Future work will look to expand this method to more general distributions, including bi-modal distributions, and combine the aleatoric and epistemic uncertainty to obtain the full predictive uncertainty.",vol3
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_21.,vol3
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,1.0,Introduction,"The shortage of labeled data is a significant challenge in medical image segmentation, as acquiring large amounts of labeled data is expensive and requires specialized knowledge. This shortage limits the performance of existing segmentation  models. To address this issue, researchers have proposed various semi-supervised learning (SSL) techniques that incorporate both labeled and unlabeled data to train models for both natural  Recently, some researchers proposed class-imbalanced semi-supervised methods  In this work, we explore the importance of heterogeneity in solving the over-fitting problem of CPS (Fig.  To achieve this, we propose DistDW (Distribution-aware Debiased Weighting) and DiffDW (Diff iculty-aware Debiased Weighting) strategies to guide the two sub-models to tackle different biases, leading to heterogeneous learning directions. Specifically, DistDW solves the data bias by calculating the imbalance ratio with the unlabeled data and forcing the model to focus on extreme minority classes through careful function design. Then, after observing the inconsistency between the imbalance degrees and the performances (see Fig.  The key contributions of our work can be summarized as follows: 1) we first state the homogeneity issue of CPS and improve it with a novel dual-debiased heterogeneous co-training framework targeting the class imbalance issue; 2) we propose two novel weighting strategies, DistDW and DiffDW, which effectively solve two critical issues of SSL: data and learning biases; 3) we introduce two public datasets, Synapse ",vol3
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,2.0,Methods,Figure ,vol3
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,2.1,Heterogeneous Co-training Framework with Consistency Supervision,"Assume that the whole dataset consists of N L labeled samples {(x l i , y i )} NL i=1 and N U unlabeled samples {x u i } NU i=1 , where x i ∈ R D×H×W is the input volume and y i ∈ R K×D×H×W is the ground-truth annotation with K classes (including background). The two sub-models of DHC complement each other by minimizing the following objective functions with two diverse and accurate weighting strategies: where p is the output probability map and ŷ(•  is the unsupervised loss function to measure the prediction consistency of two models by taking the same input volume x i . Note that both labeled and unlabeled data are used to compute the unsupervised loss. Finally, we can obtain the total loss: L total = L s + λL u , we empirically set λ as 0.1 and follow ",vol3
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,2.2,Distribution-aware Debiased Weighting (DistDW),"To mitigate the data distribution bias, we propose a simple yet efficient reweighing strategy, DistDW. DistDW combines the benefits of the SimiS  where β is the momentum parameter, set to 0.99 experimentally.",vol3
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,2.3,Difficulty-aware Debiased Weighting (DiffDW),"After analyzing the proposed DistDW, we found that some classes with many samples present significant learning difficulties. For instance, despite having the second highest number of voxels, the stomach class has a much lower Dice score than the aorta class, which has only 20% of the voxels of the stomach (Fig.  where λ k denotes the Dice score of k th class in t th iteration and = λ k,t -λ k,t-1 . du k,t and dl k,t denote classes not learned and learned after the t th iteration. I(•) is the indicator function. τ is the number accumulation iterations and set to 50 empirically. Then, we define the difficulty of k th class after t th iteration as , where is a smoothing item with minimal value. The classes learned faster have smaller d k,t , the corresponding weights in the loss function will be smaller to slow down the learn speed. After several iterations, the training process will be stable, and the difficulties of all classes defined above will be similar. Thus, we also accumulate 1λ k,t for τ iterations to obtain the reversed Dice weight w λ k,t and weight d k,t . In this case, classes with lower Dice scores will have larger weights in the loss function, which forces the model to pay more attention to these classes. The overall difficulty-aware weight of k th class is defined as: α is empirically set to 1  5 in the experiments to alleviate outliers. The difficulty-aware weights for all classes are",vol3
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,3.0,Experiments,Dataset and Implementation Details. We introduce two new benchmarks on the Synapse  Comparison with State-of-the-Art Methods. We compare our method with several state-of-the-art semi-supervised segmentation methods ,vol3
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,4.0,Conclusion,"This work proposes a novel Dual-debiased Heterogeneous Co-training framework for class-imbalanced semi-supervised segmentation. We are the first to state the homogeneity issue of CPS and solve it intuitively in a heterogeneous way. To achieve it, we propose two diverse and accurate weighting strategies: DistDW for eliminating the data bias of majority classes and DiffDW for eliminating the learning bias of well-performed classes. By combining the complementary properties of DistDW and DiffDW, the overall framework can learn both the minority classes and the difficult classes well in a balanced way. Extensive experiments show that the proposed framework brings significant improvements over the baseline and outperforms previous SSL methods considerably.",vol3
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 56.,vol3
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,1.0,Introduction,"Data-driven deep networks maintain the great potential to achieve superior performance under the large-scale medical data  A major challenge to FL in real-world scenarios is domain shift, which refers to the difference of marginal data distributions across centers and induces significant performance degradation  In this paper, we focus on generalized and personalized federated learning (GPFL), which considers both generalization and personalization to holistically combat the domain shift. We notice that one recent work IOP-FL  We seek a more effective and efficient solution to GPFL in this work. Specifically, our intuition is based on the following conjecture: a more generalizable global model can facilitate the local models to better adapt to the corresponding local distribution, and better adapted local models can then provide positive feedbacks to the global model with improved gradients. Based on the above intuition, we propose a novel method named GRAdient CorrEction (GRACE) that can achieve both generalization and personalization during training by enhancing the model consistency on both the client side and the server side. By analyzing the federated training stage in Fig.  we estimate the gradient consistency by computing the cosine similarity among the gradients from clients and re-weight the aggregation weights to mitigate the negative effect of domain shifts on the global model update. Through these two components, GRACE preserves the generalizability in global model as much as possible when personalizing it on local distributions. Comprehensive experiments are conducted on two medical FL benchmarks, which show that GRACE outperforms state-of-the-art methods in terms of both generalization and personalization. We also perform insightful analysis to validate the rationale of our algorithm design. N and we denote the weight vector as p = [p 1 , . . . , p M ] ∈ R M . This method implicitly assumes that all clients share the same data distribution, thus failing to adapt to domain shift scenarios.",vol3
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,2.0,Method,"To solve the GPFL problem, we propose a GRAdient CorrEction (GRACE) method for both local client training and server aggregation during the federated training stage. Unlike IPO-FL ",vol3
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,2.2,Local Training Phase: Feature Alignment & Personalization,"We calibrate the gradient during local training via a feature alignment constraint by meta-learning, which preserves the generalizable feature while adapting to the local distributions. We conduct the client-side gradient correction in two steps. Meta-Train: We denote θ t,k m as the personalized model parameter at the local update step k of client m in round t and η as the local learning rate. The first step is a personalization step by the task loss: where (x tr , y tr ) ∈ D m is the sampled data and θ t,k m is the updated parameter that will be used in the second step. Meta-Update: After optimizing the local task objective, we need a metaupdate to virtually evaluate the updated parameters θ t,k m on the held-out metatest data (x te , y te ) ∈ D m with a meta-objective L meta m . We add the feature alignment regularizer into the loss function of meta-update: where h(•; φ) is the feature extractor part of model f (•; θ) and φ is the corresponding parameter, and β is the weight for alignment loss which has a default value of 1.0. Here, we apply three widely-used alignment losses to minimize the discrepancy in the feature space: CORAL ",vol3
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,2.3,Aggregation Phase: Consistency-Enhanced Re-weighting,"We introduce a novel aggregation method on the server side that corrects the global gradient by enhancing the consistency of the gradients received from training clients. We measure the consistency of two gradient vectors by their cosine similarity and use the average cosine similarity among all uploaded gradients as an indicator of gradient quality on generalization.  The corrected global gradient is: ""•"" means element multiplication of two vectors and ""Norm"" means to normalize the new weight vector p with M m=1 p m = 1, p m ∈ (0, 1). Then the updated global model for round t + 1 will be θ t+1 g = θ t g -η g Δθ t g , where η g is the global learning rate with default value 1. Theoretical Analysis: We prove that the re-weighting method in Eq.(3) will enhance the consistency of the global gradient based on the FedAvg. First, we define the averaged cosine similarity of Δθ t m as c t m , where c t m = 1 M M n=1 σ t mn . Then, the consistency degree of the global gradient in FedAvg is M m=1 p m c t m . Thus, the consistency degree after applying our GRACE method has Note that, we can easily prove the inequality in Eq. ( ",vol3
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,3.1,Dataset and Experimental Setting,We evaluate our approach on two open source federated medical benchmarks: Fed-ISIC2019 and Fed-Prostate. The former is a dermoscopy image classification task ,vol3
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,3.2,Comparison with SOTA Methods,"We conduct the leave-one-client-out experiment for both benchmarks. In each experiment, one client is selected as the unseen client and the model is trained on the remaining clients. The average performance on all internal clients' test set is the in-domain personalization results, while the unseen client's performance is the out-of-domain generalization results. The final results of each method are the average of all leave-one-domain-out splits, and all results are over three independent runs. (Please see the details of experimental setup in open-source code.) Performance of In-Domain Personalization. For a fair comparison, the baseline method FedAvg ",vol3
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,3.3,Further Analysis,"Ablation Study on Different Parts of Our Method. The detailed ablation studies are shown in Table  In Table  Visualization of the Feature Alignment Loss in Eq. (2). The A-distance measurement is used to evaluate the dissimilarity between the local and global models, which is suggested to measure the cross-domain discrepancy in the domain adaptation theory  Loss Curves Comparison of FedAvg and Our Method. Fig. ",vol3
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,4.0,Conclusion,"We introduce GPFL for multi-center distributed medical data with domain shift problems, which aims to achieve both generalization for unseen clients and personalization for internal clients. Existing approaches only focus on either generalization (FedDG) or personalization (PFL). We argue that a more generalizable global model can facilitate the local models to adapt to the clients' distribution, and the better-adapted local models can contribute higher quality gradients to the global model. Thus, we propose a new method GRAdient CorrEction (GRACE), which corrects the model gradient at both the client and server sides during training to enhance both the local personalization and the global generalization. The experimental results on two medical benchmarks show that GRACE can enhance both local adaptation and global generalization and outperform existing SOTA methods in generalization and personalization.",vol3
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_2. The results are summarized in Table ,vol3
Deployment of Image Analysis Algorithms Under Prevalence Shifts,1.0,Introduction,"Model Re-calibration: After a prevalence shift models need to be recalibrated. This has important implications on the decisions made based on predicted class scores (see next point). Note in this context that deep neural networks tend not to be calibrated after training in the first place  Performance Assessment: Class frequencies observed in one test set are in general not representative of those encountered in practice. This implies that the scores for widely used prevalence-dependent metrics, such as Accuracy, F1 Score, and Matthews Correlation Coefficient (MCC), would substantially differ when assessed under the prevalence shift towards clinical practice  This importance, however, is not reflected in common image analysis practice. Through a literature analysis, we found that out of a total of 53 research works published between 01/2020 and beginning of 03/2023 that used any of the data included in our study, only one explicitly mentioned re-calibration. Regarding the most frequently implemented decision rules, roughly three quarters of publications did not report any strategy, which we strongly assume to imply use of the default argmax operator. Moreover, both our analysis and previous work show Accuracy and F1 Score to be among the most frequently used metrics for assessing classification performance in comparative medical image analysis  Striving to bridge the translational gap in AI-based medical imaging research caused by prevalence shifts, our work provides two main contributions: First, we demonstrate the potential consequences of ignoring prevalence shifts on a diverse set of medical classification tasks. Second, we assemble a comprehensive workflow for image classification, which is robust to prevalence shifts. As a key advantage, our proposal requires only an estimate of the expected prevalences rather than annotated deployment data and can be applied to any given black box model.",vol3
Deployment of Image Analysis Algorithms Under Prevalence Shifts,2.1,Workflow for Prevalence-Aware Image Classification,"Our workflow combines existing components of validation in a novel manner. As illustrated in Fig.  Re-calibration: We refer to the output of a model ϕ : X → R C before applying the softmax activation as ϕ(x). It can be re-calibrated by applying a transformation f . Taking the softmax of ϕ(x) (no re-calibration) or of f (ϕ(x)), we obtain predicted class scores s x . The probably most popular re-calibration approach is referred to as ""temperature scaling""  Decision Rule: A decision rule d is a deterministic algorithm that maps predicted class scores s x to a final prediction d(s x ) ∈ Y . The most widely used decision rule is the argmax operator, although various alternatives exist  To overcome problems caused by prevalence shifts, we propose the following workflow (Fig.  Step 1: Estimate the deployment prevalences: The first step is to estimate the prevalences in the deployment data D dep , e.g., based on medical records, epidemiological research, or a data-driven approach  Step 2: Perform prevalence-aware re-calibration: Given a shift of prevalences between the calibration and deployment dataset (P D cal = P D dep ), we can assume the likelihoods P (x|y = k) to stay identical for an anticausal problem (note that we are ignoring manifestation and acquisition shifts during deployment  Step 3: Configure validation metric with deployment prevalences: Prevalence-dependent metrics, such as Accuracy, MCC, or the F1 Score, are widely used in image analysis due to their many advantages  Step 4: Set prevalence-aware decision rule: Most counting metrics  Step 5: External validation: The proposed steps for prevalence-aware image classification have strong theoretical guarantees, but additional validation on the actual data of the new environment is indispensable for monitoring ",vol3
Deployment of Image Analysis Algorithms Under Prevalence Shifts,2.2,Experimental Design,"The purpose of our experiments was twofold: (1) to quantify the effect of ignoring prevalence shifts when validating and deploying models and (2) to show the value of the proposed workflow. The code for our experiments is available at https:// github.com/IMSY-DKFZ/prevalence-shifts. Medical Image Classification Tasks. To gather a wide range of image classification tasks for our study, we identified medical image analysis tasks that are publicly available and provide at least 1000 samples. This resulted in 30 tasks covering the modalities laparoscopy  Experiments. For all experiments, the same neural network models served as the basis. To mimic a prevalence shift, we sub-sampled datasets D dep (r) from the deployment test sets D dep according to IRs r ∈ ",vol3
Deployment of Image Analysis Algorithms Under Prevalence Shifts,3.0,Results,"Effects of Prevalence Shifts on Model Calibration. In general, the calibration error increases with an increasing discrepancy between the class prevalences in the development and the deployment setting (Fig.  Effects of Prevalence Shifts on the Decision Rule. Figure ",vol3
Deployment of Image Analysis Algorithms Under Prevalence Shifts,,top).,"Importantly, decision rules optimized on a development dataset do not generalize to unseen data under prevalence shifts (Fig. ",vol3
Deployment of Image Analysis Algorithms Under Prevalence Shifts,,Effects of Prevalence Shifts on the Generalizability of Validation,Results. As shown in Fig. ,vol3
Deployment of Image Analysis Algorithms Under Prevalence Shifts,4.0,Discussion,"Important findings, some of which are experimental confirmations of theory, are: 1. Prevalence shifts lead to miscalibration. A weight-adjusted affine recalibration based on estimated deployment prevalences compensates for this effect. 2. Argmax should not be used indiscriminately as a decision rule. For the metric EC and specializations thereof (e.g., Accuracy), optimal decision rules may be derived from theory, provided that the predicted class scores are calibrated. This derived rule may coincide with argmax, but for other common metrics (F1 Score, MCC) argmax does not lead to optimal results. 3. An optimal decision rule, tuned on a development dataset, does not generalize to datasets with different prevalences. Prevalence-aware setting of the decision rule requires data-driven adjustment or selection of a metric with a Bayes theory-driven optimal decision rule. 4. Common prevalence-dependent metrics, such as MCC and F1 Score, do not give robust estimations of performance under prevalence shifts. EC, with adjusted prevalences, can be used in these scenarios. These findings have been confirmed by repeated experiments using multiple random seeds for dataset splitting and model training. Overall, we present strong evidence that the so far uncommon metric EC offers key advantages over established metrics. Due to its strong theoretical foundation and flexibility in configuration it should, from our perspective, evolve to a default metric in image classification. Note in this context that while our study clearly demonstrates the advantages of prevalence-independent metrics, prevalence-dependent metrics can be much better suited to reflect the clinical interest  In conclusion, our results clearly demonstrate that ignoring potential prevalence shifts may lead to suboptimal decisions and poor performance assessment. In contrast to prior work ",vol3
Deployment of Image Analysis Algorithms Under Prevalence Shifts,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_38.,vol3
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,1.0,Introduction,"Cone-Beam Computed Tomography (CBCT) scans are widely used for guidance and verification in the operating room. The clinical value of this imaging modality is however limited by artifacts originating from patient and device motion or metal objects in the X-Ray beam. To compensate these effects, knowledge about the relative acquisition geometry between views can be exploited. This so-called epipolar geometry is widely used in computer vision and can be applied to CBCT imaging due to the similar system geometry  By formulating and enforcing consistency conditions based on this geometrical relationship, motion can be compensated  In segmentation refinement, the principal idea is to incorporate the known acquisition geometry to unify the binary predictions on corresponding detector pixels. This inter-view consistency can be iteratively optimized to reduce falsepositives in angiography data  In this work, we explore the idea of incorporating epipolar geometry into the learning-based segmentation process itself instead of a separate post-processing step. A differentiable image transform operator is embedded into the model architecture, which translates intermediate features across views allowing the model to adjust its predictions to this conditional information. By making this information accessible to neural networks and enabling dual-view joint processing, we expect benefits for projection domain processing tasks such as inpainting, segmentation or regression. As a proof-of-concept, we embed the operator into a segmentation model and evaluate its influence in a simulation study. To summarize, we make the following contributions: -We analytically derive formulations for forward-and backward pass of the view translation operator -We provide an open-source implementation thereof which is compatible with real-world projection matrices and PyTorch framework -As an example of its application, we evaluate the operator in a simulation study to investigate its effect on projection domain segmentation",vol3
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,2.0,Methods,"In the following sections we introduce the geometrical relationships between epipolar views, define a view translation operator, and analytically derive gradients needed for supervised learning. Epipolar Geometry and the Fundamental Matrix. A projection matrix P ∈ R 3×4 encodes the extrinsic device pose and intrinsic viewing parameters of the cone-beam imaging system. These projection matrices are typically available for images acquired with CBCT-capable C-Arm systems. Mathematically, this non-linear projective transform maps a point in volume coordinates to detector coordinates in homogeneous form  where • + denotes the pseudo-inverse, c ∈ P 3+ is the camera center in homogeneous world coordinates, and [•] × constructs the tensor-representation of a cross product. Note, that the camera center can be derived as the kernel of the projection c = ker(P ). Additional details on epipolar geometry can be found in literature  The Epipolar View Translation Operator (EVT). The goal of the proposed operator is to provide a neural network with spatially registered feature information from a second view of known geometry. Consider the dual view setup as shown in Fig.  To capture this geometric relationship and make spatially corresponding information available to the model, an epipolar map Ψ is computed from the input image p. As shown in Eq. 2, each point u in the output map Ψ , is computed as the integral along its epipolar line l = F u .  Gradient Derivation. To embed an operator into a model architecture, the gradient with respect to its inputs and all trainable parameters needs to be computed. As the proposed operator contains no trainable parameters, only the gradient with respect to the input is derived. The forward function for one output pixel Y u = Ψ (u ) can be described through a 2D integral over the image coordinates u where X u denotes the value in the input image p(u, v) at position u. Here, the indicator function δ(•) signals if the coordinate u lies on the epipolar line defined by F and u : After calculation of a loss L, it is backpropagated through the network graph. At the operator, the loss arrives w.r.t. to the predicted consistency map ∂L ∂Y . From this image-shaped loss, the gradient w.r.t. the input image needs to be derived. By marginalisation over the loss image ∂L ∂Y , the contribution of one intensity value X u in the input image can be written as Deriving Eq. 3 w.r.t. X u eliminates the integral and only leaves the indicator function as an implicit form of the epipolar line. With this inserted in Eq. 5, the loss for one pixel in the input image can be expressed as Note, that forward (Eq. 3) and backward formulation (Eq. 6) are similar and can thus be realised by the same operator. Due to the symmetry shown in Fig.  Implementation. The formulations above used to derive the gradient assume a continuous input and output distribution. To implement this operation on discrete images, the integral is replaced with a summation of constant step size of one pixel and bi-linear value interpolation. As the forward and backward functions compute the epipolar line given the current pixel position, slight mismatches in interpolation coefficients might occur. However, well-tested trainable reconstruction operators use similar approximate gradients and are proven to converge regardless ",vol3
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,3.0,Experiments,"As a proof-of-concept application, we assess the operator's influence on the task of projection domain metal segmentation. To reconstruct a CBCT scan, 400 projection images are acquired in a circular trajectory over an angular range of 200 • . To experimentally validate the effects of the proposed image operator, the images are re-sampled into orthogonal view pairs and jointly segmented using a model with an embedded EVT operator. Dual View U-Net with EVT Operator. To jointly segment two projection images, the Siamese architecture shown in Fig.  Compared Models. To investigate the effects of the newly introduced operator, the architecture described above is compared to variants of the U-Net architecture. As a logical baseline, the plain U-Net architecture from  Data. For the purpose of this study, we use a simulated projection image dataset. Analogous to DeepDRR  The training set consists of randomly selected and assembled metal objects. Each of the remaining 20 volumes is equipped with n ∈ {4, 6, 8, 10} randomly positioned (non-overlapping) metal objects creating 80 unique scenes. During simulation, the objects are randomly assigned either iron or titanium as a material which influences the choice of attenuation coefficients. For each scene, 100 projection images are generated whose central ray angles on the circular trajectory are approximately 2 • apart.",vol3
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,3.1,Model Training,"The three models are trained using the Adam optimizer with the dice coefficient as a loss function and a learning rate of 10 -5 . The best model is selected on the validation loss. As a data augmentation strategy, realistic noise of random strength is added to simulate varying dose levels or patient thickness ",vol3
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,4.0,Results,"Quantitative. The per-view averaged segmentation test set statistics are shown in Table  Qualitative. To illustrate the reported quantitative results, the segmentation prediction is compared on two selected projection images in Fig. ",vol3
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,5.0,Discussion and Conclusion,"Building upon previous work on epipolar geometry and the resulting consistency conditions in X-Ray imaging, we propose a novel approach to incorporate this information into a model. Rather than explicitly formulating the conditions and optimizing for them, we propose a feature translation operator that allows the model to capture these geometric relationships implicitly. As a proof-of-concept study, we evaluate the operator on the task of projection domain segmentation. The operator's introduction enhances segmentation performance compared to the two baseline methods, as shown by both qualitative and quantitative results. Primarily we found the information from a second view made available by the operator to improve the segmentation in two ways: (1) Reduction of false positive segmentations (2) Increased sensitivity in strongly attenuated areas. Especially for the segmentation of spinal implants, the model performance on lateral images was improved by epipolar information from an orthogonal anterior-posterior projection. Lateral images are usually harder to segment because of the drastic attenuation gradient as illustrated in Fig.  The simulation study's promising results encourage exploring the epipolar feature transform as a differentiable operator. Future work involves evaluating the presented segmentation application on measured data, analyzing the optimal integration of the operator into a network architecture, and investigating susceptibility to slight geometry calibration inaccuracies. As the U-Net's generalization on real data has been demonstrated, we expect no issues with our method. In conclusion, this work introduces an open-source Data Use Declaration. The work follows appropriate ethical standards in conducting research and writing the manuscript, following all applicable laws and regulations regarding treatment of animals or human subjects, or cadavers of both kind. All data acquisitions were done in consultation with the Institutional Review Board of the University Hospital of Erlangen, Germany.",vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,1.0,Introduction,"Invariance to geometrical transformations has been long sought-after in the field of machine learning  Early G-CNNs were mainly concerned with operating on 2D inputs. With the increase in computing power, G-CNNs were extended to 3D G-CNNs. Volumetric data is prevalent in many medical settings, such as in analyzing protein structures  An important consideration regarding 3D convolutions that operate on volumetric data is overfitting. Due to the dense geometric structure in volumetric data and the high parameter count in 3D convolution kernels, 3D convolutions are highly susceptible to overfitting ",vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,,Contributions.,"In this work, we introduce regular continuous group convolutions equivariant to SE(3), the group of roto-translations. Motivated by the work on separable group convolutions  1. We introduce separable regular SE(3) equivariant group convolutions that generalize to the continuous setting using RBF interpolation and randomly sampling equidistant SO(3) grids. 2. We show the advantages of our approach on volumetric medical image classification tasks over regular CNNs and discrete subgroup equivariant G-CNNs, achieving up to a 16.5% gain in accuracy over regular CNNs. 3. Our approach generalizes to SE(n) and requires no additional hyperparameters beyond setting the kernel and sample resolutions. 4. We publish our SE(3) equivariant group convolutions and codebase for designing custom regular group convolutions as a Python package. Paper Outline. The remainder of this paper is structured as follows. Section 2 provides an overview of current research in group convolutions. Section 3 introduces the group convolution theory and presents our approach to SE(3) equivariant group convolutions. Section 4 presents our experiments and an evaluation of our results. We give our concluding remarks in Sect. 5.",vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,2.0,Literature Overview,"Since the introduction of the group convolutional neural network (G-CNN), research in G-CNNs has grown in popularity due to their improved performance and equivariant properties over regular CNNs. Work on G-CNNs operating on volumetric image data has primarily been focused on the 3D roto-translation group SE(3) ",vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,3.0,Separable SE(n) Equivariant Group Convolutions,"This work introduces separable SE(3) equivariant group convolutions. However, our framework generalizes to SE(n). Hence, we will describe it as such. Section 3.1 presents a brief overview of the regular SE(n) group convolution. Section 3.2 introduces our approach for applying this formulation to the continuous domain.",vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,3.1,Regular Group Convolutions,"The traditional convolution operates on spatial signals, i.e., signals defined on R n . Intuitively, one signal (the kernel) is slid across the other signal. That is, a translation is applied to the kernel. From a group-theoretic perspective, this can be viewed as performing the group action from the translation group. The convolution operator can then be formulated in terms of the group action. By commuting to a group action, the group convolution produces an output signal that is equivariant to the transformation imposed by the corresponding group. The SE(n) Group Convolution Operator. Instead of operating on signals defined on R n , SE(n)-convolutions operate on signals defined on the group SE(n) = R n SO(n). Given an n-dimensional rotation matrix R, and SE(3)signals f and k, the SE(n) group convolution is defined as follows: The Lifting Convolution Operator. Input data is usually not defined on SE(n). Volumetric images are defined on R 3 . Hence, the input signal should be lifted to SE(n). This is achieved via a lifting convolution, which accepts a signal f defined on R n and applies a kernel k defined on SO(n), resulting in an output signal on SE(n). The lifting convolution is defined as follows: The lifting convolution can be seen as a specific case of group convolution where the input is implicitly defined on the identity group element.",vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,3.2,Separable SE(n) Group Convolution,"The group convolution in Eq. 1 can be separated into a convolution over R followed by a convolution over R n by assuming This improves performance and significantly reduces computation time  The Separable SE(n) Kernel. Let i and o denote in the input and output channel indices, respectively. We separate the SE(3) kernel as follows: Here, k SO(n) performs the channel mixing, after which a depth-wise separable spatial convolution is performed. This choice of separation is not unique. The channel mixing could be separated from the SO(n) kernel. However, this has been shown to hurt model performance  Discretizing the Continuous SO(n) Integral. The continuous group integral over SO(n) in Eq. 1 can be discretized by summing over a discrete SO(n) grid. By randomly sampling the grid elements, the continuous group integral can be approximated  Similarly to the authors of ",vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,,Continuous SO(n) Kernel with Radial Basis Function Interpolation.,"The continuous SO(n) kernel is parameterized via a similarly discrete SO(n) uniform grid. Each grid element R i has corresponding learnable parameters k i . We use radial basis function (RBF) interpolation to evaluate sampled grid elements. Given a grid of resolution N , the continuous kernel k SO(n) is evaluated for any R as: Here, a d,ψ (R, R i ) represents the RBF interpolation coefficient of R corresponding to R i obtained using Gaussian RBF ψ and Riemannian distance d. The uniformity constraint on the grid allows us to scale ψ to the grid resolution dynamically. This ensures that the kernel is smooth and makes our approach hyperparameter-free.",vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,4.0,Experiments and Evaluation,"In this section, we present our results and evaluation. ",vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,4.1,Evaluation Methodology,"From here on, we refer to our approach as the SE(3)-CNN. We evaluate the SE(3)-CNNs for different group kernel resolutions. The sample and kernel resolutions are kept equal. We use a regular CNN as our baseline model. We also compare discrete SE(3) subgroup equivariant G-CNNs. K-CNN and T-CNN are equivariant to the 180 and 90 • rotational symmetries, containing 4 and 12 group elements, respectively. All models use the same ResNet  We evaluate the degree of SE(3) equivariance obtained by the SE(3)-CNNs on OrganMNIST3D ",vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,4.2,SE(3) Equivariance Performance,Table ,vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,4.3,Performance on MedMNIST,"The accuracies obtained on FractureMNIST3D, NoduleMNIST3D, AdrenalM-NIST3D, and SynapseMNIST3D are reported in Table ",vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,4.4,Model Generalization,"The scores obtained on both the train set and test sets of SynapseMNIST3D in Figs.  We do observe a higher variance in scores of the SE(3)-CNNs, which we attribute to the random nature of the convolution kernels.",vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,4.5,Future Work,"With an increase in the sample resolution, a better approximation to SE(3) equivariance is achieved. However, we observe that this does not necessarily improve model performance, e.g., in the case of isotropic features. This could indicate that the equivariance constraint is too strict. We could extend our approach to learn partial equivariance. Rather than sampling on the entire SO(3) manifold, each group convolution layer could learn sampling in specific regions. This suggests a compelling extension of our work, as learning partial invariance has shown to increase model performance ",vol3
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,5.0,Conclusion,"This work proposed an SE(3) equivariant separable G-CNN. Equivariance is achieved by sampling uniform kernels on a continuous function over SO(3) using RBF interpolation. Our approach requires no additional hyper-parameters compared to CNNs. Hence, our SE(3) equivariant layers can replace regular convolution layers. Our approach consistently outperforms CNNs and discrete subgroup equivariant G-CNNs on challenging medical image classification tasks. We showed that 3D CNNs and discrete subgroup equivariant G-CNNs suffer from overfitting. We showed significantly improved generalization capabilities of our approach. In conclusion, we have demonstrated the advantages of equivariant methods in medical image analysis that naturally deal with rotation symmetries. The simplicity of our approach increases the accessibility of these methods, making them available to a broader audience.",vol3
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,1.0,Introduction,"Tau accumulation in the form of neurofibrillary tangles in the brain is an important pathology hallmark in Alzheimer's disease (AD)  The human brain is a complex system that is biologically wired by white matter fibers  To answer these fundamental questions, we put our spotlight on the spreading flows of tau aggregates. As shown in Fig.  We have applied our TauFlowNet on the longitudinal neuroimaging data in ADNI dataset. We compare the prediction accuracy of future tau accumulation with the counterpart methods and explore the propagation mechanism of tau aggregates as the disease progresses, where our physics-informed deep model yields more accurate and interpretable results. The promising results demonstrate great potential in discovering novel neurobiological mechanisms of AD through the lens of machine learning.",vol3
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,2.0,Methods,"In the following, we first elucidate the relationship between GNN, E-L equation, and Lagrangian mechanics, which sets the stage for the method formulation and deep model design of our TauFlowNet in Sect. 2.1. Then, we propose the TV-based graph regularization for GAN-based deep learning in Sect. 2.2, which allows us to characterize the spreading flow of tau aggregates from longitudinal tau-PET scans. Suppose the brain network is represented by a graph G = (V , W ) with N nodes (brain regions) V = {v i |i = 1, . . . , N } and the adjacency matrix W = w ij N i,j=1 ∈ R N ×N describing connectivity strength between any two nodes. For each node v i , we have a graph embedding vector indicates the feature difference between v i and v j weighed by the connectivity strength w ij . Thus, the graph diffusion process  2 dx on top of the graph topology. The GNN depth is blamed for over-smoothing ",vol3
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,2.1,Problem Formulation for Discovering Spreading Flow of Tau Propagation,"Neuroscience Assumption. Our brain's efficient information exchange facilitates the rapid spread of toxic tau proteins throughout the brain. To understand this spread, it's essential to measure the intrinsic flow information (such as flux and bandwidth) of tau aggregates in the complex brain network. Problem Formulation from the Perspective of Machine Learning. The overarching goal is to estimate the time-dependent flow field , where f ij (t) stands for the directed flow from the region v i to v j . As the toy example shown in Fig.  , where u i (t) is output of a nonlinear process φ reacting to the tau accumulation x i at the underlying region v i , i.e., u i = φ(x i ). The potential energy field drives the flow of tau aggregates in the brain, similar to the gravity field driving water flow. Thereby the spreading of tau is defined by the gradient of potential energy between connected regions: Thus, the fundamental insight of our model is that the spreading flow f ij (t) is formulated as an ""energy transport"" process of the tau potential energy field. Taking together, the output of our model is a mechanistic equation M (•) of the dynamic system that can predict the future flow based on the history flow sequences, i.e., Transport Equation for Tau Propagation in the Brain. A general continuity transport equation  where q is the flux of the potential energy u (conserved quantity). The intuition of Eq. 2 is that the change of energy density (measured by regional tau SUVR x) leads to the energy transport throughout the brain (measured by the flux of PEF). As flux is often defined as the rate of flow, we further define the energy flow as q ij = α • f ij , where α is a learnable parameter characterizing the contribution of the tau flow f ij to the potential energy flux q ij . By plugging u t = φ(x t ) and q ij = α • f ij into Eq. 2, the energy transport process of tau spreading flow f can be described as: Note, φ and α are trainable parameters that can be optimized through the supervised learning schema described below.",vol3
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,2.2,TauFlowNet: An Explainable Deep Model Principled with TV-Based Lagrangian Mechanics,"To solve the flow field f in Eq. 3, the naïve deep model is a two-step approach (shown in the left red panel of Fig.  (1) Estimate the PEF u by fixing the flow f . By letting f = ∇ G u (in Eq. 1), the solution of u follows a reaction process u = φ(x) and a graph diffusion process ∂u ∂t = -α -1 div ∇ G u = -α -1 u, where = div(∇ G ) is the graph Laplacian operator. The parameters of φ and α can be learned using a multi-layer perceptron (MLP) and a graph convolution layer (GCN), respectively. Thus, the input is the observed tau SUVR x t and the loss function aims to minimize the prediction error from x t to x t+1 . (2) Calculate spreading flow f . Given u, it is straightforward to compute each flow f ij (t) by Eq. 1. In Sect. 2.1, we have pointed out that the GNN architecture is equivalent to the graph diffusion component in Eq. 3. Since the PDE of the graph diffusion process ∂u ∂t =u is essentially the Euler-Lagrange (E-L) equation of the quadratic functional J (u) = min u ∇ G u 2 du, the major issue is the ""over-smoothness"" in u that might result in vanishing flows (i.e., f → 0). To address the over-smoothing issue, we propose to replace the quadratic Laplacian regularizer with total variation, i.e., J TV (u) = min u ∇ G u du, which has been successfully applied in image denoising  After that, we boil down the minimization of J TV (u) into a dual min-max functional as where we maximize f such that J TV (u, f ) is close enough to J TV (u). In this regard, the E-L equation from the Gâteaux variations leads to two coupled PDEs: The alternative solution for Eq. 4 is that we minimize PEF u through the Lagrangian mechanics defined in the transport equation ∂u ∂t = -φ α -1 div(f ) where the system dynamics is predominated by the maximum flow field f . Since the accurate estimation of flow field f (t) and PEF u(t) is supposed to predict the future tau accumulation x(t +1) by , we can further tailor the min-max optimization for Eq. 4 into a supervised learning scenario as the TauNetFlow described next.",vol3
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,,TauFlowNet: A GAN Network Architecture of TV-Based Transport,"Equation. Here, we present an explainable deep model to uncover the spreading flow of tau aggregates f from the longitudinal tau-PET scans. Our deep model is trained to learn the system dynamics (in Eq. 4), which can predict future tau accumulations. The overall network architecture of TauFlowNet is shown in Fig. ",vol3
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,3.0,Experiments,"In this section, we evaluated the performance of the proposed TauFlowNet for uncovering the latent flow of tau spreading on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset (https://adni.loni.usc.edu/). In total, 163 subjects with longitudinal tau-PET scans are used for training and testing the deep model. In addition, each subject has T1-weighted MRI and diffusion-weighted imaging (DWI) scan, from which we construct the structural connectome. Destrieux atlas ",vol3
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,3.1,Evaluate the Prediction Accuracy of Future Tau Accumulation,"We first evaluate the prediction performance between the ground truth and the estimated SUVR values, where we use the mean absolute error (MAE) to quantify the prediction accuracy. The statistics of MAE by our TauFlowNet, GCN, and DNN are shown in the first column (with shade) of Table  As part of the ablation study, we implement the two-step approach (the beginning of Sect. 2.2), where we train the model (MLP + GCN) shown in the left panel of Fig. ",vol3
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,3.2,Examine Spatiotemporal Patterns of the Spreading Flow of Tau Aggregates,We examine the pattern of spreading flows on an individual basis (Fig. ,vol3
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,4.0,Conclusion,"In this paper, we propose a physics-informed deep neural network (TauFlowNet) by combining the power of dynamic systems (with well-studied mechanisms) and machine learning (fine-tuning the best model) to discover the novel propagation mechanism of tau spreading flow from the longitudinal tau-PET scans. We have evaluated our TauFlowNet on ADNI dataset in forecasting tau accumulation and elucidating the spatiotemporal patterns of tau propagation in the different stages of cognitive decline. Our physics-informed deep model outperforms existing state-of-the-art methods in terms of prediction accuracy and model explainability. Since the region-to-region spreading flow provides rich information for understanding the tau propagation mechanism, our learning-based method has great applicability in current AD studies.",vol3
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,1.0,Introduction,"Lowering radiation dose is desired in the computed tomography (CT) examination. Various strategies, i.e., lowering incident photons directly (low-mAs), reducing sampling views (sparse-view) and reducing sampling angles (limitedview) can be used for low-dose CT imaging. However, the reconstructed images under these conditions would suffer from severe quality degradation. A number of reconstruction algorithms have been proposed to improve low-dose CT image quality  However, most DL-based CT reconstruction methods are condition-specific, i.e., dose-specific, and geometry-specific. In the dose-specific case, these methods are constructed on the dataset at one specific dose level, which might fail to obtain promising result at other dose levels. Centralized learning via collecting data at different dose levels is an alternative way, but it is difficult to collect sufficient data efficiently. In the geometry-specific case, the DL-based methods, especially the cross-domain learning methods, usually reconstruct the final image from the measured sinogram data with a specific imaging geometry that takes the geometry parameters into account during reconstruction. However, the geometry parameters in the scanner are vendor-specific and different from each other. Then the DL-based methods trained on data from one geometry would fail to be transferred to those from the other geometry due to the different characteristics distributions and big data heterogeneity among different geometries. Xia et al. constructed a framework for modulating deep learning models based on CT imaging geometry parameters to improve the reconstruction performance of the DL models under multiple CT imaging geometries  Different from the centralized learning, federated learning has potential to the train model on decentralized data without the need to centralized or share data, which provides significant benefits over centralized learning methods  Inspired by the previous work ",vol3
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,2.1,iRadonMAP,"As a cross-domain learning framework for Radon inversion in CT reconstruction, iRadonMAP consists of a sinogram domain sub-network, an image domain subnetwork, and a learnable back-projection layer, which can be written as follows  where μ is the final image, p is the sinogram data, F s (•; θ s ) and F i (•; θ i ) denote the sinogram domain sub-network (i.e., SinoNet) and the image domain subnetwork (i.e., ImageNet) of iRadonMAP, ) is learnable back projection layer, the details of F R -1 (•; θ R -1 ) are available in  Although iRadonMAP can obtain promising reconstruction result, the generalization is still an area that is poorly exploited. When iRadonMAP is trained on CT data with a particular condition (i.e., specific dose level and imaging geometry) and is inferred on CT data with a different condition, it would become unstable as a result of data heterogeneity among different conditions. Federated learning framework is an alternative strategy to address this issue.",vol3
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,2.2,Proposed FedCG Method,"In this study, inspired by the previous work  Central Data Guidance Training. Different from the existing federated learning framework, the central server has labeled data that is closer to real world, as shown in Fig.  where ω 0 is the weight of central model, ω k = n k n , where n k denotes the number of local iterations. Condition Generalization in FL. To fully consider the unique distribution in each client, inspired by Xia et al.  where ρ represents the condition parameter in each client, g 1 , • • • , g n are normalized imaging geometric parameters. C is a parameter that represents protocol parameter (i.e., dose level, sparse views, and limited angles). In the kth local client, ρ k is fed into the local model along with the input data. And in the central server, all condition parameters are fed into the central model. As shown in Fig.  h 1 , h 2 are MLPs with shared parameters. f is the feature map of network layer, f is the modulated feature map. Then, the total loss function of the proposed FedCG can be written as follows: where μ * 0 is the noise-free image in the central server, μ * k is the noise-free image in the kth client, μ 0 is the output image of central ImageNet, μ k is the output image of iRadonMAP in the kth client, p * k is the noise-free sinogram in the kth client, p k is the intermediate output of SinoNet in the kth client, λ is the parameter that controls the singoram loss function in the local client.",vol3
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,3.1,Dataset,"The experiments are carried out on three publicly available datasets (120, 000 CT images)  In the Condition #1, the three publicly available datasets are collected for the central server, Data #1, Data #2, and Data #3 are selected for three local clients (Client #1, Client #2 and Client #3), respectively. We obtained the corresponding low-dose sinogram data at different dose levels from the normal-dose CT images based on the previous study, respectively ",vol3
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,3.2,Implementation Details,FedCG is constructed by Pytorch toolbox ,vol3
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,4.0,Result,"In this work, five algorithms are selected for comparison. The classical FBP algorithm and the iRadonMAPs trained on condition-specific dataset, FedAvg ",vol3
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,4.1,Reuslt on Condition #1,Figure ,vol3
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,4.2,Result on Condition #2,Figure ,vol3
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,4.3,Ablation Experiments,"ω 0 plays a key role in the proposed FedCG reconstruction performance, then we conduct ablation experiments with different ω 0 on settings Condition #1. Figure ",vol3
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,5.0,Conclusion,"In this work, we propose a condition generalization method under a federated learning framework to reconstruct CT images on different conditions. Experiments on different dose levels with different geometries, and different sampling shcemes with different geometries show that the proposed FedCG achieves improved reconstruction performance compared with the other competing methods at all the cases qualitatively and quantitatively.",vol3
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_5.,vol3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,1.0,Introduction,"Primary liver cancer is one of the most common and deadly cancer diseases in the world, and liver resection is a highly effective treatment  However, automatic and accurate Couinaud segmentation from CT images is a challenging task. Since it is defined based on the anatomical structure of live vessels, even no intensity contrast (Fig.  In this paper, to tackle the aforementioned challenges, we propose a pointvoxel fusion framework that represents the liver CT in continuous points to better learn the spatial structure, while performing the convolutions in voxels to obtain the complementary semantic information of the Couinaud segments. Specifically, the liver mask and vessel attention maps are first extracted from the CT images, which allows us to randomly sample points embedded with vessel structure prior in the liver space and voxelize them into a voxel grid. Subsequently, points and voxels pass through two branches to extract features. The point-based branch extracts the fine-grained feature of independent points and explores spatial topological relations. The voxel-based branch is composed of a series of convolutions to learn semantic features, followed by de-voxelization to convert them back to points. Through the operation of voxelization and devoxelization at different resolutions, the features extracted by these two branches can achieve multi-scale fusion on point-based representation, and finally output the Couinaud segment category of each point. Extensive experiments on two publicly available datasets named 3Dircadb ",vol3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,2.0,Method,The overview of our framework to segment Couinaud segments from CT images is shown in Fig. ,vol3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,2.1,Liver Mask and Vessel Attention Map Generation,"Liver segmentation is a fundamental step in Couinaud segmentation task. Considering that the liver is large and easy to identify in the abdominal organs, we extracted the liver mask through a trained 3D UNet ",vol3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,2.2,Couinaud Segmentation,"Based on the above work, we first use the M and the L to sample get point data, which can convert into a voxel grid through re-voxelization. The converted voxel grid embeds the vessel prior and also dilutes the liver parenchyma information. Inspired by ",vol3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,,Continuous Spatial Point Sampling Based on the Vessel Attention,"Map. In order to obtain the topological relationship between Couinaud segments, a direct strategy is to sample the coordinate point data with 3D spatial information from liver CT and perform point-wise classification. Hence, we first convert the image coordinate points I = i 1 , i 2 , ..., i t , i t ∈ R 3 in liver CT into the world coordinate points P = p 1 , p 2 , ..., p t , p t ∈ R 3 : where Spacing represents the voxel spacing in the CT images, Direction represents the direction of the scan, and Origin represents the world coordinates of the image origin. Based on equation(1), we obtain the world coordinate p t = (x t , y t , z t ) corresponding to each point i t in the liver space. However, directly feeding the transformed point data as input into the point-based branch undoubtedly ignores the vessel structure, which is crucial for Couinaud segmentation.  where R denotes the rounding integer function. Based on this, we achieve arbitrary resolution sampling in the continuous space covered by the M . ",vol3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,,Re,"where r denotes the voxel resolution, I [•] is the binary indicator of whether the coordinate pt belongs to the voxel grid (u, v, w), f t,c denotes the cth channel feature corresponding to pt , and N u,v,w is the number of points that fall in that voxel grid. Note that the re-voxelization in the model is used three times (as shown in Fig.  Multi-scale Point-Voxel Fusion Network. Intuitively, due to the image intensity between different Couinaud segments being similar, the voxel-based CNN model is difficult to achieve good segmentation performance. We propose a multi-scale point-voxel fusion network for accurate Couinaud segmentation, take advantage of the topological relationship of coordinate points in 3D space, and leverage the semantic information of voxel grids. As shown in Fig.  where the superscript 1 of (p t , f 1 t ) indicates that the fused point data and corresponding features f 1 t are obtained after the first round of point-voxel operation. Then, the point data (p t , f 1 t ) is voxelized again and extracted point features and voxel features through two branches. Note that the resolution of the voxel grid in this round is reduced to half of the previous round. After three rounds of pointvoxel operations, we concatenate the original point feature f t and the features f 1 t , f 2 t , f 3 t with multiple scales, then send them into a point-wise decoder D, parameterized by a fully connected network, to predict the corresponding Couinaud segment category: where {0, 1, ..., 7} denotes the Couinaud segmentation category predicted by our model for the point p t . We employ the BCE loss and the Dice loss to supervise the learning process. More method details are shown in the supplementary materials.",vol3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,3.1,Datasets and Evaluation Metrics,"We evaluated the proposed framework on two publicly available datasets, 3Dircadb  We have used three widely used metrics, i.e., accuracy (ACC, in %), Dice similarity metric (Dice, in %), and average surface distance (ASD, in mm) to evaluate the performance of the Couinaud segmentation.",vol3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,3.2,Implementation Details,"The proposed framework was implemented on an RTX8000 GPU using PyTorch. Based on the liver mask has been extracted, we train a 3D UNet ",vol3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,3.3,Comparison with State-of-the-Art Methods,"We compare our framework with several SOTA approaches, including voxelbased 3D UNet  Quantitative Comparison. Table  Qualitative Comparison. To further evaluate the effectiveness of our method, we also provide qualitative results, as shown in Fig.  Besides, compared with the 3D view, it is obvious that the voxel-based CNN methods are easy to pay attention to the local area and produce a large area of error segmentation, so the reconstructed surface is uneven. The point-based method obtains smooth 3D visualization results, but it is more likely to cause  segmentation blur in boundary areas with high uncertainty. Our method combines the advantages of point-based and voxel-based methods, and remedies their respective defects, resulting in smooth and accurate Couinaud segmentation.",vol3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,3.4,Ablation Study,"To further study the effectiveness of our proposed framework, we compared two ablation experiments: 1) random sampling of T points in the liver space, with-out considering the guidance of vascular structure, and 2) considering only the voxel-based branch, where the Couinaud segments mask is output by a CNN decoder. Figure ",vol3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,4.0,Conclusion,"We propose a multi-scale point-voxel fusion framework for accurate Couinaud segmentation that takes advantage of the topological relationship of coordinate points in 3D space, and leverages the semantic information of voxel grids. Besides, the point sampling strategy embedded with vascular prior increases the access of our method to important regions, and also improves the segmentation accuracy and robustness in uncertain boundaries. Experimental results demonstrate the effectiveness of our proposed method against other cutting-edge methods, showing its potential to be applied in the preoperative application of liver surgery.",vol3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 45.,vol3
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,1.0,Introduction,"Segmentation of the pulmonary vessels is the foundation for the clinical diagnosis of pulmonary vascular diseases such as pulmonary embolism (PE), pulmonary hypertension (PH) and lung cancer  In the literature, several conventional methods  To summarize, there exist several challenges for pulmonary vessel segmentation in non-contrast CT images:  To address the above challenges, we propose a H ierarchical E nhancement N etwork (HENet) for pulmonary vessel segmentation in non-contrast CT images by enhancing the representation of vessels at both image-and feature-level. For the input CT images, we propose an Auto Contrast Enhancement (ACE) module to automatically adjust the range of HU values in different areas of CT images. It mimics the radiologist in setting the window level (WL) and window width (WW) to better enhance vessels from surrounding voxels, as shown in Fig. ",vol3
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,2.0,Method,The overview of the proposed method is illustrated in Fig. ,vol3
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,2.1,Auto Contrast Enhancement,"In non-contrast CT images, the contrast between pulmonary vessels and the surrounding voxels is pretty low. Also, the HU values of vessels in different regions vary significantly as ranging from -850 HU to 100 HU. Normally, radiologists have to manually set the suitable window level (WL) and window width (WW) for different regions in images to enhance vessels according to the HU value range of surrounding voxels, just as different settings to better visualize the extrapulmonary and intrapulmonary vessels (Fig.  The ACE module leverages convolution operations to generate dynamic WL and WW for the input CT images according to the HU values covered by the kernel. Here we set the kernel size as 15 × 15 × 15. First, we perform min-max normalization to linearly transform the HU values of the original image X to the range (-1, 1). Then, it passes through a convolution layer to be downsampled into half-size of the original shape, which is utilized to derive the following shift map and scale map. Here, the learned shift map and scale map act as the window level and window width settings of the ""width/level"" scaling in CT images. We let values in the shift map be the WL, so the tanh activation function is used to limit them within (-1, 1). The values in the scale map denote the half of WW, and we perform the sigmoid activation function to get the range (0, 1). It matches the requirement of the positive integer for WW. After that, the shift map and scale map will be upsampled by the nearest neighbor interpolation into the original size of the input X. This operation can generate identical shift and scale values with each 2 × 2 × 2 window, avoiding sharp contrast changes in the neighboring voxels. The upsampled shift map and scale map are denoted as M shif t and M scale , respectively, and then the contrast enhancement image X ACE can be generated through: It can be observed that the intensity values of input X are re-centered and re-scaled by M shif t and M scale (Fig. ",vol3
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,2.2,Cross-Scale Non-local Block,"There are studies  Inspired by  Next, these three embeddings are reshaped to where N represents the total count of the spatial locations, i.e., N = D × H × W and S is equivalent to the concatenated output size after the spatial pyramid pooling, i.e., setting S = 648. The similarity matrix between Q and K p is obtained through matrix multiplication and normalized by softmax function to get a unified similarity matrix. The attention output is acquired by: where the output O ∈ R N × Ĉ . The final output of ANB is given as: where the final convolution is used as a weighting parameter to adjust the importance of this non-local operation and recover the channel dimension to C h . ANB-P has the same structure as ANB-H, but the inputs F h and F l here are the same, which is the output of ANB-H at the same level. The ANB-P is developed to further enhance the intra-scale connection of the fused features in different regions, which is equivalent to the self-attention mechanism. Note that, O 1 is directly skipped from I 1 . For the first level of CSNB, the input F l of ANB-H is the I 1 , while for the other levels, the input F l is the output of ANB-P of the above level. That is, each level of CSNB can fuse the feature maps from its corresponding level with the fused feature maps from all of the above lower levels. Thereby, the response of multi-scale vessels can be enhanced.",vol3
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,3.0,Experiments and Results,"Dataset and Evaluation Metrics. We use a total of 160 non-contrast CT images with the inplane size of 512 × 512, where the slice number varies from 217 to 622. The axial slices have the same spacing ranging from 0.58 to 0.86 mm, and the slice thickness varies from 0.7 to 1.0 mm. The annotations of pulmonary vessels are semi-automatically segmented in 3D by two radiologists using the 3D Slicer software. This study is approved by the ethical committee of West China Hospital of Sichuan University, China. These cases are randomly split into a training set (120 scans) and a testing set (40 scans). The quantitative results are reported by Dice Similarity Coefficient (Dice), mean Intersection over Union (mIoU), False Positive Rate (FPR), Average Surface Distance (ASD), and Hausdorff Distance (HD). For the significance test, we use the paired t-test. Implementation Details. Our experiments are implemented using Pytorch framework and trained using a single NVIDIA-A100 GPU. We pre-process the data by truncating the HU value to the range of [-900, 900] and then linearly scaling it to [-1, 1]. In the training stage, we randomly crop sub-volumes with the size of 192 × 192 × 64 near the lung field, and then the cropped sub-volumes are augmented by random horizontal and vertical flipping with a probability of 0.5. In the testing phase, we perform the sliding window average prediction with strides of (64, 64, 32) to cover the entire CT images. For a fair comparison, we use the same hyper-parameter settings and Dice similarity coefficient loss across all experiments. In particular, we use the same data augmentation, no post-processing scheme, Adam optimizer with an initial learning rate of 10 -4 , and train for 800 epochs with a batch size of 4. In our experiments, we use a two-step optimization strategy: 1) first, train the ACE module with the basic U-Net; 2) Integrate the trained ACE module and a new CSNB module into the U-Net, and fix the parameters of ACE module when training this network. Ablation Study. We conduct ablation studies to validate the efficacy of the proposed modules in our HENet by combining them with the baseline U-Net  To validate the efficacy of ACE module, we show the qualitative result in Fig. ",vol3
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,4.0,Conclusion,"In this paper, we have proposed a hierarchical enhancement network to enhance the representation of vessels at both image-and feature-level for pulmonary vessel segmentation first time in non-contrast CT images. In the proposed HENet, an Auto Contrast Enhancement module is designed to enhance vessels in different regions of the input CT. And the Cross-Scale Non-local Block is further designed as the information bridge between encoders and decoders, to enhance the ability to capture and integrate vascular features of multiple scales. Experimental results show that our method outperforms the competing methods and demonstrates effectiveness of the proposed ACE module and CSNB. At the same time, it can be observed that the learning of M shif t and M scale is only supervised by the final segmentation loss. One of our future research directions is to develop explicit constraints for the ACE module to better re-center and re-scale the CT images.",vol3
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,,Acknowledgments,. This work was supported in part by ,vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,1.0,Introduction,"Image segmentation is a fundamental task in medical image analysis. One of the key design choices in many segmentation pipelines that are based on neural networks lies in the selection of the loss function. In fact, the choice of loss function goes hand in hand with the metrics chosen to assess the quality of the predicted segmentation  Another mechanism to further improve the predicted segmentation that has gained significant interest in recent years, is the use of soft labels during training. Soft labels can be the result of data augmentation techniques such as label smoothing (LS)  This work investigates how the medical imaging community can combine the use of SDL with soft labels to reach a state of synergy. While the original SDL surrogate was posed as a relaxed form of the Dice score, naively inputting soft labels to SDL is possible (e.g. in open-source segmentation libraries  Motivated by this observation, we first (in Sect. 2) propose two probabilistic extensions of SDL, namely, Dice semimetric losses (DMLs). These losses satisfy the conditions of a semimetric and are fully compatible with soft labels. In a standard setting with hard labels, DMLs are identical to SDL and can safely replace SDL in existing implementations. Secondly (in Sect. 3), we perform extensive experiments on the public QUBIQ, LiTS and KiTS benchmarks to empirically confirm the potential synergy of DMLs with soft labels (e.g. averaging, LS, KD) over hard labels (e.g. majority voting, random selection).",vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,2.0,Methods,We adopt the notation from ,vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,2.1,Existing Extensions,"If we want to optimize the Dice score, hence, minimize the Dice loss Δ Dice = 1 -Dice in a continuous setting, we need to extend Δ Dice with Δ Dice such that it can take any predicted segmentation x ∈ [0, 1] p as input. Hereinafter, when there is no ambiguity, we will use x and x interchangeably. The soft Dice loss (SDL)  The soft Jaccard loss (SJL)  A major limitation of loss functions based on L 1 relaxations, including SDL, SJL, the soft Tversky loss  Loss functions that utilize L 2 relaxations  JMLs are shown to be a metric on [0, 1] p , according to the definition below. Note that reflexivity and positivity jointly imply x = y ⇔ f (x, y) = 0, hence, a loss function that satisfies these conditions will be compatible with soft labels.",vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,2.2,Dice Semimetric Losses,"We focus here on the Dice loss. For the derivation of the Tversky loss and the focal Tversky loss, please refer to our full paper on arXiv. Since Dice",vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,,2-Δ IoU,". There exist several alternatives to define Δ IoU , but not all of them are feasible, e.g., SJL. Generally, it is easy to verify the following proposition: Proposition 1. Δ Dice satisfies reflexivity and positivity iff Δ IoU does. Among the definitions of Δ IoU , Wang and Blaschko  Δ Dice that is defined over integers does not satisfy the triangle inequality  ( Functions that satisfy the relaxed triangle inequality for some fixed scalar ρ and conditions (i)-(iii) of a metric are called semimetrics. Δ Dice is a semimetric on {0, 1} p  Theorem 1. Δ DML,1 and Δ DML,2 are semimetrics on [0, 1] p . The proof can be found in Appendix A. Moreover, DMLs have properties that are similar to JMLs and they are presented as follows: The proofs are similar to those given in ",vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.0,Experiments,"In this section, we provide empirical evidence of the benefits of using soft labels. In particular, using QUBIQ ",vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.1,Datasets,"QUBIQ is a recent challenge held at MICCAI 2020 and 2021, specifically designed to evaluate the inter-rater variability in medical imaging. Following ",vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.2,Implementation Details,"We adopt a variety of backbones including ResNet50/18  We train the models using SGD with an initial learning rate of 0.01, momentum of 0.9, and weight decay of 0.0005. The learning rate is decayed in a poly policy with an exponent of 0.9. The batch size is set to 8 and the number of epochs is 150 for QUBIQ, 60 for both LiTS and KiTS. We leverage a mixture of CE and DMLs weighted by 0.25 and 0.75, respectively. Unless otherwise specified, we use Δ DML,1 by default. In this work, we are mainly interested in how models can benefit from the use of soft labels. The superiority of SDL over CE has been well established in the medical imaging community ",vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.3,Evaluation,"We report both the Dice score and the expected calibration error (ECE)  For all experiments, we conduct 5-fold cross validation, making sure that each case is presented in exactly one validation set, and report the mean values in the aggregated validation set. We perform statistical tests according to the procedure detailed in ",vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.4,Results on QUBIQ,"In Table  In the literature  Generally, models trained with soft labels exhibit improved accuracy and calibration. In particular, averaging annotations with uniform weights obtains the highest BDice, while a weighted average achieves the highest Dice score. It is worth noting that the weighted average significantly outperforms the majority votes in terms of the Dice score which is evaluated based on the majority votes themselves. We hypothesize that this is because soft labels contain extra interrater information, which can ease the network optimization at those ambiguous regions. Overall, we find the weighted average outperforms other methods, with the exception of Brain Tumor T2, where there is a high degree of disagreement among raters. We compare our method with state-of-the-art (SOTA) methods using UNet-ResNet50 in Table ",vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.5,Results on LiTS and KiTS,"Wang and Blaschko  That is, the bias of the estimation is bounded above by the calibration error and this explains why the calibration of the teacher would be important for the student. Inspired by this, we apply a recent kernel density estimator (KDE)  In Table ",vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.6,Ablation Studies,"In Table  We find models trained with SDL can still benefit from soft labels to a certain extent because (i) models are trained with a mixture of CE and SDL, and CE is compatible with soft labels; (ii) although SDL pushes predictions towards vertices, it can still add some regularization effects in a binary segmentation setting. However, SDL is notably outperformed by DMLs. As for DMLs, we find Δ DML,1 is slightly superior to Δ DML,2 and recommend using Δ DML,1 in practice. In Table ",vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,4.0,Future Works,"In this study, our focus is on extending the Dice loss within the realm of medical image segmentation. It may be intriguing to apply DMLs in the context of longtailed classification ",vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,5.0,Conclusion,"In this work, we introduce the Dice semimetrics losses (DMLs), which are identical to the soft Dice loss (SDL) in a standard setting with hard labels, but are fully compatible with soft labels. Our extensive experiments on the public QUBIQ, LiTS and KiTS benchmarks validate that incorporating soft labels leads to higher Dice score and lower calibration error, indicating that these losses can find wide application in diverse medical image segmentation problems. Hence, we suggest to replace the existing implementation of SDL with DMLs.",vol3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_46.,vol3
Fully Bayesian VIB-DeepSSM,2.0,Background,"We denote a set of paired training data as D = {X , Y}. X = {x n } N n=1 is a set of N unsegmented images, where x n ∈ R H×W ×D . Y = {y n } N n=1 is the set of PDMs comprised of M 3D correspondence points, where y n ∈ R 3M . VIB utilizes a stochastic latent encoding Z = {z n } N n=1 , where z n ∈ R L and L 3M . InBayesian modeling, model parameters Θ are obtained by maximizing the likelihood p(y|x, Θ). The predictive distribution is found by marginalizing over Θ, which requires solving for the posterior p(Θ|D). In most cases, p(Θ|D) is not analytically tractable; thus, an approximate posterior q(Θ) is found via variational inference (VI). Bayesian networks maximize the VI evidence lower bound (ELBO) by minimizing: where p(Θ) is the prior on network weights, and β is a weighting parameter. The deep Variational Information Bottleneck (VIB)  The entropy of the p(y|z) distribution (computed via sampling) captures aleatoric uncertainty. The VIB objective has also been derived using an alternative motivation: Bayesian inference via optimizing a PAC style upper bound on the true negative log-likelihood risk ",vol3
Fully Bayesian VIB-DeepSSM,3.1,Bayesian Variational Information Bottleneck,"In fully-Bayesian VIB (BVIB), rather than fitting the model parameters Θ = {φ, θ} via MLE, we use VI to approximate the posterior p(Θ|D). There are now two intractable posteriors p(z|x, φ) and p(Θ|x, y). The first is approximated via q(z|x, φ) as in Eq. 2 and the second is approximated by q(Θ) as in Eq. 1. Minimizing these two KL divergences via a joint ELBO gives the objective (see Appendix A) for derivation details): where Θ ∼ q(Θ) and ẑ ∼ q(z|x, φ). This objective is equivalent to the BVIB objective acquired via applying a PAC-Bound with respect to the conditional expectation of targets and then another with respect to parameters ",vol3
Fully Bayesian VIB-DeepSSM,3.2,Proposed BVIB-DeepSSM Model Variants,"In adapting VIB-DeepSSM to be fully Bayesian, we propose utilizing two approaches that have demonstratively captured epistemic uncertainty without significantly increasing computational and memory costs: concrete dropout  Naive Ensemble (NE) models combine outputs from several networks for improved performance. Networks trained with different initialization converge to different local minima, resulting in test prediction disagreement  Batch Ensemble (BE)  Novel Integration of Dropout and Ensembling: Deep ensembles have historically been considered a non-Bayesian competitor for uncertainty estimation. However, recent work argues that ensembles approximate the predictive distribution more closely than canonical approximate inference procedures (i.e., VI) and are an effective mechanism for approximate Bayesian marginalization ",vol3
Fully Bayesian VIB-DeepSSM,3.3,BVIB-DeepSSM Implementation,We compare the proposed BVIB approaches with the original VIB-DeepSSM formulation ,vol3
Fully Bayesian VIB-DeepSSM,4.0,Results,"We expect well-calibrated prediction uncertainty to correlate with the error, aleatoric uncertainty to correlate with the input image outlier degree (given that it is data-dependent), and epistemic uncertainty to correlate with the shape outlier degree (i.e., to detect out-of-distribution data). The outlier degree value for each mesh and image is quantified by running PCA (preserving 95% of variability) and then considering the Mahalanobis distance of the PCA scores to the mean (within-subspace distance) and the reconstruction error (off-subspace distance). The sum of these values provides a measure of similarity to the whole set in standard deviation units ",vol3
Fully Bayesian VIB-DeepSSM,4.1,Supershapes Experiments,Supershapes (SS) are synthetic 3D shapes parameterized by variables that determine the curvature and number of lobes  Figure ,vol3
Fully Bayesian VIB-DeepSSM,4.2,Left Atrium Experiments,"The left atrium (LA) dataset comprises 1041 anonymized LGE MRIs from unique patients. The images were manually segmented at the University of Utah Division of Cardiovascular Medicine with spatial resolution 0.65 × 0.65 × 2.5 mm 3 , and the endocardium wall was used to cut off pulmonary veins. The images were cropped around the region of interest, then downsampled by a factor of 0.8 for memory purposes. This dataset contains significant shape variations, including overall size, LA appendage size, and pulmonary veins' number and length. The input images vary widely in intensity and quality, and LA boundaries are blurred and have low contrast with the surrounding structures. Shapes and image pairs with the largest outlier degrees were held out as outlier test sets, resulting in a shape outlier test set of 40 and image outlier test set of 78. We randomly split the remaining samples (90%, 10%, 10%) to get a training set of 739, a validation set of 92, and an inlier test set of 92. The target PDMs were optimized with ShapeWorks  The accuracy and uncertainty calibration analysis in Figs. ",vol3
Fully Bayesian VIB-DeepSSM,5.0,Conclusion,"The traditional computational pipeline for generating Statistical Shape Models (SSM) is expensive and labor-intensive, which limits its widespread use in clinical research. Deep learning approaches have the potential to overcome these barriers by predicting SSM from unsegmented 3D images in seconds, but such a solution cannot be deployed in a clinical setting without calibrated estimates of epistemic and aleatoric uncertainty. The VIB-DeepSSM model provided a principled approach to quantify aleatoric uncertainty but lacked epistemic uncertainty. To address this limitation, we proposed a fully Bayesian VIB model that can predict anatomical SSM with both forms of uncertainty. We demonstrated the efficacy of two practical and scalable approaches, concrete dropout and batch ensemble, and compared them to the baseline VIB and naive ensembling. Finally, we proposed a novel combination of dropout and ensembling and showed that the proposed approach provides improved uncertainty calibration and model robustness on synthetic supershape and real left atrium datasets. While combining Bayesian methods with ensembling increases memory costs, it enables multimodal marginalization improving accuracy. These contributions are an important step towards replacing the traditional SSM pipeline with a deep network and increasing the feasibility of fast, accessible SSM in clinical research and practice.",vol3
Fully Bayesian VIB-DeepSSM,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_34.,vol3
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,1.0,Introduction,"Deep learning have become the de facto solution for medical image segmentation. Nevertheless, despite their ability to learn highly discriminative features, these models have shown to be poorly calibrated, often resulting in over-confident predictions, even when they are wrong ",vol3
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Contribution.,"In this work, we propose a novel method based on entropy maximization to enhance the quality of pixel-level segmentation posteriors. Our hypothesis is that penalizing low entropy on the probability estimates for erroneous pixel predictions during training should help to avoid overconfident estimates in situations of high uncertainty. The underlying idea is that, if a pixel is difficult to classify, it is better assigning uniformly distributed (i.e. high entropy) probabilities to all classes, rather than being overconfident on the wrong class. To this end, we design two simple regularization terms which push the estimated posteriors for misclassified pixels towards a uniform distribution by penalizing low entropy predictions. We benchmark the proposed method in two challenging medical image segmentation tasks. Last, we further show that assessing segmentation models only from a discriminative perspective does not provide a complete overview of the model performance, and argue that including calibration metrics should be preferred. This will allow to not only evaluate the segmentation power of a given model, but also its reliability, of pivotal importance in healthcare.",vol3
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Related Work.,"Obtaining well-calibrated probability estimates of supervised machine learning approaches has attracted the attention of the research community even before the deep learning era, including approaches like histogram  Another alternative is to address the calibration problem during training, for example by clamping over-confident predictions. In  An in-depth analysis of the calibration quality obtained by training segmentation networks with the two most commonly used loss functions, Dice coefficient and cross entropy, was conducted in ",vol3
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,2.0,Maximum Entropy on Erroneous Predictions,"Let us have a training dataset D = {(x, y) n } 1≤n≤|D| , where x n ∈ R Ωn denotes an input image and y n ∈ {0, 1} Ωn×K its corresponding pixel-wise one-hot label. Ω n denotes the spatial image domain and K the number of segmentation classes. We aim at training a model, parameterized by θ, which approximates the underlying conditional distribution p(y|x, θ), where θ is chosen to optimize a given loss function. The output of our model, at a given pixel i, is given as ŷi , whose associated class probability is p(y|x, θ). Thus, p(ŷ i,k = k|x i , θ) will indicate the probability that a given pixel (or voxel) i is assigned to the class k ∈ K. For simplicity, we will denote this probability as pi,k . Since confident predictions correspond to low entropy output distributions, a network is overconfident when it places all the predicted probability on a single class for each training example, which is often a symptom of overfitting  where | • | is used to denote the set cardinality. As we aim at maximizing the entropy of the output probabilities ŷw (Eq. ( ",vol3
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Proxy for Entropy Maximization:,"In addition to explicitly maximizing the entropy of predictions (or to minimizing the negative entropy) as proposed in Eq. 1, we resort to an alternative regularizer, which is a variant of the KL divergence  with q being the uniform distribution and the symbol K = representing equality up to an additive or multiplicative constant associated with the number of classes. We refer the reader to the Appendix I in  Global Learning Objective: Our final loss function takes the following form: L = L Seg (y, ŷ) -λL me (ŷ w ), where ŷ is the entire set of pixel predictions, L Seg the segmentation loss 1 , L me is one of the proposed maximum entropy regularization terms and λ balances the importance of each objective. Note that L me can take the form of the standard entropy definition, i.e. L me (ŷ w ) = L H (ŷ w ) (eq. (  Baseline Models: We trained baseline networks using a simple loss composed of a single segmentation objective L Seg , without adding any regularization term. We used the two most popular segmentation losses: cross-entropy (L CE ) and the negative soft Dice coefficient (L dice ) as defined by  We train two baseline models using the aforementioned regularizer L H (ŷ), considering cross-entropy (L CE ) and Dice losses (L dice ). We also assess the performance of focal-loss  Post-hoc Calibration Baselines. We also included two well known calibration methods typically employed for classification ",vol3
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,3.0,Experiments and Results,"Dataset and Network Details. We benchmark the proposed method in the context of Left Atrial (LA) cavity and White Matter Hyperintensities (WMH) segmentation in MR images. For LA, we used the Atrial Segmentation Challenge dataset ",vol3
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Training Details and Evaluation Metrics.,"As baselines, we used networks trained with L CE and L dice only. We also included the aforementioned posthoc calibration methods (namely IR and PS) as post-processing step for these vanilla models. We also implemented the confidence penalty-based method  To assess segmentation performance we resort to Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD), whereas we use standard calibration metrics: Brier score ",vol3
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Results.,"Our main goal is to improve the estimated uncertainty of the predictions, while retaining the segmentation power of original losses. Thus, we first assess whether integrating our regularizers leads to a performance degradation. Table  Regarding calibration performance, recent empirical evidence  When evaluating the proposed MEEP regularizers (L KL ( Ŷw ) and L H ( Ŷw )) combined with the segmentation losses based on DSC and CE, we observe that DSC with L KL ( Ŷw ) consistently achieves better performance in most of the cases. However, for CE, both regularizers alternate best results, which depend on the dataset used. We hypothesize that this might be due to the different gradient dynamics shown by the two regularizers",vol3
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,4.0,Conclusions,"In this paper, we presented a simple yet effective approach to improve the uncertainty estimates inferred from segmentation models when trained with popular segmentation losses. In contrast to prior literature, our regularizers penalize highconfident predictions only on misclassified pixels, increasing network uncertainty in complex scenarios. In addition to directly maximizing the entropy on the set of erroneous pixels, we present a proxy for this term, formulated with a KL regularizer modeling high uncertainty over those pixels. Comprehensive results on two popular datasets, losses and architectures demonstrate the potential of our approach. Nevertheless, we have also identified several limitations. For example, we have not assessed the effect of the proposed regularizers under severe domain shift (e.g. when testing on images of different organs). In this case it is not clear whether the model will output highly uncertain posteriors, or result again on overconfident but wrong predictions.",vol3
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 27.,vol3
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,1.0,Introduction,"Dynamic contrast-enhanced (DCE) cardiac MRI (CMRI) is an established medical imaging modality for detecting coronary artery disease and stress-induced myocardial blood flow abnormalities. Free-breathing CMRI protocols are preferred over breath-hold exam protocols due to the greater patient comfort and applicability to a wider range of patient cohorts who may not be able to perform consecutive breath-holds during the exam. Once the CMRI data is acquired, a key initial step for accurate analysis of the DCE scan is contouring or segmentation of the left ventricular myocardium. In settings where non-rigid motion correction (MoCo) fails or is unavailable, this process can be a time-consuming and labor-intensive task since a typical DCE scan includes over 300 time frames. Deep neural network (DNN) models have been proposed as a solution to this exhausting task  Our contributions in this work are two-fold: (i) we propose an innovative spatiotemporal dynamic quality control (dQC) tool for model-agnostic test-time assessment of DNN-derived segmentation of free-breathing DCE CMRI; (ii) we show the utility of the proposed dQC tool for improving the performance of DNN-based analysis of external CMRI datasets in a human-in-the-loop framework. Specifically, in a scenario where only 10% of the dataset can be referred to the human expert for correction, although random selection of cases does not improve the performance (p = n.s. for Dice), our dQC-guided selection yields a significant improvement (p < 0.001 for Dice). To the best of our knowledge, this work is the first to exploit the test-time agreement/disagreement between spatiotemporal patch-based segmentations to derive a dQC metric which, in turn, can be used for human-in-the-loop analysis of dynamic CMRI datasets.",vol3
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,2.1,Training/testing Dynamic CMRI Datasets,"Our training/validation dataset (90%/10% split) consisted of DCE CMRI (stress first-pass perfusion) MoCo image-series from 120 subjects, which were acquired using 3T MRI scanners from two medical centers over 48-60 heartbeats in 3 short-axis myocardial slices ",vol3
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,2.2,Patch-Based Quality Control,"Patch-based approaches have been widely used in computer vision applications for image segmentation  Let Θ(w) be a patch extraction operator decomposing dynamic DCE-CMRI image I ∈ R M ×N ×T into spatiotemporal patches θ ∈ R K×K×T by using a sliding window with a stride w in each spatial direction. Also, let Γ m,n be the set of overlapping spatiotemporal patches that include the spatial location (m, n) in them. Also, p i m,n (t) ∈ R T denotes the segmentation DNN's output probability score for the i th patch at time t and location (m, n). The binary segmentation result S ∈ R M ×N ×T is derived from the mean of the probability scores from the patches that are in Γ m,n followed by a binarization operation. Specifically, for a given spatial coordinate (m, n) and time t, the segmentation solution is: The patch-combination operator, whereby probability scores from multiple overlapping patches are averaged, is denoted by Θ -1 (w). The dynamic quality control (dQC) map M ∈ R M ×N ×T is a space-time object and measures the discrepancy between different segmentation solutions obtained at space-time location (m, n, t) and is computed as: where std is the standard deviation operator. Note that to obtain S and M, the same patch combination operator Θ -1 (w) was used with w M < w S . Further, we define 3 quality-control metrics based on M that assess the segmentation quality at different spatial levels: pixel, frame, and slice (image series). First, Q pixel m,n (t) ∈ R is the value of M at space-time location (m, n, t) normalized by the segmentation area at time t: Next, Q frame (t) ∈ R T quantifies the per-frame segmentation uncertainty as perframe energy in M normalized by the corresponding per-frame segmentation area at time t: where • F is the Frobenius norm and M(t) ∈ R M ×N denotes frame t of the dQC map M. Lastly, Q slice assesses the overall segmentation quality of the acquired myocardial slice (image series) as the average of the per-frame metric along time:",vol3
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,2.3,DQC-Guided Human-in-the-Loop Segmentation Correction,As shown in Fig. ,vol3
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,2.4,DNN Model Training,We used a vanilla U-Net ,vol3
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,3.1,Baseline Model Performance,"The ""baseline model"" performance, i.e., the DNN output without the human-inthe-loop corrections, yielded an average spatiotemporal (2D+time) Dice score of 0.767 ± 0.042 for the test set, and 16.2% prevalence of non-contiguous segmentations, which is one of the criteria for failed segmentation (e.g., S(t 1 ) in Fig. ",vol3
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,3.2,Human-in-the-Loop Segmentation Correction,Two approaches were compared for human-in-the-loop framework: (i) referring the top 10% most uncertain time frames detected by our proposed dQC tool (Fig. ,vol3
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,3.3,Difficulty Grading of DCE-CMRI Time Frames vs. Q frame,"To assess the ability of the proposed dQC tool in identifying the most challenging time frames in a DCE-CMRI test dataset, a human expert reader assigned ""difficulty grades"" to each time frame in our test set. The criterion for difficulty was inspired by clinicians' experience in delineating endo-and epicardial contours. Specifically, we assigned the following two difficulty grades: (i) Grade 1: both the endo-and epicardial contours are difficult to delineate from the surrounding tissue; (ii) Grade 0: at most one of the endo-or epicardial contours are challenging to delineate. To better illustrate, a set of example time frames from the test set and the corresponding grades are shown in Fig. ",vol3
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,3.4,Representative Cases,Figure ,vol3
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,4.0,Discussion and Conclusion,"In this work, we proposed a dynamic quality control (dQC) method for DNNbased segmentation of dynamic (time resolved) contrast enhanced (DCE) cardiac MRI. Our dQC metric leverages patch-based analysis by analyzing the discrepancy in the DNN-derived segmentation of overlapping patches and enables automatic assessment of the segmentation quality for each DCE time frame. To validate the proposed dQC tool and demonstrate its effectiveness in temporal localization of uncertain image segmentations in DCE datasets, we considered a human-A.I. collaboration framework with a limited time/effort budget (10% of the total number of images), representing a practical clinical scenario for the eventual deployment of DNN-based methods in dynamic CMRI. Our results showed that, in this setting, the human expert correction of the dQC-detected uncertain segmentations results in a significant performance (Dice score) improvement. In contrast, a control experiment using the same number of randomly selected time frames for referral showed no significant increase in the Dice score, showing the ability of our proposed dQC tool in improving the efficiency of human-in-the-loop analysis of dynamic CMRI by localization of the time frames at which the segmentation has high uncertainty. In the same experiment, dQC-guided corrections resulted in a superior performance in terms of reducing failed segmentations, with a notably lower prevalence vs. random selection (11.3% vs. 14.4%). This reduced prevalence is potentially impactful since quantitative analysis of DCE-CMRI data is sensitive to failed segmentations. A limitation of our work is the subjective nature of the ""difficulty grade"" which was based on feedback from clinical experts. Since the data-analysis guidelines for DCE CMRI by the leading society ",vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,1.0,Introduction,"The medical imaging segmentation plays a crucial role in the computer-assisted diagnosis and monitoring of diseases. In recent years, deep neural networks have demonstrated remarkable results in automatic medical segmentation  Reducing the number of annotations from dense annotations to scribbles or even points poses two challenges in segmentation:  This study aims to address the challenges of obtaining sufficient supervisions and generating high-quality pseudo labels. Previous works  Figure  Taking inspiration from the similar feature patterns observed in segmentation and reconstruction features, we propose a novel framework that utilizes a memory bank to generate pseudo labels. Our framework consists of an encoder that extracts visual features, as well as two decoders: one for segmenting target objects using scribble or point annotations, and another for reconstructing the input image. To address the challenge of seeking sufficient supervision, we employ the reconstruction branch as an auxiliary task to provide additional supervision and enable the network to learn visual representations. To tackle the challenge of generating high-quality pseudo labels, we use a VQ memory bank to store texture-oriented and global features, which we use to generate the pseudo labels. We then combine information from the global dataset and local image to generate improved, confident pseudo labels. The contributions of this work can be summarized as follows. Firstly, a universal framework for annotation-efficient medical segmentation is proposed, which is capable of handling both scribble-supervised and point-supervised segmentation. Secondly, an auxiliary reconstruction branch is employed to provide more supervision and backwards sufficient gradients to learn visual representations. Thirdly, a novel pseudo label generation method from memory bank is proposed, which utilizes the VQ memory bank to store texture-oriented and global features to generate high-quality pseudo labels. To boost the model training, we generate high-quality pseudo labels by mixing the segmentation prediction and pseudo labels from the VQ bank. Finally, experimental results on public MRI segmentation datasets demonstrate the effectiveness of the proposed method. Specifically, our method outperforms existing scribble-supervised segmentation approaches on the ACDC dataset and also achieves better performance than several semi-supervised methods.",vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,2.0,Method,"In this study, we focus on the problem of annotation-efficient medical image segmentation and propose a universal and adaptable framework for both scribblesupervised and point-supervised learning, as illustrated in Fig. ",vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Overview:,"The proposed framework for annotation-efficient medical image segmentation is illustrated in Fig.  Feature Extraction: In this work, we employ a U-Net ",vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Segmentation Branch:,"The segmentation branch g seg takes the feature map F as input and produces the final segmentation masks based on the available scribble or point annotations. Following recent works such as  VQ Memory Bank: Motivated by the similar feature patterns observed in medical images, we utilize the Vector Quantization (VQ) memory bank to store texture-oriented and global features, which are then employed for pseudo label generation. The pseudo label generation process involves three stages, as illustrated in Fig. ",vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Memory Bank Definition.,"In accordance with the VQVAE framework  Memory Update Stage. The feature map F recon is obtained from the last layer in the reconstruction branch and is utilized to update the VQ memory bank and retrieve an augmented feature Frecon . For each spatial location f j ∈ R 1×64 in F recon ∈ R 64×256×256 , we use L2 is used to compute the distance between f j and e k and find the nearest feature e i ∈ R 1×64 in the VQ memory bank, as follows: fj = e i , i = arg min k |f j -e k | 2 2 . Following  , where sg denotes the stop-gradient operator.",vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Pseudo Label Table Update,"Stage. The second stage mainly updates a pseudo label table, using the labelled regions on the reconstruction features and assigning pseudo labels on memory vectors. In particular, it uses the labelled pixels and their corresponding reconstruction features and finds the nearest vectors in the memory bank. As shown in Fig.  Pseudo Label Generation Stage. The third stage utilizes the pseudo label table to generate the pseudo labels. It takes the feature map F recon as inputs, then finds their nearest memory vectors, and retrieve the pseudo label according to the vector indices. The generated pseudo label is generated by the repetitive texture patterns on the reconstruction branch, which would include the segmenation information as well as other things.",vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Pseudo Label Generation:,"The generation of pseudo labels from the reconstruction branch is based on a texture-oriented and global view, as the memory bank stores the features extracted from the entire dataset. However, relying solely on it may not be sufficient, and it is necessary to incorporate more segmentation-specific information from the segmentation branch. Therefore, we leverage both approaches to enhance the model training. To incorporate both the segmentation-specific information and the textureoriented and global information, we dynamically mix the predictions y 1 from the segmentation branch and the pseudo labels y 2 from the VQ memory bank to generate the final pseudo labels y *  , where α is uniformly sampled from [0, 1]. The argmax function is used to generate hard pseudo labels. We then use the generated y * to supervise y 1 and assist in the network training. The pseudo label loss is defined as L pl (P L, y 1 ) = 0.5 × L dice (y * , y 1 ), where L dice is the dice loss, which can be substituted with other segmentation loss functions such as cross-entropy loss. Loss Function: Finally, our loss function is calculated as where λ V Q is hyper weights with λ V Q = 0.1.",vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,3.1,Experimental Setting,We use the PyTorch ,vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,3.2,Performance Comparisons,"We conducted an evaluation of our model on the ACDC dataset, utilizing the 3D Dice Coefficient (DSC) and the 95% Hausdorff Distance (95) as the metrics. In this study, we compare our proposed model with various state-ofthe-art methods and designed baselines. These include: (1) scribble-supervised segmentation methods such as pCE only ",vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,3.3,Ablation Study,We investigate the effect of our proposed method on the ACDC datasets.,vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Effect of the Auxiliary Task:,We designed a baseline by removing the reconstruction branch and VQ memory. The results in Table ,vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Effect of Pseudo Labels:,We also designed a baseline using only the predictions as pseudo labels. In Table ,vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Effect of Different Levels of Aannotations:,"We also evaluated the impact of using different levels of annotations in point-supervised learning, ranging from more annotations, e.g. 10 points, to fewer annotations, e.g. 2 points. The results in Table ",vol3
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,4.0,Conclusion,"In this study, we introduce a universal framework for annotation-efficient medical segmentation. Our framework leverages an auxiliary reconstruction branch to provide additional supervision to learn visual representations and a novel pseudo label generation method from memory bank, which utilizes the VQ memory bank to store global features to generate high-quality pseudo labels. We evaluate the proposed method on a publicly available MRI segmentation dataset, and the experimental results demonstrate its effectiveness.",vol3
