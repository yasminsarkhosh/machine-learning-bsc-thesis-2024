Paper Title,Header Number,Header Title,Text,Volume
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,1.0,Introduction,"To reduce radiologists' reading burden and make the diagnostic process more manageable, especially when the number of experts is scanty, computer-aided diagnosis (CAD) systems, particularly deep learning-based anomaly detection  In this paper, inspired by the success of the MAE approach, we propose a two-stage algorithm for ""Adaptation of pre-trained Masked AutoEncoder"" (AMAE) to leverage simultaneously normal and unlabeled images for anomaly detection in chest X-rays. As for Stage 1 of our method, (i) AMAE creates synthetic anomalies from only normal training images, and the usefulness of pretrained MAE ",vol1
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,2.0,Method,"Notation. We first formally define the problem setting for the proposed dualdistribution anomaly detection. Contrary to previous unsupervised anomaly detection methods, AMAE fully uses unlabeled images, yielding a training data T train = T n ∪ T u consisting of both normal T n and unlabeled T u training sets. We denote the normal training set as T n = {x ni } N i=1 , with N normal images, and the unlabeled training set as T u = {x ui } M i=1 , with M unlabeled images to be composed of both normal and abnormal images. At test time, given a test set T test = {(x ti , y i )} S i=1 with S normal or abnormal images, where y i ∈ {0, 1} is the corresponding label to x ti (0 for normal (negative) and 1 for abnormal (positive) image), the trained anomaly detection model should identify whether the test image is abnormal or not. Architecture. Our architecture is <-shaped: the ViT-small (ViT-S/16) ",vol1
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,2.1,Stage 1-Proxy Task to Detect Synthetic Anomalies,"AMAE starts the first training stage using only normal training images by defining a proxy task to detect synthetic anomalies shorn of real known abnormal images. For this purpose, we utilize the state-of-the-art (SOTA) anatomy-aware cut-and-paste augmentation, AnatPaste  where Aug (•) is the AnatPaste augmentation (see  We set the label for the normal image to 0 and 1 otherwise (synthetic anomaly). The above gradient-based optimization produces a trained classifier projection head h 0 . Thus, the whole architecture can be trained with much fewer parameters while making only the classifier projection head specialized at recognizing anomalies without influencing the ViT encoder.",vol1
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,2.2,Stage 2-MAE Inter-Discrepancy Adaptation,"The proposed MAE adaptation scheme is inspired by  in which a different small subset of the patches (ratio of 25%) is retained each time to be fed to the ViT encoder f . The lightweight ViT decoder g receives unmasked patches' embeddings and adds learnable masked tokens to replace the masked-out patches. Subsequently, the full set of embeddings of visible patches and masked tokens with added positional embeddings to all tokens is processed by the ViT decoder g to reconstruct the missing patches of each image in pixels. This yields the reconstructed image x(j) = g • f m (j) (x) , which is then compared against the input image x to optimize both ViT encoder  All pixels in the t th patch of both input image and reconstructed images are multiplied by m (j) t ∈ {0, 1}. The above self-supervised loss term averages L pixel-wise mean squared errors for each image. Module A is trained on a combination of the normal training set T n and pseudo-labeled normal images from the unlabeled set T un . In contrast, Module B is trained using only pseudo-labeled abnormal images from an unlabeled set T ua . Optimization for Eq. 3 always starts from pre-trained f 0 and g 0 , and we reset the MAE weights to f 0 and g 0 before training each module. A high discrepancy between the reconstruction outputs of the two modules can indicate potential abnormal regions. Similar to the training stage, we apply L random masks to the test image x t ∼ T test to obtain L reconstructions (see Fig.  where p is the index of pixels, μA and μB are the mean maps of L reconstructed images from Module A and Module B, respectively. The pixel-level anomaly scores for each image are averaged, yielding the image-level anomaly score.",vol1
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,3.0,Experiments,"Datasets. We evaluated our method on three public CXR datasets: 1) the RSNA Pneumonia Detection Challenge dataset 1 , 2) the VinBigData Chest X-ray Abnormalities Detection Challenge dataset (VinDrCXR) 2  1 https://www.kaggle.com/c/rsna-pneumonia-detection-challenge. Implementation Details. We adopt AdamW  Comparison with SOTA Methods. Table ",vol1
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,4.0,Conclusion,"We present AMAE, an adaptation strategy of the pre-trained MAE for dual distribution anomaly detection in CXRs, which makes our method capable of more effectively apprehending anomalous features from unlabeled images. Experiments on the three CXR benchmarks demonstrate that AMAE is generalizable to different model architectures, achieving SOTA performance. As for the limitation, an adequate number of normal training images is still required, and we will extend our pseudo-labeling scheme in our future work for robust anomaly detection bypassing any training annotations.",vol1
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_19.,vol1
Unsupervised Domain Adaptation for Anatomical Landmark Detection,1.0,Introduction,"Anatomical landmark detection is a fundamental step in many clinical applications such as orthodontic diagnosis  In recent years, deep learning based methods have achieved great progresses in anatomical landmark detection. For supervised learning, earlier works  Despite the success of recent methods, they mostly focus on single-domain data, which assume the training and test sets follow the same distribution. However, such an assumption is not always true in practice, due to the differences in patient populations and imaging devices. Figure  In this paper, we aim to investigate anatomical landmark detection under the setting of UDA. Our preliminary experiments show that a well-performed model will yield significant performance drop on cross-domain data, where the mean radial error (MRE) increases from 1.22 mm to 3.32 mm and the success detection rate (SDR) within 2 mm drops from 83.76% to 50.05%. To address the domain gap, we propose a unified framework, which contains a base landmark detection model, a self-training strategy, and a domain adversarial learning module. Specifically, self-training is adopted to effectively leverage the unlabeled data from the target domain via pseudo-labels. To handle confirmation bias ",vol1
Unsupervised Domain Adaptation for Anatomical Landmark Detection,2.0,Method,Figure ,vol1
Unsupervised Domain Adaptation for Anatomical Landmark Detection,2.1,Landmark Detection Model,"Recently, coordinate regression  Global Localization. We adopt Transformer decoder  Local Refinement. This module outputs a score map ŷs ∈ R L×H×W and an offset map ŷo ∈ R 2L×H×W via 1 × 1 convolutional layers by taking f as input. The score map indicates the likelihood of each grid to be the target landmark, while the offset map represents the relative offsets of the neighbouring grids to the target. The ground-truth (GT) landmark of the score map is smoothed by a Gaussian kernel  The loss function of the landmark detection model can be summarized as where S is source domain data, λ s and λ o are balancing coefficients. Empirically, we set λ s = 100 and λ o = 0.02 in this paper.",vol1
Unsupervised Domain Adaptation for Anatomical Landmark Detection,2.2,Landmark-Aware Self-training,"Self-training  where M represents the mask operation, T = {x T j , y , and y T is the estimated pseudo-labels from the last self-training round. Note that the masks of the source domain data S always equal to one as they are ground truths. However, the landmark-level selection leads to unbalanced pseudo-labels between landmarks, as shown in Fig. ",vol1
Unsupervised Domain Adaptation for Anatomical Landmark Detection,2.3,Domain Adversarial Learning,"Although self-training has been shown effective, it inevitably contains bias towards source domain because its initial model is trained with source domain data only. In other words, the data distribution of target domain is different from the source domain, which is known as covariate shift  where d is domain label, with d = 0 and d = 1 indicating the images are from source and target domain, respectively. The domain classifier is trained to minimize L D , while the feature extractor F is encouraged to maximize it such that the learned feature is indistinguishable to the domain classifier. Thus, the adversarial objective function can be written as To simplify the optimization, we adopt gradient reversal layer (GRL)  where λ D is a balancing coefficient.",vol1
Unsupervised Domain Adaptation for Anatomical Landmark Detection,3.1,Experimental Settings,"In this section, we present experiments on cephalometric landmark detection. See lung landmark detection in Appendix A. Source Domain. The ISBI 2015 Challenge provides a public dataset  Target Domain. The ISBI 2023 Challenge provides a new dataset ",vol1
Unsupervised Domain Adaptation for Anatomical Landmark Detection,3.2,Results,"For the comparison under UDA setting, several state-of-the-art UDA methods were implemented, including FDA ",vol1
Unsupervised Domain Adaptation for Anatomical Landmark Detection,3.3,Model Analysis,"We first do ablation study to show the effectiveness of each module, which can be seen in Table ",vol1
Unsupervised Domain Adaptation for Anatomical Landmark Detection,3.4,Qualitative Results,Figure ,vol1
Unsupervised Domain Adaptation for Anatomical Landmark Detection,4.0,Conclusion,"In this paper, we investigated anatomical landmark detection under the UDA setting. To mitigate the performance drop caused by domain shift, we proposed a unified UDA framework, which consists of a landmark detection model, a self-training strategy, and a DAL module. Based on the predictions and confidence scores from the landmark model, a self-training strategy is proposed for domain adaptation via landmark-level pseudo-labels with dynamic thresholds. Meanwhile, the model is encouraged to learn domain-invariant features via adversarial training so that the unaligned data distribution can be addressed. We constructed a UDA setting based on two anatomical datasets, where the experiments showed that our method not only reduces the domain gap by a large margin, but also outperforms other UDA methods consistently. However, a performance gap still exists between the current UDA methods and the supervised model in target domain, indicating more effective UDA methods are needed to close the gap.",vol1
Unsupervised Domain Adaptation for Anatomical Landmark Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_66.,vol1
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",1.0,Introduction,"High-resolution magnetic resonance (MR) images (MRI) provide a wealth of structural details, which facilitate early and precise diagnosis  This study was partially supported by the National Natural Science Foundation of China via project U20A20199. time and signal-noise ratio  Deep learning-based algorithms for single MR image super-resolution show great potential in restoration of HR images from LR inputs  To this end, we propose a CT-guided, unsupervised MRI super-resolution reconstruction method based on joint cross-modality image translation (CIT) and super-resolution reconstruction, eliminating the requirement of HR MR images for training. Specifically, our network design features a super-resolution Network (SRNet) and a cross-modality image translation network (CITNet) based on disentanged representation learning. After pretraining, the SRNet can generate pseudo HR MR images from LR MR images. The generated pseudo HR MR images are then taken together with the HR CT images as the input to the CITNet, which can generate quality-improved pseudo HR MR images by combining disentangled content code of the input CT data with the attribute  ",vol1
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",2.1,Super-Resolution Network (SRNet),"We choose to use the residual dense network (RDN) as the SRNet. The RDN utilizes cascaded residual dense blocks (RDBs), a powerful convolutional block that leverages residual and dense connections to fully aggregate hierarchical features. For further details on the structure of the RDN, please refer to the original paper ",vol1
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",2.2,Cross-Modality Image Translation Network (CITNet),"The CITNet is inspired by MUNIT  The encoder in each domain disentangles an input image separately into a domain-invariant content space C and a domain-specific attribute space A. And the generator networks combine a content code with an attribute code to generate translated images in the target domain. For instance, when translating CT image y H ∈ Y to MR image x H ∈ X , we first randomly sample from the prior distribution p(A x ) ∼ N (0, I) to obtain an MRI attribute code A x , which is empirically set as a 8-bit vector. We then combine A x with the disentangled content code of the CT image and A y is also sampled from the prior distribution p(A y ) ∼ N (0, I). Disentangled Representation Learning. Cross-modality image translation is based on disentangled representation learning, trained with self-and crosscycle reconstruction losses. As shown in Fig.  where Specially, in the cross-cycle translation processes, we employe a latent reconstruction loss to maintain the invertible mapping between the image and the latent space. In details, we have: We further use pretrained vgg16 network, denoted as φ(•), to extract highlevel features for computing the perceptual loss  where C, H, W indicate the channel number and the image size, respectively. Adversarial Learning. As shown in Fig.  Joint Optimization. The SRNet and the CITNet are jointly optimized by minimizing following loss function: where λ 1 , λ 2 , and λ 3 are parameters controlling the relative weights of different losses.",vol1
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",2.3,Training Strategy,"Empirically, we found that training the network shown in Fig. ",vol1
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",,Stage 1. Let's denote the downsampling function as D(•).,"In this stage, we pretrain the SRNet using the HR CT images, as shown in Fig.  Stage 3. The MR images generated by the model pretrained at the first two stages can be further improved. In stage 3, we conduct joint optimization of the SRNet and the CITNet as shown in Fig.  and the SRNet by minimizing L disentangle as defined in Eq.  The training procedure of our method is illustrated by Algorithm 1. Implementation Details. To train the proposed network, each training sample is unpaired LR MRI and HR CT images. All images are normalized to the range between -1.0 and 1.0. Optimization is performed using Adam with a batch size of 1. The initial learning rate is set to 0.0001 and decreased by a factor of 5 every 2 epochs. We empirically set λ 1 = 10, λ 2 = λ 3 = 1 and T = 100, 000. Table  Table  Our method is trained in two pretrain stages and one joint optimization stage. We thus conduct ablation study on dataset from Site1 to analyze the quality of the generated pseudo HR MR images at each stage. As shown in Table ",vol1
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",4.0,Conclusion,"In this paper, we proposed a CT-guided, unsupervised MRI super-resolution reconstruction method based on joint cross-modality image translation and super-resolution reconstruction, eliminating the requirement of HR MRI for training. We conducted experiments on two datasets respectively acquired from two different clinical centers to validate the effectiveness of the proposed method. Quantitatively and qualitatively, the proposed method achieved superior performance over the SOTA unsupervised SR methods.",vol1
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,1.0,Introduction,"The electrocardiogram (ECG) is a monitoring tool widely used to evaluate the heart status of patients and provide information on cardiac electrophysiology. Developing automated analysis systems capable of detecting and identifying abnormal signals is crucial in light of the importance of ECGs in medical diagnosis and the need to ease the workload of clinicians. However, training a classifier on labeled ECGs that focus on specific diseases may not recognize new abnormal statuses that were not encountered during training, given the diversity and rarity of cardiac diseases  The current anomaly detection techniques, including one-class discriminative approaches  This paper proposes a novel multi-scale cross-restoration framework for ECG anomaly detection and localization. To our best knowledge, this is the first work to integrate both local and global characteristics for ECG anomaly detection. To take into account multi-scale data, the framework adopts a two-branch autoencoder architecture, with one branch focusing on global features from the entire ECG and the other on local features from heartbeat-level details. A multiscale cross-attention module is introduced, which learns to combine the two feature types for making the final prediction. This module imitates the diagnostic process followed by experienced cardiologists who carefully examine both the entire ECG and individual heartbeats to detect abnormalities in both the overall rhythm and the specific local morphology of the signal  To comprehensively evaluate the performance of the proposed method on a large number of individuals, we adopt the public PTB-XL database ",vol1
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,2.0,Method,"In this paper, we focus on unsupervised anomaly detection and localization on ECGs, training based on only normal ECG data. Formally, given a set of N normal ECGs denoted as {x i , i = 1, ..., N }, where x i ∈ R D represents the vectorized representation of the i-th ECG consisting of D signal points, the objective is to train a computational model capable of identifying whether a new ECG is normal or anomalous, and localize the regions of anomalies in abnormal ECGs.",vol1
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,2.1,Multi-scale Cross-restoration,"In Fig.  Masking and Encoding. Given a pair consisting of a global ECG signal x g ∈ R D and a randomly selected local heartbeat x l ∈ R d segmented from x g for training, as shown in Fig.  , where denotes the element-wise product. Multi-scale Cross-attention. To capture the relationship between global and local features, we use the self-attention mechanism  is the square root of the feature dimension used as a scaling factor. Self-attention is achieved by setting The cross-attention feature, f ca , is obtained from the self-attention mechanism, which dynamically weighs the importance of each element in the combined feature. To obtain the final outputs of the global and local features, f out g and f out l , containing cross-scale information, we consider residual connections: ). An uncertainty-aware restoration loss is used to incorporate restoration uncertainty into the loss functions, where for each function, the first term is normalized by the corresponding uncertainty, and the second term prevents predicting a large uncertainty for all restoration pixels following  Trend Generation Module. The trend generation module (TGM) illustrated in Fig.  The restoration loss is defined as the Euclidean distance between x g and xt , This process guides global feature learning using time-series trend information, emphasizing rhythm characteristics while de-emphasizing morphological details. Loss Function. The final loss function for optimizing our model during the training process can be written as where α and β are trade-off parameters weighting the loss function. For simplicity, we adopt α = β = 1.0 as the default.",vol1
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,2.2,Anomaly Score Measurement,"For each test sample x, local ECGs from the segmented heartbeat set {x l,m , m = 1, ..., M } are paired with the global ECG x g one at a time as inputs. The anomaly score A(x) is calculated to estimate the abnormality, where the three terms correspond to global restoration, local restoration, and trend restoration, respectively. For localization, an anomaly score map is generated in the same way as Eq. (  The anomalies are indicated by relatively large anomaly scores, and vice versa.",vol1
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,3.0,Experiments,"Datasets. Three publicly available ECG datasets are used to evaluate the proposed method, including PTB-XL  -   Implementation Details. The ECG is pre-processed by a Butterworth filter and Notch filter ",vol1
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,3.1,Comparisons with State-of-the-Arts,"We compare our method with several time-series anomaly detection methods, including heartbeat-level detection method BeatGAN  Anomaly Detection. The anomaly detection performance on PTB-XL is summarized in Table  Anomaly Localization Visualization. We present visualization results of anomaly localization on several samples from our proposed benchmark in Fig. ",vol1
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,3.2,Ablation Study and Sensitivity Analysis,"Ablation studies were conducted on PTB-XL to confirm the effectiveness of individual components of the proposed method. Table  We conduct a sensitivity analysis on the mask ratio, as shown in Table ",vol1
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,4.0,Conclusion,"This paper proposes a novel framework for ECG anomaly detection, where features of the entire ECG and local heartbeats are combined with a maskingrestoration process to detect anomalies, simulating the diagnostic process of cardiologists. A challenging benchmark, with signal point-level annotations provided by experienced cardiologists, is proposed, facilitating future research in ECG anomaly localization. The proposed method outperforms state-of-the-art methods, highlighting its potential in real-world clinical diagnosis.",vol1
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_9.,vol1
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,1.0,Introduction,"Normative modelling is a popular method to study heterogeneous brain disorders. Normative models assume disease cohorts sit at the tails of a healthy population distribution and quantify individual deviations from healthy brain patterns. Typically, a normative analysis constructs a normative model per variable, e.g., using Gaussian Process Regression (GPR)  Most previous deep-learning normative and anomaly detection models measure deviations in the feature space  There are many approaches to extending VAEs to integrate information from multiple modalities and learn informative joint latent representations. Most multi-modal VAE frameworks learn separate encoder and decoder networks for each modality and aggregate the encoding distributions to learn a joint latent representation. Wu and Goodman  As far as we are aware, only one other multi-modal VAE normative modelling framework has been proposed in the literature which uses the PoE (PoE-normVAE)  Our contributions are two-fold. Firstly, we present two novel multi-modal normative modelling frameworks, MoE-normVAE and gPoE-normVAE, which capture the joint distribution between different imaging modalities. Our proposed models outperform baseline methods on two neuroimaging datasets. Secondly, we present a deviation metric, based on the latent space, suitable for detecting deviations in multi-modal normative distributions. We show that our metric better leverages the benefits of multi-modal normative models compared to feature space-based metrics.",vol1
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,2.0,Methods,"Multi-modal Variational Autoencoder (mVAE). Let X = {x m } M m=1 be the observations of M modalities. We use a mVAE to learn a multi-modal generative model (Fig.  where the second term is the KL divergence between the approximate joint posterior q φ (z | X) and the prior p(z). We model the posterior, likelihood, and prior distributions as isotropic gaussians. Approximate Joint Posterior. To train the mVAE, we must specify the form of the joint approximate posterior q φ (z | X). Wu and Goodman  , where the experts, i.e., individual posterior distributions q φm (z | x m ), are parameterised by encoder networks with parameters ), the parameters of joint posterior distribution can be computed  (see Supp. for proofs). However, overconfident but miscalibrated experts may bias the joint posterior distribution (see Fig.  Shi et al.  In the MoE setting, each uni-modal posterior q φ (z | x m ) is evaluated with the generative model p θ (X, z) such that the ELBO becomes: (2) However, this approach only takes each uni-modal encoding distribution separately into account during training. Thus, there is no explicit aggregation of information from multiple modalities in the latent representation for reconstruction by the decoder networks. For modalities with a high degree of modality-specific variation, this enforces an undesirable upperbound on the ELBO potentially leading to a sub-optimal approximation of the joint distribution  Generalised Product-of-Experts Joint Posterior. We propose an alternative approach to mitigate the problem of overconfident experts by factorising the joint posterior as a generalised Product-of-Experts (gPoE)  where α m is a weighting for modality m such that M m=1 α m = 1 for each latent dimension and 0 < α m < 1. We optimise α during training allowing the model to weight experts in such a way as to learn an approximate joint posterior q φ (z | X) where the likelihood distribution p θ (X | z) is maximised. This provides a means to down-weigh overconfident experts. Furthermore, as α is learnt per latent dimension, different modality weightings can be learnt for different vectors, thus explicitly incorporating modality specific variation in addition to shared information in different dimensions of the joint latent space. Similarly to the PoE approach, we can compute the parameters of the joint posterior distribution; Recently, a gPoE mVAE was proposed for learning joint representations of hand-poses and surgical videos ",vol1
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,,Multi-modal Normative Modelling.,"We propose two mVAE normative modelling frameworks shown in Fig.  To compare our normVAE models to a classical normative approach, we trained one GPR (using the PCNToolkit) per feature on a sub-set of 2000 healthy UK Biobank individuals and used extreme value statistics to calculate subjectlevel abnormality index  2 . Kumar et al.  where μ norm d norm ij is the mean and σ norm d norm ij the standard deviation of the deviations d norm ij of a holdout healthy control cohort. However, in the multi-modal setting, feature space-based deviation metrics may not highlight the benefits of multi-modal models over their uni-modal counterparts. The goal of the joint latent representation is to capture information from all modalities. Thus, decoders for each modality must extract the information from the joint latent representation, which now carries information from all other modalities as well. Therefore, data reconstructions capture only information relevant to a particular modality and may also be poorer compared to uni-modal methods. As such, particularly when incorporating modalities with a high degree of modality-specific variation, we believe latent space deviation metrics would better capture deviations from normative behaviour across multiple modalities. Then, once an abnormal subject has been identified, feature space metrics can be used to identify deviating brain regions (e.g. Supp. Fig.  We propose a latent deviation metric to measure deviations from the joint normative distribution. To account for correlation between latent vectors and derive a single multivariate measure of deviation, we measure the Mahalanobis distance from the encoding distribution of the training cohort: where z j ∼ q (z j | X j ) is a sample from the joint posterior distribution for subject j, μ(z norm ) is the mean and Σ(z norm ) the covariance of the healthy cohort latent position. We use robust estimates of the mean and covariance to account for outliers within the healthy control cohort. For closer comparison with D ml , we derive the following multivariate feature space metric: where d j = {d ij , . . . , d Ij } is the reconstruction error for subject j for brain regions (i = 1, ..., I), μ(d norm ) is the mean and Σ(d norm ) the covariance of the healthy cohort reconstruction error. Assessing Deviation Metric Performance. For each model, we calculated D ml and D mf for a healthy holdout cohort and disease cohort. For each deviation metric, we identified individuals whose deviations were significantly different from the healthy training distribution (p < 0.001)  In order to calculate significance ratios, we calculated D uf relative to the training cohort for the healthy holdout and disease cohorts (Bonferroni adjusted p=0.05/N features ) ",vol1
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,3.0,Experiments,"Data Processing. To train the normVAE models, we used 10,276 healthy subjects from the UK Biobank  We also tested the models using an external dataset. We extracted 213 subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI) 1 [10] dataset with significant memory concern (SMC; N=27), early mild cognitive impairment (EMCI; N=63), late mild cognitive impairment (LMCI; N=34), Alzheimer's disease (AD; N=43) as well as healthy controls (HC; N=45). We used the healthy controls to fine-tune the models in a transfer learning approach. The same T1 and DTI features as for the UK Biobank were extracted for the ADNI dataset. Rather than conditioning on covariates as done in some related work ",vol1
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,,UK Biobank Results.,"As expected, we see greater significance ratios for all models when using D ml rather than D mf (Table  ADNI Results. Previous work ",vol1
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,4.0,Discussion and Further Work,"We have built on recent works  Our models provide a more informative joint representation compared to baseline methods as evidenced by the better significance ratio for the UK Biobank dataset and greater sensitivity to disease staging and correlation with cognitive measures in the ADNI dataset. We also proposed a latent deviation metric suitable for detecting deviations in the multivariate latent space of multi-modal normative models which gave an approximately 4-fold performance increase over metrics based on the feature space. Further work will involve extending our models to more data modalities, such as genetic variants, to better characterise the behaviour of a physiological system. We note that, for fair comparison across models, we remove the effects of confounding variables prior to analysis. However, confounding effects could be removed during analysis via condition variables  Normative models have been successfully applied to the study of a range of heterogeneous diseases. Diseases often present abnormalities across a range of neuroimaging, biological and physiological features which provide different information about the underlying disease process. Normative systems that incorporate features from different data modalities offer a holistic picture of the disease and will be capable of detecting abnormalities across a broad range of different diseases. Furthermore, multi-modal normative modelling captures the relationship between different modalities in healthy individuals, with disruption to this relationship potentially leading to a disease signal. Code is publicly available at https://github.com/alawryaguila/multimodal-normative-models.",vol1
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,,Acknowledgements,. This work is supported by the ,vol1
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 41.,vol1
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,1.0,Introduction,"CT technology can recover the internal details of the human body in a noninvasive way and has been widely used in clinical practice. However, if there is metal in the tissue, metal artifacts (MA) will appear in the reconstructed CT image, which will corrupt the image and affect the medical diagnosis  In light of the clinical need for MA reduction, various traditional methods  For unsupervised methods in the image domain, Liao et al.  Considering the importance of low-level features in the latent space for generating the artifact-free component, we propose a novel Dense Transformer based Enhanced Coding Network (DTEC-Net) for unsupervised metal artifact reduction, which can obtain low-level features with hierarchical information and map them to a clean image space through adversarial training. DTEC-Net contains our developed Hierarchical Disentangling Encoder (HDE), which utilizes longrange correspondences obtained by a lightweight transformer and a high-order dense process to produce the enhanced coded sequence. To ease the burden of decoding the sequence, we also propose a second-order disentanglement method to finish the sequence decomposition. Extensive empirical results show that our method can not only reduce the MA greatly and generate high-quality images, but also surpasses the competing unsupervised approaches.",vol1
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,2.0,Methodology,"We design a Hierarchical Disentangling Encoder(HDE) that can capture lowlevel sequences and enable high-performance restoration. Moreover, to reduce the burden of the decoder group brought by the complicated sequences, we propose a second-order disentanglement mechanism. The intuition is shown in Fig. ",vol1
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,2.1,Hierarchical Disentangling Encoder (HDE),"As shown in Fig.  coding. Specifically, for the HDE's input image x a ∈ R 1×H×W with MA, HDE first uses a convolution for the preliminary feature extraction and produces a high-dimensional tensor x l0 with c channels. Then, x l0 will be encoded by three Dense Transformers for Disentanglement (DTDs) in a first-order reuse manner  In Eq. (1), f s-hde represents the channel compression of the concatenation of multiple DTDs' outputs, and N represents the total number of DTDs in the HDE. As shown in Fig.  As shown in Fig. ",vol1
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,2.2,Dense Transformer for Disentanglement (DTD),"In addition to the first-order feature multiplexing given in Eq. (  Specifically, the input x 1 ∈ R C×H×W of the DTD will be processed sequentially by the lightweight transformer and groups of convolutions in the form of second-order dense connections. The output x j+1 of the jth convolution with ReLU, which is connected in a second-order dense pattern, can be expressed as: In Eq. (  where represents the Hadamard product, f MLP indicates a multi-layer perceptron with only one hidden layer, and f pooling represents global pooling. Because the transformer usually requires a large amount of data for training and CT image datasets are usually smaller than those for natural images, we do lightweight processing for the Swin transformer. Specifically, for an input tensor x ∈ R C×H×W of the lightweight transformer, the number of channels will be reduced from C to C in to lighten the burden of the attention matrix. Then, a residual block is employed to extract information with low redundancy. After completing lightweight handling, the tensor will first be partitioned into multiple local windows and flattened to x in ∈ R ( HW P 2 )×P 2 ×Cin according the pre-operation  In actual operation, we use window-based multi-head attention (MSA) ",vol1
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,2.3,Second-Order Disentanglement for MA Reduction (SOD-MAR),"As mentioned in Sect. 2.1, X l represents the hierarchical sequence and facilitates the generator's representation. However, X l needs to be decoded by a highcapacity decoder to match the encoder. Considering that Decoder2 does not directly participate in the restoration branch and already loaded up the complicated artifact part x m in traditional first-order disentanglement learning ",vol1
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,,(a). In order,"Moreover, we don't only map the x h into Decoder1 and Decoder2 while dropping the X l \{x h } to implement the burden reduction, because the low-level information in X l \{x h } is vital for restoring artifact-free images. Furthermore, x h will be disturbed by noise from the approaching target x a of Decoder2 while information X l \ {x h } upstream from the HDE can counteract the noise disturbance to a certain extent. The reason behind the counteraction is that the update to upstream parameters is not as large as that of the downstream parameters.",vol1
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,2.4,Loss Function,"Following  (5) The above x a , y c represent the input as shown in Fig. ",vol1
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,3.0,Empirical Results,Synthesized DeepLesion Dataset. Following  Real Clinic Dataset. We randomly combine 6165 artifacts-affected images and 20729 artifacts-free images from SpineWeb,vol1
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,3.1,Ablation Study,"To verify the effectiveness of the proposed methods, ablation experiments were carried out on Synthesized DeepLesion. The results are shown in Table  The Impact of DTD in HDE. In this experiment, we change the encoding ability of HDE by changing the number of DTDs. We first use only one DTD to build the HDE, then the PSNR is 0.65 dB lower than our DTEC-Net using three DTDs. Additionally, the average MSE in this case is much higher than DTEC-Net. When the number of DTDs increases to two, the performance improves by 0.25 dB and is already better than the SOTA method  Only Transformer in DTD. Although the transformer can obtain better longrange correspondence than convolutions, it lacks the multiplexing of low-level information. For every DTD in DTEC-Net, we delete the second-order feature reuse pattern and only keep the lightweight transformer, the degraded version's results are 0.8 dB lower than our DTEC-Net. At the same time, great instability appears in generative adversarial training. So, only using the transformer cannot achieve good results in reducing metal artifacts. Removing SOD-MAR. Although SOD-MAR mainly helps by easing the burden of decoding as discussed in Sect. 2.3, it also has a performance gain compared to first-order disentanglement. We delete the SOD-MAR in DTEC-Net and let x h be the unique feature decoded by Decoder1. The Performance is 0.2 dB lower than our DTEC-Net, while MSE increases by 1.47.  Unsupervised RCN ",vol1
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,3.2,Comparison to State-of-the-Art (SOTA),"For a fair comparison, we mainly compare with SOTA methods under unsupervised settings: ADN  Because ADN has open-source code, we run their code for qualitative results. Quantitative Results. As shown in Table  Qualitative Results. A visual comparison is shown in Fig. ",vol1
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,4.0,Conclusion,"In this paper, we proposed a Dense Transformer based Enhanced Coding Network (DTEC-Net) for unsupervised metal-artifact reduction. In DTEC-Net, we developed a Hierarchical Disentangling Encoder (HDE) to represent longrange correspondence and produce an enhanced coding sequence. By using this sequence, the DTEC-Net can better recover low-level characteristics. In addition, to decrease the burden of decoding, we specifically design a Second-order Disentanglement for MA Reduction (SOD-MAR) to finish the sequence decomposition. The extensive quantitative and qualitative experiments demonstrate our DTEC-Net's effectiveness and show it outperforms other SOTA methods.",vol1
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_8.,vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,1.0,Introduction,"Accurate medical representation is crucial for clinical decision-making. Deep learning has shown promising results in medical image analysis, but the accuracy of these models heavily relies on the quality and quantity of data and annotations  While the random masking strategy is commonly used in current MIM-based works, randomly selecting a percentage of patches to mask. We argue that such a strategy may not be the most suitable approach for medical images due to the domain particularity. Medical images commonly present relatively fixed anatomical structures, while subtle variations between individuals, such as sporadic lesions that alter the texture and morphology of surrounding tissues or organs, may exist. These pathology characteristics may be minute and challenging to perceive visually but are indispensable for early screening and clinical diagnosis. Representation learning should capture these desired target representations to improve downstream diagnosis models' reliability, interpretability, and generalizability. Random masking is less likely to deliberately focus on these important parts. We put forward a straightforward principle, i.e., masking and reconstructing meaningful characteristics, encouraging the network to explore stronger representations from medical images. We advocate utilising radiological reports to locate relevant characteristics and guide mask generation. These reports are routinely produced in clinical practice by expert medical professionals such as radiologists, and can provide a valuable source of semantic knowledge at little to no additional cost  In this paper, we propose a new approach called MedIM (Masked medical Image Modelling). MedIM aligns semantic correspondences between medical images and radiology reports and reconstructs regions masked by the guidance of learned correspondences. Especially we introduce two masking strategies: knowledge word-driven masking (KWM) and sentence-driven masking (SDM). KWM uses Medical Subject Headings (MeSH) words  Our contributions mainly include three-fold: (1) we present a novel masking approach MedIM, which is the first work to explore the potential of radiology reports in mask generation for medical images, offering a new perspective to enhance the accuracy and interpretability of medical image representation; ",vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,2.0,Approach,As shown in Fig. ,vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,2.1,Image and Text Encoders,Image Encoder. We use the vision Transformer (ViT) ,vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,2.2,Report-Guided Mask Generation,"We introduce two radiology report-guided masking strategies, i.e., KWM and SDM, identifying different cues to guide the mask generation.",vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,Knowledge Word-Driven Masking (KWM).,"MeSH words shown in Fig.  where N MeSH represents the number of MeSH words in the report r. Then, we compute an attention map C MeSH to identify image regions mapped to MeSH words as follows where H = W = N img , T and R represent the transpose and reshape functions, and the softmax function normalizes the elements along the image dimension to find the focused region matched to each MeSH word. The summation operation performs on the text dimension to aggregate the attentions related to all MeSH words. Subsequently, the high-activated masking is presented to remove the discovered attention regions. Here, we define a corresponding binary mask m ∈ {0, 1} H×W formulated as m (i,j) = I(C ). Here C [γ * Nimg] MeSH refers to the (γ * N img )-th largest activation in C MeSH , andγ is the masking ratio that determines how many activations would be suppressed. With this binary mask, we can compute the masked representations produced by KWM as where [MASK] is a masked placeholder.",vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,Sentence-Driven Masking (SDM).,"Medical reports often contain multiple sentences that describe different findings related to the image, which inspires SDM to introduce sentence-level information during mask generation. For the report r, we randomly select a sentence s and extract its representations as where N s represents the length of s. Then, an attention map C s can be computed to identify regions mapped to this sentence as After that, the high-activated masking is performed based on C s to compute the masked representations M(C s ; λ) sdm . We also select an image-report pair and visualize the corresponding attention map and generated mask procured by KWM and SDM in Fig. ",vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,2.3,Decoder for Reconstruction,Both masked representations M(C MeSH ; λ) kwm and M(C s ; λ) sdm are mapped to the decoder D(•) that includes four conv-bn-relu-upsample blocks. We design two independent reconstruction heads to respectively accept the decoded features D(M(C MeSH ; λ) kwm ) and D(M(C s ; λ) sdm ) and generate the final reconstruction results y kwm and y sdm .,vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,2.4,Objective Function,"MedIM creates a more challenging reconstruction objective by removing then restoring the most discriminative regions guided by radiological reports. We optimize this reconstruction learning process with the mean square error (MSE) loss function, expressed as MedIM also combines the cross-modal alignment constraint, which aligns medical images' visual and semantic aspects with their corresponding radiological reports, benefiting in better identifying the reported-guided discriminative regions during mask generation. We follow the work ",vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,2.5,Downstream Transfer Learning,"After pre-training, we can transfer the weight parameters of the MedIM to various downstream tasks. For the classification task, we use the commonly used Linear probing, i.e., freezing the pre-trained image encoder and solely training a randomly initialized linear classification head. For the segmentation task, the encoder and decoder are first initialized with the MedIM pre-trained weights, and a downstream-specific head is added to the network. The network is then fine-tuned end-to-end. For the retrieval task, we take an image or report as an input query and retrieve target reports or images by computing the similarity between the query and all candidates using the learned image and text encoders.",vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,3.1,Experimental Details,Pre-training Setup. We use the MIMIC-CXR-JPG dataset ,vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,Downstream Setup.,We validate the transferability of learned MedIM representations on four X-ray-based downstream tasks: (1) multi-label classification on CheXpert  (2) multi-class classification on COVIDx ,vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,3.2,Comparisons with Different Pre-training Methods,We compare the downstream performance of our MedIM pre-training with five pre-training methods in Table ,vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,3.3,Discussions,"Ablation Study. Ablation studies are performed over each component of MedIM, including knowledge word-driven masking (KWM) and Sentence-driven masking (SDM), as listed in Table  Masking Strategies. To demonstrate the effectiveness of the High-activated masking strategy, we compare it with three counterparts, No masking, Random masking, and Low-activated masking. Here No masking means that the recon- ",vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,Methods T2I I2T,R@1 R@5 R@10 R@1 R@5 R@10  struction is performed based on the complete image encoder representations instead of the masked one. Low-activated masking refers to masking the tokens exhibiting a low response in both KWM and SDM strategies. The comparison on the left side of Fig. ,vol1
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,4.0,Conclusion,"We propose a new masking approach called MedIM that uses radiological reports to guide the mask generation of medical images during the pre-training process. We introduce two masking strategies KWM and SDM, which effectively identify different sources of discriminative cues to generate masked inputs. MedIM is pre-trained on a large dataset of image-report pairs to restore the masked regions, and the learned image representations are transferred to three medical image analysis tasks and image-text/report-text retrieval tasks. The results demonstrate that MedIM outperforms strong pre-training competitors and the random masking method. In the future, we will extend our MedIM to handle other modalities, e.g., 3D medical image analysis.",vol1
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,1.0,Introduction,The success of supervised learning methods in the medical domain led to countless breakthroughs that might be translated into clinical routine and have the potential to revolutionize healthcare ,vol1
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Domain Transfer Method:,We present an entirely new sim-to-real transfer approach based on conditional invertible neural networks (cINNs) (cf. Fig. ,vol1
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Instantiation to Spectral Imaging:,We show that our method can generically be applied to two complementary modalities: photoacoustic tomography (PAT; image-level) and hyperspectral imaging (HSI; pixel-level).,vol1
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Comprehensive Validation:,"In comprehensive validation studies based on more than 2,000 PAT images (real: ∼1,000) and more than 6 million spectra for HSI (real: ∼6 million) we investigate and subsequently confirm our two main hypotheses: (H1) Our cINN-based models can close the domain gap between simulated and real spectral data better than current state-of-the-art methods regarding spectral plausibility. (H2) Training models on data transferred by our cINN-based approach can improve their performance on the corresponding (clinical) downstream task without them having seen labeled real data.",vol1
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,2.1,Domain Transfer with Conditional Invertible Neural Networks,"Concept Overview. Our domain transfer approach (cf. Fig.  Model Training. In the following, the proposed cINN with its parameters θ will be referred to as f (x, DY, θ) and its inverse as f -1 for any input x ∼ p D from domain D ∈ {D sim , D real } with prior density p D and its corresponding latent space variable z. The condition DY is the combination of domain label D as well as the tissue label Y ∈ {Y sim , Y real }. Then the maximum likelihood loss ML for a training sample x i is described by For the adversarial training, we employ the least squares training scheme  Finally, the full loss for the proposed model comprises the following: Model Inference. The domain transfer is done in two steps: 1) A simulated image is encoded in the latent space with conditions D sim and Y sim to its latent representation z, 2) z is decoded to the real domain via D real with the simulated tissue label Y sim :",vol1
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,2.2,Spectral Imaging Data,Photoacoustic Tomography Data. PAT is a non-ionizing imaging modality that enables the imaging of functional tissue properties such as tissue oxygenation  Hyperspectral Imaging Data. HSI is an emerging modality with high potential for surgery ,vol1
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,3.0,Experiments and Results,"The purpose of the experiments was to investigate hypotheses H1 and H2 (cf. Sect. 1). As comparison methods, a CycleGAN  Realism of Synthetic Data (H1) : According to qualitative analyses (Fig.  The dashed lines represent the mean difference value, and each dot represents the difference for one wavelength. A principal component analysis (PCA) performed on all artery and vein spectra of the real and synthetic datasets demonstrates that the distribution of the synthetic data is much closer to the real data after applying our domain transfer approach (cf. Fig. ",vol1
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Benefit of Domain-Transferred Data for Downstream Tasks (H2):,"We examined two classification tasks for which reference data generation was feasible: classification of veins/arteries in PAT and organ classification in HSI. For both modalities, we used the completely untouched real test sets, comprising 162 images in the case of PAT and ∼ 920,000 spectra in the case of HSI. For both tasks, a calibrated random forest classifier (sklearn  As shown in Table ",vol1
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,4.0,Discussion,"With this paper, we presented the first domain transfer approach that combines the benefits of cINNs (exact maximum likelihood estimation) with those of GANs (high image quality). A comprehensive validation involving qualitative and quantitative measures for the remaining domain gap and downstream tasks suggests that the approach is well-suited for sim-to-real transfer in spectral imaging. For both PAT and HSI, the domain gap between simulations and real data could be substantially reduced, and a dramatic increase in downstream task performance was obtained -also when compared to the popular UNIT approach. The only similar work on domain transfer in PAT has used a cycle GANbased architecture on a single wavelength with only photon propagation as PAT image simulator instead of full acoustic wave simulation and image reconstruction  The main limitation of our approach is the high dimensionality of the parameter space of the cINN as dimensionality reduction of data is not possible due to the information and volume-preserving property of INNs. This implies that the method is not suitable for arbitrarily high dimensions. Future work will comprise the rigorous validation of our method with tissue-mimicking phantoms for which reference data are available. In conclusion, our proposed approach of cINN-based domain transfer enables the generation of realistic spectral data. As it is not limited to spectral data, it could develop into a powerful method for domain transfer in the absence of labeled real data for a wide range of image modalities in the medical domain and beyond.",vol1
Anatomy-Driven Pathology Detection on Chest X-rays,1.0,Introduction,"Chest radiographs (chest X-rays) represent the most widely utilized type of medical imaging examination globally and hold immense significance in the detection of prevalent thoracic diseases, including pneumonia and lung cancer, making them a crucial tool in clinical care  We, therefore, propose a novel approach towards pathology detection that uses anatomical region bounding boxes, solely defined on anatomical structures, as proxies for pathology bounding boxes. These region boxes are easier to annotate -the physiological shape of a healthy subject's thorax can be learned relatively easily by medical students -and generalize better than those of pathologies, such that huge labeled datasets are available  -We propose anatomy-driven pathology detection (ADPD), a pathology detection approach for chest X-rays, trained with pathology classification labels together with anatomical region bounding boxes as proxies for pathologies. -We study two training approaches: using localized (anatomy-level) pathology labels for our model Loc-ADPD and using image-level labels with multiple instance learning (MIL) for our model MIL-ADPD. -We train our models on the Chest ImaGenome ",vol1
Anatomy-Driven Pathology Detection on Chest X-rays,2.0,Related Work,"Weakly Supervised Pathology Detection. Due to the scarcity of bounding box annotations, pathology detection on chest X-rays is often tackled using weakly supervised object detection with Class Activation Mapping (CAM)  In AGXNet ",vol1
Anatomy-Driven Pathology Detection on Chest X-rays,3.1,Model,"Figure  For predicting whether the associated region is present, we use a binary classifier with a single linear layer, for bounding box prediction we use a three-layer MLP followed by sigmoid. We consider the prediction of observed pathologies as a multi-label binary classification task and use a single linear layer (followed by sigmoid) to predict the probabilities of all pathologies. Each of these predictors is applied independently to each region with their weights shared across regions. We experimented with more complex pathology predictors like an MLP or a transformer layer but did not observe any benefits. We also did not observe improvements when using several decoder layers and observed degrading performance when using ROI pooling to compute region features.",vol1
Anatomy-Driven Pathology Detection on Chest X-rays,3.2,Inference,"During inference, the trained model predicts anatomical region bounding boxes and per-region pathology probabilities, which are then used to predict pathology bounding boxes in two steps, as shown in Fig. ",vol1
Anatomy-Driven Pathology Detection on Chest X-rays,3.3,Training,"The anatomical region detector is trained using the DETR loss  For our MIL-ADPD model, we experiment with a weaker form of supervision, where pathology classification labels are only available on the per-image level. We utilize multiple instance learning (MIL), where an image is considered a bag of individual instances (i.e. the anatomical regions), and only a single label (per pathology) is provided for the whole bag, which is positive if any of its instances is positive. To train using MIL, we first aggregate the predicted pathology probabilities of each region over all detected regions in the image using LSE pooling  In both models, the ASL loss is weighted by a factor of 0.01 before adding it to the DETR loss. We train using AdamW ",vol1
Anatomy-Driven Pathology Detection on Chest X-rays,3.4,Dataset,"Training Dataset. We train on the Chest ImaGenome dataset [4,21,22] We use the provided jpg-images  During training, we use random resized cropping with size 224 × 224, apply contrast and brightness jittering, random affine augmentations, and Gaussian blurring. Evaluation Dataset and Class Mapping. We evaluate our method on the subset of 882 chest X-ray images with pathology bounding boxes, annotated by radiologists, from the NIH ChestXray-8 (CXR8) dataset  The dataset contains bounding boxes for 8 unique pathologies. While partly overlapping with the training classes, a one-to-one correspondence is not possible for all classes. For some evaluation classes, we therefore use a many-to-one mapping where the class probability is computed as the mean over several training classes. We refer to the supp. material for a detailed study on class mappings.",vol1
Anatomy-Driven Pathology Detection on Chest X-rays,4.1,Experimental Setup and Baselines,We compare our method against several weakly supervised object detection methods (CheXNet  Table ,vol1
Anatomy-Driven Pathology Detection on Chest X-rays,,Method,"Supervision IoU@10-70 IoU@10 IoU@30 IoU@50 Box Class mAP AP loc-acc AP loc-acc AP loc-acc MIL-ADPD (ours) An Pa For all models, we only consider the predicted boxes with the highest box score per pathology, as the CXR8 dataset never contains more than one box per pathology. We report the standard object detection metrics average precision (AP) at different IoU-thresholds and the mean AP (mAP) over thresholds (0.1, 0.2, . . . , 0.7), commonly used thresholds on this dataset ",vol1
Anatomy-Driven Pathology Detection on Chest X-rays,4.2,Pathology Detection Results,"Comparison with Baselines. Table  For detailed results per pathology we refer to the supp. material. We found that the improvements of MIL-ADPD are mainly due to improved performance on Cardiomegaly and Mass detection, while Loc-ADPD consistently outperforms all baselines on all classes except Nodule, often by a large margin. Ablation Study. In Table  Qualitative Results. As shown in Fig. ",vol1
Anatomy-Driven Pathology Detection on Chest X-rays,5.0,Discussion and Conclusion,"Limitations. While our proposed ADPD method outperforms all competing models, it is still subject to limitations. First, due to the dependence on region proxies, for pathologies covering only a small part of a region, our models predict the whole region, as highlighted by their incapability to detect nodules. We however note that in clinical practice, chest X-rays are not used for the final diagnosis of such pathologies and even rough localization can be beneficial. Additionally, while not requiring pathology bounding boxes, our models still require supervision in the form of anatomical region bounding boxes, and Loc-ADPD requires anatomy-level labels. However, anatomical bounding boxes are easier to annotate and predict than pathology bounding boxes, and the used anatomylevel labels were extracted automatically from radiology reports ",vol1
Anatomy-Driven Pathology Detection on Chest X-rays,,Conclusion.,"We proposed a novel approach tackling pathology detection on chest X-rays using anatomical region bounding boxes. We studied two training approaches, using anatomy-level pathology labels and using image-level labels with MIL. Our experiments demonstrate that using anatomical regions as proxies improves results compared weakly supervised methods and supervised training on little data, thus providing a promising direction for future research.",vol1
Anatomy-Driven Pathology Detection on Chest X-rays,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_6.,vol1
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,1.0,Introduction,"Positron Emission Tomography (PET) is a 3D imaging modality using radiopharmaceuticals, such as F-18-fluorodeoxyglucose (FDG), as tracers. Newly introduced long axial field-of-view PET scanners have enabled dynamic PET (dPET) with frame duration < 1 min  Parametric images are reported to be superior in lesion detection and delineation when compared to standard-of-care activity-and weight-normalized static PET volumes, known as Standard Uptake Value (SUV) volumes  To address the problem of the generation of micro-parametric images, we propose a custom 3D UNet  -A self-supervised formulation of the problem of kinetic micro-parameters estimation -A spatio-temporal deep neural network for parametric images estimation -A quantitative and qualitative comparison with conventional methods for PBPK modeling The code is available at: https://github.com/FrancescaDB/self_supervised_PBPK_modelling.",vol1
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,1.1,Related Work,"Finding the parameters of a KM is a classical optimization problem  To limit the drawbacks of the non-linear parameter fitting, the identification of KPs is commonly performed using simplified linearized versions of the KM  Preliminary works towards KM parameter estimation in dPET imaging have recently begun to be explored. Moradi et al. used an auto-encoder along with a Gaussian process regression block to select the best KM to describe simulated kinetic data ",vol1
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2.0,Methodology and Materials,"We propose to compute the kinetic micro-parameters in a self-supervised setting by directly including the KM function in the loss function and comparing the predicted TAC to the measured TAC. For this reason, an understanding of the KM is fundamental to describing our pipeline.",vol1
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2.1,Kinetic Modelling,"The concentration of the tracer C(t) [Bq/ml] in each tissue can be described as a set of ordinary differential equations  where  , and solved using the Laplace transform  (2)",vol1
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2.2,Proposed Pipeline,"Our network takes as input a sequence of 2D axial slices and returns a 4-channel output representing the spatial distribution of the KM parameters of a 2TC for FDG metabolisation  We imposed that the KPs predicted by the network satisfy Eq. 2 by including it in the computation of the loss. At a pixel level, we computed the mean squared error between the TAC estimated using the corresponding predicted parameters ( TAC i ) and the measured one (TAC i ), as seen in Fig.  We introduced a final activation function to limit the output of the network to the valid parameter domain of the KM function. Using the multi-clamp function, each channel of the logits is restricted to the following parameter spaces: The limits of the ranges were defined based on the meaning of the parameter (as in V B ), mathematical requirements (as in the minimum values of k 2 and k 3 , whose sum can not be zero)  We evaluated the performance of the network using the Mean Absolute Error (MAE) and the Cosine Similarity (CS) between TAC i and TAC i .",vol1
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2.3,Curve Fit,"For comparison, parameter optimization via non-linear fitting was implemented in Python using the scipy.optimize.curve_fit function (version 1.10), with step equal to 0.001. The bounds were the same as in the DNN.",vol1
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2.4,Dataset,"The dataset is composed of 23 oncological patients with different tumor types. dPET data was acquired on a Biograph Vision Quadra for 65 min, over 62 frames. The exposure duration of the frames were 2 × 10 s, 30 × 2 s, 4 × 10 s, 8 × 30 s, 4 × 60 s, 5 × 120 s and 9 × 300 s. The PET volumes were reconstructed with an isotropic voxel size of 1.6 mm. The dataset included the label maps of 7 organs (bones, lungs, heart, liver, kidneys, spleen, aorta) and one image-derived input function A(t) [Bq/ml] from the descending aorta per patient. Further details on the dataset are presented elsewhere  The PET frames and the label map were resampled to an isotropic voxel size of 2.5 mm. Then, the dataset was split patient-wise into training, validation, and test set, with 10, 4, and 9 patients respectively. Details on the dataset split are available in the Supplementary Material (Table ",vol1
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,3.0,Results,Table ,vol1
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,,(Color figure online),"The most important design choice is the selection of the final activation function. Indeed, the multi-clamp final activation function was proven to be the best both in terms of CS (Exp 4.1: CS = 0.78 ± 0.05) and MAE (Exp 4.2: MAE = 3.27 ± 2.01). Compared to the other final activation functions, when the multi-clamp is used the impact of the max-pooling design is negligible also in terms of MAE. For the rest of the experiments, the selected configuration is the one from Exp. 4.1 (see Table  Figure  In terms of run-time, the DNN needed ≈ 1 min to predict the KPs of the a whole-body scan (≈ 400 slices), whereas curve fit took 8.7 min for a single slice: the time reduction of the DNN is expected to be ≈ 3.500 times.",vol1
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,4.0,Discussion,"Even though the choice of the final activation function has a greater impact, the selection of the kernel design is important. Using spatial and temporal convo- lution results in an increase in the performance (+0.01 in CS) and reduces the number of trainable parameters (from 2.1 M to 8.6 K), as pointed out by  An analysis per slice of the metrics shows that the CS between TAC i and TAC i changes substantially depending on the region: CS max = 0.87 within the liver boundaries and CS min = 0.71 in the region corresponding to the heart and lungs (see Fig.  Figure  The KP DNN are more homogeneous than KP CF , as can be seen in the exemplary K 1 axial slice shown in Fig.  The major limitation of this work is the lack of ground truth and a canonical method to evaluate quantitatively its performance. This limitation is inherent to PBPK modeling and results in the need for qualitative analyses based on expected physiological processes. A possible way to leverage this would be to work on simulated data, yet the validity of such evaluations strongly depends on how realistic the underlying simulation models are. As seen in Fig. ",vol1
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,5.0,Conclusion,"In this work, inspired by PINNs, we combine a self-supervised spatio-temporal DNN with a new loss formulation considering physiology to perform kinetic modeling of FDG dPET. We compare the best DNN model with the most commonly used conventional PBPK method, curve fit. While no ground truth is available, the proposed method provides similar results to curve fit but qualitatively more plausible images in physiology and with a radically shorter run-time. Further, our approach can be applied to other KMs without significantly increasing the complexity and the need for computational power. In general, Eq. 2 should be modified to represent the desired KM  Overall, this work offers scalability and a new research direction for analysing pharmacokinetics.",vol1
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_28.,vol1
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,1.0,Introduction,"Deep learning nowadays offers expert-level and sometimes even super-expertlevel performance, deepening and widening its applications in medical imaging and resulting in numerous public datasets for research, competitions, and challenges. These datasets are generally small as annotating medical images is challenging, but achieving superior performance by deep learning demands massive annotated data for training. For example, Google's proprietary CXR Foundation Model (CXR-FM) was trained on 821,544 labeled and mostly private CXRs  To address this need, we have developed a framework, called Ark for its ability of accruing and reusing knowledge embedded in heterogeneous expert annotations with numerous datasets, as illustrated in Fig.  This performance enhancement is attributed to a simple yet powerful observation that aggregating numerous public datasets costs nearly nothing but enlarges data size, diversifies patient populations, and accrues expert knowledge from a large number of sources worldwide; thereby offering unprecedented performance yet reducing annotation cost. More important, Ark is fundamentally different from self-supervised learning (SSL) and federated learning (FL) in concept. SSL can naturally handle images from different sources, but their associated expert annotations are left out of pretraining  Through this work, we have made the following contributions: (1) An idea that aggregates public datasets to enlarge and diversify training data; (2) A student-teacher model with multi-task heads via cyclic pretraining that accrues expert knowledge from existing heterogeneous annotations to achieve superior and robust performance yet reduce annotation cost; (3) Comprehensive experiments that evaluate our Ark via fine-tuning, linear-probing, and few-shot learning on a variety of target tasks, demonstrating Ark's better generalizability and transferability in comparison with SOTA methods and Google CXR-FM; and (4) Empirical analyses for a critical yet often overlooked aspect of medical imaging models-robustness to underdiagnosis and gender imbalance, highlighting Ark significantly enhances reliability and safety in clinical decision-making. Table ",vol1
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,2.0,Accruing and Reusing Knowledge,"Our Ark aims to learn superior and robust visual representations from largescale aggregated medical images by accruing and reusing the expert knowledge embedded in all available heterogeneous labels. The following details our Ark. Accruing Knowledge into the Student via Cyclic Pretraining. A significant challenge with training a single model using numerous datasets created for different tasks is label inconsistency (i.e., heterogeneity) (see Table  Accruing Knowledge into the Teacher via Epoch-Wise EMA. To further summarize the accrued knowledge and accumulate the learning experiences in the historical dimension, we introduce into Ark a teacher model that shares the same architecture with the student. The teacher is updated using exponential moving average (EMA)  Reusing Accrued Knowledge from the Student to Bolster Cyclic Pretraining. If the model learns from multiple tasks sequentially, it may ""forget"" the previously learned knowledge, and its performance on an old task may degrade catastrophically  Reusing Accrued Knowledge from the Teacher to Mitigate Forgetting. To leverage the accumulated knowledge of the teacher model as an additional self-supervisory signal, we incorporate a consistency loss between the student and the teacher, as shown in Fig. ",vol1
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,,Ark has the following properties:,"• Knowledge-centric. Annotating medical images by radiologists for deep learning is a process of transferring their in-depth knowledge and expertise in interpreting medical images and identifying abnormalities to a medium that is accessible for computers to learn. Ark's superior and robust performance is attributed to the accumulation of expert knowledge conveyed through medical imaging annotations from diverse expert sources worldwide. At the core of Ark is acquiring and sharing knowledge: ""knowledge is power"" (Mac Flecknoe) and ""power comes not from knowledge kept but from knowledge shared"" (Bill Gates). • Label-agnostic, task-scalable and annotation-heterogeneous. Ark is label-agnostic as it does not require prior label ""understanding"" of public datasets, but instead uses their originally-provided labels. It is designed with pluggable multi-task heads and cyclic pretraining to offer flexibility and scalability for adding new tasks without manually consolidating heterogeneous labels or training task-specific controllers/adapters ",vol1
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,3.0,Experiments and Results,Our Ark-5 and Ark-6 take the base version of the Swin transformer (Swin-B) ,vol1
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,,Results and Analysis:,As shown in Table ,vol1
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,3.2,Ark Provides Generalizable Representations for Segmentation Tasks,"Experimental Setup: To evaluate the generalizability of Ark's representations, we transfer the Ark models to five segmentation tasks involving lungs, heart, clavicles, and ribs, and compare their performance with three SOTA fully/selfsupervised models. We build the segmentation network upon UperNet ",vol1
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,,Results and Analysis:,"As seen in Table  For instance, a pneumothorax can be detected by observing a visible ""visceral pleural line"" along part or all of the length of the lateral chest wall ",vol1
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,3.3,Ark Offers Embeddings with Superior Quality over Google CXR-FM,"Experimental Setup: To highlight the benefits of learning from more detailed diagnostic disease labels, we compare our Ark models with Google CXR-FM. CXR-FM was trained on a large dataset of 821,544 CXRs from three different sources, but with coarsened labels (normal or abnormal). By contrast, our Ark models are trained with less data, but aims to fully utilize all labels provided by experts in the original datasets. Furthermore, Ark models employ a much smaller backbone (88M parameters) compared with CXR-FM using EfficientNet-L2 (480M parameters). Since Google CXR-FM is not released and cannot be finetuned, we resorted to its released API to generate the embeddings (informationrich numerical vectors) for all images in the target tasks. For the sake of fairness, we also generated the embeddings from Ark's projector, whose dimension is the same as Google's. To evaluate the quality of the learned representations of these models, we conduct linear probing by training a simple linear classifier for each target task. The performance of both models is evaluated on six target tasks, including an unseen dataset, 10.SIIM, where the images have not been previously seen by the Ark models during pretraining. Additionally, we perform the same evaluation on 10.SIIM with partial training sets or even few-shot samples to further demonstrate the high quality of our Ark models' embeddings. shows that both Ark-5 and Ark-6 consistently outperform CXR-FM in small data regimes, highlighting the superiority of Ark's embeddings, which carry richer information that can be utilized more efficiently. These results demonstrate that Ark models learn higher-quality representations with less pretraining data while employing a much smaller backbone than CXR-FM, highlighting that learning from more granular diagnostic labels, such as Ark, is superior to learning from coarsened normal/abnormal labels.",vol1
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,3.4,Ark Shows a Lower False-Negative Rate and Less Gender Bias,"Experimental Setup: Underdiagnosis can lead to delayed treatment in healthcare settings and can have serious consequences. Hence, the false-negative rate (FNR) is a critical indicator of the robustness of a computer-aided diagnosis (CAD) system. Furthermore, population-imbalanced data can train biased models, adversely affecting diagnostic performance in minority populations. Therefore, a robust CAD system should provide a low false-negative rate and strong resilience to biased training data. To demonstrate the robustness of our Ark models in comparison with Google CXR-FM, we first compute the FNRs in terms of gender on 1.CXPT and 2.NIHC. We further investigate gender biases in Ark-6 and CXR-FM on 1.CXPT using gender-exclusive training sets. We follow the train/test splits in ",vol1
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,4.0,Conclusions and Future Work,"We have developed Foundation Ark, the first open foundation model, that realizes our vision: accruing and reusing knowledge retained in heterogeneous expert annotations with numerous datasets offers superior and robust performance. Our experimental results are strong on CXRs, and we plan to extend Ark to other modalities. We hope Ark's performance encourages researchers worldwide to share codes and datasets big or small for creating open foundation models, accelerating open science, and democratizing deep learning for medical imaging.",vol1
LOTUS: Learning to Optimize Task-Based US Representations,1.0,Introduction,"Ultrasound (US) imaging is a widely used modality in medical diagnosis for screening and follow-up examinations. Hence, precise segmentation of the target organs is crucial for diagnosing or tracking disease progression. Recently, the application of deep learning for ultrasound image segmentation has emerged as a powerful tool. However, accurate segmentation of US images remains a challenging task due to the complexity of the modality, as it has limited resolution and often contains clutter, shadowing and reverberation artefacts. This leads to a general lack of annotated data, and additionally, due to varying operator skills, there is high heterogeneity of ground truth data labels, which is the primary factor hampering solid segmentation performance  On the other hand, large pixel-level labeled CT datasets are freely available online. Thus to overcome the lack of ground truth ultrasound data, researchers have utilized ultrasound simulators to generate large sets of ultrasound-like images from CT label maps and use them for training  Generally, ultrasound simulators can be categorized into two types based on their modeling techniques: finite difference models of the wave equation, modeling the mechanical propagation of sound waves through tissues, and simulating ray casting through tissue maps represented by ultrasound tissue properties  Thus, one main challenge when working with simulated data is reducing the domain shift between simulated and real data. In a supervised sense, many works have investigated the realistic parametrization of ultrasound simulators to reduce the domain shift between simulated and real data ultrasound data  However, those methods require separate training for each part of the architecture, limiting the models' flexibility. Notably,  Contributions. In this paper, we propose a novel approach for learning to optimize task-based ultrasound image representations. During training, we render an intermediate US image representation from segmented public CT scans and use it as input to a segmentation network. Our ultrasound renderer is fully differentiable and learns to optimize the parameters necessary for physics-based ultrasound simulation, guided by the downstream segmentation task. At the same time, we train an image style transfer network between real and simulated data to achieve simultaneous image synthesis as well as automatic segmentation on US images in an end-to-end training setting. In addition, no labels are required for the real ultrasound images, which are also unpaired with the simulated ultrasound images. We evaluate our method on aorta and vessel segmentation. Our quantitative and qualitative results demonstrate that our method learns the optimal image for the task of interest. The source code for our method is publicly available",vol1
LOTUS: Learning to Optimize Task-Based US Representations,2.1,Differentiable Ultrasound Renderer,"where R i (d) is the energy reflected from the interfaces between two tissues as the beam passes through them and B i (d) represents the energy backscattered from the scattering points along the scanline. The reflection of the ray is described as: where I i (d) is the remaining energy of the ray, which gets attenuated during tissue traversal. We model I i (d) by approximating the Beer-Lambert Law as: , where α is the attenuation coefficient of the medium and d the distance travelled. To construct the final 2D attenuation map, we calculate, for each ray, the cumulative product of the attenuation as it traverses through various tissues, thereby modeling how the signal's strength diminishes. The reflection coefficient Z = (Z 2 -Z 1 ) 2 /(Z 2 +Z 1 ) 2 , is computed from the acoustic impedances of two adjacent tissues: Z 1 and Z 2 . The P (d) is the Point Spread Function (PSF) along the ray, and G(d) is a boundary map, where 1 is assigned for points on the boundary of the surface and 0 otherwise. For simplicity, we model the PSF as a two-dimensional normalized Gaussian. The amount of the reflected signal, denoted by φ r , equals the result of multiplying the reflection coefficient by the boundary condition. To build our final 2D reflection map, for each ray, we compute the cumulative product of the residual signal, defined as 1φ r . The output represents the fraction of the signal that propagates forward. In additionally to the reflection term, a backscattered energy term B i (d) in the returning echo is calculated: the residual ultrasound wave energy I i (d) is multiplied with the PSF P (d), which has been convolved with a texture T of random scatterers for each (x, y), where: This texture is constructed using two random textures T 0 (x, y) and T 1 (x, y) with Gaussian normalized distributions and the parameters μ 0 , μ 1 , and σ 0 , which represent the brightness, density and standard deviation of scatterers respectively. To make the function fully differentiable, we replace the conditional operation T 1 ≤ μ 1 with a differentiable approximation: where σ(z) = 1 1+e -z is the sigmoid function and β is a scaling factor that adjusts its steepness. The resulting function is fully differentiable as the sigmoid function smoothly approximates the step function and all operations involved are differentiable. Additionally, we apply temporal gain compensation (TGC) to enhance tissues deeper in the image. The final rendered ultrasound image is constructed from the three sub-maps (see Fig. ",vol1
LOTUS: Learning to Optimize Task-Based US Representations,2.2,End-to-End Learning,"The proposed method's architecture is shown in Fig.  where the generated images G(x) resemble images from domain Y, and D(.) differentiates between translated and real images y. However, the adversarial loss alone does not ensure that the translated image will preserve the structure of the anatomy. An additional contrastive loss must be imposed, which maximizes mutual information across corresponding image patches from the source and the output image. We use the Patch Sampler from CUT to extract image patches and calculate the contrastive NCE (L NCE ) loss  where, the L NCE is calculated on two pairs, a sample from the source domain (x) paired with the generated output G(x) and a sample from the target domain (y) paired with the G(y) which we denote as the identity image. The loss over the second pair serves as an identity loss and prevents the generator from making unnecessary changes to the image. CT Labelmap → Segmentation: The segmentation network forward pass has a nested structure. First, we obtain a 2D slice from the CT label map and pass it to the differentiable ultrasound renderer. The resulting rendered US is passed through the frozen Generator network, and the identity image output of the Generator is used as an input to the segmentation network to ensure the same distribution as the target domain. We update both the segmentation network and the Renderer using dice loss. The label for computing the dice loss comes directly from the input label map used for generating the rendered US. Stopping Criterion: Once the segmentation network validation loss converges, we employ a small subset of 10 labeled images from the real US domain as a stopping indicator for the entire training pipeline.",vol1
LOTUS: Learning to Optimize Task-Based US Representations,3.0,Experimental Setup,"CT Dataset: We use 12 CT volumes from a publicly available dataset Synapse In-vivo Images: We acquired abdominal ultrasound sweeps from eleven volunteers of age 26 ± 3 (m:7/f:4). For each person, one sweep was acquired with a convex probe  Training Details: We train the network with a learning rate of 10 -5 for the segmentation network, 10 -3 for the US Renderer, and 5 -6 for the image adaptation network, with a batch size of 1, Adam optimizer and dice loss. We employ rotation, translation, and scaling augmentations on the CT label maps and split them randomly in an 80-20% ratio for training and validation, respectively. For the supervised approach, we trained the networks, for 120 epochs, with a learning rate of 10 -3 and the Adam optimizer. Experiments. We test the proposed framework quantitatively for two segmentation tasks: all vessels and the aorta only. We evaluate the accuracy of the proposed method by comparing it to a supervised network. For this, we train a 5-fold cross-validation U-Net ",vol1
LOTUS: Learning to Optimize Task-Based US Representations,4.0,Results and Discussion,"In Table  For the task of vessels segmentation it also achieved the best DSC of 90.9 ± 0.06. Figure  The results presented in this work demonstrate the effectiveness of LOTUS for segmenting organs in ultrasound images. Our physics-based simulator generates synthetic training data, which is especially useful in scenarios where obtaining labeled data is time-consuming or costly. We believe that learning from transferred labels from CT contributes to a more accurate model since CT data is more accessible and labels are more refined. Our quantitative results indicate that LOTUS can achieve accurate segmentation of aorta boundaries and other vessels. Furthermore, the end-to-end framework enables the differentiable US renderer and the unsupervised image translation to get optimized dynamically during the training. Thus, the intermediate representation image is not static but changes during the training. This illustrates the adaptivity of the proposed method to the downstream task, highlighting its prospective applicability across diverse applications and anatomies. Moreover, rather than directly using the rendered US image as an input to the segmentation network, we use the identity image output from the Generator. This yielded significant improvement in the segmentation result as it learns from a distribution consistent with the reconstructed US while looking similar to the rendered US. As a result, during inference stage, the distribution of the translated real US is closer to the distribution the segmentation network was trained on, thereby improving the performance of the model. One of the challenges when employing generative adversarial networks is that the loss is not an indicator of the best result. We determine the optimal model by utilizing a small subset of labeled images after the convergence of the segmentation network, to ensure robustness during inference. Further stopping criteria can be studied to achieve higher automation of the pipeline. Currently, our model incorporates the basic physics of ultrasound imaging without considering artifacts explicitly. Thus, exploring the robustness of the method against artifacts could yield valuable future improvements.",vol1
LOTUS: Learning to Optimize Task-Based US Representations,5.0,Conclusion,"This paper presents a novel approach to learning task-based ultrasound image representations. LOTUS leverages CT labelmaps to simulate ultrasound data via differentiable ray-casting. The proposed ultrasound simulator is fully differentiable and learns to optimize the parameters for generating physics-based ultrasound images guided by the downstream segmentation task. We also introduce an image adaptation network to achieve simultaneous image synthesis and automatic segmentation on US images in an end-to-end training setting without needing paired real and simulated images. Our method is evaluated on aorta and vessel segmentation tasks and shows promising quantitative results. Furthermore, we demonstrate the potential of our approach for other organs through qualitative results of optimized image representations. The ability to learn from unlabeled data and simulate the ultrasound modality has the potential for various clinical tasks beyond segmentation. We believe that our work has the potential to improve ultrasound imaging interpretation and learning.",vol1
LOTUS: Learning to Optimize Task-Based US Representations,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_42.,vol1
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,1.0,Introduction,"The advancement of deep learning models heavily depends on the availability of large-scale datasets with high-quality annotated labels  To overcome this issue, this paper proposes a robust Semi-supervised Contrastive Learning (SSCL) paradigm, that simultaneously benefits from semisupervised learning and contrastive learning for combating the medical noisy labels and promoting stability and robustness of the diagnostic model. Three important components, i.e., the Mixup Feature Embedding (MFE) module, the Semi-supervised Learning (SSL) module, and the Similarity Contrastive Learning (SCL) module, are proposed in the SSCL framework. The architecture of the SSCL framework is shown in Fig.  -This paper presents a robust semi-supervised contrastive learning paradigm that effectively incorporates semi-supervised learning and contrastive learning to mitigate the effect of medical label noise. Our approach represents the first attempt to address this issue in the field of medical image analysis. -The pseudo-labeling promotion strategy can re-correct the supervised information of noisy labels, while the pair-wise selection strategy can guide the confident pairs to dominate the contrasting learning process. -The proposed SSCL framework is evaluated on multiple benchmark datasets, and extensive experiments demonstrate the generalization performance of our method in comparison with state-of-the-art baselines. 2 Related Work",vol1
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,2.1,Conventional Methods with Noisy Labels,"To eliminate the memorization effect of noise labels in the training phase, recent works ",vol1
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,2.2,Semi-supervised Learning,"Compared with the existing learning methods, semi-supervised learning ",vol1
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,2.3,Contrastive Learning,"With the advancement of deep learning, researchers have found the potential of contrastive-based similarity learning frameworks for representation learning ",vol1
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,3.1,Mixup Feature Embedding,"Mixup. Let D = {(x i , y i )} N i denotes the training minibatch of image-label pairs x i and y i , where N is the batch size. Initially, the operations of data augmentation are first conducted in the MFE module, to generate various levels of hybrid augmented images with Mixup  where λ ∈ [0, 1]∼Beta(α l , α r ) is used to control the mixing strength of training samples, x a and x b are the training samples randomly drawn from each minibatch, and x i is the enhanced image generated by the mixup preprocessing. Feature Embedding. By taking the corresponding augmented images as inputs, the MFE module aims to capture abstract distributed feature representations. The MFE module consists of different branches, including a deep encoder with projection and classifier heads, and a momentum encoder with an information bottleneck (IB)  where θ denotes the encoder parameters, and m ∈ [0, 1) is a momentum coefficient. Specifically, the feature representations Q would be mapped to the samedimensional representations Q by the projection head, while K is used to predict the categorical outputs with classifier. The feature representation V is derived from the momentum encoder, possessing identical dimensions to Q. Moreover, we also utilize Kullback-Leibler divergence ",vol1
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,3.2,Semi-Supervised Learning,"Selecting Confident Samples. The main core of the proposed SSL module is to recognize the confident samples with clean labels, and re-correct the supervision information of label noise. To this end, we first select confident samples based on their confidence score provided by the classifier. Denote the confident samples with clean label belonging to n-th class as where p i ∈ [0, 1] is the classification probability of the enhanced image x i , and γ n is a dynamic confidence threshold for the n-th class to ensure a class-balanced set of identified confident examples.",vol1
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,,Pseudo-Labeling.,"To generate accurate supervision signals, a flexible pseudolabeling promotion strategy is introduced to replace noisy labels with pseudolabels, Benefiting from the unique semi-supervised learning structure, our SSL module can effectively reduce the impact of noise based on statistical classification. The objective function for semi-supervised learning is defined as: where ω ∈ (0, 1] is a tunable focusing parameter, which is utilized to exploit the benefits of both the noise-robustness and the implicit weighting scheme.",vol1
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,3.3,Similarity Contrastive Learning,"Selecting Confident Pairs. To achieve a precise estimation of noisy pairs, a novel pair-wise selection strategy is proposed to identify the reliable confident pairs out of noisy pairs. By calculating their similarity distribution of representation learning, the SCL module can transform identified confident examples into a set of associated confident pairs S without knowing noise rates, where τ is considered as a confidence threshold. Therefore, the objective loss on each sample pair for contrastive learning can be defined as: Consistent with Eq. 3, a symmetry loss function is applied for each minibatch, Queue. It is noted that blindly increasing the size of the minibatch will be limited by computing resources  Objective Loss of SSCL. Based on the analysis of the above modules, the total objective loss for the proposed SSCL method can be obtained by, where α and β are loss weight. In our experiments, both α and β are set to 0.25.",vol1
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,4.1,Implementation Details and Settings,"By randomly replacing labels for a percentage of the training data with all possible labels, the proposed SSCL method is extensively validated on four benchmarks with symmetric noise. ISIC-19 ",vol1
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,4.2,Comparisons with the State-of-the-arts,"In this part, we evaluate the performance of the proposed SSCL framework with classification accuracy under different label noise rates (NR). As shown in Table ",vol1
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,4.3,Parameter Analysis and Ablation Studies and Visualizations,"As the key hyperparameter in Eq. 7, the threshold τ is designed to reduce the wrong sample pairs to achieve the best classification boundary construction. In this part, we empirically conduct the proposed SSCL framework with a range of different values τ . As shown in Fig. ",vol1
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,5.0,Conclusion,"This paper presents a robust and reliable semi-supervised contrastive learning method that benefits greatly from the potential synergistic efficacy of semisupervised learning and contrastive learning, which aims to tackle the challenge of learning with medical noisy labels. By explicitly selecting confident samples and pairs, our approach exhibits a powerful ability to learn discriminative feature representations, mitigating the impact of medical label noise. To demonstrate the effectiveness and versatility of our proposed approach in various practical scenarios, our future works would extend the proposed SSCL method to a broader range of real-world noisy datasets and tasks.",vol1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,1.0,Introduction,"Deep learning techniques have greatly improved medical image segmentation by automatically extracting specific tissue or substance location information, which facilitates accurate disease diagnosis and assessment. However, most deep learning approaches for segmentation require fully or partially labeled training datasets, which can be time-consuming and expensive to annotate. To address this issue, recent research has focused on developing segmentation frameworks that require little or no segmentation labels. To meet this need, many researchers have devoted their efforts to Weakly-Supervised Semantic Segmentation (WSSS)  The literature has not adequately addressed the issue of low-resolution Class-Activation Maps (CAMs), especially for medical images. Some existing methods, such as dilated residual networks  In this paper, we propose an Attentive Multiple-Exit CAM (AME-CAM) for brain tumor segmentation in magnetic resonance imaging (MRI). Different from recent CAM methods, AME-CAM uses a classification model with multipleexit training strategy applied to optimize the internal outputs. Activation maps from the outputs of internal classifiers, which have different resolutions, are then aggregated using an attention model. The model learns the pixel-wise weighted sum of the activation maps by a novel contrastive learning method. Our proposed method has the following contributions: -To tackle the issues in existing CAMs, we propose to use multiple-exit classification networks to accurately capture all the internal activation maps of different resolutions. -We propose an attentive feature aggregation to learn the pixel-wise weighted sum of the internal activation maps. -We demonstrate the superiority of AME-CAM over state-of-the-art CAM methods in extracting segmentation results from classification networks on the 2021 Brain Tumor Segmentation Challenge (BraTS 2021) ",vol1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,2.0,Attentive Multiple-Exit CAM (AME-CAM),"The proposed AME-CAM method consists of two training phases: activation extraction and activation aggregation, as shown in Fig.  In the activation aggregation phase, we create an efficient hierarchical aggregation method to generate the aggregated activation map M f by calculating the pixel-wise weighted sum of the activation maps M i . We use an attention network A(•) to estimate the importance of each pixel from each activation map. The attention network takes in the input image I masked by the activation map and outputs the pixel-wised importance score S xyi of each activation map. We formulate the operation as follows: where [•] is the concatenate operation, n(•) is the min-max normalization to map the range to [0,1], and ⊗ is the pixel-wise multiplication, which is known as image masking. The aggregated activation map M f is then obtained by the pixel-wise weighted sum of M i , which is We train the attention network with unsupervised contrastive learning, which forces the network to disentangle the foreground and the background of the aggregated activation map M f . We mask the input image by the aggregated activation map M f and its opposite (1 -M f ) to obtain the foreground feature and the background feature, respectively. The loss function is defined as follows: where v f i and v b i denote the foreground and the background feature of the i-th sample, respectively. SimM in and SimM ax are the losses that minimize and maximize the similarity between two features (see C 2 AM  Finally, we average the activation maps M 1 to M 4 and the aggregated map M f to obtain the final CAM results for each image. We apply the Dense Conditional Random Field (DenseCRF) ",vol1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,3.1,Dataset,We evaluate our method on the Brain Tumor Segmentation challenge (BraTS) dataset ,vol1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,3.2,Implementation Details and Evaluation Protocol,"We implement our method in PyTorch using ResNet-18 as the backbone classifier. We pretrain the classifier using SupCon  We use the Dice score and Intersection over Union (IoU) to evaluate the quality of the semantic segmentation, following the approach of Xu et al.  Interested readers can refer to the supplementary material for results on other network architectures.  ",vol1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,4.1,Quantitative and Qualitative Comparison with State-of-the-Art,"In this section, we compare the segmentation performance of the proposed AME-CAM with five state-of-the-art weakly-supervised segmentation methods, namely Grad-CAM  Compared to the unsupervised baseline (UL), C&F is unable to separate the tumor and the surrounding tissue due to low contrast, resulting in low dice scores in all experiments. With pixel-wise labels, the dice of supervised C&F improves significantly. Without any pixel-wise label, the proposed AME-CAM outperforms supervised C&F in all modalities. The fully supervised (FSL) Optimized U-net achieves the highest dice score and IoU score in all experiments. However, even under different levels of supervision, there is still a performance gap between the weakly supervised CAM methods and the fully supervised state-of-the-art. This indicates that there is still potential room for WSSS methods to improve in the future. Qualitatively, Fig. ",vol1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,Effect of Different Aggregation Approaches:,"In Table  As a baseline, we first conducted the average of four activation maps generated by the multiple-level activation extraction (Avg. ME). We then applied C 2 AM  Table ",vol1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,Selected Exit,"Dice Effect of Single-Exit and Multiple-Exit: Table  The comparisons show that the activation map obtained from the shallow layer M 1 and the deepest layer M 4 result in low dice scores, around 0.15. This is because the network is not deep enough to learn the tumor region in the shallow layer, and the resolution of the activation map obtained from the deepest layer is too low to contain sufficient information to make a clear boundary for the tumor. Results of the internal classifiers from the middle of the network (M 2 and M 3 ) achieve the highest dice score and IoU, both of which are around 0.5. To evaluate whether using results from all internal classifiers leads to the highest performance, we further apply the proposed method to the two internal classifiers with the highest dice scores, i.e., M 2 and M 3 , called M 2 + M 3 . Compared with using all internal classifiers (M 1 to M 4 ), M 2 + M 3 results in 18.6% and 22.1% lower dice and IoU, respectively. In conclusion, our AME-CAM still achieves the optimal performance among all the experiments of single-exit and multiple-exit. Other ablation studies are presented in the supplementary material due to space limitations.",vol1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,5.0,Conclusion,"In this work, we propose a brain tumor segmentation method for MRI images using only class labels, based on an Attentive Multiple-Exit Class Activation Mapping (AME-CAM). Our approach extracts activation maps from different exits of the network to capture information from multiple resolutions. We then use an attention model to hierarchically aggregate these activation maps, learning pixel-wise weighted sums. Experimental results on the four modalities of the 2021 BraTS dataset demonstrate the superiority of our approach compared with other CAM-based weakly-supervised segmentation methods. Specifically, AME-CAM achieves the highest dice score for all patients in all datasets and modalities. These results indicate the effectiveness of our proposed approach in accurately segmenting brain tumors from MRI images using only class labels.",vol1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 17.,vol1
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,1.0,Introduction,"Despite the remarkable advancements achieved through the use of deep learning for automatic medical image segmentation, the scarcity of precisely annotated training data remains a significant obstacle to the widespread adoption of such techniques in clinical settings. As a solution, the concept of semi-supervised segmentation has been proposed to enable models to be trained using less annotated but abundant unlabeled data. Recently, methods that adopt the co-teaching  With the rapid advancement of semi-supervised learning, the importance of unlabeled data has garnered increased attention across various disciplines in recent years. However, the role of labeled data has been largely overlooked, with the majority of semi-supervised learning techniques treating labeled data supervision as merely an initial step of the training pipeline or as a means to ensure training convergence  Based on the aforementioned conception, we propose a novel Correlation Aware Mutual Learning (CAML) framework to explicitly model the relationship between labeled and unlabeled data to effectively utilize the labeled data. Our proposed method incorporates two essential components, namely the Crosssample Mutual Attention module (CMA) and the Omni-Correlation Consistency module (OCC), to enable the effective transfer of labeled data information to unlabeled data. The CMA module establishes mutual attention among a group of samples, leading to a mutually reinforced representation of co-salient features between labeled and unlabeled data. Unlike conventional methods, where supervised signals from labeled and unlabeled samples are separately back-propagated, the proposed CMA module creates a new information propagation path among each pixel in a group of samples, which synchronously enhances the feature representation ability of each intra-group sample. In addition to the CMA module, we introduce the OCC module to regularize the segmentation model by explicitly modeling the omni-correlation between unlabeled features and a group of labeled features. This is achieved by constructing a memory bank to store the labeled features as a reference set of features or basis vectors. In each iteration, a portion of features from the memory bank is utilized to calculate the omni-correlation with unlabeled features, reflecting the similarity relationship of an unlabeled pixel with respect to a set of basis vectors of the labeled data. Finally, we constrain the omni-correlation matrix of each sub-model to be consistent to regularize the entire framework. With the proposed omni-correlation consistency, the labeled data features serve as anchor groups to guide the representation learning of the unlabeled data feature and explicitly encourage the model to learn a more unified feature distribution among unlabeled data. In summary, our contributions are threefold: ",vol1
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,2.1,Overview,"Figure  where l o represents the proposed omni-correlation consistency loss, while L s and l c are the supervised loss and the cross-supervised loss implemented in the Cross Pseudo Supervision(CPS) module. λ c and λ o are the weights to control l c and l o separately. During the training procedure, a batch of mixed labeled and unlabeled samples are fed into the network. The supervised loss is only applied to labeled data, while all samples are utilized to construct cross-supervised learning. Please refer to ",vol1
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,2.2,Cross-Sample Mutual Attention Module,"To enable information propagation through any positions of any samples in a mini-batch, one can simply treat each pixel's feature vector as a token and perform self-attentions for all tokens in a mini-batch. However, this will make the computation cost prohibitively large as the computation complexity of selfattention is O(n 2 ) with respect to the number of tokens. We on the other hand adopt two sequentially mounted self-attention modules along different dimensions to enable computation efficient mutual attention among all pixels. As illustrated in Fig.  In CAML, we employ the proposed CMA module in the auxiliary segmentation network f a , whereas the vanilla segmentation network f v remains the original V-Net structure. The reasons can be summarized into two folds. From deployment perspective, the insertion of the CMA module requires a batch size of large than 1 to model the attention among samples within a mini-batch, which is not applicable for model inference(batchsize=1). From the perspective of model design, we model the vanilla and the auxiliary branch with different architectures to increase the architecture heterogeneous for better performance in a mutual learning framework.",vol1
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,2.3,Omni-Correlation Consistency Regularization,"In this chapter, we introduce Omni-Correlation Consistency (OCC) to formulate additional model regularization. The core of the OCC module is omnicorrelation, which is a kind of similarity matrix that is calculated between the feature of an unlabeled pixel and a group of prototype features sampled from labeled instances features. It reflects the similar relationship of an unlabeled pixel with respect to a set of labeled reference pixels. During the training procedure, we explicitly constrain the omni-correlation calculated using heterogeneous unlabeled features from those two separate branches to remain the same. In practice, we use an Omni-correlation matrix to formulate the similarity distribution between unlabeled features and the prototype features. Let g v and g a denote two projection heads attached to the backbones of f v and f a separately, and z v ∈ R m×c and z a ∈ R m×c represent two sets of embeddings sampled from their projected features extracted from unlabeled samples, where m is the number of sampled features and c is the dimension of the projected features. It should be noted that z v and z a are sampled from the embeddings corresponding to the same set of positions on unlabeled samples. Suppose z p ∈ R n×c represents a set of prototype embeddings sampled from labeled instances, where n represents the number of sampled prototype features, the omni-correlation matrix calculation between z v and z p can be formulated as: where cos means the cosine similarity and t is the temperature hyperparameter. sim vp ∈ R m×n is the calculated omni-correlation matrix. Similarly, the similarity distribution sim ap between z a and z p can be calculated by replacing z v with z a . To constrain the consistency of omni-correlation between dual branches, the omni-correlation consistency regularization can be conducted with the crossentropy loss l ce as follows: Memory Bank Construction. We utilize a memory bank T to iteratively update prototype embeddings for OCC computation. Specifically, T initializes N slots for each labeled training sample and updates prototype embeddings with filtered labeled features projected by g v and g a . To ensure the reliability of the features stored in T , we select embeddings on the positions where both f v and f a have the correct predictions and update T with the mean fusion of the projected features projected by g v and g a . For each training sample, following  Embeddings Sampling. For computation efficiency, omni-correlation is not calculated on all labeled and unlabeled pixels. Specifically, we have developed a confidence-based mechanism to sample the pixel features from the unlabeled data. Practically, to sample z v and z a from unlabeled features, we first select the pixels where f v and f a have the same prediction. For each class, we sort the confidence scores of these pixels, and then select features of the top i pixels as the sampled unlabeled features. Thus, m = i × C, where C represents the number of classes. With regards to the prototype embeddings, we randomly sample j embeddings from each class among all the embeddings contained in T and n = j × C to increase its diversity.",vol1
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,3.0,Experiments and Results,"Dataset. Our method is evaluated on the Left Atrium (LA) dataset  Implementation Details. We implement our CAML using PyTorch 1.8.1 and CUDA 10.2 on an NVIDIA TITAN RTX GPU. For training data augmentation, we randomly crop sub-volumes of size 112 × 112 × 80 following  Quantitative Evaluation and Comparison. Our CAML is evaluated on four metrics: Dice, Jaccard, 95% Hausdorff Distance (95HD), and Average Surface Distance (ASD). It is worth noting that the previous researchers reported results (Reported Metrics in Table  The results on LA are presented in Table  It is evident from Table ",vol1
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,,Ablation Study.,"In this section, we analyze the effectiveness of the proposed CMA module and OCC module. We implement the MC-Net as our baseline, which uses different up-sampling operations to introduce architecture heterogeneity. Table ",vol1
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,4.0,Conclusion,"In this paper, we proposed a novel framework named CAML for semi-supervised medical image segmentation. Our key idea is that cross-sample correlation should be taken into consideration for semi-supervised learning. To this end, two novel modules: Cross-sample Mutual Attention(CMA) and Omni-Correlation Consistency(OCC) are proposed to encourage efficient and direct transfer of the prior knowledge from labeled data to unlabeled data. Extensive experimental results on the LA dataset demonstrate that we outperform previous state-of-the-art results by a large margin without extra computational consumption in inference.",vol1
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,,Acknowledgements,. This work is funded by the ,vol1
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,1.0,Introduction,"Ultrasound (US) imaging has been widely used in clinical diagnosis due to its advantages of safety, repeatability, and real-time imaging. Compared with 2D US, 3D US can provide more comprehensive spatial information. Freehand 3D US can enhance the understanding of physicians about the scanned region of interest without increasing the complexity of scanning  Sensorless freehand 3D US reconstructs the volume by calculating the relative transformation of a series of US images. Previous studies were mainly based on speckle decorrelation  Due to the low cost, small size, and low power consumption of micro-electromechanical-systems (MEMS), the sensor called inertial measurement unit (IMU) has been widely used in navigation systems. Prevost et al.  In this study, we propose a multi-IMU-based online self-consistency network (OSCNet) for freehand 3D US reconstruction. Our contribution is two-fold. First, we equip multiple IMUs (see Fig. ",vol1
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,2.0,Methodology,"Figure  In the training phase, we input an N -length scanning sequence , where θ i includes 3-axis translations t i = (t x , t y , t z ) i and rotation angles φ i = (φ x , φ y , φ z ) i between image I i and I i+1 . The multiple IMU data consists of M independent IMU data The pre-processing process for Φ i and A i is consistent with ",vol1
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,2.1,Modal-Level Self-supervised Strategy,"Multiple IMUs mounted in different directions provide diverse measurement constraints for the model's estimation, as shown in Fig.  As shown in the top of Fig.  where Cov, σ and • 1 denote the covariance, the standard deviation, and L1 normalization, respectively. As shown in the bottom of Fig.  where ( tj i-1 ) -1 represents the translations in the inversion of θj i-1 . Similar to  In addition, the consistency among multiple IMU data itself also provides the possibility to improve the reconstruction performance. It constrains the backbone to obtain similar estimated parameters for different IMU data inputs from the same scan. Specifically, we construct multi-IMU consistency constraints as:",vol1
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,2.2,Sequence-Level Self-consistency Strategy,"Consistent context should lead to consistent parameter estimation, which constrains the model at the sequence level, reducing the estimation instability caused by scanning differences such as frame rates. Inspired by contrastive learning  we construct an online sequence-level self-consistency strategy (SCS). SCS randomly generates sub-sequences with consistent context for each scan. The hierarchical consistency constraint among the generated sub-sequences and the original sequence improves the reconstruction performance of the backbone. Specifically, as shown in Fig.  where H τ converts the parameters, sequences, or IMU data under interval sampling and flipping operation τ . B denotes the backbone.",vol1
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,3.0,Experiments,"Materials and Implementation. The equipment we used to collect data includes a portable US machine, four IMU sensors (WT901C-232, WitMotion) and an electromagnetic (EM) positioning system. The US images were acquired with a linear probe at 10 MHz, and the depth was set at 4 cm. As shown in Fig.  The arm and carotid datasets were randomly divided into 200/40/48 and 150/30/36 scans based on volunteer level to construct training/validation/test set. To prevent overfitting and enhance the model's robustness, we performed random augmentations on each scan, including sub-sequence intercepting, interval sampling, and sequence inversion. We randomly augmented each training scan to 20 sequences and each test scan to 10 sequences to simulate complex real-world situations. We used the Adam optimizer to optimize the OSCNet. During the training phase, the epochs and batch size are set to 200 and 1, respectively. To avoid overfitting, we set the initial learning rate to 2 × 10 -4 and used a learning rate decay strategy that halves the learning rate every 30 epochs. During the online learning phase, the iteration epoch and learning rate  are set to 60 and 2 × 10 -6 , respectively. All code was implemented in PyTorch and executed on an RTX 3090 GPU. Quantitative and Qualitative Analysis. To demonstrate the effectiveness of our OSCNet, we compared it with three state-of-the-art methods, including CNN ",vol1
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,4.0,Conclusion,"In this study, we propose a novel multi-IMU-based online self-consistency network (OSCNet) to conduct freehand 3D US reconstruction. We propose an online modal-level self-supervised strategy (MSS) that integrates multiple IMUs to reduce the influence of single IMU noise and enhance reconstruction performance. We propose an online sequence-level self-consistency strategy (SCS) to improve the reconstruction stability using hierarchical consistency among the generated sub-sequences and the original sequence. The experimental results on the arm and carotid datasets show that our OSCNet achieves state-of-the-art reconstruction performance. Future research will focus on exploring more general reconstruction methods.",vol1
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,1.0,Introduction,"Medical VQA is a specialized domain of VQA that aims to generate answers to natural language questions about medical images. It is very challenging to train deep learning based medical VQA models from scratch, since the medical VQA datasets available for research are relatively small in scale. Many existing works are proposed to leverage pre-trained visual encoders with external datasets to solve downstream medical VQA tasks, such as utilizing denoising autoencoders  Unlike unimodal pretraining approaches, both image and text feature presentations can be enhanced by learning through the visual and language interactions, given relatively richer resources of medical image caption datasets  In this paper, we proposed a new self-supervised vision language pre-training (VLP) approach that applied Masked image and text modeling with Unimodal and Multimodal Contrastive losses (MUMC) in the pre-training phase for solving downstream medical VQA tasks. The model was pretrained on image caption datasets for aligning visual and text information, and transferred to downstream VQA datasets. The unimodal and multimodal contrastive losses in our work are applied to (1) align image and text features; (2) learn unimodal image encoders via momentum contrasts of different views of the same image (i.e. different views are generated by different image masks); (3) learn unimodal text encoder via momentum contrasts. We also introduced a new masked image strategy by randomly masking the patches of the image with a probability of 25%, which serves as a data augmentation technique to further enhance the performance of the model. Our approach outperformed existing methods and sets new benchmarks on three medical VQA datasets ",vol1
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,2.0,Methods,"In this section, we provide the detailed description of the proposed approach, which includes the network architectures, self-supervised pre-training objectives, and the way to fine-tune on downstream medical VQA tasks.",vol1
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,2.1,Model Architecture,"In the pre-training phase, the network architecture comprises an image encoder, a text encoder, and a multimodal encoder, which are all based on the transformer architecture ",vol1
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,2.2,Unimodal and Multimodal Contrastive Losses,"The proposed self-supervised objective attempts to capture the semantic discrepancy between positive and negative samples across both unimodal and multimodal domains at the same time. The unimodal contrastive loss (UCL) aims to differentiate between examples of one modality, such as images or text, in a latent space to make similar examples close. And the multimodal contrastive loss (MCL) learns the alignments between both modalities by maximizing the similarity between images and their corresponding text captions, while separating from the negative examples. In the implementation, we maintain two momentum models for image and text encoders respectively to generate different perspectives or representations of the same input sample, which serve as positive samples for contrastive learning. In detail, we denote the image and caption embeddings from the unimodal image encoder and text encoder as v cls and t cls , which are further processed through the transformations g v and g t , to normalize and map the image and text embeddings to be lowerdimensional representations. The embeddings are inserted into a lookup table, and only the most recent 65,535 pairs of image-text embedding are stored for contrastive learning. We utilize the momentum update technique originally proposed in MoCo  where s denotes cosine similarity function, and τ is a learnable temperature parameter.",vol1
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,2.3,Image Text Matching,"We adopt the image text matching (ITM) strategy similar to prior works  the function H (, ) represents a cross-entropy computation, where y itm denotes the groundtruth label and p itm (V , T ) is a function for predicting the class.",vol1
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,2.4,Masked Language Modeling,"Masked Language Modeling (MLM) is another pre-trained objective in our approach, that predicts masked tokens in text based on both the visual and unmasked contextual information. For each caption text, 15% of tokens are randomly masked and replaced with the special token,  where H (, ) is a cross-entropy calculation, T denotes the masked text token, y mlm represents the ground-truth of the masked text token and p mlm (V , T ) is the predicted probability of a masked token.",vol1
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,2.5,Masked Image Strategy,"Besides the training objectives, we introduce a masked image strategy as a data augmentation technique. In our experiment, input images are partitioned into patches which are randomly masked with a probability of 25%, and only the unmasked patches are passed through the network. Unlike the previous methods ",vol1
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,3.1,Datasets,"Our model is pre-trained on three datasets: ROCO  For the downstream medical VQA task, we fine-tune and validate the model on three public medical VQA datasets: VQA-RAD  There are two types of questions: closed-ended questions that have limited answer choices (e.g. ""yes"" or ""no"") and open-ended questions that VQA models are required to generate answers in free text, which are more challenging.",vol1
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,3.2,Implementation Details,"Our method was implemented in Python 3.8 and PyTorch 1.10. The experiments were conducted on a server with an Intel Xeon(R) Platinum 8255C and 2 NVIDIA Tesla V100 GPUs with 32 GB memory each. We pre-trained our model on three medical image caption datasets for 40 epochs with a batch size of 64. AdamW  For downstream medical VQA tasks, we fine-tuned our model for 30 epochs with a batch size of 8. We used the AdamW optimizer with a reduced learning rate of 2e -5 , which decayed to 1e -8 . Besides, we increased image inputs from a resolution of 256 × 256 to 384 × 384 and interpolated the positional encoding following ",vol1
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,3.3,Comparison with the State-of-the-Arts,We performed a comparative evaluation of our model against the existing approaches ,vol1
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,3.4,Ablation Study,"To further verify the effectiveness of the proposed methods in learning multimodal representations, we conducted an ablation study across all three medical VQA datasets. Table ",vol1
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,3.5,Visualization,We utilized Grad-CAM ,vol1
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,4.0,Conclusion,"In this paper, we propose a new method to tackle the challenge of medical VQA tasks, which is pre-trained on the medical image caption datasets and then transferred to the downstream medical VQA tasks. The proposed self-supervised pre-training approach with unimodal and multimodal contrastive losses leads to significant performance improvement on three public VQA datasets. Also, using masked images as a data augmentation technique is proven to be effective for learning representations on medical visual and language tasks. As a result, our proposed method not only outperformed the state-of-the-art methods by a significant margin, but also demonstrated the potential for model interpretability.",vol1
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,1.0,Introduction,"Computed tomography (CT) is a common tool for medical diagnosis but increased usage has led to concerns about the possible risks caused by excessive radiation exposure. The well-known ALARA (as low as reasonably achievable)  Recently, deep learning (DL) has been introduced for low-dose computed tomography (LDCT) image restoration. The utilization of convolutional neural networks (CNNs) for image super-resolution, as described in  This study presents a novel unsupervised framework for denoising low-dose CT (LDCT) images, which utilizes contrastive learning (CL) and doesn't require paired data. Our approach possesses three major contributions as follows: Firstly, We discard the use of CycleGAN that most unpaired frameworks employ, instead adopting contrastive learning to design the training framework. As a result, the training process becomes more stable and imposes a lesser computational burden. Secondly, our approach can adapt to almost all end-to-end image translation neural networks, demonstrating excellent flexibility. Lastly, the proposed interslice consistency loss makes our model generates stable output quality across slices, in contrast to most slice based methods that exhibit inter-slice instability. Our model outperforms almost all other models in this regard, making it the superior option for LDCT denoising. Further experimental data about this point will be presented in this paper.",vol1
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,2.0,Method,"LDCT image denoising can be expressed as a noise reduction problem in the image domain as x = f (x), where x and x denote the denoised output and corresponding LDCT image. f represents the denoising function. Rather than directly denoising LDCT images, an encoder-decoder model is used to extract important features from the LDCT images and predict corresponding NDCT images. Most CNN-based LDCT denoising models are based on supervised learning and require both the LDCT and its perfectly paired NDCT images to learn f . However, it is infeasible in real clinical practice. Currently, some unsupervised models, including CUT and CycleGAN, relax the constraint on requiring paired data for training. Instead, these models can be trained with unpaired data.",vol1
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,2.1,Contrastive Learning for Unpaired Data,"The task of LDCT image denoising can be viewed as an image translation process from LDCT to NDCT. CUT provides a powerful framework for training a model to complete image-to-image translation tasks. The main concept behind CUT is to use contrastive learning for enhanced feature extraction aided by an adversarial loss. The key principle of contrastive learning is to create positive and negative pairs of samples, in order to help the model gain strong feature representation ability. The loss of contrastive learning can be formulated as: where v, v + , v -denote the anchors, positive and negative pairs, respectively. N is the number of negative pairs. τ is the temperature factor which is set to 0.07 in this paper. The generator G we used contains two parts, an encoder E and a decoder. A simple MLP H is used to module the features extracted from the encoder. The total loss of CUT for image translation is defined as: where D denotes the discriminator. X represents the input images, for which L P atchNCE (G, H, X) utilizes contrastive learning in the source domain (represented by noisy images). Y indicates the images in the target domain, which means NDCT images in this paper. L P atchNCE (G, H, Y ) employs contrastive learning in this target domain. As noted in a previous study  As shown in Fig. ",vol1
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,2.2,Contrastive Learning for Inter-slice Consistency,"Due to various constraints, most denoising methods for LDCT can only perform on the slice plane, resulting in detail loss among different slices. While 3D models can mitigate this issue to a certain degree, they require significant computational costs and are prone to model collapse during training, leading to a long training time. Additionally, most methods are unable to maintain structural consistency between slices with certain structures (e.g., bronchi and vessels) appearing continuously across several adjacent slices. To address this issue, we design an inter-slice consistency loss based on contrastive learning. This approach helps to maintain structural consistency between slices, and then improve the overall denoising performance. As illustrated in Fig.  where P denotes the patch selection function. A good denoising generator can minimize the feature difference between similar slices while maximizing the feature difference between different slices. By utilizing contrastive learning, we can treat the former condition as a positive pair and the latter as a negative pair. After computing the cosine similarity of the pairs, a softmax operation is applied to assign 1 to the positive pairs and 0 to the negative pairs. Compared to the original contrastive learning, which focuses on patch pairs, we apply this technique to measure feature differences, which stabilizes the features and improves the consistency between slices. ",vol1
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,2.3,Boosted Contrastive Learning,"Original contrastive Learning approaches treat every positive and negative pair equally. However, in CT images, some patches may be very similar to others (e.g., patches from the same organ), while others may be completely different. Therefore, assigning the same weight to different pairs may not be appropriate.  For our inter-slice consistency loss, only one positive and negative pair can be generated at a time, making it unnecessary to apply reweighting. However, we include additional negative pairs in the patchNCE loss for unpaired translation, making reweighting between pairs more critical than in the original CUT model. As a result, Eq. 1 is updated as follows: where w stands for a weight factor for each negative patch. According to  In summary, the less similar two patches are, the easier they can be distinguished, the more weight the pair is given for learning purposes.",vol1
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,3.1,Dataset and Training Details,"While our method only requires unpaired data for training, many of the compared methods rely on paired NDCT. We utilized the dataset provided by the Mayo Clinic called ""NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge""  The model parameters were initialized using a random Gaussian distribution with zero-mean and standard deviation of 10 -2 . The learning rate for the optimizer was set to 10 -4 and halved every 5 epochs for 20 epochs total. The experiments were conducted in Python on a server with an RTX 3090 GPU. Two metrics, peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) ",vol1
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,3.2,Comparison of Different Methods,"To demonstrate the denoising performance of our model, we conducted experiments to compare our method with various types of denoising methods including unsupervised denoising methods that only use LDCT data, fully supervised methods that use perfectly registered LDCT and NDCT pairs, and semisupervised methods, including CycleGAN and CUT, which utilize unpaired data. A representative slice processed by different methods is shown in Fig.  Our framework is flexible and can work with different autoencoder frameworks. In our experiments, the well-known residual encoder-decoder network (RED) was adopted as our network backbone. The quantitative results and computational costs of unsupervised methods are presented in Table  As shown in Table  Moreover, our framework is lightweight, which has a similar model scale to RED. It's worth noting that adding perceptual loss to our model will decrease the PSNR result, and it is consistent with the previous studies that perceptual loss may maintain more details but decrease the MSE-based metric, such as PSNR. Furthermore, the reweighting mechanism demonstrates its effectiveness in improving our model's results. The improvement by introducing the reweighting mechanism can be easily noticed.  ",vol1
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,3.3,Line Plot over Slices,"Although our method may only be competitive with supervised methods, we are able to demonstrate the effectiveness of our proposed inter-slice consistency loss. The line plot in Fig.  In Fig. ",vol1
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,3.4,Discussion,"Our method achieves competitive results and obtains the highest PSNR value in all the methods with unpaired samples. Although we cannot surpass supervised methods in terms of some metrics, our method produces promising results across consecutive slices that are more consistent and closer to the GT.",vol1
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,4.0,Conclusion,"In this paper, we introduce a novel low-dose CT denoising model. The primary motivation for this work is based on the fact that most CNN-based denoising models require paired LD-NDCT images, while we usually can access unpaired CT data in clinical practice. Furthermore, many existing methods using unpaired samples require extensive computational costs, which can be prohibitive for clinical use. In addition, most existing methods focus on a single slice, which results in inconsistent results across consecutive slices. To overcome these limitations, we propose a novel unsupervised method based on contrastive learning that only requires a single generator. We also apply modifications to the original contrastive learning method to achieve SOTA denoising results using relatively a low computational cost. Our experiments demonstrate that our method outperforms existing SOTA supervised, semi-supervised, and unsupervised methods in both qualitative and quantitative measures. Importantly, our framework does not require paired training data and is more adaptable for clinical use.",vol1
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,1.0,Introduction,"Chest X-ray (CXR) is the most commonly performed diagnostic radiograph in medicine, which helps spot abnormalities or diseases of the airways, blood vessels, bones, heart, and lungs. Given the complexity and workload of clinical CXR reading, there is a growing interest in developing automated methods for anatomical abnormality detection in CXR  To completely relieve the burden of annotation, a few works  In this paper, we present a co-evolutionary image and report distillation (CEIRD) framework for semi-supervised anatomical abnormality detection in CXR, incorporating the weak supervision by radiology reports. Above all, on the basis of TSD  To summarize, our contributions include: (1) the complementary RPDLR and APCLR for noise reduction in both vision and language pseudo labels for improved semi-supervised training via mutual grounding, (2) the co-evolution strategy for joint optimization of the primary and auxiliary tasks, and (3) the SA-NMS for dynamic intra-image-modal pseudo label refinement, all contributing to the superior performance of the proposed CEIRD framework.",vol1
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,2.0,Method,"Problem Setting. In semi-supervised anatomical abnormality localization, a data set comprising both unlabeled samples i=1 is provided for training, where x and r are a CXR and accompanying report, respectively, A i = {(y l , B l )} is the annotation for a labeled sample including both bounding boxes {B l } and corresponding categories {y l }, and N l N u for practical use scenario. It is worth noting that {y l } can also be considered as classification labels for the report. The objective is to obtain a detection model that can accurately localize and classify the abnormalities in any testing CXR (without report in practice), by making good use of both the labeled and unlabeled CXRs plus the accompanying reports in the training set. detection in CXR is given, together with a pretrained language model F R s for multi-label abnormality classification of reports. On the one hand, we generate for an unlabeled image x u i pseudo detection labels with F I t and filter the pseudo labels by self-adaptive non-maximum suppression (NMS). Meanwhile, we feed the corresponding report r u i into F R s and use the prediction for report-guided pseudo detection label refinement (RPDLR). To this end, we obtain refined pseudo labels to supervise the student vision model F I s toward better anatomical abnormality localization. On the other hand, we also pass the detection predictions by F I s to a teacher language model F R t for abnormality-guided pseudo classification label refinement (APCLR), to better supervise the student language model F R s on unlabeled data for report-based abnormality classification. In turn, the better language model F R s helps train better vision models via RPDLR, thus both types of models co-evolve during training. Note that the real labels are used to train both student models along with the pseudo ones. After training, we only need the student vision model F I s for abnormality localization in testing CXRs.",vol1
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,,Preliminary Pseudo Label Distillation for Semi-supervised Learning.,"Both of our baseline semi-supervised vision and language models follow the teacher-student knowledge distillation (TSD) procedure  where L R cls is the cross-entropy loss, {ŷ} = F R s (r) is the prediction by the student model, {y pr } = F R t (r u ) is the set of pseudo labels generated by the teacher model. In each batch, labeled and unlabeled instances are sampled according to a controlled ratio. The resulting report classification model F R s will be utilized later to help with the primary task of abnormality detection in CXR. Similarly, a student vision model F I s for abnormality detection in CXR is trained in semisupervised setting by distilling from a teacher vision model F I t trained on labeled CXRs, with the loss function: where {(ŷ, B)} = F I s (x) are the predictions by the student model, {(y pv , B pv )} = F I t (x u ) are the pseudo class and bounding box labels generated by the teacher model, L I cls is the focal loss  Self-adaptive Non-maximum Suppression. During the TSD, the teacher vision model F I t is kept fixed. While its knowledge suffices for guiding the student vision model F I s in the early stage of TSD, it may somehow impede the learning of F I s when F I s gradually improves by also learning from the large amount of unlabeled data. Therefore, to gradually improve quality and robustness of the pseudo detection labels as F I s learns, we propose to perform self-adaptive non-maximum suppression (SA-NMS) to combine the pseudo labels {(y pv , B pv )} output by F I t and the predictions {(ŷ, B)} by F I s in each mini batch. Specifically, we perform NMS on the combined set of the pseudo labels and predictions: {(y cv , B cv )} = NMS {(y pv , B pv )} {(ŷ, B)} , and replace {(y pv , B pv )} in Eq. (2) with {(y cv , B cv )} for supervision by unlabeled CXRs. In this way, highly confident predictions by the maturing student can rectify imprecise ones by the teacher, leading to better supervision signals stemming from unlabeled data. Report-Guided Pseudo Label Refinement. In routine clinics, almost every radiograph in archive is accompanied by a report describing findings, abnormalities (if any), and diagnosis. Compared with the captions of natural images, the report texts constitute a unique (to medical image analysis) and rich source of extra information in addition to the image modality. To this end, we propose report-guided pseudo detection label refinement (RPDLR) to make use of this cross-modal information for semi-supervised anatomical abnormality detection in CXR. Specifically, we use the student language model F R s (trained with Eq. (  Eventually, we train the student vision model F I s using {(y v , B v )} in Eq. ( ",vol1
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,,Co-evolutionary Semi-supervised Learning with Cycle Pseudo Label,"Refinement. As the auxiliary student language model F R s plays an important role in RPDLR, it is reasonable to optimize its performance which in turn would benefit the primary task of abnormality detection. Therefore, we further propose an inverse, abnormality-guided pseudo classification labels refinement (APCLR) to help with semi-supervised training of the report classification model. Similarly in concept to the RPDLP, given a pair of unlabeled image x u and report r u , we obtain the set of abnormalities {(ŷ, B)} detected in x u by the student vision model F I s , and the set of classification pseudo labels {y pr } generated for r u by the teacher language model F R t . We retain only the pseudo labels {y pr j |y pr j ∈ {ŷ}}, by excluding the report-classified abnormalities not detected in the paired CXR. Ideally, one should use an optimal report classification model for refinement of the abnormality detection pseudo labels, and vice versa. However, the two models are mutually dependent on each other in a circle. To solve this dilemma, we implement an alternative co-evolution strategy to refine the abnormality detection and report classification pseudo labels iteratively, in generations. As shown in Fig. ",vol1
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,3.0,Experiments,"Dataset and Evaluation Metrics. We conduct experiments on the chest radiography dataset MIMIC-CXR  Implementation. The PyTorch  For report classification, we employ the BERT-base uncased model  Our implementation and hyper-parameters follow the official settings  Comparison with State-of-the-Art (SOTA) Methods. We compare our proposed co-evolution image and report distillation (CEIRD) framework with several up-to-date detection methods, including weakly supervised: CAM  The results are shown in Table  Ablation Study. We conduct ablation studies on the validation data to investigate efficacy of the novel building elements of our CEIRD framework, including: report-guided pseudo detection label refinement (RPDLR), co-evolution strategy (CoE) with abnormality-guided pseudo classification label refinement (APCLR), and self-adaptive non-maximum suppression (SA-NMS). We use the preliminary teacher-to-student pseudo label distillation as baseline (Eq. ( ",vol1
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,4.0,Conclusion,"In this work, we proposed a new co-evolutionary image and report distillation (CEIRD) framework for semi-supervised anatomical abnormality detection in chest X-ray. On the basis of a preliminary teacher-student pseudo label distillation, we first presented self-adaptive NMS to mingle highly confident predictions by both the teacher and student for improved pseudo labels. We then proposed report-guided pseudo detection label refinement (RPDLR) that used abnormalities classified from the accompanying radiology reports by an auxiliary language model to eliminate unmatched pseudo labels. Meanwhile, we further proposed an inverse, abnormality-guided pseudo classification label refinement (APCLR) making use of the abnormalities detected in X-ray images for better language model training. In addition, we implemented a co-evolution strategy that looped the RPDLR and APCLR to iteratively optimize the main vision detection model and auxiliary report classification model in an alternative manner. Experimental results showed that our CEIRD framework achieved superior performance to up-to-date semi-/weakly-supervised methods.",vol1
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 35.,vol1
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,1.0,Introduction,"Resting-state functional magnetic resonance imaging (rs-fMRI) has been increasingly used to help us understand pathological mechanisms of neurological disorders by revealing abnormal or dysfunctional brain connectivity patterns  To this end, we propose a modularity-constrained dynamic representation learning (MDRL) framework for interpretable brain disorder analysis with rs-fMRI. As shown in Fig. ",vol1
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,2.1,Subjects and Image Preprocessing,"Two public datasets (i.e., ABIDE  All rs-fMRI data were preprocessed using the Data Processing Assistant for Resting-State fMRI (DPARSF) pipeline ",vol1
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,2.2,Proposed Method,As shown in Fig. ,vol1
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,Dynamic Graph Construction.,"Considering that brain functional connectivity (FC) patterns change dynamically over time  The original feature for the j-th node is represented by the j-th row in X t for segment t. Considering all connections in an FC network may include some noisy or redundant information, we retain the top 30% strongest edges in each FC network to generate an adjacent matrix A t ∈ {0, 1} N ×N for segment t. Thus, the obtained dynamic graph sequence of each subject can be described as",vol1
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,Modularity-Constrained Spatiotemporal GNN.,"With the dynamic graph sequence {G t } T t=1 as input, we design a modularity-constrained spatiotemporal graph neural network (MSGNN) to learn interpretable and discriminative graph embeddings, with two unique constraints: 1) a modularity constraint, and 2) a graph topology reconstruction constraint. In MSGNN, we first stack two graph isomorphism network (GIN) layers  where ψ is nonlinear activation, ε (i) is a parameter at the i-th GIN layer, I is an identity matrix, and W (i) is the weight for the fully connected layers in GIN.",vol1
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,1) Modularity Constraint.,"It has been demonstrated that the central executive network (CEN), salience network (SN) and default mode network (DMN) are three crucial neurocognitive modules in the brain and these three modules have been consistently observed across different individuals and experimental paradigms, where CEN performs high-level cognitive tasks (e.g., decision-making and rule-based problem-solving), SN mainly detects external stimuli and coordinates brain neural resources, and DMN is responsible for self-related cognitive functions  Based on such prior knowledge and clinical experience, we reasonably assume that the learned embeddings of nodes within the same neurocognitive module tend to be similar. We develop a novel modularity constraint to encourage similarity between paired node-level embeddings in the same module. Mathematically, the proposed modularity constraint is formulated as: where h t,k i and h t,k j are embeddings of two nodes in the k-th module (with N k ROIs) at segment t, and K is the number of modules (K = 3 in this work). With Eq. ( ",vol1
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,2) Graph Topology Reconstruction Constraint.,"To further enhance discriminative ability of learned embeddings, we propose to preserve graph topology by reconstructing adjacent matrices. For the segment t, its adjacent matrix A t can be reconstructed through Ât = σ(H t • H t ), where σ is a nonlinear mapping function. The graph topology reconstruction constraint is then formulated as: where Ψ is a cross-entropy loss function. We then apply an SERO operation  3) Temporal Feature Learning . To further capture temporal information, a single-head transformer is used to fuse features derived from T segments, with a self-attention mechanism to model temporal dynamics across segments. We then sum the learned features {h i } T i=1 to obtain the whole-graph embedding. Prediction and Biomarker Detection. The whole-graph embedding is fed into a fully connected layer with Softmax for prediction, with final loss defined as: where L C is a cross-entropy loss for prediction, and λ 1 and λ 2 are two hyperparameters. To facilitate interpretation of our learned graph embeddings, we calculate PC coefficients between paired node embeddings for each segment and average them across segments to obtain an FC network for each subject. The upper triangle of each FC network is flattened into a vector and Lasso  Implementation. The MDRL is implemented in PyTorch and trained using an Adam optimizer (with learning rate of 0.001, training epochs of 30, batch size of 8 and τ = 20). We set window size Γ = 40 for NYU and Γ = 70 for the rest, and results of MDRL with different Γ values are shown in Supplementary Materials. In the modularity constraint, we randomly select m = 50% of all N k (N k -1) 2 paired ROIs in the k-th module (with N k ROIs) to constrain the MDRL.",vol1
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,3.0,Experiment,"Competing Methods. We compare the MDRL with 2 shallow methods: 1) linear SVM with node-level statistics (i.e., degree centrality, clustering coefficient, betweenness centrality, and eigenvector centrality) of FC networks as fMRI features (with each FC network constructed using PC), 2) XGBoost with the same features as SVM; and 4 state-of-the-art (SOTA) deep models with default architectures: 3) GCN  Experimental Setting. Three classification tasks are performed: 1) ASD vs. HC on ABIDE, 2) MDD vs. HC on MDD, and 3) ANI vs. HC on HAND. A 5-fold cross-validation (CV) strategy is employed. Within each fold, we also perform an inner 5-fold CV to select optimal parameters. Five evaluation metrics are used: area under ROC curve (AUC), accuracy (ACC), sensitivity (SEN), specificity (SPE), and balanced accuracy (BAC). Paired sample t-test is performed to evaluate whether the MDRL is significantly different from a competing method. Classification Results. Results achieved by different methods in three classification tasks on three datasets are reported in Tables 1-2 and Fig.  Ablation Study. We compare the proposed MDRL with its three degenerated variants: 1) MDRLw/oM without the modularity constraint, 2) MDRLw/oR without the graph topology reconstruction constraint, and 3) MDRLw/oMR without the two constraints. The results are reported in Fig. ",vol1
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,4.0,Discussion,"Parameter Analysis. To investigate the influence of hyperparameters, we vary the values of two parameters (i.e., λ 1 and λ 2 ) in Eq. (  Influence of Modularity Ratio. In the main experiments, we randomly select m = 50% of all N k (N k -1) 2 paired ROIs in the k-th module (with N k ROIs) to constrain the MDRL. We now vary the modularity ratio m within [0%, 25%, • • • , 100%] and record the results of MDRL in three tasks in Fig.  Influence of Network Construction. We use PC to construct the original FC networks in MDRL. We also use sparse representation (SR) and low-rank representation (LR) for network construction in MDRL and report results in Table ",vol1
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,5.0,Conclusion and Future Work,"In this work, we propose a modularity-constrained dynamic graph representation (MDRL) framework for fMRI-based brain disorder analysis. We first construct dynamic graphs for each subject and then design a modularity-constrained GNN to learn spatiotemporal representation, followed by prediction and biomarker detection. Experimental results on three rs-fMRI datasets validate the superiority of the MDRL in brain disease detection. Currently, we only characterize pairwise relationships of ROIs within 3 prominent neurocognitive modules (i.e., CEN, SN, and DMN) as prior knowledge to design the modularity constraint in MDRL. Fine-grained modular structure and disease-specific modularity constraint will be considered. Besides, we will employ advanced harmonization methods ",vol1
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,1.0,Introduction,"In the medical domain, obtaining a large amount of high-confidence labels, such as histopathological diagnoses, is arduous due to the cost and required technicality. It is however possible to obtain lower confidence assessments for a large amount of images, either by a clinical questioning, or directly by a radiological diagnosis. To take advantage of large volumes of unlabeled or weakly-labeled images, pre-training encoders with self-supervised methods showed promising results in deep learning for medical imaging  Naive extensions of contrastive learning methods, such as  We apply our method to the classification of histology-proven liver cirrhosis, with a large volume of (weakly) radiologically-annotated CT-scans and a small amount of histopathologically-confirmed cirrhosis diagnosis. We compare the proposed approach to existing self-supervised methods.",vol1
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,2.0,Method,"Let x t be an input 2D image, usually called anchor, extracted from a 3D volume, y t a corresponding discrete weak variable and d t a related continuous variable. In this paper, y t refers to a weak radiological annotation and d t corresponds to the normalized depth position of the 2D image within its corresponding 3D volume: if V max corresponds to the maximal depth-coordinate of a volume V , we compute d t = pt Vmax with p t ∈ [0, V max ] being the original depth coordinate. Let x - j and x + i be two semantically different (negative) and similar (positive) images with respect to x t , respectively. The definition of similarity is crucial in CL and is the main difference between existing methods. For instance, in unsupervised CL, methods such as SimCLR  where , with sim a similarity function defined here as sim(a, b) = a T b τ with τ > 0. In the presence of discrete labels y, the definition of negative (x - j ) and positive (x + i ) samples may change. For instance, in SupCon  , where K σ is, for instance, a Gaussian kernel, with user defined hyper-parameter σ and 0 ≤ w σ ≤ 1. It is interesting to notice that, for discrete labels, one could also define a kernel as: w δ (y, y i ) = δ(yy i ), δ being the Dirac function, retrieving exactly SupCon  In this work, we propose to leverage both continuous d and discrete y labels, by combining (here by multiplying) the previously defined kernels, w σ and w δ , into a composite kernel loss function. In this way, samples will be considered as similar (positive) only if they have a composite degree of ""positiveness"" greater than zero, namely both kernels have a value greater (or different) than 0 (w σ > 0 and w δ = 0). An example of resulting representation space is shown in Fig.  where the indices t, i, j traverse all N images in the batch since there are no ""hard"" positive or negative samples, as in SimCLR or SupCon, but all images are considered as positive and negative at the same time. As commonly done in CL  arg min (3) By defining P (t) = {i : y i = y t } as the set of indices of images x i in the batch with the same discrete label y i as the anchor x t , we can rewrite our final loss function as: where In practice, it is rather easy to find a good value of σ, as the proposed kernel method is quite robust to its variation. A robustness study is available in the supplementary material. For the experiments, we fix σ = 0.1.",vol1
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,3.0,Experiments,"We compare the proposed method with different contrastive and non-contrastive methods, that either use no meta-data (SimCLR ",vol1
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,3.1,Datasets,"Three histo . It corresponds to absent fibrosis (F0), mild fibrosis (F1), significant fibrosis (F2), severe fibrosis (F3) and cirrhosis (F4). This score is then binarized to indicate the absence or presence of advanced fibrosis  In all datasets, we select the slices based on the liver segmentation of the patients. To gain in precision, we keep the top 70% most central slices with respect to liver segmentation maps obtained manually in D radio , and automatically for D 1  histo and D 2 histo using a U-Net architecture pretrained on D radio ",vol1
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,3.2,Architecture and Optimization,"Backbones. We propose to work with two different backbones in this paper: TinyNet and ResNet-18  Data Augmentation, Sampling and Optimization. CL methods  For sampling, inspired by  Finally, we run our experiments on a Tesla V100 with 16GB of RAM and a 6 CPU cores, and we used the PyTorch-Lightning library to implement our models. All models share the same data augmentation module, with a batch size of B = 64 and a fixed number of epochs n epochs = 200. For all experiments, we fix a learning rate (LR) of α = 10 -4 and a weight decay of λ = 10 -4 . We add a cosine decay learning rate scheduler ",vol1
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,4.0,Results and Discussion,"We present in Table  To illustrate the impact of the proposed method, we report in Fig.  To assess the clinical performance of the pretraining methods, we also compute the balanced accuracy scores (bACC) of the trained classifiers, which is compared in Table  Radiologists achieved a bACC of 82% with respect to the histological reference. The two bestperforming methods surpassed this score: depth-Aware and the proposed WSP approach, improving respectively the radiologists score by 2% and 3%, suggesting that including 3D information (depth) at the pretraining phase was beneficial.",vol1
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,5.0,Conclusion,"In this work, we proposed a novel kernel-based contrastive learning method that leverages both continuous and discrete meta-data for pretraining. We tested it on a challenging clinical application, cirrhosis prediction, using three different datasets, including the LIHC public dataset. To the best of our knowledge, this is the first time that a pretraining strategy combining different kinds of meta-data has been proposed for such application. Our results were compared to other stateof-the-art CL methods well-adapted for cirrhosis prediction. The pretraining methods were also compared visually, using a 2D projection of the representation vectors onto the first two PCA modes. Results showed that our method has an organization in the representation space that is in line with the proposed theory, which may explain its higher performances in the experiments. As future work, it would be interesting to adapt our kernel method to non-contrastive methods, such as SimSIAM ",vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,1.0,Introduction,"Methods for out-of-distribution (OOD) detection are a crucial component of any machine learning pipeline that is deployed in the real world. They are particularly necessary for pipelines that employ neural networks, which perform well on data drawn from the distribution they were trained on but can produce unexpected results when given OOD data. For medical applications, methods for OOD detection must be able to detect both far-OOD data, such as images of a different organ or modality to the in-distribution data, and near-OOD data, such as in-distribution data corrupted by imaging artefacts. It is also necessary that these methods can operate on high-resolution 3D data. In this work, we focus on methods trained in a fully unsupervised way; without any labels or access to OOD data at train time. Recently, Latent Transformer Models (LTMs)  However, LTMs have some disadvantages. Firstly, likelihood models have well documented weaknesses when used for OOD detection  A promising avenue for OOD detection is denoising diffusion probabilistic model (DDPM)-based OOD detection  The proposed LDM-based OOD detection offers the potential to address the three disadvantages of an LTM-based approach. Firstly, as the method is not likelihood based, it is not necessary that the VQ-GAN provides an ill-defined 'good representation'. Rather, the only requirement is that it reconstructs the inputs well, something easy to quantify using reconstruction quality metrics. Secondly, DDPMs have more favourable memory scaling behaviour than Transformers, allowing them to be trained on higher-dimensional representations. Finally, as the comparisons are performed at the native resolution, LDMs can produce high-resolution spatial anomaly maps. We evaluate both the LTM and the proposed LDM model on several far-and near-OOD detection tasks and show that LDMs overcome the three main failings of LTMs: that their performance is less reliant on the quality of the first stage model, that they can be trained on higher dimensional inputs, and that they produce higher resolution anomaly maps.",vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,2.0,Methods,We begin with a brief overview of LDMs and relevant notation before describing how they are used for OOD detection and to estimate spatial anomaly maps.,vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,2.1,Latent Diffusion Models,"LDMs are trained in two stages. A first stage model, here a VQ-GAN, is trained to compress the input image into a latent representation. A DDPM ",vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,,VQ-GAN:,"The VQ-GAN operates on a 3D input of size x ∈ R H×W ×D and consists of an encoder E that compresses to a latent space z ∈ R h×w×d×n , where n is the dimension of the latent embedding vector. This representation is quantised by looking up the nearest value of each representation in a codebook containing K elements and replacing the embedding vector of length d with the codebook index, k, producing z q ∈ R h×w×d . A decoder G operates on this quantised representation to produce a reconstruction, x ∈ R H×W ×D . In a VQ-VAE  The encoder and decoder are convolutional networks of l levels. There is a simple relationship between the spatial dimension of the latent space, the input, and number of levels: h, w, d = H 2 l , W 2 l , D 2 l , so the latent space is 2 3l times smaller spatially than the input image, with a 4 × 2 3l reduction in memory size when accounting for the conversion from a float to integer representation. In practice, most works use l = 3 (512× spatial compression) or l = 4 (4096× spatial compression); it is challenging to train a VQ-GAN at higher compression rates.",vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,,DDPM:,"A DDPM is then trained on the latent embedding z (the de-quantised latent). During training, noise is added to z according to a timestep t and a fixed Gaussian noise schedule defined by β t to produce noised samples z t , such that where we use z 0 to refer to the noise-free latent z, we have 0 ≤ t ≤ T , and α t := 1β t and ᾱt := t s=1 α s . We design β t to increase with t such that the latent z T is close to an isotropic Gaussian. We seek to train a network that can perform the reverse or denoising process, which can also be written as a Gaussian transition: In practice, following  where n ∼ N (0, I). While in most applications an isotropic Gaussian is drawn and iteratively denoised to draw samples from the model, in this work, we take a latent input z 0 and noise to z t for a range of values of t < T and obtain their reconstructions, ẑ0,t = p θ (z 0 |z t ).",vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,2.2,OOD Detection with LDMs,"In  In order to scale to 3D data, we reconstruct an input x in the latent space of the VQ-GAN, z = E (x). Reconstructions are performed using the PLMS sampler ",vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,2.3,Spatial Anomaly Maps,"To highlight spatial anomalies, we aggregate a set of reconstruction error maps. We select reconstructions from t-values = [100, 200, 300, 400], calculate the pixelwise mean absolute error (MAE), z-score these MAE maps using the pixelwise mean and standard deviation from the validation set, and then average to produce a single spatial map per input image.",vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,3.1,Data,We use three datasets to test the ability of our method to flag OOD values in both the near-and far-OOD cases. The CROMIS dataset ,vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,3.2,Implementation Details,"All models were implemented in PyTorch v1.13.1 using the MONAI framework v1.1.0  LDMs: VQ-GANS were trained with levels l = 2, 3, or 4 levels with 1 convolutional layer and 3 residual blocks per level, each with 128 channels. Training with l = 3/4 represents standard practice, training with l = 2 (64× spatial compression) was done to simulate a situation with higher-resolution input data. All VQ-GANs had an embedding dim of 64, and the 2, 3, 4 level models have a codebook size of 64, 256, 1024, respectively. Models were trained with a perceptual loss weight of 0.001, an adversarial weight loss of 0.01, and all other losses unweighted. Models were trained with a batch size of 64 for 500 epochs on an A100, using the Adam optimizer ",vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,,LTM:,The Latent Transformer Models were trained on the same VQ-GAN bases using the procedure described in ,vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,4.0,Results and Discussion,Results and associated statistical tests are shown in Table ,vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,,Dataset,"Recent research shows that at higher resolutions, the effective SNR increases if the noise schedule is kept constant  Memory and time requirements for all models are tabulated in Supplementary Table ",vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,5.0,Conclusion,"We have introduced Latent Diffusion Models for 3D out-of-distribution detection. Our method outperforms the recently proposed Latent Transformer Model when assessed on both near-and far-OOD data. Moreover, we show LDMs address three key weaknesses of LTMs: their performance is less sensitive to the quality of the latent representation they are trained on, they have more favourable memory scaling that allows them to be trained on higher resolution inputs, and they provide higher resolution and more accurate spatial anomaly maps. Overall, LDMs show tremendous potential as a general-purpose tool for OOD detection on high-resolution 3D medical imaging data.",vol1
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 43.,vol1
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,1.0,Introduction,"Deep neural networks (DNNs) have been successfully applied to various supervised 3D biomedical image analysis tasks, such as classification  Recently, unsupervised part discovery in 2D natural images has gained significant attention  We hypothesize that deep generative models are good feature extractors for unsupervised structure discovery for the following reasons. First, these models do not require expert labels as they are trained in a self-supervised way. Second, the ability to generate high-quality images suggests that these models capture semantically meaningful information. Third, generative representation learning has been successfully applied to global and dense prediction tasks in 2D images  Besides creating stunning image generation results, diffusion-based generative models  1) We pretrain 3D diffusion models, use them as feature extractors (Fig. ",vol1
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,2.0,Background on Diffusion Models,"Diffusion models  where ), can be written as: The reverse pass is a corresponding T -step denoising process using a neural network (usually, U-Net  ( Practically, instead of μ θ (x t , t) and Σ θ (x t , t), models are designed to predict either the noise t at timestep t, or a less noisier version of image x t-1 directly.",vol1
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,3.0,Method,"We formulate the 3D structure discovery task in biomedical images as an unsupervised segmentation into K parts. Given a one-channel 3D image We use three losses for unsupervised training (see Fig.  For an arbitrary representation h(x 0 ) of an image x 0 with voxels u, the consistency of this representation C(h(x 0 )) across K predicted parts in the form of segmentation M is defined as: where N is the number of voxels. This is a form of volume-normalized K-means loss with z k describing the mean feature value of partition k. Feature Consistency. We pretrain generative 3D diffusion models and use them as feature extractors  Visual Consistency. The extracted features are upsampled from low spatial resolutions and therefore do not accurately align with image boundaries. To alleviate this problem, we use a voxel visual consistency loss: where I(x 0 ) is the identity feature extractor, i.e. I(x 0 ) = x 0 . Photometric Invariance. As biomedical images often show acquisition differences (e.g., based on MR or CT scanner), they can be heterogeneous in their voxel intensities  We assume our images are min-max normalized (x 0 ∈ [0, 1]). We then use gamma-correction of the form T (x 0 ) = x γ 0 as a photometric transformation. We draw γ from the uniform distribution: ",vol1
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,4.1,Datasets,To compare with state-of-the-art unsupervised 3D segmentation methods we follow  The synthetic dataset of  The BraTS'19 dataset ,vol1
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,4.2,Implementation Details,All diffusion models use the same architecture shown in Fig.  Our segmentation networks (f in Fig.  For all experiments we used Pytorch and 4 NVIDIA A6000 GPUs (48 Gb).,vol1
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,4.3,Results,"We compare our method with state-of-the-art unsupervised 3D structure discovery approaches including clustering using 3D feature learning  For the synthetic datasets, we used K = 2 (background and cell) for Level 1, K = 4 (background, cell, vesicle, mitochondria) for Level 2, and K = 8 (background, cell, vesicle, mitochondria, and 4 small protein aggregates) for Level 3 predictions. The evaluation metric is the average Dice score on the annotated test labels. As the label order may differ we use the Hungarian algorithm to match the predicted masks with the ground truth segmentations. Table  For the Brain Tumor Segmentation (BraTS'19) dataset, we use the whole tumor (WT) segmentation mask for evaluation, which is detectable based on the FLAIR images alone. We train segmentation models with K = 3 parts (background, brain, tumor). The evaluation metric, as in the BraTS'19 challenge  We perform ablation studies on the BraTS'19 dataset (Table  This might be due to the fact that predictive modeling involves learning from a distribution of images and a model may therefore extract useful knowledge from a collection of images. To evaluate the significance of the diffusion features, we replaced our diffusion feature extractor with a 3D ResNet from Med3D ",vol1
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,5.0,Conclusion,"In this work, we showed that features from 3D generative diffusion models using a ladder-like U-Net-based architecture can discover intrinsic 3D structures in biomedical images. We trained predictive unsupervised segmentation models using losses that encourage the decomposition of biomedical volumes into nested subvolumes aligned with their hierarchical structures. Our method outperforms existing unsupervised segmentation approaches and discovers meaningful hierarchical concepts on challenging biologically-inspired synthetic datasets and on the BraTS brain tumor dataset. While we tested our approach for unsupervised image segmentation it is conceivable that it could also be useful in semisupervised settings and that could be applied to data types other than images.",vol1
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,1.0,Introduction,"Wireless capsule endoscopy (WCE) is an emerging examination technique that offers several advantages over traditional electronic endoscopy, including noninvasiveness, safety, and non-cross-infection. It enables the examination of the T. Liu and Z. Chen-contributed equally to this work. entire human gastrointestinal tract and is widely used in clinical practice  Most super-resolution methods based on deep learning are supervised by paired low-resolution (LR) and high-resolution (HR) images  What causes this domain gap? It might seem reasonable to adopt the simple linear degradation assumption by simply analogizing the domain gap between a mobile camera and a professional camera to a WCE and an electronic endoscope. However, what cannot be ignored is the different examination environment, where the WCE requires filling the stomach with water, while the electronic endoscopy inflates the stomach, which directly leads to the difference between the two image domains in terms of villi pose and speckle reflection, as shown in Fig.  Recently, many studies have utilized the CycleGAN  images with degenerate distributions similar to the real LR images, and then train the SR model on the generated LR-HR paired dataset  In this work, we propose a Multi-level Domain Adaptation Super-Resolution (MDA-SR) for WCE images to bridge the domain gap between electronic endoscopy images and WCE images. MDA-SR leverages prior knowledge of HR electronic endoscopy images to guide the SR process of WCE images, as illustrated in Fig. ",vol1
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,2.1,Overview of the Proposed Method,"Given a set of WCE images and HR electronic endoscopy images I LR cap , I HR ele , we aim to learn a SR function R (•) that maps an observed I LR cap to its HR version according to the distribution defined by I HR ele in testing. As shown in Fig. ",vol1
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,2.2,Adaptive Degradation,"The purpose of the adaptive degradation is to obtain I LR gen with a degradation distribution similar to I LR cap . To achieve this, we employ the architecture of GAN  G is optimized by maximizing the loss in Eq. (  The process of degradation from HR images to LR images is unknown. We adopt an adaptive downsampling kernel k  where I HR ele,i denotes an i-th example to estimate the kernel, N is the total number of samples that have been used and ↓ s represents downsampling operation with scale factor s. Finally, the data fidelity term L data is defined as follows: Given the definitions of adversarial and data losses above, the training loss of our adaptive degradation is defined as: where λ is a hyperparameter.",vol1
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,2.3,Domain Adaptation SR,"The SR function R (•) can then be supervised by the aligned image pair set I LR gen , I HR ele obtained from adaptive degradation. As shown in Fig.  To further improve the performance of the WCE SR, it is crucial to bridge the domain gap between WCE images and electronic endoscopy images, even though the adaptive degradation generator G already learns the degradation distribution of WCE images through domain adaptation at the image level. To achieve this, we improve the domain adaptation at the latent level during the SR process. A straightforward way is to adopt a GAN  As a result, the discriminator F latent is trained with its corresponding loss in Eq.  where μ is a hyperparameter.",vol1
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,3.1,Experiment Settings,"Datasets. Our proposed model is trained on WCE image dataset and electronic endoscopy image dataset, and tested on WCE images. The WCE image dataset contains 14090 images, and the electronic endoscopy image dataset contains 2033 images, including 1302 images from the public Kvasir dataset  The quality of a given test image is then expressed as the distance between a multivariate gaussian (MVG) fit of the ESS features extracted from the test image and a MVG model of the quality-aware features extracted from the corpus of HR electronic endoscopy images. Additionally, we use the no-reference metric BRISQUE  To better illustrate the subjective quality, we conduct a mean opinion score (MOS) test for comparison with other methods. We randomly select 100 different WCE images from the test set to subjectively evaluate the quality of the 2x and 4x WCE SR images. Four gastroenterology clinicians rate the visual perceptual qualities by assigning scores. Scores from 0 to 5 are used to indicate the qualities from low to high.",vol1
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,3.2,Training Details,"Throughout the framework, the discriminators F image and F latent use the patchbased discriminator ",vol1
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,3.3,Results and Discussions,"Comparison with Previous Methods. To validate the effectiveness of our proposed method, we compare it with existing state-of-the-art conventional SR methods without domain adaptation ",vol1
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,4.0,Conclusion,"In this paper, we propose a multi-level domain adaptation SR for real WCE images. Our method first utilizes adaptive degradation to simulate the degradation distribution of WCE and generate LR electronic endoscopy images. We then employ implicit domain adaptation at the latent level during the SR process to further bridge the domain gap between WCE images and electronic endoscopy images. Through extensive experiments on real WCE images, we demonstrate the superiority of our method over other state-of-the-art SR methods, and its efficacy in reality. Further evaluation for downstream tasks such as disease classification, region segmentation, or depth and pose estimation from the generated SR WCE images is warranted.",vol1
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,1.0,Introduction,Colorectal cancer is a leading cause of cancer-related deaths worldwide  Dual-branch learning has been widely adopted in annotation-efficient learning to encourage mutual consistency through co-teaching. While existing approaches are typically designed for learning in the spatial domain ,vol1
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,2.1,Preliminaries,"Spectral-domain learning  Besides consistency constraints, utilizing pseudo labels as supplementary supervision is another principle in label-efficient learning  where α is the random mixing ratio. p 1 , p 2 , and p mix denote the probability maps from the two spatial decoders and their mixture. These approaches only operate in the spatial domain, regardless of single or dual branches, while we consider both spatial and spectral domains and propose to adaptively merge dual-branch outputs with respective pixel-wise entropy guidance.",vol1
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,2.2,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning,"Spatial-Spectral Cross-domain Mutual Teaching. In contrast to prior weakly-supervised learning methods that have merely emphasized spatial considerations, our approach designs a dual-branch structure consisting of a spatial branch f spa (x, θ spa ) and a spectral branch f spe (x, θ spe ), with x and θ being the input image and randomly initialized model parameters. As illustrated in Fig.  where ""→"" denotes supervision Entropy-Guided Pseudo Label Ensemble Learning. In addition to mutual teaching, we consider aggregating the pseudo labels from the spatial and spectral branches in ensemble learning, aiming to take advantage of the distinctive yet complementary properties of the cross-domain features. As we know, a pixel characterized by a higher entropy value indicates elevated uncertainty in terms of its corresponding prediction. We can observe from the entropy maps H spa and H spe in Fig.  where C is the number of classes that equals 2 in our task. Unlike previous image-level fixed-ratio mixing or random mixing as Eq. (  where ""⊗"" denotes pixel-wise multiplication. p s 2 is the merged probability map and can be further converted to the pseudo label by ŷs 2 = arg max p s 2 to supervise the spatial and spectral branch in the context of ensemble learning following ŷs 2 → f spa and ŷs 2 → f spe . ( By absorbing strengths from the spatial and spectral branches, ensemble learning from the mixed pseudo labels facilitates model optimization with reduced overfitting, increased stability, and improved generalization and robustness. Hybrid Loss Supervision from Scribbles and Pseudo Labels. Besides the scribble annotations for partial pixels, the aforementioned three types of pseudo labels ŷspa , ŷspe , and ŷs 2 can offer complementary supervision for every pixel, with different learning regimes. Overall, our hybrid loss supervision is based on Cross Entropy loss CE and Dice loss Dice . Specifically, we employ the partial Cross Entropy loss  where y denotes the scribble annotations. Furthermore, the mutual teaching loss with supervision from domain-specific pseudo labels is .  where λ mt and λ el serve as weighting coefficients that regulate the relative significance of various modes of supervision. The hybrid loss considers all possible supervision signals in the spatial-spectral dual-branch network and exceeds partial combinations of its constituent elements, as evidenced in the ablation study.",vol1
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,3.1,Experimental Setup,Datasets. We employ the SUN-SEG  Implementation Details. We implement our method with PyTorch ,vol1
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,3.2,Results and Analysis,"Fully-CE 0.713±0.021 0.617±0.023 0.746±0.027 4.405±0.119 The performance of weakly-supervised methods is assessed with four metrics,i.e., Dice Similarity Coefficient (DSC), Intersection over Union (IoU), Precision (Prec), and a distance-based measure of Hausdorff Distance (HD). As shown in Table ",vol1
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,3.3,Ablation Studies,Network Structures. We first conduct the ablation analysis on the network components. As shown in Table  Hybrid Loss Supervision. We decompose the proposed hybrid loss L hybrid in Eq. ( ,vol1
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,4.0,Conclusion,"To our best knowledge, we propose the first spatial-spectral dual-branch network structure for weakly-supervised medical image segmentation that efficiently leverages cross-domain patterns with collaborative mutual teaching and ensemble learning. Our pixel-level entropy-guided fusion strategy advances the reliability of the aggregated pseudo labels, which provides valuable supplementary supervision signals. Moreover, we optimize the segmentation model with the hybrid mode of loss supervision from scribbles and pseudo labels in a holistic manner and witness improved outcomes. With extensive in-domain and out-ofdomain evaluation on four public datasets, our method shows superior accuracy, generalization, and robustness, indicating its clinical significance in alleviating data-related issues such as data shift and corruption which are commonly encountered in the medical field. Future efforts can be paid to apply our approach to other annotation-efficient learning contexts like semi-supervised learning, other sparse annotations like points, and more medical applications.",vol1
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,,Acknowledgements,. This work was supported by ,vol1
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,1.0,Introduction,"Schizophrenia is a severe and chronic psychiatric disorder characterized by psychotic episodes, cognitive impairment, and impaired functioning with high rates of disability  There is an increasing interest in developing accurate and robust techniques to classify subjects into groups using neuroimaging data. Previous studies focused on handcrafted feature-based machine learning approach, which requires the process of feature extraction and selection prior to disease classification  With the rapid development of deep learning methods, recent studies have shown great potential for integrating neuroimaging data and deep learning models for an automatic diagnosis of brain diseases  Recent studies have highlighted the potential of cortical networks, so-called morphometric similarity networks (MSNs), to predict individual differences in brain morphometry  To fill these gaps, we developed a generalizable graph convolutional framework for population-based schizophrenia classification using the MSNs inferred from structural MRI data. Our contributions are summarized as follows: 1) We propose a new population graph model for integrating MSN-driven features derived from structural MRI and phenotypic information; 2) A novel feature selection strategy is introduced to leverage graph-theoretical measures of the MSNs, which is new and generalizable for graph neural networks; 3) We validate the feasibility of our proposed method by conducting a comprehensive evaluation on a large schizophrenia dataset, which shows superior performance in classification over traditional machine learning approaches; 4) A complete sensitivity analysis for key parameters in our GCN-based classification framework is performed; 5) The most salient regions contributing to classification are primarily identified in the middle temporal gyrus and superior temporal gyrus.",vol1
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,2.0,Materials and Methods,Figure ,vol1
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,2.1,Datasets,"In this study, structural T1-weighted magnetic resonance imaging (MRI) scans were collected from six public databases, including DecNef ",vol1
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,2.2,Morphometric Similarity Networks,The structural MRI scans from all subjects were preprocessed using the recon-all command from FreeSurfer (version 7.2.0) ,vol1
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,2.3,Graph Construction,"We consider a population comprising N subjects; each subject being associated with a set of imaging and phenotypic information. We define the population graph as an undirected weighted graph G = (V , E, W ), where V is a set of |V | = N nodes, E is a set of edges. W ∈ R N ×N is a weighted adjacency matrix of population graph G. Each node v represents a subject, and the feature vectors for nodes are extracted from imaging data. Each edge is defined as the similarity between nodes. The two main decisions required to build the population model are the definition of the feature vector representing each node and the connectivity of the graph corresponding to its edges and weights. Feature Vector. The MSN has a high dimensionality with 47,278 features considering only the upper triangle of the MSN. Using the Brain Connectivity Toolbox (BCT)  We investigated two additional feature selection approaches. Firstly, we applied the ridge classifier to perform recursive feature elimination (RFE). RFE iteratively removes the irrelevant features to achieve the desired number of features while maximizing the ridge classification accuracy. Secondly, we used the k-best selection with ANOVA, a univariate feature selection algorithm that selects the k-best features with the highest F-values. Graph Edge. The process of computing graph edges involved two steps: the calculation of initial edge weights and adaptive learning through encoders. The initial similarity between nodes were determined using imaging features and categorical information. Considering a set H categorical phenotypic measures M = {M h }, such as sex and scan site, the initial edge weight W I between node v and w are defined as follows: (1) where Sim(v, w) is a measure of similarity between node v and node w in the imaging features. γ is a measure of similarity between node v and node w in the categorical variables. ρ is the Pearson's correlation distance. x(v) and x(w) are the topological feature vectors of node v and w, respectively. h denotes the width of the kernel. We set a threshold on W I (v, w) via quantile Q, , and adaptively calculated the edge weights only for the remaining edges. We investigated four different encoders (PAE, EA, L2, and Cosine + Tanh) that were used to determine the edge weights between node v and node w based on phenotypic information (e.g., sex, age, and scan site). The subject's normalized phenotypic inputs were projected into a latent space ϕ ∈ R 64 using MLP. The pairwise association encoder (PAE)  2 ), where ||ϕ v -ϕ w || 2 2 denotes L2 distance of two latent vectors. Finally, we designed the Cosine + Tanh encoder combining Tanh with cosine similarity as follows: The encoders were optimized with graph convolution models using gradient descent algorithm.",vol1
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,2.4,Spectral Graph Convolutions,"To extract spatial features from each graph node, we used the spectral graph convolution in the Fourier domain. A spectral convolution of signal x with a filter g θ = diag(θ ) defined in the Fourier domain is defined as a multiplication in the Fourier domain: , where U is the matrix of the eigenvectors of the normalized Laplacian matrix. The normalized graph Laplacian L is defined as where I and D are the identity matrix and the diagonal degree matrix, respectively. To reduce computational complexity of convolution, we used Chebyshev spectral convolutional network (ChebConv)  where L is the rescaled Laplacian and θ k are filter parameters. The output spatial features from l th convolutional layer with input features H l ∈ R N ×C l can be calculated as: are the trainable weights for the polynomial of order k in the l th layer. The model consists of a GCN with L hidden layers. Each hidden layer is followed by a ReLU activation function to introduce non-linearity. The output layer is full connected and comprises two convolutional layers with 256 and 2 channels. The model was trained using the entire population graph. During training, the training nodes were labeled and the test nodes were masked. Cross-entropy loss computed on the training nodes was used to train the GCN and edge encoder. The performance of the model was evaluated using the test nodes.",vol1
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,2.5,Interpretability,We identified relevant node features that contribute to the classification using GNNExplainer ,vol1
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,3.0,Experiments and Results,"Experimental Settings. We used pooled stratified cross-validation to split the samples into training and testing sets for the evaluation of the GCN. The pooled samples were randomly divided into 5-folds, of which 1-fold served as the testing set, and the remaining 4-folds were used as the training set. This strategy ensures that training and testing sets contain the equivalent proportions of each class. The model performance was evaluated on the testing set in terms of balanced accuracy (BAC), sensitivity (SEN), specificity (SPE), F1-score, and area under the receiver operating characteristic curve (AUC). In our experiments, we set Chebyshev polynomial order K = 3, quantile Q = 0.5, and hidden layer L = 3. The number of channels was set to  Competing Methods. To validate the superiority of our proposed model, we compared the GCN model with traditional machine learning algorithms including support vector machine (SVM), random forest (RF), and K-nearest neighbor (KNN). The upper triangle of the MSN was used as input features. We tuned hyperparameters using 5-fold crossvalidation to find the optimal parameters via grid search.",vol1
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,3.1,Results and Analysis,Results of Schizophrenia Classification. Table ,vol1
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,3.2,Ablation Studies,"We provide detailed investigation of three key components (the density of MSNs, feature selection strategy, and edge encoder) and their influence on classification results. Influence of the Connection Density of MSNs. We constructed a series of MSNs with different connection densities ranging from 10% to 100% in 10% increments. For each matrix, we computed strength, betweenness centrality, and clustering coefficient to examine their influence on classification performance. In Fig. ",vol1
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,4.0,Conclusion,"In this study, we proposed an improved GCN structure, which combines graphtheoretical measures of MSNs derived from structural MRI data and non-imaging phenotypic measures for disease classification. This study explored the application value of our proposed GCN model based on cortical networks (MSNs) in differentiating patients with schizophrenia from healthy controls. Using a large multi-site dataset and various validation strategies, reliable and generalizable classification accuracy of 81.8% can be achieved. These results indicate that the MSNs serve as a useful and clinicallyrelevant phenotype and GCN modeling shows promise in detecting individual patients with schizophrenia. Further, we investigated different graph structures and their influence on classification performance. The GCN model making use of subject-specific clustering coefficient as imaging feature vectors and the PAE encoder performed best. By examining the saliency patterns contributing to GCN classification, we identified the most salient regions in the middle temporal gyrus and superior temporal gyrus. These findings suggest the potential utility of GCN for enhancing our understanding of the underlying neural mechanisms of schizophrenia by identifying clinically-relevant disruptions in brain network topology.",vol1
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,1.0,Introduction,"Deep learning has made significant progress in medical computer vision  We propose a novel VLP framework named Medical vision-language pretraining with Frozen language models and Latent spAce Geometry optimization method (M-FLAG). Different from most existing VLP approaches, M-FLAG is computationally efficient as it only requires training the vision model, while keeping the language model frozen. To harmonize the latent spaces in vision and language models, we relax the visual-language alignment objective with a orthogonality loss to alleviate the latent space collapse problem. The main contributions of this work include: (1) To the best of our knowledge, this is the first work to explore the collapsed latent space problem in medical VLP. (2) A novel and effective VLP framework is proposed to alleviate the collapsed latent space problem by explicitly optimizing the latent geometry towards orthogonal using our orthogonality loss in addition to the visual-language alignment loss, encouraging the in-dependency between latent variables and maximizing its informativeness for downstream tasks. (3) M-FLAG consistently outperforms existing medical VLP methods on three downstream tasks: medical image classification, segmentation, and object detection, while reducing 78% trainable parameters due to the frozen language model strategy.",vol1
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,Related Works:,"To connect vision and language modalities, the idea of VLP was proposed in CLIP  It has been suggested that optimal vision and language latent spaces should be of different geometry ",vol1
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,2.0,Methods,"The proposed M-FLAG is a simple and light VLP framework that aims to learn visual and text representations by leveraging both medical images and radiology Fig.  reports. We employ a freeze strategy for the text encoder E T to mitigate ambiguity in vision-text latent space alignment. Additionally, we explicitly optimize the latent space geometry using a orthogonality loss. By doing so, we encourage the visual latent space to keep a stable geometry and reduce the risk of collapse. Figure ",vol1
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,Vision Encoder and Frozen Text Encoder:,"The paired medical image and text are denoted as x v , x t , respectively. As illustrated in Fig.  Vision Embedding: E V , the vision embedding z v ∈ R B×N is extracted from the last pooling layer of E V . N denotes the dimension of the latent space and B represents the batch size. Text Embedding: A text encoder E T extracts text embedding of word tokens from a medical report. Similar to BERT ",vol1
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,2.1,Frozen Language Model,"In this work, we use a frozen text encoder E T , which can be obtained from any general language model. The latent space of z v is thus stable without the risk of latent space perturbation ",vol1
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,2.2,Alignment and Uniformity,"As illustrated in Fig.  We compute a composite loss L total to train the vision encoder E V and the projector p(•), which consists of two parts, alignment loss L align and orthogonality loss L orth : where {i, j} ∈ {1, ..., dim(z v )} 2 . We implement 2 -normalization on z a , z t , z v to obtain za , zt , zv . L align minimizes the discrepancy between za and zt , while L orth maximizes the independence among latent features in zv , forcing its empirical correlation matrix to be an identity matrix. In other words, we expect different latent feature dimensions to be independent. The objective of the first term on the right side in Eq. ( ",vol1
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,3.1,Dataset for Pre-training,"M-FLAG is pre-trained on the MIMIC-CXR (MIMIC) dataset  Pre-training takes 100 epochs on 8 A100 GPUs, with a batch size of 128 for each GPU and a learning rate of 0.001 using the LARS ",vol1
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,3.2,Datasets for Downstream Tasks,"The pre-trained model is evaluated on 3 downstream tasks across 5 datasets: Medical image classification is implemented on MIMIC, CheXpert (CXP), and NIH  To reduce sampling bias and maintain consistency, we follow the dataset split in CheXclusion  Image segmentation is evaluated on two datasets, RSNA  Object detection is implemented on the RSNA  Table  Segmentation RSNA ",vol1
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,Medical Image Classification:,"The AUC scores on MIMIC, CXP, and CXR14 are reported in Table  Segmentation and Object Detection: Table ",vol1
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,3.4,Dimensional Collapse Analysis,Recent studies ,vol1
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,3.5,Ablation Study,"Ablation Study: Table  The performance of the model pre-trained with only L align drops dramatically in segmentation and detection tasks, although less severe in the classification tasks. On the other hand, the model pre-trained with only L orth does not suffer severe performance drop across the three tasks, indicating that the uniform latent space could have a considerable contribution to the performance of M-FLAG. Overall, these results underscore the importance of both loss functions in M-FLAG and highlight their complementary contributions.  Comparing M-FLAG with Frozen vs. Unfrozen Language Models: We conducted further experiments to evaluate the performance of M-FLAG while unfreezing the last few layers of the language model. This not only increases the number of trainable parameters but also influences the model performance. Table ",vol1
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,4.0,Conclusion,"Simple architecture means low computational cost and stable training. In this work, we propose a simple and efficient VLP framework that includes a frozen language model and a latent space orthogonality loss function. Extensive experiments show that M-FLAG outperforms SOTA medical VLP methods with 78% fewer parameters. M-FLAG also demonstrates its robustness by achieving the highest performance when transferred to unseen test sets and diverse downstream tasks for medical image classification, segmentation, and detection. This indicates the benefits of freezing the language model and regularizing the latent space. The results exhibit promising potential for improving the pre-training of vision-language models in the medical domain. In addition, the latent space geometry explored in this work provides useful insight for future work in VLP.",vol1
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,1.0,Introduction,"Endoscopy is an important medical procedure with many applications, from routine screening to detection of early signs of cancer and minimally invasive treatment. Automatic analysis and understanding of these videos raises many opportunities for novel assistive and automatization tasks on endoscopy procedures. Obtaining 3D models from the intracorporeal scenes captured in endoscopies is an essential step to enable these novel tasks and build applications, for example, for improved monitoring of existing patients or augmented reality during training or real explorations. 3D reconstruction strategies have been studied for long, and one crucial step in these strategies is feature detection and matching which serves as input for Structure from Motion (SfM) pipelines. Endoscopic images are a challenging case for feature detection and matching, due to several well known challenges for these tasks, such as lack of texture, or the presence of frequent artifacts, like specular reflections. These problems are accentuated when all the elements in the scene are deformable, as it is the case in most endoscopy scenarios, and in particular in the real use case studied in our work, the lower gastrointestinal tract explored with colonoscopies. Existing 3D reconstruction pipelines are able to build small 3D models out of short clips from real and complete recordings  This work introduces SuperPoint-E, a new model to extract interest points from endoscopic images. We build on the well known SuperPoint architecture ",vol1
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,2.0,Related Work,3D reconstruction is an open problem for laparoscopic and endoscopic settings  Well known SfM and SLAM pipelines rely on accurate and robust feature extraction methods. COLMAP  Deep learning methods for feature extraction and matching is a very active research field. The survey Ma et al.  In this work we improve the performance of SuperPoint ,vol1
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,3.0,Tracking Adaptation for Local Feature Learning,"Superpoint supervision is referred to as Homographic Adaptation and assumes that the surfaces are locally plane, which is not the case in our data. Instead, we propose to use 3D reconstructions of points tracked along image sequences. This makes no assumptions about the local surface shapes and we will show in Sect. 4 that this yields a better trained network. We will refer to this as Tracking Adaptation and we will here describe how we obtain the tracks.",vol1
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,SfM as Supervision for Feature Extraction.,"We generate examples of good features by identifying features that were successfully reconstructed with existing methods for each sequence in our training set. Our training set contains short sequences (4-7 s) from the complete colonoscopy recordings in EndoMapper dataset where COLMAP software was able to obtain a 3D reconstruction. This is a very challenging domain, and existing SfM pipelines fail in longer videos.",vol1
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,3D Reconstruction of Training Set Videos.,"We generate 3D reconstructions for all our training sequences with out-of-the-box COLMAP. In particular, we use the following blocks: feature extractor, exhaustive matcher and mapper. Configuration parameters are detailed in the supplementary materials. We turn on the ""guided matching"" option for the exhaustive matcher module to find the best matches possible. We additionally compute the 3D reconstruction for the same sequences with a modified COLMAP pipeline that uses the official Super-Point and SuperGlue For supervision, we only use reprojected points that fall within a reliable track. A reliable track is an interval bounded by green points. So, the reprojected points selected for training are either green or have preceding and subsequent green points along its track. The different appearances of the same 3D point in different frames of the track are our correspondences for training our models. Figure  Deep Feature Extraction for Endoscopy. SuperPoint uses a fully-convolutional network as backbone and learns to extract good features using homographic adaptation: extracting features that are robust to homographic deformations. It achieves this by using as supervision Y the average detections over several random homographic deformations of the same image. The feature extraction network then is run on an image I and a warped version I of it with a new homography. The network optimizes the loss function where X and D are the detection and description heads' outputs, respectively. Y is the supervision for the detection. S is the correspondence between I and I computed from the homography. L p is the detection loss that measures the discrepancies between the supervision Y and the detection head's output X . λ = 1 is a weighting parameter. L d is the description loss that measures the discrepancies between both description head's outputs D and D using S. Using our new supervision from SfM in the form of tracks of points, we propose a new loss to train SuperPoint that is more aligned with our goal, called tracking adaptation. Instead of an image I and a warped version I , we use different images I a and I b from the same sequence. The supervision Y for the detection in this case is the set of points that have been reprojected on I a and I b from the 3D reconstruction. The detection loss L p is calculated as in the original SuperPoint. We replace the description loss L d for a new tracking loss where D a and D b are the description head's outputs for I a and I b , respectively. T is the set of all the tracks that appear in both images. l t is a common triplet loss that measures the distance between positive pairs (weighting parameter λ t = 1 and positive margin m p = 1) and the distance between negative pairs (negative margin m n = 0.2). Two descriptors from different images d ai and d bj are a positive pair if they belong to the same track (i = j), and negative pair otherwise (i = j).",vol1
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,4.0,Experiments,"The following experiments demonstrate the proposed feature detection efficacy to obtain 3D models on real colonoscopy videos, comparing different variations of our approach and relevant baseline methods. Dataset. We seek techniques that are applicable to real medical data, so we train and evaluate with subsequences from the EndoMapper dataset  Baselines and our Variations. We use COLMAP as our first baseline. It uses SIFT features and a standard guided matching algorithm to produce very accurate camera pose estimates. We also include as baseline the results of SuperPoint (SP) with SuperGlue matches and the COLMAP reconstruction module. The configuration for both baselines is the same as detailed in Sect. 3. We evaluate different variations of the original SuperPoint. All models were trained with a modification of a PyTorch implementation of SuperPoint  -3DIm : Fraction of images from the subsequence successfully introduced in the reconstruction. The closer to 100% the better. -3DP ts : Number of points that were successfully reconstructed. The more points the better, since it means a denser coverage of the scene. -Err: Mean reprojection error of the 3D points after being reprojected onto the images of the subsequence. -Err-10K: Mean reprojection error of the best 10000 points of the reconstruction. Since all reconstructions have outliers that skew the average, this metric is more representative of the performance of the models. -len(Tr): Mean track length represents the average number of images where a point is being consecutively matched, tracked. SP-E v2 (SP-E moving forward) is our best variation, with the highest amount of reconstructed points and the lowest reprojection error for top 10000.",vol1
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,SfM Results,"Comparison. This experiment compares the performance of the considered baselines against the best configuration of our feature extraction model. Table  To provide quantitative evaluation of the camera motion estimation, we use a simulated dataset ",vol1
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,5.0,Conclusions,"This work presents a novel training strategy for SuperPoint to improve its performance in SfM from endoscopy images. This strategy has two main benefits: we show how to use 3D reconstructions of endoscopy sequences as supervision to train feature extraction models; and we design a new tracking loss to perform tracking adaptation using this supervision. The benefits of our method are explored with an ablation study and against established baselines on SfM and feature extraction. Our proposed model is able to obtain more suitable features for 3D reconstruction, and to reconstruct larger sets of images with much denser point clouds.",vol1
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 56.,vol1
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,1.0,Introduction,"Foundation models have become a significant milestone in artificial intelligence, from theoretical research to practical applications  Medical contrastive vision-language pre-training  However, semantic overlap and semantic shifting are two significant challenges in medical vision-language contrastive learning (Fig. ",vol1
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Graph,"Once lack of concept correlation and negation identification, representations with similar semantics are falsely pushed apart and those with opposite semantics are falsely pushed together, interfering with the learning of significant representation  Rethinking the existing methods and challenges of medical contrastive visionlanguage pre-training  In this paper, we propose a novel knowledge-boosting medical contrastive vision-language pre-training framework (KoBo). Our contributions are as followed. 1) Our KoBo pre-trains a powerful image encoder including visual information corresponding with the disease described in texts, where knowledge is embedded in our paradigm (Fig. ",vol1
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,2.0,Methodology,Our Knowledge-Boosting Contrastive Vision-Language Pre-training framework (Fig. ,vol1
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,2.1,Framework Formulation,"In the framework, a powerful image encoder Enc I and text encoder Enc T is pretrained, alongside a graph encoder Enc G . Given a pair of medical image and diagnostic report  Besides using reports and images as the input for our pre-training network, we also input an external knowledge graph to the whole framework for improving the correspondence of modality features and clinical knowledge. The knowledge refers to relations between clinical pathology concepts in the radiology domain in the format of triplet G = {(c h k , r k , c t k )} NG k=1 , such as UMLS ",vol1
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,2.2,Knowledge Semantic Enhancement,"To relieve the semantic overlap problem, where negative sample noise harms the effective learning of vision-language mutual information, we propose a semantic enhancement module to identify the noise using sample-wise similarities. The similarity is estimated upon sample knowledge k i , calculated from domain knowledge embedding E and concept set from texts with negation marker. Getting Sample knowledge: Firstly, we acquire a concept set that contains pathology concepts extracted from texts with Negbio N (•)  Furthermore, considering the challenge that negation expression of concepts commonly exists in radiology reports, which has opposite semantics with similar morphology for text encoder (converging shifting), we randomly generate a No Finding embedding N F and a variant of domain knowledge embedding E = { e 1 , e 2 , ..., e NE } of the same size as E with Xavier distribution. Upon the negation mark of concept, sample knowledge embedding k i = {k i,s } NES s=1 is denoted below: where P is the negation mark of concepts, and e i,s , e i,s is the corresponding position of c i,s in E and E. tunes the variance of negative sample knowledge. are k i from the image-view and text-view concept set.",vol1
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Estimation of Similarities:,"The semantic similarity is calculated upon sample knowledge. For each image-text pair, a max-match strategy is adopted to match each two sample knowledge embedding with the most similar one for calculating cosine similarities. Sample-wise similarities are aggregated with averages. (2) where N ES is the number of concepts in T Sent i , while N ES is that in T Report i . Knowledge Semantic Enhancement Loss: We utilize the sample-wise semantic similarity to estimate negative sample noise, placed in the sample weight of the contrastive loss  ) where τ G is the global temperature, and λ IT , λ T I is the sample similarity measurement. specifically, λ i,i is fixed to zero to persist the positive sample weight.",vol1
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,2.3,Knowledge Semantic Guidance,"In this section, we propose a semantic guidance module to solve the semantic shifting problem. Utilizing sample knowledge from Sect. 2.2 which contains concept correlation and negation information, the adverse effects of both disperse and converging shifting are alleviated by fusing domain-sample knowledge with global-local modality embeddings. We design four contrast schemes: knowledge anchor guidance for adjusting disperse shifting, semantic knowledge refinement for filtering converging shifting, vision semantic response for consolidating knowledge fusion, and semantic bridge guidance for narrowing the modality gap. Knowledge Anchor Guidance: Disperse shifting will be adjusted if there are unbiased anchors in semantic space as priors to attract modality embeddings towards clinical semantics, and domain knowledge embedding does a good job. We define knowledge fused embeddings  where image-weighted and text-weighted knowledge is globally contrasted. Semantic Knowledge Refinement: Wrong-converging pairs have distinct intrinsic responses on sample knowledge from image and text. Hence, we propose to utilize sample knowledge to refine these falsely gathered dissimilar pairs. We define where local semantic-weighted image and text embeddings are contrasted. Vision Semantic Response: Instead of matching single token with image subregions in  ), and the fusion of knowledge will be consolidated as below: where there is an in-sample local contrast between H IS i and vision features.",vol1
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Semantic Bridge Guidance:,"We propose to narrow disperse shifting enlarged by the modality gap between vision and language. Specifically, the gap is bridged by the fusion of domain knowledge which is better compatible with text: where the image-weighted domain knowledge is contrasted with text features between samples. Finally, L SG is aggregated by these four parts as below: 3 Experiment Experiment Protocol: Pre-training performs on MIMIC-CXR ",vol1
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,KoBo Init,ImageNet Init Basilar Atelectasis. Comparison Study: Table  Ablation Study: As is demonstrated in Fig.  Qualitative Analysis: In Fig. ,vol1
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,4.0,Conclusion,"In our paper, we propose a Knowledge-Boosting Contrastive Vision-Language Pre-traing framework (KoBo). Sample and domain knowledge are used to differentiate noisy negative samples and supplement the correspondence between modality and clinical knowledge. Our experiments on eight tasks verify the effectiveness of our framework. We hope that our work will encourage more research on knowledge-granularity alignment in medical vision-language learning.",vol1
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,1.0,Introduction,"Recently, as representative of generative self-supervised learning (SSL) methods, masked autoencoder (MAE)  On the other hand, although numerous work has been proposed for applying MAE to medical imaging across different modalities including pathological images  Based on the aforementioned analysis, in this paper, we propose a deblurring masked auto-encoder framework, which is specifically designed for ultrasound image recognition. The primary motivation for the deblurring comes from the unique imaging properties of ultrasound, e.g., high noise-to-signal ratio. Compared with nature images, the subtle details within ultrasound are particularly important for downstream analysis (e.g., microcalcifications is an important sign for malignant nodules, which is represented as tiny bright spots in ultrasound  Furthermore, to the best of our knowledge, this paper is the first attempt to apply the MAE approach to ultrasound image recognition. Our work also addresses some fundamental concerns that are of great interest to the medical imaging community with the example of ultrasound, such as the importance of in-domain data pretraining for MAE in ultrasound, as well as the finding that SSL pretraining is consistently better than the supervised pretraining as with nature images. To conclude, our contributions can be summarized as follows: 1. We propose a deblurring MAE framework that is specifically designed for ultrasound images by incorporating a deblurring task into MAE pretraining. This is motivated by the fact that ultrasound images have a high noise-tosignal ratio, and in contrast to denoising for natural images, we demonstrate that deblurring is a better recipe for ultrasound images. 2. We explore the effectiveness of various image blurring methods in our deblurring MAE and find that a simple Gaussian blurring performs the best, showing superior transferability compared with the vanilla MAE. 3. We conduct experiments on more than 10k ultrasound images for pretraining and 4,494 images for downstream thyroid nodule classification. The results demonstrate the effectiveness of the proposed deblurring MAE, achieving state-of-the-art classification performance for ultrasound images. Note that, as a representative MIM approach, the MAE is adopted to validate our proposed deblurring pretraining in this work, our method can also be seamlessly integrated with other MIM-based approaches such as ConvMAE ",vol1
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,2.1,Preliminary: MAE,"The MAE pipeline consists of two primary stages: self-supervised pretraining and transferring for downstream tasks. During the self-supervised pretraining, the model is trained to reconstruct masked input image patches using an asymmetric encoder-decoder architecture. The encoder is typically a ViT ",vol1
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,2.2,Our Proposed Deblurring MAE,"Similar to MAE, our proposed deblurring MAE also contains pretraining and transfer learning for downstream tasks. We employ the same asymmetric encoder-decoder architecture as the original MAE. Deblurring MAE Pretraining. For the pretraining, besides the original masked image modeling task in the MAE, we introduce one additional task, i.e., deblurring, into the pretraining thus making the pretraining as deblurring pretraining. As shown in Fig.  Specifically, the original ultrasound image x is first blurred by a chosen image blurring operation Blurring to obtain x b . After that, several patches in the blurred image x b are randomly masked by the Masking operation with a predefined ratio to obtain x m b . Next, the masked blurred image x m b is passed as input to the ViT Encoder, which generates a latent representation h. Finally, the Decoder receives the representation h and outputs reconstructed image x. The image blurring operation Blurring is a commonly used technique for reducing the sharpness or details of an image, resulting in a smoother, lessdetailed appearance. There exist many different methods for image blurring, with most of them involving the averaging of neighboring pixels in some way. In Fig.  Gaussian blur involves convolving an input image with a Gaussian kernel G(σ), which is a two-dimensional Gauss function that represents a normal distribution with standard deviation of σ. Mathematically, Gaussian blur can be defined as follows: where * denotes the convolution operation, and (u, v) represents the coordinates in the kernel. The degree of blurring (i.e., blurriness) in the resulting image is determined by the standard deviation σ. The SRAD is a nonlinear anisotropic diffusion technique for removing speckled noises, which has been extensively used in medical ultrasound images, due to its edge-sensitivity for speckled images and powerful preservation of useful information. The SRAD operation is implemented by repeating an anisotropic diffusion equation for N iterations. It can be formally given as: where x is the original image, N stands for the number of iterations, t means time. x(i, j, k) and c(i, j, k) represent the image and diffusivity coefficient at iteration k, respectively. ∇x is the gradient of x and div is the divergence operator. The larger N or t leads to a blurrier resulting image. The pixel-wise MSE between the reconstructed image x and the original image x is utilized as the loss function during pretraining: It should be noted that a key difference from MAE is that we compute the loss across all patches, including the masked ones. This operation is necessary due to the fact that our blurring operation covers the entire image. Through the use of the proposed deblurring MAE pretraining, we aim to leverage both masked image modeling and deblurring in order to learn a robust and effective latent representation that could be successfully applied to a range of downstream tasks. Deblurring MAE Transfer. After the deblurring MAE pretraining, only the pre-trained encoder is transferred to the downstream thyroid nodule classification task. One multi-layer perceptron (MLP) head is appended after the pretrained encoder. The transfer learning pipeline is shown in Eq. 4: It should be noted here that, in order to prevent data distribution shift between pretraining and transfer stages, the original image x also needs to be blurred before fed into the pre-trained encoder during transfer learning. The crossentropy loss between ground-truth classification label y and predicted label ŷ is used as the loss function:",vol1
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,3.1,Experimental Settings,"Dataset. All thyroid ultrasound images used in our study for both pretraining and downstream classification were acquired at West China Hospital with ethical approval. We use a total of 10,675 images for pretraining and 4,493 images for the downstream classification. To avoid any potential data leakage, the images used in pretraining were not included in the test set for thyroid nodule classification. The downstream classification dataset contains 2,576 benign and 1,917 malignant cases. We randomly split the dataset into train/validation/test subsets with a 3:1:1 ratio. The classification ground-truth labels were obtained either from the fine-needle aspiration for malignant nodules or clinical diagnosis by senior radiologists for benign nodules. Implementation Details. We use a mask ratio of 75% during the pretraining. We set the batch size to 256 for both pretraining and end-to-end fine-tuning, and 1024 for linear probing. The epochs of pretraining is 12,000 due to our relatively small data. The full detailed experimental settings are presented in the appendix. We implement our approach based on PyTorch. The image size for both pretraining and transfer learning is 224 × 224. For classification, we choose the model that performs the best on the validation set as the final model to evaluate on the test set. Three widely used metrics accuracy (ACC), F1-score (F1), and the area under the receiver operating characteristic (AUROC) are utilized for classification performance evaluation.",vol1
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,3.2,Results and Comparisons,"Our Deblurring MAE vs. Vanilla MAE. First of all, in order to evaluate the effectiveness of the proposed deblurring MAE for ultrasound images, we compare the transfer learning performance between our deblurring MAE and the vanilla MAE. Table  Comparison with State-of-the-Art Approaches. Secondly, we also compare our approach with more approaches and the results are listed in Table  We implement two variants of our deblurring MAE which differ in blurring operation: the SRAD with N equals to 40 and t equals to 0.1, and the Gaussian blur with σ equals to 1.1. We compare with methods based on supervised learning or self-supervised learning. In addition, we still add the denoising MAE for comparison, although it has proved to be ineffective for ultrasound images based on our preliminary experiments. We adopt ViT-B as the architecture for these SSL-based methods except SimCLR  The Deblurring MAE Pretraining Can Improve the Transferability of Learned Representations. First of all, both the two variants of our proposed approach (Ours [SRAD] and Ours [Gaussian]) obtain much higher classification metrics compared with the MAE pretrained using ultrasound, which indicates the learned representation of our deblurring MAE is more effective than the vanilla MAE when transferred to downstream classification. In addition, Table  Ultrasound Pretraining is Better than ImageNet Pretraining, Better than Supervised Pretraining. Table  Hyper-parameter Choices for MAE Pretraining. We conduct experiments to explore hyper-parameter choices for MAE pretraining based on ViT-B, and the results are presented in Fig.  Visualization. The comparisons of reconstructed image examples among MAE, denoising MAE, and our proposed deblurring MAE are illustrated in Fig. ",vol1
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,4.0,Conclusion and Future Work,"In this paper, we propose a novel deblurring MAE by incorporating deblurring into the proxy task during MAE pretraining for ultrasound image recognition. The deblurring task is implemented by inserting image blurring operation prior to the random masking during pretraining. The integration of deblurring enables the pretraining pay more attention to recovering the intricate details presented in ultrasound images, which are critical for downstream image classification. We explore the effect of several different image blurring methods and find that Gaussian blurring achieves the best performance and only a limited range of blurriness has a beneficial effect for pretraining. Based on the optimal blurring method and blurriness, our deblurring MAE achieves state-of-the-art performance in the downstream classification of ultrasound images, indicating the effectiveness of incorporating deblurring into MAE pretraining for ultrasound image recognition. However, this work has some limitations. For example, only one downstream task: nodule classification is evaluated in this study. We plan to extend our approach to include more tasks such as segmentation in the future.",vol1
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 34.,vol1
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,1.0,Introduction,"Semantic segmentation plays a vital role in pathological image analysis. It can help people conduct cell counting, cell morphology analysis, and tissue analysis, which reduces human labor  Recently,  Nonetheless, several recent investigations have demonstrated that the domain adaptation methods for source-free white-box models still present a privacy risk due to the potential leakage of model parameters  In this paper, we present a novel source-free domain adaptation framework for cross-tissue cell segmentation without accessing both source domain data and model parameters, which can seamlessly integrate heterogeneous models from different source domains into any cell segmentation network with high generality. To the best of our knowledge, this is the first study on the exploration of multi-source black-box domain adaptation for cross-tissue cell segmentation. In this setting, conventional multi-source ensemble methods are not applicable due to the unavailability of model parameters, and simply aggregating the black-box outputs would introduce a considerable amount of noise, which can be detrimental to the training of the target domain model. Therefore, we develop two strategies within this new framework to address this issue. Firstly, we propose a pixel-level multi-source domain weighting method, which reduces source domain noise by knowledge weighting. This method effectively addresses two significant challenges encountered in the analysis of cellular images, namely, the uncertainty in source domain output and the ambiguity in cell boundary semantics. Secondly, we also take into account the structured information from cells to images, which may be overlooked during distillation, and design an adaptive knowledge voting strategy. This strategy enables us to ignore low-confidence regions, similar to Cutout ",vol1
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,2.0,Method,Overview: Figure ,vol1
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,,Knowledge Distillation by Weighted Logits Map:,"We denote D N S = {X s , Y s } N as a collection of N source domains and D T = {X i t , Y j t } as single target domain, where the number of labeled instances Y j t X i t . We are only provided with black-box models {f n s } N n=1 trained on multiple source domains {x i s , y i s } N n=1 for knowledge transfer. The parameters {θ n s } N n=1 of these source domain predictors are not allowed to participate in gradient backpropagation as a result of the privacy policy. Thus, our ultimate objective is to derive a novel student model f t : X t → Y t that is relevant to the source domain task. Accordingly, direct knowledge transfer using the output of the source domain predictor may lead to feature bias in the student model due to the unavoidable covariance  where (i, j) denotes centre of region, and k denotes the size of k-square-neighbors. Firstly, we develop a pixel-level predictive uncertainty algorithm to aid in assessing the correlation between multiple source domains and the target domain. For a given target image x t ∈ X i t , we initially feed it into the source predictors {f n s } N n=1 to obtain their respective prediction {p n s } N n=1 . To leverage the rich semantic information from the source domain predictor predictions, we utilize predictive entropy of the softmax outputs to measure the prediction uncertainty scores. In the semantic segmentation scenario of C-classes classification, we define the pixel-level uncertainty score U (i,j) n as follow: where O n s denotes softmax output,i.e.,O n s = softmax(p n s ) from nth source predictor. Due to the unique characteristics of cell morphology, merely relying on uncertainty information is insufficient to produce high-quality ensemble logits map that accurately capture the relevance between the source and target domains. The target pseudo-label for the nth predictor f n s can be obtained by applying the softmax function to the output and selecting the category with the highest probability score, i.e., Y t = arg max c∈{1,...,C} (softmax(p n s )). Then according to C-classes classification tasks, we divide the cell region into C subsets, After that, we determine the degree of impurity in an area of interest by analyzing the statistics of the boundary region, which represents the level of semantic information ambiguity. Specifically, the number of different objects within the area is considered a proxy for its impurity level, with higher counts indicating higher impurity.The boundary impurity P (i,j) can be calculated as: where | • | denotes the number of pixels in the area. By assigning lower weights to the pixels with high uncertainty and boundary ambiguity, we can obtain pixel-level weight scores W n for each p n s , i.e., where denotes element-wise matrix multiplication. According to the pixellevel weight, we will obtain an ensemble logits map M = N n=1 W n • p n s . And the object of the knowledge distillation is a classical regularization term  where D kl denotes the Kullback-Leibler (KL) divergence loss. Adaptive Pseudo-Cutout Label: As previously mentioned, the outputs from the source domain black-box predictors have been adjusted by the pixel-level weight. However, they are still noisy and only pixel-level information is considered while ignoring structured information in the knowledge distillation process. Thus, we utilize the output of the black-box predictor on the target domain to produce an adaptive pseudo-cutout label, which will be employed to further regularize the knowledge distillation process. We have revised the method in  where α is empirically set as 0.9. Then we will aggregate the voting scores, i.e., V (i,j) = N n=1 V (i,j) n and determine whether to retain each pixel using an adaptive vote gate G ∈ {1, 2, 3, etc.}. By filtering with a threshold and integrating the voting strategy, we generate high-confidence pseudo-labels that remain effective even when the source and target domains exhibit covariance. Finally, we define the ensemble result as a pseudo-cutout label Ps and employ consistency regularization as below: where l ce denotes cross-entropy loss function. Loss Functions: Finally, we incorporate global structural information about the predicted outcome of the target domain into both distillation and semisupervised learning. To mitigate the noise effect of the source domain predictors, we introduce maximize mutual information targets to facilitate discrete representation learning by the network. We define E(p) =i p i log p i as conditional entropy. The object can be described as follow: where the increasing H(Y t ) and the decreasing H(Y t |X t ) help to balances class separation and classifier complexity  Finally, we get the overall objective: where L sup denotes the ordinary cross-entropy loss for supervised learning and we set the weight of each loss function to 1 in the training.",vol1
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,3.0,Experiments,"Dataset and Setting: We collect four pathology image datasets to validate our proposed approach. Firstly, we acquire 50 images from a cohort of patients with Triple Negative Breast Cancer (TNBC), which is released by Naylor et al  Experimental Results: To validate our method, we compare it with the following approaches: (1) CellSegSSDA  In addition, the experimental results also show that simply combining multiple source data into a traditional single source will result in performance degradation in some cases, which also proves the importance of studying multi-source domain adaptation methods. Ablation Study: To evaluate the impact of our proposed methods of weighted logits(WL), pseudo-cutout label(PCL) and maximize mutual information(MMI) on the model performance, we conduct an ablation study. We compare the baseline model with the models that added these three methods separately. We chose CRC, KIRC and BRCA as our source domains, and TNBC as our target domain. The results of these experiments, presented in the Table ",vol1
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,4.0,Conclusion,"Our proposed multi-source black-box domain adaptation method achieves competitive performance by solely relying on the source domain outputs, without the need for access to the source domain data or models, thus avoiding information leakage from the source domain. Additionally, the method does not assume the same architecture across domains, allowing us to learn lightweight target models from large source models, improving learning efficiency. We demonstrate the effectiveness of our method on multiple public datasets and believe it can be readily applied to other domains and adaptation scenarios. Moving forward, we plan to integrate our approach with active learning methods to enhance annotation efficiency in the semi-supervised setting. By leveraging multi-source domain knowledge, we aim to improve the reliability of the target model and enable more efficient annotation for better model performance.",vol1
Additional Positive Enables Better Representation Learning for Medical Images,1.0,Introduction,"Self-supervised learning (SSL) has been extremely successful in learning good image representations without human annotations for medical image applications like classification  As the positive pair in BYOL is generated from the same image, the diversity of features within the positive pair could be quite limited. For example, one skin disease may manifest differently in different patients or locations, but such information is often overlooked in the current BYOL framework. In this paper, we argue that such feature diversity can be increased by adding additional positive pairs from other samples with the same label (a.k.a. True Positives). Identifying such pairs without human annotation is challenging because of the unrelated information in medical images, such as the background normal skin areas in dermoscopic images. One straightforward way to detect positive pairs is using feature similarity: two images are considered positive if their representations are close to each other in the feature space. However, samples with different labels might also be close in the feature space because the learned encoder is not perfect. Considering them as positive might further pull them together after learning, leading to degraded performance. To solve this problem, we propose BYOL-TracIn, which improves vanilla BYOL using the TracIn influence function. Instead of quantifying the similarity of two samples based on feature similarity, we propose using TracIn to estimate their similarity by calculating the impact of training one sample on the other. TracIn ",vol1
Additional Positive Enables Better Representation Learning for Medical Images,2.0,Related Work,Self-supervised Learning. Most SSL methods can be categorized as either generative  Influence Function. The influence function (IF) was first introduced to machine learning models in ,vol1
Additional Positive Enables Better Representation Learning for Medical Images,3.1,Framework Overview,"Our BYOL-TracIn framework is built upon classical BYOL method  During training, the representations distance of x 1 and x 3 will also be minimized. We think this additional positive pair can increase the variance and diversity of the features of the same label, leading to better clustering in the feature space and improved learning performance. The pairwise TracIn matrix is computed using first-order gradient approximation which will be discussed in the next section. For simplicity, this paper only selects the top-1 additional sample, but our method can be easily extended to include top-k (k > 1) additional samples.",vol1
Additional Positive Enables Better Representation Learning for Medical Images,3.2,Additional Positive Selection Using TracIn,"Idealized TracIn and Its First-order Approximation. Suppose we have a training dataset D = {x 1 , x 2 , ..., x n } with n samples. f w (•) is a model with parameter w ∈ R, and (w, x i ) is the loss function when model parameter is w and training example is x i . The training process in iteration t can be viewed as minimizing the training loss (w t , x t ) and updating parameter w t to w t+1 using gradient descent (suppose only x t ∈ D is used for training in each iteration). Then the idealized TracIn of one sample x i on another sample x k can be defined as the total loss reduction by training x i in the whole training process. where T is the total number of iterations. If stochastic gradient descent is utilized as the optimization method, we can approximately express the loss reduction after iteration t as The parameter change in iteration t is Δw t = w t+1w t = -η t (w t , x t ), in which η t is the learning rate in iteration t, and x t is the training example. Since η t is usually small during training, we can ignore the high order term O(||Δw t || 2 ), and the first-order TracIn can be formulated as: The above equation reveals that we can estimate the influence of x i on x k by summing up their gradient dot products across all training iterations. In practical BYOL training, the optimization is usually done on mini-batches, and it is impossible to save the gradients of a sample for all iterations. However, we can use the TracIn of the current iteration to represent the similarity of two samples in the mini-batch because we care about the pairwise relative influences instead of the exact total values across training. Intuitively, if the TracIn of two samples is large in the current iteration, this means that the training of one sample can benefit the other sample a lot because they share some common features. Therefore, they are similar to each other. Efficient Batch-wise TracIn Computation. Equation 2 requires the gradient of each sample in the mini-batch for pairwise TracIn computation. However, it is prohibitively expensive to compute the gradient of samples one by one. Moreover, calculating the dot product of gradients on the entire model is computationally and memory-intensive, especially for large deep-learning models where there could be millions or trillions of parameters. Therefore, we work with the gradients of the last linear layer in the online predictor. As current deep learning frameworks (e.g., Pytorch and TensorFlow) do not support per-sample gradient when the batch size is larger than 1, we use the following method to efficiently compute the per-sample gradient of the last layer. Suppose the weight matrix of the last linear layer is W ∈ R m×n , where m and n are the numbers of input and output units. f (q) = 2 -2 • q, z /( q 2 • z 2 ) is the standard BYOL loss function, where q is the online predictor output (a.k.a., logits) and z is the target encoder output that can be viewed as a constant during training. We have q = W a, where a is the input to the last linear layer. According to the chain rule, the gradient of the last linear layer can be computed as W f (q) = q f (q)a T , in which the gradient of the logits can be computed by: Therefore, the TracIn of sample x i and x k at iteration t can be computed as: Equation 3 and 4 tell us that the per-sample gradient of the last linear layer can be computed by using the inputs of this layer and the gradient of the output logits for each sample, which can be achieved with only one forward pass on the mini-batch. This technique greatly speeds up the TracIn computation and makes it possible to be used in BYOL. Using Pre-trained Model to Increase True Positives. During the pretraining stage of BYOL, especially in the early stages, the model can be unstable and may focus on unrelated features in the background instead of the target features. This can result in the selection of wrong positive pairs while using TracIn. For example, the model may identify all images with skin diseases on the face as positive pairs, even if they are from different diagnostics, as it focuses on the face feature instead of the diseases. To address this issue, we suggest using a pre-trained model to select additional positives with TracIn to guide BYOL training. This is because a pre-trained model is more stable and well-trained to focus on the target features, thus increasing the selected true positive ratio.",vol1
Additional Positive Enables Better Representation Learning for Medical Images,4.1,Experimental Setups,"Datasets. We evaluate the performance of the proposed BYOL-TracIn on four publicly available medical image datasets. (1) ISIC 2019 dataset is a dermatology dataset that contains 25,331 dermoscopic images among nine different diagnostic categories  Training Details. We use Resnet18 as the backbone. The online projector and predictor follow the classical BYOL  Baselines. We compare the performance of our method with a random initialization approach without pre-training and the following SOTA baselines that involve pre-training. (1) BYOL ",vol1
Additional Positive Enables Better Representation Learning for Medical Images,4.2,Semi-supervised Learning,"In this section, we evaluate the performance of our method by finetuning with the pre-trained encoder on the same dataset as pre-training with limited annotations. We sample 10% or 50% of the labeled data from ISIC 2019 and ChestXray training sets and finetune the model for 100 epochs on the sampled datasets. Data augmentation is the same as pre-training. Table  From Table  To further demonstrate the superiority of TracIn over Feature Similarity (FS) in selecting additional positive pairs for BYOL, we use an image from ISIC 2019 as an example and visualize the top-3 most similar images selected by both metrics using a BYOL pre-trained model in Fig. ",vol1
Additional Positive Enables Better Representation Learning for Medical Images,4.3,Transfer Learning,"To evaluate the transfer learning performance of the learned features, we use the encoder learned from the pre-training to initialize the model on the downstream datasets (ISIC 2019 transfers to ISIC 2016, and ChestX-ray transfers to Shenzhen). We finetune the model for 50 epochs and report the precision and AUC on ISIC 2016 and Shenzhen datasets, respectively. Table ",vol1
Additional Positive Enables Better Representation Learning for Medical Images,5.0,Conclusion,"In this paper, we propose a simple yet effective method, named BYOL-TracIn, to boost the representation learning performance of the vanilla BYOL framework. BYOL-TracIn can effectively identify additional positives from different samples in the mini-batch without using label information, thus introducing more variances to learned features. Experimental results on multiple public medical image datasets show that our method can significantly improve classification performance in both semi-supervised and transfer learning settings. Although this paper only discusses the situation of one additional pair for each image, our method can be easily extended to multiple additional pairs. However, more pairs will introduce more computation costs and increase the false positive rate which may degrade the performance. Another limitation of this paper is that BYOL-TracIn requires a pre-trained model to start with, which means more computation resources are needed to demonstrate its effectiveness.",vol1
Additional Positive Enables Better Representation Learning for Medical Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_12. ,vol1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,1.0,Introduction,"In recent years, deep learning (DL) methods have demonstrated remarkable performance in detecting and localizing tumors on ultrasound images  Domain adaptation (DA) has been extensively studied to alleviate the aforementioned limitations, the goal of which is to reduce the domain gap caused by the diversity of datasets from different domains  To alleviate the problem of pseudo-label-based UDA, in this work, we propose an advanced UDA framework based on self-supervised DA with a test-time finetuning network. Test-time adaptation methods have been developed  To summarize, our contributions are three-fold: • We design a self-supervised DA framework that includes a parameter search method and provide a mathematical justification for it. With our framework, we are able to identify the best-performing parameters that result in improved performance in DA tasks. • Our framework is effective at preserving privacy, since it carries out DA using only pre-trained network parameters, without transferring any patient data. • We applied our framework to the task of segmenting breast cancer from ultrasound imaging data, demonstrating its superior performance over competing UDA methods. Our results indicate that our framework is effective in improving the accuracy of breast cancer segmentation from ultrasound images, which could have potential implications for improving the diagnosis and treatment of breast cancer. Sample batches of (t, ?) ∼ T",vol1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,2.0,Methodology,return ŷ 15: End Output: Predictions ( ŷ) on T Fig. ,vol1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,2.1,Test-Time Fine-Tuning (TTFT) Network and Its Pipeline,"Network Architecture. Our proposed TTFT network is based on selfsupervised DA  In predicting segmentation labels in the target domain (T ), D FT is also involved in the main task, and the final prediction after the fine-tuning is  where L BCE and L GAN represent the loss functions for binary cross-entropy and generative adversarial network  Fine-Tuning in Target Domain. Since the pre-trained model is likely to produce imprecise predictions in T , the model should learn domain knowledge about T . To this end, in the pretext task, for self-supervised learning, the model is fine-tuned in T to generate synthetic images identical to the input images as below: where only D gen is fine-tuned to achieve memory efficiency and to decrease the fine-tuning time, and D S gen is fine-tuned as D S→T gen . Then, D S→T gen is transferred to D FT , and knowledge distillation via self-supervised learning is realized. Hence, the precise predictions in T could be provided by Benefits of Our Dual-Pipeline. Due to the symmetric property of mutual information in information entropy (H), we have As a result, the predictions made by the fine-tuned network in the target domain (T ) lead to reduced entropy, as shown below: ( Since D S seg is fully optimized for S in a supervised manner, it guarantees a baseline segmentation performance. Furthermore, since D T FT is fine-tuned in T ",vol1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,2.2,Parameter Fluctuation: Parameter Randomization Method,"Since the loss function and its values can vary based on the distribution of inputs, and different domains can have different distributions, the local minimum identified in the source domain (S) cannot be considered as the same local minimum in T , as illustrated in Fig. ",vol1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,,|X |,"x L(M (x; θ), x), and the local minimum is different in S and T as Θ S in Fig.  seg provides the baseline segmentation performance, D T F T should provide similar feature maps to achieve the baseline performance. To this end, the mid-feature maps generated should be similar, i.e., where C i represents the convolution in D T F T , F i represents i th feature map, and which can be expressed as: Here, we denote w i -w i = f i as the fluctuation vector in the vector space, and the condition f i = 0 indicates that the sum of the fluctuation vectors should be zero under the condition of |f i | < r 1. Hence, we achieve the condition for the parameter fluctuation that the centers of parameters of Θ S and θ T should be the same in the vector space, and the length of the fluctuation vector should be less than a certain small threshold (0 < r 1). Therefore, the parameter fluctuation aims to add random vectors of which length is less than 0 < r 1 on the parameters of Θ S , and the sum of vectors should be zero. To summarize, the parameter fluctuation aims to add randomness on Θ S as follows: (5) 3 Experiments",vol1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,3.1,Experimental Set-Ups,"To evaluate the segmentation performance of our TTFT framework, we used three different ultrasound databases: BUS ",vol1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,3.2,Comparison Analysis,"Since all compared DL models show similar D. Coef, only UDA performance is comparable as a control in our experiments. In this experiment, two databases were used for training, and the remaining database was used for testing. For instance, BUS in Fig. ",vol1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,4.0,Discussion and Conclusion,"In this work, we proposed a DL-based segmentation framework for multi-domain breast cancer segmentation on ultrasound images. Due to the low resolution of ultrasound images, manual segmentation of breast cancer is challenging even for expert clinicians, resulting in a sparse number of labeled data. To address this issue, we introduced a novel self-supervised DA network for breast cancer segmentation in ultrasound images. In particular, we proposed a test-time finetuning network to learn domain-specific knowledge via knowledge distillation by self-supervised learning. Since UDA is susceptible to error accumulation due to imprecise pseudo-labels, which can lead to degraded performance, we employed a self-supervised learning-based pretext task. Specifically, we utilized an autoencoder-based network architecture to generate synthetic images that matched the input images. Moreover, we introduced a randomized re-initialization module that injects randomness into network parameters to reposition the network from the local minimum in the source domain to a local minimum that is better suited for the target domain. This approach enabled our framework to efficiently fine-tune the network in the target domain and achieve better segmentation performance. Experimental results, carried out with three ultrasound databases from different domains, demonstrated the superior segmentation performance of our framework over other competing methods. Additionally, our framework is well-suited to a scenario in which access to source domain data is limited, due to data privacy protocols. It is worth noting that we used vanilla U-Net ",vol1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_52.,vol1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,3.3,Ablation Study,"In order to assess the effectiveness of each of the proposed modules, including the parameter fluctuation and fine-tuning methods, the ablation study was carried out. Since our framework contains three types of decoders, including D S seg , D fl seg , and D S→T seg for the fine-tuning, we mainly targeted those decoders in our ablation study. Table  Furthermore, Fig. ",vol1
Decoupled Consistency for Semi-supervised Medical Image Segmentation,1.0,Introduction,"Deep learning technology can significantly assist clinicians in clinical diagnosis through accurate and robust segmentation of lesions or organs from medical images  Pseudo labeling  Despite using all available prediction data, consistency regularization focuses more on how to get two similar predictions, such as data perturbation  To sum up, this paper proposed a novel decoupled consistency regularization strategy. Specifically, inspired by CPL, we first designed a consistency threshold related to pixel confidence (Wang et al.  Overall, our contributions are four-fold: (1) we proposed a novel decoupled consistent semi-supervised medical image segmentation framework. The framework fully exploits prediction data, decoupled prediction data into data for various functions, and maximizes each function's advantages. (2) A dynamic threshold is proposed that can separate the prediction data into consistent and inconsistent parts. This threshold can effectively reflect the model's learning state and encourage more diversity of data at the start of training. (3) A novel direction consistency strategy is proposed to optimize the inconsistent part. This strategy focuses on optimizing the data around the decision boundary. The results of the experiment demonstrate that the direction consistency strategy is superior to the conventional consistent regularization. ( ",vol1
Decoupled Consistency for Semi-supervised Medical Image Segmentation,2.0,Method,Figure ,vol1
Decoupled Consistency for Semi-supervised Medical Image Segmentation,2.1,Dynamic Consistency Threshold,"In recent years, the pseudo labeling method based on threshold has achieved great success. The sampling strategy of this method can be defined as follows: where θ dA uses bi-linear interpolation for up-sampling and θ dB uses transposed convolution for up-sampling. For the labeled data we calculate the loss Lseg between them and ground-truth, for the consistent part we calculate cross pseudo supervision loss Lcps, for the inconsistent part we calculate directional consistency loss L dc , for feature maps, we calculate the feature consistency loss where γ ∈ (0, 1) is a threshold used to select pseudo labels. p is the segmentation confidence map. FlexMatch  where B is the batch size, λ is the weight coefficient that increases with the training and we set λ = t tmax . In order to sample more unlabeled data, we conduct threshold evaluation on p A and p B and select a smaller threshold as our consistency threshold. We initialize λ t as 1 C , where C indicates the number of classes. The consistency threshold γ t is finally defined and adjusted as: where γ A t and γ B t represent the threshold of p A and p B .",vol1
Decoupled Consistency for Semi-supervised Medical Image Segmentation,2.2,Decoupled Consistency,"Inconsistent Part. We decouple the inconsistent part into unreliable data, which is probably going to appear at the decision boundary, and guidance data, which is more likely to occur near the high-density area. These two parts have the same index information. The distinction is that guidance data is more confident than unreliable data. Based on the smoothing assumption, the output of these two parts should be consistent and located in the high-density area. As a result, we should concentrate on optimizing the pixels around the decision boundary in order to bring them closer to the high-density region. We initially sharpen the confidence of these pixels to bring the high-confidence pixels closer to the high-density area. These are the sharpening processes: where o represents the output of the model and T ∈ (0, 1) represents the sharpening temperature. In the experiment, we set T = 0.5. By comparing Sp A and Sp B , the high-confidence parts hSp A , hSp B (See Appendix Algorithm 2.) and low-confidence parts lSp A , lSp B can be obtained. We take L2 loss as the loss function of this part. Note that we only optimize the low-confidence part and do not back-propagate the gradient of the high-confidence part. Therefore, our loss of directional consistency can be written as: Consistent Part. Similar to CPS  where P L A and P L B represent corresponding pseudo labels. Feature Part. We incorporate the feature map into the training process to further utilize the data. In order to reduce the amount of computation, we have carried out an average mapping of the feature map to reduce its dimension(R Cm×Hm×Wm -→ R Hm×Wm ). The mapping process is as follows: where, p > 1, f m represents the feature map of m-th layer, and f mi denotes the i-th slice of f m in the channel dimension, fm represents the corresponding mapping result. In the experiment, we set p = 2. Our feature consistency loss is as follows: where N is the number of pixels of fmi , n is the number of network layers, f e mi and f d mi represent i-th pixels of m-th feature map of decoder and encoder respectively. In this paper, only feature maps of decoder B were used to calculate the loss. Total Loss. The total loss is a weighted sum of the segmentation loss L seg and the other three losses: where L seg is a Dice loss, which is applied for the few labeled data. And hyperparameter β is set as an iteration-dependent warming-up function ",vol1
Decoupled Consistency for Semi-supervised Medical Image Segmentation,3.1,Dataset,We evaluated our methods on ACDC dataset ,vol1
Decoupled Consistency for Semi-supervised Medical Image Segmentation,3.2,Implementation Details,All comparisons and ablation experiments are performed using the same experimental setting for a fair comparison. They are conducted on PyTorch using an Intel(R) Xeon(R) CPU and NVIDIA GeForce RTX 1080 Ti GPU. We adopt U-net ,vol1
Decoupled Consistency for Semi-supervised Medical Image Segmentation,3.3,Results,"Comparison with Other Semi-supervised Methods. We use the metrics of Dice, Jaccard, 95% Hausdorff Distance (95HD), and Average Surface Distance (ASD) to evaluate the results. Table ",vol1
Decoupled Consistency for Semi-supervised Medical Image Segmentation,4.0,Conclusion,"This paper proposed a framework DC-Net for semi-supervised medical image segmentation. In view of the current problem of insufficient utilization of unlabeled data, our fundamental concept is to fully exploit the benefits of data with various functionalities. Based on this, we decouple the prediction data into consistent and inconsistent parts through a dynamic threshold. Furthermore, the inconsistent part is further decoupled into guidance data and unreliable data, and optimized by a novel directional consistency strategy. Our method yielded excellent outcomes on both the ACDC and PROMISE12 datasets. In addition, directional consistency shows promising potential in the experiment, and future research will further explore the selection and treatment of directions.",vol1
Decoupled Consistency for Semi-supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 53.,vol1
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,1.0,Introduction,"Positron emission tomography (PET) is a sensitive nuclear imaging technique, and plays an essential role in early disease diagnosis, such as cancers and Alzheimer's disease  In recent years, many enhancement algorithms have been proposed to improve PET image quality. Among the earliest are filtering-based methods such as non-local mean (NLM) filter  Fortunately, the recent glowing diffusion model  Taking all into consideration, we propose the SPET-only unsupervised PET enhancement (uPETe) framework based on the latent diffusion model. Specifically, uPETe has an encoder-<diffusion model>-decoder structure that first uses the encoder to compress input the LPET/SPET images into latent representations, then uses the latent diffusion model to learn/estimate the distribution of SPET latent representations, and finally uses the decoder to recover SPET images from the estimated SPET latent representations. The keys of our uPETe include 1) compressing the 3D PET images into a lower dimensional space for reducing the computational cost of diffusion model, 2) adopting the Poisson noise, which is the dominant noise in PET imaging  Our work had three main features/contributions: i) proposing a clinicallyapplicable unsupervised PET enhancement framework, ii) designing three targeted strategies for improving the diffusion model, including PET image compression, Poisson diffusion, and CT-guided cross-attention, and iii) achieving better performance than state-of-the-art methods on the collected PET datasets.",vol1
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,2.0,Method,The framework of uPETe is illustrated in Fig. ,vol1
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,2.1,Image Compression,"The conventional diffusion model is computationally-demanding due to its numerous inverse denoising steps, which severely restricts its application to 3D PET enhancement. To overcome this limitation, we adopt two strategies including 1) compressing the input image and 2) reducing the diffusion steps (as described in Sect. 2.3). Similar to ",vol1
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,2.2,Latent Diffusion Model,"After compressing the input PET image, its latent representation is fed into the latent diffusion model, which is the key to achieving the SPET-only unsupervised PET enhancement. As described above, the LPET can be viewed as noisy SPET (even in the compressed space), so the diffusion process from SPET to pure noise actually covers the situations of LPET. That is, the diffusion model trained with SPET is capable of estimating SPET from the noisy sample (diffused from LPET). But the diffusion model is developed from photographic images, which have significant difference with the detail-sensitive PET images. To improve its applicability for PET images, we design several targeted strategies for the diffusion process and inverse process, namely Poisson diffusion and CT-guided cross-attention, respectively. Poisson Diffusion. In conventional diffusion models, the forward process typically employs Gaussian noise to gradually perturb input samples. However, in PET images, the dominant source of noise is Poisson noise, rather than Gaussian noise. Considering this, in our uPETe we choose to adopt Poisson diffusion to perturb the input samples, which facilitates the diffusion model for achieving better performance on the PET enhancement task. Let z t be the perturbation sample in Poisson diffusion, where t = 0, 1, ..., T . Then the Poisson diffusion can be formulate as follows: At each diffusion step, we apply the perturb function to the previous perturbed sample z t-1 by imposing a Poisson noise with an expectation of λ t , which is linearly interpolated from [0, 1] and incremented with t. In our implementation, we apply the same Poisson noise imposition operation as in  CT-Guided Cross-Attention. The attenuation correction of PET typically relies on the corresponding anatomical image (CT or MR), resulting in a PET scan usually accompanied by a CT or MR scan. To fully utilize the extramodality images (i.e., CT in our work) as well as improve the applicability of diffusion models, we design a CT-guided cross-attention to incorporate the CT images into the reverse process for assisting the recovery of structural details. As shown in Fig.  where d is the number of channels, B is the position bias, and Conv(•) denotes the 1 × 1 × 1 convolution with stride of 1.",vol1
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,2.3,Implementation Details,"Typically, the trained diffusion model generates target images from random noise, requiring a large number of steps T to make the final perturbed sample (z T ) close to pure noise. However, in our task, the target SPET image is generated from a given LPET image during testing, and making z T as close to pure noise as possible is not necessary since the remaining PET-related information can also benefit the image recovery. Therefore, we can considerably reduce the number of diffusion steps T to accelerate the model training, and T is set to 400 in our implementation. We evaluate the quantitative results using two metrics, including Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM).  3 Experiments",vol1
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,3.1,Dataset,"Our dataset consists of 100 SPET images for training and 30 paired LPET and SPET images for testing. Among them, 50 chest-abdomen SPET images are collected from (total-body) uEXPLORER PET/CT scanner ",vol1
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,3.2,Ablation Analysis,"To verify the effectiveness of our proposed strategies, i.e. Poisson diffusion process and CT-guided cross-attention, we design another four variant latent diffusion models (LDMs) with the same compression model, including: 1) LDM: standard LDM; 2) LDM-P: LDM with Poisson diffusion process; 3) LDM-CT: LDM with CT-guided cross-attention; 4) LDM-P-CT: LDM with Poisson diffusion process and CT-guided cross-attention. All methods use the same experimental settings, and their quantitative results are given in Table  From Table ",vol1
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,3.3,Comparison with State-of-the-Art Methods,"We further compare our uPETe with several state-of-the-art PET enhancement methods, which can be divided into two classes: 1) fully-supervised methods, including LA-GAN  Quantitative Comparison: Table  Qualitative Comparison: In Fig. ",vol1
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,3.4,Generalization Evaluation,"We further evaluate the generalizability of our uPETe to tracer dose changes by simulating Poisson noise on SPET to produce different doses for LPET, which is a common way to generate noisy PET data ",vol1
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,4.0,Conclusion and Limitations,"In this paper, we have developed a clinically-applicable unsupervised PET enhancement framework based on the latent diffusion model, which uses only the clinically-available SPET data for training. Meanwhile, we adopt three strategies to improve the applicability of diffusion models developed from photographic images to PET enhancement, including 1) compressing the size of the input image, 2) using Poisson diffusion, instead of Gaussian diffusion, and 3) designing CT-guided cross-attention to enable additional anatomical images (e.g., CT) to aid the recovery of structural details in PET. Validated by extensive experiments, our uPETe achieved better performance than both state-of-the-art unsupervised and fully-supervised PET enhancement methods, and showed stronger generalizability to the tracer dose changes. Despite the advance of uPETe, our current work still suffers from a few limitations such as (1) lacking theoretical support for our Poisson diffusion, which is just an engineering attempt, and 2) only validating the generalizability of uPETe on a simulated dataset. In our future work, we will complete the design of Poisson diffusion from theoretical perspective, and collect more real PET datasets (e.g., head datasets) to comprehensively validate the generalizability of our uPETe.",vol1
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,1.0,Introduction,"Automated segmentation of blood vessels in 3D medical images is a crucial step for the diagnosis and treatment of many diseases, where the segmentation can aid in visualization, help with surgery planning, be used to compute biomarkers, and further downstream tasks. Automatic vessel segmentation has been extensively studied, both using classical computer vision algorithms  Manually delineating 3D vessels typically involves visualizing and annotating a 3D volume through a sequence of 2D cross-sectional slices, which is not a good medium for visualizing 3D vessels. This is because often only the cross-section of a vessel is visible in a 2D slice. In order to segment a vessel, the annotator has to track the cross-section of that vessel through several adjacent slices, which is especially tedious for curved or branching vessel trees. Projecting 3D vessels to a 2D plane allows for the entire vessel tree to be visible within a single 2D image, providing a more robust representation and potentially alleviating the burden of manual annotation. Kozinski et al.  To achieve 3D vessel segmentation with only 2D supervision from projections, we first investigate which viewpoints to annotate in order to maximize segmentation performance. We show that it is feasible to segment the full extent of vessels in 3D images with high accuracy by annotating only a single randomlyselected 2D projection per training image. This approach substantially reduces the annotation effort, even compared to works training only on 2D projections. Secondly, by mapping the 2D annotations to the 3D space using the depth of the MIPs, we obtain a partially segmented 3D volume that can be used as an additional supervision signal. We demonstrate the utility of our method on the challenging task of peripancreatic arterial segmentation on contrast-enhanced arterial-phase computed tomography (CT) images, which feature large variance in vessel diameter. Our contribution to 3D vessel segmentation is three-fold: • Our work shows that highly accurate automatic segmentation of 3D vessels can be learned by annotating single MIPs. • Based on extensive experimental results, we determine that the best annotation strategy is to label randomly selected viewpoints, while also substantially reducing the annotation cost. • By incorporating additional depth information obtained from 2D annotations at no extra cost to the annotator, we almost close the gap between 3D supervision and 2D supervision.",vol1
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,2.0,Related Work,"Learning from Weak Annotations. Weak annotations have been used in deep learning segmentation to reduce the annotation effort through cheaper, less accurate, or sparser labeling  Incorporating Depth Information. Depth is one of the properties of the 3D world. Loss of depth information occurs whenever 3D data is projected onto a lower dimensional space. In natural images, depth loss is inherent through image acquisition, therefore attempts to recover or model depth have been employed for 3D natural data. For instance, Fu et al. ",vol1
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,3.0,Methodology,"Overview. The maximum intensity projection (MIP) of a 3D volume I ∈ R Nx×Ny×Nz is defined as the highest intensity along a given axis: For simplicity, we only describe MIPs along the z-axis, but they can be performed on any image axis. Exploiting the fact that arteries are hyperintense in arterial phase CTs, we propose to annotate MIPs of the input volume for binary segmentation. The hyperintensities of the arteries ensures their visibility in the MIP, while additional processing removes most occluding nearby tissue (Sect. 4). Given a binary 2D annotation of a MIP A ∈ {0, 1} Nx×Ny , we map the foreground pixels in A to the original 3D image space. This is achieved by using the first and last z coordinates where the maximum intensity is observed along any projection ray. Owing to the fact that the vessels in the abdominal cavity are relatively sparse in 2D projections and most of the occluding tissue is removed in postprocessing, this step results in a fairly complete surface of the vessel tree. Furthermore, we can partially fill this surface volume, resulting in a 3D depth map D, which is a partial segmentation of the vessel tree. We use the 2D annotations as well as the depth map to train a 3D segmentation network in a weakly supervised manner. An overview of our method is presented in Fig.  Depth Information. We can view MIP as capturing the intensity of the brightest pixel along each ray r xy ∈ R Nz , where r xy (z) = I(x, y, z). Along each projection ray, we denote the first and last z coordinates which have the same intensity as the MIP to be the forward depth z fw = arg max z I(x, y, z) and backward depth z bw = arg min z I(x, y, z). This information can be utilized for the following: (1) enhancing the MIP visualization, or (2) providing a way to map pixels from the 2D MIP back to the 3D space (depth map). The reason why the maximum intensity is achieved multiple times along a ray is because our images are clipped, which removes a lot of the intensity fluctuations.  where α ∈ [0, 1]. Our final loss is a convex combination between: (a) the crossentropy(CE) of the network output projected to 2D and the 2D annotation, as well as (b) the cross-entropy between the network output and the depth map, but only applied to positive pixels in the depth map. Notably, the 2D loss constrains the shape of the vessels, while the depth loss promotes the segmentation of the vessel interior.",vol1
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,4.0,Experimental Design,"Dataset. We use an in-house dataset of contrast-enhanced abdominal computed tomography images (CTs) in the arterial phase to segment the peripancreatic arteries  Image Augmentation and Transformation. As the annotations lie on a 2D plane, 3D spatial augmentation cannot be used due to the information sparsity in the ground truth. Instead, we apply an invertible transformation T to the input volume and apply the inverse transformation T -1 to the network output before applying the loss, such that the ground truth need not be altered. A detailed description of the augmentations and transformations used can be found in Table  Training and Evaluation. We use a 3D U-Net ",vol1
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,5.0,Results,"The Effectiveness of 2D Projections and Depth Supervision. We compare training using single random viewpoints with and without depth information against baselines that use more supervision. Models trained on full 3D ground truth represent the upper bound baseline, which is very expensive to annotate. We implement  With the exception of the single fixed viewpoint baselines where the models have the tendency to diverge towards over-or segmentation, we perform binary holefilling on the output of all of our other models, as producing hollow objects is a common under-segmentation issue. In Table  In terms of other metrics, randomly chosen projection viewpoints with and without depth improve both recall and skeleton recall even compared to fully 3D annotations, while generally reducing precision. We theorize that this is because the dataset itself contains noisy annotations and fully supervised models better overfit to the type of data annotation, whereas our models converge to following the contrast and segmenting more vessels, which are sometimes wrongfully labeled as background in the ground truth. MSD are not very telling in our dataset due to the noisy annotations and the nature of vessels, as an under-or over-segmented vessel branch can quickly translate into a large surface distance. The Effect of Dataset Size. We vary the size of the training set from |D tr | = 80 to as little as |D tr | = 10 samples, while keeping the size of the validation and test sets constant, and train models on single random viewpoints. In Table ",vol1
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,6.0,Conclusion,"In this work, we present an approach for 3D segmentation of peripancreatic arteries using very sparse 2D annotations. Using a labeled dataset consisting of single, randomly selected, orthogonal 2D annotations for each training sample and additional depth information obtained at no extra cost, we obtain accuracy almost on par with fully supervised models trained on 3D data at a mere fraction of the annotation cost. Limitations of our work are that the depth information relies on the assumption that the vessels exhibit minimal intensity fluctuations within local neighborhoods, which might not hold on other datasets, where more sophisticated ray-tracing methods would be more effective in locating the front and back of projected objects. Furthermore, careful preprocessing is performed to eliminate occluders, which would limit its transferability to datasets with many occluding objects of similar intensities. Further investigation is needed to quantify how manual 2D annotations compare to our 3D-derived annotations, where we expect occluders to affect the annotation process.",vol1
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_14.,vol1
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,1.0,Introduction,"Statistical shape modeling (SSM) is a powerful tool in medical image analysis and computational anatomy to quantify and study the variability of anatomical structures within populations. SSM has shown great promise in medical research, particularly in diagnosis  Over the years, several SSM approaches have been developed that implicitly represent the shapes (deformation fields  SSM performance depends on the underlying process used to generate shape correspondences and the quality of the input data. Various correspondence generation methods exist, including non-optimized landmark estimation and parametric and non-parametric correspondence optimization. Non-optimized methods manually label a reference shape and warp the annotated landmarks using registration techniques  Traditional SSM methods assume that population variability follows a Gaussian distribution, which implies that a linear combination of training shapes can express unseen shapes. However, anatomical variability can be far more complex than this linear approximation, in which case nonlinear variations normally exist (e.g., bending fingers, soft tissue deformations, and vertebrae with different types). Furthermore, conventional SSM pipelines are computationally intensive, where inferring PDMs on new samples entail an optimization process. Deep learning-based approaches for SSM have emerged as a promising avenue to overcoming these limitations. Deep learning models can learn complex nonlinear representations of the shapes, which can be used to generate shape models. Moreover, they can efficiently perform inference on new samples without computation overhead or re-optimization. Recent works such as FlowSSM  In this paper, we introduce Mesh2SSM 1. We introduce Mesh2SSM, a fully unsupervised correspondence generation deep learning framework that operates directly on meshes. Mesh2SSM uses an autoencoder to extract the shape descriptor of the mesh and uses this descriptor to transform a template point cloud using IM-Net  To motivate the need for the mesh feature encoder and study the effect of the template selection, we considered the box-bump dataset, a synthetic dataset of 3D shapes of boxes with a moving bump. In Fig. ",vol1
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,2.1,Correspondence Generation,"Given a set of N aligned surface meshes X = {X 1 , X 2 , ...X N }, each mesh X i = (V i , E i ), where V i and E i represent the vertices and edge connectivity, respectively. The goal of the model is to predict a set C i of M 3D correspondence points that fully describe each surface X i and are anatomically consistent across all meshes. This goal is achieved by learning a low dimensional representation of the surface mesh z m ∈ R L using the mesh autoencoder and then z m is used to transform the template point cloud via the implicit field decoder (IM-Net)  where α, γ are the hyperparameters. We consider a combination of L 1 and L 2 two-way Chamfer distance for numerical stability as the magnitude of L 2 loss can be low over epochs and L 1 can compensate for it. The correspondence generation uses two networks:",vol1
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,,Mesh Autoencoder (M-AE):,We use EdgeConv ,vol1
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,,Implicit Field Decoder (IM-NET):,The IM-NET ,vol1
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,2.2,Analysis,The Mesh2SSM model also consists of an analysis branch that acts as a shape analysis module to capture non-linear shape variations identified by the learned correspondences {C i } N i=1 and also learns a data-informed template from the latent space of correspondences to be fed back into the correspondence generation network during training. This branch uses one network module:,vol1
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,,Shape Variation Autoencoder (SP-VAE):,"The VAE  The main difference between M-AE and a SP-VAE lies in the input and output representations they handle. SP-VAE operates directly on sets of landmarks or correspondences, aiding in the analysis of shape models. It takes a set of correspondences describing a shape as input and aims to learn a compressed latent representation of the shape. Importantly, the SP-VAE maintains the same ordering of correspondences at the input and output, so it does not use permutation-invariant layers or operations like pooling.",vol1
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,2.3,Training,"We begin with a burn-in stage, where only the correspondence generation module is trained while the analysis module is frozen. After the burn-in stage, alternate optimization of the correspondence and analysis module begins. During the alternate optimization phase, we generate the data-informed template from the latent space of SP-VAE at regular intervals. The learned data-informed template is used in the correspondence generation module in the subsequent epochs. For the learned template, we sample 500 samples from the prior p(z p ) ∼ N (0, I) and pass it through the decoder of SP-VAE to get the reconstructed correspondence point set. The mean template is defined by taking the average of these generated samples. Inference with unseen meshes is straight forward; the meshes are passed through the mesh encoder and IM-NET of the correspondence generation module to get the predicted correspondences. All hyperparameters and network architecture details are mentioned in the supplementary material.",vol1
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,3.0,Experiments and Discussion,Dataset: We use the publicly available Decath-Pancreas dataset of 273 segmentations from patients who underwent pancreatic mass resection ,vol1
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,3.1,Results,We perform experiments with two templates: sphere and medoid. We compare the performance of FlowSSM ,vol1
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,3.2,Limitations and Future Scope,"As SSM is included a part of diagnostic clinical support systems, it is crucial to address the drawbacks of the models. Like most deep learning models, performance of Mesh2SSM could be affected by small dataset size, and it can produce overconfident estimates. An augmentation scheme and a layer uncertainty calibration are could improve its usability in medical scenarios. Additionally, enforcing disentanglement in the latent space of SP-VAE can make the analysis module interpretable and allow for effective non-linear shape analysis by clinicians.",vol1
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,4.0,Conclusion,"The paper presents a new systematic approach of generating non-linear statistical shape models using deep learning directly from meshes, which overcomes the limitations of traditional SSM and current deep learning approaches. The use of an autoencoder for meaningful feature extraction of meshes to learn the PDM provides a versatile and scalable framework for SSM. Incorporating template feedback loop via VAE ",vol1
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 59.,vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,1.0,Introduction,"Medical image segmentation is a critical task in computer-aided diagnosis and treatment planning. It involves the delineation of anatomical structures or pathological regions in medical images, such as magnetic resonance imaging (MRI) or computed tomography (CT) scans. Accurate and efficient segmentation is essential for various medical applications, including tumor detection, surgical planning, and monitoring disease progression. However, manual medical imaging annotation is time-consuming and expensive because it requires the domain knowledge from medical experts. Therefore, there is a growing interest in developing semi-supervised learning that leverages both labeled and unlabeled data to improve the performance of image segmentation models  Existing semi-supervised segmentation methods exploit smoothness assumption, e.g., the data samples that are closer to each other are more likely to to have the same label. In other words, the smoothness assumption encourages the model to generate invariant outputs under small perturbations. We have seen such perturbations being be added to natural input images at data-level  In this paper, we propose a novel cross-adversarial local distribution regularization for semi-supervised medical image segmentation for smoothness assumption enhancement",vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,,3),"We also propose a sufficiently approximation for the Cross-ALD by a multiple particle-based search using semantic feature Stein Variational Gradient Decent (SVGDF), an enhancement of the vanilla SVGD ",vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,2.0,Method,"In this section, we begin by reviewing the minimax optimization problem of virtual adversarial training (VAT) ",vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,2.1,The Minimax Optimization of VAT,"Let D l and D ul be the labeled and unlabeled dataset, respectively, with P D l and P D ul being the corresponding data distribution. Denote x ∈ R d as our ddimensional input in a space X. The labeled image x l and segmentation groundtruth y are sampled from the labeled dataset D l (x l , y ∼ P D l ), and the unlabeled image sampled from D ul is x ∼ P D ul . Given an input x ∼ P D ul (i.e., the unlabeled data distribution), let us denote the ball constraint around the image x as where is a ball constraint radius with respect to a norm || • || p , and x is an adversarial example where D KL is the Kullback-Leibler divergence. The inner maximization problem is to find an adversarial example near decision boundaries, while the minimization problem enforces the local smoothness of the model. However, VAT is insufficient to explore the set of of all adversarial examples within the constraint C because it only find one adversarial example x given a natural input x. Moreover, the works ",vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,2.2,Adversarial Local Distribution,"In order to overcome the drawback of VAT, we introduce our proposed adversarial local distribution (ALD) with Dice loss function instead of D KL in  where P θ (•|x) is the conditional local distribution, and Z(x; θ) is a normalization function. The Dice is the Dice loss function as shown in Eq. 3 where C is the number of classes. p θ ( ŷc |x) and p θ ( ỹc |x ) are the predictions of input image x and adversarial image x , respectively.",vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,2.3,Cross-Adversarial Distribution Regularization,"Given two random samples x i , x j ∼ P D (i = j), we define the cross-adversarial distribution (Cross-ALD) denoted Pθ as shown in Eq. 4 where γ ∼ Beta(α, α) for α ∈ (0, ∞), inspired by  where H indicates the entropy of a given distribution. When minimizing R(θ, x i , x j ) or equivalently -H(P θ (•|x i , x j )) w.r.t. θ, we encourage P θ (•|x i , x j ) to be closer to a uniform distribution. This implies that the outputs of f ( x ) = f ( x ) = a constant c, where x , x ∼ Pθ (•|x i , x j ). In other words, we encourages the invariant model outputs under small perturbations. Therefore, minimizing the Cross-ALD regularization loss leads to an enhancement in the model smoothness. While VAT only enforces local smoothness using one adversarial example, Cross-ALD further encourages smoothness of both local and mixed adversarial distributions to improve the model generalization.",vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,2.4,Multiple Particle-Based Search to Approximate the Cross-ALD Regularization,"In Eq. 2, the normalization Z(x; θ) in denominator term is intractable to find. Therefore, we propose a multiple particle-based search method named SVGDF to sample x (1) , x (2) , . . . , x (N ) ∼ P θ (•|x)). N is the number of samples (or adversarial particles). SVGDF is used to solve the optimization problem of finding a target distribution P θ (•|x)). SVGDF is a particle-based Bayesian inference algorithm that seeks a set of points (or particles) to approximate the target distribution without explicit parametric assumptions using iterative gradient-based updates. Specifically, a set of adversarial particles (x (n) ) is initialized by adding uniform noises, then projected onto the ball C . These adversarial particles are then iteratively updated using a closed-form solution (Eq. 6) until reaching termination conditions (, number of iterations). [k(Φ(x (j),(l) ), Φ(x ))∇ x (j),(l) log P (x (j),(l) |x) where x (n),(l) is a n th adversarial particle at l th iteration (n ∈ {1, 2, ..., N }, and l ∈ {1, 2, ..., L} with the maximum number of iteration L). C is projection operator to the C constraint. τ is the step size updating. k is the radial basis function . Φ is a fixed feature extractor (e.g., encoder of U-Net/V-Net). While vanilla SVGD  SVGDF approximates P θ (•|x i ) and P θ (•|x j ) in Eq. 4, where x i , x j ∼ P D ul (i = j). We form sets of adversarial particles as }. The problem (5) can then be relaxed to where γ ∼ Beta(α, α) for α ∈ (0, ∞).",vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,2.5,Cross-ALD Regularization Loss in Medical Semi-supervised Image Segmentation,"In this paper, the overall loss function total consists of three loss terms. The first term is the dice loss, where labeled image x l and segmentation ground-truth y are sampled from labeled dataset D l . The second term is a contrastive learning loss for inter-class separation cs proposed by  where λ cs and λ Cross-ALD are the corresponding weights to balance the losses. Note that our implementation is replacing vat loss with the proposed Cross-AD regularization in SS-Net code repository",vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,3.0,Experiments,"In this section, we conduct several comprehensive experiments using the ACDC",vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,3.1,Diversity of Adversarial Particle Comparison,"Settings. We fixed all the decoder models (U-Net for ACDC and V-Net for LA). We run VAT with random initialization and SVGD multiple times to produce adversarial examples, which we compared to the adversarial particles generated using SVGDF. SVGDF is the proposed algorithm, which leverages feature transformation to capture the semantic meaning of inputs. Φ is the decoder of U-Net in ACDC dataset, while Φ is the decoder of V-Net in LA dataset. We set the same radius ball constraint, updating step, and etc. We randomly pick three images from the datasets to generate adversarial particles. To evaluate their diversity, we report the sum squared error (SSE) between these particles. Higher SSE indicates more diversity, and for each number of particles, we calculate the average of the mean of SSEs.  Results. Note that the advantage of SVGD over VAT is that the former generates diversified adversarial examples because of the second term in Eq. 6 while VAT only creates one example. Moreover, vanilla SVGD is difficult to capture semantic meaning of high-resolution medical imaging because it calculates kernel k on image-level. In Fig. ",vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,3.2,Performance Evaluation on the ACDC and la Datasets,"Settings. We use the metrics of Dice, Jaccard, 95% Hausdorff Distance (95HD), and Average Surface Distance (ASD) to evaluate the results. We compare our Cross-ALD to six recent methods including UA-MT  Results. Recall that our Cross-ALD generates diversified adversarial particles using SVGDF compared to vanilla SVGD and VAT, and further enhances smoothness of cross-adversarial local distributions. In Table ",vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,3.3,Ablation Study,"Settings. We use the same network architectures and parameter settings in Sect. 3.2, and train the models with 5% labeled training data of ACDC and LA. We illustrate that crossing adversarial particles is more beneficial than random  mixup between natural inputs (RanMixup  Result. Table ",vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,4.0,Conclusion,"In this paper, we have introduced a novel cross-adversarial local distribution (Cross-ALD) regularization that extends and overcomes drawbacks of VAT and Mixup techniques. In our method, SVGDF is proposed to approximate Cross-ALD, which produces more diverse adversarial particles than vanilla SVGD and VAT with random initialization. We adapt Cross-ALD to semi-supervised medical image segmentation to achieve start-of-the-art performance on the ACDC and LA datasets compared to many recent methods such as VAT ",vol1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 18.,vol1
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,1.0,Introduction,"Ultrasound Elastography (USE) provides information related to the stiffness of the tissue. Ultrasound (US) data before and after the tissue deformation (which can be caused by an external or internal force) are collected and compared to calculate the displacement map, indicating each individual sample's relative motion. The strain is computed by taking the derivative of the displacement fields. In free-hand palpation, the force is external and applied by the operator by the probe  Convolutional Neural Networks (CNN) have been successfully employed for USE displacement estimation  Recently, physically inspired constraint in unsupervised regularized elastography (PICTURE) has been proposed  Known operators, introduced by Maier et al.  In this paper, we aim to embed two lateral displacement refinement algorithms in the CNNs to improve the lateral strains. The first algorithm limits the range of Effective Poisson's Ratio (EPR) inside the feasible range during the test time. It is important to note that in contrast to ",vol1
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.0,Materials and Methods,"In this section, we first provide a brief overview of PICTURE and underlie some differences to this work. We then introduce our method for incorporating known operators into our deep model and outline our unsupervised training technique. We then present the training and test datasets and finish the section by demonstrating the network architecture.",vol1
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.1,PICTURE,"Let ε x denote axial (x = 1), lateral (x = 2), and out-of-plane (x = 3) strains. Assuming linear elastic, isotropic, and homogeneous material that can move freely in the lateral direction, the lateral strain can be obtained from the axial strain and the Poisson's ratio by ε 2 = -v × ε. Real tissues are inhomogeneous, and boundary conditions exist; therefore, the lateral strain cannot be directly obtained by the axial strain and the Poisson's ratio alone. In such conditions, EPR, which is defined as v e = -ε22 ε11 can be employed  where v e is the EPR obtained from the estimated displacements. v emin and v emax are two hyperparameters that specify the minimum and maximum accepted EPR values, which are assumed to be 0.1 and 0.6, respectively. 2-Penalize the out-of-range lateral strains using: where < v e > is the average of EPR values within the feasible range. The operator S denotes stop gradient operation, which is employed to avoid the axial strain being affected by this regularization. It should be noted in contrast to  3-Smoothness of EPR is considered by: 4-PICTURE loss is defined as L V = L vd + λ vs × L vs , where λ vs is the weight of the smoothness loss. PICTURE loss is added to the data and smoothness losses of unsupervised training.",vol1
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.2,Known Operators,"The known operators are added to the network in the inference mode only due to the high computational complexity of unsupervised training (outlined in the next section). We employ two known operators to impose physically known constraints on the lateral displacement. The first known operator (we refer to it as Poisson's ratio clipper) limits the EPR to the feasible range of v emin -v emax . Although PICTURE tries to move all EPR values to the feasible range, in  The second algorithm employs the incompressibility of the tissue which can be formulated by: In free-hand palpation, the force is approximately uniaxial (ε 3 ε 2 ); therefore Eq. 4 can be written as: Guo et al. enforced incompressibility in an iterative algorithm ",vol1
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.3,Unsupervised Training,We followed a similar unsupervised training approach presented in ,vol1
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.4,Dataset and Quantitative Metrics,"We use publicly available data collected from a breast phantom (Model 059, CIRS: Tissue Simulation & Phantom Technology, Norfolk, VA) using an Alpinion E-Cube R12 research US machine (Bothell, WA, USA). The center frequency was 8 MHz and the sampling frequency was 40 MHz. The Young's modulus of the experimental phantom was 20 kPa and contains several inclusions with Young's modulus of higher than 40 kPa. This data is available online at http://code.sonography.ai in  In vivo data was collected at Johns Hopkins hospital from patients with liver cancer during open-surgical RF thermal ablation by a research Antares Siemens system using a VF 10-5 linear array with the sampling frequency of 40 MHz and the center frequency of 6.67 MHz. The institutional review board approved the study with the consent of the patients. We selected 600 RF frame pairs of this dataset for the training of the networks. Two well-known metrics of Contrast to Noise Ratio (CNR) and Strain Ratio (SR) are utilized to evaluate the compared methods. Two Regions of Interest (ROI) are selected to compute these metrics and they can be defined as  where the subscript t and b denote the target and background ROIs. The SR is only sensitive to the mean (s X ), while CNR depends on both the mean and the standard deviation (σ X ) of ROIs. For stiff inclusions as the target, higher CNR correlates with better target visibility, and lower SR translates to a higher difference between the target and background strains.",vol1
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.5,Network Architecture and Training,We employed MPWC-Net++  3 Results and Discussions,vol1
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,3.1,Compared Methods,"kPICTURE is compared to the following methods: -OVERWIND, an optimization-based USE method  -The post-processing method of Guo et al. ",vol1
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,3.2,Results and Discussions,"The lateral strains of ultrasound RF data collected from three different locations of the tissue-mimicking breast phantom are depicted in Fig.  Table  sample  The lateral strain results of in vivo data are depicted in Fig.  It should be noted that after incorporating the known operators, the inference time of the network increased from an average of 195 ms to 240 ms (having 10 iterations for algorithm 1 and 100 iterations for algorithm 2).",vol1
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,4.0,Conclusions,"In this paper, we proposed to incorporate two known operators inside a USE network. The network is trained by physically inspired constraints specifically designed to tackle the long-standing illusive problem of lateral strain imaging. The proposed operators provide a refinement in each pyramid level of the architecture and substantially improve the lateral strain image quality. Tissue mimicking phantom and in vivo results show that the method substantially outperforms previous displacement estimation method in the lateral direction.",vol1
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_45.,vol1
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,1.0,Introduction,"Semantic segmentation is a prerequisite for a broad range of medical imaging applications, including disease diagnosis and treatment  Driven by the need to overcome this challenge, numerous semi-supervised learning paradigms have looked to alleviate annotation requirements in the target domain. Semi-supervised learning refers to methods that encourage learning abstract representations from an unlabeled dataset and extending the decision boundaries towards a more-generalized or target dataset distribution. These techniques can be categorized into (i) consistency regularization  More recent deep self-training approaches based on pseudo labels have emerged as promising techniques for unsupervised domain adaptation. These techniques assume that a trained network can approximate the ground-truth labels for unlabeled images. Since no metric guarantees pseudo-label reliability, several methods have been developed to alleviate pseudo-label error back-propagation. To progressively improve pseudo-labeling performance, reciprocal learning  To this end, we propose a novel self-training framework with a selfassessment strategy for pseudo-label reliability. The proposed framework uses transformation-invariant highly-confident predictions in the target dataset for self-training. This objective is achieved by considering an ensemble of highconfidence predictions from transformed versions of identical inputs. To validate the effectiveness of our proposed framework on a variety of tasks, we evaluate our approach on three different semantic segmentation imaging modalities, including video (cataract surgery), optical coherence tomography (retina), and MRI (prostate), as shown in Fig. ",vol1
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,2.1,Model,"At training time, images from the source dataset are augmented using spatial g(•) and non-spatial f (•) transformations and passed through a segmentation network, N (•), by which the network is trained using a standard supervision loss. At the same time, images from the target dataset are also passed to the network. Specifically, we feed two versions of each target image to the network: (1) the original target image x T , and (2) its non-spatially transformed version, xT = f (x T ). Once fed through the network, the corresponding predictions can be defined as ỹT = σ(N (x T )) and ỹT = σ(N ( xT )), where σ(•) is the Softmax operation. We then define a confidence-mask ensemble as where refers to Hadamard product used for element-wise multiplication, and Cnf is the high confidence masking function, where τ ∈ (0.5, 1) is the confidence threshold, and H, W , and C are the height, width, and number of classes in the output, respectively. Specifically, M cnf encodes regions of confident predictions that are invariant to transformations. We can then compute the pseudo-ground-truth mask for each input from the target dataset as",vol1
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,2.2,Training,"To train our model, we simultaneously consider both the source and target samples by minimizing the following loss, where L Sup and L P s indicate the supervised and pseudo-supervised loss functions used, respectively. We set λ as a time-dependent weighing function that gradually increases the share of pseudo-supervised loss. Intuitively, our pseudosupervised loss enforces predictions on transformation-invariant highly-confident regions for unlabeled images. Discussion: The quantity and distribution of supervised data are determining factors in neural networks' performance. With highly distributed large-scale supervisory data, neural networks converge to an optimal state efficiently. However, when only limited supervisory data with heterogeneous distribution from the inference dataset are available, using more sophisticated methods to leverage a priori knowledge is essential. Our proposed use of invariance of network predictions with respect to data augmentation is a strong form of knowledge that can be learned through dataset-dependent augmentations. The trained network is then expected to provide consistent predictions under diverse transformations. Hence, the transformation variance of the network predictions can indicate the network's prediction doubt and low confidence correspondingly. We take advantage of this characteristic to assess the reliability of predictions and filter out unreliable pseudo-labels.",vol1
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,3.0,Experimental Setup,"Datasets: We validate our approach on three cross-device/site datasets for three different modalities: -Cataract: instrument segmentation in cataract surgery videos  -MRI: multi-site prostate segmentation  We follow a four-fold validation strategy for all three cases and report the average results over all folds. The average number of labeled training images (from the source domain), unlabeled training images (from the target domain), and test images per fold are equal to (207, 3189, 58) for Cataract, (391, 569, 115) for  Baseline Methods: We compare the performance of our proposed transformation-invariant self-training (SI-ST) method against seven state-of-theart semi-supervised learning methods: Π models ",vol1
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,,Networks and Training Settings:,We evaluate our TI-ST framework using two different architectures: (1) DeepLabV3+ ,vol1
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,4.0,Results,Table ,vol1
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,5.0,Conclusion,"We proposed a novel self-training framework with a self-assessment strategy for pseudo-label reliability, namely ""Transformation-Invariant Self-Training"" (TI-ST). This method uses transformation-invariant highly-confident predictions in the target dataset by considering an ensemble of high-confidence predictions from transformed versions of identical inputs. We experimentally show the effectiveness of our approach against numerous existing methods across three different source-to-target segmentation tasks, and when using different model architectures. Beyond this, we show that our approach is resilient to changes in the methods hyperparameter, making it well-suited for different applications.",vol1
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_32.,vol1
SLPD: Slide-Level Prototypical Distillation for WSIs,1.0,Introduction,"In computational histopathology, visual representation extraction is a fundamental problem  More recently, some works propose to close the gap via directly learning slidelevel representations in pre-training. For instance, HIPT  In this paper, we propose to encode the intra-and inter-slide semantic structures by modeling the mutual-region/slide relations, which is called SLPD: Slide-Level Prototypical Distillation for WSIs. Specifically, we perform the slide-level clustering for the 4096 × 4096 regions within each WSI to yield the prototypes, which characterize the medically representative patterns of the tumor (e.g., morphological phenotypes). In order to learn this intra-slide semantic structure, we encourage the region representations to be closer to the assigned prototypes. By representing each slide with its prototypes, we further select semantically simi- lar slides by the set-to-set distance of prototypes. Then, we learn the inter-slide semantic structure by building correspondences between region representations and cross-slide prototypes. We conduct experiments on two benchmarks, NSCLC subtyping and BRCA subtyping. SLPD achieves state-of-the-art results on multiple slide-level tasks, demonstrating that representation learning of semantic structures of slides can make a suitable proxy task for WSI analysis. We also perform extensive ablation studies to verify the effectiveness of crucial model components.",vol1
SLPD: Slide-Level Prototypical Distillation for WSIs,2.1,Overview,"As shown in Fig.  , where L n denotes the number of regions of WSI w n , we aim to learn a powerful encoder that maps each x l n to an embedding z l n ∈ R D . SLPD is built upon the two-stage pre-training paradigm proposed by HIPT, which will be described in Sect. 2.2. Fig ",vol1
SLPD: Slide-Level Prototypical Distillation for WSIs,2.2,Preliminaries,"We revisit Hierarchical Image Pyramid Transformer (HIPT)  where H(a, b) = -a log b, and p d is the data distribution that all regions are drawn from. The teacher and the student share the same architecture consisting of an encoder (e.g., ViT) and a projection head g t /g s . ẑ and z are the embeddings of two views at region-level yielded by the encoder. The parameters of the student are exponentially moving averaged to the parameters of the teacher.",vol1
SLPD: Slide-Level Prototypical Distillation for WSIs,2.3,Slide-Level Clustering,"Many histopathologic features have been established based on the morphologic phenotypes of the tumor, such as tumor invasion, anaplasia, necrosis and mitoses, which are then used for cancer diagnosis, prognosis and the estimation of response-to-treatment in patients  To characterize the histopathologic features underlying the slides, a straightforward practice is the global clustering, i.e., clustering the region embeddings from all the WSIs, as shown in the left of Fig. ",vol1
SLPD: Slide-Level Prototypical Distillation for WSIs,2.4,Intra-Slide Distillation,"The self-distillation utilized by HIPT in stage two encourages the correspondence between two views of a region at the macro-scale because the organizations of cells share mutual information spatially. However, the self-distillation, which solely mines the spatial correspondences inside the 4096 × 4096 region, cannot comprehensively understand the histopathologic consistency at the slide-level. In order to achieve better representations, the histopathologic connections between the WSI and its regions should be modeled and learned, which is called intraslide correspondences. With the proposed slide-level clustering in Sect. 2.3, a slide can be abstracted by a group of prototypes, which capture the semantic structure of the WSI. As shown in Fig.  We omit super-/sub-scripts of z for brevity. Through Eq. 2, we can leverage more intra-slide correspondences to guide the learning process. For further understanding, a prototype can be viewed as an augmented representation aggregating the slide-level information. Thus this distillation objective is encoding such information into the corresponding region embedding, which makes the learning process semantic structure-aware at the slide-level.",vol1
SLPD: Slide-Level Prototypical Distillation for WSIs,2.5,Inter-Slide Distillation,"Tumors of different patients can exhibit morphological similarities in some respects  where cos(•, •) measures the cosine similarity between two vectors, and S M enumerates the permutations of M elements. The optimal permutation σ * can be computed efficiently with the Hungarian algorithm  The inter-slide distillation can encode the sldie-level information complementary to that of intra-slide distillation into the region embeddings. The overall learning objective of the proposed SLPD is defined as: where the loss scale is simply set to α 1 = α 2 = 1. We believe the performance can be further improved by tuning this.",vol1
SLPD: Slide-Level Prototypical Distillation for WSIs,3.0,Experimental Results,"Datasets. We conduct experiments on two public WSI datasets for downstream tasks. With the pre-extracted embeddings, we fine-tune three aggregators (i.e., MIL  Evaluation Metrics. We adopt the 10-fold cross validated Accuracy (Acc.) and area under the curve (AUC) to evaluate the weakly-supervised classification performance. The data splitting scheme is kept consistent with HIPT. ",vol1
SLPD: Slide-Level Prototypical Distillation for WSIs,3.1,Weakly-Supervised Classification,"We conduct experiments on two slide-level classification tasks, NSCLC subtyping and BRCA subtyping, and report the results in Table  Compared with the strong baseline, i.e., the two-stage pre-training method proposed by HIPT (#6), SLPD achieves performance increases of 1.3% and 3.2% AUC on NSCLC and BRCA (#9). Nontrivial performance improvements are also observed under KNN evaluation (#10 vs.#13): +2.3% and +3.1% AUC on NSCLC and BRCA. The superior performance of SLPD demonstrates that learning representations with slide-level semantic structure appropriately can significantly narrow the gap between pre-training and downstream slide-level tasks. Moreover, intra-slide and inter-slide distillation show consistent performance over the baseline, corroborating the effectiveness of these critical components of SLPD.",vol1
SLPD: Slide-Level Prototypical Distillation for WSIs,3.2,Ablation Study,"Different Clustering Methods. As discussed in Sect. 2.3, we can alternatively use the global clustering to obtain prototypes and then optimize the network with a similar distillation objective as Eq. 2. For a fair comparison, the total number of prototypes of the two clustering methods is approximately the same.  Different Inter-slide Distillations. The proposed inter-slide distillation is semantic structure-aware at the slide-level, since we build the correspondence between the region embedding and the matched prototype (#4 in Table ",vol1
SLPD: Slide-Level Prototypical Distillation for WSIs,,Number of Prototypes.,As shown in Table  Number of Slide Neighbors. As demonstrated in Table ,vol1
SLPD: Slide-Level Prototypical Distillation for WSIs,4.0,Conclusion,"This paper reflects on slide-level representation learning from a novel perspective by considering the intra-and inter-slide semantic structures. This leads to the proposed Slide-Level Prototypical Distillation (SLPD), a new self-supervised learning approach achieving the more comprehensive understanding of WSIs. SLPD leverages the slide-level clustering to characterize semantic structures of slides. By representing slides as prototypes, the mutual-region/slide relations are further established and learned with the proposed intra-and inter-slide distillation. Extensive experiments have been conducted on multiple WSI benchmarks and SLPD achieves state-of-the-art results. Though SLPD is distillation-based, we plan to apply our idea to other pre-training methods in the future, e.g., contrastive learning ",vol1
SLPD: Slide-Level Prototypical Distillation for WSIs,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_25.,vol1
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,1.0,Introduction,"Laparoscopic surgery is a minimally invasive surgical technique in which a camera and surgical instruments are inserted through a series of small skin punctures. During this procedure, the surgeon relies heavily on a screen to visualize the surgical site, which can be a serious challenge. An inaccurate interpretation of abdominal anatomy can result in serious injury to the patient's bile ducts  The aim of UDA is to train a model on a labeled source and an unlabeled target domain for enhanced target domain performance. Various UDA methods exist, but we concentrate on two types that are relevant to our approach: self-training-based and Fourier transform-based. Self-training-based approaches  We propose a novel module for the UDA task in laparoscopic semantic segmentation, aiming to promote the network's exploration of consistency regularization between high-frequency and low-frequency images. Our approach is motivated by the observation in Fig. ",vol1
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,2.0,Method,"This paper addresses the task of domain-adaptive semantic segmentation of laparoscopic images. Suppose we have a source domain of computer-generated simulated laparoscopic images, consisting of N images N} , and a target domain of M real laparoscopic images X T = {X T j | j = 1, 2, . . . , M} without annotations. Our objective is to train a network f with robust semantic segmentation capability on the unlabeled target domain. To achieve this, we introduce a masked frequency consistency module for selflearning on the unlabeled target domain images X T i , while supervised loss is used for training on the labeled source domain images X S i . Our approach can integrate with different networks, effectively bridging the domain gap that occurs when applying networks to the target domain.",vol1
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,,Fig. 2.,"The overview of our proposed Masked Frequency Consistency (MFC) module, which can be seamlessly integrated with different UDA methods and backbone networks. The MFC module works by augmenting the input image in the frequency domain using a mask, and then using a teacher-student structure to take both the original and the augmented image as inputs. A consistency loss is applied to facilitate the bridging of domains.",vol1
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,2.1,Image Frequency Representation,"Considering an RGB image, X ∈ R H×W ×3 , we can generate its frequency representation map by applying the 2D Discrete Fourier Transform F for each channel c ∈ {0, 1, 2}, independently: where (u, v) and (h, w) donate the coordinates in frequency map and image. To facilitate subsequent operations, we rearrange the FFT data so that negative frequency terms precede positive ones, thereby centering the low frequency information. Furthermore, the inverse Fourier transform (iFFT) F -1 is utilized to transform the spectral signals back into the original image space: Computation of both the Fourier transform and its corresponding inverse is achieved through the Fast Fourier Transform (FFT) algorithm ",vol1
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,2.2,Masking Strategy,"As illustrated in Fig.  H×W that randomly erases parts of the frequency map, thereby reducing the frequency data. Specifically, a patch mask M is randomly sampled as follows: where [•] is the Iverson bracket, U(0, 1) the uniform distribution, b is the patch size, r represents the mask ratio, m and n are the patch indices. After this procedure, the patches in the mask are randomly masked. However, using the random patch mask alone may result in the loss of all low frequency information, which would exacerbate the domain gap and lead to training instability. To avoid this, we set the central elements to 1, thus preserving the low frequency information from the images as: where h and w denote the size of the low-frequency information to be preserved. We utilize the mask M to apply masking in the frequency domain and use the iFFT F -1 to transform the image back into the original spatial domain as the network input. The enhanced image can then be obtained as: where is the Hadamard product between the matrices. Moreover, with conjugate symmetry's disruption inhibiting the imaginary component's cancellation, we employ complex number magnitudes as outputs.",vol1
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,2.3,Consistency Regularization,"Consistency regularization is employed to extract common representations between high and low frequency images, thereby enhancing the generality of the network. Specifically, during training, the student segmentation network f S takes the enhanced image X m as input, whereas the original image X serves as the input for the teacher network f T . The weight θ T of the teacher network undergoes updates using the exponential moving average (EMA)  where θ T representing the weight from the previous training step. The EMA teacher generates a series of stable pseudo-labels over time, a tactic frequently utilized in both semi-supervised learning  To evaluate the prediction results, we employ the mean squared error (MSE) as as our loss function, which quantifies the divergence between the predictions:  3 Experiments",vol1
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,3.1,Datasets and Implementation,"Datasets. Our experiments were conducted on three laparoscopic datasets, described as follows: (1) Simulation dataset  Dataset Partitioning. Our experiments were performed with two different settings: (1) simulated images to real images and (2) translated images to real images. Considering the 6 categories present in the simulated dataset and the 13 categories in the CholecSeg8k dataset, we performed semantic segmentation on the following 6 categories: Background (BG), Abdominal Wall (AW), Liver, Fat, Gallbladder (GB), and Instruments (INST). Implementation. We used the mmsegmentation  State-of-the-Art Methods. We benchmarked our method against contemporary leading approaches, which include UDA methods (DAFormer ",vol1
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,3.2,Qualitative Evaluation,"In Fig.  To verify the effectiveness of the patch mask outlined in Eq. (  Translated Images → Real Images. Furthermore, we assessed the performance of various methods using translated images as the source domain, with results summarized in Table ",vol1
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,4.0,Conclusion,"This paper tackles the crucial issue of laparoscopic image segmentation, which is essential for surgical guidance and navigation. We propose a novel UDA module, called MFC, that leverages the consistency between high and low-frequency information in latent space. This consistency facilitates knowledge transfer from computer-simulated to real laparoscopic datasets for segmentation. Experimentally, MFC not only bolsters existing UDA models' performance but also outperforms leading methods, including fully supervised models that rely on annotated data. Our work unveils the potential of using computer-generated image data and UDA techniques for laparoscopic image segmentation. However, a limitation of our approach is that it does not account for the long-tail category distribution prevalent in real-world scenarios, such as venous vessels. Therefore, a future direction of our research is to extend our MFC module to handle rare category segmentation, thereby improving UDA models' generalization capabilities.",vol1
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 63.,vol1
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,1.0,Introduction,"Statistical Shape Modeling (SSM) enables population-based morphological analysis, which can reveal patterns and correlations between shape variations and clinical outcomes. SSM can help researchers understand the differences between healthy and pathological anatomy, assess the effectiveness of treatments, and identify biomarkers for diseases (e.g.,  Effective SSM from point clouds would be widely applicable in clinical research, from artery disease progression from point clouds acquired via biplane angiographic and intravascular ultrasound image data fusion ",vol1
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,2.0,Background,"Point Distribution Models. The goal of SSM is to capture the inherent characteristics or underlying parameters of a shape class that remain when global geometrical information is removed. Given a PDM, correspondence points can be averaged across subjects to provide a mean shape, and principal component analysis (PCA) can be performed to compute the modes of shape variation, which can then be visualized and used in downstream medical tasks. Furthermore, if a PDM contains sub-populations, such as disease versus control, the differences in mean shapes can be quantified and visualized, providing group characterization. Point Cloud Deep Learning. Deep learning from 3D point clouds is an emerging area of research with numerous applications in computer vision, robotics, and medicine (e.g., classification, object tracking, segmentation, registration, pose estimation)  The initial point cloud completion network, PCN ",vol1
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,3.1,Point Completion Networks for SSM,"Our experiments demonstrate that when coarse-to-fine point completion networks are trained on anatomical shapes, the bottleneck captures a populationspecific shape prior. Directly decoding the shape feature representation results in a consistent ordering of the intermediate coarse point cloud across samples, providing a PDM. This phenomenon can be intuitively understood as an application of Occam's razor, where the model prefers to learn the simplest solution, resulting in consistent output ordering. Many point completion networks contain skip connections from the feature and/or input space to the refinement network. In the case where the unordered input point cloud is fed to the refinement network, the ordering of the output dense point cloud is understandably lost.  To study whether point completion networks can learn anatomical SSM, we first extract point clouds from mesh vertices, then train point completion models, and finally evaluate the effectiveness of the predicted coarse point clouds as PDMs. Note this approach is not restricted to input point clouds obtained from meshes; point clouds from any acquisition process can be used. Global geometric information is factored out by aligning all shapes via iterative closest points  -PCN ",vol1
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,3.2,Evaluation Metrics,"In addition to CD, Fscore  We also consider PDM correspondence analysis metrics. An ideal PDM is compact, meaning that it represents the distribution of the training data using the minimum number of parameters. We quantify compactness as the number of PCA modes required to capture 99% of the variation in the correspondences. A good PDM should also generalize well from training examples to unseen examples. The generalization metric is defined as the reconstruction error (L2) between predicted correspondences of a held-out point cloud and the correspondences reconstructed via the training PDM. Finally, effective SSM is specific, generating only valid instances of the shape class in the training set. The average distance between correspondences sampled from the training PDM and the closest existing training correspondences provides the specificity metric.",vol1
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,4.0,Experiments,"We use five datasets in experiments -one synthetic ellipsoid dataset for a proofof-concept and four real anatomical shapes: proximal femurs, left atrium of the heart, spleen, and pancreas. These datasets vary greatly in size (see Table  Proof-of-Concept: Ellipsoids. As a proof-of-concept, we generate 3D axisaligned ellipsoid shapes with fixed z-radius and random x and y-radius. A testing set of 30 and a training set of just 50 were randomly defined to emulate the scarce data scenario. The results in Table  Femur. The femur dataset is comprised of 56 femoral heads segmented from CT scans, nine of which have the cam-FAI pathology characterized by an abnormal bone growth lesion that causes hip osteoarthritis ",vol1
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,,Region of CAM lesion,"Difference from normal to CAM group mean captured by PDM 0 0.5  Left Atrium. The left atrium dataset comprises of 1096 shapes segmented from cardiac LGE MRI images from unique atrial fibrillation patients. This cohort contains significant morphological variation in overall size, the size of the left atrium appendage, and the number and arrangement of the pulmonary veins. This variation is reflected in the large compactness values in Table  Pancreas. We utilize the pancreas dataset  Spleen. The spleen dataset ",vol1
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_47.,vol1
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,1.0,Introduction,"Nuclei segmentation in histopathology images is an important task for cancer diagnosis and immune response prediction  thousands of instances are tedious and the ambiguous nature of nuclei boundaries requires high-level expert annotators. To address this, weakly-supervised nuclei segmentation methods  To overcome these challenges, we propose a novel weakly supervised instance segmentation method that effectively distinguishes adjacent nuclei and is robust to point shifts. The proposed model consists of three modules responsible for binary segmentation, boundary delineation, and instance separation. To train the binary segmentation module, we generate pseudo binary segmentation masks using geodesic distance-based Voronoi labels and cluster labels from point annotations. Geodesic distance provides more precise nuclei shape information than previous Euclidean distance-based schemes. To train the offset map module, we generate pseudo offset maps by computing the offset distance between binary segmentation pixel predictions and the point label. The offset information facilitates precise delineation of the boundaries between adjacent nuclei. To make the model robust to center point shifts, we introduce an Expectation Maximization (EM)  The contributions of this paper are as follows:  (2) By utilizing geodesic distance, we produce more detailed Voronoi and cluster labels that precisely delineate the boundary between adjacent nuclei. (3) We introduce an EM algorithm-based refinement process to encourage model robustness on center-shifted point labels. (4) Ablation and evaluation studies on two public datasets demonstrate our model's ability to outperform state-of-the-art techniques not only with ideal labels but also with shifted labels.",vol1
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,2.0,Methodology,"We propose an end-to-end nuclei segmentation method that only uses point annotations P to predict nuclei instance segmentation masks Ŝ. The proposed Fig.  In the training stage, we first generate a Voronoi label V and a cluster label K along the green lines in Fig.  In the inference stage, B, Ô, Ĉ are predicted following the orange lines in Fig.  where (x, y) represents a coordinate and (x Ĉi , y Ĉi ) means the location of i th point obtained from Ĉ. Finally, the instance segmentation output Ŝ is obtained by B × I. 2.1 Loss Functions Using Pseudo Labels Segmentation Loss. We generate V and K to train the binary segmentation module. In  We cluster f into three clusters (0 for background, 1 for foreground, and 2 for ignore) to generate K (Fig.  where Ω V and Ω K are the set of foreground and background pixels in V and K, N ΩV and N ΩK denote the cardinality of Ω V and Ω K . Following  Center Map Loss. To achieve instance-level predictions, we introduce a center map module. The module predicts a keypoint heatmap Ĉ ∈ [0, 1] W ×H where Ĉ = 1 identifies nuclei centers and Ĉ = 0 for other pixels. W and H are the width and height of the input image. To train the module we employ a focal loss, commonly used in point detection problems. This loss can focus on a set of sparse hard examples while preventing easy negatives from dominating the model  where N P denotes the number of point labels. We set the focal loss hyperparameters α = 2 and β = 4 following  It is worth noting that in the early stages of training, the pseudo offset map O generated by B and P is unreliable. Thus, we empirically use L O for backpropagation after 20 epochs. We optimize the entire model using the loss , where λ B , λ O and λ C denote loss weights.",vol1
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,2.2,Refinement via Expectation Maximization Algorithm,"Training with nuclei (center) shifted point labels can lead to blurry center map predictions (see Fig.  In the E-step, we update the center of each nucleus according to Ô. We use Ô to generate refined point labels P , since Ô is reliable regardless of the point location i.e., center of the nuclei or shifted. where v i is i th Voronoi region and p i is the refined center point. We repeat this for all Voronoi regions to obtain P , and replace P with P if the distance between them is < δ. In the M -step of iteration n, we generate C by adapting the Gaussian mask to P , and then use it to train offset and center map modules. As maximizing a probability distribution is the same as minimizing the loss, the model parameter θ minimizing L is optimized as: Since reliable Ô is necessary to refine nuclei centers, refinement starts after 30 epochs. E and M steps are alternately repeated to correct imprecise annotations bringing them closer to the real nuclei center points.",vol1
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,3.0,Experiments,"Dataset. To validate the effectiveness of our model, we use two public nuclei segmentation datasets i.e., CPM17  To make point labels, we use the center point of full mask annotations. For a realistic scenario, we generate shifted point label. The shift is performed in pixels and is randomly selected between the minimum and maximum values. Implementation Details. For training, all evaluated models were run for 150 epochs with the Adam optimizer ",vol1
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,4.0,Conclusion,"In this work, we proposed a novel and robust framework for weakly supervised nuclei segmentation. We demonstrated the effectiveness of geodesic distancebased Voronoi diagrams and k-means clustering to generate accurate pseudo binary segmentation labels. This allowed us to generate reliable pseudo offset maps, and then we iteratively improve the pseudo offset maps that facilitate the precise separation of adjacent nuclei as well as progressively refine the location of the center point labels. According to our experimental results, we established a new state-of-art on two publicly available datasets across different levels of point annotation imperfections. We believe being able to use low-precision point annotations while retaining good segmentation performance is an essential step for automatic nuclei segmentation models to become a widespread tool in realworld clinical practice.",vol1
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_51.,vol1
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,1.0,Introduction,"In recent years, the workload of radiologists has grown drastically, quadrupling from 2006 to 2020 in Western Europe  There has been rising interest in using end-to-end self-supervised methods for anomaly detection. Their success is most evident at the MICCAI Medical Outof-Distribution Analysis (MOOD) Challenge ",vol1
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,,Contribution:,"We propose a cross-validation framework, using separate selfsupervision tasks to minimise overfitting on the synthetic anomalies that are used for training. To make this work effectively we introduce a number of non-trivial and seamlessly-integrated synthetic tasks, each with a distinct feature set so that during validation they can be used to approximate generalisation to unseen, real-world anomalies. To the best of our knowledge, this is the first work to train models to directly identify anomalies on tasks that are deformation-based, tasks that use Poisson blending with patches extracted from external datasets, and tasks that perform efficient Poisson image blending in 3D volumes, which is in itself a new contribution of our work. We also introduce a synthetic anomaly labelling function which takes into account the natural noise and variation in medical images. Together our method achieves an average precision score of 76.2 for localising glioma and 78.4 for identifying pathological chest X-rays, thus setting the state-of-the-art in self-supervised anomaly detection.",vol1
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,,Related Work:,"The most prevalent methods for self-supervised anomaly detection are based on generative auto-encoders that analyse the residual error from reconstructing a test sample. This is built on the assumption that a reconstruction model will only be able to correctly reproduce data that is similar to the instances it has been trained on, e.g. only healthy samples. Theoretically, at test time, the residual reconstruction error should be low for healthy tissues but high for anomalous features. This is an active area of research with several recent improvements upon the initial idea  However, the general assumption that reconstruction error is a good basis for an anomaly scoring function has recently been challenged. Auto-encoders are unable to identify anomalies with extreme textures  Self-supervised methods take a more direct approach, training a model to directly predict an anomaly score using synthetic anomalies. Foreign patch interpolation (FPI) ",vol1
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,2.0,Method,"The core idea of our method is to use synthetic tasks for both training and validation. This allows us to monitor performance and prevent overfitting, all without the need for real anomalous data. Each self-supervised task involves introducing a synthetic anomaly into otherwise normal data whilst also producing the corresponding label. Since the relevant pathologies are unknown a priori, we avoid simulating any specific pathological features. Instead, we use a wide range of subtle and well-integrated anomalies to help the model detect many different kinds of deviations, ideally including real unforeseen anomalies. In our experiments, we use five tasks, but more could be used as long as each one is sufficiently unique. Distinct tasks are vital because we want to use these validation tasks to estimate the model's generalisation to unseen classes of anomalies. If the training and validation tasks are too similar, the performance on the validation set may be an overly optimistic estimate of how the model would perform on unseen real-world anomalies. When performing cross-validation over all synthetic tasks and data partitions independently, the number of possible train/validation splits increases significantly, requiring us to train F • (T N CT ) independent models, where T N is the total number of tasks, T is the number of tasks used to train each model and F is the number of data folds, which is computationally expensive. Instead, as in our case T N = F = 5, we opt to associate each task with a single fold of the training data (Fig.  The Intra-dataset Blending Task. Poisson image blending is the current state-of-the-art for synthetic anomaly tasks  Poisson image editing  . This follows as a DST is equivalent to a discrete Fourier transform of a real sequence that is odd around the zeroth and middle points, scaled by 0.5, which can be established for our images. With this, the Poisson equation becomes congruent to a relationship of the coefficients, vd where v=(v 0 , ..., v D-1 ) and v is the DST of each component. The solution for fu can then be computed in DST space by dividing the right side through the terms on the left side and the destination image can be obtained through x i = DST -1 ( fu ). Because this approach uses a frequency transform-based solution, it may slightly alter areas outside of M h (where image gradients are explicitly edited) in order to ensure the changes are seamlessly integrated. We refer to this blending process as x = P oissonBlend(x i , x j , M h ) in the following. The intra-dataset blending task therefore results from xintra = P oissonBlend(x, x , M h ) with x, x ∈ D with samples from a common dataset D and is therefore similar to the self-supervision task used in  The inter-dataset blending task follows the same process as intra-dataset blending but uses patches extracted from an external dataset D , allowing for a greater variety of structures. Therefore, samples from this task can be defined as xinter = P oissonBlend(x, x , M h ) with x ∈ D, x ∈ D . The sink/source tasks shift all points in relation to a randomly selected deformation centre c. For a given point p, we resample intensities from a new location p. To create a smooth displacement centred on c, we consider the distance p-c 2 in relation to the radius of the mask (along this direction), d. The extent of this displacement is controlled by the exponential factor f > 1. For example, the sink task (Eqn. 1) with a factor of f = 2 would take the intensity at 0.75d and place it at 0.5d, effectively pulling these intensities closer to the centre. Note that unlike the sink equation in  The smooth intensity change task aims to either add or subtract an intensity over the entire anomaly mask. To avoid sharp discontinuities at the boundaries, this intensity change is gradually dampened for pixels within a certain margin of the boundary. This smoothing starts at a random distance from the boundary, d s , and the change is modulated by d p /d s . Anomaly Labelling: In order to train and validate with multiple tasks simultaneously we use the same anomaly labelling function across all of our tasks. The scaled logistic function, used in NSA ",vol1
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,3.0,Experiments and Results,"Data: We evaluate our method on T2-weighted brain MR and chest X-ray datasets to provide direct comparisons to state-of-the-art methods over a wide range of real anomalies. For brain MRI we train on the Human Connectome Project (HCP) dataset  For chest X-rays we use the VinDr-CXR dataset  For both VinDr-CXR test sets we evaluate at a sample and pixel level, although previous publications have only reported their results at a sample level. We again show performance above the current state-of-the-art (Table ",vol1
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,,Ablation and Sensitivity Analysis on Cross-Validation Structure:,We also investigate how performance changes as we vary the number of tasks used for training and validation (Table ,vol1
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,,Discussion:,"We demonstrate the effectiveness of our method in multiple settings and across different modalities. A unique aspect of the brain data is the domain shift. The HCP training data was acquired at a much higher isotropic resolution than the BraTS and ISLES test data, which are both anisotropic. Here we achieve the best performance using more tasks for validation, which successfully reduces overfitting and hypersensitivity. Incorporating greater data augmentations, such as simulating anisotropic spacing, could further improve results by training the model to ignore these transformations. We also achieve strong results for the X-ray data, although precise localisation remains a challenging task. The gap between current performance and clinicially useful localisation should therefore be high priority for future research.",vol1
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,4.0,Conclusion,"In this work we use multiple synthetic tasks to both train and validate selfsupervised anomaly detection models. This enables more robust training without the need for real anomalous training or validation data. To achieve this we propose multiple diverse tasks, exposing models to a wide range of anomalous features. These include patch blending, image deformations and intensity modulations. As part of this, we extend Poisson image editing to images of arbitrary dimensions, enabling the current state-of-the-art tasks to be applied beyond just 2D images. In order to use all of these tasks in a common framework we also design a unified labelling function, with improved continuity for small intensity changes. We evaluate our method on both brain MRI and chest X-rays and achieve state-of-the-art performance and above. We also report pixel-wise results, even for the challenging case of chest X-rays. We hope this encourages others to do the same, as accurate localisation is essential for anomaly detection to have a future in clinical workflows.",vol1
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,1.0,Introduction,"Magnetic resonance imaging (MRI) is widely used for diagnosis and treatment monitoring as it provides structural and physiological information related to dis-ease progression. Diffusion MRI (dMRI) measures molecular diffusion in biological tissues and provides microscopic details of tissue architecture, as molecules interact with many different obstacles while diffusing throughout tissues  Multi-shot (ms-) acquisition is an effective approach to mitigate EPI-related artifacts, which segments k-space into multiple portions covered across multiple repetition times (TRs) to reduce the effective echo spacing. However, potential shot-to-shot phase variations across multiple EPI shots can introduce additional artifacts. Recent algorithms, such as low-rank prior methods like low-rank modeling of local k-space neighborhoods (LORAKS)  In recent years, deep learning has emerged as a promising approach for image reconstruction, offering potential solutions to the challenges of existing techniques, including long reconstruction times, residual artifacts at high acceleration factors, and over-smoothing  In contrast, self-supervised learning  The virtual coil (VC) approach is a highly effective technique for enhancing the performance of parallel MRI  In this study, we propose a novel msEPI reconstruction method called zero-MIRID (zero-shot self-supervised learning of Multi-shot Image Reconstruction for Improved Diffusion MRI). Our method jointly reconstructs msEPI data by incorporating zero-shot self-supervised learning-based image reconstruction. Our key contributions are as follows: -We jointly reconstruct multiple-shot images using self-supervised learning. -We train one network for all diffusion directions, which accelerates training speed and improves performance. -We used network denoisers in both k-and image-space and employed the VC  Overall, our zero-MIRID method offers a promising approach to enhance msEPI reconstruction in dMRI, providing improved image quality and diffusion metrics through the integration of self-supervised learning techniques.",vol1
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,2.1,PI Techniques for dMRI,"For msEPI data, SENSE is commonly used for image reconstruction. SENSE individually reconstructs each shot's data using the spatial variation of the coil sensitivity profile. The m th shot image in the d th diffusion direction, x d,m , can be reconstructed as follow. where F m is the undersampled Fourier transform for the m th shot, C is the coil sensitivity map, and b d,m is the acquired k-space data of d th direction and m th shot. On the other hand, MUSSELS and LORAKS jointly reconstruct multipleshot images using the low-rank property among msEPI data. The images in the d th diffusion direction can be reconstructed using LORAKS as follows. where J is the LORAKS regularization. In this work, we utilized S-LORAKS, which employs phase information and k-space symmetry ",vol1
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,2.2,Network Design,"where V C is the VC operator, and N i and N k are denoising CNNs in the imageand k-space, respectively. We define Nx = x -Dx, where D is the CNN network, and modified the alternating minimization-based solution in  where n is the optimization step (iteration) number, η and ζ is the network denoising terms in k-and image-space, and A = FC. As proposed in the recent ZS-SSL study ",vol1
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,2.3,Zero-Shot Self-supervised Learning,"where L is the loss function, f is the zero-MIRID reconstruction, and θ is the trainable network parameters. Similarly, the loss in the d th direction in the validating phase can be described as follows. In this study, we used the normalized root mean square error (NRMSE) and normalized mean absolute error (NMAE) as the loss functions.",vol1
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,2.4,Experiment Details,"In-vivo experiments were conducted on a 3T Siemens Prisma system with a 32-channel head coil. For dMRI, we acquired the diffusion-weighted data in 32 different directions using 2-shot EPI, with each shot accelerated by 5-fold (R=5) and employing 75% Partial Fourier, resulting in 15% coverage of the k-space in each shot relative to a fully-sampled readout. Imaging parameters are; field of view (FOV)=224 × 224 × 128 mm 3 , voxel size =1 × 1 × 4 mm 3 , TR=3.5 s, and effective echo time (TE) =59 ms. SENSE and S-LORAKS reconstructions were performed with MATLAB R2022a using Intel Xeon 6248R and 512 GB RAM. All neural network implementations were conducted with Python, using the Keras library in Tensorflow 2.4.1. NVIDIA Quadro RTX 8000 (RAM: 48 GB) was used to train, validate, and test the network. The denoising CNNs consist of 16 layers of which the depth is 46. For the 16 layer-CNN, we employed a filter size of 3 × 3. The depth of our network is 46, resulting in a total of 583,114 trainable parameters. The DC layer takes ten conjugate gradient steps, and the reconstruction block iterates ten times, where the MoDL paper  Example data and code can be found in the following link: https://github.com/jaejin-cho/miccai2023",vol1
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,3.0,Results,Figure  Figure ,vol1
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,4.0,Discussion and Conclusion,"In this study, we proposed an improved image reconstruction method for msEPI and dMRI in a self-supervised deep learning manner. In-vivo experiment demonstrates the proposed method outperformed S-LORAKS, the state-of-art PI method for dMRI. Acquiring reference images of msEPI can be challenging because each shot is typically highly accelerated and shot-to-shot phase variation prevents jointly reconstructing multiple shots efficiently. Advanced PI techniques that jointly reconstruct many EPI shots can improve the PI condition and provide highfidelity images, but using a PI method may induce bias to that particular method. Therefore, supervised learning might not be an ideal solution for msEPI. On the other hand, self-supervised learning, which does not require reference images, could be a more suitable approach for msEPI. Due to the difficulty in obtaining reliable ground truth data, conventional quantitative metrics such as SSIM and NRMSE may be less reliable for evaluation. In dMRI, FA maps and 2 nd crossing fibers could be used for obtaining more suitable metrics. We trained a single network for all diffusion directions, which improved performance and reduced training time (please see the supplementary material). NRMSE and NMAE were reduced from 14.69% to 13.61% and from 15.73% to 14.41%, respectively. The training time for the proposed network was 22:30 min per diffusion direction/slice (on GPU). This is expected to be reduced by transfer learning. Inference took approximately 1 s per direction/slice, and 2-shot LORAKS took approximately 20 s per direction/slice (on CPU). Since the images are highly similar across diffusion directions, training on the entire diffusion direction has a similar effect to increasing the size of the training database, thereby enhancing network training. Moreover, using a single network for all directions reduces training time compared to training separate networks for each direction, from 40:01 min to 22:30 min per diffusion direction and slice. As a future work, the simultaneous multi-slice (SMS) technique ",vol1
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 44.,vol1
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,1.0,Introduction,"Statistical Shape Modeling (SSM) or morphological analysis, is a widespread tool used to quantify anatomical shape variation given a population of segmented 3D anatomies. Quantifying such subtle shape differences has been crucial in providing individualized treatments in medical procedures, detecting morphological pathologies, and advancing the understanding of different diseases  The two principal shape representations for building SSMs and performing subsequent statistical analyses are deformation fields and landmarks. Deformation fields encode implicit transformations between cohort samples and a predefined (or learned) atlas. In contrast, landmarks are explicit points spread on shape surfaces that correspond across the population  To address the shortcomings of existing models, we propose Image2SSM, a novel deep-learning-based approach for SSM directly from images that, given pairs of images and segmentations, can produce a statistical shape model using an implicit, continuous surface representation. Once trained, Image2SSM can produce PDMs of new images without the need for anatomy segmentations. Unlike existing deep learning-based methods for SSM from images, Image2SSM only requires image-segmentation pairs and alleviates the need for constructing PDM to supervise learning shape statistics from images. Image2SSM leverages an implicit, radial basis function (RBF)-based, representation of shapes to construct a self-supervised training signal by tasking the network to estimate a sparse set of control points and their respective suface normals that best approximate the underlying surface in the RBF sense. This novel application of RBFs to build SSMs allows statistical analyses on representative points/landmarks, their surface normals, and the shape surfaces themselves due to its compact, informative, yet comprehensive nature. Combined with deep networks to directly learn such a representation from images, this method ushers a next step towards fully end-to-end SSM frameworks that can build better and less restrictive low-dimensional shape representations more conducive to SSM analysis. In summary, the proposed method for SSM has the following strengths. -Using a continuous, but compact surface representation instead of only landmarks that allows performing analyses on points, normals, and surfaces alike. -The RBF shape representation can adapt to the underlying surface geometry, spreading more landmarks over the more complex surface regions. -A deep learning approach that bypasses any conventional correspondence optimization to construct training data for supervision, requiring virtually no hyperparameter tuning or preprocessing steps. -This method uses accelerated computational resources to perform training and outperforms existing deep learning based methods that constructs PDMs from unsegmented images.",vol1
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,2.0,Methods,"Image2SSM is a deep learning method that learns to build an SSM for an anatomical structure of interest directly from unsegmented images. It is trained on a population of I-3D images I = {I i } I i=1 as input and is supervised by their respective segmentations S = {S i } I i=1 . Image2SSM learns an RBF-based shape representation, consisting of a set of J control points P = {P i } I i=1 , and their surface normals N = {N i } I i=1 for each input shape, where the i-th shape point distribution model (PDM) is denoted by and p i,j , n i,j ∈ R 3 . The network is trained end-to-end to minimize a loss that (1) makes the learned control points and their surface normals adhere to the underlying surface, (2) approximates surface normals at each control point to encode a signed distance field to the surface, (3) promotes correspondence of these control points across shapes in the population, and (4) encourages a spread of control points on each surface that adapts to the underlying geometrical complexity. The learned control points define an anatomical mapping, or a metric, among the given shapes that enables quantifying subtle shape differences and performing shape statistics, for example, using principal component analysis (PCA) or other non-linear methods (e.g.,  In this section, we briefly elaborate on the RBF-shape representation, outline the network architecture, motivate the choices and design of the proposed losses, and detail the training protocol of Image2SSM.",vol1
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,2.1,Representing Shapes Using RBFs,"Implicit surface representation based on radial basis functions, RBF-shape for short, has been proven effective at representing intricate shapes by leveraging both surface control points and normals to inform shape reconstructions  where φ is the chosen RBF basis function (e.g., the thin plate spline φ(x, y) = (|x -y| 2 ) 2 log(|x -y| 2 ), the biharmonic φ(x, y) = |x -y| 2 or the triharmonic φ(x, y) = (|x -y| 2 ) 3 ) and c i ∈ R 3 and c 0 i ∈ R encodes the linear trend of the surface. We obtain by solving a system of equations formed by Eq. 1 over x ∈ P i , along with constraints to keep the linear part separate from the nonlinear deformations captured by the RBF term (first term in Eq. 1) to form a fully determined system. See  This representation can better represent shapes with far fewer control points due to its built-in interpolation capabilities, even further enhanced by informing the system with the point normals. Furthermore, this continuous representation allows Image2SSM to adapt to the underlying surface geometry and correct for control point placement mistakes while training.",vol1
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,2.2,Loss Functions,"Image2SSM uses four complementary loss functions to be trained on concurrently, illustrated in Fig.  where D i (p i,j ) is the distance transform value at point p i,j . Normal Loss: This loss aims to estimate the surface normal of each control point. This loss is supervised by the gradient of signed distance transforms (SDF) D = {D i } I i=1 , computed from the binary segmentations S, with respect to x, ∂D = {∂D i } I i=1 , which captures unnormalized surface normals. We use the cosine distance (in degrees) to penalize the deviation of the estimated normals from the normals computed from the distance transforms. Correspondence Loss: The notion of control points correspondence across the shape population can be quantified by the information content of the probability distribution induced by these control points in the shape space, the vector space defined by the shapes' PDMs  where P here indicates the random variable of the shape space. Sampling Loss: This loss makes f encode the signed distance to the surface while encouraging the control points to be adapted to the underlying geometry. Here, we randomly sample Rpoints ] that lie within a narrow band of thickness 2s around the surface (i.e., ±s from the zero-level set along the surface normal). The sampling loss minimizes distances between these narrow band points and the closest control point to each, scaled by the severity of the distance-to-surface approximation error. This objective guides control points to areas poorly described by f to progressively improve the signed distance-tosurface approximation and represent the shape more accurately. Let K i ∈ R R×M define the pairwise distances between each narrow band point b i,r and each control point p i,j for the i-th shape, where its r, j-th element k i r,j = b i,r -p i,j 2 . Let softmin(K i ) encode the normalized (over P i ) spatial proximity of each narrow band point to each control point, where r, the jth element of softmin(K i ) is computed as exp (-k i r,j )/ + captures the RBF approximation squared error at the narrow band points, where , where 1 M is a ones-vector of size M . The samples loss can then be written as, where ⊗ indicates the Hadamard (elementwise) multiplication of matrices and mean computes the average over the matrix elements. Image2SSM Loss: Given a minibatch of size K, the total loss of Image2SSM can be written as follows: where α, β, γ, and ζ ∈ R + are weighting hyperparameters of the losses and P K , N K are the control points and normals of the samples in the minibatch. Figure ",vol1
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,3.0,Results,"We demonstrate Image2SSM's performance against the state-of-the-art correspondence optimization algorithm, namely the particle-based shape modeling (PSM), using its open-source implementation, ShapeWorks ",vol1
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,3.1,Datasets,"We run tests on a dataset consisting of 50 proximal femur CT scans devoid of pathologies in the form of image-segmentation pairs. The femurs are reflected when appropriate and rigidly aligned to a common frame of reference. Due to space limitations, we also show similar results for a large-scale left atrium MRI dataset in the supplementary materials. For ease of comparison, we build SSMs with 128 particles for all algorithms as is sufficient to cover important femur shape features (femoral head with its fovea and the lesser and greater trochanter). Statistical Shape Model: We showcase Image2SSM in creating a statistical shape model on its training data and compare such a model with one optimized by PSM  We implement Image2SSM in PyTorch and leverage the Autograd functionality to perform gradient descent using the Adam optimizer ",vol1
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,4.0,Conclusion,"Image2SSM is a novel deep-learning framework that both builds PDMs from image-segmentation pairs and predicts PDMs from unseen images. It uses an RBF-shape able to capture detail by leveraging surface normals at control points, and allows the SSM to adaptively permeate surfaces with high-level detail. Image2SSM represents another step forward in fully end-to-end PDMs and steers the field to utilizing more compact but comprehensive representations to achieve new analytical paradigms. Future directions include removing the requirement that the image-segmentation pairs must be rougly aligned across the cohort and relaxing the Gaussian assumption from correspondence enforcement.",vol1
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_49.,vol1
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,1.0,Introduction,"Automated segmentation of histopathological images is crucial, as it can quantify the tumor micro-environment, provide a basis for cancer grading and prognosis, and improve the diagnostic efficiency of clinical doctors  Ours CAM Under the microscope, tumor epithelial Ɵssue may appear as solid nests, acinar structures, or papillary formaƟons. The cells may have enlarged and irregular nuclei.",vol1
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,,Tumor epithelial Ɵssue,"Necrosis Ɵssue Tumor-associated stroma Necrosis may appear as areas of pink, amorphous material under the microscope, and may be surrounded by viable tumor cells and stroma. Tumor-associated stroma Ɵssue is the connecƟve Ɵssue that surrounds and supports the tumor epithelial Ɵssue. Fig.  Recent studies on weakly supervised segmentation primarily follow class activation mapping (CAM)  To remedy the limitations of image-level supervision, we advocate for the integration of language knowledge into weakly supervised learning to provide reliable guidance for the accurate localization of target structures. To this end, we propose a text-prompting-based weakly supervised segmentation method (TPRO) for accurate histopathology tissue segmentation. The text information originates from the task's semantic labels and external descriptions of subtype manifestations. For each semantic label, a pre-trained medical language model is utilized to extract the corresponding text features that are matched to each feature point in the image spatial space. A higher similarity represents a higher possibility of this location belonging to the corresponding semantic category. Additionally, the text representations of subtype manifestations, including tissue morphology, color, and relationships to other tissues, are extracted by the language model as external knowledge. The discriminative information can be explored from the text knowledge to help identify and locate complete tissues accurately by jointly modeling long-range dependencies between image and text. We conduct experiments on two weakly supervised histological segmentation benchmarks, LUAD-HistoSeg and BCSS-WSSS, and demonstrate the superior quality of pseudo labels produced by our TPRO model compared to other CAM-based methods. Our contributions are summarized as follows: (1) To the best of our knowledge, this is the first work that leverages language knowledge to improve the quality of pseudo labels for weakly-supervised histopathology image segmentation. ",vol1
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,2.0,Method,Figure ,vol1
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,2.1,Classification with Deep Text Guidance,"Vision Encoder. The vision encoder is composed of four stages that encode the input image into image features. The image features are denoted as T s ∈ R Ms×Cs , where 2 ≤ s ≤ 4 indicates the stage number. Label Encoder. The label encoder encodes the text labels in the dataset into N label features, denoted as L ∈ R N ×C l , where N represents the number of classes in the dataset and C l represents the dimension of label features. Since the label features will be used to calculate the similarity with image features, it is important to choose a language model that has been pre-trained on image-text pairs. Here we use MedCLIP Knowledge Encoder. The knowledge encoder is responsible for embedding the descriptions of subtype manifestations into knowledge features, denoted as K ∈ R N ×C k . The knowledge features guide the image features to focus on regions relevant to the target tissue. To encode the subtype manifestations description into more general semantic features, we employ ClinicalBert  Adaptive Layer. We freeze the label and knowledge encoders for training efficiency but add an adaptive layer after the text encoders to better tailor the text features to our dataset. The adaptive layer is a simple FC-ReLU-FC block that allows for fine-tuning of the features extracted from the text encoders. Label-Pixel Correlation. After the input image and text labels are embedded. We employ the inner product to compute the similarity between image features and label features, denoted as F s . Specially, we first reshape the image features from a token format into feature maps. We denote the feature map as I s ∈ R Hs×Ws×Cs , where H s and W s mean the height and width of the feature map. F s is computed with the below formula (1) Then, we perform a global average-pooling operation on the produced similarity map to obtain the class prediction, denoted as P s ∈ R 1×N . We then calculate the binary cross-entropy loss between the class label Y ∈ R 1×N and the class prediction P s to supervise the model training, which is formulated as: Deep Supervision. To leverage the shallow features in the network, we employ a deep supervision strategy by calculating the similarity between the image features from different stages and the label features from different adaptive layers. Class predictions are derived from these similarity maps. The loss of the entire network is computed as: (3)",vol1
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,2.2,Knowledge Attention Module,"To enhance the model's understanding of the color, morphology, and relationships between different tissues, we gather text representations of different subtype manifestations from the Internet and encode them into external knowledge via the knowledge encoder. The knowledge attention module uses this external knowledge to guide the image features toward relevant regions of the target tissues. The knowledge attention module, shown in Fig. ",vol1
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,2.3,Pseudo Label Generation,"In the classification process, we calculate the similarity between image features and label features to obtain a similarity map F , and then directly use the result of global average pooling on the similarity map as a class prediction. That is, the value at position (i, j, k) of F represents the probability that pixel (i, j) is classified into the k th class. Therefore we directly use F as our localization map. We first perform min-max normalization on it, the formula is as follows where 1 ≤ c ≤ N means c th class in the dataset. Then we calculate the background localization map by the following formula: where α ≥ 1 denotes a hyper-parameter that adjusts the background confidence scores. Referring to  Finally, we perform argmax operation on F all to obtain the final pseudo-label. 3 Experiments ",vol1
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,3.2,Implementation Details,"For the classification part, we adopt MixTransformer ",vol1
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,3.3,Compare with State-of-the-Arts,Comparison on Pseudo-Labels. Table ,vol1
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,3.4,Ablation Study,The results of our ablation experiments are presented in Table ,vol1
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,4.0,Conclusion,"In this paper, we propose the TPRO to address the limitation of weakly supervised semantic segmentation on histopathology images by incorporating text supervision and external knowledge. We argue that image-level labels alone cannot provide sufficient information and that text supervision and knowledge attention can provide additional guidance to the model. The proposed method achieves the best results on two public datasets, LUAD-HistoSeg and BCSS-WSSS, demonstrating the superiority of our method.",vol1
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_11.,vol1
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,1.0,Introduction,"The interpretability of deep learning models is especially a concern for applications related to human health, such as analyzing longitudinal brain MRIs. To avoid interpretation during post-hoc analysis  Intriguing as it sounds, we found their application to (longitudinal) 3D brain MRIs unstable during training and resulted in uninformative SOMs. These models get stuck in local minima so that only a few SOM representations are updated during backpropagation. The issue has been less severe in prior applications  To generate SOMs informative to neuroscientists, we extend SOMs to the longitudinal setting such that the latent space and corresponding SOM grid encode brain aging. Inspired by  Named Longitudinally-consistent Self-Organized Representation learning (LSOR), we evaluate our method on a longitudinal T1-weighted MRI dataset of 632 subjects from ADNI to encode the brain aging of Normal Controls (NC) and patients diagnosed with static Mild Cognitive Impairment (sMCI  on longitudinal MRIs, i.e., without using any tabular data such as age, cognitive measure, or diagnosis. To visualize aging effects on the grid, we compute (post-hoc) a 2D similarity grid for each MRI that stores the similarity scores between the latent representation of that MRI and all SOM representations. As the SOM grid is an encoding of brain aging, the similarity grid indicates the likelihood of placing the MRI within the ""spectrum"" of aging. Given all MRIs of a longitudinal scan, the change across the corresponding similarity grids over time represents the brain aging process of that individual. Furthermore, we infer brain aging on a group-level by first computing the average similarity grid for an age group and then visualizing the difference of those average similarity grids across age groups. With respect to the downstream tasks of classification (sMCI vs. pMCI) and regression (i.e., estimating the Alzheimer's Disease Assessment Scale-Cognitive Subscale (ADAS-Cog) on all subjects), our latent representations of the MRIs is associated with comparable or higher accuracy scores than representations learned by other state-of-the-art self-supervised methods.",vol1
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,2.0,Method,As shown in Fig. ,vol1
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,2.1,LSOR,"Following  where E defines the expected value. The remainder describes the three novel components of our SOM representation. Explicitly Regularizing Closeness. Though L recon implicitly encourages close proximity between z × and g × , it does not inherently optimize g × as z × is not differentiable with respect to g × . Therefore, we introduce an additional 'commitment' loss explicitly promoting closeness between them: Soft Weighting Scheme. In addition to update z × 's closest SOM representation g × , we also update all SOM representations g i,j by introducing a soft weighting scheme as proposed in  where δ(w) := w i,j wi,j ensures that the scale of weights is constant during training and τ > 0 is a scaling hyperparameter. Now, we design the following loss L som so that SOM representations close to × on the grid are also close to z × in the latent space (measured by the Euclidean distance z × -g i,j 2 ): To improve robustness, we make two more changes to Eq. 3. First, we account for SOM representations transitioning from random initialization to becoming meaningful cluster centers that preserve the high-dimensional relationships within the 2D SOM grid. We do so by decreasing τ in Eq. 2 with each iteration so that the weights gradually concentrate on SOM representations closer to g × as training proceeds: with τ min being the minimum and τ max the maximum standard deviation in the Gaussian kernel, and t represents the current and T the maximum iteration. The second change to Eq. 3 is to apply the stop-gradient operator sg[•]  Longitudinal Consistency Regularization. We derive a SOM grid related to brain aging by generating an age-stratified latent space. Specifically, the latent space is defined by a smooth trajectory field (Fig.  α is the EMA keep rate, k denotes the index of the sample pair, N bs symbolizes the batch size, 1[•] is the indicator function, and |Ω i,j | denotes the number of sample pairs with u = (i, j) within a batch. Then in each iteration, Δh i,j (Fig.  where θ[•, •] denotes the angle between two vectors. Since Δg is optimized by EMA, the stop-gradient operator is again incorporated to only compute the gradient with respect to Δz in L dir . Objective Function. The complete objective function is the weighted combination of the prior losses with weighing parameters λ commit , λ som , and λ dir : The objective function encourages a smooth trajectory field of aging on the latent space while maintaining interpretable SOM representations for analyzing brain age in a pure self-supervised fashion.",vol1
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,2.2,SOM Similarity Grid,"During inference, a (2D) similarity grid ρ is computed by the closeness between the latent representation z of an MRI sample and the SOM representations: std denotes the standard deviation of the distance between z to all SOM representations. As the SOM grid is learned to be associated with brain age (e.g., represents aging from left to right), the similarity grid essentially encodes a ""likelihood function"" of the brain age in z. Given all MRIs of a longitudinal scan, the change across the corresponding similarity grids over time represents the brain aging process of that individual. Furthermore, brain aging on the group-level is captured by first computing the average similarity grid for an age group and then visualizing the difference of those average similarity grids across age groups.",vol1
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,3.1,Experimental Setting,"Dataset. We evaluated the proposed method on all 632 longitudinal T1weighted MRIs (at least two visits per subject, 2389 MRIs in total) from ADNI-1  To quantify the interpretability of the SOM grid, we correlated the coordinates of the SOM grid with quantitative measures related to brain age, e.g., chronological age, the percentage of subjects with severe cognitive decline, and Alzheimer's Disease Assessment Scale-Cognitive Subscale (ADAS-Cog). We illustrated the interpretability with respect to brain aging by visualizing the changes in the SOM similarity maps over time. We further visualized the trajectory vector field along with SOM representations by projecting the 1024-dimensional representations to the first two principal components of SOM representations. Lastly, we quantitatively evaluated the quality of the representations by applying them to the downstream tasks of classifying sMCI vs. pMCI and ADAS-Cog prediction. We measured the classification accuracy via Balanced accuracy (BACC) and Area Under Curve (AUC) and the prediction accuracy via R2 and rootmean-square error (RMSE). The classifier and predictor were multi-layer per- ceptrons containing two fully connected layers of dimensions 1024 and 64 with a LeakyReLU activation. We compared the accuracy metrics to models using the same architecture with encoders pre-trained by other representation learning methods, including unsupervised methods (AE, VAE ",vol1
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,3.2,Results,"Interpretability of SOM Embeddings. Fig.  Interpretability of Trajectory Vector Field. Fig.  Downstream Tasks. To evaluate the quality of the learned representations, we froze encoders trained by each method without fine-tuning and utilized their representations for the downstream tasks (Table ",vol1
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,4.0,Conclusion,"In this work, we proposed LSOR, the first SOM-based learning framework for longitudinal MRIs that is self-supervised and interpretable. By incorporating a soft SOM regularization, the training of the SOM was stable in the highdimensional latent space of MRIs. By regularizing the latent space based on longitudinal consistency as defined by longitudinal MRIs, the latent space formed a smooth trajectory field capturing brain aging as shown by the resulting SOM grid. The interpretability of the representations was confirmed by the correlation between the SOM grid and cognitive measures, and the SOM similarity map. When evaluated on downstream tasks sMCI vs. pMCI classification and ADAS-Cog prediction, LSOR was comparable to or better than representations learned from other state-of-the-art self-and un-supervised methods. In conclusion, LSOR is able to generate a latent space with high interpretability regarding brain age purely based on MRIs, and valuable representations for downstream tasks.",vol1
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 27.,vol1
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,1.0,Introduction,"Accurate 3D models of blood vessels are increasingly required for several purposes in Medicine and Science  Within the existing literature on generating vascular 3D models, we identified two primary types of algorithms: fractal-based, and space-filling algorithms. Fractal-based algorithms use a set of fixed rules that include different branching parameters, such as the ratio of asymmetry in arterial bifurcations and the relationship between the diameter of the vessel and the flow  In recent years, deep neural networks led to the development of powerful generative models  In this work we propose a novel data-driven framework named VesselVAE for synthesizing blood vessel geometry. Our generative framework is based on a Recursive variational Neural Network (RvNN), that has been applied in various contexts, including natural language ",vol1
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,2.0,Methods,"Input. The network input is a binary tree representation of the blood vessel 3D geometry. Formally, each tree is defined as a tuple (T, E), where T is the set of nodes, and E is the set of directed edges connecting a pair of nodes (n, m), with n, m ∈ T . In order to encode a 3D model into this representation, vessel segments V are parameterized by a central axis consisting of ordered points in Euclidean space: V = v 1 , v 2 , . . . , v N and a radius r, assuming a piece-wise tubular vessel for simplicity. We then construct the binary tree as a set of nodes T = n 1 , n 2 , . . . , n N , where each node n i represents a vessel segment v and contains an attribute vector 4 with the coordinates of the corresponding point and its radius r i . See Sect. 3 for details. Network Architecture. The proposed generative model is a Recursive variational Neural Network (RvNN) consisting of two main components: the Encoder (Enc) and the Decoder (Dec) networks. The Encoder transforms a tree structure into a hierarchical encoding on the learned manifold. The Decoder network is capable of sampling from this encoded space to decode tree structures, as depicted in Fig.  Within the RvNN Decoder network there are two essential components: the Node Classifier (Cls) and the Features Decoder Multi-Layer Perceptron (Features Dec-MLP). The Node Classifier discerns the type of node to be decoded, whether it is a leaf node or an internal node with one or two bifurcations. This is implemented as a multi-layer perceptron trained to predict a three-category bifurcation probability based on the encoded vector as input. Complementing the Node Classifier, the Features Dec-MLP is responsible for reconstructing the attributes of each node, specifically its coordinates and radius. Furthermore, two additional components, the Right and Left Dec-MLP, are in charge of recursively decoding the next encoded node in the tree hierarchy. These decoder's branches execute based on the classifier prediction for that encoded node. If the Node Classifier predicts a single child for a node, a right child is assumed by default. In addition to the core architecture, our model is further augmented with three auxiliary, shallow, fully-connected neural networks: f μ , f σ , and g z . Positioned before the RvNN bottleneck, the f μ and f σ networks shape the distribution of the latent space where encoded tree structures lie. Conversely, the g z network, situated after the bottleneck, facilitates the decoding of latent variables, aiding the Decoder network in the reconstruction of tree structures. Collectively, these supplementary networks streamline the data transformation process through the model. All activation functions used in our networks are leaky ReLUs. See the Appendix for implementation details. Objective. Our generative model is trained to learn a probability distribution over the latent space that can be used to generate new blood vessel segments. After encoding, the decoder takes samples from a multivariate Gaussian distribution: , where Enc is the recursive encoder and f μ , f σ are two fully-connected neural networks. In order to recover the feature vectors x for each node along with the tree topology, we simultaneously train the regression network (Features Dec-MLP in Fig.  where the reconstruction loss is defined as and the topology objective is a three-class cross entropy loss L topo = Σ 3 c=1 x c log(Cls(Dec(x)) c ). Notice that x c is a binary indicator (0 or 1) for the true class of the sample x. Specifically, x c = 1 if the sample belongs to class c and 0 otherwise. Cls(Dec(x)) c is the predicted probability of the sample x belonging to class c (zero, one, or two bifurcations), as output by the classifier. Here, Dec(x) denotes the encoded-decoded node representation of the input sample x. 3D Mesh Synthesis. Several algorithms have been proposed in the literature to generate a surface 3D mesh from a tree-structured centerline ",vol1
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,3.0,Experimental Setup,"Materials. We trained our networks using a subset of the open-access IntrA dataset Implementation Details. For the centerline extraction, we set the advancement ratio in the VMTK script to 1.05. The script can sometimes produce multiple cross-sections at centerline bifurcations. In those cases, we selected the sample with the lowest radius, which ensures proper alignment with the centerline principal direction. All attributes were normalized to a range of [0, 1]. For the mesh reconstruction we used 4 iterations of Catmull-Clark subdivision algorithm. The data pre-processing pipeline and network code were implemented in Python and PyTorch Framework. Training. In all stages, we set the batch size to 10 and used the ADAM optimizer with β 1 = 0.9, β 2 = 0.999, and a learning rate of 1 × 10 -4 . We set α = .3 and γ = .001 for Eq. 1 in our experiments. To enhance computation speed, we implemented dynamic batching  Metrics. We defined a set of metrics to evaluate our trained network's performance. By using these metrics, we can determine how well the generated 3D models of blood vessels match the original dataset distribution, as well as the diversity of the generated output. The chosen metrics have been widely used in the field of blood vessel 3D modeling, and have shown to provide reliable and accurate quantification of blood vessels main characteristics ",vol1
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,4.0,Results,"We conducted both quantitative and qualitative analyses to evaluate the model's performance. For the quantitative analyses, we implemented a set of metrics commonly used for characterizing blood vessels. We computed histograms of the radius, total length, and tortuosity for the real blood vessel set and the generated set (700 samples) in Fig.  The qualitative analyses consisted of a visual evaluation of the reconstructed outputs provided by the decoder network. We visually compared them to stateof-the-art methods in Fig. ",vol1
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,5.0,Conclusions,"We have presented a novel approach for synthesizing blood vessel models using a variational recursive autoencoder. Our method enables efficient encoding and decoding of binary tree structures, and produces high-quality synthesized models. In the future, we aim to explore combinations of our approach with representing surfaces by the zero level set in a differentiable implicit neural representation (INR) ",vol1
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_7.,vol1
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,1.0,Introduction,"Existing out-of-distribution (OOD) detection methods work well when the indistribution (ID) classes have low heterogeneity (low variance) but fail when in-distribution classes have high heterogeneity  In this paper, we propose a Dual-Conditioned Diffusion Model (DCDM) to detect OOD samples when in-distribution data has high variance and test the performance by detecting heart views in fetal US videos as an example application. Specifically, an Ultrasound (US) typically comprises 13 anatomies and their views. However, analysis models are usually developed for anatomy-specific tasks. Hence, to separate heart views from other 12 anatomies (head, abdomen, femur etc.) we develop an OOD detection algorithm. Our in-distribution data comprises five structurally different heart views captured across different cardiac cycles of a beating heart during obstetric US scanning. We develop a diffusionbased model for reconstruction-based OOD detection, which extends ",vol1
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.0,Related Work,OOD detection  Denoising Diffusion Probabilistic Models (DDPMs) ,vol1
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.1,Dual Conditioned Diffusion Models,"Diffusion models are generative models that rely on two Markov processes known as forward and backward diffusion  In backward diffusion, we aim to reverse the forward diffusion process and predict z t-1 given z t . To predict (z t-1 |z t ), we train a denoising U-Net  The dual embedding vector d 0 is obtained by combining IDCC (f cls ) and LIFC (f img ) vectors, which we explain in Sect. 3.2. The output z t-1 is again input to θ . This process is repeated until z 0 is obtained. The final model optimisation objective is given by Eq. 3 where is the original noise added during the forward diffusion process. (3) Once we obtain z 0 from the backward diffusion process, it is passed on to the decoder D and mapped back to the pixel space to give generated image x 0 .",vol1
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.2,Dual Conditioning Mechanism,Image features and in-distribution class information are utilised in our proposed dual conditioning mechanism. This guides the DCDM to generate images that are spatially and semantically similar to the input image for in-distribution samples and dissimilar for OOD samples.,vol1
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Latent Image Feature Conditioning (LIFC):,"The image conditioning dictates the desired appearance of generated images in terms of shape and texture. In our model, we use the features extracted by a pretrained encoder for conditioning. Empirically, we use the same encoder E as our feature extractor to obtain latent feature vector z 0 as shown in Fig. ",vol1
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Cross Attention Guidance:,"To integrate the dual-conditioning guidance into the diffusion model, we use a cross-attention ",vol1
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.3,In-Distribution Classifier,"The in-distribution classifier (CFR) serves two main functions. First, it provides labels for the class conditioning during inference; second, it is utilized as a feature extractor for calculating the OOD score. Inference Class Guidance. IDCC requires in-distribution class information to generate the class conditional embedding. However, class information is only available during training. To obtain class information during inference, we separately train a ConvNext CNN based classifier (accuracy = 88%) on the indistribution data and use its predictions as the class information. During inference, the input image x 0 is passed through the classifier, and the predicted label is used to generate the class embedding by feeding to the label encoder as shown in Fig. ",vol1
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Feature-Based OOD Detection,"To evaluate the performance of the DCDM, the cosine similarity between features of the input image x 0 and the generated image x 0 from the in-distribution classifier is calculated and is referred as an OOD score where f 0 and f 0 are the features of x 0 and x 0 , respectively: An input image x 0 is classified as in-distribution (ID) or OOD based on Eq. 5 where τ is a pre-defined threshold and y pred is the prediction of our feature-based OOD detection algorithm.",vol1
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,4.0,Experiments and Results,"Dataset and Implementation. For our experiments, we utilized a fetal ultrasound dataset of 359 subject videos that were collected as part of the PULSE project  To train the models, we randomly sampled 5000 fetal heart images and used 500 images for evaluating image generation performance. To test the performance of our final model and compare it with other methods, we used an held-out dataset of 7471 images, comprising 4309 images of different heart views and 3162 images (about 1000 for each anatomy) of out-of-distribution classes. Further details about the dataset are given in Supp. Fig.  All models were trained using PyTorch version 1.12 with a Tesla V100 32 GB GPU. During training, we used T=1000 for noising the input image and a linearly increasing noise schedule that varied from 0.0015 to 0.0195. To generate samples from our trained model, we used DDIM ",vol1
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,4.1,Results,"We evaluated the performance of the dual-conditioned diffusion models (DCDMs) for OOD detection by comparing them with two current state-ofthe-art unsupervised reconstruction-based approaches and one likelihood-based approach. The first baseline is Deep-MCDD  Quantitative Results. The performance of the DCDM, along with comparisons with the other approaches, are shown in the input image and belonging to the same heart view for ID samples while structurally diverse heart views for OOD samples. In Fig. ",vol1
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,4.2,Ablation Study,"Ablation experiments were performed to study the impact of various conditioning mechanisms on the model performance both qualitatively and quantitatively. When analyzed quantitatively, as shown in    separately, improves performance with an AUC of 75.27% and 77.40%, respectively. The best results are achieved when both mechanisms are used (DCDM), resulting in an 11% improvement in the AUC score relative to the unconditional model. Although there is a small margin of performance improvement between the combined model (DCDM) and the LIFC model in terms of AUC, the precision improves by 3%, demonstrating the combined model is more precise and hence the best model for OOD detection. As shown in Fig. ",vol1
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,5.0,Conclusion,"We introduce novel dual-conditioned diffusion model for OOD detection in fetal ultrasound videos and demonstrate how the proposed dual-conditioning mechanisms can manipulate the generative space of a diffusion model. Specifically, we show how our dual-conditioning mechanism can tackle scenarios where the in-distribution data has high inter-(using IDCC) and intra-(using LIFC) class variations and guide a diffusion model to generate similar images to the input for in-distribution input and dissimilar images for OOD input images. Our approach does not require labelled data for OOD classes and is especially applicable to challenging scenarios where the in-distribution data comprises more than one class and there is high similarity between the in-distribution and OOD classes.",vol1
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,1.0,Introduction,"Brain computer interface (BCI) technology plays an increasingly crucial role in the rehabilitation process of patients with nerve damage. However, the lack of large-scale labeled data makes the identification results more vulnerable to be affected. BCI technology with motor imagery (MI), which can decode neural activities through EEG signals to identify the movement intention of the human being, is widely applied in the field of EEG decoding, where the substantial problem of BCI decoding is to extract as much effective information as possible from the multi-channel and non-linear EEG signals for understanding the oscillating activities in the brain  According to the development of EEG signal processing technology, the traditional EEG signal feature extraction methods  The starting point of this paper is that the following biological findings can be observed in the MI decoding  Therefore, based on abductive learning (ABL)  1) A novel EEG decoding method is proposed to tackle the MIR (motion intention recognition) problem with small-sample EEG signals. This method does not rely on strict mathematical assumptions for datasets and its accuracy and robustness are well-maintained under strong interference. 2) A multi-scale feature fusion network is designed to enhance abstract features, which can capture temporal and frequency information across multi-channel EEG signals and spatial relationships among different electrodes. 3) An effective knowledge base module of motor imagery is constructed and symbolized, which can upgrade the model space under this constraint by mining the potential facts of large-scale unlabeled EEG signals.",vol1
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,2.1,Problem Definition,"In the SSE-ABL framework for EEG decoding, the given input is defined as Input = {X l , X u , KB θ }, where tensor X l denotes labeled EEG data, tensor X u denotes unlabeled EEG data, the data size of X u is much larger than that of X l , and KB θ denotes knowledge base on brain science in MI task. Concretely, X l = {(x l1 , y l1 ), (x l2 , y l2 ), . . . , (x li , y li )}, where variable x li represents multi-channel EEG signals, and y li implies the corresponding labels. And the assignment of X l is to learn a mapping from x to y. X u = {x u1 , x u2 , . . . , x uj |i j}, where x uj denotes unlabeled multi-channel EEG signals, which are utilized to boost representative capability of the above mapping. KB θ consists of a series of first-order logic sentences with learnable parameters θ , which integrate labels EEG data X l and unlabeled data X u to optimize a perceptual model and train parameters θ with the constraint of knowledge base. The final result is regarded as Output = {f , W, θ }, where f is a mapping from EEG signals to motion intention, W ∈ R m×n indicates the proportion of the m th sub-band to the n th channel in EEG data, θ ∈ R k represents the contribution rate of the k th channel to the whole in MIR. The SSE-ABL algorithm yields the corresponding pseudo-labels to the unlabeled data by the classifier optimized by a small amount of labeled EEG signals, and the produced labels may be incorrect due to the small number of training samples, which is difficult to guarantee good performance. Therefore, the SSE-ABL modifies the pseudo-labels and optimizes the internal parameters of the knowledge base at the same time, so that the consistency of them is maximized under the constraint of the knowledge base. Formally, the problem definition of the SSE-ABL can be summarized as an optimization problem of searching Output under a given Input: where y uj is the pseudo-label corresponding to the j th unlabeled instance, which is generated by the perceptual module. δ(•) indicates a heuristic function obtained by optimization, which aims to revise pseudo-labels by logical abduction process. In addition to correcting inconsistent pseudo-labels, this goal also helps the knowledge base to learn accurate parameter θ . It can be seen from Eq. 1 and Eq. 2 that the major challenge is how to mine the effective information of massive unlabeled EEG data under the KB θ constraints and react to the iterative update of itself.",vol1
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,2.2,Architecture of the SSE-ABL Framework,"The proposed method, as shown in Fig.  1) Phase 1 (Sample Representation): For the purpose of promoting the ability to describe the local and global details of the brain activities, a time-frequency-space data representation method based on brain region division is proposed, which makes the receptive field of view cover the whole brain region in a fine-grained way. 2) Phase 2 (Multiscale Feature Fusion): Inspired by the neuroscience findings shown in Sect. 1, a multi-scale feature fusion model is proposed to adaptively integrate time-frequency-space information of EEG signals aiming at reducing the potential difference of feature distribution and selectively focusing on the MI-related materials. 3) Phase 3 (Motion Intention Estimation): Based on the common laws of EEG signals distribution, a motor intention evaluation model is constructed, in which the rules are used as the supervisory information to judge the authenticity of motor imagery recognition, so as to ensure that the overall process conforms to the actual criteria. 4) Phase 4 (Abductive Reasoning Optimization): When the classifier is insufficienttrained, the pseudo-labels could be wrong, the SSE-ABL method in this paper needs to correct the wrong pseudo-labels to achieve consistent abductions by using gradient free optimization method under the principle of minimal inconsistency ",vol1
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,2.3,Sample Representation,"The preprocessed EEG signals are first divided into five bands containing delta (1-4 Hz), theta (4-8 Hz), alpha (8-14 Hz), beta (14-31 Hz) and gamma (31-50 Hz) wavebands by using digital band-pass filters whose corresponding cut-off frequency is performed according to the division standard in  where represents the decomposition of the signal f (i) (t) into j sub-bands. The inner product process is the principle of CWT, Cat (x,y) implies that the y th tensor data is concatenated according to the x th dimension. The procedure of fusing time-frequency information directly in multiple frequency bands may ignore the spatial distribution of the electrode. Therefore, multi-channel EEG signals are executed across the channel direction through the channel-by-channel convolutional operations, which aims at capturing spatial dynamic correlation characteristics (S t ∈ R N ×C×H ×W ) among brain regions, as shown in Eq. 4. where g(•) implies 1-D convolutional operation, tensor W and b denotes convolution weights and bias, Append (•) means adding each element in turn. With the above configurations, the mixed sample (MS t ∈ R N ×2C×H ×W ), which is composed of tf t andS t , are represented as 4-D tensor, where (C, H , W ) is the number of channels and the resolution of the feature map respectively.",vol1
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,2.4,Multiscale Feature Fusion,"Given the mixed samples, the multiscale feature fusion model embeds them into feature vectors and extracts the time-frequency-space features of multi-channel EEG signals. We introduce the embedding layers in phase 2 to better describe the semantic and location information of EEG and improve the EEG transformer encoder to pay attention to seizing the global and local information of long-term EEG signals. Concretely, the normalized operation, the position mark of multiple MS t and the order inside it are added to attain the input of the network by conducting token, segment and position embedding steps, respectively, as showed in Eq. 5. Note that the addition here is bitwise addition. Tok, Seg and Pos correspond to the above three operations. Then tensor-patches in MS t are vectorized and carried to the network to calculate temporal and spatial self-attention block described in ",vol1
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,2.5,Motion Intention Estimation,"We construct MI classification module based on attention mechanism and knowledge rules, which consists of two parts: domain knowledge expressed by first-order logic formula and learnable parameters weights, as shown in Fig. ",vol1
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,3.0,Results,"We adopt the 2008 BCI competition IV-2a EEG dataset (https://www.bbci.de/competiti on/iv/) including 9 subjects with a 250 Hz sampling rate and band-pass filtered from 0.5 to 100 Hz, which consists of four MI tasks: imagine left hand (class 1), right hand (class 2), foot (class 3) and tongue (class 4) movements. And the preprocessing operations in this article refers to eliminating the wrong experiments and using digital band-pass filters of 1-50 Hz. In the experiments, all methods with self-teaching plan are set to the same number of modules using the default hyperparameters in the source code. The following experimental tests of the proposed method (SSE-ABL) are designed to conduct: 1) Compared with the current mainstream algorithms containing transformer (TF)  3) The visualization of the underlying information flow of EEG decoding by the proposed method is displayed. As shown in Table  Then, the visualization results of experiment 3 is exhibited in Fig. ",vol1
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,4.0,Conclusion,"A novel small-sample method with EEG signals based on abductive reasoning is studied for MI recognition. This method solves the problem of low precision and poor robustness ability of EEG decoding under a small amount of labeled data. The multiscale feature fusion module based on self-attention mechanism improves the ability of adaptive feature mining by capturing time-frequency-space information cover the whole brain region, which realizes the enhancement of abstract features. An effective knowledge base module of motor imagery is constructed and symbolized, which can upgrade the model space under this constraint by mining the potential facts of large-scale unlabeled EEG signals. Through the comparison experiments with other mainstream inversion methods, it can be founded that our method with 10% and 50% labeled EEG data can reach the accuracy of 80.03% and 81.53%, achieving the high precision standard of EEG decoding. In the future, we will consider a fine-grained visualization method of EEG signals in the case of partial data damage, such as signal coupling.",vol1
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,1.0,Introduction,"Nascent geographic atrophy (nGA), originally described by Wu et al.  However, identifying and grading the location of nGA lesions in OCT volume scans can be a laborious task, and would be an operationally expensive undertaking in clinical trials. Automation of this task would be invaluable when seeking to quantify the number of nGA lesions present, or when seeking to identify a smaller subset of B-scans for manual expert review (an ""AI-assisted"" approach). While the localization of nGA could be tackled by supervised object detection models -as demonstrated in other types of lesions  In this work, we sought to develop a deep learning-based method to automate the localization of nGA lesions on OCT imaging, trained only on the information about the presence or absence of nGA at the volume level. A weakly supervised algorithm was developed that utilizes the saliency maps from Gradient Class Activation Maps (GradCAM) technique ",vol1
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,2.1,Dataset,"This study included participants in the sham treatment arm of the Laser Intervention in the Early Stages of AMD study (LEAD, clinicaltrials.gov identifier, NCT01790802). The LEAD study was conducted according to the International Conference on Harmonization Guidelines for Good Clinical Practice and the tenets of the Declaration of Helsinki. Institutional review board approval was obtained at all sites and all participants provided written informed consent. The participants of LEAD study were required to have bilateral large drusen and a best-corrected visual acuity of 20/40 or better in both eyes at baseline  Multimodal imaging was used to assess the development of late AMD as an endpoint in the LEAD study, which included nGA detected on OCT imaging. In order to evaluate the association between nGA and the subsequent development of GA as detected on color fundus photographs (CFP; the historical gold standard for atrophic AMD) in a previous study, OCT imaging and CFP were independently re-graded for the presence of nGA and GA respectively  A total of 1,884 OCT volumes from 280 eyes of 140 individuals were included in this analysis (1,910 volumes were collected, but 26 volumes were excluded from the study due to the development of neovascular AMD in the eye). In this study, the development of nGA was assessed by manual grading of all 49 B-scans of each OCT volume scans, and nGA was defined by the subsidence of the outer plexiform layer and inner nuclear layer, and/or the presence of a hyporeflective wedge-shaped band within Henle's fiber layer, as per the original definition  Overall, nGA was graded as being absent and present in 1,766 and 118 OCT volume scans respectively. In the context of this study, note that nGA also includes lesions that could also meet the criteria for having complete retinal pigment epithelium and outer retinal atrophy (cRORA), if the lesion also had choroidal signal hypertransmission and retinal pigment epithelium (RPE) attenuation or disruption of ≥250 µm ",vol1
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,2.2,Deep Learning Architecture,"A late-fusion model with a 2D ResNet backbone was developed to classify 3D OCT volumes, considering their anisotropic nature. As shown in Fig.  The details of the B-scan classifier are shown in Fig.  The classification model was evaluated on its own both in terms of volume-wise and slice-wise performance in classifying nGA. After it was confirmed that the classification model worked well, the ability of the model to localize the lesions within individual OCT slices was evaluated. Given an OCT volume and a trained model, saliency maps were generated with the GradCAM technique ",vol1
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,,S l n h,"where S is the sigmoid function, l is the individual B-scan classification logit, and n is the number of B-scans in a volume, h is the mean saliency in the detected region and Σh is the total mean saliency of all detected regions within the B-scan. A higher confidence score implies a higher possibility that the detected region covers nGA lesions. Since only class labels of 3D OCT volume are required for training, the proposed lesion localization algorithm was weakly supervised. ",vol1
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,2.3,"Model Training, Tuning, and Validation Test","Considering the relatively small number of participants in the dataset, a five-fold crossvalidation was applied to evaluate the proposed method's performance. We followed the nomenclature for data splitting as recommended previously  Pre-processing was performed on B-scans for standardization. The B-scans were first resized to 512 × 496, followed by rescaling the intensity range to [0, 1]. Data augmentation, including rotation of small angles, horizontal flips, add on of Gaussian noises, and Gaussian blur were randomly applied to improve the model's invariance to those transformations. A Resnet-18 backbone pre-trained on the ImageNet dataset was used. During the model training, the Adam optimizer was used to minimize focal loss. The L2 weight decay regularization was used to improve the model's generalization. As a benchmark for the weakly supervised lesion localization, a fully supervised YOLOv3 object detector  A successful lesion localization was recorded only if the bounding box output overlapped with the bounding boxes annotated by clinicians with an intersection over union (IoU) value of at least 0.05. The area under the Precision-Recall curve (AUPRC) was calculated to evaluate the model performance. In patient screening, a high recall is preferred over precision. Considering the difference of the two methods, different strategies were used to determine the confidence threshold in calculating the precision and recall values in the validation test dataset. For the weakly supervised method, the threshold for confidence score that would achieve a recall value of 0.98 for nGA volume classification in the training and tuning sets is used. For the supervised method, the confidence threshold which would achieve a recall value of 0.9 for bounding box detection in the turning set is used.",vol1
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,3.1,Performance and Saliency Map Analysis of the nGA Classification Model on OCT Volumes,"The deep learning based nGA classification model achieved an AUPRC of 0.83(±0.09) in classifying 3D OCT volumes. Based on the trained 3D OCT volume classification model and input OCT volumes, we generated the corresponding saliency map using GradCAM technique. Examples of GradCAM output are shown in Fig. ",vol1
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,3.2,Performance of the Weakly Supervised Localization of nGA Lesions,"The weakly supervised algorithm achieved a similar level of performance for localizing nGA when compared to the YOLOv3 based fully supervised method, without utilizing bounding box annotations and these findings are illustrated in Fig. ",vol1
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,4.0,Conclusion and Discussion,"This study demonstrates that the performance for localizing nGA lesions by only using OCT volume-wise classification labels with the GradCAM technique was on par with a fully supervised approach using B-scan level annotations with the YOLOv3 detector. These findings therefore underscore the potential of a weakly supervised approach for enabling the development of a robust model for lesion localization without the need for laborious, lesion-level annotations on OCT B-scans. One limitation of the GradCAM-based lesion localization is its relatively large bounding box size, often exceeding the annotated region. This is expected, considering the low spatial resolution of GradCAM saliency map, but also potentially because this approach identified contextual features that are distinguishing of nGA lesions that were not annotated by the graders. In addition, the weakly supervised model uses adaptive threshold of the saliency to determine the bounding box size, which was not optimized to match the ground truth grading. This limitation with the larger bounding box size could impact the quantification of the number of nGA lesions present, but it would unlikely have a substantial impact on the task of identifying a subset of OCT B-scans requiring manual review in an AI-assisted evaluation. In conclusion, this study demonstrates that a weakly supervised method, requiring only volume-wise tags, can achieve a similar level of performance for localizing lesions compared to a fully supervised method using slice-wise bounding box labels. A weakly supervised approach could thus minimize the labeling burden when seeking to develop a lesion localization model, and could even leverage existing volume-wise labels for its development.",vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,1.0,Introduction,"Transfer learning has become a standard practice in medical image analysis as collecting and annotating data in clinical scenarios can be costly. The pre-trained parameters endow better generalization to DNNs than the models trained from scratch  diversity between domains and tasks and privacy concerns related to pre-training data. Consequently, recent work  Previous studies have shown that the transferability of lower layers is often higher than higher layers that are near the model output  To search for optimal layer combinations for fine-tuning, manually selecting transferable layers  In summary, this work makes the following three contributions. 1) We introduce MetaLR, a meta-learning-based LR tuner that can adaptively adjust layerwise LRs based on transfer learning feedback from various medical domains. 2) We enhance MetaLR with a proportional hyper-LR and a validation scheme using batched training data to improve the algorithm's stability and efficacy. 3) Extensive experiments on both lesion detection and tumor segmentation tasks were conducted to demonstrate the superior efficiency and performance of Met-aLR compared to current SOTA medical fine-tuning techniques.",vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,2.0,Method,This section provides a detailed description of the proposed MetaLR. It is a meta-learning-based ,vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,2.1,Formulation of Meta Learning Rate,"Let (x, y) denote a sample-label pair, and {(x i , y i ) | i = 1, ..., N } be the training data. The validation dataset {(x v i , y v i ) | i = 1, ..., M } is assumed to be independent and identically distributed as the training dataset. Let ŷ = Φ(x, θ) be the prediction for sample x from deep model Φ with parameters θ. In standard training of DNNs, the aim is to minimize the expected risk for the training set:   Based on the generalization, one can tune the hyper-parameters of the training process to improve the model. The key idea of MetaLR is considering the layer-wise LRs as self-adaptive hyper-parameters during the training and automatically adjusting them to achieve better model generalization. We denote the LR and model parameters for the layer j at the iteration t as α t j and θ t j . The LR scheduling scheme α = {α t j | j = 1, ..., d; t = 1, ..., T } is what MetaLR wants to learn, affecting which local optimal θ * (α) the model parameters θ t = {θ t j | j = 1, ..., d} will converge to. The optimal parameters θ * (α) are given by optimization on the training data. At the same time, the best LR tuning scheme α * can be optimized based on the feedback for θ * (α) from",vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,Input:,"Training data D, validation data D v , initial model parameter {θ 0 1 , ..., θ 0 d }, LRs {α 0 1 , ..., α 0 d }, batch size n, max iteration T; Output: Final model parameter Step forward for one step to get { θt 1 (α   MetaLR aims to use the validation set to optimize α through an automatic process rather than a manual one. The optimal scheme α * can be found by a nested optimization ",vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,2.2,Online Learning Rate Adaptation,"Inspired by the online approximation  We adopt Stochastic Gradient Descent (SGD) as the optimizer to conduct the meta-learning. The whole training process is summarized in Algorithm 1. At the iteration t of training, a training data batch {(x i , y i ) | i = 1, ..., n} and a validation data batch {(x v i , y v i ) | i = 1, ..., n} are sampled, where n is the size of the batches. First, the parameters of each layer are updated once with the current LR according to the descent direction on training batch. This step of updating aims to get feedback for LR of each layer. After taking derivative of the validation loss w.r.t. α t j , we can utilize the gradient to know how the LR for each layer should be adjusted. So the second step of MetaLR is to move the LRs along the meta-objective gradient on the validation data: where η is the hyper-LR. Finally, the updated LRs can be employed to optimize the model parameters through gradient descent truly. For practical use, we constrain the LR for each layer to be α t j ∈ [10 -6 , 10 -2 ]. Online MetaLR optimizes the layer-wise LRs as well as the training objective on a single task, which differentiates it from traditional meta-learning algorithms ",vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,2.3,Proportional Hyper Learning Rate,"In practice, LRs are often tuned in an exponential style (e.g., 1e-3, 3e-3, 1e-2) and are always positive values. However, if a constant hyper-LR is used, it will linearly update its corresponding LR regardless of numerical constraints. This can lead to fluctuations in the LR or even the risk of the LR becoming smaller than 0 and being truncated. To address this issue, we propose using a proportional hyper-LR η = β × α t j , where β is a pre-defined hyper-parameter. This allows us to rewrite Eq. (3) as: The exponential update of α t j guarantees its numerical stability.",vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,2.4,Generalizability Validation on Training Data Batch,"One limitation of MetaLR is that the LRs are updated using separate validation data, which reduces the amount of data available for the training process. This can be particularly problematic for medical transfer learning, where the amount of downstream data has already been limited. In Eq. 2 and Eq. 3, the update of model parameter θ t j and LR α t j is performed using different datasets to ensure that the updated θ t j can be evaluated for generalization without being influenced by the seen data. As an alternative, but weaker, approach, we explore using another batch of training data for Eq. 3 to evaluate generalization. Since this batch was not used in the update of Eq. 2, it may still perform well for validation in meta-learning. The effect of this approach is verified in Sect. 3.2, and the differences between the two methods are analyzed in Sect. 3.4.",vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,3.1,Experimental Settings,We extensively evaluate MetaLR on four transfer learning tasks (as shown in Table ,vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,3.2,Ablation Study,"In order to evaluate the effectiveness of our proposed method, we conduct an ablation study w.r.t. the basic MetaLR algorithm, the proportional hyper-LR, and batched-training-data validation (as shown in Table ",vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,3.3,Comparative Experiments,"In our study, we compare MetaLR with several other fine-tuning schemes, including tuning only the last layer / all layers with constant LRs, layer-wise finetuning ",vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,Results on Lesion Detection Tasks.,MetaLR consistently shows the best performance on all downstream tasks (Table  Results on Segmentation Task. MetaLR achieves the best Dice performance on the LiTS segmentation task (Table ,vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,3.4,Discussion and Findings,"The LRs Learned with MetaLR. For ResNet-18 (Fig.  The first layer has a decreasing LR (from 2.8 × 10 -3 to 3 × 10 -4 ) throughout the process, reflecting its higher transferability. For 3D U-Net (Fig. ",vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,The Effectiveness of Proportional Hyper-LR and Training Batches,"Validation. We illustrate the LR curves with a constant hyper-LR instead of a proportional one. The LR curves of ""Block 3-1"" and ""Block 4-2"" become much more fluctuated (Fig. ",vol1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,4.0,Conclusion,"In this work, we proposed a new fine-tuning scheme, MetaLR, for medical transfer learning. It achieves significantly superior performance to the previous SOTA fine-tuning algorithms. MetaLR alternatively optimizes model parameters and layer-wise LRs in an online meta-learning fashion with a proportional hyper-LR. It learns to assign lower LRs for the layers with higher transferability and higher LRs for the less transferable layers. The proposed algorithm is easy to implement and shows the potential to replace manual layer-wise fine-tuning schemes. Future works include adapting MetaLR to a wider variety of clinical tasks.",vol1
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,1.0,Introduction,"Alzheimer's disease (AD) is an irreversible neurodegenerative disease that leaves patients with impairments in memory, language and cognition  Semi-supervised learning (SSL) methods are commonly used in medical image analysis to address the lack of manually annotated data  The proposal of evidential deep learning (EDL)  Our main contributions include: 1) Adjusting the loss function of evidential regression so it can obtain more accurate results and better separate AU and EU; 2) Building a multi-layer and multi-step network to implement evidential regression and a semi-supervised learning method of step-by-step training is proposed; 3) A new SOTA of semi-supervised learning is achieved on the ADNI dataset, and performance close to supervised learning can be achieved with only a small amount of labeled data.",vol1
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,2.1,Original Deep Evidential Regression (DER),"DER  And it follows that: where m = NN(w) is specified by a neural network (NN), λ is a hyperparameter, and Φ = 2γ i + α i represents the total evidence gained from training.",vol1
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,2.2,Evidential Regression Beyond DER,"Although DER has achieved some success in both theoretical and practical applications  After practice and theoretical proof, Meinert et al.  And, correspondingly, we use the residual 1/ √ ν i part of u a and u e in the original definition to represent EU: where ν i , α i , and β i are part of the parameters of the evidence distribution m = (γ, ν, α, β), and we verify the performance of this new uncertainty estimation method through experiments.",vol1
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,2.3,Model and Workflow,"With efficient estimation of AU and EU, our model has the basis for implementation. As shown in Fig. ",vol1
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,,AU for Training Classifier.,"Based on the manifold assumption, the real data is gathered on the low-dimensional manifold of the high-dimensional space, and the noise of the data is located on the edge of the manifold for the corresponding category. When using ER to fit the manifold, these noise data will make marginal data with high AU. By optimizing the classifier to iteratively reduce the AU, optimal classification result under the current conditions can be obtained. We use L a to optimize AU: In the above formula λ a = [0.005, 0.01] is a parameter that controls the degree of deviations of the regularization part and w St uses the previous definition, and Φ is the total amount of evidence learned by the model. In order to better motivate the learning of the model, we adopted the work of Liu et al.  EU for Training Extractor. If only the AU part is optimized, there will always be this gap between the model prediction and the real data. EU is mainly used to optimize the feature extractor since EU mainly reflects the bias of the model in the prediction. For data D l , given groundtruth labels, we use which can make the model more conservative about making predictions in the next iteration. This reduces our models being affected by misleading evidence and obtains better performance by retaining higher uncertainty to allow the model to have more room to optimize. In order to effectively combine labeled and unlabeled data we adjust the weights of different data: where μ l + μ u = 1, μ l , μ u ∈ [0, 1], are two weight factors. Model. In terms of the feature extractor, we use the latest EfficientNetV2, which, in Feng et al.  In order to avoid overfitting, we used the minimum model in this network and added Dropout to the output end. At the same time, in order to fill the differences between multi-modality data and model input, we have added the fully connected (FC) layer and convolutional layer (Conv) to adaptive adjust input channels. We employed three evidential FC layers proposed by Amini et al. ",vol1
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,3.0,Experiments and Results,"Data Description. In this paper, we assess the effectiveness of our multimodal semi-supervised evidential recycle framework on the ADNI-2 dataset 1 , which comprises multi-center data consisting of various modalities, including imaging and multiple phenotype data. Specifically, the dataset consists of four categories: normal control (NC), early mild cognitive impairment (EMCI), late mild cognitive impairment (LMCI), and Alzheimer's disease (AD). To ensure the effectiveness of our training and balance the number of categories, we used a sample of 515 patients, utilizing their MRI, PET, demographics, and APOE as inputs. On MRI images, we used 3T T1-weighted and FLAIR MR images, and the preprocessing process used CAT12 and SPM tools. All MRI data were processed using standard pipeline, including anterior commissure (AC)-posterior commissure (PC) correction, intensity correction, and skull stripping. Affine registration is performed to linearly align each MRI to the Colin27 template and resample to 224 × 224 × 91 for subsequent processing. For PET images, we used the official pre-processed AV-45 PET image and resampled them in the same way as the MRIs. We chose to include APOE in our analysis, as it is a well-established genetic risk factor for developing AD. Evaluation. We evaluated our model from three aspects. First, for the sake of comparison, we followed the technical conventions of most similar studies and selected three comparison tasks: AD vs NC, LMCI vs NC, and EMCI vs LMCI. Second, we compared and demonstrated the results of our model under different numbers of ground truth labels to verify that its performance improves as the label data volume increases. Third, we conducted different ablation experiments, which shows in Fig.  AU and EU represent the training process using only the corresponding parts. DER uses our proposed complete training process but does not use our improved u a and u e estimations, instead continuing to use the estimation method u A and u E which proposed in the original DER paper  Implementation Details. The upper bound in performance is the result obtained when the model is trained with all the input data are labeled. In the current supervised learning algorithms, the performance of each algorithm on  each task is not consistent, so we selected three papers in supervised learning, each representing the SOTA performance of the three tasks  Results. We compared our model with the semi-supervised learning methods currently achieving the best performance on the ADNI-2 dataset, as well as other top models in the semi-supervised learning field. As shown in Table  Our ablation experiment results are shown in Fig.  In addition, we have plotted the error rate of our framework under different labeled data counts in Fig. ",vol1
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,4.0,Conclusions,"We proposed an evidential regression-based semi-supervised learning framework, using the characteristics of AU and EU to train classifiers and extractors, respectively. Our model achieves SOTA performance on the ADNI-2 dataset. And due to the characteristics of semi-supervised learning, our model has unique advantages in adding private data, fine-tuning downstream tasks, and avoiding overfitting, which makes our model have great potential in clinical applications.",vol1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,1.0,Introduction,"Modern microscopes allow the digitalization of conventional glass slides into gigapixel Whole-Slide Images (WSIs)  retrieval, but also introducing multiple challenges. On the one hand, annotating WSIs requires strong medical expertise, is expensive, time-consuming, and labels are usually provided at the slide or patient level. On the other hand, feeding modern neural networks with the entire gigapixel image is not a feasible approach, forcing to crop data into small patches and use them for training. This process is usually performed considering a single resolution/scale among those provided by the WSI image. Recently, Multi-Instance Learning (MIL) emerged to cope with these limitations. MIL approaches consider the image slide as a bag composed of many patches, called instances; afterwards, to provide a classification score for the entire bag, they weigh the instances through attention mechanisms and aggregate them into a single representation. It is noted that these approaches are intrinsically flat and disregard the pyramidal information provided by the WSI  To profit from the multi-resolution structure of WSI, we propose a pyramidal Graph Neural Network (GNN) framework combined with (self) Knowledge Distillation (KD), called DAS-MIL (Distilling Across Scales). A visual representation of the proposed approach is depicted in Fig. ",vol1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,2.0,Related Work,MIL Approaches for WSI Classification. We herein summarize the most recent approaches; we refer the reader to ,vol1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,,Single-Scale.,"A classical approach is represented by AB-MIL  Multi-Scale. Recently, different authors focused on multi-resolution approaches. DSMIL-LC  Knowledge Distillation. Distilling knowledge from a more extensive network (teacher ) to a smaller one (student) has been widely investigated in recent years ",vol1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,3.0,Method,"Our approach aims to promote the information flow through the different employed resolutions. While existing works  Feature Extraction. Our work exploits DINO, the self-supervised learning approach proposed in  Architecture. The representations yield by DINO provide a detailed description of the local patterns in each patch; however, they retain poor knowledge of the surrounding context. To grasp a global guess about the entire slide, we allow patches to exchange local information. We achieve it through a Pyramidal Graph Neural Network (PGNN) in which each node represents an individual WSI patch seen at different scales. Each node is connected to its neighbors (8-connectivity) in the euclidean space and between scales following the relation ""part of"" In general terms, such a module takes as input multi-scale patch-level representations X = [X 1 X 2 ], where X 1 ∈ R N1×F and X 2 ∈ R N2×F are respectively the representations of the lower and higher scale. The input undergoes two graph layers: while the former treats the two scales as independent subgraphs A 1 ∈ R N1×N1 and A 2 ∈ R N2×N2 , the latter process them jointly by considering the entire graph A (see Fig.  where H ≡ [H 1 H 2 ] stands for the output of the PGNN obtained by concatenating the two scales. These new contextualized patch representations are then fed to the attention-based MIL module proposed in  Aligning Scales with (Self ) Knowledge Distillation. We have hence obtained two distinct sets of predictions for the two resolutions: namely, a bag-level score (e.g., a tumor is either present or not) and a patch-level one (e.g., which instances contribute the most to the target class). However, as these learned metrics are inferred from different WSI zooms, a disagreement may emerge: indeed, we have observed (see Table  Formally, the first term seeks to align bag predictions from the two scales through (self) knowledge distillation  where KL stands for the Kullback-Leibler divergence and τ is a temperature that lets secondary information emerge from the teaching signal. The second aligning term regards the instance scores. It encourages the two resolutions to assign criticality scores in a consistent manner: intuitively, if a lowresolution patch has been considered critical, then the average score attributed to its children patches should be likewise high. We encourage such a constraint by minimizing the Euclidean distance between the low-resolution criticality grid map z 1 and its subsampled counterpart computed by the high-resolution branch: (2) In the equation above, GraphPooling identifies a pooling layer applied over the higher scale: to do so, it considers the relation ""part of"" between scales and then averages the child nodes, hence allowing the comparison at the instance level. Overall Objective. To sum up, the overall optimization problem is formulated as a mixture of two objectives: the one requiring higher conditional likelihood w.r.t. ground truth labels y and carried out through the Cross-Entropy loss L CE (•; y); the other one based on knowledge distillation: where λ is a hyperparameter weighting the tradeoff between the teaching signals provided by labels and the higher resolution, while β balances the contributions of the consistency regularization introduced in Eq. ( ",vol1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,4.0,Experiments,"WSIs Pre-processing. We remove background patches through an approach similar to the one presented in the CLAM framework  Optimization. We use Adam as optimizer, with a learning rate of 2 × 10 -4 and a cosine annealing scheduler (10 -5 decay w/o warm restart). We set τ = 1.5, β = 1, and λ = 1. The DINO feature extractor has been trained with two RTX5000 GPUs: differently, all subsequent experiments have been performed with a single RTX2080 GPU using Pytorch-Geometric  Camelyon16. ",vol1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,,TCGA Lung Dataset.,"It is available on the GDC Data Transfer Portal and comprises two subsets of cancer: Lung Adenocarcinoma (LUAD) and Lung Squamous Cell Carcinoma (LUSC), counting 541 and 513 WSIs, respectively. The aim is to classify LUAD vs LUSC; we follow the split proposed by DSMIL ",vol1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,4.1,Comparison with the State-of-the-art,Table ,vol1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,4.2,Model Analysis,"On the Impact of Knowledge Distillation. To assess its merits, we conducted several experiments varying the values of the corresponding balancing coefficients (see Table  Single-Scale vs Multi-Scale.  The Impact of the Feature Extractors and GNNs. Table  H 2 -MIL exploits a global pooling layer (IHPool) that fulfils only the spatial structure of patches: as a consequence, if non-tumor patches surround a tumor patch, its contribution to the final prediction is likely to be outweighed by the IHPool module of H 2 -MIL. Differently, our approach is not restricted in such a way, as it can dynamically route the information across the hierarchical structure (also based on the connections with the critical instance).",vol1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,5.0,Conclusion,"We proposed a novel way to exploit multiple resolutions in the domain of histological WSI. We conceived a novel graph-based architecture that learns spatial correlation at different WSI resolutions. Specifically, a GNN cascade architecture is used to extract context-aware and instance-level features considering the spatial relationship between scales. During the training process, this connection is further amplified by a distillation loss, asking for an agreement between the lower and higher scales. Extensive experiments show the effectiveness of the proposed distillation approach.",vol1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 24.,vol1
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",1.0,Introduction,"Over the past few years, Cryo Electron Microscopy (Cryo-EM) has made remarkable progress in biomolecule structure analysis, becoming a major structural analysis technique along with X-ray crystallography. Based on a brief history of the early period of the technique, researchers had proposed methods that reconstruct the static structures of protein molecules from a set of single particle Cryo-EM images, e.g., EMAN2  Recently, taking advantage of method capturing the nonstatic structures, researchers have tried to define a plausible conformational pathway (continuous change with 3D density map) from only single particle Cryo-EM images  In this study, we propose a deep AE with a trainable prior that is expressed by a Gaussian Mixture Model (GMM). We name the AE cryoTWIN Our main contributions are as follows: i) we propose cryoTWIN: a deep AE model with the beneficial property for computing the conformational pathway and ii) in our numerical experiments, we confirm that the pathway computed using cryoTWIN is sufficiently consistent with an existing one, that was manually determined by researchers. In Sect. 2, we describe representatives of methods introduced at the beginning of this section in detail. Furthermore, we explain the differences of cryoTWIN compared with RaDOGAGA and cryoDRGN, as our method is partly inspired by them. In Sect. 3, we give a detailed account of cryoTWIN with theoretical guarantees. Additionally, we introduce an algorithm to compute the conformational pathway. In Sect. 4, we present the results obtained in our numerical experiments using a ribosome dataset. Finally, in Sect. 5, we conclude this study and discuss our future work.",vol1
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",2.1,Existing Reconstruction Methods,The representative method from the static structure category is cryoSPARC ,vol1
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",2.2,RaDOGAGA Revisit,"RaDOGAGA  -log Q zi ) corresponds to the distortion (resp. rate). Here, x i , (i = 1, ..., n) is the i-th original data. Additionally, xzi = g φ (z i + ), z i = f θ (x i ), and each element of is uniformly sampled from [-T /2, T/2]. Moreover, β > 0 is a hyper-parameter, and Q zi is given by where U (z) is a rectangular window function: holds for all j (z j is the j-th element of z), and U (z) = 0 otherwise. Let δ 1 and δ 2 be infinitesimal vectors with arbitrary directions. Then, the optimally trained AE, whose decoder is g φ * , has the following isometric property at all z for any δ 1 and δ 2 : where xz = g φ * (z). Because of Eq. ( ",vol1
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",2.3,Differences Between cryoTWIN and Existing Methods,"To understand the difference between cryoTWIN and RaDOGAGA, let V (resp. I) denote a 3D density map (resp. the Cryo-EM image). For cryoTWIN, only I is required to make the latent space isometric to a space of V , whereas V (usually inaccessible) is required to make RaDOGAGA have the same isometricity. Differences between cry-oTWIN and cryoDRGN are i) the latent distribution of cryoTWIN is theoretically proportional to a distribution of V , whereas cryoDRGN does not hold such property and ii) cryoTWIN can fit the prior to the latent distribution, whereas cryoDRGN can not. ",vol1
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",3.0,Proposed Method,"We first describe our deep AE, cryoTWIN, with the theoretical guarantees before we explain how to compute the conformational pathway using cryoTWIN. Given a set of single particle Cryo-EM images I = {I i } n i=1 , the AE is trained via two steps: the first step is preprocessing and the second one is to train the AE under a rate distortion theory-based objective, such as RaDOGAGA (see Sect. In Sect. 3.3, we describe an algorithm to compute the conformational pathway.",vol1
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",3.1,cryoTWIN,"As shown in Fig.  , where z i is the latent variable of X i . The parameter w i is used to enforce the latent empirical distribution to follow P ψ . Secondly, using z i , Ri , and a 2D position ν = (s, t, 0) in X i , the decoder g φ returns Xzi (v) = g φ (z i + , v), v = Ri ν, where Xzi (v) means the predicted value at the 3D position v in a 3D Fourier volume and ∈ R d is a random noise vector, each of whose elements is uniformly sampled from [-T /2, T/2]. By computing Xzi (v), v = Ri ν for all 2D positions ν (i.e., for all pairs (s, t)), the predicted Fourier image Xzi is defined via the set { Xzi (v)} s,t . Training Objective: We introduce the -th ( ≥ 1) training with a mini-batch B = {X i } i , | B| = m, where X i = FI i , I i ∈ B ⊂ I. Firstly, partially inspired by  Secondly, we update θ and φ based on an objective inspired by rate-distortion theory, i.e., minimization of i) the distortion (reconstruction error), and ii) the rate. The first minimization problem is defined by min θ,φ Let us simplify the problem by min θ,φ , where is the Hadamard product, and the (s, t)-th The second minimization problem is defined by min θ -1 m m i=1 log Q zi , where Q zi is given by Eq. (  where β > 0 is a hyper-parameter whose appropriate value depends on I. For the solver, we use the RAdam optimizer  Prediction After Training: Let ψ * , θ * , and φ * be the trained parameters in the deep AE, where Let F -1 denote the Inverse FT (IFT). Given a latent variable z, the trained decoder g φ * predicts the corresponding 3D density map similar to cryoDRGN ",vol1
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",3.2,Analysis of CryoTWIN Theorem 1. If we assume i) n,"1 for I = {I i } n i=1 (a set of Cryo-EM images), then Eq. (  where V z denotes the true 3D density map with a latent variable z, and β > 0 is a hyper-parameter. The symbol Ṽzi means the predicted 3D density map during the training and it is defined by {g φ (z i + , v)} v and F -1 . Therefore, from Sect. 2.2, the Algorithm 1: Pseudocode for computing conformational pathway Input: Two means of the GMM P ψ * : μ * i as the start and μ * j as the end point, Hyper-parameters: ω > 1 (Larger ω returns less continuous pathway), K, M ∈ N Output: A sequence of reconstructed structures from Vμ * i to Vμ * j 1 Set μ * i and μ * j as to z i→j (0) and z i→j (K), respectively. 2 for k = 1, ..., K -1 do 3 Fix αi and αj to cos( πk 2K ) and (1 -cos( πk 2K )) ω respectively. Let α-i,j denote (C -2)-dimensional vector made by removing the i-th and j-th element from the C-dimensional vector (α1, ..., αC ) . Then, generate M samples for α-i,j under the constraint c =i,j αc = 1 -αiαj and αc ≥ 0. Let α (m) -i,j (m = 1, ..., M ) denote the m-th sample. For all m, define z (m) using αi, αj, α Cost function for z (m) (m=1,...,M ) in the latent space . 4 Using {z i→j (k)} K k=0 and the trained decoder g φ * , compute the following sequence; latent space of the trained cryoTWIN can be isometric with a space for the predicted 3D density map, i.e., ∀(z, z );  A brief derivation of the theorem is given in Appendix A. The assumption ii) of the theorem is realizable if we set a large integer as the number of components C in the GMM P ψ . This is empirically confirmed in Sect. 4. For assumption iii), as Eq. ( ",vol1
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",3.3,Computation for Conformational Pathway,"We compute the plausible conformational pathway that is defined as a sequence of 3D density maps. If we assume that an AE of cryoTWIN with large C is trained by the Cryo-EM images {I i } n i=1 , n 1, then because of Eq. (  Thus, an output of Algorithm 1 can be interpreted as a sequence of 3D density maps, which is generated directly in the space of the 3D density map under Max-Flux objective. We usually cannot access the space of the 3D density map.",vol1
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",4.0,Numerical Experiment,"Using common single particle Cryo-EM images of ribosomes Expt1: Setting): Firstly, we evaluate the isometricity of cryoTWIN by the correlation coefficient between dz 1 • dz 2 and d V1 • d V2 as described by  Expt2: Setting): Firstly, as the preliminary experiment of Expt2, using structural labels of ",vol1
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",,Time and Memory Complexities:,"The training time of cryoTWIN (resp. cryoDRGN) is eleven hours (resp. four hours), whereas their memory complexities are comparable. After training cryoTWIN, the running time to compute the pathways including the evaluation time is around one hour, which is much shorter than the running time of the state-of-the-art protocol of a few days ",vol1
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",5.0,Conclusion and Future Work,"We propose cryoTWIN for computing plausible pathways from Cryo-EM images, and the efficiency is demonstrated in our numerical experiments. For further research, it would be better to estimate the orientation of the image simultaneously in the training of cryoTWIN, since the preliminary estimation gives an bias to the predicted structure, as explained in ",vol1
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,1.0,Introduction,"The development of deep neural networks has greatly promoted medical imagingbased computer-aided diagnosis. Due to the large amount of learnable parameters in neural networks, sufficient annotated training samples are required for training. However, the labeling process of medical images is tedious and timeconsuming. To address this problem, the common paradigm of transfer learning, which first pre-trains a model on upstream image datasets and then fine-tunes it on various target tasks, has been widely investigated in recent years  Previous works mainly focused on the fine-tuning strategy to effectively adapt the knowledge from the pre-trained models to target tasks  Considering the issues mentioned above, this work focused on source-free pre-trained model selection for segmentation tasks in the medical image. As shown in Fig.  In our work, we propose a new method using class consistency and feature variety(CC-FV) with an efficient framework to estimate the transferability in medical image segmentation tasks. Class consistency employs the distribution of features extracted from foreground voxels of the same category in each sample to model and calculate their distance, the smaller the distance the better the result; feature diversity utilizes features sampled in the whole global feature map, and the uniformity of the feature distribution obtained by sampling is used to measure the effectiveness of the features themselves. Extensive experiments have proved the superiority of our method compared with baseline methods. 2 Methodology",vol1
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,2.1,Problem Formulation,"In our work, a model bank M consisting of pre-trained models {M i } K i=1 are available to be fine-tuned and evaluated with a target dataset , where X j is the image and Y j is the ground truth of segmentation. After fine-tuning, the performance of M i can be measured with the segmentation metric (e.g. Dice score), which is denoted by P i s→t in this paper. Our work is to directly estimate the transferability score T i s→t without fine-tuning the model on target datasets. A perfect transferability score should preserve the ordering, i.e. T i s→t > T j s→t if and only if P i s→t > P j s→t .",vol1
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,2.2,Class Consistency with Feature Variety Constraint TE Method,"The transferability of models from a weakly related source domain to a target domain can be compromised if the domains are not sufficiently comparable  Class Consistency. The pre-trained models are trained with specific pretext tasks based on the upstream dataset. Therefore, features extracted by the pretrained models cannot perfectly distinguish the foreground and background of target data. If the features are generalizable, foreground region features will likely follow a similar distribution even without fine-tuning. Given a pair of target data X j and X j , the distribution of the features is modeled with the n-dimensional Gaussian distribution. Since the size of the foreground class varies across the cases, we therefore randomly sample the pixels/voxels of X j and X j for each class and establish the feature distribution F k j , F k j based on the voxels of the k th class to approximate the case-wise distribution of different classes. The class consistency between the data pair is measured by the Wasserstein distance  where are covariance matrices of F k j and F k j . Compared to some commonly used metrics like KL-divergence or Bhattacharyya distance  Given that 3D medical images are computationally intensive, and prone to causing out-of-memory problems, in the sliding window inference process for each case, we do not concatenate the output of each patch into the final prediction result, but directly sample from the patched output and concatenate them into the final sampled feature matrix. In the calculation of class consistency, we only sample the foreground voxels with a pre-defined sampling number which is proportional to the voxel number of each class in the image because of the severe class imbalance problem. Feature Variety. Class consistency is not the only criterion for transferability estimation. As a result of learning some trivial solutions, some overfitted models have limited generalization capacity and are difficult to apply to new tasks. We believe that the essential reason for this phenomenon is that class consistency is only concerned with local homogeneity of information while neglecting the integral feature quality assessment. Hence we propose the feature variety constraint, which measures the expressiveness of the features themselves and the uniformity of their probability distribution. Highly complex features are not easily overfitted in the downstream tasks and do not collapse to cause a trivial solution. To calculate the variety of features we need to analytically measure the properties of the feature distribution over the full feature space. Besides, to prevent overfitting and trivial features, we expect the distribution of features in the feature space to be as uniform and dispersed as possible. Therefore we employ the following hyperspherical potential energy E s as Here v is sampled feature of each image with point-wise embedding v i and L is the length of the feature, which is also the number of sampled voxels. We randomly sample from the whole case so that the features can better express the overall representational power of the model. The feature vectors will be more widely dispersed in the unit sphere if the hyperspherical energy (HSE) is lower  Overall Estimation. As for semantic segmentation problems, the feature pyramid structure is critical for segmentation results  The final transferability of pre-trained model m to dataset t T m→t is where D is the number of decoder layers used in the estimation.",vol1
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,3.1,Experiment on MSD Dataset,The Medical Segmentation Decathlon (MSD)  Table ,vol1
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,3.2,Ablation Study,In Table ,vol1
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,4.0,Conclusion,"In our work, we raise the problem of model selection for upstream and downstream transfer processes in the medical image segmentation task and analyze the practical implications of this problem. In addition, due to the ethical and privacy issues inherent in medical care and the computational load of 3D image segmentation tasks, we design a generic framework for the task and propose a transferability estimation method based on class consistency with feature variety constraint, which outperforms existing model transferability estimation methods as demonstrated by extensive experiments.",vol1
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_64.,vol1
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,1.0,Introduction,"Alzheimer's disease (AD) is a progressive neurodegenerative disease, which affects the quality of life as it causes memory loss, difficulty in thinking and learning  The cognitive scores prediction with longitudinal brain networks via deep learning models faces many challenges as follows: (i) The available longitudinal brain networks are scarce due to few volunteers or subject dropout  To cope with the above challenges, we propose a self-supervised multi-task learning paradigm for AD progression modeling with longitudinal brain networks. The proposed paradigm consists of a self-supervised spatio-temporal representation learning module for exploiting the spatio-temporal characteristics of longitudinal brain networks and a temporal multi-task module for modeling the relationship among cognitive scores prediction tasks at multiple time points. In summary, our contributions are threefold: 1) To the best of our knowledge, our work is the first attempt to predict cognitive scores with longitudinal brain networks through a self-supervised multi-task paradigm. 2) We design a self-supervised spatio-temporal representation learning module (SSTR), involving masked graph auto-encoder and temporal contrastive learning are jointly pre-trained to capture the structural and evolutional features of longitudinal brain networks simultaneously. The SSTR module can lead to more robust high-level representations for longitudinal brain networks. 3) We assume that inherent correlations exist among the prediction tasks at multiple future time points. Consequently, we propose a temporal multi-task learning paradigm to assist multiple time points cognitive scores prediction, which enhances the model generalization by exploiting the commonalities and differences among different prediction tasks when limited data is available. ",vol1
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,2.1,Problem Formalization,"The input to the proposed model is a set of N subjects, each of which has T longitudinal brain networks. Let  ] and Y T +k,p i is the p-th cognitive score of subject i at time T + k. As illustrated in Fig. ",vol1
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,2.2,Overview,The overview of the proposed SMP-Net is shown in Fig. ,vol1
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,2.3,Self-supervised Spatio-Temporal Representation Learning,"Although brain networks provide rich structure information, the pure supervised learning scheme limits the representation capacity of the models due to insufficient supervision. To solve this problem, we introduce the self-supervised spatio-temporal representation learning module, SSTR. The procedure of SSTR involves two stages as follow: ",vol1
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,,Stage 1 Masked Graph Auto-encoder for Graph Reconstruction.,"In stage 1, a masked graph auto-encoder, containing a topology-aware encoder and a decoder, is designed to exploit the crucial structural information in brain networks. To sufficiently exploit the graph structure, we randomly mask some nodes and the associated edges. The unmasked nodes and edges fed into the topologyaware encode to learn the latent representations. Let H u indicate the feature map in the encoding stage. We define the adjacent matrix of the unmasked nodes as A u , which is taken as the input of the topology-aware encoder, that is The topology-aware encoder consists of three parts: 1) The edge convolution with multiple cross-shaped filters for capturing the locality in the graph according to where w r ∈ R 1×M and w c ∈ R M ×1 are convolution kernels. 2) The node convolution for learning the latent node embedding. It is defined as: n , where w n is the learned filter vector, n ∈ R M ×Dn is the latent unmasked node embedding and D n is the channels in NC. 3) The graph aggregation for achieving the global graph embedding through: where w g is the learned filter vector, H g ∈ R M ×Dg is the graph embedding and D g is the dimensionality in GA. The decoder takes the masked nodes and the latent unmasked node embeddings as inputs, and then produces predictions for the masked nodes and edges by graph convolution operations and the masked edge prediction. The graph convolution is defined as: (l) , where A ∈ R M ×M is the binary adjacency matrix, W denotes trainable weight, H ∈ R M ×D n is the node embedding and D n is the hidden layer size of graph convolutional layers. The masked edge prediction is defined as: Â = H (l+1) (H (l+1) ) T . The reconstruction loss between the prediction graphs and corresponding targets is , where Ât i is the reconstructed brain networks of subject i at time t. Stage 2 Temporal Contrastive Learning. The longitudinal brain networks of a subject acquired at multiple visits characterize gradual disease progression of the brain over time, which manifests a temporal progression trajectory when projected to the latent space. We assume that brain networks features at two consecutive time points from the same subject are similar, while dissimilar from different subjects. Based on this assumption, we introduce a temporal contrastive loss by enforcing an across-sample relationship in the learning process. Specifically, H t g(i) is the brain network features of subject i at time t, H t g(i) and H t+1 g(j) are considered as the positive sample pair if i = j, otherwise they are considered as the negative sample pair. The temporal contrastive framework aims to enlarge the similarity between positive sample pair, and reduce it between the negative sample pair. The similarity calculation function s can be any distance function, and here we utilize cosine similarity. The loss for temporal contrastive learning can be represented as: where τ is a temperature factor that controls the model's discrimination against negative sample pair and exp(.) is an exponential function.",vol1
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,2.4,Temporal Multi-task Learning,"Existing studies have demonstrated the effectiveness of multi-task learning for the extraction of a robust feature representation  where W 1 , W 2 , b 1 , b 2 are learnable parameters of LSTM. Errors between the actual observations Y t and predictions Ŷ t are used to update the model parameters through the regression loss as follow: The overall loss function L is described as Eq. (  3 Experiments",vol1
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,3.1,Dataset and Experimental Settings,"In this work, we choose 219 longitudinal resting-state fMRI scans of 73 subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset ",vol1
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,3.2,Effectiveness Evaluation,We compare the performance of our SMP-Net with three state-of-the-art (SOTA) sequential graph learning methods: evolveGCN ,vol1
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,3.3,Discussion,"Ablation Analysis. To valid the effect of each proposed module, we consider the following variants for evaluation: 1) SMP-Net-c: the temporal contrastive loss is removed; 2) SMP-Net-r: the reconstruction loss is removed; 3) SMP-Netrc: both temporal contrastive loss and graph reconstruction loss are removed; 4) SMP-Net-m: the temporal multi-task paradigm is ignored.  indicating the effectiveness of the temporal multi-task paradigm. It also indicates that the multi-task paradigm in SMP-Net is more helpful for the prediction at farther time points. The reason is that prediction tasks at farther time points are more difficult due to the insignificant relationship between the brain networks and the cognitive scores. Temporal multi-task paradigm enforces the long-term prediction to benefit from short-term prediction, making the prediction tasks at farther time points gain more improvements. Moreover, we can observe that models with SSTR perform better than the ones without SSTR. For instance, SMP-Net-m and SMP-Net show superior performance than SMP-Net-r, SMP-Net-c and SMP-Net-rc. This demonstrates that SSTR facilitates the learning of structural and evolutional features in the condition of limited samples and insufficient supervision, thereby leading to more robust high-level representations for downstream tasks. Evaluating Robustness. To evaluate the robustness of the SSTR module, we pre-train SMP-Net with fMRI at three time points (M0, M6, M12) and fine-tune it with different downstream tasks of predicting cognitive scores at different time points. As shown in Fig. ",vol1
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,4.0,Conclusion,"This paper proposes an AD progression model SMP-Net from multi-task and self-supervised learning perspective with longitudinal brain networks. In the proposed SMP-Net, self-supervised spatio-temporal representation learning is designed to learn more robust structural and evolutional features from longitudinal brain networks. The temporal multi-task paradigm is designed for boosting the ability of cognitive score prediction at multiple time points. Experimental results on the ADNI dataset with fewer samples demonstrate the advantage of self-supervised spatio-temporal representation learning and temporal multi-task learning.",vol1
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 30.,vol1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,1.0,Introduction,"Medical image segmentation often relies on supervised model training  Secondly, the resulting models may not generalize well to unseen data domains. Even small changes in the task may result in a significant drop in performance, requiring re-training from scratch  Self-supervised learning (SSL) is a promising solution to these limitations. SSL pre-trains a model backbone to extract informative representations from unlabeled data. Then, a simple linear or non-linear head on top of the frozen pre-trained backbone can be trained for various downstream tasks in a supervised manner (linear or non-linear probing). Alternatively, the backbone can be finetuned for a downstream task along with the head. Pre-training the backbone in a self-supervised manner enables scaling to larger datasets across multiple data and task domains. In medical imaging, this is particularly useful given the growing number of available datasets. In this work, we focus on contrastive learning  Several works have implemented contrastive learning of dense representations in medical imaging  The common weakness of all the above works is that they do not evaluate their SSL models in linear or non-linear probing setups, even though these setups are de-facto standards for evaluation of SSL methods in natural images  Our contributions are threefold. First, we propose vox2vec, a framework for contrastive learning of voxel-level representations. Our simple negative sampling strategy and the idea of storing voxel-level representations in a feature pyramid form result in high-dimensional, fine-grained, multi-scale representations suitable for the segmentation of different organs and tumors in full resolution. Second, we employ vox2vec to pre-train a FPN architecture on a diverse collection of six unannotated datasets, totaling over 6,500 CT images of the thorax and abdomen. We make the pre-trained model publicly available to simplify the reproduction of our results and to encourage practitioners to utilize this model as a starting point for the segmentation algorithms training. Finally, we compare the pretrained model with the baselines on 22 segmentation tasks on seven CT datasets in three setups: linear probing, non-linear probing, and fine-tuning. We show that vox2vec performs slightly better than SotA models in the fine-tuning setup and outperforms them by a huge margin in the linear and non-linear probing setups. To the best of our knowledge, this is the first successful attempt to evaluate dense SSL methods in the medical imaging domain in linear and non-linear probing regimes.",vol1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,2.0,Related Work,"In recent years, self-supervised learning in computer vision has evolved from simple pretext tasks like Jigsaw Puzzles  Several methods produce dense or pixel-wise vector representations  The methods initially proposed for natural images are often used to pretrain models on medical images. In  Several methods allows to obtain voxel-wise features. The model ",vol1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,3.0,Method,"In a nutshell, vox2vec pre-trains a neural network to produce similar representations for the same voxel placed in different contexts (positive pairs) and predict distinctive representations for different voxels (negative pairs). In the following Sects. 3.1, 3.2, 3.3, we describe in detail the main components of our method: 1) definition and sampling of positive and negative pairs of voxels; 2) modeling voxel-level representations via a neural network; 3) computation of the contrastive loss. The whole pre-training pipeline is schematically illustrated in Fig. ",vol1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,3.1,Sampling of Positive and Negative Pairs,"We define a positive pair as any pair of voxels that correspond to the same location in a given volume. Conversely, we call a negative pair any pair of voxels that correspond to different locations in the same volume as well as voxels belonging to different volumes. Figure  In our experiments we set (H, W, D) = (128, 128, 32), n = 10 and m = 1000. We exclude the background voxels from the sampling and do not penalize their representations. We obtain the background voxels by using a simple twostep algorithm: 1) thresholding voxels with an intensity less than -500 HU; 2) keep voxels from the same connected component as the corner voxel of the CT volume, using a flood fill algorithm.",vol1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,3.2,Architecture,"A standard architecture for voxel-wise prediction is 3D UNet  To address this issue, we utilize a 3D FPN architecture instead of a standard 3D UNet. FPN returns voxel-level representations in the form of a feature pyramid. The pyramid's base is a feature map with 16 channels of the same resolution as the input patch. Each next pyramid level has twice as many channels and two times lower resolution than the previous one. Each voxel's representation is a concatenation of the corresponding feature vectors from all the pyramid levels. We use FPN with six pyramid levels, which results in 1008-dimensional representations. See Fig. ",vol1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,3.3,Loss Function,"At each pre-training iteration, we fed 2 • n patches to the FPN and obtain the representations for N positive pairs of voxels. We denote the representations in i-th positive pair as h (1) i and h  i ) , i = 1, . . . , N. Similar to  where",vol1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,3.4,Evaluation Protocol,"We evaluate the quality of self-supervised voxel-level representations on downstream segmentation tasks in three setups: 1) linear probing, 2) non-linear probing, and 3) end-to-end fine-tuning. Linear or non-linear probing means training a voxel-wise linear or non-linear classifier on top of the frozen representations. If the representations are modeled by the UNet model, such classifier can be implemented as one or several 1 × 1 convolutional layers with a kernel size 1 on top of the output feature map. A linear voxel-wise head (linear FPN head) can be implemented as follows. Each pyramid level is separately fed to its own convolutional layer with kernel size 1. Then, as the number of channels on all pyramid levels has decreased, they can be upsampled to the full resolution and summed up. This operation is equivalent to applying a linear classifier to FPN voxel-wise representations described in Sect. 3.2. Linear FPN head has four orders of magnitude fewer parameters than FPN. The architecture of the non-linear voxel-wise head replicates the UNet's decoder but sets the kernel size of all convolutions to 1. It has 50 times fewer parameters than the entire FPN architecture. In the end-to-end fine-tuning setup, we attach the voxel-wise non-linear head, but in contrast to the non-linear probing regime, we also train the backbone.",vol1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,4.1,Pre-training,"We use vox2vec to pre-train both FPN and UNet models (further vox2vec-FPN and vox2vec-UNet) in order to ablate the effect of using a feature pyramid instead of single full-resolution feature map for modeling voxel-wise representations. For pre-training, we use 6 public CT datasets ",vol1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,4.2,Evaluation,"We evaluate our method on the Beyond the Cranial Vault Abdomen (BTCV)  For our method, the pre-processing steps are the same for all datasets, as at the pre-training stage, but in addition, intensities are clipped to (-1350, 1000) HU window and rescaled to (0, 1). We compare our results with the current state-of-the-art self-supervised methods  We train all models for 45000 batches of size 7 (batch size for SwinUNETR is set to 3 due to memory constraints), using the Adam optimizer with a learning rate of 0.0003. In the fine-tuning setup, we freeze the backbone for the first 15000 batches and then exponentially increase the learning rate for the backbone parameters from 0.00003 up to 0.0003 during 1200 batches.",vol1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,5.0,Results,"The mean value and standard deviation of Dice score across 5 folds on the BTCV dataset for all models in all evaluation setups are presented in Table  Nevertheless, vox2vec-FPN significantly outperforms other models in linear and non-linear regimes. On top of that, we observe that in non-linear probing regime, it performs (within the standard deviation) as well as the FPN trained from scratch while having x50 times fewer trainable parameters (see Fig.  We reproduce the key results on MSD challenge CT datasets, which contain tumor and organ segmentation tasks. Table ",vol1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,6.0,Conclusion,"In this work, we present vox2vec -a self-supervised framework for voxel-wise representation learning in medical imaging. Our method expands the contrastive learning setup to the feature pyramid architecture allowing to pre-train effective representations in full resolution. By pre-training a FPN backbone to extract informative representations from unlabeled data, our method scales to large datasets across multiple task domains. We pre-train a FPN architecture on more than 6500 CT images and test it on various segmentation tasks, including different organs and tumors segmentation in three setups: linear probing, nonlinear probing, and fine-tuning. Our model outperformed existing methods in all regimes. Moreover, vox2vec establishes a new state-of-the-art result on the linear and non-linear probing scenarios. Still, this work has a few limitations to consider. We plan to investigate further how the performance of vox2vec scales with the increasing size of the pre-training dataset and the pre-trained architecture size. Another interesting research direction is exploring the effectiveness of vox2vec with regard to domain adaptation to address the challenges of domain shift between different medical imaging datasets obtained from different sources. A particular interest is a lowshot scenario when only a few examples from the target domain are available.",vol1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_58.,vol1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,1.0,Introduction,"Deep learning has brought medical image segmentation into the era of datadriven approaches, and has made significant progress in this field  Domain adaptation (DA) has been proposed and investigated to combat distribution shift in medical image segmentation. Many researchers proposed using adversarial learning to tackle distribution shift problems  To tackle the aforementioned issues, we propose utilizing prompt learning to take full advantage of domain information. Prompt learning  In this paper, we introduce a domain prompt learning method (prompt-DA) to tackle distribution shift in multi-target domains. Different from the recent prompt learning methods, we generate domain-specific prompts in the encoding feature space instead of the image space. As a consequence, it can improve the quality of the domain prompts, more importantly, we can easily consolidate the prompt learning with the other DA methods, for instance, adversarial learning based DA. In addition, we propose a specially designed fusion module to reinforce the respective characteristics of the encoder features and domain-specific prompts, and thus generate domain-aware features. As a way to prove the prompt-DA is compatible with other DAs, a very simple adversarial learning module is jointly adopted in our method to further enhance the model's generalization ability (we denote this model as comb-DA). We evaluate our proposed method on two multi-domain datasets: 1). the infant brain MRI dataset for cross-age segmentation; 2). the BraTS2018 dataset for cross-grade tumor segmentation. Experiments show our proposed method outperforms state-of-the-art methods. Moreover, ablation study demonstrates the effectiveness of the proposed domain prompt learning and the feature fusion module. Our claim about the successful combination of prompt learning with adversarial learning is also well-supported by experiments.",vol1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,2.0,Methodology,"Our proposed prompt-DA network consists of three main components as depicted in Fig.  There are various encoder-decoder segmentation networks, many of which are well known. As a result, we donot introduce the details of the encoder-decoder and just choose two typical networks to work as the segmentation backbone, that is, 3D-UNet ",vol1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,2.1,Learning Domain-Specific Prompts,"In our designed prompt learning based DA method, it is essential to learn domain-specific prompts. Moreover, the quality of generated prompts directly determines the domain-aware features. Therefore, we specially designed a prompt generation module to learn domain-specific prompts which mainly consists of two components, i.e., a classifier and a prompt generator. Our approach incorporates domain-specific information into the prompts to guide the model in adapting to the target domain. To achieve this, we introduce a classifier h(x) that distinguishes the domain (denoted as d) of the input image, shown in Eq. 1. where x is the image or abstracted features from the encoder. To optimize the parameters, we adopt cross-entropy loss to train the classifier, as shown in Eq. 2. where d is the predicted domain information, and d is the ground truth domain information. Prompt Generation: Instead of directly using d as the category information, we fed the second-to-last layer's features (i.e., z) of the classifier to a prompt generation, namely, g(z). In particular, the g(z) is a multi-layer-perception, as defined in Eq. 3. where φ can be a Conv+BN+ReLU sequence. Note this module does not change the size of the feature map, instead, it transforms the extracted category features into domain-specific prompts.",vol1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,2.2,Learning Domain-Aware Representation by Fusion,"The learned prompt captures clearly about a certain domain and the features from the encoder describe the semantics as well as spatial information for the images. We can combine them to adapt the image features to domain-aware representations. Basically, suppose we have an image denoted as I, and the prompt encodings for the domain knowledge is g(e(I)) (where e(I) is the features from a shallow layer), E(I) is the encoder features for this image. Then the domain-aware features (i.e., F ) are extracted by a fusion module as Eq. 4. As the learned prompt and encoder feature capture quite different aspects of the input data, we cannot achieve good effect by simply using addition, multiplication or concatenation to serve as the fusion function ψ. Specifically, while the encoder feature emphasizes spatial information for image segmentation, the prompt feature highlights inter-channel information for domain-related characteristics. To account for these differences, we propose a simple attention-based fusion (denoted as AFusion) module to smoothly aggregate the information. This module computes channel-wise and spatial-wise weights separately to enhance both the channel and spatial characteristics of the input. Figure  Our module utilizes both channel and spatial branches to obtain weights for two input sources. The spatial branch compresses the encoder feature in the channel dimension using an FC layer to obtain spatial weights. Meanwhile, the channel branch uses global average pooling and two FC layers to compress the prompt and obtain channel weights. We utilize FC layers for compression and rescaling, denoted as f cp and f re respectively. The spatial and channel weights are computed according to Eq. 5. Afterward, we combine the weights from the spatial and channel dimensions to obtain a token that can learn both high-level and low-level features from both the encoder feature and the prompt, which guides the fusion of the two features. The process is illustrated as follows: This module introduces only a few parameters, yet it can effectively improve the quality of the prompted domain-aware features after feature fusion. In the experimental section (i.e., Sect. 3.3), we conducted relevant experiments to verify that this module can indeed improve the performance of our prompt-DA method.",vol1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,2.3,Adversarial Learning to Enhance the Generalization Ability,"As aforementioned, our proposed prompt-DA is fully compatible with other DA algorithms. We thus use adversarial learning, which is widely adopted in medical image DA, to work as an optional component in our network to continuously enhance the domain adaptation ability. Specially, inspired by the adversarial DA in ",vol1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,2.4,Total Loss,"To optimize the segmentation backbone network, we use a combined loss function, L seg , that incorporates both dice loss  By summing the above-introduced losses, the total loss to train the segmentation network can be defined by Eq. 7. where λ is the scaling factor to balance the losses.",vol1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,2.5,Implementation Details,We use basic 3D-UNet ,vol1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,3.1,Datasets,"Our proposed method was evaluated using two medical image segmentation DA datasets. The first dataset, i.e., cross-age infant segmentation  The first dataset is for infant brain segmentation (white matter, gray matter and cerebrospinal fluid). To build the cross-age dataset, we take advantage 10 brain MRIs of 6-month-old from iSeg2019  The 2nd dataset is for brain tumor segmentation (enhancing tumor, peritumoral edema and necrotic and non-enhancing tumor core), which has 285 MRI samples (210 HGG and 75 LGG). We take HGG as the source domain and LGG as the target domain. ",vol1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,3.2,Comparison with State-of-the-Art (SOTA) Method,"We compared our method with four SOTA methods: ADDA  For fair comparison, we have replaced the backbone of these models with the same we used in our approach. The quantitative comparison results of cross-age infant brain segmentation is presented in Table  When transferring to a single target domain in the brain tumor segmentation task, our proposed DA solution improves about 3.09 DICE in the target LGG domain. Also, the proposed method shows considerable improvements over ADDA and CyCADA, but very subtle improvements to the SIFA and ADR methods (although ADR shows a small advantage on the Whole category). We also visualize the segmentation results on a typical test sample of the infant brain dataset in Fig. ",vol1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,3.3,Ablation Study,"Prompt-DA vs. adv-DA: Since the performance reported in Table  The corresponding experiments are conducted on the infant brain dataset and experimental results are shown in Table  As observed in Table  Fusion Strategy for Learning Domain-Aware Features: One of the key components of the prompt-DA is to learn domain-aware features through fusion. We have evaluated the effectiveness of our proposed feature fusion strategy in both 3D and 2D models. For comparison, we considered several other fusion strategies, including 'add/mul', which adds or multiplies the encoder feature and prompt directly, 'conv', which employs a single convolutional layer to process the concatenated features, and 'rAFusion', which utilizes a reverse version of the AFusion module, sending the prompt to the spatial branch and the encoder feature to the channel branch. The results of these experiments are presented in Table ",vol1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,4.0,Conclusion,"In this paper, we propose a new DA paradigm, namely, prompt learning based DA. The proposed prompt-DA uses a classifier and a prompt generator to produce domain-specific information and then employs a fusion module (for encoder features and prompts) to learn domain-aware representation. We show the effectiveness of our proposed prompt-DA in transfer ability, and also we prove that the prompt-DA is smoothly compatible with the other DA algorithms. Experiments on two DA datasets with two different segmentation backbones demonstrate that our proposed method works well on DA problems.",vol1
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,1.0,Introduction,"A common challenge for deploying deep learning to clinical problems is the discrepancy between data distributions across different clinical sites  Few approaches  In a different direction, numerous UDA methods have shown high performance in various tasks ",vol1
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,2.0,Methods,"We denote the labeled source domain as X S = {(x s n , y s n )} N n=1 and the unlabeled K-shot target domain as ",vol1
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,2.1,Domain-Distance-Modulated Spectral Sensitivity (DoDiSS),The prior research ,vol1
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,Domain Distance Measurement.,"To overcome the limitations of lacking target domain images, we first augment the few-shot images from the target domain with random combinations of various geometric transformations, including random cropping, rotation, flipping, and JigSaw  , where W 1 is the 1-Wasserstein distance.",vol1
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,DoDiSS Computation.,"With the measured domain difference, we can now compute the DoDiSS map of a model. As shown in Fig.  To analyze the model's generalization weakness with respect to the frequency (i, j), we generate perturbed source domain images by adding the Fourier basis noise N i,j = r • D W (i, j) • U i,j to the original source domain image x s as x s + N i,j . D W (i, j) controls the 2 -norm of N i,j and r is randomly sampled to be either -1 or 1. The N i,j only introduces perturbations at the frequency components (i, j) to the original images. The D W (i, j) guarantees that images are perturbed across all frequency components following the real domain shift. For RGB images, we add N i,j to each channel independently following ",vol1
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,2.2,Sensitivity-Guided Spectral Adversarial Mixup (SAMix),"Using the DoDiSS map M S and an adversarially learned parameter λ * as a weighting factor, SAMix mixes the amplitude spectrum of each source image with the spectrum of a target image. DoDiSS indicates the spectral regions where the model is sensitive to the domain difference. The parameter λ * mines the heard-tolearn samples to efficiently enrich the target domain samples by maximizing the task loss. Further, by retaining the phase of the source image, SAMix preserves the semantic meaning of the original source image in the generated target-style sample. Specifically, as shown in Fig.  The target-style image is reconstructed by x st λ * = IFFT (A st λ * , Φ s ). The adversarially learned parameter λ * is optimized by maximizing the task loss L T using the projected gradient descent with T iterations and step size of δ: In the training phase, as shown in Fig.  where L t is the supervised task loss in the source domain; JS is the Jensen-Shannon divergence ",vol1
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,3.0,Experiments and Results,We evaluated SAMix on two medical image datasets. Fundus ,vol1
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,3.1,Implementation Details,SAMix is evaluated as a plug-in module for four UDA models: AdaptSeg ,vol1
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,3.2,Method Effectiveness,"We demonstrate the effectiveness of SAMix by comparing it with two sets of baselines. First, we compare the performance of UDA models with and without SAMix. Second, we compare SAMix against other FSUDA methods  Fundus. Table  To assess the functionality of the target-aware spectral sensitivity map in measuring the model's generalization performance on the target domain, we computed the DoDiSS maps of the four models (AdaptSeg, ASM, SM-PPM, and AdaptSeg+SAMix). The results are presented in Fig. ",vol1
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,3.3,Data Efficiency,"As the availability of target domain images is limited, data efficiency plays a crucial role in determining the data augmentation performance. Therefore, we evaluated the model's performance with varying numbers of target domain images in the training process.   ",vol1
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,3.4,Ablation Study,"To assess the efficacy of the components in SAMix, we conducted an ablation study with AdaptSeg+SAMix and DALN+SAMix (Full model) on Fundus and Camelyon datasets. This was done by 1) replacing our proposed DoDiSS map with the original one in ",vol1
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,4.0,Discussion and Conclusion,"This paper introduces a novel approach, Sensitivity-guided Spectral Adversarial MixUp (SAMix), which utilizes an adversarial mixing scheme and a spectral sensitivity map to generate target-style samples effectively. The proposed method facilitates the adaptation of existing UDA methods in the few-shot scenario. Thorough empirical analyses demonstrate the effectiveness and efficiency of SAMix as a plug-in module for various UDA methods across multiple tasks.",vol1
UOD: Universal One-Shot Detection of Anatomical Landmarks,1.0,Introduction,"Robust and accurate detecting of anatomical landmarks is an essential task in medical image applications  In the past years, lots of fully supervised methods  However, one-shot methods are not robust enough because they are dependent on the choice of labeled template and the accuracy of detected landmarks may decrease a lot when choosing a sub-optimal image to annotate. To address this issue, Quan et al.  Motivated by above challenges, to detect robust multi-domain label-efficient landmarks, we design domain-adaptive models and propose a universal oneshot landmark detection framework called Universal One-shot Detection (UOD), illustrated in Fig. ",vol1
UOD: Universal One-Shot Detection of Anatomical Landmarks,2.0,Method,As Fig. ,vol1
UOD: Universal One-Shot Detection of Anatomical Landmarks,2.1,Stage I: Contrastive Learning,As Fig. ,vol1
UOD: Universal One-Shot Detection of Anatomical Landmarks,2.2,Stage II: Supervised Learning,"In stage II, we design a universal transformer to capture global relationship of multi-domain data and train it with the pseudo landmarks generated in stage I. The universal transformer has a domain-adaptive transformer encoder and domain-adaptive convolution decoder. The decoder is based on a U-Net  As illustrated in Fig.  where Overall Pipeline. Given that a random input  2 ≤ σ and 0 otherwise. We further add an exponential weight to the Gaussian distribution to distinguish close heatmap pixels and obtain the ground truth heatmap Y d n (i, j) = α Ỹ d n (i,j) . As illustrated in Fig. ",vol1
UOD: Universal One-Shot Detection of Anatomical Landmarks,3.0,Experiment,"Datasets. For performance evaluation, we adopt three public X-ray datasets from different domains on various anatomical regions of head, hand, and chest. (i) Head dataset is a widely-used dataset for IEEE ISBI 2015 challenge  Implementation Details. UOD is implemented in Pytorch and trained on a TITAN RTX GPU with CUDA version being 11. All encoders are initialized with corresponding pre-trained weights. We set batch size to 8, σ to 3, and α to 10. We adopt binary cross-entropy (BCE) as loss function for both stages. In stage I, we resize each image to the same shape of 384 × 384 and train universal convolution model by Adam optimizer for 1000 epochs with a learning rate of 0.00001. In stage II, we resize each image to the same shape of 576 × 576 and optimize the universal transformer by Adam optimizer for 300 epochs with a learning rate of 0.0001. When calculating metrics, all predicted landmarks are resized back to the original size. For evaluation, we choose model with minimum validation loss as the inference model and adopt two metrics: mean radial error (MRE) 2 and successful detection rates (SDR) within different thresholds t:  ",vol1
UOD: Universal One-Shot Detection of Anatomical Landmarks,3.1,Experimental Results,"The Effectiveness of Universal Model: To demonstrate the effectiveness of universal model for multi-domain one-shot learning, we adopt head and hand datasets for evaluation. In stage I, the convolution models are trained in two ways: 1) single: trained on every single dataset respectively, and 2) universal: trained on mixed datasets together. With a fixed one-shot sample for the hand dataset, we change the one-shot sample for the head dataset and report the MRE and SDR of the head dataset. As Fig.  Comparisons with State-of-the-Art Methods: As Table ",vol1
UOD: Universal One-Shot Detection of Anatomical Landmarks,4.0,Conclusion,"To improve the robustness and reduce domain preference of multi-domain oneshot learning, we design a universal framework in that we first train a universal model via contrastive learning to generate pseudo landmarks and further use these labels to learn a universal transformer for accurate and robust detection of landmarks. UOD is the first universal framework of one-shot landmark detection on multi-domain data, which outperforms other one-shot methods on three public datasets from different anatomical regions. We believe UOD will significantly reduce the labeling burden and pave the path of developing more universal framework for multi-domain one-shot learning.",vol1
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,1.0,Introduction,"Medical image segmentation plays an essential role in computer-aided diagnosis systems in different applications and has been tremendously advanced in the past few years  Although impressive performance has been achieved, these UDA methods may be limited for some real-world medical image segmentation tasks where labeled source images are not available for adaptation. This is not a rare scenario because medical images are usually highly sensitive in privacy and copyright protection such that labeled source images may not be allowed to be distributed. This motivates the investigation of source-free domain adaptation (SFDA) where adapts a source segmentation model trained on labeled source data (in a privateprotected way) to the target domain only using unlabeled data. A few recent SFDA works have been proposed. OSUDA  Although these methods have achieved some success in model adaptation, they still suffer from two major issues. First, they tend to be fairly unstable. Without any supervision signal from labeled data, the model heavily relies on the predictions generated by itself, which are always noisy and could easily make the training process unstable, causing catastrophic error accumulation after several training epochs as shown in Fig.  In this paper, we propose the Class-Balanced Mean Teacher (CBMT) method to address the limitations of existing methods. To mitigate the negative impacts of incorrect pseudo labels, we propose a weak-strong augmented mean teacher learning scheme which involves a teacher model and a student model that are both initialized from the source model. We use the teacher to generate pseudo label from a weakly augmented image, and train the student that takes strongly augmented version of the same image as input. We do not train the teacher model directly by back-propagation but update its weights as the moving average of the student model. This prevents the teacher model from being abruptly impacted by incorrect pseudo labels and meanwhile accumulates new knowledge learned by the student model. To address the imbalance between foreground and background, we propose to calibrate the segmentation loss and highlight the foreground class, based on the prediction statistics derived from the global information. We maintain a prediction bank to capture global information, which is considered more reliable than that inside one image. Our contributions can be summarized as follows:  (3) Our proposed CBMT reaches state-of-the-art performance on two popular benchmarks for adaptive fundus image segmentation.",vol1
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,2.0,Method,"Source-Free Domain Adaptive (SFDA) fundus image segmentation aims to adapt a source model h, trained with N S labeled source images S = {(X i , Y i )} NS i=1 , to the target domain using only N T unlabeled target images T = {X i } NT i=1 . Y i ∈ {0, 1} H×W ×C is the ground truth, and H, W , and C denote the image height, width, and class number, respectively. A vanilla pseudo-labeling-based method generates pseudo labels ŷ ∈ R C from the sigmoided model prediction p = h(x) for each pixel x ∈ X i with source model h: where 1 is the indicator function and γ ∈ [0, 1] is the probability threshold for transferring soft probability to hard label. p k and y k is the k-th dimension of p and y, respectively, denoting the prediction and pseudo label for class k. Then (x, ŷ) is utilized to train the source model h with binary cross entropy loss: Most existing SFDA works refine this vanilla method by proposing techniques to calibrate p and get better pseudo label ŷ, or measure the uncertainty of p and apply a weight when using ŷ for computing the loss  Our proposed CBMT model addresses the two problems by proposing the weak-strong augmented mean teacher learning scheme and the global knowledgeguided loss calibration technique. Figure ",vol1
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,2.1,Weak-Strong Augmented Mean Teacher,"To avoid error accumulation and achieve a robust training process, we introduce the weak-strong augmented mean teacher learning scheme where there is a teacher model h t and a student model h s both initialized from the source model h. We generate pseudo labels with h t and use the pseudo labels to train h s . To enhance generalization performance, we further introduce a weak-strong augmentation mechanism that feeds weakly and strongly augmented images to the teacher model and the student model, respectively. Concretely, for each image X i , we generate a weakly-augmented version X w i by using image flipping and resizing. Meanwhile, we generate a stronglyaugmented version X s i . The strong augmentations we used include a random eraser, contrast adjustment, and impulse noises. For each pixel x w ∈ X w i , we generate pseudo label ŷw = h t (x) by the teacher model h t with Eq. (  where Lbce is the refined binary cross entropy loss which we will introduce later. It is based on Eq. (  We update the student model by back-propagating the loss defined in Eq.  where θ, θ are the teacher and student model weights separately. Instead of updating the model with gradient directly, we define the teacher model as the exponential moving average of students, which makes the teacher model more consistent along the adaptation process. With this, we could train a model for a relatively long process and safely choose the final model without accuracy validation. From another perspective, the teacher model can be interpreted as a temporal ensemble of students in different time steps ",vol1
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,2.2,Global Knowledge Guided Loss Calibration,"For a fundas image, the foreground object (e.g., cup) is usually quite small and most pixel will the background. If we update the student model with Eq. (  A naive way to address the foreground and background imbalance is to calculate the numbers of pixels falling into the two categories, respectively, within each individual image and devise a loss weighting function based on the numbers. This strategy may work well for the standard supervised learning tasks, where the labels are reliable. But with pseudo labels, it is too risky to conduct the statistical analysis based on a single image. To remedy this, we analyze the class imbalance across the whole dataset, and use this global knowledge to calibrate our loss for each individual image. Specifically, we store the predictions of pixels from all images and maintain the mean loss for foreground and background as, where L is the segmentation loss mentioned above, and ""fg"" and ""bg"" represent foreground/background. The reason we use the mean of the loss, rather than the number of pixels, is that the loss of each pixel indicates the ""hardness"" of each pixel according to the pseudo ground truth. This gives more weight to those more informative pixels, thus more global knowledge is considered. With each average loss, the corresponding learning scheme could be further calibrated. We utilize the ratio of η fg k to η bg k to weight background loss L bg k : The calibrated loss ensures fair learning among different classes, therefore alleviating model degradation issues caused by class imbalance. Since most predictions are usually highly confident (very close to 0 or 1), they are thus less informative. We need to only include pixels with relatively large loss scales to compute mean loss. We realize this by adopting constraint threshold α to select pixels: |f (xi)-γ| | ŷi-γ| > α, where α is set by default to 0.2. α represents the lower bound threshold of normalized prediction, which can filter well-segmented uninformative pixels out.",vol1
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,3.0,Experiments,"Implementation Details Datasets and Metrics. We evaluate our method on widely-used fundus optic disc and cup segmentation datasets from different clinical centers. Following previous works, We choose the REFUGE challenge training set  We compare our CBMT model with several state-of-the-art domain adaptation methods, including UDA methods BEAL ",vol1
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,3.1,Experimental Results,The quantitative evaluation results are shown in Table ,vol1
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,3.2,Further Analyses,"Ablation Study. In order to assess the contribution of each component to the final performance, we conducted an ablation study on the main modules of CBMT, as summarized in Table  Hyper-parameter Sensitivity Analysis. We further investigate the impact of different hyper-parameter. Figure  To evaluate the variation of the loss calibration weight η fg k /η bg k with different constraint thresholds α, we present the results in Table ",vol1
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,4.0,Conclusion,"In this work, we propose a class-balanced mean teacher framework to realize robust SFDA learning for more realistic clinical application. Based on the observation that model suffers from degradation issues during adaptation training, we introduce a mean teacher strategy to update the model via an exponential moving average way, which alleviates error accumulation. Meanwhile, by investigating the foreground and background imbalance problem, we present a global knowledge guided loss calibration module. Experiments on two fundus image segmentation datasets show that CBMT outperforms previous SFDA methods.",vol1
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,1.0,Introduction,"In medical image analysis, the availability of a substantial quantity of accurately annotated 3D data is a prerequisite for achieving high performance in tasks like segmentation and detection  Generating realistic synthetic data presents a promising solution to the above challenges as it eliminates the need for manual annotation and alleviates privacy risks. However, most prior studies  However, there has been no prior research on generating whole 3D volumetric images with the corresponding segmentation masks. Generating 3D volumetric images with corresponding segmentation masks faces two major obstacles. First, directly feeding entire 3D volumes to neural networks is impractical due to GPU memory constraints, and downsizing the resolution may compromise the quality of the synthetic data. Second, treating the entire 3D volume as a single data point during training is suboptimal because of the limited availability of annotated 3D data. Thus, innovative methods are required to overcome these challenges and generate high-quality synthetic 3D volumetric data with corresponding segmentation masks. We propose MedGen3D, a novel diffusion-based deep generative framework that generates paired 3D volumetric medical images and multi-label masks. Our approach treats 3D medical data as sequences of slices and employs an autoregressive process to sequentially generate 3D masks and images. In the first stage, a Multi-Condition Diffusion Probabilistic Model (MC-DPM) generates mask sequences by combining conditional and unconditional generation processes. Specifically, the MC-DPM generates mask subsequences (i.e., several consecutive slices) at any position directly from random noise or by conditioning on existing slices to generate subsequences forward or backward. Given that medical images have similar anatomical structures, slice indices serve as additional conditions to aid the mask subsequence generation. In the second stage, we introduce a conditional image generator with a seq-to-seq model from  The main contributions of our work are as follows: 1) Our proposed framework is the first to address the challenge of synthesizing complete 3D volumetric medical images with their corresponding masks; 2) we introduce a multicondition diffusion probabilistic model for generating 3D anatomical masks with high fidelity and diversity; 3) we leverage the generated masks to condition an image sequence generator and a semantic diffusion refiner, which produces realistic medical images that align accurately with the generated masks; and 4) we present experimental results that demonstrate the fidelity and diversity of the generated 3D multi-label medical images, highlighting their potential benefits for downstream segmentation tasks.",vol1
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,2.1,Diffusion Probabilistic Model,A diffusion probabilistic model (DPM)  where θ is predicted noise and θ is the model parameters.,vol1
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,2.2,Classifier-Free Guidance,Samples from conditional diffusion models can be improved with classifier-free guidance  where ∅ represents a null condition and s ≥ 1 is the guidance scale.,vol1
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,3.0,Methodology,"We propose a sequential process to generate complex 3D volumetric images with masks, as illustrated in Fig. ",vol1
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,3.1,3D Mask Generator,"Due to the limited annotated real data and GPU memory constraints, directly feeding the entire 3D volume to the network is impractical. Instead, we treat 3D  medical data as a series of subsequences. To generate an entire mask sequence, an initial subsequence of m consecutive slices is unconditionally generated from random noise. Then the subsequence is expanded forward and backward in an autoregressive manner, conditioned on existing slices. Inspired by classifier-free guidance in Sect. 2.2, we propose a general Multi-Condition Diffusion Probabilistic Model (MC-DPM) to unify all three conditional generations (unconditional, forward, and backward). As shown in Fig.  Furthermore, as 3D medical data typically have similar anatomical structures, slices with the same relative position roughly correspond to the same anatomical regions. Therefore, we can utilize the relative position of slices as conditions to guide the MC-DPM in generating subsequences of the target region and control the length of generated sequences. Train: For a given 3D multi-label mask M ∈ R D×H×W , subsequneces of m consecutive slices are selected as {M z , M z+1 , . . . , M z+(m-1) }, with z as the randomly selected starting indices. For each subsequence, we determine the conditional slices X C ∈ {R n×H×W , ∅} by selecting either the first or the last n slices, or no slice, based on a probability p C ∈ {p F orward , p Backward , p U ncondition }. The objective of the MC-DPM is to generate the remaining slices, denoted as To incorporate the position condition, we utilize the relative position of the subsequence z = z/D, where z is the index of the subsequence's starting slice. Then we embed the position condition and concatenate it with the time embedding to aid the generation process. We also utilize a binary indicator for each slice in the subsequence to signify the existence of conditional slices. The joint distribution of reverse diffusion process (RDP) with the conditional slices X C can be written as: where p(X P T ) = N X P T ; 0, I , z = z/D and p θ is the distribution parameterized by the model. Overall, the model will be trained by minimizing the following loss function, with Inference: During inference, MC-DPM first generates a subsequence of m slices from random noise given a random location z. The entire mask sequence can then be generated autoregressively by expanding in both directions, conditioned on the existing slices, as shown in Fig. ",vol1
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,3.2,Conditional Image Generator,"In the second step, we employ a sequence-to-sequence method to generate medical images conditioned on masks, as shown in Fig. ",vol1
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Image Sequence Generator:,"In the sequence-to-sequence generation task, new slice is the combination of the warped previous slice and newly generated texture, weighted by a continuous mask  Semantic Diffusion Refiner: Despite the high cross-slice consistency and spatial continuity achieved by vid2vid, issues such as blocking, blurriness and suboptimal texture generation persist. Given that diffusion models have been shown to generate superior images  For each of the 3 different views, we train a semantic diffusion model (SDM), which takes 2D masks and noisy images as inputs to generate images aligned with input masks. During inference, we only apply small noising steps (10 steps) to the generated images so that the overall anatomical structure and spatial continuity are preserved. After that, we refine the images using the pre-trained semantic diffusion model. The final refined 3D images are the mean results from 3 views. Experimental results show an evident improvement in the quality of generated images with the help of semantic diffusion refiner.",vol1
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Datasets:,"We conducted experiments on the thoracic site using three thoracic CT datasets and the brain site with two brain MRI datasets. For both generative models and downstream segmentation tasks, we utilized the following datasets: -SegTHOR  Implementation: For thoracic datasets, we crop and pad CT scans to (96 × 320 × 320). The annotations of six organs (left lung, right lung, spinal cord, esophagus, heart, and trachea) are examined by an experienced radiation oncologist. We also include a body mask to aid in the image generation of body regions. For brain MRI datasets, we use Freesurfer ",vol1
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Setup:,We compare the synthetic image quality with DDPM  According to Table ,vol1
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,4.3,Evaluate the Benefits for Segmentation Task,"We explore the benefits of synthetic data for downstream segmentation tasks by comparing Sørensen-Dice coefficient (DSC) of 4 segmentation models, including Unet2D  According to Table ",vol1
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,5.0,Conclusion,"This paper introduces MedGen3D, a new framework for synthesizing 3D medical mask-image pairs. Our experiments demonstrate its potential in realistic data generation and downstream segmentation tasks with limited annotated data. Future work includes merging the image sequence generator and semantic diffusion refiner for end-to-end training and extending the framework to synthesize 3D medical images across modalities. Overall, we believe that our work opens up new possibilities for generating 3D high-quality medical images paired with masks, and look forward to future developments in this field.",vol1
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 72.,vol1
PET Image Denoising with Score-Based Diffusion Probabilistic Models,1.0,Introduction,"Positron emission tomography (PET) is an imaging modality in nuclear medicine that has been successfully applied in oncology, neurology, and cardiology. By injecting a radioactive tracer into the human body, the molecular-level activity in tissues can be observed. To mitigate the radiation risk to the human body, it is essential to reduce the dose or shorten the scan time, leading to a low signalto-noise ratio and further negatively influencing the accuracy of diagnosis. Recently, the denoising diffusion probabilistic model (DDPM)  In this paper, we propose a conditional diffusion probabilistic model for lowcount PET image denoising in an unsupervised manner without the Gaussian noise assumption or paired datasets. Our model is divided into two stages. In the training stage, we leverage the standard DDPM to train the score function network to learn a prior distribution of PET images. Once the network is trained, we transplant it into the sampling stage, in which we design two conditions to control the generation of high-count PET images given corresponding lowcount PET images. One condition is that the denoised versions of low-count PET images are similar to high-count PET images. The other condition is that when we add noise to high-count PET images, they degrade to low-count PET images. As a result, our model is named the bidirectional condition diffusion probabilistic model (BC-DPM). In particular, to simulate the formation of PET noise, we add noise in the sinogram domain. Additionally, the two proposed conditions are implemented in latent space. Notably, Our model is 'one for all', that is, once we have trained the score network, we can utilize this model for PET images with different count levels.",vol1
PET Image Denoising with Score-Based Diffusion Probabilistic Models,2.0,Method,"Letting X ⊂ X be a high-count PET image dataset and Y ⊂ Y be a low-count PET image dataset, x 0 and y 0 denote instances in X and Y , respectively. Our goal is to estimate a mapping F(Y) = X , and the proposed BC-DPM provides an unsupervised technique to solve this problem. BC-DPM includes two stages. In the training stage, it requires only X without paired (X, Y ), and in the sampling stage, it produces the denoised x 0 for a given y 0 .",vol1
PET Image Denoising with Score-Based Diffusion Probabilistic Models,2.1,Training Stage,"BC-DPM acts the same as the original DDPM in the training stage, it consists of a forward process and a reverse process. In the forward process, x 0 is gradually contaminated by fixed Gaussian noise, producing a sequence of latent space data {x 1 , x 2 , ..., x T }, where x T ∼ N (0, I). The forward process can be described formally by a joint distribution q(x 1:T |x 0 ) given x 0 . Under the Markov property, it can be defined as: where {β 1 , β 2 , ..., β T } is a fixed variance schedule with small positive constants and I represents the identity matrix. Notably, the forward process allows x t to be sampled directly from x 0 : where ᾱt := t s=1 α s , α t := 1β t and ∼ N (0, I). The reverse process is defined by a Markov chain starting with p(x T ) = N (x T ; 0, I): (3) Given the reverse process, p θ (x 0 ) can be expressed by setting up an integral over the x 1:T variables p θ (x 0 ) := p θ (x 0:T )dx 1:T , and the parameter θ can be updated by optimizing the following simple loss function: The θ (x t , t) used in this paper heavily relies on that proposed by Dhariwal et al. ",vol1
PET Image Denoising with Score-Based Diffusion Probabilistic Models,,Algorithm 1:,Training stage. until convergence,vol1
PET Image Denoising with Score-Based Diffusion Probabilistic Models,2.2,Sampling Stage,"The main difference between BC-DPM and the original DDPM lies in the sampling stage. Due to the stochasticity of the reverse process p θ (x 0:T ), it is difficult for the original DDPM to generate images according to our expectation. To overcome this obstacle, the proposed BC-DPM models p θ (x 0 |c) given condition c instead of modeling p θ (x 0 ) as Condition c derives from specific prior knowledge from the high-count PET image x 0 and the low-count PET image y 0 . With c, BC-DPM can control the generation of x 0 given y 0 . Then, the core problem is to design a proper condition c. A natural choice is D(y 0 ) ≈ x 0 , that is, the restoration task itself. We must clarify that it will not cause a 'deadlock' for the following two reasons. One is that the final form of the condition D(y 0 ) ≈ x 0 does not involve x 0 , and the other is that we choose a relatively simple denoiser in the condition, which can be viewed as a 'coarse to fine' operation. In practice, we utilize a Gaussian filter GF(•) as the denoiser in this condition. However, the Gaussian filter usually leads to smoothed images. Based on this property, we observe that the PSNR value between GF(y 0 ) and x 0 is usually inferior to that between GF(y 0 ) and GF(x 0 ), which means that the condition GF(y 0 ) ≈ GF(x 0 ) is more accurate than GF(y 0 ) ≈ x 0 . Thus, we choose GF(y 0 ) ≈ GF(x 0 ) in our experiments. However, if we only utilize the above condition, the training is unstable, and distortion may be observed. To address this problem, another condition needs to be introduced. The above condition refers to denoising, so conversely, we can consider adding noise to x 0 ; that is, y 0 ≈ A(x 0 ). According to the characteristics of PET noise, Poisson noise is used in the sinogram domain instead of the image domain. We define this condition as P † (P o(P(x 0 ) + r + s)) ≈ y 0 , where P, P o, P † , r and s represent the Radon transform, Poisson noise insertion, inverse Radon transform, random coincidence and scatter coincidence, respectively. Now, we have two conditions GF(y 0 ) ≈ GF(x 0 ) and P † (P o(P(x 0 )+r + s)) ≈ y 0 from the perspectives of denoising and noise insertion, respectively. Since the conditions involve x 0 , we have to convert the conditions from the original data space into latent space under certain circumstances to avoid estimating x 0 . Let us denote each transition in the reverse process under global conditions as: In Eq. (  Similarly, applying the same diffusion process to y 0 , we have {y 1 , y 2 , ..., y T }, and y 0 can be expressed with y t and : Replacing x 0 and y 0 with f θ (x t , t) and f θ (y t , t) in Eq. (  Assume that + λ(y t-1 + x t-1 -P † (P o(P(x t-1 ) + r + s)),  (11) Finally, we have p θ (x t-1 |x t , GF(x 0 ) = GF(y 0 ), P † (P o(P(x 0 ) + r + s)) = y 0 ) = p θ (x t-1 |x t , GF(x t-1 ) = GF(y t-1 ), P † (P o(P(x t-1 ) + r + s)) = y t-1 ),  Algorithm 2: Sampling stage. Figure ",vol1
PET Image Denoising with Score-Based Diffusion Probabilistic Models,3.1,Experimental Setup,"To evaluate the proposed method, real clinical data downloaded from TCIA were tested  Our method was implemented with PyTorch on a GeForce GTX 1080Ti GPU. We trained the network using the AdamW algorithm with β 1 = 0.9, β 2 = 0.999, and weight decay = 0.01. The learning rate was set to 0.0001, and the batch size was 8. In our experiments, similar to DDPM, we set the number of diffusion steps to T = 1000. For the variance schedule in the forward process, we employed a linear schedule from β 1 = 0.0001 to β T = 0.02. In the sampling stage, we evenly sampled 100 steps from 1 to T and then performed generation only on these 100 steps, reducing the number of steps from 1000 to 100 by employing the trick in  We compared our method with two conventional methods, Gaussian Filter and BM3D, and two unsupervised/unpaired methods, Noise2Void with parameter transfer (N2V-PT) ",vol1
PET Image Denoising with Score-Based Diffusion Probabilistic Models,3.2,Experimental Results,Figure ,vol1
PET Image Denoising with Score-Based Diffusion Probabilistic Models,4.0,Conclusion,"In conclusion, a PET denoising model based on diffusion probabilistic models is proposed in this paper. Our model is trained in an unsupervised manner and denoises low-count PET images without any anatomical prior as a reference. To enable the DPM to generate high-count PET images from corresponding lowcount PET images, we design bidirectional conditions derived from relations between the low-count image and the potential high-count image. One condition is that the denoised low-count image approximates the high-count image. The other is that after adding noise, the high-count image approximates the lowcount image. For implementation, we transfer the bidirectional conditions to latent space, which helps free the model from its dependence on the high-count image. Experiments on real clinical data demonstrate that our model is superior in noise suppression and detail preservation to other state-of-the-art methods.",vol1
PET Image Denoising with Score-Based Diffusion Probabilistic Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 26.,vol1
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,1.0,Introduction,"In combination with machine learning techniques, functional magnetic resonance imaging (fMRI) has recently been widely used in predictions of individual traits (e.g., age and intelligence quotient (IQ))  Contrastive learning techniques can be favorable choices for representations of human brain function  Generation of similar/dissimilar samples is critical for contrastive learning  In this study, we proposed a framework named contrastive learning with amplitudedriven data augmentation (CL-ADDA) for effective representations of human brain function and ultimately fMRI-based individualized predictions. Two augmented samples of CL-ADDA were generated through excerpting fMRI frames with relatively high amplitude and those with relatively low amplitude. With two augmented samples of the same subject used as inputs, a SimSiam-based contrastive learning framework was used to learn effective representations of human brain function. For the consideration that label information can guide SimSiam to learn more prediction-relevant representation  Our major contributions are as follows: -SimSiam was utilized to learn representations of human brain function. -A neuroscience-oriented amplitude-driven data augmentation method was introduced to generate positive pairs. -Predictions were made in an end-to-end way to improve the generalizability of the predictive models. -CL-ADDA outperformed a variety of state-of-the-art methods for fMRI-based individualized predictions.",vol1
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,2.1,Overall Workflow of CL-ADDA,Figure ,vol1
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,2.2,Amplitude-Driven Data Augmentation,"Figure  where e t jk is the co-fluctuation of ROIs-j and k at time t; x t j x t k is the z-scored fMRI signal amplitude of ROI-j (-k) at time t; e jk is obtained the co-fluctuation time series, and T is the length of fMRI time series. Accordingly, a C  ",vol1
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,2.3,Contrastive Learning on Functional Connectivity Maps,"We constructed the contrastive learning model based mainly on SimSiam. Setting no requirement on large batches, SimSiam can be a favorable choice for fMRI-based representation learning  For the consideration that spatial locality does not exist among adjacent elements on FC maps ",vol1
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,2.4,Individualized Prediction and Loss Function,"As shown in Fig.  where ξ ∈ R N×N denotes a FC map, F denotes the encoder, and α is a hyper-parameter. In the model testing stage, prediction can directly be made as ŷ = (F(ξ )), where ξ can be a FC map calculated based on the whole fMRI scan (rather than augmented data). The whole loss function for CL-ADDA includes two parts: contrastive loss and prediction loss. Contrastive learning minimizes the negative cosine similarity between the two branches: where , G denotes the projector in Fig.  Following  L1 loss was used as the prediction loss: where y and ŷ are the actual and predicted labels, respectively. The total loss was defined as follows: where λ is hyper-parameter.",vol1
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,3.1,Dataset,"The resting-state fMRI data included in the dataset collected and released by the Cambridge Centre for Ageing and Neuroscience (Cam-CAN)  The public dataset contains multi-modal data from a large cohort of adult lifespan population-based samples. After removing the subjects with excessive head motions (translation/rotation more than 2.0 mm/2.0°in/around any of the x, y, or z directions) throughout the scan, 600 subjects remained (18-87, 53.900 ± 18.549 years), and IQ scores of 568 of them were available ",vol1
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,3.2,The Performance of CL-ADDA,"Age and IQ predictions were taken as test cases to evaluate the performance of the proposed method, based on the Cam-CAN dataset. Amplitude-driven data augmentation was performed on the 200 ROI time series of each subject, and two FC maps were obtained based on two augmented samples for each subject. The two augmented FC maps were used as inputs for SimSiam for representation learning and later individualized predictions. We performed 1000 epochs of model training, with the batch size set to 128. For the consideration that high-amplitude FC maps (FC high ) may carry more detailed information about individuals' brain function, we empirically weight the predictions based on FC high more, by setting the α in Eq. ( ",vol1
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,3.3,Comparison Experiments,"We compared the performance of our proposed method with six deep learning methods for fMRI-based individualized predictions, namely, spatial-temporal graph convolutional network (ST-GCN)  Table ",vol1
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,3.4,Ablation Experiments,"To evaluate the effectiveness of the proposed amplitude-driven data augmentation strategy, we performed age and IQ predictions with the data augmented using classic methods, and the strategy of excerpting non-overlapping segments as proposed in ",vol1
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,4.0,Conclusion,"In this study, we proposed CL-ADDA for effective representation learning and ultimately precise fMRI-based individualized predictions. Originating from a recent neuroscientific finding, the proposed amplitude-driven data augmentation method provides the contrastive learning module discrepant-enough positive pairs for effective representation learning. SimSiam-based contrastive learning enables effective representation learning on fMRI dataset including limited samples. We evaluated the performance of CL-ADDA with age and IQ predictions based on a public dataset, and the experiments demonstrate that CL-ADDA achieved state-of-the-art predictions for both age and IQ.",vol1
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,1.0,Introduction,"Surgical instrument identification and classification are critical to deliver several priorities in surgical data science  Fig.  Domain adaptation methods aim to attempt to mitigate the drop in algorithm performance across domains  1. We define a novel loss for feature alignment called BFAL that doesn't require large batch sizes and encourages learning non-redundant, domain agnostic features. 2. We use BFAL to generate an end-to-end system called the Barlow Adaptor that performs UDA. We evaluate the effectiveness of this method and compare it with existing UDA methods for instrument classification in cataract surgery images. 3. We motivate new research on methods for generalizable deep learning models for surgical instrument classification using cataract surgery as the test-bed. Our work proposes a solution to the problem of lack of generalizability of deep learning models that was identified in previous literature on cataract surgery instrument classification.",vol1
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,2.0,Related Work,"Instrument Identification in Cataract Surgery Video Images. The motivation for instrument identification is its utility in downstream tasks such as activity localization and skill assessment  Unsupervised Domain Adaptation. UDA is a special case of domain adaptation, where a model has access to annotated training data from a source domain and unannotated data from a target domain  Another line of research for UDA involves adversarial training. Domain Adaptive Neural Network (DANN) ",vol1
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,3.0,Proposed Method,"In the UDA task, we are given n s observations from the source domain D S . Each of these observations is in the form of a tuple (x s , y s ), where x s denotes an image from the source training data and y s denotes the corresponding label, which is the instrument index present in the image. In addition, we are given n t observations from the target domain D T . Each of these can be represented by x t , which represents the image from the target training data. However, there are no labels present for the target domain during training. The goal of UDA is to predict the labels y t for the target domain data.",vol1
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,,Barlow Feature Alignment Loss (BFAL).,"We introduce a novel loss, which encourages features between the source and target to be similar to each other while reducing the redundancy between the learnt features. BFAL works on pairs of feature projections of the source and target. More specifically, let f s ∈ R BXD and f t ∈ R BXD be the features corresponding to the source and target domain, respectively. Here B represents the batch size and D represents the feature dimension. Similar to  Finally, the BFAL is computed using the L2 loss between the elements of C 1 and the identity matrix I as follows where μ is a constant. Intuitively, the first term of the loss function can be thought of as a feature alignment term since we push the diagonal elements in the covariance matrix towards 1. In other words, we encourage the feature projections between the source and target to be perfectly correlated. On the other hand, by pushing the off-diagonal elements to 0, we decorrelate different components of the projections. Hence, this term can be considered a redundancy reduction term, since we are pushing each feature vector component to be independent of one another. BFAL is inspired by a recent technique in self-supervised learning, called the Barlow Twins  There are two main sub-parts of the architecture -the Feature Extractor F , and the Source Classifier C. First, we divide the training images randomly into batches of pairs {x s , x t } and apply F on them, which gives us the features extracted from these sets of images. For the Feature Detector, we show the effectiveness of our novel loss using ViT and ResNet50 both of which have been pre-trained on ImageNet. The features obtained are denoted as f s and f t for the source and target domains, respectively. Next, we apply C on these features to get logits for the classification task. The source classifier is a feed forward neural network, which is initialized from scratch. These logits are used, along with the source labels y s to compute the source cross entropy loss as , where M represents the number of classes, B represents the total minibatches, while m and b represent their respective indices. The features f s and f t are further used to compute the Correlation Alignment(CORAL) loss and the BFAL, which enforce the feature extractor to align its weights so as to learn features that are domain agnostic as well as nonredundant. The BFAL is calculated as mentioned in the previous subsection. The CORAL loss is computed as depicted in Eq. 4, following the UDA method Deep CORAL  where Each of these three losses plays a different role in the UDA task. The cross entropy loss encourages the model to learn discriminative features between images with different instruments. The CORAL loss pushes the features between the source and target towards having a similar distribution. Finally, the BFAL tries to make the features between the source and the target non-redundant and same. BFAL is a stricter loss than CORAL as it forces features to not only have the same distribution but also be equal. Further, it also differs from CORAL in learning independent features as it explicitly penalizes non-zero non-diagonal entries in the correlation matrix. While using BFAL alone gives good results, using it in addition to CORAL gives slightly better results empirically. We note these observations in our ablation studies. Between the cross entropy loss and the BFAL, an adversarial game is played where the former makes the features more discriminative and the latter tries to make them equal. The optimal features thus learnt are different in aspects required to identify instruments but are equal for any domain-related aspect. This property of the Barlow Adaptor is especially useful for surgical domains where the background has similar characteristics for most of the images within a domain. For example, for cataract surgery images, the position of the pupil or the presence of blood during the usage of certain instruments might be used by the model for classification along with the instrument features. These features depend highly upon the surgical procedures and the skill of the surgeon, thus making them highly domain-specific and possibly unavailable in the target domain. Using BFAL during training attempts to prevent the model from learning such features.",vol1
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,4.0,Experiments and Results,"We evaluate the proposed UDA method for the task of instrument classification using two cataract surgery image datasets. In our experiments, one dataset is used as the source domain and the other is used as the target domain. We use micro and macro accuracies as our evaluation metrics. Micro accuracy denotes the number of correctly classified observations divided by the total number of observations. In contrast, macro accuracy denotes the average of the classwise accuracies and is effective in evaluating classes with less number of samples. Datasets. The first dataset we use is CATARACTS  Experimental Setup. We train the Barlow Adaptor for multi-class classification with the above-mentioned 14 classes in Pytorch. For the Resnet50 backbone, we use weights pretrained on Imagenet  Results. Table  Ablation Study. We tested the performance gain due to each part of the Barlow Adaptor. Specifically, the Barlow Adaptor has CORAL loss and BFAL as its two major feature alignment losses. We remove one component at a time and observe a decrease in performance with both ResNet and ViT backbones (Table ",vol1
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,5.0,Conclusion,"Domain shift between datasets of cataract surgery images limits generalizability of deep learning methods for surgical instrument classification. We address this limitation using an end-to-end UDA method called the Barlow Adaptor. As part of this method, we introduce a novel loss function for feature alignment called the BFAL. Our evaluation of the method shows larger improvements in classification performance compared with other state-of-the-art methods for UDA. BFAL is an independent module and can be readily integrated into other methods as well. BFAL can be easily extended to other network layers and architectures as it only takes pairs of features as inputs.",vol1
Gall Bladder Cancer Detection from US Images with only Image Level Labels,1.0,Introduction,"GBC is a deadly disease that is difficult to detect at an early stage  Instead of training a classification pipeline, we propose to solve an object detection problem, which involves predicting a bounding box for the malignancy. The motivation is that, running a classifier on a focused attention/ proposal region in an object detection pipeline would help tackle the low inter-class and high intra-class variations. However, since we only have image-level labels available, we formulate the problem as a Weakly Supervised Object Detection (WSOD) problem. As transformers are increasingly outshining CNNs due to their ability to aggregate focused cues from a large area  Inspired by the success of the Multiple Instance Learning (MIL) paradigm for weakly supervised training on medical imaging tasks ",vol1
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Contributions:,The key contributions of this work are: -We design a novel DETR variant based on MIL with self-supervised instance learning towards the weakly supervised disease detection and localization task in medical images. Although MIL and self-supervised instance learning has been used for CNNs ,vol1
Gall Bladder Cancer Detection from US Images with only Image Level Labels,2.0,Datasets,"Gallbladder Cancer Detection in Ultrasound Images: We use the public GBC US dataset  Note that, we use only the image labels for training. We report results on 5-fold cross-validation. We did the cross-validation splits at the patient level, and all images of any patient appeared either in the train or validation split. Polyp Detection in Colonoscopy Images: We use the publicly available Kvasir-SEG  Since the patient information is not available with the data, we use random stratified splitting for 5-fold cross-validation.",vol1
Gall Bladder Cancer Detection from US Images with only Image Level Labels,3.0,Our Method,"Revisiting DETR: The DETR  Proposed Architecture: Fig.  MIL Setup: The decoder of the fine-tuning DETR generates R d-dimensional output embeddings. Each embedding corresponds to a proposal generated by the class-agnostic DETR. We pass these embeddings as input to two branches with FC layers to obtain the matrices X c ∈ R R×Nc and X r ∈ R R×Nc , where R is the number of object queries (same as proposals) and N c is the number of object (disease) categories. Let σ(•) denote the softmax operation. We then generate the class-wise and detection-wise softmax matrices C ∈ R R×Nc and D ∈ R R×Nc , where C ij = σ((X c ) T j )i and D ij = σ(X r i )j, and X i denotes the i-th row of X. C provides classification probabilities of each proposal, and D provides the relative score of the proposals corresponding to each class. The two matrices are element-wise multiplied and summed over the proposal dimension to generate the image-level classification predictions, φ ∈ R Nc : Notice, φ j ∈ (0, 1) since C ij and D ij are normalized. Finally, the negative loglikelihood loss between the predicted labels, and image labels y ∈ R Nc is computed as the MIL loss: The MIL classifier further suffers from overfitting to the distinctive classification features due to the mismatch of classification and detection probabilities  To tackle this, we further use a self-supervised module to improve the instances.",vol1
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Self-supervised Instance Learning:,"Inspired by  The loss over the instances is given by Eq. 4: Here x n ij denotes the score of i-th instance for j-th class at layer n. Following  is applied to stabilize the loss. Assuming λ to be a scaling value, the overall loss function is given in Eq. 5:",vol1
Gall Bladder Cancer Detection from US Images with only Image Level Labels,4.0,Experiments and Results,"Experimental Setup: We use a machine with Intel Xeon Gold 5218@2.30GHz processor and 8 Nvidia Tesla V100 GPUs for our experiments. The model is trained using SGD with LR 0.001 (for MIL head), weight decay 10 -6 , and momentum 0.9 for 100 epochs with batch size 32. The LR at backbone and transformer are 0.003, and 0.0003, respectively. We use a cosine annealing of the LR. Comparison with SOTA: Table ",vol1
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Generality of the Method:,"We assess the generality of our method by applying it to polyp detection on colonoscopy images. The applicability of our method on two different tasks -(1) GBC detection from US and (2) Polyp detection from Colonoscopy, indicates the generality of the method across modalities.",vol1
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Ablation Study:,"We show the detection sensitivity to the self-supervised instance learning module in Table  Classification Performance: We compare our model with the standard CNNbased and Transformer-based classifiers, SOTA WSOD-based classifiers, and SOTA classifiers using additional data or annotations (Table ",vol1
Gall Bladder Cancer Detection from US Images with only Image Level Labels,5.0,Conclusion,"GBC is a difficult-to-detect disease that benefits greatly from early diagnosis. While automated GBC detection from US images has gained increasing interest from researchers, training a standard image classification model for this task is challenging due to the low inter-class variance and high intra-class variability of malignant regions. Current SOTA models for GBC detection require costly bounding box annotation of the pathological regions, or additional US video data, which limit their applicability. We proposed to formulate GBC detection as a weakly supervised object detection/ localization problem using a DETR with selfsupervised instance learning in a MIL framework. Our experiments show that the approach achieves competitive performance without requiring additional annotation or data. We hope that our technique will simplify the model training at the hospitals with easily available data locally, enhancing the applicability and impact of automated GBC detection.",vol1
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 20.,vol1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,1.0,Introduction,"Precision medicine efforts are shifting cancer care standards by providing novel personalised treatment plans with promising outcomes. Patient selection for such treatment regimes is based principally on the assessment of tissue biopsies and the characterisation of the tumor microenvironment. This is typically performed by experienced pathologists, who closely inspect chemically stained histopathological whole slide images (WSIs). Increasingly, clinical centers are investing in the digitisation of such tissue slides to enable both automatic processing as well as research studies to elucidate the underlying biological processes of cancer. The resulting images are of gigapixel size, rendering their computational analysis challenging. To deal with this issue, multiple instance learning (MIL) schemes based on weakly supervised training are used for WSI classification tasks. In such schemes, the WSI is typically divided into a grid of patches, with general purpose features derived from pretrained ImageNet  State space models are designed to efficiently model long sequences, such as the sequences of patches that arise in WSI MIL. In this paper, we present the first use of state space models for WSI MIL. Extensive experiments on three publicly available datasets show the potential of such models for the processing of gigapixel-sized images, under both weakly and multi-task schemes. Moreover, comparisons with other commonly used MIL schemes highlight their robust performance, while we demonstrate empirically the superiority of state space models in processing the longest of WSI sequences with respect to commonly used MIL methods.",vol1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,2.0,Related Work,"Using pretrained networks for patch-wise feature extraction is a well established strategy for histopathology analysis  The state space model is a linear differential equation, that is widely studied in control theory, and describes a continuous time process for input and output signals u(t) ∈ R p and y(t) ∈ R q , and state signal x(t) ∈ R n , and where the process is governed by matrices In HiPPO  Whereas in HiPPO, the state matrix A is a fixed constant, the linear state space layer (LSSL) ",vol1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,3.0,Method,"Given that the patch extraction of whole slide images at high magnifications results in long sequences of patches, we propose to incorporate a state space layer in a MIL aggregation network to better represent each patch sequence.",vol1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,3.1,Neural State Space Models,"In practice, neural state space models (SSM) simulate Eq. 1 in discrete time, invoking a recurrence relation on the discretised hidden state, where the sequences u t , x t , and y t are the discretised u(t), x(t), and y(t), and the modified model parameters arise from a bilinear discretisation  where u ∈ R L and y ∈ R L are the full input and output sequences, and the sequence K ∈ R L is defined as, which is computed efficiently by the S4D algorithm ",vol1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,3.2,MIL Training,"In our pipeline (Fig.  The architecture of F is composed of an initial linear projection layer, used to lower the dimensionality of each vector in the input sequence. A SSM layer is then applied feature-wise by applying the S4D algorithm. That is, Eq. 3, including the skip connection, transforms the sequence {u 1,d , u 2,d , . . . , u L,d } for all features d, and the resulting sequences are concatenated. A linear ""mixing"" layer is applied token-wise, doubling the dimensionality of each token, followed by a gated linear unit  where ŷcm denotes the probability corresponding to c m , the slide-level label of the sequence corresponding to the m th of M whole slide images.",vol1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,3.3,Multitask Training,"One advantage of processing an entire slide as a sequence is the ease with which additional supervision may be incorporated, when available. A patch-level ground truth creates the opportunity for multitask learning, which can enhance the representations learned for slide-level classification. As an extension of our base model in Eq. 6, we train a multitask model to jointly predict a slide-level and patch-level labels. Prior to the max pooling layer of the base model, an additional linear layer is applied to each sequence token, yielding L additional model outputs. This multitask model is trained according to a sum of log losses, where c m,l indexes the class of the l th patch in the m th training slide and λ is a tunable hyperparameter used to modulate the relative importance of each task.",vol1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,3.4,Implementation Details,We extracted patches of size 256 × 256 from the tissue regions of WSIs at 20x magnification. Following CLAM ,vol1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,4.1,Data,"CAMELYON16  TCGA-LUAD is a TCGA lung adenocarcinoma dataset that contains 541 WSIs along with genetic information about each patient. We obtained genetic information for this cohort using Xena browser  TCGA-RCC is a TCGA dataset for three kidney cancer subtypes (denoted KICH, KIRC, and KIRP). It consists of 936 WSIs (121 KICH, 518 KIRC, and 297 KIRP). The average sequence length is 12234 (ranging from 319 to 62235).",vol1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,4.2,Results,"Multiple Instance Learning Results. We evaluate our method on each dataset by accuracy and area under receiver operating characteristic curve (AUROC). For multiclass classification, these were computed in a one-versus-rest manner. Table  We further compare our method with respect to model and time complexity. In Table ",vol1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,,Multitask Learning Results.,"We explored the ability of our model to combine slide-and patch-level information on the CAMEYLON16 dataset. We compared our model with the best performing model on CAMELYON16, TransMIL. Both models were trained according to Eq. 7 with λ = 5 tuned by hand. In Table ",vol1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,5.0,Conclusions,"In this work we have explored the ability of state space models to act as multiple instance learners on sequences of patches extracted from histopathology images. These models have been developed for their ability to memorise long sequences, and they have proven competitive with state of the art MIL models across a range of pathology problems. Additionally, we demonstrated the ability of these models to perform multiclass classification, which furthermore allowed us to visualise the localisation of metastasic regions. Finally, we demonstrated that on the longest sequences in our datasets, state space models offer better performance than competing models, confirming their power in modeling long-range dependencies.",vol1
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,1.0,Introduction,"Muscle wasting, also known as muscle atrophy (see Fig.  In recent years, self-supervised learning (SSL) has gained popularity for automated diagnosis in the field of medical imaging due to its ability to learn from unlabeled data  In this paper, we focus on the underinvestigated application of view matching for longitudinal RF muscle US examinations to assess muscle wasting. Our method uses a CL approach (see Fig. ",vol1
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,2.1,Problem Formulation,"Muscle wasting assessment requires matching of corresponding cross-sectional US views of the RF over subsequent (days to weeks apart) examinations. The first acquisition is carried out following a protocol to place the transducer half way through the thigh and perpendicular to the skin, but small variations in translation and angulation away from this standard view are common. This scan produces the reference view at time T 1 (RT 1 ). The problem is as follows: given RT 1 , the task is to retrieve the corresponding view (V T 2 ) at a later time (T 2 ) from a sequence of US images captured by the operator using the transducer at approximately the same location and angle as for T 1 . The main challenges of this problem include: (1) the transducer pose and angle might be different, (2) machine settings might be slightly different, and (3) parts of the anatomy (specifically the RF) might change in shape and size over time. As a result, our aim is to develop a model that can select the most similar view acquired during T 2 to the reference view RT 1 acquired at T 1 .",vol1
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,2.2,Contrastive Learning Framework for Muscle View Matching,"Inspired by the SimCLR algorithm  The contrastive loss function for a positive pair (Xi, Xj) is defined as: where 1 ∈ (0, 1), τ is a temperature parameter and sim(•) denotes the pairwise cosine similarity. z is a representation vector, calculated by z = g(f (X)), where f(•) indicates a shared encoder and g(•) is a projection head. L i C is computed across all positive pairs in a mini-batch. Then f (•) and g(•) are trained to maximize similarity using this contrastive loss.",vol1
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,2.3,The Model Architecture,"The model architecture is shown in Fig.  Second, we use the trained encoder f (•) for the training of our main task (i.e. the downstream task), which is the classification of positive and negative matches (corresponding and non-corresponding views) of our test set. For that, we feed a reference image X ref , and a candidate frame X j to the encoder to obtain the representations hi, hj and feed these in turn to a classification network (shown in Fig. ",vol1
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,3.0,Materials,"The muscle US exams were performed using GE Venue Go and GE Vivid IQ machines, both with linear probes (4.2-13.0 MHz), by five different doctors. During examination, patients were in supine position with the legs in a neutral rotation with relaxed muscle and passive extension. Measurements were taken at the point three fifths of the way between the anterior superior iliac spine and the patella upper pole. The transducer was placed perpendicular to the skin and to the longitudinal axis of the thigh to get the cross-sectional area of the RF. An excess of US gel was used and pressure on the skin was kept minimal to maximise image quality. US measurements were taken at ICU admission (T 1 ), 2-7 d after admission (T 2 ) and at ICU discharge (T 3 ). For this study, 67 Central Nervous System (CNS) and Tetanus patients were recruited and their data were acquired between June 2020 and Feb 2022. Each patient had an average of six muscle ultrasound examinations, three scans for each leg, totalling 402 examinations. The video resolution was 1080 × 1920 with a frame rate of 30fps. This study was performed in line with the principles of the Declaration of Helsinki. Approval was granted by the Ethics Committee of the Hospital for Tropical Diseases, Ho Chi Minh City and Oxford Tropical Research Ethics Committee. The contrastive learning network was trained without any annotations. However, for the view matching classification task, our test data were annotated automatically as positive and negative pairs based upon manual frame selection by a team of five doctors comprising three radiologists and two ultrasound specialists with expertise in muscle ultrasound. Specifically, each frame in an examination was manually labelled as containing a similar view to the reference RT 1 or not. Based upon these labelings, as shown in Fig. ",vol1
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,4.1,Implementation Details,"Our model was implemented using Tensorflow 2.7. During training, input videos underwent experimentation with clip sizes of 256 × 256, 128 × 128, and 64 × 64. Eventually, they were resized to 64 × 64 clips, which yielded the best performance. All the hyperparameters were chosen using the validation set. For the CL training, the standard Adam optimizer was used with learning rate =0.00001, kernel size = 3 × 3, batch size = 128, batch normalization, dropout with p = 0.2 and L2 regularization of the model parameters with a weight = 0.00001. The CL model was trained on 80% of the muscle US data for 500 epochs. For the view retrieval model, the standard Adam optimizer with learning rate = 0.0001, batch size = 42 and dropout of p = 0.2 was used. The classifier was trained on the remaining 20% of the data (of which 80% were used for training, 10% for validation and 10% for testing) and the network converged after 60 epochs. For the supervised baseline model, the standard Adam optimizer was used with learning rate =0.00001, kernel size = 3 × 3, batch size = 40, and batch normalization. Here, we used the same data splitting as our view retrieval classifier. The code we used to train and evaluate our models is available at https://github.com/ hamidehkerdegari/Muscle-view-retrieval.",vol1
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,4.2,Results,"Quantitative Results. We carried out two quantitative experiments. First, we evaluated the performance of the view classifier. Second, we evaluated the quality of the resulting cross-sectional areas segmented using a U-Net  The classifier performance was carried out by measuring, for the view retrieval task, the following metrics: Area Under the Curve (AUC), precision, recall, and F1-score. Because there is no existing state of the art for this task, we created two baseline models to compare our proposed model to: first, a naive image-space comparison using normalized cross-correlation (NCC)  We applied a trained U-Net model (already trained with 1000 different US muscle images and manual segmentations). Results showed an overall cross-sectional mean relative absolute area error of 5.7% ± 0.24% on the test set (Full details provided in Fig.  Qualitative Results. We conducted a user study survey to qualitatively assess our model's performance. The survey was conducted blindly and independently by four clinicians and consisted of thirty questions. In each, clinicians were shown two different series of three views of the RF: (1) RT 1 , GT match from T 2 and model prediction from T 2 , and (2) RT 1 , a random frame from T 2 and model prediction from T 2 . They were asked to indicate which (second or third) was the best match with the first image. The first question aimed to determine if the model's performance was on par with clinicians, while the second aimed to determine if the model's selection of images was superior to a randomly picked frame. As shown in Fig. ",vol1
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,5.0,Discussion and Conclusion,"This paper has presented a self-supervised CL approach for automatic muscle US view retrieval in ICU patients. We trained a classifier to find positive and negative matches. We also computed the cross-sectional area error between the ground truth frame and the model prediction in each acquisition time to evaluate model performance. The performance of our model was evaluated on our muscle US video dataset and showed AUC of 73.52% and 5.7% ± 0.24% error in cross-sectional view. Results showed that our model outperformed the supervised baseline approach. This is the first work proposed to identify corresponding ultrasound views over time, addressing an unmet clinical need.",vol1
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,1.0,Introduction,"Longitudinal lesion or tumor tracking is a fundamental task in treatment monitoring workflows, and for planning of re-treatments in radiation therapy. Based on longitudinal imaging for a given patient it requires establishing which lesions are corresponding (i.e., same lesion, observed at different timepoints), which lesions have disappeared and which are new compared to prior scanning. This information can be leveraged to assess treatment response, e.g., by analyzing the evolution of size and morphology for a given tumor  In practice, the development of automatic and reliable lesion tracking solutions is hindered by the complexity of the data (over different modalities), the absence of large, annotated datasets, and the difficulties associated with lesion identification (i.e., varying sizes, poses, shapes, and sparsely distributed locations). In this work, we present a multi-scale self-supervised learning solution for lesion tracking in longitudinal studies using the capabilities of contrastive learning  Our proposed method brings two elements of novelty from a technical point of view: (1) the multi-scale approach for the anatomical embedding learning and (2) a positive sampling approach that incorporates anatomically significant landmarks across different subjects. With these two strategies, the goal is to ensure a high degree of robustness in the computation of the lesion matching across different lesion sizes and varying anatomies. Furthermore, a significant focus and contribution of our research is the experimental study at a very large scale: we (1) train a pixel-wise self-supervised system using a very large and diverse dataset of 52,487 CT volumes and (2) evaluate on two publicly available datasets. Notably, one of the datasets, NLST, presents challenging cases with 68% of lesions being very small (i.e., radius < 5 mm).",vol1
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,2.0,Background and Motivation,"The problem of lesion tracking in longitudinal data is typically divided into two steps: (1) detection of lesions and (2) tracking the same lesion over multiple time points. Classical methods to solve this problem rely on image registration, where tracking is performed via image alignment and rule-based correspondence matching ",vol1
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,3.1,Problem Definition,"Let I 1 (i.e., template or baseline image) and I 2 (i.e., query or follow-up image) be two 3D-CT scans acquired at time t 1 and t 2 , respectively, Additionally, let p 1 and p 2 denote the point of interest (i.e., the lesion center) in both images. The problem of lesion tracking can be formulated as finding the optimal transformation that maps p 1 to its corresponding location, p 2 , in I 2 .",vol1
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,3.2,Training Stage,"Let D = {X 1 , X 2 , ..., X N } be a set of N unpaired and unlabeled 3D-CT volumes. As shown in Fig.  Given the nature of contrastive learning, the sampling strategy (extracting negative and positive pixel pairs from augmented 3D paired patches) is essential to achieving discriminative pixel-wise embeddings. We arbitrary sample n pos positive pixel pairs from the overlapping area of X a and X q , denoted by a + = {a + 1 , ..., a + j ..., a + npos }, q + = {q + 1 , ..., q + j , ..., q + npos }, 1 ≤ j ≤ n pos . To further enhance embeddings, 10% of the positive pixel pairs are derived from biologically-meaningful points across different volumes in the batch. We use data-driven models  Next, at each scale, we extract the embedding vectors for positive and negative pixel pairs from F i a , F i q , guided by the corresponding locations, a + , q + , h -, which are downsampled to match the scale. We denote the positive embeddings at ith scale at pixel location a + j , q + j as f a i j , f q i j ∈ R L . Similarly, we denote the negative embeddings at pixel location h - k associated to a positive positive pixel pair (a + j , q + j ) as f i jk ∈ R L . We use L2-norm to normalize the embedding vectors before the loss computation. We use pixel-wise InfoNCE loss  where τ = 0.5 is a temperature parameter. The final loss is then calculated as the average of all these individual losses.",vol1
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,3.3,Inference Stage,"Let X a be a 3D-CT volume template with an input point of interest p a ∈ X a , and X q a corresponding query 3D-CT volume. The first step is to project the image X a into a multi-scale feature space, creating a hierarchy of multi-scale semantic embeddings F a for each pixel in the image (i.e., a 4D feature map). Next, we follow a similar process for the query image X a and acquire the pixellevel embeddings F q . To measure the similarity between the embeddings of the input X a at the point of interest p a and the query embeddings F q , we compute cosine similarity maps at each scale: (2) Finally, we combine the multi-scale similarity maps through summation and select the voxel with the highest similarity as the matching point in the query volume.",vol1
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,4.1,Datasets and Setup,"Datasets: We train the universal and fine-grained anatomical point matching model using an in-house CT dataset (VariousCT). The training dataset contains 52,487 unlabeled 3D CT volumes capturing various anatomies, including chest, head, abdomen, pelvis, and more. The evaluation is based on two datasets, the publicly released Deep Longitudinal Study (DLS) dataset  System Training: Our learning model is implemented in PyTorch and uses the TorchIO library  We employ a U-Net-based encoder-decoder architecture ",vol1
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,,Method,"For data augmentation, we apply random cropping, scaling, rotation, and Gaussian noise injections. A windowing approach that covers the intensity ranges of lungs and soft tissues is used to scale CT intensity values to [-1, 1]. The sampling hyperparameters consist of 100 positive pixel pairs (n pos = 100), 100 hard negative pixel pairs, and 200 diverse negative pixel pairs (n neg = 300).",vol1
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,,Evaluation Metrics:,"We use mean Euclidean distance (MED) to measure the distance between predicted lesion center and ground truth, and the center point matching accuracy (i.e., percentage of accurately matched lesions given the annotated lesion radius), denoted with CPM@Radius. For lesions of large sizes, we set a maximum distance limit of 10 mm as acceptance criteria ",vol1
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,4.2,Evaluation,"For the lesion tracking task on DLS dataset, we quantitatively compare our system against existing trackers in Table ",vol1
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,5.0,Conclusion,"In conclusion, this paper presents an effective method for longitudinal lesion tracking based on multi-scale self-supervised learning. The method is generic, it does not require expert annotations or longitudinal data for training and can generalize to different types of tumors/organs/modalities. The multi-scale approach ensures a high degree of robustness and accuracy for small lesions. Through large-scale experiments and validation on two longitudinal datasets, we highlight the superiority of the proposed method in comparison to state-of-theart. We found that adopting a multi-scale approach (instead of the global/local approach as proposed in  Disclaimer: The concepts and information presented in this paper/presentation are based on research results that are not commercially available. Future commercial availability cannot be guaranteed.",vol1
Geometry-Invariant Abnormality Detection,1.0,Introduction,"The use of machine learning for anomaly detection in medical imaging analysis has gained a great deal of traction over previous years. Most recent approaches have focused on improvements in performance rather than flexibility, thus limiting approaches to specific input types -little research has been carried out to generate models unhindered by variations in data geometries. Often, research assumes certain similarities in data acquisition parameters, from image dimensions to voxel dimensions and fields-of-view (FOV). These restrictions are then carried forward during inference  Unsupervised methods have become an increasingly prominent field for automatic anomaly detection by eliminating the necessity of acquiring accurately labelled data  Even though these methods are state-of-the-art, they have stringent data requirements, such as having a consistent geometry of the input data, e.g., in a whole-body imaging scenario, it is not possible to crop a region of interest and feed it to the algorithm, as this cropped region will be wrongly detected as an anomaly. This would happen even in the case that a scan's original FOV was restricted  As such, we propose a geometric-invariant approach to anomaly detection, and apply it to cancer detection in whole-body PET via an unsupervised anomaly detection method with minimal spatial labelling. Through adapting the VQ-VAE Transformer approach in ",vol1
Geometry-Invariant Abnormality Detection,2.0,Background,"The main building blocks behind the proposed method are introduced below. Specifically, a VQ-VAE plus a Transformer are jointly used to learn the probability density function of 3D PET images as explored in prior research ",vol1
Geometry-Invariant Abnormality Detection,2.1,Vector-Quantized Variational Autoencoder,The VQ-VAE model provides a data-efficient encoding mechanism-enabling 3D inputs at their original resolution-while generating a discrete latent representation that can trivially be learned by a Transformer network ,vol1
Geometry-Invariant Abnormality Detection,2.2,Transformer,"After training a VQ-VAE model, the next stage is to learn the probability density function of the discrete latent representations. Using the VQ-VAE, we can obtain a discrete representation of the latent space by replacing the codebook elements in Z q with their respective indices in the codebook yielding Z iq . To model the imaging data, we require the discretized latent space Z iq to take the form of a 1D sequence s, which we achieve via a raster scan of the latent. The Transformer is then trained to maximize the log-likelihoods of the latent tokens sequence in an autoregressive manner. By doing this, the Transformer can learn the codebook distribution for position i within s with respect to previous codes p(s i ) = p(s i |s <i ). As with ",vol1
Geometry-Invariant Abnormality Detection,2.3,Anomaly Detection via Kernel Density Estimation Maps,"Building on  In this work, abnormalities are defined as deviations between the distribution of ""healed"" reconstructions and the observed data, measured using a Kernel Density Estimation (KDE) approach. We generate multiple healed latent sequences by sampling multiple times for each position i with a likelihood p(s i ) < t. In each resampling, the Transformer outputs the likelihood for every possible token at position i. Based on these probabilities, we can create a multinomial distribution showcasing the probability of each token. We can then randomly sample multiple tokens. Each of these healed latent spaces is then decoded via the VQ-VAE multiple times with dropout. This generates multiple healed representations of the original image. A voxel-wise KDE anomaly map is generated by fitting a KDE independently at each voxel position to estimate the probability density function f across reconstructions. This is then scored at the original intensity of that voxel in the scan. Our KDE implementation used 60 samples for each anomalous token in s, followed by five decodings with dropout, yielding 300 ""healed"" reconstructions that are then used to calculate the KDE.",vol1
Geometry-Invariant Abnormality Detection,3.1,VQ-VAE Spatial Conditioning,"To date, there has been little research on generating autoencoder models capable of using images of varying sizes and resolutions (i.e. the input tensor shape to a autoencoder is assumed to be fixed). Although fully convolutional models can ingest images of varying dimensions, we have found that using training data with varying resolutions resulted in poor auto-encoder reconstructions. In this work, we take inspiration from CoordConv  A CoordConv layer is a concatenation of channels to the input image referencing a predefined coordinate system. After concatenation, the input is simply fed through a standard convolutional layer. For a 3D scan, we would have 3 coordinates, ijk, where the i coordinate channel is an h × w × d rank-1 matrix with its first row filled with 0's, its second row with 1's, and so on. This would be the same for the j coordinate channel, except the columns would be filled with constant values, not the rows, and likewise for the k coordinate channel in a depth-wise fashion. These channels are then normalised between [0, 1]. The advantage of the CoordConv implementation is the constant scale of 0-1 across the channels regardless of image resolution. For example, two wholebody images with large differences in voxel-size will have CoordConv channels from 0-1 along each axis, thus conveying the notion of spatial resolution to the network. We found when training the VQ-VAE model on data with varying resolutions and dimensions that reconstructions showcased unwanted and significant artifacts, while by adding the CoordConv channels this issue was not present (See Appendix C for examples). Furthermore, when dealing with images of a ranging FOV, we adapted the [0, 1] channel values to convey the image's FOV. For example, suppose a whole body image (neck to upper leg) represented our range [0, 1] where 0 is the upper leg, and 1 is the neck. In that case, we can contract this range to represent the area displayed in the image (Fig.  We used random crops during training to simulate varying FOVs of wholebody data. The random crop parameters are then used to define the coordinate system. For the implementation of the CoordConv layer, these channels are added once to the original input image and at the beginning of the VQ-VAE decoder, concatenated to the latent space, using the same value ranges but at a lower resolution given the reduced spatial dimension of the latent space.",vol1
Geometry-Invariant Abnormality Detection,3.2,Transformer Spatial Conditioning,"Numerous approaches have used Transformers in the visual domain  where sp is the quantized spatial value allocated to a given token at position ijk in the latent space, and b represents the binned value along a given channel for that token, and B is a pre-defined bin size. The choice of B = 20 bins was empirically chosen to closely resemble the average latent dimension of images. During training, whole-body images and random crops are used. The spatial conditioning tokens are then generated and fed through an embedding layer of equal dimension to the CT embedding. The two embedded sequences (CT and spatial) are then added together and fed to the Transformer via cross-attention. For reference, this mechanism can be visualised in Fig. ",vol1
Geometry-Invariant Abnormality Detection,3.3,Data,"For this work we leveraged whole-body PET/CT data from different sources to explore the efficacy of our approach for varying image geometries. 211 scans from NSCLC Radiogenomics  All baseline models work in a single space with constant dimensions, obtained by registering the AutoPET images to the space of the NSCLC dataset. For evaluation, we use four testing sets: a lower resolution set derived from both the NSCLC and the private dataset; a higher resolution set from AutoPET; a testing set with random crops of the same NSCLC/private testing dataset and finally a testing set that has been rotated through 90 • using the high resolution testing data. As the cropped and rotated dataset cannot be fed into the baseline models, we pad the images to the common image sizing before inference.",vol1
Geometry-Invariant Abnormality Detection,4.0,Results,"The proposed model was trained on the data described in Sect. 3.3, with random crops applied while training. Model and anomaly detection hyperparameter tuning was done on our validation samples using the best DICE scores. We then test our model and baselines on 4 hold-out test sets: a low-resolution whole-body set, a low-resolution cropped set, a high-resolution rotated set and a high-resolution test set of PET images with varying cancers. The visual results shown in Fig. ",vol1
Geometry-Invariant Abnormality Detection,5.0,Conclusion,"Detection and segmentation of anomalous regions, particularly for cancer patients, is essential for staging, treatment and intervention planning. Generally, the variation scanners and acquisition protocols can cause failures in models trained on data from single sources. In this study, we proposed a system for anomaly detection that is robust to variances in geometry. Not only does the proposed model showcase strong and statistically-significant performance improvements on varying image resolutions and FOV, but also on whole-body data. Through this, we demonstrate that one can improve the adaptability and flexibility to varying data geometries while also improving performance. Such flexibility also increases the pool of potential training data, as they dont require the same FOV. We hope this work serves as a foundation for further exploration into geometry-invariant deep-learning methods for medical-imaging.",vol1
Geometry-Invariant Abnormality Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_29.,vol1
