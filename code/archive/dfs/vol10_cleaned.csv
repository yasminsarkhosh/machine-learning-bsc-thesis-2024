Paper Title,Header Number,Header Title,Text
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,1.0,Introduction,"Convolutional neural networks (CNN) have emerged as one of the most popular methods for noise removal and restoration of LDCT images  Furthermore, the noise variance is also influenced by the differences in patient size and shape, imaging protocol, etc. "
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,2.0,Method,"Motivation: Each convolutional layer in a neural network performs the sum of the product operation between the weight vector and input features. However, as tissue density changes in LDCT images, the noise intensity also changes, leading to a difference in the magnitude of intermediate feature values. If the variation in input noise intensity is significant, the magnitude of the output feature of the convolutional layer can also change substantially. This large variation in input feature values can make the CNN layer's response unstable, negatively impacting the denoising performance. To address this issue, we propose to modulate the weight vector values of the CNN layer based on the noise level of the input image. This approach ensures that the CNN layer's response remains consistent, even when the input noise variance changes drastically. Weight Modulation: Figure  Here, ŵl is the modulated weight value, and represents component wise multiplication. Next, the scaled weight vector is normalized by its L2 norm across channels as follows: Normalizing the modulated weights takes care of any possible instability arise due to high or too low weight value and also ensures that the modulated weight has consistent scaling across channels, which is important for preserving the spatial coherence of the denoised image "
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,,Relationship with Recent Methods:,The proposed weight modulation technique leveraged the recent concept of style-based image synthesis proposed in StyleGAN2 
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,,Implementation Details:,"The proposed dynamic convolutional layer is very generic and can be integrated into various backbone networks. For our denoising task, we opted for the encoder-decoder-based UNet "
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,3.0,Experimental Setting,"We used two publicly available data sets, namely, 1. TCIA Low Dose CT Image and Projection Data, 2. 2016 NIH-AAPM-Mayo Clinic low dose CT grand challenge database to validate the proposed method. The first dataset contains LDCT data of different patients of three anatomical sites, i.e., head, chest, and abdomen, and the second dataset contains LDCT images of the abdomen with two different slice thicknesses (3 mm, 1 mm). We choose 80% data from each anatomical site for training and the remaining 20% for testing. We used the Adam optimizer with a batch size of 16. The learning rate was initially set to 1e -4 and was assigned to decrease by a factor of 2 after every 6000 iterations. "
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,4.0,Result and Discussion,"Comparison with Baseline: This section discusses the efficacy of the proposed weight modulation technique, comparing it with a baseline UNet network (M1) and the proposed weight-modulated convolutional network (M2). The networks were trained using LDCT images from a single anatomical region and tested on images from the same region. Table  Figure  Figure "
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,5.0,Conclusion,This study proposes a novel noise-conditioned feature modulation layer to address the limitations of convolutional denoising networks in handling variability in noise levels in low-dose computed tomography (LDCT) images. The
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,1.0,Introduction,"Magnetic Resonance Imaging (MRI) is an essential non-invasive technique that enables high-resolution and reproducible assessments of structural and functional information, for clinical diagnosis and prognosis, without exposing the patient to radiation. Despite its widely use in clinical practice, MRI still suffers from the intrinsically slow data acquisition process, which leads to uncomfortable patient experience and artefacts from voluntary and involuntary physiological movements  Diffusion models  Diffusion models have been widely applied for inverse problems  Most existing diffusion models, including the original DDPM, SMLD and their variants, are strongly based on the use of Gaussian noise, which provides the 'random walk' for 'hot' diffusion. Cold Diffusion Model  In this work, a novel Cold Diffusion-based MRI Reconstruction method (CDiffMR) is proposed (see Fig.  We propose two novel k -space conditioning strategies to guide the reverse process and to reduce the required time steps. 1) Starting Point Conditioning (SPC). The k -space undersampled zero-filled images, which is usually regarded as the network input, can act as the reverse process starting point for conditioning. The number of reverse time steps therefore depends on the undersamping rate, i.e., the higher k -space undersampling rate (lower AF, easier task), the fewer reverse time steps required. 2) Data Consistency Conditioning (DCC). In every step of the reverse process, data consistency is applied to further guide the reverse process in the correct way. It is note that our CDiffMR is a one-for-all model. This means that once CDiffMR is trained, it can be reused for all the reconstruction tasks, with any reasonable undersampling rates conditioning, as long as the undersampling rate is larger than the preset degraded images x T at the end of forward process (e.g., 1% undersampling rate). Experiments were conducted on FastMRI dataset  • An innovative Cold Diffusion-based MRI Reconstruction methods is proposed. To best of our knowledge, CDiffMR is the first diffusion model-based MRI reconstruction method that exploits the k -space undersampling degradation. • Two novel k -space conditioning strategies, namely SPC and DCC, are developed to guide and accelerate the reverse process. • The pre-trained CDiffMR model can be reused for MRI reconstruction tasks with a reasonable range of undersampling rates."
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,2.0,Methodology,This section details two key parts of the proposed CDiffMR: 1) the optimisation and training schemes and 2) the k -space conditioning reverse process.  min for LogSR schedule (see Fig. 
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,2.1,Model Components and Training,"where M t is the undersampling mask at step t corresponding to SR t , and M 0 = I is the fully-sampling mask (identity map). F and F - The restoration operator R θ (•, t) is an improved U-Net with time embedding module, following the official implementation 1 of Denoising Diffusion Implicit Models  For the training of the restoration operator R θ (•, t), x true is the fully-sampled images randomly sampled from target distribution X . Practically, time step t is randomly chosen from (1, T ] during the training. The driven optimisation scheme reads: min"
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,2.2,K-Space Conditioning Reverse Process,"Two k -space conditioning strategies, SPC and DCC, are designed to guide and accelerate the reverse process. Starting Point Conditioning enables the reverse process of CDiffMR to start from the half way step T instead of step T (see Fig.  Sampling Rate: With the start step T , the reverse process is conditioned by the reverse process of the initial image x T ← F -1 y. The framework of the reverse process follows Algorithm 2 in  x t-1 = x t -D(x 0,t , t) + D(x 0,t , t -1), s.t. t = T ...1. (6)"
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,3.0,Experimental Results,This section describes in detail the set of experiments conducted to validate the proposed CDiffMR. 
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,3.1,Implementation Details and Evaluation Methods,"The experiments were conducted on the FastMRI dataset  Undersampling mask M and M t were generated by the fastMRI official implementation. We applied AF×8 and ×16 Cartesian mask for all experiments. Our proposed CDiffMR was trained on two NVIDIA A100 (80 GB) GPUs, and tested on an NVIDIA RTX3090 (24 GB) GPU. CDiffMR was trained for 100,000 gradient steps, using the Adam optimiser with a learning rate 2e-5 and a batch size 24. We set the total diffusion time step to T = 100 for both LinSR and LogSR degradation schedules. The minimal sampling rate (when t = 100) was set to 1%. For comparison, we used CNN-based methods D5C5  In the quantitative experiments, Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), Learned Perceptual Image Patch Similarity (LPIPS) "
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,3.2,Comparison and Ablation Studies,The quantitative results are reported in Table 
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,4.0,Discussion and Conclusion,"This work has exploited Cold Diffusion-based model for MRI reconstruction and proposed CDiffMR. We have designed the novel KSUD for degradation operator D(•, t) and trained a restoration function R θ (•, t) for de-aliasing under various undersampling rates. We pioneered the harmonisation of the degradation function in reverse problem (k -space undersampling in MRI reconstruction) and the degradation operator in diffusion model (KSUD). In doing so, CDiffMR is able to explicitly learn the k -space undersampling operation to further improve the reconstruction, while providing the basis for the reverse process acceleration. Two k -space conditioning strategies, SPC and DCC, have been designed to guide and accelerate the reverse process. Experiments have demonstrated that k -space undersampling can be successfully used as degradation in diffusion models for MRI reconstruction. In this study, two KSUD schedules, i.e., have been designed for controlling the k -space sampling rate of every reverse time steps. According to Table  In the ablation studies, we have further examined the selection of reverse process starting point T . Figure  We also explored the validity of DDC in the ablation studies. Figure  The proposed CDiffMR heralds a new kind of diffusion models for solving inverse problems, i.e., applying the degradation model in reverse problem as the degradation module in diffusion model. CDiffMR has proven that this idea performs well for MRI reconstruction tasks. We can envision that our CDiffMR can serve as the basis for general inverse problems."
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,1.0,Introduction,"Deformable image registration has always been an important focus in the society of medical imaging, which is essential for the preoperative planning, intraoperative information fusion, disease diagnosis and follow-ups  Transformers have been successfully used in the society of computer vision and have recently made an impact in the field of medical image computing  In this study, we propose a novel motion decomposition Transformer (ModeT) to explicitly model multiple motion modalities by fully exploiting the intrinsic capability of the Transformer structure for deformation estimation. Experiments on two public brain magnetic resonance imaging (MRI) datasets demonstrate our method outcompetes several cutting-edge registration networks and Transformers. The main contributions of our work are summarized as follows: • We propose to leverage the Transformer structure to naturally model the correspondence between images and convert it into the deformation field, "
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,2.1,Network Overview,"The proposed deformable registration network is illustrated in Fig.  The feature maps M 5 and F 5 are sent into the ModeT to generate multiple deformation subfields, and then the generated deformation sub-fields are input into the CWM to obtain the fused deformation field ϕ 1 of the coarsest decoding layer as the initial of the total deformation field φ. The moving feature map M 4 is deformed using φ, and the deformed moving feature map is fed into the ModeT along with F 4 to generate multiple sub-fields, which are input into the CWM to get ϕ 2 . Then ϕ 2 is compounded with previous total deformation field to generate the updated φ. The feature maps M 3 and F 3 go through the similar operations. As the decoding feature maps become finer, the number of motion modes at position p decreases, along with the number of attention heads we need to model. At the F 2 /M 2 and F 1 /M 1 levels, we no longer generate multiple deformation sub-fields, i.e., the number of attention heads in ModeT is 1. Finally, the obtained total deformation field φ is used to warp I m to obtain the registered image. To guide the network training, the normalized cross correlation L ncc  where • is the warping operation, and λ is the weight of the regularization term."
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,2.2,Motion Decomposition Transformer (ModeT),"In deep-learning-based registration networks, a position p in the low-resolution feature map contains semantic information of a large area in the original image and therefore may often have multiple possibilities of different motion modalities. To model these possibilities, we employ a multi-head neighborhood attention mechanism to decompose different motion modalities at low-resolution level. The illustration of the motion decomposition is shown in Fig.  Let F, M ∈ R c×h×w×l stand for the fixed and moving feature maps from a specific level of the hierarchical encoder, where h, w, l denote feature map size and c is the channel number. The feature maps F and M go through linear projection (proj) and LayerNorm (LN )  Q ={Q (1) , Q (2) , . . . , Q (S) }, K ={K (1) , K (2) , . . . , K (S) },  We then calculate the neighborhood attention map. We use c(p) to denote the neighborhood of voxel p. For a neighborhood of size n × n × n, ||c(p)|| = n 3 . The neighborhood attention map of multiple heads is obtained by: where B ∈ R S×n×n×n is a learnable relative positional bias, initialized to all zeros. We pad the moving feature map with zeros to calculate boundary voxels because the registration task sometimes requires voxels outside the field-of-view to be warped. Equation (3) shows how the neighborhood attention is computed for the s-th head at position p, so that the semantic information of voxels on low resolution can be decomposed to compute similarity one by one, in preparation for modeling different motion modalities. Moreover, the neighborhood attention operation narrows the scope of attention calculation to reduce the computational effort, which is very friendly to volumetric processing. The next step is to obtain the multiple sub-fields at this level by computing the regular displacement field weighted via the neighborhood attention map: where ϕ (s) ∈ R h×w×l×3 , V ∈ R n×n×n , and V (value) represents the relative position coordinates for the neighborhood centroid, which is not learned so that the multi-head attention relationship can be naturally transformed into a multicoordinate relationship. With the above steps, we obtain a series of deformation sub-fields for this level: ϕ (1) , ϕ (2) , . . . , ϕ (S) (5)"
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,2.3,Competitive Weighting Module (CWM),"Multiple low-resolution deformation fields need to be reasonably fused when deforming a high-resolution feature map. As shown in Fig.  w (1) , w (2) , . . . , w (S) = W Conv(cat(ϕ (1) , ϕ (2) , . . . , ϕ (S) )), where w (s) ∈ R h×w×l , and ϕ (s) has already been upsampled. W Conv represents the ConvBlock used to calculate weights, as shown in the right part of Fig. "
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,3.0,Experiments,"Datasets. Experiments were carried on two public brain MRI datasets, including LPBA  Evaluation Metrics. To quantitatively evaluate the registration performance, Dice score (DSC)  56.7 ± 1.5 1.38 ± 0.09 < 0.00001% 70.1 ± 6.2 1.72 ± 0.12 < 0.0004% VM  where lr m represents the learning rate of m-th epoch and lr init = 1e -4 represents the learning rate of initial epoch. We set the batch size as 1, M as 30 for training. In the inference phase, our method averagely took 0.56 second and 9GB memory to register a volume pair of size 160 × 192 × 160. Comparison Methods. We compared our method with several state-of-theart registration methods: (1) SyN  Quantitative and Qualitative Analysis. The numerical results of different methods on datasets Mindboggle and LPBA are reported in Table "
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,4.0,Conclusion,"We present a motion decomposition Transformer (ModeT) to naturally model the correspondence between images and convert this into the deformation field, which improves the interpretability of the deep-learning-based registration network. The proposed ModeT employs the multi-head neighborhood attention mechanism to identify various motion patterns of a voxel in the low-resolution feature map. Then with the help of competitive weighting module and pyramid structure, the motion modes contained in a voxel can be gradually fused and determined in the coarse-to-fine pyramid decoder. The experimental results have proven the superior performance of the proposed method. In our future study, we attempt to implement our ModeT in a more efficient way, and also investigate more effective fusion strategy to combine the displacement field from multiple attention heads."
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,,Acknowledgements,. This work was supported in part by the 
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,1.0,Introduction,"Computed tomography (CT) image is reconstructed from sinogram, which is tomographic raw data collected from detectors. According to kernels being used for CT image reconstruction, there is a trade-off between spatial resolution and noise, and it affects intensity and texture quantitative values  However, once CT image is reconstructed with a specific kernel, sinogram is usually removed because of its large capacity and limited storage. Therefore, clinicians have difficulty to analyze qualitative or quantitative results without CT image reconstructed with different kernels, and this limitation reveals on retrospective or longitudinal studies that cannot control technical parameters, particularly  Recently, many studies have achieved improvement in kernel conversion  Our contributions are as follows: "
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,2.1,Related Work,"In deep learning methods for CT kernel conversion, there were proposed methods using convolutional neural networks  Generator-guided discriminator regularization (GGDR)  Recently, it has been shown that dense contrastive learning can have a positive effect on learning dense semantic labels. In dense prediction tasks such as object detection and semantic segmentation "
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,2.2,Generator-Guided Contrastive Learning,GGDR 
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,,Multi-Domain Image-To-Image Translation.,"We apply generator-guided contrastive learning (GGCL) to StarGAN  where L D and L G are the discriminator and generator losses, respectively. They both have L adv , which is the adversarial loss. L r cls and L f cls are the domain classification losses for a real and fake image, respectively. L cyc , which is the cycle consistency loss, has an importance for the translated image to maintain the structure of source image."
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,,Patch-Based Contrastive Learning.,"Our method is to add PatchNCE loss  Total Objective. GGCL follows the concept of GGDR, which the generator supervises the semantic representations to the discriminator, so it is a kind of the discriminator regularization. Discriminator conducts real/fake classification, domain classification and semantic label map segmentation, so it can be also a kind of the multi-task learning  where λ cls , λ cyc and λ ggcl are hyper-parameters that weight the importance of domain classification loss, cycle consistency loss and GGCL loss, respectively. We used λ cls = 1, λ cyc = 10 and λ ggcl = 2 in our experiments. 3 Experiments and Results Implementation Details. We maintained the original resolution 512 × 512 of CT images and normalized their Hounsfield unit (HU) range from [-1024HU ~3071HU] to [-1 ~1] for pre-processing. For training, the generator and the discriminator were optimized by Adam  Architecture Improvements. Instead of using original StarGAN "
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,3.2,Comparison with Other Image-to-Image Translation Methods,We compared GGCL with two-domain image-to-image translation methods such as CycleGAN  Qualitative Results. We showed the qualitative results of image-to-image translation methods including GGDR and GGCL applied to StarGAN for kernel conversion from Siemens into Siemens (see Fig.  Quantitative Results. We showed the quantitative results of image-to-image translation methods including GGDR and GGCL applied to StarGAN for kernel conversion from Siemens into the Siemens (see Table 
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,3.3,Ablation Study,"We implemented ablation studies about the number of patches, size of pooling and loss weight for GGCL to find out the best performance. We evaluated our method while preserving the network architecture. Ablation studies were also evaluated by PSNR and SSIM. All ablation studies were to change one factor and the rest of them were fixed with their best configurations. The results about the number of patches showed improvement when the number of patches was 64 (see Table "
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,4.0,Discussion and Conclusion,"In this paper, we proposed CT kernel conversion method using multi-domain imageto-image translation with generator-guided contrastive learning (GGCL). In medical domain image-to-image translation, it is important to maintain anatomical structure of the source image while translating style of the target image. However, GAN based generation has limitation that the training process may be unstable, and the results may be inaccurate so that some fake details may be generated. Especially in unsupervised manner, the anatomical structure of the translated image relies on cycle consistency mainly. If trained unstably, as the translated image to the target domain would be inaccurate, the reversed translated image to the original domain would be inaccurate as well. Then, the cycle consistency would fail to lead the images to maintain the anatomical structure. CycleGAN "
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,1.0,Introduction,"The ultimate goal of entire cortex registration (ECR) is to achieve functional region alignment across subjects. However, existing software often sacrifices the local functional alignment accuracy to accomplish ECR  To address this issue, existing works have introduced the single-scale graph structure to represent and align GHs  (2) We design a MsHE module to extract the high-order relations among GHs at different scales and a MsHM module to propagate GHs features to align GHs. Moreover, the feature distribution of GH optimized by the inter-scale semantic consistency further improves the alignment accuracy. (3) Extensive experiments verify that the proposed matching framework improves the GH alignment remarkably and outperforms state-of-the-art methods. "
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,2.0,Method,"The overview of our proposed framework is shown in Fig.  in the MsHE module (Fig.  where θ represents the neural network parameters, the H is the incidence matrix of hypergraph  We use a neural network to predict C (k ) . The details will be introduced below."
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,2.1,Multi-scale Hypergraph Establishment (MsHE),"As shown in Fig.  s/t , we capture the topological relations among brain atlases to serve as the incidence matrix. The element H (k ) i,j indicates that the i -th GH is included in the j -th hyperedge (brain region or GH). Notably, when k =3, the row sum of s/t is in the diagonal format. As for the hypergraph nodes V s/t , i.e., GHs, we capture various cues in the subject to serve as the raw node feature. Specifically, considering the limited representation of the single vertex knowledge, we expand each GH on the surface by two rings, resulting in a total of 19 vertices as the GHs' raw features. Each vertex features contain three-dimensional coordinates (×3), normal vector coordinates (×3), curvature (×1), convexity (×1), and cortical thickness (×1). Then, the aforementioned features are contacted to obtain the raw node features , where m/n represent the number of source/target subject GHs, respectively. After that, the raw features are sent to Dynamic Graph CNN  s/t . The proposed multi-scale hypergraphs can well model significant surface morphological information and multi-scale GH relations for better GH alignment."
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,2.2,Multi-scale Hypergraph Matching (MsHM),"As shown in Fig.  s/t , and then sent to MsHM to solve the multi-scale matching with four sub-stages, including hyperedge relation learning, self-hyperedge reasoning, bipartite hypergraph matching, and cross-hyperedge reasoning. Hyperedge Relation Learning: To dynamically learn hyperedge relation with better structural information, we propose hyperedge relation learning to model the high-order relation among hyperedges, instead of directly using the handcraft hyperedge weight metric W s/t  where f emb is a transformer-based feature embedding function."
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,,Self-hyperedge Reasoning:,We utilize a self-hyperedge reasoning network to capture the self-correlation of hyperedge features. Propagating features achieve this by hyperedge weight matrix within each hypergraph. It can be written as: where s/t ∈ R D×D denotes the learnable parameters of self-hyperedge reasoning network. And σ(•) is the nonlinear activation function.
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,,Bipartite Hypergraph Matching:,"We use bipartite matching to determine a soft correspondence matrix between two subjects, which is achieved using the following expression: , where Gbm consists of an Affinity layer, an Instance normalization layer, a quadratic constrain (QC) layer, and a Sinkhorn layer. Initially, the affinity matrix is computed as t , where M (k) ∈ R D×D is the learnable parameter matrix in the affinity layer. Next, we apply instance normalization  where , and this loss can optimize the Eq. 1. The k -scale soft assignment matrix of the MsHM l -th layer output is"
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,,Cross-Hyperedge Reasoning:,"We further enhance the hyperedge features by exploring cross-correlation through cross-hyperedge reasoning. Different from self-hyperedge reasoning, the proposed cross-hyperedge reasoning enables subject-aware message propagation, facilitating effective interaction between subjects. The more similar a pair of hyperedges is between two subjects, the better features will be aggregated in better alignment. It can be written as: where f cross consists of a feature concatenate and a fully connected layer. Finally, the new GHs features with a symmetric normalization can be written as: where σ(•) is the nonlinear activation function. s/t are the diagonal node degree matrix and hyperedge degree matrix, respectively. Φ ∈ R D×D denotes the learnable parameters of cross-hyperedge reasoning network."
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,2.3,Inter-Scale Consistency (ISC),"To avoid the potential disagreement among different scales, it is critical to introduce the consistency constraint across scales, which achieves collaborative optimization in different scales and prevents sub-optimal alignment. Specifically, we propose a novel ISC mechanism that leverages the local properties of the tree structure. For a parent hyperedge e p and its child hyperedges {e q c } Q q=1 (Q is the number of children nodes belonging to the same parent node.), which represent the same brain region at different scales, we enforce semantic-level consistency between the child hyperedges and their parent hyperedge to facility the reliable model learning, which is denoted as follows: where L scale denotes the loss function that enforces inter-scale semantic consistency among source/target subjects tree structure T s/t . By incorporating this loss, the model learns to maintain consistent semantic information across different scales for hyperedges corresponding to the same brain region."
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,2.4,Model Optimization,"In the training of this work, we introduce a hyperparameter to add up L m and L scale . Then, the overall train loss for the GH alignment model is denoted as: where β is a hyperparameter to control the intensity. In testing, we utilize the Hungarian algorithm "
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,3.1,Experimental Setup,"Dataset: We evaluate our proposed framework effectiveness by conducting experiments on the GH alignment dataset, which includes the ground truth of GH correspondences across 250 subjects. Each GH contains brain atlas information, various morphological features from T1-weighted (T1w) MRI, and task activation vectors obtained from the task fMRI (tfMRI). The Five Lobe Atlas consists of 10 brain regions, while the DK Atlas includes 66 brain regions. The T1w MRI and tfMRI are acquired from the WU-Minn Human Connectome Project (HCP) consortium  Evaluation: In this study, we adopt the accuracy, the correlation (×10 -2 ), and the mean geodesic errors  Implementation Details To train our model, we utilize the Adam optimizer "
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,3.2,Experimental Results,Comparison with State-of-the-Arts: We present the comparison results in Table  Ablation Studies: Table 
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,,Sensitivity Analysis:,We conduct experiments with varying hyperparameters to investigate the sensitivity of β in Eq. 8 and record the results in Table  Qualitative Analysis: Figure 
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,4.0,Conclusion,"In this paper, we propose a novel framework H 2 GM for brain landmark alignment. Specifically, H 2 GM consists of a MsHE module for constructing the multiscale hypergraphs, a MsHM module for matching them, and ISC for incorporating the semantic consistency among scales. Experimental results demonstrate that our proposed H 2 GM outperforms existing approaches significantly."
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,1.0,Introduction,"Recent deep learning-based registration methods have shown great potential in solving medical image registration problems  DA has widely been studied for classification and segmentation tasks  Contributions. We introduce two complementary strategies to denoise the Mean Teacher for domain adaptive point cloud registration, addressing the above limitations (see Fig.  2) The selection process considers both teacher and student predictions and can thus prevent detrimental supervision by the teacher. Our second strategy follows the intuition that good teachers should not pose problems to which they do not know the solution. Instead, they should come up with novel tasks with precisely known solutions. Consequently, we propose a completely novel teacher paradigm, where predicted deformations by the teacher are used to synthesize new training pairs for the student, consisting of the original moving inputs and their warps. These input pairs come with precise noise-free displacement labels and significantly differ from static hand-crafted synthetic deformations "
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,2.1,Problem Setup and Standard Mean Teacher,"In point cloud registration, we are given fixed and moving point clouds F ∈ R NF×3 , M ∈ R NM×3 and aim to predict a displacement vector field ϕ ∈ R NM×3 that spatially aligns M to F as M + ϕ. We address the task in a domain adaptation setting with training data comprising a labeled source dataset S of triplets (M s , F s , ϕ s ) and a shifted unlabeled target dataset T of tuples (M t , F t ). While the formulation of our method is agnostic to the specific domain shift between S and T , in this work, we generate the source samples on the fly as random synthetic deformations of the target clouds using a fixed handcrafted deformation function def : R N ×3 → R N ×3 , i.e. source triplets are given as (def Note that def preserves point correspondences enabling ground truth computation through point-wise subtraction. Given the training data, we aim to learn a function f that predicts deformation vector fields as φ = f (M , F ) with optimal performance in the target domain. Baseline Mean Teacher. To solve the problem, the standard Mean Teacher framework  consisting of the supervised loss L sup on source data and the consistency loss L con on target data, weighted by λ 1 and λ 2 . L con guides the learning of the student in the target domain with pseudo-supervision from the teacher, which, as a temporal ensemble, is expected to be superior to the student. Nonetheless, predictions by the teacher can still be noisy and inaccurate, limiting the efficacy of the adaptation process."
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,2.2,Chamfer Distance-Based Filtering of Pseudo Labels,"In a worst-case scenario, the student might predict an accurate displacement field φt , which is strongly penalized by the consistency loss due to an inaccurate teacher prediction φ t . To prevent such detrimental supervision, we aim to select only those teacher predictions for supervision that are superior to the corresponding student predictions, which, however, is complicated by the absence of ground truth. We, therefore, propose to assess the quality of student and teacher registrations by measuring the similarity/distance between fixed and warped moving clouds, with higher similarities/lower distances indicating more accurate registrations. Among existing similarity measures, we opt for the symmetric Chamfer distance  (2) While we experimentally found the Chamfer distance insufficient as a direct loss function -presumably due to sparse differentiability and susceptibility to local minima, we still observed a strong correlation between Chamfer distance and actual registration error, making it a suitable choice for our purposes. We also explored other measures (Laplacian curvature  and reformulate the consistency loss in Eq. 1 as (4)"
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,2.3,Synthesizing Inputs with Noise-Free Supervision,"While the above filtering strategy mitigates detrimental supervision, the selected pseudo labels are still inaccurate.  To our knowledge, there is no prior work with a similarly ""generative"" teacher model. Altogether, we train the student network by minimizing the loss 3 Experiments"
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,3.1,Experimental Setup,"Datasets. We evaluate our method for inhale-to-exhale registration of lung vessel point clouds on the public PVT dataset  Implementation Details. The registration network f is implemented as the default 4-scale architecture of PointPWC-Net  Optimization is performed with the Adam optimizer. We first pre-train the network on source data (batch size 4) for 160 epochs and subsequently minimize the joint loss (Eq. 6) for 140 epochs, both with a constant learning rate of 0.001, which requires up to 11 GB and 13/23 h on an RTX2080. For joint optimization, we use mixed batches of 4 source and 4 target samples, set λ 1 = λ 2 = λ 3 = 10, and the EMA-parameter to α = 0.996. While the original PVT data pairs represent the target domain in all experiments, we consider two variants of the function def to synthesize source data pairs: a realistic task-specific 2-scale random field similar to  Comparison Methods. 1) The source-only model is exclusively trained on source data without DA. 2) We adopt the standard Mean Teacher  Metrics. We interpolate the predicted displacements from the moving input cloud to the annotated moving landmarks with an isotropic Gaussian kernel (σ = 5 mm) and measure the target registration error (TRE) with respect to the fixed landmarks. To assess the smoothness of the predictions, we interpolate the sparse displacement fields to the underlying image grid and measure the standard deviation of the logarithm of the Jacobian determinant (SDlogJ)."
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,3.2,Results,"Quantitative results are shown in Table  2) The standard Mean Teacher proves effective under the weaker domain shift (-40.7% TRE compared to source-only) but only achieves a slight improvement of 16.0% in the more challenging scenario, where pseudo labels by the teacher are naturally noisier, in turn limiting the efficacy of the adaptation process. 3) Our proposed strategy to filter pseudo labels (ours w/o L syn ) improves the standard teacher and its uncertainty-aware extension, particularly notable under the more severe domain shift (-59.8/-55.0% TRE). 4) Synthesizing novel data pairs with the teacher (ours w/o L con ) alone is slightly inferior to the standard teacher for realistic deformations in the source domain but substantially superior for simple rigid transformations. 5) Combining our two strategies yields further considerable improvements to TREs of 2.31 and 2.38 mm, demonstrating their complementarity. Thus, our method improves the standard Mean Teacher by 13.5/62.8%, outperforms all competitors by statistically significant margins (p < 0.001 in a Wilcoxon signed-rank test), and sets a new state-of-the-art accuracy. Remarkably, our method achieves almost the same accuracy for simple rigid transformations in the source domain as for complex, realistic deformations. Thus, it eliminates the need for designing taskspecific deformation models, which requires strong domain knowledge. Qualitative results are presented in Fig. "
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,4.0,Conclusion,"Our work addressed domain adaptive point cloud registration to bridge the gap between synthetic source and real target deformations. Starting from the established Mean Teacher paradigm, we presented two novel strategies to tackle the noise of pseudo labels from the teacher model, which is a persistent, significant limitation of the method. Specifically, we 1) proposed to prevent detrimental supervision through the teacher by filtering pseudo labels according to Chamfer distances of student and teacher registrations and 2) introduced a novel teacherstudent paradigm, where the teacher synthesizes novel training data pairs with perfect noise-free displacement labels. Our experiments for lung vessel registration on the PVT dataset demonstrated the efficacy of our method under two scenarios, outperforming the standard Mean Teacher by up to 62.8% and setting a new state-of-the-art accuracy (TRE = 2.31 mm). As such, our method even favorably compares to popular image-based deep learning methods (VoxelMorph  In particular, medical point cloud registration, currently primarily focusing on lung anatomies, still needs to be investigated for other anatomical structures (abdomen, brain) in future work, which might benefit from our generic approach. Second, our method is conceptionally transferable to dense image registration (e.g., intensity-based similarity metrics "
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,1.0,Introduction,"Deformable image registration is one of the fundamental tasks in computer vision and has been widely used in medical image processing. In recent years, deep learning methods based on convolutional neural networks are widely applied in deformable image registration. Balakrishnan et al.  Inspired by the capabilities of Transformer in NLP, recent researchers have extended Transformer to computer vision tasks  In this paper, we propose a novel Pyramid-Iterative Vision Transformer (PIViT) by combining Swin Transformer-based long-range correlation decoder and the proposed pyramid-iterative registration framework shown in Fig.  Extensive experiments on 3D brain MRI and liver CT registration tasks demonstrate that PIViT achieves state-of-the-art performance in terms of accuracy but consumes less time and parameters."
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,2.0,The Proposed Method,"In this section, we first propose a novel pyramid-iterative registration framework to solve large deformation image registration. The pyramid-iterative registration framework combines the advantages of iteration and pyramid registration framework to achieve fast and accurate registration. Then, we introduce a long-range correlation decoder based on Swin Transformer into the iterative registration stage of the proposed framework and utilize the global receptive field of the Swin Transformer to capture global correlations, thereby implementing high accurate and fast registration. "
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,2.1,Pyramid-Iterative Registration Framework,"As shown in Fig.  Dual-Stream Feature Extraction: Similar to pyramid registration network, the proposed framework utilizes a weight-sharing feature encoder to construct feature pyramids for the fixed image I f and the moving image I m , respectively. At the i th step (i ∈ [1 • • • N ]), the feature maps of I f and I m are formulated as F i f and F i m , respectively. The weight-sharing feature encoder reuses the same network blocks to extract the feature maps F i f and F i m without adding parameters or complicating the training process while ensuring that F i f and F i m are in the same feature space. Low-Scale Iterative Registration: The pyramid-iterative registration uses two different decoding modules at different scales. To capture large deformation, we adopt low-scale feature maps to obtain the coarse distribution of large deformation fields without considering the fine distribution in this paper. Therefore, at the last N th level of feature pyramid, deformation field is predicted from F N f and F N m multiple times through an iterative structure. Similar to iterativebased registration methods, F N m is warped by the predicted deformation field φ N t , where t is the number of iterations. The warped F N,t m and F N f are used for the next iteration. In the first iteration, the decoder obtains the initial deformation field φ N 1 , and in the subsequent iterations, the residual deformation field Δφ N t is obtained in each prediction and the updated overall deformation field φ N t is obtained. This procedure can be formulated as: where T is the upper limit of iteration, • denotes warping the feature map with deformation fields, and + denotes element-wise summation of deformation fields. Compared with other iterative registration methods, the advantage of iterating only at the N th level is that there is no need to re-extract image features, thus the computational complexity and time consumption of our method can be greatly reduced. This can greatly accelerate the speed of model training and deformation field prediction, and better solve large deformation. Multi-scale Pyramid Registration: After the implementation of low-scale iterative registration, the deformation field φ N T is rescaled by a factor of 2 and the rescaled flow φ N is obtained. The subsequent process is the same as that of the pyramid registration method. At each level, warped feature ) are concatenated and the residual deformation field Δφ i is predicted by 3D convolution. Δφ i is used to update φ i+1 so as to obtain the deformation field φ i corresponding to the i th layer. φ i is rescaled by a factor of 2 and warps moving feature F i-1 m . The purpose of introducing multi-scale pyramid registration is to supplement the lack of fine information caused by only using low-scale features in the iterative registration stage. This process is repeated at each level of the feature pyramid until the deformation field is rescaled to the original image resolution. Finally, the pyramid-iterative registration framework obtains the predicted global deformation field."
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,2.2,Long-Range Correlation Decoder,"To capture large deformation at low-scale registration, the study of the decoder is very essential. Therefore, we propose a long-range correlation decoder (LCD) in the iterative registration phase. As shown in Fig.  Current Transformer-based registration methods usually directly migrate the Transformer structure to the 3D image registration task, which leads to a large number of parameters and a remarkably long inference time. In contrast, the proposed PIViT models long-range correlations in low-scale iterative registration with LCD to warp corresponding voxels between feature maps to spatial neighborhoods, thus it is not necessary to use the Transformer on large feature maps at high scales. In addition, LCD also removes position embedding, only uses single-head self-attention and reduces the number of channels. These operations accelerate the speed of PIViT and significantly reduce parameters. "
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,2.3,Loss Function,"PIViT is an unsupervised end-to-end registration network. In this section, we design a loss function to train the proposed network. In the final stage of pyramid registration, PIViT obtains the deformation field φ between I m and I f and the warped image I w = I m • φ by using the differential operation based on the spatial transformer network  In order to ensure the continuity and smoothness of the deformation field φ in space, a regular term on its spatial gradient is introduced. The complete loss function is: where λ is the regularization hyperparameter."
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,3.0,Experiments,"Data and Pre-processing: We evaluate the performance of PIViT on brain MRI datasets and liver CT datasets. In the experiments, we compare the proposed method with commonly used 3D convolutional registration methods Voxelmorph  Implementation: We set λ to 1 for PIViT to guarantee the smoothness of the deformation field. Algorithm runtimes are computed on an NVIDIA GeForce RTX 3090 GPU and an Intel(R) Xeon(R) Silver 4210R CPU. We implement the model using Keras with a Tensorflow  Results:  As shown in Table  In addition to the superior Dice score, another advantage of the proposed PIViT is its fast and lightweight registration. Table  The visualization result of the experiment on the LPBA dataset is shown in Fig. "
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,,Number of Iterations and Decoder Type:,"In this section, we explore how the number of iterations and decoder type of block in the long-range correlation decoder affect the registration performance. We select three different blocks, i.e. LCD, CNN and GRU "
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,4.0,Conclusion,"In this paper, we propose an unsupervised pyramid-iterative vision Transformer (PIViT) for large deformation image registration. PIViT is an iterative and pyramid composite framework to achieve fine registration of large deformable images by iterative registration of low-scale feature maps and pyramid feature supplementation on high-scale feature maps. Furthermore, in the iterative decoding stage, a Swin Transformer-based long-range correlation decoder is introduced to capture the long-distance dependencies between feature maps, which further improves the ability to handle large deformation. Experiments on brain MRI scans and liver CT scans demonstrate that our method can accurately register 3D large deformation medical images. Furthermore, our method has significant advantages in terms of parameters and time, which can make it more suitable for time-sensitive tasks."
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_57.
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,1.0,Introduction,"Medical imaging is essential during diagnosis, surgical planning, surgical guidance, and follow-up for treating brain pathology. Images from multiple modalities are typically acquired to distinguish clinical targets from surrounding tissues. For example, intra-operative ultrasound (iUS) imaging and Magnetic Resonance Imaging (MRI) capture complementary characteristics of brain tissues that can be used to guide brain tumor resection. However, as noted in  Medical image synthesis aims to predict missing images given available images. Deep-learning based methods have reached the highest level of performance  To tackle this challenge, unified approaches have been proposed. These approaches are designed to have the flexibility to handle incomplete image sets as input, improving practicality as only one network is used for generating missing images. To handle partial inputs, some studies proposed to fill missing images with arbitrary values  have not yet been extended to multi-modal settings to synthesize missing images. In this work, we introduce Multi-Modal Hierarchical Latent Representation VAE (MHVAE), the first multi-modal VAE approach with a hierarchical latent representation for unified medical image synthesis. Our contribution is four-fold. First, we integrate a hierarchical latent representation into the multi-modal variational setting to improve the expressiveness of the model. Second, we propose a principled fusion operation derived from a probabilistic formulation to support missing modalities, thereby enabling image synthesis. Third, adversarial learning is employed to generate realistic image synthesis. Finally, experiments on the challenging problem of iUS and MR synthesis demonstrate the effectiveness of the proposed approach, enabling the synthesis of high-quality images while establishing a mathematically grounded formulation for unified image synthesis and outperforming non-unified GAN-based approaches and the state-of-the-art method for unified multi-modal medical image synthesis."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,2.0,Background,Variational Auto-Encoders (VAEs). The goal of VAEs  where KL[q||p] is the Kullback-Leibler divergence between distributions q and p.
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,,Multi-modal Variational Auto-Encoders (MVAE).,Multi-modal VAEs  (2)
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,3.0,Methods,"In this paper, we propose a deep multi-modal hierarchical VAE called MHVAE that synthesizes missing images from available images. MHVAE's design focuses on tackling three challenges: (i) improving expressiveness of VAEs and MVAEs using a hierarchical latent representation; (ii) parametrizing the variational posterior to handle missing modalities; (iii) synthesizing realistic images."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,3.1,Hierarchical Latent representation,"be a complete set of paired (i.e. co-registered) images of different modalities where M is the total number of image modalities and N the number of pixels (e.g. M = 2 for T 2 MRI and iUS synthesis). The images x i are assumed to be conditionally independent given a latent variable z. Then, the conditional distribution p θ (x|z) parameterized by θ can be written as: Given that VAEs and MVAEs typically produce blurry images, we propose to use a hierarchical representation of the latent variable z to increase the expressiveness the model as in HVAEs  where p(z L ) = N (z L ; 0, I) is an isotropic Normal prior distribution and the conditional prior distributions p θ l (z l |z >l ) are factorized Normal distributions with diagonal covariance parameterized using neural networks, i.e. p θ l (z l |z >l ) = N (z l ; μ θ l (z >l ), D θ l (z >l )). Note that the dimension of the finest latent variable z 1 ∈ R H1 is similar to number of pixels, i.e. H 1 = O(N ) and the dimension of the latent representation exponentially decreases with the depth, i.e. H L H 1 . Reusing Eq. 1, the evidence log (p θ (x)) is lower-bounded by the tractable variational ELBO L ELBO MHVAE (x; θ, φ): where q φ (z|x) = L l=1 q φ (z l |x, z >l ) is a variational posterior that approximates the intractable true posterior p θ (z|x)."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,3.2,Variational Posterior's Parametrization for Incomplete Inputs,"To synthesize missing images, the variational posterior (q φ (z l |x, z >l )) L l=1 should handle missing images. We propose to parameterize it as a combination of unimodal variational posteriors. Similarly to MVAEs, for any set π ⊆ {1, ..., M } of images, the conditional posterior distribution at the coarsest level L is expressed where p(z L ) = N (z L ; 0, I) is an isotropic Normal prior distribution and q φL (z|x i ) is a Normal distribution with diagonal covariance parameterized using CNNs. For the other levels l ∈ {1, .., L -1}, we similarly propose to express the conditional variational posterior distributions as a product-of-experts: where q φ i l (z l |x i , z >l ) is a Normal distribution with diagonal covariance parameterized using CNNs, i.e. ). This formulation allows for a principled operation to fuse content information from available images while having the flexibility to handle missing ones. Indeed, at each level l ∈ {1, ..., L}, the conditional variational distributions q PoE φ l ,θ l (z l |x π , z >l ) are Normal distributions with mean μ φ l ,θ l (x π , z >l ) and diagonal covariance D φ l ,θ l (x π , z >l ) expressed in closed-form solution  with D θL (z >L ) = I and μ θL (z >L ) = 0."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,3.3,Optimization Strategy for Image Synthesis,"The joint reconstruction and synthesis optimization goal is to maximize the expected evidence E x∼p data [log(p(x))]. As the ELBO defined in Eq. 5 is valid for any approximate distribution q, the evidence, log(p θ (x)), is in particular lowerbounded by the following subset-specific ELBO for any subset of images π: Hence, the expected evidence E x∼p data [log(p(x))] is lower-bounded by the average of the subset-specific ELBO, i.e.: Consequently, we propose to average all the subset-specific losses at each training iteration. The image decoding distributions are modelled as Normal with variance σ, i.e. p θ (x i |z 1 ) = N (x i ; μ i (z 1 ), σI), leading to reconstruction losses log(p θ (x i |z 1 )), which are proportional to ||x iμ i (z 1 )|| 2 . To generate sharper images, the L 2 loss is replaced by a combination of L 1 loss and GAN loss via a PatchGAN discriminator  Following standard practices "
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,4.0,Experiments,"In this section, we report experiments conducted on the challenging problem of MR and iUS image synthesis. Data. We evaluated our method on a dataset of 66 consecutive adult patients with brain gliomas who were surgically treated at the Brigham and Women's hospital, Boston USA, where both pre-operative 3D T2-SPACE and pre-dural opening intraoperative US (iUS) reconstructed from a tracked handheld 2D probe were acquired. The data will be released on TCIA in 2023. 3D T2-SPACE scans were affinely registered with the pre-dura iUS using NiftyReg  for the encoder and decoder from MobileNetV2  Evaluation. Since paired data was available for evaluation, standard supervised evaluation metrics are employed: PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity), and LPIPS  Ablation Study. To quantify the importance of each component of our approach, we conducted an ablation study. First, our model (MHVAE) was compared with MVAE, the non-hierarchical multi-modal VAE described in  Overall, this set of experiments demonstrates that variational auto-encoders with hierarchical latent representations, which offer a principled formulation for fusing multi-modal images in a shared latent representation, are effective for image synthesis."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,5.0,Discussion and Conclusion,"Other Potential Applications. The current framework enables the generation of iUS data using T 2 MRI data. Since image delineation is much more efficient on MRI than on US, annotations performed on MRI could be used to train a segmentation network on pseudo-iUS data, as performed by the top-performing teams in the crossMoDA challenge "
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,,Conclusion and Future Work.,"We introduced a multi-modal hierarchical variational auto-encoder to perform unified MR/iUS synthesis. By approximating the true posterior using a combination of unimodal approximates and optimizing the ELBO with multi-modal and uni-modal examples, MHVAE demonstrated state-of-the-art performance on the challenging problem of iUS and MR synthesis. Future work will investigate synthesizing additional imaging modalities such as CT and other MR sequences."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 43.
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,1.0,Introduction,"Deformable image registration  To address this issue, we incorporate a Self-supervised Anatomical eMbedding (SAM)  Registration has also been formulated as a discrete optimization problem  We propose SAMConvex, a coarse-to-fine discrete optimization method for CT registration. Specifically, SAMConvex presents two main components: (1) a discriminative feature extractor that encodes global and local embeddings for each voxel; (2) a lightweight correlation pyramid that constructs multi-scale 6D cost volume by taking the inner product of SAM embeddings. It enjoys the strengths of state-of-the-art accuracy (Sect. 3.2), good generalization (Sect. 3.2 and Sect. 3.3) and fast runtime (Sect. 3.2). "
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,2.1,Problem Formulation,"Given a source image I s : Ω s → R and a target image I t : Ω t → R within a spatial domain Ω ⊂ R n , our goal is to find the spatial transformation ϕ - where the transformation model is the displacement vector field (DVF) ϕ -1 = Id + u with Id being the identity transformation, the E D is the similarity term measuring the similarity between I t and warped image is the diffusion regularizer that advocates the regularity of ϕ -1 . λ weights between the data matching term and the regularization term."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,2.2,Decoupling to Convex Optimizations,"We conduct the optimization scheme proposed in  the optimization then can be decomposed into two sub-optimization problems ( with each can be solved via global optimizations. By alternatively optimizing the two subproblems and progressively reducing θ during the alternative optimization, we get v ≈ û and obtain the solution of Eq. 1 as ϕ -1 = Id + v. To be noted, the first problem in Eq. 3 can be solved point-wise because the spatial derivatives of v is not involved. We can search over the cost volume of each point to obtain the optimal solution of the first problem in Eq. 3. For the second problem, we are inspired by mean-field inference "
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,2.3,"E D (•, •) Using Global and Local Semantic Embedding","Presumably, handling complex registration tasks rely on: (1) distinct features that are robust to inter-subject variation, organ deformation, contrast injection, and pathological changes, and (2) global/contextual information that can benefit the registration accuracy in complex deformation. To achieve these goals, we adopt self-supervised anatomical embedding (SAM)  To be specific, given an image I ∈ R H×W ×D , we utilize a pre-trained SAM model that outputs a global embedding We resample the global embedding with bilinear interpolation to match the shape of f global to f local and concatenate them to get We normalize f global and f local before concatenation and adopt the dot product < •, • > as the similarity measure in the following part, with a higher value indicating better alignment. With f SAM , we construct E D (•, •) as the cost volume in the SAM feature space within a neighborhood of [-N, N ], where N represents searching radius. Given two images I 0 and I 1 , the resulting E D (•, •) can then be formulated as where f 0 SAM and f 1 SAM are the SAM embedding of I 0 and I 1 , respectively. x is any voxel in the image domain and d is the voxel displacement within the neighborhood."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,2.4,Coarse-to-Fine Optimization Strategy,"Since E D (•, •) is computed within the neighborhood, the magnitude of deformation is bounded by the size of the neighborhood. Our approach to addressing this issue is based on a surprisingly simple but effective observation: performing registration based on a coarse-to-fine scheme with a small search range at each level instead of a large search range at one resolution benefits to 1) significantly improve the efficiency such as low computation burden and fast running time; 2) enlarge receptive field and improve registration accuracy. We first build an image pyramid of I s and I t . At each resolution, we warp the source image with the composed deformation computed from all the previous levels (starting from the coarsest resolution), transform the warped image and target image to the SAM space, and conduct the decoupling to convex optimizations strategy to obtain the deformation between the warped image and target at the current resolution. With such a coarse-to-fine strategy, we estimate a sequence of displacement fields from a starting identity transformation. The final field is computed via the composition of all the displacement fields  To be noted, the cost volume at each level is computed by taking the inputs of {1, 1  2 , 1 4 } resolution. Adding one coarser level consumes less computation than doubling the size of the neighborhood at the fine resolution but yields the same search range."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,2.5,Implementation Detail,"We conduct the registration in 3 levels of resolutions. At each level, we solve Eq. 3 via five alternative optimizations with 1 2θ = {0.003, 0.01, 0.03, 0.1, 0.3, 1}. To further improve the registration performance, we append a SAM-based instance optimization after the coarse-to-fine registration with a learning rate of 0.05 for 50 iterations. It solves an instance-specific optimization problem "
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,3.1,Datasets and Evaluation Metrics,"We split Abdomen and HeadNeck into a training set and test set to accommodate the requirement of the training dataset of comparing learning-based methods. Lung dataset contains 35 CT pairs, which is not sufficient for developing learningbased methods. Hence, it is only used as a testing set for optimization-based methods. All the methods are evaluated on the test set. The SAM is pre-trained on NIH Lymph Nodes dataset "
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Inter-patient Task on Abdomen:,The Abdomen CT dataset 
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Inter-patient Task on HeadNeck:,The HeadNeck CT dataset 
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Intra-patient Task on Lung:,"The images are acquired as part of the radiotherapy planning process for the treatment of malignancies. We collect a lung 4D CT dataset containing 35 patients, each with inspiratory and expiratory breath-hold image pairs, and take this dataset as an extra test set. Each image has labeled malignancies, and we try to align two phases for motion estimation of malignancies. The images are resampled to a spacing of 2 × 2 × 2 mm and cropped to 256 × 256 × 112. All images are used as testing cases. Evaluation Metrics: We use the average Dice score (DSC) to evaluate the accuracy and compute the standard deviation of the logarithm of the Jacobian determinant (SDlogJ) to evaluate the plausibility of deformation fields, also comparing running time (T test ) on the same hardware."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,3.2,Registration Results,"We compare with five widely-used and top-performing deformable registration methods, including NiftyReg  As shown in Table  To better understand the performance of different deformable registration methods, we display organ-specific results of the inter-patient registration task of abdomen CT in Fig. "
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,3.3,Ablation Study on SAMConvex,"Loss Landscape of SAM. We explore the loss landscapes of SAM-global, SAMlocal, SAM (SAM-global and SAM-local), and MIND via varying the transformation parameters. We conduct experiments on the abdomen dataset Ablation on Coarse-to-Fine. We study how the number of coarse-to-fine layers and how the size of the neighborhood window affect the registration result on the Abdomen dataset. From Table "
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,4.0,Conclusion,"We present SAMConvex, a coarse-to-fine discrete optimization method for CT registration. It extracts per-voxel semantic global and local features and builds a series of lightweight 6D correlation volumes, and iteratively updates a flow field by performing lookups on the correlation volumes. The performance on two interpatient and one intra-patient registration datasets demonstrates state-of-the-art accuracy, good generalization, and high computation efficiency of SAMConvex."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 53.
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,1.0,Introduction,"Ultrasound (US) is widely used in clinical practice because of its availability, realtime imaging capabilities, lack of side effects for the patient and flexibility. US is a dynamic modality that heavily relies on operator experience and on-the-fly interpretation, which requires many years of training and/or Machine Learning (ML) support that can handle image sequences. However, clinical reporting is conventionally done via single, selected images that rarely suffice for clinical audit or as training data for ML. Simulating US from anatomical information, e.g. Computed Tomography (CT) "
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,,Contribution:,"In this paper, we propose a new method for video diffusion  Related Work: Video Generation has been a research area within computer vision for many years now. Prior works can be organized in three categories: (1) pixel-level autoregressive models  Ultrasound simulation has been attempted with three major approaches: (1) physics-based simulators  LVEF is a major metric in the assessment of cardiac function and diagnosis of cardiomyopathy. The EchoNet-dynamic dataset "
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,2.0,Method,"Diffusion probabilistic models  where y is a training data point and n is noise. By following the definition of ordinary differential equations (ODE) we can continuously increase or decrease the noise level of our data point by moving it forward or backward in the diffusion process, respectively. To define the ODE we need a schedule σ(t) that sets the noise level given the time step t, which we set to σ(t) = t. The probability flow ODE's characteristic property is that moving a sample ) and this requirement is satisfied by setting dx = -σ(t)σ(t)∇ x log p(x; σ(t))dt where σ denotes the time derivative and ∇ x log p(x; σ) is the score function. From the score function, we can thus write ∇ x log p(x; σ) = (D(x; σ)-x)/σ 2 in the case of our denoising function, such that the score function isolates the noise from the signal x and can either amplify it or diminish it depending on the direction we take in the diffusion process. We define D(x; σ) to transform a neural network F , which can be trained inside D by following the loss described in Eq. (  5 and c noise (σ) = log(σ t )/4 where σ q is the standard deviation of the real data distribution. In this paper, we focus on generating temporally coherent and realistic-looking echocardiograms. We start by generating a low resolution, low-frame rate video v 0 from noise and condition on arbitrary clinical parameters and an anatomy instead of the commonly used text-prompt embeddings  where F θ0 is the neural network transformed by D θ0 and D θ0 outputs v 0 . For all subsequent models in the cascade, the conditioning remains similar, but the models also receive the output from the preceding model, such that: This holds ∀s > 0 and inputs I c , v s-1 are rescaled to the spatial and temporal resolutions expected by the neural network F θs as a pre-processing. We apply the robustness trick from  Sampling from the EDM is done through a stochastic sampling method. We start by sampling a noise sample x 0 ∼ N (0, t 2 0 I), where t comes from our previously defined σ(t i ) = t i and sets the noise level. We follow  The correction step doubles the number of executions of the model, and thus the sampling time per step, compared to DDPM  Conditioning: Our diffusion models are conditioned on two components. First, an anatomy, which is represented by a randomly sampled frame I c . It defines the patient's anatomy, but also all the information regarding the visual style and quality of the target video. These parameters cannot be explicitly disentangled, and we therefore limit ourselves to this approach. Second, we condition the model on clinical parameters λ c . This is done by discarding the text-encoders that are used in  Parameters: As video diffusion models are still in their early stage, there is no consensus on which are the best methods to train them. In our case, we define, depending on our experiment, 1-, 2-or 4-stages CDMs. We also experiment with various schedulers and parametrizations of the model. "
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,3.0,Experiments,"Data: We use the EchoNet-Dynamic  Architectural Variants: We define three sets of models, and present them in details in Table  Training: All our models are trained from scratch on individual cluster nodes, each with 8 × NVIDIA A100. We use a per-GPU batch size of 4 to 8, resulting in batches of 32 to 64 elements after gradient accumulation. The distributed training is handled by the accelerate library from HuggingFace. We did not see any speed-up or memory usage reduction when enabling mixed precision and thus used full precision. As pointed out by  Results: We evaluate our models' video synthesis capabilities on two objectives: LVEF accuracy (R 2 , MAE, RMSE) and image quality (SSIM, LPIPS, FID, FVD). We formulate the task as counterfactual modelling, where we set (1) a random conditioning frame as confounder, (2) the ground-truth LVEF as a factual conditioning, and (3) a random LVEF in the physiologically plausible range from 15% to 85% as counterfactual conditioning. For each ground truth video, we sample three random starting noise samples and conditioning frames. We use the LVEF regression model to create a feedback loop, following what  The results in Table  We outperform previous work for LVEF regression: counterfactual video generation improves with our method by a large margin of 38 points for the R 2 score as shown in Table  Table  Downstream Task: We train our LVEF regression model on rebalanced datasets and resampled datasets. We rebalance the datasets by using our 4SCM model to generate samples for LVEF values that have insufficient data. The resampled datasets are smaller datasets randomly sampled from the real training set. We show that, in small data regimes, using generated data to rebalance the dataset improves the overall performance. Training on 790 real data samples yields an R 2 score of 56% while the rebalanced datasets with 790 samples, ∼50% of which are real, reaches a better 59% on a balanced validation set. This observation is mitigated when more data is available. See Appendix Table  Discussion: Generating echocardiograms is a challenging task that differs from traditional computer vision due to the noisy nature of US images and videos. However, restricting the training domain simplifies certain aspects, such as not requiring a long series of CDMs to reach the target resolution of 112 × 112 pixels and limiting samples to 2 s, which covers any human heartbeat. The limited pixel-intensity space of the data also allows for models with fewer parameters. In the future, we plan to explore other organs and views within the US domain, with different clinical conditionings and segmentation maps."
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,4.0,Conclusion,"Our application of EDMs to US video generation achieves state-of-the-art performance on a counterfactual generation task, a data augmentation task, and a qualitative study by experts. This significant advancement provides a valuable solution for downstream tasks that could benefit from representative foundation models for medical imaging and precise medical video generation."
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_14.
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,1.0,Introduction,"Image registration is a crucial prerequisite for image comparison, data integration, and group studies in contemporary medical and neuroscience research. In research and clinical settings, pairs of images often show similar anatomical structures but may contain additional features or artifacts, such as specific staining, electrodes, or lesions, that are not present in the other image. This difficulty of finding corresponding structures for automatically aligning images complicates image registration. In this work, we address the challenging problem of the gene expression image registration in the marmoset brain. Brain atlases of gene expression, created using images of brain tissue processed through in situ hybridization (ISH), offer single-cell resolution of spatial gene expression patterns across the entire brain  Traditional pair-wise image registration methods use optimization algorithms to find the deformation field that maximizes the similarity between a pair of images. While several deep learning methods based on convolutional neural networks (CNNs) have been proposed for calculating the deformation field between two images  In this work, we propose a novel INR-based framework well-suited to address the challenging problem of gene expression brain image registration. We associate the registration problem with an image decomposition task. We utilize implicit neural networks to decompose the ISH image into two separate images: a support image and a residual image. The support image corresponds to the part of the ISH image that is well-aligned with the registration template image in respect to the texture. On the contrary, the residual image presents features of the ISH image, such as artifacts or texture patterns (e.g. gene expression), which presumably undermine the registration procedure. The support image is used to improve the deformation field calculations. We also introduce an exclusion loss to encourage clearer separation of the support and residual images. The usefulness of the proposed method is demonstrated using 2D ISH gene expression images of the marmoset brain."
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,2.1,Registration with Implicit Networks,"The goal of the pairwise image registration is to determine a spatial transformation that maximizes the similarity between the moving image M and the target fixed template image F . INRs serve as a continuous, coordinate based approximation of the deformation field obtained through a fully connected neural network. In this study, as the backbone for our method, we utilized the standard approach to registration with INRs, as described in  To train the deformation network, the following loss function based on correlation coefficients was applied to assess the similarity between the moved image T Φ (M ) and the fixed template image F : where NCC and LNCC stand for the normalized cross-correlation and local normalized cross-correlation based loss functions averaged over the entire image domain consisting of N elements. NCC was used to stabilize the training of the network, while LNCC ensured good local registration results. Additionally, following the standard approach to INR based registration, we regularized the deformation field based on the Jacobian matrix determinant |J Φ(x) | using following equation [16]:"
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,2.2,Registration Guided Image Decomposition,"Our aim is to improve the registration performance associated with the implicit deformation network D. The proposed framework is presented in Fig.  stating that the support M S and residual M R images should sum up to the moving image M . To ensure that the support image M S contributes to the registration with respect to the fixed image F , we utilize the cross-correlation based loss function L cc (F, T Φ (M S )) (Eq. 1), where T Φ (M S ) stands for the transformed support image M S . Therefore, the deformation network is trained to provide the transformation field Φ(x) both for the moving image and the support image For this, we utilize the following exclusion loss to encourage the gradient structure of the implicit networks S and R to be decorrelated  ), ⊗ indicates element-wise multiplication and indices i, j go over all elements of the matrix Γ . In our framework, we jointly optimize all three implicit networks (D, S and R) using the following composite loss function: The first row of Eq. 5 can be perceived as a standard registration loss, while the second row stands for a regularized image reconstruction loss."
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,2.3,Evaluation,"We designed the proposed method with the aim to address the problem of ISH gene expression image registration. For the evaluation, we used neonate marmoset brain ISH images collected at the Laboratory for Molecular Mechanisms of Brain Development, RIKEN Center for Brain Science, Wako, Japan (geneatlas.brainminds.jp)  Performance of the proposed approach was compared to the SynthMorph network and the ANTs SyN registration algorithm based on mutual information metric, as these two methods do not require pre-training and can serve as off-theshelf registration tools for neuroscience "
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,2.4,Implementation,We utilized sinusoidal representation networks to determine the implicit representations 
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,3.1,Qualitative Results,Support and residual images generated with the proposed method are shown in Fig.  Figure 
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,3.2,Quantitative Results,Table 
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,4.0,Conclusion,"Our approach based on implicit networks and registration-guided image decomposition has demonstrated excellent performance for the challenging task of registering ISH gene expression images of the marmoset brain. The results show that our approach outperformed pairwise registration methods based on ANTs and SynthMorph CNN, highlighting the potential of INRs as versatile off-the-shelf tools for image registration. Moreover, the proposed registration-guided image decomposition mechanism not only improved the registration performance, but also could be used to effectively separate the patterns that diverge from the target fixed image. In the future, we plan to investigate the possibility of using image decomposition for simultaneous registration and pattern segmentation, and methods to speed up the training "
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 61.
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,1.0,Introduction,"Three-dimensional (3D) microscopy imaging is crucial in revealing biological information from the nanoscale to the microscale. Isotropic high resolution across M. Pan and Y. Gan-Equal contribution. all dimensions is desirable for visualizing and analyzing biological structures. Most 3D imaging techniques have lower axial than lateral resolution due to physical slicing interval limitation or time-saving consideration  Recently, deep learning methods have made significant progress in image analysis  This paper proposes DiffuseIR, an unsupervised method based on diffusion models, to address the isotropic reconstruction problem. Unlike existing methods, DiffuseIR does not train a specific super-resolution model from low-axialresolution to high-axial-resolution. Instead, we pre-train a diffusion model θ to learn the structural distribution p θ (X lat ) of biological tissue from lateral microscopic images X lat , which resolution is naturally high. Then, as shown in Fig.  To sum up, our contributions are as follows: (1) We are the first to introduce diffusion models to isotropic reconstruction and propose DiffuseIR. Benefiting from the flexibility of SSCS, DiffuseIR is naturally robust to unseen anisotropic spatial resolutions. "
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,2.0,Methodology,"As shown in Fig.  DDPM Pretrain on Lateral. Our method differs from existing approaches that directly train super-resolution models. Instead, we pre-train a diffusion model to learn the distribution of high-resolution images at lateral, avoiding being limited to a specific axial resolution. Diffusion models  where α t controls the scale of noises. During inference, the model θ predicts x t-1 from x t . A U-Net θ is trained for denoising process p θ , which gradually reverses the diffusion process. This denoising process can be represented as: During training, we use 2D lateral slices, which is natural high-resolution to optimize θ by mean-matching the noisy image obtained in Eq. 1 using the MSE loss  Sparse Spatial Condition Sampling on Axial. We propose Sparse Spatial Condition Sampling (SSCS) to condition the generation process of θ and generate high-axial-resolution reconstruction results. SSCS substitutes every reversediffusion step Eq. 2. We first transform the input axial LR slice x axi to match the lateral resolution by intra-row padding: (α -1) rows of zero pixels are inserted between every two rows of original pixels, where α is the anisotropic spatial factor. We denote M as the mask for original pixels in x con 0 , while (1 -M ) represents those empty pixels inserted. In this way, we obtain x con 0 , which reflects the sparse spatial content at axial, and further apply Eq. 1 to transform noise level: Algorithm 1: Isotropic reconstruction using basic DiffuseIR Then, SSCS sample x t-1 at any time step t, conditioned on x con t-1 . The process can be described as follows: where x * t-1 is obtained by sampling from the model θ using Eq. 2, with x t of the previous iteration. x * t-1 and x con t-1 are combined with M . By iterative denoising, we obtain the reconstruction result x 0 . It conforms to the distribution p θ (X lat ) learned by the pre-trained diffusion model and maintains semantic consistency with the input LR axial slice. Since SSCS is parameter-free and decoupled from the model training process, DiffuseIR can adapt to various anisotropic spatial resolutions by modifying the padding factor according to α while other methods require re-training. This makes DiffuseIR a more practical and versatile solution for isotropic reconstruction. Refine-in-Loop Strategy. We can directly use SSCS to generate isotropic results, but the reconstruction quality is average. The diffusion model is capable of extracting context from the sparse spatial condition. Still, we have discovered a phenomenon of texture discoordination at the mask boundaries, which reduces the reconstruction quality. For a certain time step t, the content of x * t-1 may be unrelated to x con t-1 , resulting in disharmony in x t-1 generated by SSCS. During the denoising of the next time step t-1, the model tries to repair the disharmony of x t-1 to conform to p θ distribution. Meanwhile, this process will introduce new inconsistency and cannot converge on its own. To overcome this problem, we propose the Refine-in-loop strategy: For x t-1 generated by SSCS at time step t, we apply noise to it again and obtain a new x t and then repeat SSCS at time step t. Our discovery showed that this uncomplicated iterative refinement method addresses texture discoordination significantly and enhances semantic precision. The total number of inference steps in DiffuseIR is given by T total = T • K. As T total increases, it leads to a proportional increase in the computation time of our method. However, larger T total means more computational cost. Recent works such as "
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,3.0,Experiments and Discussion,"Dataset and Implement Details. To evaluate the effectiveness of our method, we conducted experiments on two widely used public EM datasets, FIB-25  Quantitative and Visual Evaluation. To evaluate the effectiveness of our method, we compared DiffuseIR with SoTA methods and presented the quantitative results in Table "
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,,Methzod,"Further Analysis on Robustness. We examined the robustness of our model to variations in both Z-axis resolutions and domain shifts. Specifically, we investigated the following: (a) Robustness to unseen anisotropic spatial factors. The algorithm may encounter unseen anisotropic resolution due to the need for different imaging settings in practical applications. To assess the model's robustness to unseen anisotropic factors, we evaluated the model trained with the anisotropic factor α = 4. Then we do inference under the scenario of anisotropic factor α = 8. For those methods with a fixed super-resolution factor, we use cubic interpolation to upsample the reconstructed result by 2x along the axis. (b) Robustness to the domain shifts. When encountering unseen data in the real world, domain shifts often exist, such as differences in biological structure features and physical resolution, which can impact the model's performance  Ablation Study. We conducted extensive ablation experiments Fig. "
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,4.0,Conclusion,"We introduce DiffuseIR, an unsupervised method for isotropic reconstruction based on diffusion models. To the best of our knowledge, We are the first to introduce diffusion models to solve this problem. Our approach employs Sparse Spatial Condition Sampling (SSCS) and a Refine-in-loop strategy to generate results robustly and efficiently that can handle unseen anisotropic resolutions. We evaluate DiffuseIR on EM data. Experiments results show our methods achieve SoTA methods and yield comparable performance to supervised methods. Additionally, our approach offers a novel perspective for addressing Isotropic Reconstruction problems and has impressive robustness and generalization abilities."
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,5.0,Limitation,"DiffuseIR leverages the powerful generative capabilities of pre-trained Diffusion Models to perform high-quality isotropic reconstruction. However, this inevitably results in higher computational costs. Fortunately, isotropic reconstruction is typically used in offline scenarios, making DiffuseIR's high computational time tolerable. Additionally, the community is continuously advancing research on accelerating Diffusion Model sampling, from which DiffuseIR can benefit."
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_31.
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,1.0,Introduction,"Metal implants can heavily attenuate X-rays in computed tomography (CT) scans, leading to severe artifacts in reconstructed images. It is essential to remove metal artifacts in CT images for subsequent diagnosis. Recently, with the emergence of deep learning (DL)  In this work, our goal is to explicitly reduce the domain gap between synthetic and clinical metal-corrupted CT images for improved clinical MAR performance. Some domain adaptation-based networks  To close the domain gap, we propose a novel semi-supervised MAR framework. In this work, the clean image domain acts as the bridge, where the bijection is substituted with two simple mappings and the strict assumption introduced by the cycle-consistency loss is relaxed. Our goal is to convert simulated and clinical metal-corrupted data back and forth. As an intermediate product, clean images are our target. To improve the transformations of two metal-corrupted images into metal-free ones, we propose a feature selection mechanism, denoted as Artifact Filtering Module (AFM), where AFM acts as a filter to eliminate features helpless in recovering clean images."
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,2.1,Problem Formulation,"The metal corruption process can be formulated as a linear superposition model as  where X ma , X free , and A represent metal-affected CT images, metal-free CT images, and metal artifacts, respectively. X ma is the observation signal and X free is the target signal to be reconstructed. "
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,2.2,Overview,Figure 
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,2.3,Image Translation,"According to Eq. 1, if two metal artifact reduction translations are completed, the subsequent two transformations can be obtained by subtracting the output of the network from the original input. Therefore, only two translators are needed. The first translator is used to convert I s into I f , denoted as G s2f and the second translator is used to convert I c into I f , denoted as G c2f . In this work, two translators, G s2f and G c2f , share the same network architecture, consisting of one encoder and one decoder, as shown in Fig. "
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,(b) and (d).,"Encoding Stage. In most DL-based MAR methods, the networks take a metalcorrupted CT image as input and map it to a metal-free CT image domain. Noise-related features are involved in the entire process, which negatively affects the restoration of clean images. In the principal component analysis (PCA)-based image denoising method  where λ is a small constant to prevent division by zero and we set λ = 1e -7 in this paper. Feature maps with high scores will be selected. Therefore, we can dynamically select different feature maps according to the inputs. Decoding Stage. At the encoding stage, features that are helpless to reconstruct the clean image are filtered out. Decoder then maps the remaining features, which contain useful information, back into the image domain. To push the generated image to fall into the clean image domain, we employ conditional normalization layers "
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,Metal Artifacts Reduction and Generation Stage.,"The framework consists of four image translation branches: two metal artifacts reduction branches I s ←I f and I c ←I f , and two metal artifact generation branches I f ←I s and I f ←I c . (1) I s ←I f : In this transformation, we employ G s2f to learn the mapping from the synthetic metal-affected image domain to the metal-free image domain, which is denoted as: where X s is synthetic metal-affected CT image in I s and X s2f is the corrected result of X s . According to Eq. 1, the metal artifacts A s can be obtained as follows: (2) I c ←I f : In this transformation, we use G c2f to learn the mapping from the clinical metal-affected image domain to the metal-free image domain, resulting in metal-corrected CT image X c2f and the metal artifacts A c . This process is the same as the transformation of I s ←I f and can be formulated as follows: where X c is clinical metal-affected CT image in I c . (3): I f ←I s : We use the artifacts of X s to obtain a synthetic domain metalcorrupted image X c2s by adding A s to the learned metal-free CT image X c2f : (4): I f ←I c : Synthesizing clinical domain metal-corrupted image X s2c can be achieved by adding A c to the learned metal-free CT image X s2f :"
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,2.4,Loss Function,"In our framework, the loss function contains two parts: adversarial loss and reconstruction loss. Adversarial Loss. Due to the lack of paired clinical metal-corrupted and metalfree CT images, as well as paired clinical and synthetic metal-corrupted CT images, we use PatchGAN-based discriminators, D f , D s , and D c , and introduce an adversarial loss for weak supervision. D f learns to distinguish whether an image is a metal-free image, D s learns to determine whether an image is a synthetic metal-affected CT image, and D c learns to determine whether an image is a clinical metal-affected CT image. The total adversarial loss L adv is written as: Reconstruction Loss. The label X gt of X s is employed to guide the G s2f to reduce the metal artifacts. The reconstruction loss L s2f on X x2f can be formulated as: When X syn is transformed into the clinical domain, G c2f can also reduce the metal artifacts with the help of X gt . The reconstruction loss L sc2f on X sc2f can be formulated as: where X sc2f is the MAR results of X s2c . To obtain optimal MAR results, it is necessary to remove any noise-related features while preserving as much of the content information as possible. When the input image is already metal-free, the input image has no noise-related features, and the reconstructed image should not suffer from any information loss. Here, we employed the model error loss to realize this constrain: where X f 2f is a reconstructed image from X f using G s2f and X f 2f is a reconstructed image from X f using G c2f .  Overall Loss. The overall loss function is defined as follows: where λ recon is the weighting parameter and as suggested by "
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,3.1,Dataset and Implementation Details,"In this work, we used one synthesized dataset and two clinical datasets, denoted as SY, CL1 and CL2, respectively. The proposed method was trained on SY and CL1. For data simulation, we followed the procedure of  The learning rate was initialized to 0.0001 and halved every 20 epochs. The network was trained with 60 epochs on one NVIDIA 1080Ti GPU with 11 GB memory, and the batch size was 2."
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,3.2,Comparison with State-of-the-Art Methods,The proposed method was compared with several classic and state-of-the-art (SOTA) MAR methods: LI 
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,MAR Performance on SY:,The quantitative scores are presented in Table  The sizes of the 16 metal implants in the testing dataset are:  MAR Performance on CL1: Figure 
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,MAR Performance on CL2:,"To assess the generalization capability of our method, we further evaluated MAR performance on CL2 with the model trained on SYN and CL1. Two practical cases are presented in Fig. "
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,3.3,Ablation Study,"In this section, we investigate the effectiveness of the proposed AFM. Table "
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,4.0,Conclusion,"In this paper, we explicitly bridge the domain gap between synthetic and clinical metal-corrupted CT images. We employ the clean image domain as the bridge and break the cycle-consistency loss, thereby eliminating the necessity for strict bijection assumption. At the encoding step, feature maps with limited influence will be eliminated, where the encoder acts as a bottleneck only allowing useful information to pass through. Experiments demonstrate that the performance of the proposed method is competitive with several SOTA MAR methods in both qualitative and quantitative aspects. In particular, our method exhibits good generalization ability on clinical data."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,1.0,Introduction,"2D-3D registration refers to the highly challenging process of aligning an input 2D image to its corresponding slice inside a given 3D volume  In this paper, we propose to leverage the structural features of tissue and more particularly the rigid areas to guide the registration process with two distinct contributions: (1) a cascaded rigid alignment driven by stiff regions and coupled with recursive plane selection, and (2) an improved 2D/3D deformable motion model with distance field regularization to handle out-of-plane deformation. To our knowledge, no previous study proposed 2D/3D registration combined with structure awareness. We also use the CycleGAN for image translation and direct monomodal signal comparison "
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,2.1,Histology-to-CT Modality Translation,Our three-step structure-aware pipeline is thoroughly detailed in Fig. 
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,2.2,Recursive Cascaded Plane Selection,"We replace the volume reconstruction step with a recursive dual model. We first rotate and translate the 3D CT to match the stack of slices H, which is crucial as an initialization step to help the 2D-3D network to focus on small out-of-plane deformations and avoid local minima. Then, we perform a precise plane selection and solve the spacing gap by adjusting the z-position of each slice to its most similar CT section. These two steps are performed iteratively until convergence, with a recursive algorithm to reduce computational cost (Fig.  For rigid initialization, the hypothesis is that the histological specimen is cut with an unknown spacing and angle, but the latter is supposed constant between WSIs. A rigid alignment is thus sufficient to reorient moving CT onto fixed H. Based on a theoretical axial slice sequence Z = (Z 1 , ..., Z m ), we define Fig.  H as a sparse 3D volume the same size as CT , filled in with h i at z = Z i and zeros elsewhere (the same applies for the corresponding sCT from the previous module). Because soft tissues undergo too large out-of-plane deformations, we leverage the rigid structures which are supposed not to be distorted or shrunk during the histological process. We extract their segmentation masks M ct , M h for both modalities (see preprocessing in Sect. 3), concatenate and fed them into an encoder followed by a fully connected layer that outputs six transformation parameters (3 rotations, 3 translations). A differentiable spatial transform R finally warps M ct for similarity optimization with M h . Similarly to  Additionally, R also warps CT without gradient backpropagation and is the input with sCT for plane selection. We then introduce a sequence alignment problem, the objective being to update the slice sequence Z of sCT by mapping it to a corresponding sequence J of 2D images from CT . We define S a similarity matrix, where S(i, j) is the total similarity (measured with MI) when mapping sct Z1 , ..., sct Zi with ct J1 , ..., ct which means that each row of S will be filled by computing the sum of the MI for the corresponding column j and the maximum similarity from the last row. Like any dynamic programming method, we want to find the optimal sequence J by following the backward path of S building. To do so, we retrieve the new index j that yielded the maximized similarity for each step J = [max i (S(i, j)] j , and we update Z ← J accordingly. In addition, the J sequence cannot be too different from Z as it would induce overlap between ordered WSIs. We thus constrained the possible matching values with k ∈ [Z i -2, Z i + 2]. Based on these rigid registration and plane selection blocks, we build a cascaded module to iteratively refine the alignment where the intermediate warping becomes the new input. We defined the number of iterations as a hyperparameter to reach a good balance between computational time and similarity maximization. This dual model is crucial for initialization but does not take into account out-ofplane deformations and a perfect alignment is not accessible yet. The deformable framework bridges this gap by focusing on irregular displacements caused by tissue manipulation and refining the rigid warping."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,2.3,Deformable 2D-3D Registration,"Given one fixed multi-slice sCT and a moving rigidly warped R(CT ) from the previous module, we adopt an architecture close to Voxelmorph  Finally, we add two sources of regularization. Soft tissues away from bones and cartilage are more subject to shrinkage or disruption, so we harness the information from the cartilage segmentation mask of CT to generate a distance transform map Δ defined as Δ(v) = min m∈MCT ||v -m|| 2 . It maps each voxel v of CT to its distance with the closest point m to the rigid area M CT . We can then control the displacement field, with close tissue being more highly constrained than isolated areas: Φ = Φ (Δ + ), where is the Hadamard product and is a hyperparameter matrix allowing small displacement even for cartilage areas for which distance transform is null. A second regularization takes the form of a loss L regu (Φ ) = v∈R 3 ||∇Φ (v)|| 2 on the volume to constrain spatial gradients and thus encourage smooth deformation, which is essential for empty slices which are excluded from L defo . The total loss is a weighted sum of L defo and L regu . "
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,3.0,Experiments,"Dataset and Preprocessing. Our clinical dataset consists of 108 patients for whom were acquired both a pre-operative H&N CT scan and 4 to 11 WSIs after laryngectomy (with a total amount of 849 WSIs). The theoretical spacing between each slice is 5 mm, and the typical pixel size before downsampling is 100K × 100K. Two expert radiation oncologists on CT delineated both the thyroid and cricoid cartilages for structure awareness and the Gross Tumor Volume (GTV) for clinical validation, while two expert pathologists did the same on WSIs. They then meet and agreed to place 6 landmarks for each slice at important locations (not used for training). We ended up with images of size 256 × 256 (×64 for 3D CT) of 1 mm isotropic grid space. We split the dataset patient-wise into three groups for training (64), validation  Hyperparameters. We drew our code from CycleGAN and Voxelmorph implementations with modifications explained above, and we thank the authors of MSV-RegSynNet for making their code and data available to us  Evaluation. We benchmarked our method against three baselines: First, to assess the benefit of modality translation over the multimodal loss, we re-used the original 3D VoxelMorph model with MIND as a multimodal metric for optimization. We also modified this approach by masking the loss function to account for the 2D-3D setting. Next, we implemented the modality translation-based MSV-RegSyn-Net and modified it for our application to measure the importance of joint structure-aware initialization and regularization. Finally, to differentiate the latter contributions, we tested two ablation studies: without the cascaded rigid mapping or without the distance field control. According to the MR/CT application in RT, we compared our model against the state-of-the-art results of MSV-RegSynNet which were computed on the same dataset."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,4.1,Modality Translation,Three samples from the test set are displayed in Fig. 
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,4.2,Registration,"We present visual results in Fig.  Our method outperforms all baselines, proving the necessity of a singular approach to handle the specific case of histology. The popular Voxelmorph framework fails, and the 2D-3D adaptation demonstrates the value of the masked loss function. The superior performance of MSV-RegSynNet advocates for a modality translation-based method compared to a direct multimodal similarity criterion. In addition, the ablation studies prove the benefit of the distance field regularization and more importantly the cascaded initialization. Concerning the GPU runtime, with a 3-step cascade for initialization, the inference remains in a similar time scale to baseline methods and performs mapping in less than 3s. We also compared against MSV-RegSynNet on its own validation dataset for generalization assessment: we yielded comparable results for the first cohort and significantly better ones for the second, which proves that StructuRegNet behaves well on other modalities and that the structure awareness is an essential asset for better registration, as pelvis is a location where organs are moving. Visuals of registration results are displayed in the supplementary material. Eventually, an important clinical endpoint of our study is to compare the GTV delineated on CT with gold-standard tumor extent after co-registration to highlight systematic errors and better understand the biological environment from the radiologic signals. We show in (f) that the GTV delineated on CT overestimates the true tumor extent of around 31%, but does not always encompass the tumor with a proportion of histological tumor contained within the CT contour of 0.86. The typical error cases are the inclusion of cartilage or edema, which highlights the limitations and variability of radiology-based examinations, leading to increased toxicity or untreated areas in RT."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,5.0,Discussion and Conclusion,"We introduced a novel framework for 2D/3D multimodal registration. Struc-tuRegNet leverages the structure of tissues to guide the registration through both initial plane selection and deformable regularization; it combines adversarial training for modality translation with a 2D-3D mapping setting and does not require any protocol for 3D reconstruction. It is worth noticing that even if the annotation of cartilage was manual, automating this process is not a bottleneck as the difference in contrast between soft tissue and stiff areas is clear enough to leverage any image processing tool for this task. Finally, it is entirely versatile as we designed our experiments for CT-WSI but any 3D radiological images are suitable. We achieve superior results than state-of-the-art methods in DL-based registration in a similar time scale, allowing precise mapping of both modalities and a better understanding of the tumor microenvironment. The main limitation lies in the handling of organs without any rigid areas like the prostate. Future work also includes a study with biomarkers from immunohistochemistry mapped onto radiology to go beyond binary tumor masks and move toward virtual biopsy."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 73.
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,1.0,Introduction,"Tomographic imaging estimates body density using hundreds of X-ray projections, but it's slow and harmful to patients. Acquisition time may be too high for certain applications, and each projection adds dose to the patient. A quick, low-cost 3D estimation of internal structures using only bi-planar X-rays can revolutionize radiology, benefiting dental imaging, orthopedics, neurology, and more. This can improve image-guided therapies and preoperative planning, especially for radiotherapy, which requires precise patient positioning with minimal radiation exposure. However, this task is an ill-posed inverse problem: X-ray measurements are the result of attenuation integration across the body, which makes them very Fig.  ambiguous. Traditional reconstruction methods require hundreds of projections to get sufficient constraints on the internal structures. With very few projections, it is very difficult to disentangle the structures for even coarse 3D estimation. In other words, many 3D volumes may have generated such projections a priori. Classical analytical and iterative methods  As illustrated in Fig.  Compared to other 3D GANs, it is proven to provide the best disentanglement of the feature space related to semantic features  By contrast with feed-forward methods, our approach does not require paired projections-reconstructions, which are very tedious to acquire, and it can be used with different numbers of projections and different projection geometries without retraining. Compared to NeRF-based methods, our method exploits prior knowledge from many patients to require only two projections. We evaluate our method on reconstructing cancer patients' head-and-neck CTs, which involves intricate and complicated structures. We perform several experiments to compare our method with a feed-forward-based method  We show that our method allows to retrieve results with the finest reconstructions and better matching structures, for a variety of number of projections. To summarize, our contributions are two-fold: (i) A new paradigm for 3D reconstruction with biplanar X-rays: instead of learning to invert the measurements, we leverage a 3D style-based generative model to learn deep image priors of anatomic structures and optimize over the latent space to match the input projections; (ii) A novel unsupervised method, fast and robust to sampling ratio, source energy, angles and geometry of projections, all of which making it general for downstream applications and imaging systems."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,2.0,Method,Figure 
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,2.1,Problem Formulation,"Given a small set of projections {I i } i , possibly as few as two, we would like to reconstruct the 3D tomographic volume v that generates these projections. This is a hard ill-posed problem, and to solve it, we need prior knowledge about the possible volumes. To do this, we look for the maximum a posteriori (MAP) estimate given the projections {I i } i : (1) Term L(v, I i ) is a log-likelihood. We take it as: Fig.  where A i is an operator that projects volume v under view i. We provide more details about operator A in Sect. 2.3. L p is the perceptual loss  It is crucial as it is the term that embodies prior knowledge about the volume to reconstruct. As discussed in the introduction, we rely on a generative model, which we describe in the next section. Then, we describe how exactly we use this generative model for regularization term R(v) and how this changes our optimization problem."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,2.2,Manifold Learning,"To regularize the domain space of solutions, we leverage a style-based generative model to learn deep priors of anatomic structures. Our model relies on Style-GAN2  Our generator G generates a volume v given a latent vector w and Gaussian noise vectors n = {n j } j : v = G(w, n). Latent vector w ∈ N (w|μ, σ) is computed from an initial latent vector z ∈ N (0, I ) mapped using a learned network m: w = m(z). w controls the global structure of the predicted volumes at different scales by its components w i , while the noise vectors n allow more fine-grained details. The mean μ and standard deviation σ of the mapped latent space can be computed by mapping over initial latent space N (0, I ) after training. The mapping network learns to disentangle the initial latent space relatively to semantic features which is crucial for the inverse problem. We train this model using the non-saturating logistic loss "
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,2.3,Reconstruction from Biplanar Projections,"Since our generative model provides a volume v as a function of vectors w and n, we can reparameterize our optimization from Eq. (  Note that by contrast with  Term L w (w) =k log N (w k |μ, σ) ensures that w lies on the same distribution as during training. N (•|μ, σ) represents the density of the standard normal distribution of mean μ and standard deviation σ. Term L c (w) =i,j log M(θ i,j |0, κ) encourages the w i vectors to be collinear so to keep the generation of coarse-to-fine structures coherent. M(•; μ, κ) is the density of the Von Mises distribution of mean μ and scale κ, which we take fixed, and θ i,j = arccos( wi•wj wi wj ) is the angle between vectors w i and w j . Term L n (n) =j log N (n j |0, I ) ensures that the n j lie on the same distribution as during training, i.e., a multivariate standard normal distribution. The λ * are fixed weights. Projection Operator. In practice, we take operator A as a 3D cone beam projection that simulates X-ray attenuation across the patient, adapted from  with μ(m, E) the linear attenuation coefficient of material m at energy state E that is known  For materials, we consider the bones and tissues that we separate by threshold on electron density. A inverts the attenuation intensities I atten to generate an X-ray along few directions successively. We make A differentiable using  3 Experiments and Results"
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.1,Dataset and Preprocessing,"Manifold Learning. We trained our model with a large dataset of 3500 CTs of patients with head-and-neck cancer, more exactly 2297 patients from the publicly available The Cancer Imaging Archive (TCIA)  3D Reconstruction. To evaluate our approach, we used an external private cohort of 80 patients who had undergone radiotherapy for head-and-neck cancer, with their consent. Planning CT scans were obtained for dose preparation, and CBCT scans were obtained at each treatment fraction for positioning with full gantry acquisition. As can be seen in Fig. "
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.2,Implementation Details,"Manifold Learning. We used Pytorch to implement our model, based on Style-GAN2  3D Reconstruction. For the reconstruction, we performed the optimization on GPU V100 PCI-E using Adam, with learning rate of 1e-3. By grid search on the validation set, we selected the best weights that well balance between structure and fine-grained details, λ 2 = 10, λ p = 0.1, λ w = 0.1, λ c = 0.05, λ n = 10. We perform 100 optimization steps starting from the mean of the mapped latent space, which takes 25 s, enabling clinical use."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.3,Results and Discussion,"Manifold Learning. We tested our model's ability to learn the low-dimensional manifold. We used FID  Baselines. We compared our method against the main feed-forward method X2CT-GAN  3D Reconstruction. To evaluate our method's performance with biplanar projections, we focused on positioning imaging for radiotherapy. Figure "
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,4.0,"Conclusion, Limitations, and Future Work","We proposed a new unsupervised method for 3D reconstruction from biplanar X-rays using a deep generative model to learn the structure manifold and retrieve the maximum a posteriori volume with the projections, leading to stateof-the-art reconstruction. Our approach is fast, robust, and applicable to various human body parts, making it suitable for many clinical applications, including positioning and visualization with reduced radiation. Future hardware improvements may increase resolution, and our approach could benefit from other generative models like latent diffusion models. This approach may provide coarse reconstructions for patients with rare abnormalities, as most learning methods, but a larger dataset or developing a prior including tissue abnormalities could improve robustness."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_66.
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,1.0,Introduction,"Microvasculature and neural tissue in the retina can be directly and noninvasively visualized in vivo  These issues make it difficult to discriminate important indicators and biomarkers, leading to the failure of early detection of diseases and proper intervention. Therefore, image enhancement of low-quality retina images is highly necessary. However, retina image enhancement is a challenging task due to several reasons. Intuitively, it requires paired low and high-quality retina images to learn to map low-quality images to their high-quality ones and such data are difficult to acquire in practice  To deal with the issues above, a structure-preserving guided retina image filter (SGRIF) was proposed to enhance unpaired retina images with prior knowledge of clouding effect  In this context, we propose an unpaired Retina image Enhancement with Scattering Transform (REST) which preserves the anatomical structure (e.g., vessels, optic disc, and cup) and maps the tone (e.g., color and illuminance) effectively by utilizing an unpaired dataset. Our model contains a genuine generator that includes two branches: the Anatomy Preserving Branch (APB) and the Tone Transferring Branch (TTB). The APB incorporates scattering transform, which effectively captures anatomic structures and the TTB employs a multilayer convolution to refine the tone of the image as that of high-quality images. Constructing a cyclic architecture "
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,2.0,Related Works,The SGRIF proposed in 
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,3.0,Methods,"We propose a generative framework for unpaired retina image enhancement, which translates low-quality retina images X to high-quality images Y through two separate branches: APB and TTB. As illustrated in Fig. "
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,3.1,Anatomy Preserving Branch,"In order to preserve fine anatomical details, such as the shape of an optic disc, an optic cup, and vessels, the model must accurately capture high-frequency components, i.e., edges. To extract these high-frequency components, we designed the APB, which employs a wavelet scattering transform over multiple encoding layers. Wavelet scattering transform extracts high-frequency factors and invariants with respect to translation and rotation  The 0-th order scattering coefficient S 0 J x is computed by convolving ( * ) x with Φ. The first-order coefficients set S 1 J x is obtained by convolving x with a set of wavelet filters Ψ λ and modulus operation followed by a low pass filter Φ. Since the wavelet filter sets Ψ λ have diverse frequencies and angles (i.e., Λ) within 2D image space as well as scales (i.e., J), the filters ensure the scattering transform to capture the structural information with respect to the frequency and direction  where, ( The input of each layer e APB i-1 undergoes both the scattering transform S 1 and convolution with a trainable kernel k 1×1 and k APB Ei , except for the last layer in the encoding process. The C i and SC i are concatenated ( ) to yield an input for a subsequent layer. In the final layer, the input undergoes only convolution with a kernel k APB Ei . After the completing N encoding processes, the decoding process, denoted as D APB i , commences. The final encoded feature map of the encoder, e APB N , undergoes N + 1 decoding processes. The decoding process is composed of a resizing R(•) and convolution with trainable kernel k APB Di as In the upscaling phase (0 < i ≤ N ), a combination of resizing and convolution techniques is implemented instead of transpose convolution. This approach prevents the occurrence of checkerboard artifacts, which can arise from uneven overlap during the transpose convolution "
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,3.2,Tone Transferring Branch,"As the quality of a retina image depends on the tone of the image such as color, illumination, and contrast in addition to anatomical structures  ( After the encoding process, the N sequence of decoding operations D TTB i commences with the layer i = N . In the first decoding layer, the output of the encoder e TTB N is up-scaled using the transposed convolution operator ( ). For subsequent layers (1 ≤ i < N), the output of the previous decoding layer d TTB i+1 and the corresponding output of the encoding layer e TTB i are concatenated processed through the transposed convolution operator as Along with our objective function containing GAN Loss "
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,3.3,Loss Function for Unpaired Image Enhancement,"The popular adversarial training architecture for unpaired image translation  To ensure that generated images look like genuine images, the GAN losses To preserve the contents of the input image, we aim to make F (G(x)) ≈ x and G(F (y)) ≈ y with the cycle consistency losses as  where λ c1 and λ c2 are hyperparameters. To make sure that the generators avoid translation when there is no need, i.e., a high-quality image as an input to G should lead to a high-quality image without changes, identity losses are defined as  with the hyperparameters λ i1 and λ i2 . With the aforementioned losses, we aim to obtain generators G * , F * that minimize the loss function while discriminators D * X , D * Y that maximize it as After training, G * is utilized at the inference stage to enhance retina images."
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,4.1,Datasets and Experimental Setup,"Datsets. From UKB, 2000 images, i.e., 1000 high-quality and 1000 low-quality images respectively, were randomly sampled and split into train and test sets by a ratio of 2:1. For the EyeQ, we utilized 23252 labeled images, i.e., 6434 lowquality images labeled 'usable' and 16818 high-quality images labeled 'good'  Implmenemtation. The batch size was set to 4 and the Adam optimizer was adopted with the initial learning rate of 0.0002 linearly decaying. For scattering transform, a Morlet wavelet was used with eight different angles within 2D image space and J was set to 1. A 2D Gabor filter was used as a low-pass filter. Parameters in losses, i.e., λ c1 , λ c2 , λ i1 and λ i2 , were set to 10, 10, 5 and 5 respectively. "
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,4.2,Results,"Regarding the experiments on UKB, the result of the qualitative evaluation is illustrated in   As can be seen in Fig.  For the experiment on the EyeQ dataset, the result of the qualitative evaluation is illustrated in Table "
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,5.0,Conclusion,"In the study, we proposed a novel approach to unpaired retina image enhancement, i.e., REST. While using only the authentic unpaired images, the proposed method effectively preserves anatomical structures during the enhancement process. This is done by utilizing APB for preserving the details by the scattering transform and TTB for transferring the tone of the images. Notably, REST has demonstrated commendable performance on both two different datasets."
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 45.
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,1.0,Introduction,"Magnetic resonance imaging (MRI) and computed tomography (CT) are two commonly used cross-sectional medical imaging techniques. MRI and CT produce different tissue contrast and are often used in tandem to provide complementary information. While MRI is useful for visualizing soft tissues (e.g. muscle,   fat), CT is superior for visualizing bony structures. Some medical procedures, such as radiotherapy for brain tumors, craniosynostosis, and spinal surgery, typically require both MRI and CT for planning. Unfortunately, CT imaging exposes patients to ionizing radiation, which can damage DNA and increase cancer risk  Most synthesis methods adopt supervised learning paradigms and train generative models to synthesize CT  Recent unsupervised methods impose structural constraints on the synthesized CT through pixel-wise or shape-wise consistency. Pixel-wise consistency methods "
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,2.0,Proposed Method,"In this section, we first introduce the MaskGAN architecture, shown in Fig. "
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,2.1,MaskGAN Architecture,"The network comprises two generators, each learning an MRI-CT and a CT-MRI translation. Our generator design has two branches, one for generating masks and the other for synthesizing the content in the masked regions. The mask branch learns N attention masks A i , where the first N -1 masks capture foreground (FG) structures and the last mask represents the background (BG). The content branch synthesizes N -1 outputs for the foreground structures, denoted as C. Each output, C i , represents the synthetic content for the corresponding foreground region that is masked by the attention mask A i . Intuitively, each channel A i in the mask tensor A focuses on different anatomical structures in the medical image. For instance, one channel emphasizes on synthesizing the skull, while another focuses on the brain tissue. The last channel A N in A corresponds to the background and is applied to the original input to preserve the background contents. The final output is the sum of masked foreground contents and masked background input. Formally, the synthetic CT output generated from the input MRI x is defined as The synthetic MRI output from the CT scan y is defined similarly based on the attention masks and the contents from the MR generator. The proposed network is trained using three training objectives described in the next sections."
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,2.2,CycleGAN Supervision,"The two generators, G MR and G CT , map images from MRI domain (X) and CT domain (Y ), respectively. Two discriminators, D MR and D CT , are used to distinguish real from fake images in the MRI and CT domains. The adversarial loss for training the generators to produce synthetic CT images is defined as (2) The adversarial loss L MR for generating MRI images is defined in a similar manner. For unsupervised training, CycleGAN imposes the cycle consistency loss, which is formulated as follows (3) The CycleGAN's objective L GAN is the combination of adversarial and cycle consistency loss."
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,2.3,Mask and Cycle Shape Consistency Supervision,"Mask Loss. To reduce spurious mappings in the background regions, MaskGAN explicitly guides the mask generator to differentiate the foreground objects from the background using mask supervision. We extract the coarse mask B using basic image processing operations. Specifically, we design a simple but robust algorithm that works on both MRI and CT scans, with a binarization stage followed by a refinement step. In the binarization stage, we normalize the intensity to the range [0, 1] and apply a binary threshold of 0.1, selected based on histogram inspection, to separate the foreground from the background. In the post-processing stage, we refine the binary image using morphological operations, specifically employing a binary opening operation to remove small artifacts. We perform connected component analysis  We introduce a novel mask supervision loss that penalizes the difference between the background mask A N learned from the input image and the groundtruth background mask B in both MRI and CT domains. The mask loss for the attention generators is formulated as Discussion. Previous shape-aware methods  Cycle Shape Consistency Loss. Spurious mappings can occur when the anatomy is shifted during translation. To preserve structural consistency across domains, we introduce the cycle shape consistency (CSC) loss as our secondary contribution. Our loss penalizes the discrepancy between the background attention mask A MR N learned from the input MRI image and the mask ÃCT N learned from synthetic CT. Enforcing consistency in both domains, we formulate the shape consistency loss as The final loss for MaskGAN is the sum of three loss objectives weighted by the corresponding loss coefficients: L = L GAN + λ mask L mask + λ shape L shape ."
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,3.1,Experimental Settings,"Data Collection. We collected 270 volumetric T1-weighted MRI and 267 thinslice CT head scans with bony reconstruction performed in pediatric patients under routine scanning protocols Evaluation Metrics. To provide a quantitative evaluation of methods, we compute the same standard performance metrics as in previous works "
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,3.2,Results and Discussions,"Comparisons with State-of-the-Art. We compare the performance of our proposed MaskGAN with existing state-of-the-art image synthesis methods, including CycleGAN "
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,,Methods,"Primary: MRI-to-CT Secondary: CycleGAN  We perform an ablation study by removing the cycle shape consistency loss (w/o Shape). Compared with shape-CycleGAN, MaskGAN using only a mask loss significantly reduces MAE by 6.26%. The combination of both mask and cycle shape consistency losses results in the largest improvement, demonstrating the complementary contributions of our two losses. Robustness to Error-Prone Coarse Masks. We compare the performance of our approach with shape-CycleGAN "
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,4.0,Conclusion,This paper proposes MaskGAN -a novel automated framework that maintains the shape consistency of prominent anatomical structures without relying on expert annotated segmentations. Our method generates a coarse mask outlining the shape of the main anatomy and synthesizes the contents for the masked foreground region. Experimental results on a clinical dataset show that MaskGAN significantly outperforms existing methods and produces synthetic CT with more consistent mappings of anatomical structures.
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 6.
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,1.0,Introduction,"Sparse-view Computed Tomography (CT) is an important class of low-dose CT techniques for fast imaging with reduced X-ray radiation dose. Due to the significant undersampling of sinogram data, the sparse-view CT reconstruction problem is severely ill-posed. As such, applying the standard filtered-backprojection (FBP) algorithm,  Deep learning (DL) has emerged in recent years as a powerful tool for image reconstruction. Deep learning parameterizes the functions of interests, such as the mapping from incomplete and/or noisy data to reconstructed images, as deep neural networks. The parameters of the networks are learned by minimizing some loss functional that measures the mapping quality based on a sufficient amount of data samples. The use of training samples enables DL to learn more enriched features, and therefore, DL has shown tremendous success in various tasks in image reconstruction. In particular, DL has been used for medical image reconstruction applications  DL-based methods for CT reconstruction have also evolved fast in the past few years. One of the most successful DL-based approaches is known as unrolling  Recently, a new class of DL-based methods known as learnable descent algorithm (LDA) "
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,2.0,Learnable Variational Model,"We formulate the dual-domain reconstruction model as the following two-block minimization problem: where (x, z) are the image and sinogram to be reconstructed and s is the sparseview sinogram. The first two terms in (1) are the data fidelity and consistency, where A and P 0 z represent the Radon transform and the sparse-view sinogram, respectively, and • ≡ • 2 . The last two terms represent the regularizations, which are defined as the l 2,1 norm of the learnable convolutional feature extraction mappings in  where θ 1 , θ 2 are learnable parameters. We use is the vector at position i across all channels. The feature extractor g r (•) is a CNN consisting of several convolutional operators separated by the smoothed ReLU activation function as follows: where {w r i } l i=1 denote convolution parameters with d r kernels. Kernel sizes are "
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,3.0,A Learned Alternating Minimization Algorithm,"This section formally introduces the Learned Alternating Minimization Algorithm (LAMA) to solve the nonconvex and nonsmooth minimization model  The first stage of LAMA aims to reduce the nonconvex and nonsmooth problem in (1) to a nonconvex smooth optimization problem by using an appropriate smoothing procedure where (r, y) represents either (R, x) or (Q, z) and Note that the non-smoothness of the objective function (1) originates from the non-differentiability of the l 2,1 norm at the origin. To handle the non-smoothness, we utilize Nesterov's smoothing technique  The smoothed regularizations take the form of the Huber function, effectively removing the non-smoothness aspects of the problem. The second stage solves the smoothed nonconvex problem with the fixed smoothing factor ε = ε k , i.e. min x,z where f (x, z) denotes the first two data fitting terms from  where α k and β k are step sizes. Since the proximal point u x k+1 and u z k+1 are are difficult to compute, we approximate Q ε k (u) and R ε k (u) by their linear approximations at b k+1 and c k+1 , i.e. Then by a simple computation, u x k+1 and u z k+1 are now determined by the following formulas where αk = α k p k α k +p k , βk = β k q k β k +q k . In deep learning approach, the step sizes α k , αk , β k and βk can also be learned. Note that the convergence of the sequence {(u z k+1 , u x k+1 )} is not guaranteed. We proposed that if (u z k+1 , u x k+1 ) satisfy the following Sufficient Descent Conditions (SDC): for some η > 0, we accept x k+1 = u x k+1 , z k+1 = u z k+1 . If one of (10a) and (10b) is violated, we compute (v z k+1 , v x k+1 ) by the standard Block Coordinate Descent (BCD) with a simple line-search strategy to safeguard convergence: Let ᾱ, β be positive numbers in (0, 1) compute Set x k+1 = v x k+1 , z k+1 = v z k+1 , if for some δ ∈ (0, 1), the following holds: Otherwise we reduce (ᾱ, β) ← ρ(ᾱ, β) where 0 < ρ < 1, and recompute v x k+1 , v z k+1 until the condition (13) holds. The third stage checks if ∇Φ ε has been reduced enough to perform the second stage with a reduced smoothing factor ε. By gradually decreasing ε, we obtain a subsequence of the iterates that converges to a Clarke stationary point of the original nonconvex and nonsmooth problem. The algorithm is given below."
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,,Algorithm 1. The Linearized Alternating Minimization Algorithm (LAMA),"Input: Initializations: x0, z0, δ, η, ρ, γ, ε0, σ, λ 1: "
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,4.0,Network Architecture,"The architecture of the proposed multi-phase neural networks follows LAMA exactly. Hence we also use LAMA to denote the networks as each phase corresponds to each iteration in Algorithm 1. The networks inherit all the convergence properties of LAMA such that the solution is stabilized. Moreover, the algorithm effectively leverages complementary information through the inter-domain connections shown in Fig. "
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,5.0,Convergence Analysis,"Since we deal with a nonconvex and nonsmooth optimization problem, we first need to introduce the following definitions based on the generalized derivatives. "
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,,Definition 1. (Clarke subdifferential). Suppose that,"where w 1 , v 1 stands for the inner product in R n and similarly for w 2 , v 2 ."
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,,Definition 2. (Clarke stationary point) For a locally Lipschitz function f defined as in,"We can have the following convergence result. All proofs are given in the supplementary material. Theorem 1. Let {Y k = (x k , z k )} be the sequence generated by the algorithm with arbitrary initial condition Y 0 = (x 0 , z 0 ), arbitrary ε 0 > 0 and ε tol = 0. Let { Ỹl } =: (x k l +1 , z k l +1 )} be the subsequence, where the reduction criterion in the algorithm is met for k = k l and l = 1, 2, .... Then { Ỹl } has at least one accumulation point, and each accumulation point is a Clarke stationary point."
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,6.1,Initialization Network,"The initialization (x 0 , z 0 ) is obtained by passing the sparse-view sinogram s defined in (1) through a CNN consisting of five residual blocks. Each block has four convolutions with 48 channels and kernel size "
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,6.2,Experiment Setup,"Our algorithm is evaluated on the ""2016 NIH-AAPM-Mayo Clinic Low-Dose CT Grand Challenge"" and the National Biomedical Imaging Archive (NBIA) datasets. We randomly select 500 and 200 image-sinogram pairs from AAPM-Mayo and NBIA, respectively, with 80% for training and 20% for testing. We evaluate algorithms using the peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and the number of network parameters. The sinograms have 512 detector elements, each with 1024 evenly distributed projection views. The sinograms are downsampled into 64 or 128 views while the image size is 256 × 256, and we simulate projections and back-projections in fan-beam geometry using distance-driven algorithms  where μ is the weight for SSIM loss set as 0.01 for all experiments, x(i) is ground truth image, and final reconstructions are (x We use the Adam optimizer with learning rates of 1e-4 and 6e-5 for the image and sinogram networks, respectively, and train them with a warm-up approach. The training starts with three phases for 300 epochs, then adding two phases for 200 epochs each time until the number of phases reaches 15. The algorithm is implemented in Python using the PyTorch framework. Our experiments were run on a Linux server with an NVIDIA A100 Tensor Core GPU. "
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,6.3,Numerical and Visual Results,We perform an ablation study to compare the reconstruction quality of LAMA and BCD defined in  We evaluate LAMA by applying the pipeline described in Sect. 6.2 to sparseview sinograms from the test set and compare with state-of-the-art methods where the numerical results are presented in Table 
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,7.0,Conclusion,"We propose a novel, interpretable dual-domain sparse-view CT image reconstruction algorithm LAMA. It is a variational model with composite objectives and solves the nonsmooth and nonconvex optimization problem with convergence guarantees. By introducing learnable regularizations, our method effectively suppresses noise and artifacts while preserving structural details in the reconstructed images. The LAMA algorithm leverages complementary information from both domains to estimate missing information and improve reconstruction quality in each iteration. Our experiments demonstrate that LAMA outperforms existing methods while maintaining favorable memory efficiency."
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 17.
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,1.0,Introduction,"As an in vivo nuclear medical imaging technique, positron emission tomography (PET) enables the visualization and quantification of molecular-level activity and has been extensively applied in hospitals for disease diagnosis and intervention  In the past decade, deep learning has demonstrated its promising potential in the field of medical images  To remedy the above limitation, several studies focus on the more challenging direct reconstruction methods  In this paper, to resolve the first limitation above, we propose to represent the reconstructed SPET images in the frequency domain, then encourage them to resemble the corresponding real SPET images in the high-frequency part. As for the second limitation, we draw inspiration from the remarkable progress of vision transformer  Overall, we propose an end-to-end transformer model dubbed TriDo-Former that unites triple domains of sinogram, image, and frequency to directly reconstruct the clinically acceptable SPET images from LPET sinograms. Specifically, our TriDo-Former is comprised of two cascaded transformers, i.e., a sinogram enhancement transformer (SE-Former) and a spatial-spectral reconstruction transformer (SSR-Former). The SE-Former aims to predict denoised SPET-like sinograms from LPET sinograms, so as to prevent the noise in sinograms from propagating into the image domain. Given that each row of the sinogram is essentially the projection at a certain imaging views angle, dividing it into 2D patches and feeding them directly into the transformer will inevitably break the continuity of each projection view. Therefore, to retain the inner-structure of sinograms and filter the noise, we split a sinogram by rows and obtain a set of 1D sequences of different imaging view angles. Then, the relations between view angles are modeled via the self-attention mechanism in the SE-Former. Note that the SE-Former is designed specifically for the sinogram domain of LPET to effectively reduce noise based on the imaging mechanisms of PET. The denoised sinograms can serve as a better basis for the subsequent sinogram-to-image reconstruction. The SSR-Former is designed to reconstruct SPET images from the denoised sinograms. In pursuit of better image quality, we construct the SSR-Former by adopting the powerful swin transformer  The contributions of our proposed method can be described as follows. (1) To fully exploit the triple domains of sinogram, image, and frequency while capturing global context, we propose a novel triple-domain transformer to directly reconstruct SPET images from LPET sinograms. To our knowledge, we are the first to leverage both triple-domain knowledge and transformer for PET reconstruction. "
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,2.0,Methodology,The overall architecture of our proposed TriDo-Former is depicted in Fig. 
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,2.1,Sinogram Enhancement Transformer (SE-Former),"As illustrated in Fig.  Feature Embedding: We denote the input LPET sinogram as S L ∈ R C s ×H s ×W s , where H s , W s are the height, width and C s is the channel dimension. As each row of sinogram is a projection view angle, the projection at the i-th (i = 1, 2, . . . H s ) row can be defined as TransEncoder: Following the standard transformer architecture  where F j denotes the output of j-th TransEncoder block. After applying T identical TransEncoder blocks, the non-local relationship between projections at different view angles is accurately preserved in the output sequence F T ∈ R H s ×d ."
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Feature Mapping:,"The feature mapping module is designed for projecting the sequence data back to the sinogram. Concretely, ) and then fed into a linear projection layer to reduce the channel dimension from C to C s . Through these operations, the residual sinogram S R ∈ R C s ×H s ×W s of the same dimension as S L , is obtained. Finally, following the spirit of residual learning, S R is directly added to the input S L to produce the output of SE-Former, i.e., the predicted denoised sinogram"
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,2.2,Spatial-Spectral Reconstruction Transformer (SSR-Former),The SSR-Former is designed to reconstruct the denoised sinogram obtained from the SE-Former to the corresponding SPET images. As depicted in Fig. 
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Spatial-Spectral Transformer Layer (SSTL):,As shown in Fig. 
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Window-based Spatial Multi-Head Self-Attention (W-SMSA):,"Denoting the input feature embedding of certain W-SMSA as e in ∈ R C I ×H I ×W I , where H I ,W I and C I represent the height, width and channel dimension, respectively. As depicted in Fig. "
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Global Frequency Parser (GFP):,"After passing the W-SMSA, the feature e spa are already spatially representative, but still lack accurate spectral representations in the frequency domain. Hence, we propose a GFP module to rectify the high-frequency component in the frequency domain. As illustrated in Fig.  The parameterized attentive map A can adaptively adjust the frequency components of the frequency domain and compel the network to restore the high-frequency part to resemble that of the supervised signal, i.e., the corresponding real SPET images (ground truth), in the training process. Finally, we reverse e spe back to the image domain by adopting 2D IDFT, thus obtaining the optimized feature e spa = DFT (e spe ). In this manner, more high-frequency details are preserved for generating shaper constructions."
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,2.3,Objective Function,"The objective function for our TriDo-Former is comprised of two aspects: 1) a sinogram domain loss L sino and 2) an image domain loss L img . The sinogram domain loss aims to narrow the gap between the real SPET sinograms S S and the EPET sinograms S E that are denoised from the input LPET sinograms. Considering the critical influence of sinogram quality, we apply the L2 loss to increase the error punishment, thus forcing a more accurate prediction. It can be expressed as: For the image domain loss, the L1 loss is leveraged to minimize the error between the SPET images I S and the EPET images I E while encouraging less blurring, which can be defined as: Overall, the final objective function is formulated by the weighted sum of the above losses, which is defined as: where λ is the hyper-parameters to balance these two terms."
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,2.4,Details of Implementation,Our network is implemented by Pytorch framework and trained on an NVIDIA GeForce GTX 3090 with 24 GB memory. The whole network is trained end-to-end for 150 epochs in total using Adam optimizer with the batch size of 4. The learning rate is initialized to 4e-4 for the first 50 epochs and decays linearly to 0 for the remaining 100 epochs. The number T of the TransEncoder in SE-Former is set to 2 and the window size M is set to 4 in the W-SMSA of the SSR-Former. The weighting coefficient λ in Eq. ( 
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,3.0,Experiments and Results,"Datasets: We train and validate our proposed TriDo-Former on a real human brain dataset including 8 normal control (NC) subjects and 8 mild cognitive impairment (MCI) subjects. All PET scans are acquired by a Siemens Biograph mMR system housed in Biomedical Research Imaging Center. A standard dose of 18F-Flurodeoxyglucose ([ 18 F] FDG) was administered. According to standard protocol, SPET sinograms were acquired in a 12-minute period within 60-minute of radioactive tracer injection, while LPET sinograms were obtained consecutively in a 3-min shortened acquisition time to simulate the acquisition at a quarter of the standard dose. The SPET images which are utilized as the ground truth in this study were reconstructed from the corresponding SPET sinograms using the traditional OSEM algorithm  Experimental Settings: Due to the limited computational resources, we slice each 3D scan of size 128 × 128 × 128 into 128 2D slices with a size of 128 × 128. The Leave-One-Out Cross-Validation (LOOCV) strategy is applied to enhance the stability of the model with limited samples. To evaluate the performance, we adopt three typical quantitative evaluation metrics including peak signal-to-noise (PSNR), structural similarity index (SSIM), and normalized mean squared error (NMSE). Note that, we restack the 2D slices into complete 3D PET scans for evaluation. Comparative Experiments: We compare our TriDo-Former with four direct reconstruction methods, including (1) OSEM "
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Evaluation on Clinical Diagnosis:,"To further prove the clinical value of our method, we further conduct an Alzheimer's disease diagnosis experiment as the downstream task. Specifically, a multi-layer CNN is firstly trained by real SPET images to distinguish between NC and MCI subjects with 90% accuracy. Then, we evaluate the PET images reconstructed by different methods on the trained classification model. Our insight is that, if the model can discriminate between NC and MCI subjects from the reconstructed images more accurately, the quality of the reconstructed images and SPET images (whose quality is preferred in clinical diagnosis) are closer. As shown in Fig.  Ablation Study: To verify the effectiveness of the key components of our TriDo-Former, we conduct the ablation studies with the following variants: (1) replacing SE-Former and SSR-Former with DnCNN "
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,4.0,Conclusion,"In this paper, we innovatively propose a triple-domain transformer, named TriDo-Former, for directly reconstructing the high-quality PET images from LPET sinograms. Our model exploits the triple domains of sinogram, image, and frequency as well as the ability of the transformer in modeling long-range interactions, thus being able to reconstruct PET images with accurate global context and sufficient high-frequency details. Experimental results on the real human brain dataset have demonstrated the feasibility and superiority of our method, compared with the state-of-the-art PET reconstruction approaches."
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,1.0,Introduction,"Inflammatory bowel disease (IBD) is a relatively common, but easily overlooked disease. Its insidious clinical presentation  As a result, many patients' images are degraded by respiration, involuntary movements and peristalsis. Furthermore, due to technical limitations, it is difficult to acquire HR images in all scan orientations. This limits the assessment of the complete volume in 3D. Given these problems, we aim to develop a novel method that can perform both motion correction (MC) and super-resolution (SR) to improve the quality of 3D IBD MRI and to support accurate interpretation and diagnosis. Motion can cause multiple issues for MR acquisition. Abdominal MRI scans are usually 2D multi-slice acquisitions  Despite these challenges, MC and SR are crucial because corrupted MR images can lead to inaccurate interpretation and diagnosis "
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Contribution:,"Our method (MoCoSR) alleviates the need for semantic knowledge and manual paired-annotation of individual structures and the requirement for acquiring multiple image stacks from different orientations, e.g.,  There are several methodological contributions of our work: (1) First, to account for non-isotropic voxel sizes of abdominal images, we reconstruct spatial resolution from corrupted bowel MR images by enforcing cycle consistency. (2) Second, volumes are corrected by incorporating latent features in the LR domain. The complementary spatial information from unpaired quality images is exploited via cycle regularisation to provide an explicit constraint. Third, we conduct extensive evaluations on 200 subjects from a UK Crohn's disease study, and a public abdominal MRI dataset with realistic respiratory motion. (3) Experimental evaluation and analysis show that our MoCoSR is able to generate high-quality MR images and performs favourably against other, alter-native methods. Furthermore, we explore confidence in the generated data and improvements to the diagnostic process. (4) Experiments with existing models for predicting the degree of small bowel inflammation in Crohn's disease patients show that MoCoSR can retain diagnostically relevant features and maintain the original HR feature distribution for downstream image analysis tasks. Related Work: MRI SR. For learning-based MRI super-resolution,  Automated Evaluation of IBD. In the field of machine learning and gastrointestinal disease, "
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,2.0,Method,"Problem Formulation and Preliminaries: In 3D SR, the degradation process is modeled with: I LR = D(I HR ; k I , ↓ s )+n, D() represents the downsampling with the blur kernel k I , scaling factor s, and noise n. In this work, we propose the motion corruption term M , which operates on LR latent space. And our MC-SR model can be refined to MoCoSR Concept: As shown in Fig.  Loss Functions: Rather than aiming to reconstruct motion-free and HR images in high dimensional space with paired data, we propose to regularize in low dimensional latent space to obtain a high quality LR feature that can be used for upscaling. A L MC between ZQ downsampled from QHR and Z Q cycled after LLR and CLR, defines in an unpaired manner as follows: As the ultimate goal, SR focuses on the recovery and upscaling of detailed high-frequency feature, L SR is proposed to optimize the reconstruction capability by passing through cycles at various spaces: The dual adversarial L DAdv is applied to improve the generation capability of the two sets of single generating processes in the cyclic network: The corresponding two task-specific discriminators L DM C and L DSR for discriminating between corrupted and quality images followed are used for the purpose of staged reconstruction Z Q and Ŷ of MC at latent space and SR at spatial space, respectively. Furthermore, a joint cycle consistency loss is used to improve the stability of training in both spaces: For MoCoSR generators, the joint loss function is finally defined as follows: Network Architecture: In the paired encoder-decoder structure, we developed a 3D Global and Local Residual Blocks (GLRB) with Local Residual Modules (LRM) in Fig. "
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,3.0,Experiments,"Data Degradation: We use 64 × 64 × 64 patches. For downsampling and the degradation associated with MRI scanning, (1) Gaussian noise with a standard deviation of 0.25 was added to the image. (2) Truncation at the k-space, retaining only the central region of the data. On top of this, we have developed a motion simulation process to represent the pseudo-periodic multi-factor respiratory motion (PMRM) that occurs during scanning, as shown in Fig. "
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Setting:,"We compare with interpolation of bicubic and bilinear techniques, rapid and accurate MR image SR (RAISR-MR) with hashing-based learning "
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Results:,"The quantitative results are presented in Table  Complete results including LR degraded image, SR image reconstructed by MRESR, CMRSR, and MoCoSR, are shown in Table  Discussion: According to the sensitivity analysis and comparison results, our MoCoSR method shows superior results compared to the forward adversarial reconstruction algorithms and encoder-decoder structures. Combining multiscale image information in the feature space of different resolution image domains yields better results than inter-domain integration. The cycle consistency network splits the different resolution spaces and latent space, which facilitates the flexibility of the neural network to customize the MC according to the specific purpose and ensures consistency of the corrected data with the unpaired data. Furthermore, although these methods can obtain acceptable SSIM and PSNR, the key features used by the classifier for downstream tasks are potentially lost during the reconstructions. Conversely, the reconstruction result will implicitly cause domain shift. This leads to a distribution shift in the samples, which makes the disease prediction biased as shown in Fig. "
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,4.0,Conclusion,"MoCoSR is a DL-based approach to reconstruct high-quality SR MRI. MoCoSR is evaluated extensively and compared to the various image SR reconstruction algorithms on a public abdominal dataset, simulating different degrees of respiratory motion, and an IBD dataset with inherent motion. MoCoSR demonstrated superior performance. To test if our learned reconstruction preserves clinically relevant features, we tested on a downstream disease scoring method and found no decrease in disease prediction performance with MoCoSR."
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,1.0,Introduction,"Recent successes of machine learning algorithms in computer vision and natural language processing suggest that training on large datasets is beneficial for model performance  Although image harmonization and synthesis  We propose an image synthesis method for diverse modalities based on multimodal metric learning and k-NN regression. To learn the metric, we use image retrieval as the target task, which aims at embedding images such that matching pairs of different modalities are close in the embedding space. We use a triplet loss  We use knee osteoarthritis as the driving medical problem and evaluate our proposed approach using the OAI image data. Specifically, we predict cartilage thickness maps obtained from 3D MR images using 2D radiographs. This is a highly challenging task and therefore is a good test case for our approach for the following reasons: 1) cartilage is not explicitly visible on radiographs. Instead, the assessment is commonly based on joint space width (JSW), where decreases in JSW suggest decreases in cartilage thickness  The main contributions of our work are as follows. 1. We propose an image synthesis method for diverse modalities based on multimodal metric learning using image retrieval and k-NN regression. We carefully construct the learning scheme to account for longitudinal data. 2. We extensively test our approach for osteoarthritis, where we synthesize cartilage thickness maps derived from 3D MR using 2D radiographs. 3. Experimental results show the superiority of our approach over commonly used image synthesis methods, and the synthesized images retain sufficient information for downstream tasks of KL grading and progression prediction. Left top: encoding the region of interest from radiographs, extracted using the method from "
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,2.0,Method,"In this work, we use multi-modal metric learning followed by k-NN regression to synthesize images of diverse modalities. Our method requires 1) a database containing matched image pairs; 2) target images aligned to an atlas space."
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,2.1,Multi-modal Longitudinally-Aware Metric Learning,"Let {(x i a , y i a )} be a database of multiple paired images with each pair containing two modalities x and y of the a-th subject and i-th timepoint if longitudinal data is available. We aim to learn a metric that allows us to reliably identify related image pairs, which in turn relate structures of different modalities. Specifically, we train our deep neural network via a triplet loss so that matching image pairs are encouraged to obtain embedding vectors closer to each other than mismatched pairs. Figure  Denoting the two CNNs as f (•; θ) and g(•; φ), where θ and φ are the CNN parameters, we measure the feature distance between two images x and y using cosine similarity where the output of f and g are vectors of the same dimension (2) where m is the margin for controlling the minimum distance between positive and negative pairs. We sum over all subjects at all timepoints for each batch. To avoid explicitly tracking the subjects in a batch, we can simplify the above equation by randomly picking one timepoint per subject during each training epoch. This then simplifies our multi-modal longitudinally aware triplet loss to a standard triplet loss of the form"
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,2.2,Image Synthesis,"After learning the embedding space, it can be used to find the most relevant images with a new input, as shown in Fig.  where the weights are normalized weights based on the cosine similarities. This requires us to work in an atlas space for the modality y, where all images in the database S I are spatially aligned. However, images of the modality x do not need to be spatially aligned, as long as sensible embeddings can be captured by f θ . As we will see, this is particularly convenient for our experimental setup, where the modality x is a 2D radiograph and the modality y is a cartilage thickness map derived from a 3D MR image, which can easily be brought into a common atlas space. As our synthesized image, ŷ, is a weighted average of multiple spatially aligned images, it will be smoother than a typical image of the target modality. However, we show in Sect. 3 that the synthesized images still retain the general disease patterns and retain predictive power. Note also that our goal is not image retrieval or image reidentification, where one wants to find a known image in a database. Instead, we want to synthesize an image for a patient who is not included in our image database. Hence, we expect that no perfectly matched image exists in the database and therefore set k > 1. Based on theoretical analyses of k-NN regression "
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,3.0,Experimental Results,"Fig.  patients between the ages of 45 to 79 years at the time of recruitment. Each patient is longitudinally followed for up to 96 months. Images. The OAI acquired images of multiple modalities, including T2 and DESS MR images, as well as radiographs. We use the paired DESS MR images and radiographs in our experiments. After excluding all timepoints when patients do not have complete MR/radiograph pairs, we split the dataset into three sets by patient (i.e., data from the same patient are in the same sets): Set 1) to train the image retrieval model (2, 000 patients; 13, 616 pairs). This set also acts as a database during image synthesis; Set 2) to train the downstream task (1, 750 patients; 16, 802 pairs); Set 3) to test performance (897 patients; 8, 418 pairs). Preprocessing. As can be seen from the purple dashed box in Fig. "
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,3.2,Network Training,"During multi-modal metric learning, our two branches use the ResNet-18 [10] model with initial parameters obtained by ImageNet pre-training "
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,3.3,Results,"This section shows our results for image retrieval, synthesis, and downstream tasks based on the questions posed above. All images synthesized from MRIS are based on the weighted average of the retrieved top k = 20 thickness maps. Image Retrieval. To show the importance of the learned embedding space, we perform image retrieval on the test set, where our goal is to correctly find the corresponding matching pair. Since our training process does not compare images of the same patient at different timepoints, we test using only the baseline images for each patient  Osteoarthritis is commonly assessed via Kellgren-Lawrence grade "
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,4.0,Conclusion,"In this work, we proposed an image synthesis method using metric learning via multi-modal image retrieval and k-NN regression. We extensively validated our approach using the large OAI dataset and compared it with direct synthesis approaches. We showed that our method, while conceptually simple, can effectively synthesize alignable images of diverse modalities. More importantly, our results on the downstream tasks showed that our approach retains diseaserelevant information and outperforms approaches based on direct image regression. Potential shortcomings of our approach are that the synthesized images tend to be smoothed due to the weight averaging and that spatially aligned images are required for the modality to be synthesized."
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,3.0,Does our prediction retain disease-relevant information for downstream tasks?,We test the performance of our predicted cartilage thickness maps in predicting KLG and osteoarthritis progressors; 4. How does our approach compare to existing image synthesis models? We show that our approach based on simple k-NN regression compares favorably to direct image synthesis approaches.
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,3.1,Dataset,"We perform a large-scale validation of our method using the Osteoarthritis Initiative (OAI) dataset on almost 40,000 image pairs. This dataset includes 4, 796"
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,1.0,Introduction,"X-ray computed tomography (CT) is an established diagnostic tool in clinical practice; however, there is growing concern regarding the increased risk of cancer induction associated with X-ray radiation exposure  In recent years, the success of deep learning has attracted much attention in the field of sparse-view CT reconstruction. Existing learning-based approaches mainly include image-domain methods  Motivation. We view the sparse-view CT image reconstruction as a two-step task: artifact removal and detail recovery. For the former, few work has investigated the fact that the artifacts exhibit similar pattern across different sparseview scenarios, which is evident in Fourier domain as shown in Fig.  While Fourier domain band-pass maps help capture the pattern of the artifacts, restoring the image detail contaminated by strong artifacts may still be difficult due to the entanglement of artifacts and details in the residues. Consequently, we propose a self-guided artifact refinement network (SeedNet) that provides supervision signals to aid FreeNet in refining the image details contaminated by the artifacts. With these novel designs, we introduce a simple yet effective model termed FREquency-band-awarE and SElf-guidED network (FreeSeed), which enhances the reconstruction by modeling the pattern of artifacts from a frequency perspective and utilizing the artifact to restore the details. FreeSeed achieves promising results with only image data and can be further enhanced once the sinogram is available. Our contributions can be summarized as follows: 1) a novel frequency-bandaware network is introduced to efficiently capture the pattern of global artifacts in the Fourier domain among different sparse-view scenarios; 2) to promote the restoration of heavily corrupted image detail, we propose a self-guided artifact refinement network that ensures targeted refinement of the reconstructed image and consistently improves the model performance across different scenarios; and 3) quantitative and qualitative results demonstrate the superiority of FreeSeed over the state-of-the-art sparse-view CT reconstruction methods. "
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,2.1,Overview,"Given a sparse-view sinogram with projection views N v , let I s and I f denote the directly reconstructed sparse-and full-view images by FBP, respectively. In this paper, we aim to construct an image-domain model to effectively recover I s with a level of quality close to I f . The proposed framework of FreeSeed is depicted in Fig. "
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,2.2,Frequency-Band-Aware Artifact Modeling Network,"To learn the globally distributed artifact, FreeNet uses band-pass Fourier convolution blocks as the basic unit to encode artifacts from both spatial and frequency aspects. Technically, Fourier domain knowledge is introduced by fast Fourier convolution (FFC)  where F real and F -1 real denote the real Fourier transform and its inverse version, respectively. f denotes vanilla convolution. "" "" is the Hadamard product. Specifically, for c-th channel frequency domain feature Z (c) in ∈ C U ×V (c = 1, ..., C in ), the corresponding band-pass attention map H (c) ∈ R U ×V is defined by the following Gaussian transfer function: where D (c) is the c-th channel of the normalized distance map with entries denoting the distance from any point (u, v) to the origin. Two learnable parameters, w (c) > 0 and d , represent the bandwidth and the normalized inner radius of the band-pass map, respectively, and are initialized as 1 and 0, respectively. is set to 1 × 10 -12 to avoid division by zero. The right half part of the second row of Fig.  The pixel-wise difference between the predicted artifact A of FreeNet and the groundtruth artifact A f = I s -I f is measured by 2 loss: (4)"
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,2.3,Self-guided Artifact Refinement Network,"Areas heavily obscured by the artifact should be given more attention, which is hard to achieve using only FreeNet. Therefore, we propose a proxy network SeedNet that provides supervision signals to focus FreeNet on refining the clinical detail contaminated by the artifact under the guidance of the artifact itself. SeedNet consists of residual Fourier convolution blocks. Concretely, given sparseview CT images I s , FreeNet predicts the artifact A and restored image I = I s -A; the latter is fed into SeedNet to produce targeted refined result I. To guide the network on refining the image detail obscured by heavy artifacts, we design the transformation T that turns A into a mask M using its mean value as threshold: M = T ( A), and define the following masked loss for SeedNet: (5)"
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,2.4,Loss Function for FreeSeed,"FreeNet and SeedNet in our proposed FreeSeed are trained in an iterative fashion, where SeedNet is updated using L mask defined in Eq. (  where α > 0 is empirically set as 1. The pseudo-code for the training process and the exploration on the selection of α can be found in our Supplementary Material. Once the training is complete, SeedNet can be dropped and the prediction is done by FreeNet."
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,2.5,Extending FreeSeed to Dual-Domain Framework,"Dual-domain methods are effective in the task of sparse-view CT reconstruction when the sinogram data are available. To further enhance the image reconstruction quality, we extend FreeSeed to the dominant dual-domain framework by adding the sinogram-domain sub-network from DuDoNet "
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,3.1,Experimental Settings,"We conduct experiments on the dataset of ""the 2016 NIH-AAPM Mayo Clinic Low Dose CT Grand Challenge""  The models are implemented in PyTorch "
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,3.2,Overall Performance,"We compare our models (FreeSeed and FreeSeed dudo ) with the following reconstruction methods: direct FBP, DDNet  Not surprisingly, we find that the performance of conventional image-domain methods is inferior to the state-of-the-art dual-domain method, mainly due to the failure of removing the global artifacts. We notice that dual-domain methods underperform FBPConv when N v = 18 because of the secondary artifact induced by the inaccurate sinogram restoration in the ultra-sparse scenario. Notably, FreeSeed outperforms the dual-domain methods in most scenarios. Figure "
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,3.3,Ablation Study,By comparing the first two rows of Table 
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,4.0,Conclusion,"In this paper, we proposed FreeSeed, a simple yet effective image-domain method for sparse-view CT reconstruction. FreeSeed incorporates Fourier knowledge into the reconstruction network with learnable band-pass attention for a better grasp of the globally distributed artifacts, and is trained using a self-guided artifact refinement network to further refine the heavily damaged image details. Extensive experiments show that both FreeSeed and its dual-domain counterpart outperformed the state-of-the-art methods. In future, we will explore FFC-based network for sinogram interpolation in sparse-view CT reconstruction."
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 24.
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,1.0,Introduction,"With the success of deep learning in the field of computer vision and image processing, many deep learning-based methods have been proposed and achieved promising results in low-dose CT (LDCT) denoising  To address this issue, a few studies  In this paper, we focus on taking advantage of the inherent anatomical semantics in LDCT denoising from a contrastive learning perspective  Second, to exploit inherent anatomical semantics, we present the MAC-Net that employs a disentangled U-shaped architecture  Our contributions are summarized as follows. 1) We propose a novel ASCON framework to explore inherent anatomical information in LDCT denoising, which is important to provide interpretability for LDCT denoising. 2) To better explore anatomical semantics in MAC-Net, we design an ESAU-Net, which utilizes a channel-wise self-attention mechanism to capture both local and global contexts. 3) We propose a MAC-Net that employs a disentangled U-shaped architecture and incorporates both global non-contrastive and local contrastive modules. This enables the exploitation of inherent anatomical semantics at the patch level, as well as improving anatomical consistency at the pixel level. 4) Extensive experimental results demonstrate that our ASCON outperforms other state-ofthe-art methods, and provides anatomical interpretability for LDCT denoising."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,2.1,Overview of the Proposed ASCON,"Figure  Then, to explore inherent anatomical semantics and remain inherent anatomical consistency, the denoised CT Y and NDCT Y are passed to the MAC-Net to compute a global MSE loss L global in a patch-wise non-contrastive module and a local infoNCE loss L local in a pixel-wise contrastive module. During training, we use an alternate learning strategy to optimize ESAU-Net and MAC-Net separately, which is similar to GAN-based methods "
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,2.2,Efficient Self-attention-Based U-Net,"To better leverage anatomical semantic information in MAC-Net and adapt to the high-resolution input, we design the ESAU-Net that can capture both local and global contexts during denoising. Different from previous works that only use self-attention in the coarsest level  Specifically, in each level, given the feature map F l-1 as the input, we first apply a 1×1 convolution and a 3×3 depth-wise convolution to aggregate channelwise contents and generate query (Q), key (K), and value (V ) followed by a reshape operation, where Q ∈ R C×HW , K ∈ R C×HW , and V ∈ R C×HW (see Fig.  where w(•) first reshapes the matrix back to the original size C ×H ×W and then performs 1 × 1 convolution; α is a learnable parameter to scale the magnitude of the dot product of K and Q. We use multi-head attention similar to the standard multi-head self-attention mechanism "
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,2.3,Multi-scale Anatomical Contrastive Network,"Overview of MAC-Net. The goal of our MAC-Net is to exploit anatomical semantics and maintain anatomical embedding consistency, First, a disentangled U-shaped architecture  16 after four down-sampling layers, and learn local representation F l ∈ R 64×H×W by removing the last output layer. And we cut the connection between the coarsest feature and its upper level to make F g and F l more independent "
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,Patch-Wise Non-contrastive Module.,"To better learn anatomical representations, we introduce a patch-wise non-contrastive module, also shown in Fig.  We then select the top-4 positive patches {f g } j∈P (i) based on s(i, j), where P (i) is a set of selected patches (i.e., |P (i) | = 4). To obtain patch-level features g (i) ∈ R 512 for each patch f (i) g and its positive neighbors, we aggregate their features using global average pooling (GAP) in the patch dimension. For the local representation of f (i) g , we select positive patches as same as P (i) , i.e., {f (j) g } j∈P (i) . Formally, From the patch-level features, the online network outputs a projection z g (i) = p g (g (i) ) and a prediction q (z g (i) ) while target network outputs the target projection z g (i) = p g (g (i) ). The projection and prediction are both multilayer perceptron (MLP). Finally, we compute the global MSE loss between the normalized prediction and target projection  where N g pos is the indices set of positive samples in the patch-level embedding. Pixel-Wise Contrastive Module. In this module, we aim to improve anatomical consistency between the denoised CT and NDCT using a local InfoNCE loss  neg |)×256 . The local InfoNCE loss in the pixel level is defined as where N l pos is the indices set of positive samples in the pixel level. v and v (j) l ∈ R 256 are the query, positive, and negative sample in z (i) l , respectively."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,2.4,Total Loss Function,"The final loss is defined as L = L global + L local + λL pixel , where L pixel consists of two common supervised losses: MSE and SSIM, defined as L pixel = L MSE + L SSIM . λ is empirically set to 10."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,3.1,Dataset and Implementation Details,We use two publicly available low-dose CT datasets released by the NIH AAPM-Mayo Clinic Low-Dose CT Grand Challenge in 2016 
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,3.2,Performance Comparisons,"Quantitative Evaluations. We use three widely-used metrics including peak signal-to-noise ratio (PSNR), root-mean-square error (RMSE), and SSIM. Table  Ablation Studies. We start with a ESAU-Net using MSE loss and gradually insert some loss functions and our MAC-Net. Table "
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,4.0,Conclusion,"In this paper, we explore the anatomical semantics in LDCT denoising and take advantage of it to improve the denoising performance. To this end, we propose an Anatomy-aware Supervised CONtrastive learning framework (ASCON), consisting of an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net), which can capture both local and global contexts during denoising and exploit inherent anatomical information. Extensive experimental results on Mayo-2016 and Mayo-2020 datasets demonstrate the superior performance of our method, and the effectiveness of our designs. We also validated that our method introduces interpretability to LDCT denoising."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,Acknowledgements,. This work was supported in part by 
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 34.
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,1.0,Introduction,"Image registration has been widely studied in both academia and industry over the past two decades. In general, the goal of deformable image registration is to estimate a suitable nonlinear transformation that overlaps the pair of images with corresponding spatial relationships  A variety of approaches have been proposed to handle the noncorrespondence problem in medical image registration. These methods can be roughly divided into three main categories: 1) Cost function masking. The authors of  Therefore, to effectively address the non-correspondence problem in registering pathology images, it is necessary to incorporate both a data-independent segmentation module and a modality-adaptive inpainting module into the registration pipeline. To bridge this gap, we introduce the semantic information of the category based on  -We propose a collaborative learning method for the simultaneous optimization of registration, segmentation, and inpainting networks. -We show the effectiveness of using mutual information minimization in an unsupervised manner for pathological image segmentation and registration by incorporating semantic information through the registration process. -We perform a series of experiments to validate our method's superiority in accurately finding lesions and effectively registering pathological images."
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,2.0,Method,Our proposed framework (Fig. 
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,2.1,Collaborative Optimization,"The most critical problem in pathological image registration is identifying and dealing with the lesion area. If we naively register a source pathological image S to a template T without caring about the lesion boundary, the deformation field near the boundary would be uncontrollable because a healthy template does not have a lesion. A possible approach here is to initialize an inflating boundary containing the lesion area, followed by calculating the registration loss either outside of the boundary only or based on a modified S that is inpainted within the given boundary. However, the registration error has no sensitivity to the location of the inflated boundary as long as it is larger than the real one. On the other hand, if we compared the inpainted image and the pathological image S within the boundary only, we can notice that their dissimilarity increases when the boundary shrinks as the inpainting algorithm only generates healthy parts. This mechanism can then induce a segmentation module that segments the lesion as the foreground and the remaining as the background, which iteratively serves as the input mask for the inpainting module. Further, as the registration loss is calculated based on the registered inpainted image and the target image, the registration provides a regularization for the inpainting module such that the inpainting is specialized to facilitate the registration. Specially for the input and output of the three modules, RegNet takes images S and T as input and generates the deformation field from S to T and T to S as ϕ ST and ϕ T S respectively. InpNet takes the background (foreground) cropped by SegNet and image T •ϕ T S warped by RegNet as input and outputs foreground (background) with a normal appearance. SegNet takes the pathology image S as input and employs the normal foreground and background inpainted by InpNet to segment the lesion region based on MMI. SegNet and InpNet are actually in an adversarial relationship. Through this joint optimization approach, the three networks collectively work to achieve registration and segmentation of pathological images under entirely unsupervised conditions, without being limited by the specific network structure. For the sake of simplicity, we employ a Unet-like "
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,2.2,Network Modules,"RegNet. The primary objective of registration is to generate a deformation field that minimizes the dissimilarity between the source image (S) and the template image (T). The deformation is usually required to satisfy constraints like smoothness and even diffeomorphism. In terms of pathological image registration, the deformation field is only valid off the lesion area. Thus the registration loss should be calculated on the normal area only. Suppose that the lesion area is already obtained as θ(S) and inpainted with normal tissue, the registration loss can then be formulated as where ϕ ST = ψ(S, T ), ϕ T S = ψ(T, S) are the deformation fields that warp S → T and T → S respectively. The symbol • denotes element-wise multiplication. Furthermore, L sym denotes the registration loss of SymNet  SegNet. Minimal Mutual Information (MMI) is a typically used unsupervised segmentation method that distinguishes foreground from background. However, for a pathological image, the lesion regions often have a similar intensity to normal tissues near the boundary, which prevents the MMI from accurate segmentation without the semantic information. To address this limitation, we warp a healthy image T onto a pathology image S using a deformation field ϕ T S = ψ(T, S). This process maximizes the mutual information between corresponding regions of the two images and minimizes that of non-corresponding regions, thereby facilitating accessible lesion segmentation with MMI. Let Ω ∈ R denote the image domain, M denote the mask, F θ = Ω•M and B θ = Ω•M denote the foreground and background, where M = 1 -M, M ∈ {0, 1}. Regarding a pathological image S, when the background (normal) is given, the inpainted foreground (normal) will be different from the true foreground (lesion). When the foreground (lesion) is given, the inpainted background will remain the same as the background (normal). Thus we can formulate the adversarial loss of unsupervised segmentation as where D is the distance function given by localized normalized cross-correlation (LNCC)  InpNet. Let M denote the mask and ϕ T S denote the deformation field from T to S. To handle the potential domain differences between the masked image S •M and the aligned image T •ϕ T S , InpNet employs two encoders. The adversarial loss function of InpNet is represented as L MI . To incorporate semantic information, we include an additional similarity term L sim that prevents InpNet from focusing too heavily on the foreground (lesion) and encourages it to produce healthy tissue. The proposed loss function L inp is then formulated as the combination of mutual information loss defined through the normalized correlation coefficient (NCC) and similarity loss through the mean squared error (MSE): with where λ represents the weight that balances the contributions of mutual information loss and similarity loss, and T M denotes image T after histogram matching. We modify the histogram of T •ϕ T S to be similar to that of S in order to mitigate the effects of domain differences."
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,3.0,Experiments,"Our experimental design focuses on two common clinical tasks: atlas-based registration, which involves warping pathology images to a standard atlas template, and longitudinal registration, which involves registering pre-operative images to post-operative images for the purpose of tracking changes over time. Dataset and Pre-processing. For our study, we selected the ICBM 152 Nonlinear Symmetric template as our atlas "
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,,3D Pseudo Brain MRI.,"To evaluate the performance of atlas-based registration, it is essential to have the correct mapping of pathological regions to healthy brain regions. To create such a mapping, we created a pseudo dataset by utilizing images from the OASIS-1 and BraTS2020. From the resulting t1 sequences, a pseudo dataset of 300 images was randomly selected for further analysis. Appendix B provides a detailed process for creating the pseudo dataset. Real Data with Landmarks. BraTS-Reg 2022  Atlas-Based Registration. After creating the pseudo dataset, we warped brain MR images without tumors to the atlas and used the resulting deformation field as the gold standard for evaluation. We then evaluated the mean deformation error (MDE)  Longitudinal Registration. To perform the longitudinal registration task, we registered each pre-operative scan to the corresponding follow-up scan of the same patient and measured the mean target registration error (TRE) of the paired landmarks using the resulting deformation field. For this purpose, we leveraged SegNet, trained on BraTS2020, to segment the tumor of BraT-SReg2022 and separated the landmarks into two regions: near tumor and far from tumor. Figure  To quantitatively evaluate the segmentation capability of our proposed framework, we compared its performance with other unsupervised segmentation techniques methods, including unsupervised clustering toolbox AUCseg  Ablation Study. We compared the performance of the InpNet trained with histogram matching (HM) and the SegNet trained with ground truth masks (Supervised). The results, shown in Table "
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,4.0,Conclusion,"In this paper, we proposed a novel tri-net framework for joint image registration and unsupervised segmentation in medical imaging based on mutual information minimization in collaborative learning. Our experiments demonstrate that the proposed framework is effective for both atlas-based and longitudinal pathology image registration. We also observed that the accuracy of the segmentation network is significantly influenced by the quality of the inpainting, which, in turn, affects the registration outcome. In the future, our research will focus on enhancing the performance of InpNet to address domain differences better to improve the registration results."
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 51.
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,1.0,Introduction,"Cardiac motion estimation is vital in evaluating cardiac function, detecting heart diseases, and understanding cardiac biomechanics. Deformable image registration (DIR) is the critical technique of cardiac motion estimation. It minimizes the differences between the warped moving and fixed images to estimate a displacement vector field (DVF). Unsupervised deep-learning-based image registration has recently become mainstream due to the required non-annotation data  The probabilistic generative model shows potential in the unsupervised learning registration  This paper proposed a novel variational image registration model to cope with the above issues by employing the Transformers with the cross-attention mechanism and introducing an importance-weighted ELBO (iwELBO)  -A novel VAE network architecture is proposed, which employs the Transformer architecture to focus on cross-attention between the moving and fixed images. The predictive results of the transformation parameter distribution using our architecture are more accurate than traditional VAE architecture. -We optimized the importance-weighted ELBO in the variational image registration model. We use approximated aggregated posterior as the prior to regularizing posterior. To our best knowledge, we are the first to combine the iwELBO and aggregated posterior to close the gap between the real and variational posterior. -A parametric transformation based on multi-supports CSRBFs is embedded in our variational registration model. By imposing a sparse constraint, the coefficients of multi-CSRBFs are regularized to be sparse to select the optimal support for multi-support CSRBFs. The parametric transformation model improves registration accuracy significantly and makes it easy to regularize the smoothness of DVFs. 2 Proposed Method"
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,2.1,Importance-Weighted Variational Image Registration Model,"Given the moving and fixed images M, F , and n control points {p i } n i=1 , the parametric transformation based on multi-supports CSRBFs is ) is a CSRBF with support c; υ -p i 2 is the Euclidean distance between the pixel υ and p i . s different supports {c k } s are provided for each CSRBF. z = {z k } s k=1 , z k = {z k,i } n i=1 is the latent variable whose distribution is required to be estimated. The parametric transformation can control deformations using different supports. By imposing sparse constraints, selecting the optimal support from these given supports is possible, leading to more flexible deformation results. The variational registration model aims to estimate the posterior of p(z|F, M ). In the vanilla variational registration model, q(z|F, M ) is estimated to approximate p(z|F, M ) by optimizing ELBO = E z ∼q(z|F,M ) log p(F |z ,M )p(z ) q(z|F,M ) , where p(F |z, M) is the probability of occurring F when the moving image M is deformed using a transformation f z with latent variables z. It can be expressed as Boltzmann distribution p(F |z, M) ∝ e sim(F,M (fz )) using a similarity metric sim. p(z) is the prior of z. The importance-weighted evidence lower bound (iwELBO)  where z 1 , . . . , z K are K-samples of latent variable z sampled from q(z|F, M ). It is assumed that q(z|F, M ) ∼ N (μ(F, M ), Σ(F, M )), and z 1 , . . . , z K is sampled as , the gradient of iwELBO can be interpreted as normalized importance-weighted average gradients of each sample, which implies the sample with larger w k contributes more to iwELBO. It is challenging to compute w k directly due to the high dimensional latent variable z k . We tackle this problem by a trick. iwELBO is a tighter evidence lower bound of the log-likelihood of data; it approaches the log-likelihood log p(F |M ) as K → ∞. From the view of image registration, the iwELBO tends to converge to a transformation with the optimal w k from K samples. When a large hyperparameter λ is used, ) is relative smaller compared with the similarity term. That implies the iwELBO prefers the sample z k leading to the optimal similarity between the warped moving and fixed images, which is beneficial to push the network to predict more accurate z. Besides, Huang et al. "
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,2.2,Aggregate Posterior as the Prior,"A simple prior, such as the standard Normal in VAE, incurred over-regularization on the posterior and widened the gap between the variational posterior and the real posterior. Many researchers resolve this mismatch by proposing various priors  The optimal prior should maximize the expectation of iwELBO: ) It can be derived that the optimal prior p * (z) is approximated as the aggregated posterior E p(F,M ) q(z|F, M ). Substituting the optimal prior p * (z) into Eq. (  where p 0 (z) is a simple given prior. To estimate the density ratio log p * (z ) p0(z ) , a binary discriminator T (z) is trained by maximizing  where σ is the sigmoid function. The discriminator is a neural network composed of four fully connected layers, with the final layer outputting density ratio. A dropout layer is added before the output to prevent the discriminator network from overfitting. When T (z) is well trained, where z k T Bz k is the bending energy of DVF using multi-supports CSRBFs aiming to regularize DVF smooth. The sampling size for k is 5; λ is 110000. We optimize our network by iterating a two-step procedure. The encoder is updated using Eq. ( "
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,2.3,Network,"Our network architecture consists of an encoder and a decoder, as shown in Fig.  The total loss function of our network L is combing the iwELBO and sparse constraints on z as L = iwELBO -z 1 ."
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,3.0,Experiments,"Four public cardiac datasets are used to evaluate our method, including MIC-CAI2009 "
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,3.1,Results,To compare the performance of our method with unsupervised registration networks KrebsDiff 
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,3.2,Ablation Study,Ablation experiments are performed on the ACDC+ dataset to validate the influence of different components in our method. Table 
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,4.0,Conclusion,"In this paper, we proposed a novel variational registration model using Transformer to pay attention to cross-attention between images. The importanceweighted ELBO and the aggregated posterior as prior close the gap between the real posterior and the variational posterior. Our transformation using multisupports CSRBFs generates flexible DVFs. Evaluation results on public cardiac datasets show that our method outperforms the state-of-art networks."
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_55.
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,1.0,Introduction,"Medical image registration is a fundamental task in medical imaging with applications ranging from multi-modal data fusion to temporal data analysis. In recent years, deep learning has advanced learning-based registration methods  Self-training is a widespread training strategy for semi-supervised learning  Contributions. We introduce a novel learning paradigm for unsupervised registration by adapting the concept of self-training to the problem. This involves two principal challenges. First, labeled data for the pre-training stage is unavailable, raising the question of how to generate initial pseudo labels. Second, as a general problem in self-training, the negative impact of noise in the pseudo labels needs to be mitigated. In our pursuit to overcome these challenges, we made two decisive observations (see Fig. "
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,2.1,Problem Setup,"Given a data pair (F , M ) of a fixed and a moving image as input, registration aims at finding a displacement field ϕ that spatially aligns M to F . We address the task in an unsupervised setting, where training data consists of |T | unlabeled data pairs. Given the training data, we aim to learn a function f with parameters θ f , (partially) represented by a deep network, which predicts displacement fields as φ = f (F , M ; θ f )."
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,2.2,Cyclical Self-training,"We propose to solve the above problem with a cyclical self-training strategy visualized in Fig.  The approach is based on our empirical observation that a suitable optimization algorithm h can predict reasonable initial displacement fields φ(0) from random features provided by a network g (0) with random initialization θ (0) g , which is in Deep Network (initialised with )  t) with pseudo labels generated based on the features from the network g (t-1) from the previous stage. For optimal feature learning, the pseudo displacements from the optimizer are further refined and regularized. line with recent studies on the inductive bias of CNNs  with TRE( φ(1) i , φ(0) i ) denoting the mean over the element-wise target registration error between the displacement fields φ(1) i and φ(0) i . A critical problem of this basic setup is that the network might overfit the initial pseudo labels and learn to reproduce random features. Therefore, in the spirit of recent techniques from contrastive learning  Once the first stage of self-training has converged, we repeat the process T times. Specifically, at stage t, we generate refined pseudo labels with the trained network g (t-1) from the previous stage, initialize the learning network g (t) with the weights from g (t-1) and perform a warm restart on the learning rate to escape potential local minima from the previous stage."
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,2.3,Registration Framework,"Our proposed self-training scheme is a flexible, modular framework, agnostic to the input modality and the specific implementation of feature extractor g and optimizer h. This section describes our specific design choices for g and h for image and point cloud registration, with the former being our main focus. Image Registration. To extract features from 3D input volumes, we implement g a standard 3D CNN with six convolution layers with kernel sizes 3 × 3 × 3 and 32, 64, or 128 channels. Each convolution is followed by BatchNorm and ReLU, and every second convolution contains a stride of 2, yielding a downsampling factor of 8. The outputs for both images are mapped to 16-dimensional features using a 1 × 1 × 1 convolution and fed into a correlation layer  As the optimizer, we adapt the coupled convex optimization for learningbased 3D registration from  Point Cloud Registration. For point cloud registration, we implement the feature extractor as a graph CNN and rely on sparse loopy belief propagation for differentiable optimization, as introduced in "
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,3.1,Experimental Setup,"Datasets. We conduct our main experiments for inter-patient abdomen CT registration using the corresponding dataset of the Learn2Reg (L2R) Challenge We perform a second experiment for inhale-to-exhale lung CT registration on the DIR-Lab COPDGene dataset Implementation Details. We implement all methods in Pytorch and optimize network parameters with the Adam optimizer. For abdomen registration, we train for T = 8 stages, each stage comprising 1000 iterations with a batch size of 2. The learning rate follows a cosine annealing warm restart schedule, decaying from 10 -3 to 10 -5 at each stage. Hyper-parameters were set based on the DSC on three cases from the training set. For lung registration, the model converged after T = 5 stages of 60 epochs with batch size 4, with an initial learning rate of 0.001, decreased by a factor of 10 at epochs 40 and 52. Here, hyper-parameters were adopted from "
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,3.2,Results,"Abdomen. First, we analyze our method in several ablation experiments. In Fig. "
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,4.0,Conclusion,"We introduced a novel cyclical self-training paradigm for unsupervised registration. To this end, we developed a modular registration pipeline of a deep feature extraction network coupled with a differentiable optimizer, stabilizing learning from noisy pseudo labels through regularization and iterative, cyclical refinement. That way, our method avoids pitfalls of popular metric supervision (NCC, MIND), which relies on shallow features or image intensities and is prone to noise and local minima. By contrast, our supervision through optimization-refined and -regularized pseudo labels promotes learning task-specific features that are more robust to noise, and our cyclical learning strategy gradually improves the expressiveness of features to avoid local minima. In our experiments, we demonstrated the efficacy and flexibility of our approach, which outperformed the competing state-of-the-art methods and learning strategies for dense image-based abdomen and point cloud-based lung registration. In summary, we did not only present the first fully unsupervised self-training scheme but also a new perspective on unsupervised learning-based registration. In particular, we consider our strategy complementary to existing techniques (metric-based and contrastive learning), opening up the potential for combined training schemes in future work."
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_64.
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,1.0,Introduction,"Medical images are essential in diagnosing and monitoring various diseases and patient conditions. Different imaging modalities, such as computed tomography (CT) and magnetic resonance imaging (MRI), and different parametric images, such as T1 and T2 MRI, have been developed to provide clinicians with a comprehensive understanding of the patients from multiple perspectives  With the rise of deep learning, numerous studies have emerged and are dedicated to medical image synthesis  Different from natural images, most medical images are volumetric. Previous studies employ 2D networks as backbones to synthesize slices of medical volumetric data due to their ease of training  In this paper, we propose Make-A-Volume, a diffusion-based pipeline for cross-modality 3D brain MRI synthesis. Inspired by recent works that factorize video generation into multiple stages "
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,,Fig. 1. Overview of our proposed two-stage Make-A-Volume framework.,"A latent diffusion model is used to predict the noises added to the image and synthesize independent slices from Gaussian noises. We insert volumetric layers and quickly finetune the model, which extends the slice-wise model to be a volume-wise model and enables synthesizing volumetric data from Gaussian noises."
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,2.1,Preliminaries of DDPMs,"In the diffusion process, DDPMs produce a series of noisy inputs x 0 , x 1 , ..., x T , via sequentially adding Gaussian noises to the sample over a predefined number of timesteps T . Formally, given clean data samples which follow the real distribution x 0 ∼ q(x), the diffusion process can be written down with variances β 1 , ..., β T as Employing the property of DDPMs, the corrupted data x t can be sampled easily from x 0 in a closed form: where α t = 1β t , ᾱt = t s=1 α s , and ∼ N (0, 1) is the added noise. In the reverse process, the model learns a Markov chain process to convert the Gaussian distribution into the real data distribution by predicting the parameterized Gaussian transition p(x t-1 |x t ) with the learned model θ: In the model training, the model tries to predict the added noise with the simple mean squared error (MSE) loss:"
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,2.2,Slice-Wise Latent Diffusion Model,"To improve the computational efficiency of DDPMs that learn data in pixel space, Rombach et al. "
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,2.3,From Slice-Wise Model to Volume-Wise Model,"Figure  In the slice-wise model, distribution of the latent z ∈ R bs×c×h×w is learned by the U-Net, where b s , c, h, w are the batch size of slice, channels, height, and width dimensions respectively, and there is where little volume-awareness is introduced to the network. Since we target in synthesizing volumetric data and assume each volume consists of N slices, we can factorize the batch size of slices as b s = b v n, where B v represents the batch size of volumes. Now, volumetric layers are injected and help the U-Net learn to latent feature f ∈ R (bv×n)×c×h×w with volumetric consistency. The volumetric layers are basic 1D convolutional layers and the i-th volumetric layer l i v takes in feature f and outputs f as: Here, the 1D conv layers combined with the pretrained 2D conv layers, serve as pseudo 3D conv layers with little extra memory cost. We initialize the volumetric 1D convolution layers as Identity Functions for more stable training and we empirically find tuning is efficient. With the volume-aware network, the model learns volume data {x i } n i=1 , predicts {z i } n i=1 , and reconstruct {x i } n i=1 . For diffusion model training, in the first stage, we randomly sample timestep t for each slice. However, when tuning the second stage, the U-Net with volumetric layers learns the relationship between different slices in one volume. As a result, fixing t for each volume data is necessary and we encourage the small t values to be sampled more frequently for easy training. In detail, we sample the timestep t with replacement from multinomial distribution, and the pre-normalized weight (used for computing probabilities after normalization) for timestep t equals 2Tt, where T is the total number of timesteps. Therefore, we enable a seamless translation from the slice-wise model which processes slices individually, to a volume-wise model with better volumetric consistency."
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,3.0,Experiments,"Datasets. The experiments were conducted on two brain MRI datasets: SWIto-MRA (S2M) dataset and RIRE  Implementation Details. To summarize, for the S2M dataset, we randomly select 91 paired volumes for training and 20 paired volumes for inference; for the RIRE T1-to-T2 dataset, 14 volumes are randomly selected for training and 3 volumes are used for inference. All the volumes are resized to 256 × 256 × 100 for S2M and 256 × 256 × 35 for RIRE, where the last dimension represents the z-axis dimension, i.e., the number of slices in one volume for 2D image-to-image setting. Our proposed method is built upon U-Net backbones. We use a pretrained KL autoencoder with a downsampling factor of f = 4. We train our model on an NVIDIA A100 80 GB GPU. Quantitative Results. We compare our pipeline to several baseline methods, including 2D-based methods: (1) Pix2pix  (2) Palette  Qualitative Results. Figure "
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,4.0,Conclusion,"In this paper, we propose Make-A-Volume, a diffusion-based framework for crossmodality 3D medical image synthesis. Leveraging latent diffusion models, our method achieves high performance and can serve as a strong baseline for multiple cross-modality medical image synthesis tasks. More importantly, we introduce a generic paradigm for volumetric data synthesis by utilizing 2D backbones and demonstrate that fine-tuning volumetric layers helps the two-stage model capture 3D information and synthesize better images with volumetric consistency. We collected an in-house SWI-to-MRA dataset with clear blood vessels to evaluate volumetric data quality. Experimental results on two brain MRI datasets demonstrate that our model achieves superior performance over existing baselines. Generating coherent 3D and 4D data is at an early stage in the diffusion models literature, we believe that by leveraging slice-wise models and extending them to 3D/4D models, more work can help achieve better volume synthesis with reasonable memory requirements. In the future, we will investigate more efficient approaches for more high-resolution volumetric data synthesis."
Fast Reconstruction for Deep Learning PET Head Motion Correction,1.0,Introduction,"Positron emission tomography (PET) has been widely used in human brain imaging, thanks to the availability of a vast array of specific radiotracers. These compounds allow for studying various neurotransmitters and receptor dynamics for different brain targets  The first step of PET head motion correction is motion tracking. When head motion information is acquired, either frame-based motion correction or eventby-event (EBE) motion correction methods can be applied in the reconstruction workflow to derive motion-free PET images. EBE motion correction provides better results for real-time motion tracking compared to frame-based methods, as the latter does not allow for correction of motion that occurs within each dynamic frame  In this study, we proposed a new method to perform deep learning-based brain PET motion prediction across multiple subjects by utilizing high-resolution one-second fast reconstruction images (FRIs) with TOF. A novel encoder and data augmentation strategy was also applied to improve model performance. Ablation studies were conducted to assess the individual contributions of key method components. Multi-subject studies were conducted on a dataset of 20 subject and its results were quantitatively and qualitatively evaluated by MOLAR reconstruction studies and corresponding brain Region of Interest (ROI) Standard Uptake Values (SUV) evaluation."
Fast Reconstruction for Deep Learning PET Head Motion Correction,2.1,Data,"We identified 20 18 F-FPEB studies from a database of brain PET scans acquired on a conventional mCT scanner (Siemens, Germany) at the Yale PET center. All subjects are healthy controls and the mean activity injected was 3.90 ± 1.02 mCi. The mean translational and rotational motions across time and scans are 3.75 ± 6.88 mm and 3.30 ± 8.77 • , respectively. PET list-mode data and Vicra motion tracking information are available for each subject, as well as T1-weighted MR images and MR-space to PET-space transformation matrices. We consider data acquired between 60 and 90 min post injection (30 min total)."
Fast Reconstruction for Deep Learning PET Head Motion Correction,2.2,Fast Reconstruction Images,"To overcome the challenges when using low-quality, noisy PCI for motion correction, ultra-fast reconstruction techniques "
Fast Reconstruction for Deep Learning PET Head Motion Correction,2.3,Motion Correction Framework,"Network Architecture. We propose a modified version of the DL-HMC framework to learn rigid head motion in a supervised manner  Network Training and Inference. To train the network, we randomly sampled image pairs (t ref , t mov ) under the condition (t ref < t mov ). The network was optimized by minimizing the mean square error (MSE) between the predicted motion estimate and Vicra parameters. More specifically, the prediction error for a given pair of reference and moving clouds is defined as L( θ, θ) = θθ 2 with θ = [t x , t y , t z , r x , r y , r z ] the Vicra information for the three translational and three rotational parameters (t x , t y , t z ) and (r x , r y , r z ), respectively, and θ the network prediction. After training the model, we perform motion tracking inference by setting the image from first time point t ref = 3,600 (60 min post-injection) as the reference image and predict the motion from this reference image to all subsequent one-second image frames in the next 30 min (1,800 one-second time points)."
Fast Reconstruction for Deep Learning PET Head Motion Correction,3.0,Results,We performed quantitative and qualitative experiments to validate our approach. We evaluated motion correction performance by comparing our proposed method to DL-HMC  Table 
Fast Reconstruction for Deep Learning PET Head Motion Correction,,Method,"Val. loss Test Set Total loss Translation (mm) Rotation ( Quantitative Evaluation. For quantitative comparisons of motion tracking, we compare MSE loss in the validation set and test set. We calculate MSE for the 6 parameter rigid motion as well as the translation and rotational components separately. To verify feasibility of traditional intensity-based registration method on FRIs, we use BIS with a multi-resolution hierarchical representation (3 levels) and minimize the sum of squared differences (SSD) similarity metric (Fig.  (ii) DL-HMC with FRI as input (DL-HMC FRI); (iii) proposed network with PCI as input (Proposed PCI); (iv) proposed network with FRI as input (Proposed FRI); and (v) proposed method with FRI but without the data augmentation module (Proposed w/o DA); Results demonstrate that the proposed network with FRI input provides the best motion tracking performance in both validation and testing data. We also observes that using FRI yields a lower loss for the proposed network, indicating that high image quality enhanced the motion correction performance. For testing translation results, Proposed PCI outperforms Proposed FRI and has similar total motion loss, which indicates that the proposed network can still estimate motion on testing subjects even with noisy input. Figure  Qualitative Reconstruction Evaluation. After inference, the 6 rigid degrees of freedom transformation estimated from the network were used to reconstruct the PET images using Motion-compensation OSEM List-mode Algorithm for Resolution-Recovery Reconstruction (MOLAR) algorithm  Based on the tracer distribution of 18 F-PEB, we selected some frames from reconstructed images to illustrate the proposed FRI motion correction performance. In general, the Proposed FRI results in qualitatively enhanced anatomical interpretation of PET images. In Fig.  In addition, brain region of interest (ROI) analyses were also performed for quantitative use. Each subject's MR image was segmented into 74 regions using FreeSurfer software "
Fast Reconstruction for Deep Learning PET Head Motion Correction,4.0,Discussion and Conclusion,"In this work, we propose a new head motion correction approach using fast reconstructions as input. The proposed method outperforms other methods in a multi-subject cohort, and ablation studies demonstrate the effectiveness of our strategies. We apply our proposed FRI motion correction to get motion-free reconstruction using MOLAR. The proposed FRI method achieves good image quality and similar ROI evaluation results compared to Vicra gold-standard HMT. In this study, we showed that conventional intensity-based registration fails at performing motion tracking on FRI data. This is likely due to the PET dynamic changes and non-optimal registration parameters. Compared with previous deep learning motion correction "
Fast Reconstruction for Deep Learning PET Head Motion Correction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 67.
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,1.0,Introduction,"Positron emission tomography (PET) is a widely-used molecular imaging technique that can help reveal the metabolic and biochemical functioning of body tissues. According to the dose level of injected radioactive tracer, PET images can be roughly classified as standard-(SPET) and low-dose PET (LPET) images. SPET images offer better image quality and more information in diagnosis compared to LPET images containing more noise and artifacts. However, the higher radiation exposure associated with SPET scanning poses potential health risks to the patient. Consequently, it is crucial to reconstruct SPET images from corresponding LPET images to produce clinically acceptable PET images. In recent years, deep learning-based PET reconstruction approaches  Fortunately, likelihood-based generative models offer a new approach to address the limitations of GANs. These models learn the distribution's probability density function via maximum likelihood and could potentially cover broader data distributions of generated samples while being more stable to train. As an example, Cui et al.  -We introduce a novel PET reconstruction framework based on DPMs, which, to the best of our knowledge, is the first work that applies DPMs to PET reconstruction. -To mitigate the computational overhead of DPMs, we employ a coarse-to-fine design that enhances the suitability of our framework for real-world clinical applications. -We propose two novel strategies, i.e., an auxiliary guidance strategy and a contrastive diffusion strategy, to improve the correspondence between the LPET and RPET images and ensure that RPET images contain reliable clinical information. 2 Background: Diffusion Probabilistic Models Diffusion Probabilistic Models (DPMs): DPMs  where α 1:T is the constant variance schedule that controls the amount of noise added at each time step, and q(x T ) ∼ N (x T ; 0, I) is the stationary distribution. Owing to the Markov property, a data x t at an arbitrary time step t can be sampled in closed form: where γ t = t i=1 α i . Furthermore, we can derive the posterior distribution of and σ 2 t are subject to x 0 , x t and α 1:T . Based on this, we can leverage the reverse process from x T to x 0 to gradually denoise the latent variables by sampling from the posterior distribution q(x t-1 |x 0 , x t ). However, since x 0 is unknown during inference, we use a transition distribution p θ (x t-1 |x t ) := q(x t-1 |H θ (x t , t), x t ) to approximate q(x t-1 |x 0 , x t ), where H θ (x t , t) manages to reconstruct x 0 from x t and t, and it is trained by optimizing a variational lower bound of logp θ (x). Conditional DPMs: Given an image x 0 with its corresponding condition c, conditional DPMs try to estimate p(x 0 |c). To achieve that, condition c is concatenated with x t  Simplified Training Objective: Instead of training H θ to reconstruct the x 0 directly, we use an alternative parametrization D θ named denoising network  where the distribution p γ is the one used in WaveGrad "
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,3.0,Methodology,Our proposed framework (Fig. 
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,3.1,Coarse-to-Fine Framework,"To simplify notation, we use a single conditioning variable c to represent the input required by both CPM and IRM, which includes the LPET image x lpet and the auxiliary guidance x aux . During inference, CPM first generates a coarse prediction x cp = P θ (c), where P θ is the deterministic prediction network in CPM. The IRM, which is the reverse process of DPM, then tries to sample the residual r 0 (i.e., x 0 in Sect. 2) between the coarse prediction x cp and the SPET image y via the following iterative process: Herein, the prime symbol above the variable indicates that it is sampled from the reverse process instead of the forward process. When t = 1, we can obtain the final sampled residual r 0 , and the RPET image y can be derived by r 0 + x cp . In practice, both CPM and IRM use the same network architecture shown in Fig. "
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,3.2,Auxiliary Guidance Strategy,"In this section, we will describe our auxiliary guidance strategy in depth which is proposed to enhance the reconstruction process at the input level by incorporating two auxiliary guidance, i.e., neighboring axial slices (NAS) and the spectrum. Our findings indicate that incorporating NAS provides insight into the spatial relationship between the current slice and its adjacent slices, while incorporating the spectrum imposes consistency in the frequency domain. To effectively incorporate these two auxiliary guidances, as illustrated in Fig.  To empower the feature extractor to contain information of its high-quality counterpart y aux , we constrain it with L 1 loss through a convolution layer C θ (•): where opt ∈{NAS, spectrum} denotes the kind of auxiliary guidance."
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,3.3,Contrastive Diffusion Strategy,"In addition to the auxiliary guidance at the input level, we also develop a contrastive diffusion strategy at the output level to amplify the correspondence between the condition LPET image and the corresponding RPET image. In detail, we introduce a set of negative samples Neg = y 1 , y 2 , ..., y N , which consists of N SPET slices, each from a randomly selected subject that is not in the current batch for training. Then, for the noisy latent residual r t at time step t, we obtain its corresponding intermediate RPET y, and draw it close to the corresponding SPET y while pushing it far from the negative sample y i ∈ Neg. Before this, we need to estimate the intermediate residual corresponding to r t firstly, denoted as r 0 . According to Sect. 2, the denoising network D θ manages to predict the Gaussian noise added to r 0 , enabling us to calculate r 0 directly from r t : Then r 0 is added to the coarse prediction x cp to obtain the intermediate RPET y = x cp + r 0 . Note that y is a one-step estimated result rather than the final RPET y . Herein, we define a generator p θ (y|r t , c) to represent the above process. Subsequently, the contrastive learning loss L CL is formulated as: Intuitively, as illustrated in Fig. "
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,3.4,Training Loss,"Following  In summary, the final loss function is: where m, n and k are the hyper-parameters controlling the weights of each loss."
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,3.5,Implementation Details,"The proposed method is implemented by the Pytorch framework using an NVIDIA GeForce RTX 3090 GPU with 24GB memory. The IRM in our framework is built upon the architecture of SR3  The number of downsampling operations M is 3, and the negative sample set number N is 10. 4 neighboring slices are used as the NAS guidance and the spectrums are obtained through discrete Fourier transform. As for the weights of each loss, we set m = n = 1, and k = 5e-5 following "
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,4.0,Experiments and Results,"Datasets and Evaluation: We conducted most of our low-dose brain PET image reconstruction experiments on a public brain dataset, which is obtained from the Ultra-low Dose PET Imaging Challenge 2022  Comparison with SOTA Methods: We compare the performance of our method with 6 SOTA methods, including DeepPET  Rows from top to bottom display the reconstructed results, zoom-in details, and error maps. As can be seen, our method generates the lowest error map while the details are well-preserved, consistent with the quantitative results. Ablation Study: To thoroughly evaluate the impact of each component in our method, we perform an ablation study on the public dataset by breaking down our model into several submodels. We begin by training the SR3 model as our baseline (a). Then, we train a single CPM with an L2 loss (b), followed by the incorporation of the IRM to calculate the residual (c), and the addition of the auxiliary NAS guidance (d), the spectrum guidance (e), and the L CL loss term (f). Quantitative results are presented in Table "
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,5.0,Conclusion,"In this paper, we propose a DPM-based PET reconstruction framework to reconstruct high-quality SPET images from LPET images. The coarse-to-fine design of our framework can significantly reduce the computational overhead of DPMs while achieving improved reconstruction results. Additionally, two strategies, i.e., the auxiliary guidance strategy and the contrastive diffusion strategy, are proposed to enhance the correspondence between the input and output, further improving clinical reliability. Extensive experiments on both public and private datasets demonstrate the effectiveness of our method."
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_23.
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,1.0,Introduction,"Magnetic resonance imaging (MRI) is a widely used non-invasive imaging technique. However, MRI is sensitive to subject motion due to the long time for k-space data acquisition  The typical methods for motion artifacts correction in MRI include the prospective and retrospective methods. The prospective methods measure the subject motion using external tracking devices or navigators during the scan for motion correction  In this paper, we propose a dual domain motion correction network (i.e., D 2 MC-Net) to correct the motion artifacts in 2D multi-slice MRI. Instead of explicitly estimating motion parameters, we design a dual domain regularized model with an uncertainty-guided data consistency term, which models the motion corruption by k-space uncertainty to guide the MRI reconstruction. Then the alternating iterative algorithm of the model is unfolded to be a novel deep network, i.e., D 2 MC-Net. As shown in Fig.  Different from the optimization-based methods "
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,2.1,Problem Formulation,"In our approach, we model the motion corruption by measuring the uncertainty of k-space data. Specifically, we assume that the distribution of motion-corrupted k-space data ŷ ∈ C N at each position obeys a non-i.i.d. and pixel-wise Gaussian distribution, where N is the number of the k-space data. Specifically, considering the i-th position of the ŷ, we have where w ∈ [0, 1] N represents the k-space uncertainty with the elements w . p(x) and p(y) are the prior distributions of the motion-free data in image domain and k-space domain. The likelihood distribution log p(ŷ|x, w) ) has been modeled by Eq. (  where H I and H K are learnable denoisers with parameters θ I and θ K , which adopt the U-Net "
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,2.2,Dual Domain Motion Correction Network,"Our proposed D 2 MC-Net is designed based on the alternating optimization algorithm to solve Eq. (  K-space Uncertainty Module. This module is designed to update k-space uncertainty w in Eq. (  where H W is the sub-network with parameters θ W . When t=1, we only send ŷ into the KU-Module because we do not have the estimate of the reconstructed MR images in such case. Dual Domain Reconstruction Module. This module is designed to update k-space data y and MR image x in Eq. (  where W t = diag(w t ) ∈ [0, 1] N ×N is a diagonal matrix, thus the matrix inversion in Eq. (  Equation ( "
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,2.3,Network Details and Training Loss,"In the proposed D 2 MC-Net, we use T = 3 stages for speed and accuracy tradeoff. Each stage has three sub-networks (i.e., H W , H K and H I ) as shown in Fig.  The overall loss function in image space and k-space is defined as: where x t and y t are the reconstructed MR image and k-space data at t-th stage. x gt and y gt are the motion-free MR image and k-space data. SSIM "
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,3.0,Experiments,"Dataset. We evaluate our method on the T2-weighted brain images from the fastMRI dataset  Motion Artifacts Simulation. We simulate in-plane and through-plane motion according to the forward model ŷ = M FT θ x  And we keep 7% of the k-space lines in the center for preventing excessive distortion of the images. The motion vectors are randomly selected from a Gaussian distribution N (0, 10). We follow the motion trajectories (i.e., piecewise constant, piecewise transient and Gaussian) used in the paper  In Table  Effect of Different Loss Functions. We also investigate the effect of the kspace loss by adjusting the values of hyperparameters γ in Eq. "
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,4.0,Conclusion,"In this paper, we proposed a novel dual domain motion correction network (D 2 MC-Net) to correct the motion artifacts in MRI. The D 2 MC-Net consists of KU-Modules and DDR-Modules. KU-Module measures the uncertainty of kspace data corrupted by motion. DDR-Module reconstructs the motion-free MR images in k-space and image domains under the guidance of the uncertainty estimated by KU-Module. Experiments on fastMRI dataset show the superiority of the proposed D 2 MC-Net. In the future work, we will extend the D 2 MC-Net to be a 3D motion correction method for 3D motion artifacts removal."
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,1.0,Introduction,"Image registration is fundamental to many medical image analysis applications, e.g., motion tracking, atlas construction, and disease diagnosis  Recent advances  Gradient surgery (GS) projects conflicting gradients of different losses during the optimization process of the model to mitigate gradient interference. This has proven useful in multi-task learning  -We propose GSMorph, a gradient-surgery-based DLR model. Our method can circumvent tuning the hyperparameter in composite loss function with a gradient-level reformulation to reach the trade-off between registration accuracy and smoothness of the deformation field. -Existing GS approaches have operated the parameters' gradients independently or integrally. We propose a layer-wise GS to group by the parameters for optimization to ensure the flexibility and robustness of the optimization process. -Our method is model-agnostic and can be integrated into any DLR network without extra parameters or losing inference speed."
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,2.0,Methodology,"Deformable image registration estimates the non-linear correspondence field φ between the moving, M , and fixed, F , images (Fig.  where α is the learning rate; L sim is the similarity loss to penalize differences in the appearance of the moving and fixed images (e.g., mean square error, mutual information or local negative cross-correlation); L reg is the regularization loss to encourage the smoothness of the deformation field (this can be computed by the gradient of the deformation field); λ is the hyperparameter balancing the tradeoff between L sim and L reg to achieve desired registration accuracy while preserving the smoothness of the deformation field in the meantime. However, hyperparameter tuning is time-consuming and highly experience-dependent, making it tough to reach the optimal solution. Insight into the optimization procedure in Eq. 1, as registration accuracy and spatial smoothness are potentially controversial in model optimization, the two constraints might have different directions and strengths while going through the gradient descent. Based on this, we provide a geometric view to depict the gradient changes for θ based on the gradient surgery technique. The conflicting relationship between two controversial constraints can be geometrically projected as orthogonal vectors. Depending on the orthogonal relationship, merely updating the gradients of the similarity loss would automatically associate with the updates of the regularization term. In this way, we avoid tuning the hyperparameter λ to optimize θ. The Eq. 1 can then be rewritten into a non-hyperparameter pattern: where Φ(•) is the operation of proposed GS method."
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,2.1,Layer-Wise Gradient Surgery,"Figure  In this study, we propose updating the parameters of neural networks by the original g sim independently, when g sim and g reg are non-conflicting, representing g sim has no incompatible component of the gradient along the direction of g reg . Consequently, optimization with sole g sim within a non-conflicting scenario can inherently facilitate the spatial smoothness of deformations. Conversely, as shown in Fig.  Existing studies  Non-conflicting Conflicting Fig.  each layer separately. Then, the conflicting relationship between the two gradients is calculated based on their inner production. Once the two gradients are non-conflicting, the gradients used to update its corresponding parameter group will be only the original gradients of similarity loss; on the contrary, the gradients will be the projected similarity gradients orthogonal to the gradients of regularization, which can be calculated as g i sim - After performing GS on all layer-wise parameter groups in the network, the final gradients will be used to update the model's parameters. "
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,2.2,Network Architecture,Our network architecture (seen in Fig. 
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,3.1,Datasets and Implementations,"Datasets. In this study, we used two public cardiac cine-MRI datasets for investigation and comparison: ACDC "
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,3.2,Alternative Methods and Evaluation Criteria,"To demonstrate the advantages of our proposed method in medical image registration, we compared it with two conventional deformable registration methods, i.e., Demons  In this study, we used six criteria to evaluate the efficacy and efficiency of the investigated methods, including Dice score (Dice) and 95% Hausdorff distance (HD95) to validate the registration accuracy of the regions of interest, Mean square error (MSE) to evaluate the pixel-level appearance difference between the moved and fixed image-pairs, the percentage of pixels with negative Jacobian determinant (NJD) values to compare the smoothness and diffeomorphism of the deformation field, the number of parameters (Param) of the neural network and inference speed (Speed) to investigate the efficiency."
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,3.3,Results,As summarized in Table  The comparison in the GS-based method shows the flexibility and robustness of our approach. Figure  In Table 
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,4.0,Conclusion,"This work presents a gradient-surgery-based registration framework for medical images. To the best of our knowledge, this is the first study to employ gradient surgery to refine the optimization procedure in learning the deformation fields. In our GSMorph, the gradients from the similarity constraint were projected onto the plane orthogonal to those from the regularization term. In this way, merely updating the gradients in optimizing the registration accuracy would result in a joint updating of the gradients from the similarity and regularity constraints. Then, no additional regularization loss is required in the network optimization and no hyperparameter is further required to explicitly trade off between registration accuracy and spatial smoothness. Our model outperformed the conventional registration methods and the alternative DLR models. Finally, the proposed method is model-agnostic and can be integrated into any DLR network without introducing extra parameters or compromising the inference speed. We believe GSMorph will facilitate the development and deployment of DLR models and alleviate the influence of hyperparameters on performance."
Motion Compensated Unsupervised Deep Learning for 5D MRI,1.0,Introduction,"Magnetic Resonance Imaging (MRI) is currently the gold standard for assessing cardiac function. It provides detailed images of the heart's anatomy and enables accurate measurements of parameters such as ventricular volumes, ejection fraction, and myocardial mass. Current clinical protocols, which rely on serial breathheld imaging of the different cardiac slices with different views, often require long scan times and are associated with reduced patient comfort. Compressed sensing  5D free-breathing MRI approaches that rely on 3D radial readouts  The main focus of this work is to introduce a motion-compensated reconstruction algorithm for 5D MRI. The proposed approach models the images at every time instance as a deformed version of a static image template. Such an image model may not be a good approximation in 2D schemes  We validate the proposed scheme on cardiac MRI datasets acquired from two healthy volunteers. The results show that the approach is capable of resolving the cardiac motion, while offering similar image quality for all the different phases. In particular, the motion-compensated approach can combine the image information from all the motion states to obtain good quality images."
Motion Compensated Unsupervised Deep Learning for 5D MRI,2.1,Acquisition Scheme,"In vivo acquisitions were performed on a 1.5T clinical MRI scanner (MAGNE-TOM Sola, Siemens Healthcare, Erlangen, Germany). The free-running research sequence used in this work is a bSSFP sequence, in which all chemically shiftselective fat saturation pulses and ramp-up RF excitations were removed, in order to reduce the specific absorption rate (SAR) and to enable a completely uninterrupted acquisition "
Motion Compensated Unsupervised Deep Learning for 5D MRI,2.2,Forward Model,"We model the measured k-space data at the time instant t as the multichannel Fourier measurements of ρ t = ρ(r, t), which is the image volume at the time instance t: Here, C denotes the multiplication of the images by the multi-channel coil sensitivities, while F k denotes the multichannel Fourier operator. k t denotes the k-space trajectory at the time instant t. In this work, we group 22 radial spokes corresponding to a temporal resolution of 88 ms. An important challenge associated with the bSSFP acquisition without intermittent fat saturation pulses is the relatively high-fat signal compared to the myocardium and blood pool. Traditional parallel MRI and coil combination strategies often result in significant streaking artifacts from the fat onto the myocardial regions, especially in the undersampled setting considered in this work. We used the coil combination approach introduced in "
Motion Compensated Unsupervised Deep Learning for 5D MRI,2.3,Image and Motion Models,"The overview of the proposed scheme is shown in Fig.  Here, φ t is the deformation map and the operator I denotes the deformation of η. We implement (2) using cubic Bspline interpolation. This approach allows us to use the k-space data from all the time points to update the template, once the motion maps φ t are estimated. Classical MoCo approaches use image registration to estimate the motion maps φ t from approximate (e.g. low-resolution) reconstructions of the images ρ(r, t). However, the quality of motion estimates depends on the quality of the reconstructed images, which are often low when we aim to recover the images at a fine temporal resolution (e.g. 88 ms). We propose to estimate the motion maps directly from the measured kt space data. In particular, we estimate the motion maps φ t such that the multichannel measurements of ρ(r, t) specified by (2) match the measurements b t . We also estimate the template η from the k-t space data of all the time points. To constrain the recovery of the deformation maps, we model the deformation maps as the output of a convolutional neural network in response to low-dimensional latent vectors z t . Here, G θ is a convolutional neural network, parameterized by the weights θ. We note that this approach constrains the deformation maps as points on a low-dimensional manifold. They are obtained as non-linear mappings of the low-dimensional latent vectors z t , which capture the motion attributes. The non-linear mapping itself is modeled by the CNN."
Motion Compensated Unsupervised Deep Learning for 5D MRI,2.4,Estimation of Latent Vectors from SI Navigators,"We propose to estimate the latent vectors z t from the SI navigators using an auto-encoder. In this work, we applied a low pass filter with cut-off frequency of 2.8 Hz to the SI navigators to remove high-frequency oscillations. Similarly, an eighth-degree Chebyshev polynomial is fit to each navigator voxel and is subtracted from the signal to remove drifts. The auto-encoder involves an encoder that generates the latent vectors z t = E ϕ (y t ), are the navigator signals. The decoder reconstructs the navigator signals as y t = D ψ (z t ). In this work, we restrict the dimension of the latent space to three, two corresponding to respiratory motion and one corresponding to cardiac motion. To encourage the disentangling of the latent vectors to respiratory and cardiac signals, we use the prior information on the range of cardiac and respiratory frequencies as in  Here, Z ∈ R 3×T and Y are matrices whose columns are the latent vectors and the navigator signals at different time points. F is the Fourier transformation in the time domain. denotes the convolution of the latent vectors with band-stop filters with appropriate stop bands. In particular, the stopband of the respiratory latent vectors was chosen to be 0.05-0.7 Hz, while the stopband was chosen as the complement of the respiratory bandstop filter Hz for the cardiac latent vectors. We observe that the median-seeking 1 loss in the Fourier domain is able to offer improved performance compared to the standard 2 loss used in conventional auto-encoders."
Motion Compensated Unsupervised Deep Learning for 5D MRI,2.5,Motion Compensated Image Recovery,"Once the auto-encoder parameters ϕ, ψ described in (3) are estimated from the navigator signals of the subject, we derive the latent vectors as Z = E ϕ * (Y). Using the latent vectors, we pose the joint recovery of the static image template η(r) and the deformation maps as The above optimization scheme can be solved using stochastic gradient optimization. Following optimization, one can generate real-time images shown in Fig.  The optimization scheme described in (4) requires T non-uniform fast Fourier transform steps per epoch. When the data is recovered with a high temporal resolution, this approach translates to a high computational complexity. To reduce computational complexity, we introduce a clustering scheme. In particular, we use k-means clustering to group the data to N << T clusters. This approach allows us to pool the k-space data from multiple time points, all with similar latent codes. The above approach is very similar to (4). Here, b n , A n , and z n are the grouped k-space data, the corresponding forward operator, and the centroid of the cluster, respectively. Note that once N → T , both approaches become equivalent. In this work, we used N = 30. Once the training is done, one can still generate the realtime images as I (η, G θ [z t ]). In the first step shown in (a), we estimate the latent variables that capture the motion in the data using a constrained auto-encoder, as described in Fig. "
Motion Compensated Unsupervised Deep Learning for 5D MRI,2.6,Motion Resolved 5D Reconstruction for Comparison,"We compare the proposed approach against a compressed sensing 5D reconstruction. In particular, we used the SI navigators to bin the data into 16 bins, consisting of four cardiac and four respiratory phases as described. We use a total variation regularization similar to  We note that the dataset with 6.15 min acquisition is a highly undersampled setting. In addition, because this dataset was not acquired with intermittent fat saturation pulses, it suffers from streaking artifacts that corrupt the reconstructions."
Motion Compensated Unsupervised Deep Learning for 5D MRI,3.0,Results,We show the results from the two normal volunteers in Fig. 
Motion Compensated Unsupervised Deep Learning for 5D MRI,4.0,Discussion,"The comparisons in Fig.  We note that the comparison in this work is preliminary. The main focus of this work is to introduce the proposed motion-compensated reconstruction algorithm and the auto-encoder approach to estimate the latent vectors and to demonstrate its utility in 5D MRI. In our future work, we will focus on rigorous studies, including comparisons with 2D CINE acquisitions."
An Unsupervised Multispectral Image Registration Network for Skin Diseases,1.0,Introduction,"Skin disease is common in clinic, which is characterized by the complexity of pathological morphology and etiology, as well as the diversity of disease types and locations. Multispectral imaging (MSI) has the characteristics of non-tissue contact puncture, no radiation and no need for exogenous contrast agents. The imaging mechanism characterizes specific, correlated and complementary tissue features, which makes it having a broad, promising and advantageous application prospect in the diagnosis of skin diseases  The difficulties for MSI registration are twofold. Firstly, conventional registration method is to register two images  In the field of image registration, many inspiring methods based on traditional or deep learning techniques have been developed and applied to computer vision tasks in medical imaging, remote sensing, etc. Whereas, the traditional methods  To address the aforementioned issues, we propose an end-to-end multispectral image registration (MSIR) network with unsupervised learning for multiple types of human skin diseases, which improves the capability of CNN architecture to learn the cross-band transformation relationship among pathological features, so as to obtain an efficient and robust RF solution. First, we design a basic adjacentband pair registration (ABPR) model, which simultaneously models a series of image pairs from adjacent bands based on CNN, and makes full use of the feature transformation relationship between images to obtain their corresponding RFs. Second, we introduce a multispectral attention module (MAM), which is used to achieve extraction and adaptive weight allocation for the high-level pathological features. Third, we design a registration field refinement module (RFRM) to obtain a general RF solution of MSI through rectifying and reconstructing the RFs learned from all adjacent-band MSI pairs. Fourth, we propose an unsupervised center-toward registration loss function, combining a similarity loss for features in the frequency domain and a smoothness loss for RF. We perform extensive experiments on a MSI dataset of multi-type skin diseases. The evaluation results demonstrate that our method not only outperforms prior arts on MSI registration task, but also contributes to the subsequent task of benign and malignant disease classification."
An Unsupervised Multispectral Image Registration Network for Skin Diseases,2.0,Methodology,As shown in Fig. 
An Unsupervised Multispectral Image Registration Network for Skin Diseases,2.1,Adjacent-Band Pair Registration (ABPR) Model,"The ABPR model is based on Unet  The input to ABPR model is the concatenation of a pair of MSI images in adjacent bands P i ∈ R H×W ×2 (images I i and I i+1 , 1 ≤ i ≤ n -1. n is the number of bands). The module constructs a function Γ i = F ρ (P i ) to synchronously extracts the mutual transformation relationship. F represents the registration function fit by the designed Unet architecture. ρ refers to the weights and bias parameters of the kernels of the convolutional layers. Γ i ∈ R H×W ×2 is the RF between the given P i , where 2 denotes two channels along the x-axis and y-axis. Since P i in multi-band imaging has unequal contribution to the solution of the final RF, we introduce a multispectral attention module (MAM) into ABPR model, which can not only facilitate the extraction of mutual complementary non-local features between MSI, but also guide the model to pay more attention to features from specific spectra with high impact. Specifically, we carry out global average pooling operation on the high-level features F i of the encoder for image pair P i , and flatten F i into a feature vector V i . Then we obtain a feature map M ∈ R (n-1)×H ×W ×C by concatenating the accumulated n -1 feature vectors in column. The formula for reallocating attention weights is defined as  where W Q , W K and W V are weights of the fully connection layers. d k represents the dimension of V i . Next, we reshape M to obtain the updated high-level features F i and continue the subsequent decoding calculation in ABPR model."
An Unsupervised Multispectral Image Registration Network for Skin Diseases,2.2,Registration Field Refinement Module (RFRM),"In order to make full use of the potential complementarity of corresponding features between cross-band images, we design a RFRM to obtain a general RF of MSI through rectifying and reconstructing a series of RFs learned from multiple adjacent-band image pairs, which is more conducive to further improving the accuracy and generalization of the MSI registration network. First, RFRM concatenates all Γ i ∈ R H×W ×2 learned from n -1 adjacentband image pairs. The obtained RF∈ R (n-1)×H×W ×2 is then refined through three 3D residual blocks, and the reliable Γ ∈ R H×W ×2 is finally generated. T Γ means a coordinate mapping that is parameterized by Γ and its spatial transformation. That is, for each pixel p ∈ I i , there is a Γ such that I i (p) and T Γ (I i )(p) are two corresponding points in adjacent bands."
An Unsupervised Multispectral Image Registration Network for Skin Diseases,2.3,Center-Toward Registration Loss Function,"The ultimate goal of image registration is to obtain a RF from MSI with significant amplitude difference and geometric distortion, so that the perceived images corrected by the RF have the best similarity with each other. The pixel similarity in spatial domain among MSI is prone to large difference due to spectral intensity, while the feature similarity in the frequency domain is more stable. In order to optimize the adaptive center-toward registration of a group of images simultaneously, we propose an unsupervised loss function, including a similarity loss for features in the frequency domain and a smoothness loss for RF. The specific scheme is described in detail as follows: (1) The first is a similarity loss function based on the residual complexity of features in the frequency domain, which is used to penalize differences in appearance and optimize the registration effect under different lighting conditions. The residual complexity loss function is defined as  where m is the number of pixels of the images I and I . DCT denotes discrete cosine transform whose weight is regulated by a hyperparameter α with an empirical value of 0.05. It is worth noting that the similarity loss consists of two components. For the image with band i, we first use the fused Γ and its spatial transformation to obtain the warped image I i = T Γ (I i ). Then we evaluate its similarity to the reference image with the adjacent band I i+1 and the warped image I i+1 = T Γ (I i+1 ), which not only ensures that the transformed images do not deviate from the original spatial distribution, but also realizes center-toward registration of a group of images synchronously and uniformly. Then the total similarity loss can be obtained through adding the residual complexity results from n -1 image pairs. The formula is defined as: (2) The second is an auxiliary loss function that constrains the smoothness of RF and penalizes the local spatial variation. L smooth is constructed based on Γ i through differentiable operation on spatial gradient, and the formula is as follows  where is the gradient operator calculated along the x-axis and y-axis. Then the loss function of our module L total is the weighted sum of L sim and L smooth , which is defined as: where λ is a hyperparameter used to balance the similarity and smoothness of RF, and the empirical value 4 is taken, which is consistent with the Voxelmorph "
An Unsupervised Multispectral Image Registration Network for Skin Diseases,3.1,Dataset and Implementation Details,"We validated the proposed MSIR network on an in-house MSI dataset containing 85 cases with multi-type skin diseases collected from our partner hospital from November 2021 to March 2022, including 36 cases with benign diseases (keloid, fibrosarcoma, cyst, lipoma, hemangioma and nevus) and 49 cases with malignant diseases (eczematoid paget disease, squamous cell carcinoma, malignant melanoma and basal cell carcinoma). Specifically, each case consists of 22 scans with wavelengths ranging from 405 nm to 1650 nm and a uniform size of 640 × 512. For each case, a clinician manually annotated four corresponding landmarks for registration on each scan, and their positions were further reviewed by an experienced medical expert. These landmarks are used to evaluate the registration accuracy. The detailed information for the training set and test set is shown in Table "
An Unsupervised Multispectral Image Registration Network for Skin Diseases,3.2,Quantitative and Qualitative Evaluation,"In this paper, we adopt three evaluation indexes to assess the registration performance of different methods, including normalized mutual information (NMI), registration feature error (RFE) and target registration point error (TRE)  Table  Next, we conduct ablation experiments to explore the registration performance based on images with different subsets of bands, as illustrated in Table  To verify the generalization of the proposed method, we test the model using a synthetic dataset (with |5| degrees of rotation, |0.02| of scaling, |6| and |8| pixels of translation along the x-axis and y-axis). The visualization results are shown in Fig.  In addition, in order to verify the effect of this registration method to subsequent tasks, we further conduct a classification task for benign and malignant diseases based on the established MSI dataset. Table "
An Unsupervised Multispectral Image Registration Network for Skin Diseases,4.0,Conclusion,"In this study, an efficient and robust framework for multispectral image registration is proposed and validated on a self-established dataset of multiple types of skin diseases, which holds great potentials for the further analysis, such as the classification of benign and malignant diseases. We intend to release the MSI dataset in future. The quantitative results of experiments demonstrate the superiority of our method over the current state-of-the-art methods."
An Unsupervised Multispectral Image Registration Network for Skin Diseases,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 68.
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,1.0,Introduction,"Unlike natural images that are typically processed in 8-bit depth, medical images, including X-ray, CT, and MR images, are processed in 12-bit or 16-bit depth to retain more detailed information. Among medical images, CT images are scaled using a quantitative measurement known as the Hounsfield unit (HU), which ranges from -1024 HU to 3071 HU in 12-bit depth. However, in both clinical practice and research, the dynamic range of HU is typically clipped to emphasize the region of interest (ROI). Such clipping of CT images, called windowing, can increase the signal-to-noise ratio (SNR) in the ROI. Therefore, most research on CT images performs windowing as a pre-processing method  Recent advancements in computational resources have enabled the development of 3D deep learning models such as 3D classification and 3D segmentation. 3D models have attracted much attention in the medical domain because they can utilize the 3D integrity of anatomy and pathology. However, the access to 3D medical imaging datasets is severely limited due to the patient privacy. The inaccessibility problem of 3D medical images can be addressed by generating high quality synthetic data. Some researches have shown that data insufficiency or data imbalance can be overcome using a welltrained generative model  The present study proposes a 2D-based 3D-volume generation method. To preserve the 3D integrity and transfer spatial information across adjacent axial slices, prior slices are utilized to generate each adjacent axial slice. We call this method Adjacent Slicebased Conditional Iterative Inpainting, ASCII. Experiments demonstrated that ASCII could generate 3D volumes with intact 3D integrity. Recently, score-based diffusion models have shown promising results in image generation "
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,2.0,Related Works,"Score-based generative models  where f and g are the coefficients of the drift and diffusion terms in the SDE, respectively, and w induces the Wiener process (i.e., Brownian motion). The backward process was defined as the following reverse-SDE: where w is the backward Wiener process. We define each variance σ t as a monotonically increasing function. To solve the reverse-SDE given by above equation we train a score network S θ (x, t) to estimate the score function of the perturbation kernel ∇xlogp t (x t |x 0 ). Therefore, the objective of the score network is to minimize the following loss function: 3 Adjacent Slice-Based Conditional Iterative Inpainting, ASCII"
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,3.1,Generating 12-Bit Whole Range CT Image,"VESDEs experimented on four noise schedules and two GAN models  For quantitative comparison, we randomly generated 1,000 slices by each parameter and measured the CV. As shown Fig.  In the case of GAN, as the convolution operator can be described as a high-pass filter "
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,3.2,Adjacent Slice-Based Conditional Iterative Inpainting,"To generate a 3D volumetric image in a 2D slice-wise manner, a binary mask was used, which was moved along the channel axis. A slice x 0 = {-1024HU } D filled with intensity of air was padded before the first slice x 1 of CT and used as the initial seed. Then, the input of the model was given by x t : x t+K-1 , where t ∈ [0, N s -K + 1] and N s and K are the total slice number of CT and the number of contiguous slices, respectively. In addition, we omit augmentation because the model itself might generate augmented images. After training, the first slice was generated through a diffusion process using the initial seed. The generated first slice was then used as a seed to generate the next slice in an autoregressive manner. Subsequently, the next slices were generated through the same process. We call this method adjacent slice-based conditional iterative inpainting, ASCII. Two experiments were conducted. The first experiment preprocessed the CT slices by clipping it with brain windowing [-10HU , 70HU ] and normalizing it to [-1, 1] with σ min set to 0.01 and σ max set to 1,348, while the second experiment preprocessed the whole windowing [-1024HU , 3071HU ] and normalized it to [-1, 1] with σ min set to 0.001 and σ max set to 1,348. As shown Fig. "
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,3.3,Intensity Calibration Network (IC-Net),"It was noted that the intensity mismatch problem only occurs in whole range generation. To address this issue, we first tried a conventional non-trainable post-processing, such as histogram matching. However, since each slice has different anatomical structure, the histogram of each slice image was fitted to their subtle anatomical variation. Therefore, anatomical regions were collapsed when the intensities of each slice of 3D CT were calibrated using histogram matching. Finally, we propose a solution for this intensity mismatching, a trainable intensity calibration network: IC-Net. To calibrate the intensity mismatch, we trained the network with a self-supervised manner. First, adjacent two slices from real CT images, x t , x t+1 were clipped using the window of which every brain anatomy HU value can be contained. Second, the intensity of x t+1 in ROI is randomly changed and the result is x t+1 c = x t+1x t+1 * µ + x t+1 , where x t+1 and μ are the mean of x t+1 and shifting coefficient, respectively. And µ was configured to prevent the collapse of anatomical structures. Finally, intensity calibration network, IC-Net was trained to calibrate the intensity of x t+1 to the intensity of x t . The objective of IC-Net was only to calibrate the intensity of x t and preserve both the subtle texture and the shape of a generated slice. The IC-Net uses the prior slice to calibrate the intensity of generated slice. The objective function of IC-Net is given by, As shown in Fig. "
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,4.1,ASCII with IC-Net in 12-Bit Whole Range CT Generation,"The description of the dataset and model architecture is available in the Supplementary Material. We experimented ASCII on continuous K slices of K = 2 and 3 and called them ASCII(2) and ASCII(3), respectively. We generated a head & neck CT images via ASCII(2) and ASCII(3) with and without IC-Net, and slice-to-3D VAE  The quantitative results in whole range are shown in Table "
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,4.2,Calibration Robustness of IC-Net on Fixed Value Image Shift,"To demonstrate the performance of IC-Net, we conducted experiments with the 7, 14, 21 and 28th slices, which sufficiently contain complex structures to show the calibration performances. The previous slice was used as an input to the IC-Net along with the target slice whose pixel values were to be shifted. And the absolute errors were measured between GT and predicted slice using IC-Net. As shown in Fig. "
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,4.3,Visual Scoring Results of ASCII with IC-Net,"Due to the limitations of slice-based methods in maintaining connectivity and 3D integrity, an experienced radiologist with more than 15 years of experience evaluated the images. Seeded with the 13th slices of real CT scans, in which the ventricle appears, ASCII(2) with IC-Net generated a total of 15 slices. Visual scoring shown in Table . 2 on a three-point scale was conducted blindly for 50 real and 50 fake CT scans, focusing on the continuity of eight anatomical structures. Although most of the fake regions were scored similarly to the real ones, the basilar arteries were evaluated with broken continuity. The basilar artery was frequently not generated, because it is a small region. As the model was trained on 5-mm thickness non-contrasted enhanced CT scans, preserving the continuity of the basilar artery is excessively demanding. "
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,5.0,Conclusion,"We proposed a high-performance slice-based 3D generation method (ASCII) and combined it with IC-Net, which is trained in a self-supervised manner without any annotations. In our method, ASCII generates a 3D volume by iterative generation using previous slices and automatically calibrates the intensity mismatch between the previous and next slices using IC-Net. This pipeline is designed to generate high-quality medical image, while preserving 3D integrity and overcoming intensity mismatch caused in 12-bit generation. ASCII had shown promising results in 12-bit depth whole range and windowing range, which are crucial in medical contexts. The integrity of the generated images was also confirmed in qualitative and quantitative assessment of 3D integrity evaluations by an expert radiologist. Therefore, ASCII can be used in clinical practice, such as anomaly detection in normal images generated from a seed image "
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,1.0,Introduction,"Positron emission tomography (PET) is a general nuclear imaging technique, which has been widely used to characterize tissue metabolism, protein deposition, etc.  The pseudo CT images should satisfy two-fold requests. Firstly, the pseudo CT images should be visually similar in anatomical structures to corresponding actual CT images. Secondly, PET images corrected by pseudo CT images should be consistent with that corrected by actual CT images. However, current techniques of image generation tend to produce statistical average values and patterns, which easily erase significant tissues (e.g., bones and lungs). As a result, for those tissues with relatively similar metabolism but large variances in attenuation coefficient, these methods could cause large errors as they are blind to the correct tissue distributions. Therefore, special techniques should be investigated to guarantee the fidelity of anatomical structures in these generated pseudo CT images. In this paper, we propose a deep learning framework, named anatomical skeleton enhanced generation (ASEG), to generate pseudo CT from NAC-PET for attenuation correction. ASEG focuses more on the fidelity of tissue distribution, i.e., anatomical skeleton, in pseudo CT images. As shown in Fig. "
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,2.0,Method,"We propose the anatomical skeleton enhanced generation (ASEG, as illustrated in Fig.  Network Architecture. As illustrated in Fig.  Model Formulation. Let X nac and X ac denote the NAC-PET and AC-PET images, and Y be the actual CT image used for AC. Since CT image is highly crucial in conventional AC algorithms, they generally have a relationship as under an AC algorithm F. To avoid scanning an additional CT image, we attempt to predict Y from X nac as an alternative in AC algorithm. Namely, a mapping G is required to build the relationship between Y and X nac , i.e., Ŷ = G(X nac ). Then, X ac can be acquired by This results in a pioneering AC algorithm that requires only a commonly reusable mapping function G for all PET images rather than a corresponding CT image Y for each PET image. As verified in some previous studies  Herein, G 1 devotes to delineating anatomical skeleton Y as from X nac , thus providing a prior tissue distribution to G 2 while G 2 devotes to rendering the tissue details from X nac and Ŷas = G 1 (X nac ). To avoid annotating the ground truth, Y as can be derived from the actual CT image by a segmentation algorithm (denoted as S : Y as = S(Y )). As different tissues have obvious differences in intensity ranges, we define S as a simple thresholding-based algorithm. Herein, we first smooth each non-normalized CT image with a small recursive Gaussian filter to suppress the impulse noise, and then threshold this CT image to four binary masks according to the Hounsfield scale of tissue density  General Constraints. As mentioned above, two generative modules {G 1 , G 2 } work for two tasks, namely the skeleton prediction and tissue rendering, respectively. Thus, they are trained with different target-oriented constraints. In the training scheme, the loss function for G 1 is the combination of Dice loss L dice and cross-entropy loss L ce  Meanwhile, the loss function for G 2 combines the mean absolute error (MAE) L mae , perceptual feature matching loss L fm  where the anatomical consistency loss L st is explained below. Anatomical consistency. It is generally known that CT images can provide anatomical observation because different tissues have a distinctive appearance in Hounsfield scale (linear related to attenuation coefficients). Therefore, it is crucial to ensure the consistency of tissue distribution in the pseudo CT images, tracking which we propose to use the tissue distribution consistency to guide the network learning. Based on the segmentation algorithm S, both the actual and generated CTs {Y, Ŷ } can be segmented to anatomical structure/tissue distribution masks {S(Y ), S( Ŷ )}, and their consistency can then be measured by Dice coefficient. Accordingly, the anatomical-consistency loss L ac is a Dice loss as During the inference phase, only the NAC-PET image of each input subject is required, where the pseudo CT image is derived by Ŷ ≈ G 2 (G 1 (X nac ), X nac )."
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,3.1,Materials,"The data used in our experiments are collected from The Cancer Image Archive (TCIA)  Each sample contains co-registered (acquired with PET-CT scans) CT, PET, and NAC-PET whole-body scans. In our experiments, we re-sampled all of them to a voxel spacing of 2×2×2 and re-scaled the intensities of NAC-PET/AC-PET images to a range of [0, 1], of CT images by multiplying 0.001. The input and output of our ASEG framework are cropped patches with the size of 192 × 192 × 128 voxels. To achieve full-FoV output, the consecutive outputs of each sample are composed into a single volume where the overlapped regions are averaged."
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,3.2,Comparison with Other Methods,"We compared our ASEG with three state-of-the-art methods, including (i ) a U-Net based method  Quantitative Analysis of CT. As the most import application of CT that is to display the anatomical information, we propose to measure the anatomical consistency between the pseudo CT images and actual CT images, where the Dice coefficients on multiple anatomical regions that extracted from the pseudo/actual CT images are calculated. To avoid excessive self-referencing in evaluating anatomical consistency, instead of employing the simple thresholding segmentation (i.e., S), we resort to the open-access TotalSegmentator  Effectiveness in Attenuation Correction. As the pseudo CT images generated from NAC-PET are expected to be used in AC, it is necessary to further evaluate the effectiveness of pseudo CT images in PET AC. Because we cannot access the original scatters  It can be observed from Table "
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,4.0,Conclusion,"In this paper, we proposed the anatomical skeleton-enhance generation (ASEG) to generate pseudo CT images for PET attenuation correction (AC), with the goal of avoiding acquiring extra CT or MR images. ASEG divided the CT generation into the skeleton prediction and tissue rendering, two sequential tasks, addressed by two designed generative modules. The first module delineates the anatomical skeleton to explicitly enhance the tissue distribution which are vital for AC, while the second module renders the tissue details based on the anatomical skeleton and NAC-PET. Under the collaboration of two modules and specific anatomical-consistency constraint, our ASEG can generate more reasonable pseudo CT from NAC-PET. Experiments on a collection of public datasets demonstrate that our ASEG outperforms existing methods by achieving advanced performance in anatomical consistency. Our study support that ASEG could be a promising and lower-cost alternative of CT acquirement for AC. Our future work will extend our study to multiple PET tracers."
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_3.
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,1.0,Introduction,"Cone-beam computed tomography (CBCT) is a common 3D imaging technique used to examine the internal structure of an object with high spatial resolution and fast scanning speed  To maintain image quality, CBCT typically requires hundreds of projections involving high radiation doses from X-rays, which could be a concern in clinical practice. Sparse-view reconstruction is one of the ways to reduce radiation dose by reducing the number of scanning views (10× fewer). In this paper, we study a more challenging problem, extremely sparse-view CBCT reconstruction, aiming to reconstruct a high-quality CT volume from fewer than 10 projection views. Compared to conventional CT (e.g., parallel beam, fan beam), CBCT reconstructs a 3D volume from 2D projections instead of a 2D slice from 1D projections, as comparison shown in Fig.  In this work, our goal is to reconstruct a CBCT of high image quality and high spatial resolution from extremely sparse (≤10) 2D projections, which is an important yet challenging and unstudied problem in sparse-view CBCT reconstruction. Unlike previous voxel-based methods that represent the CT as discrete voxels, we formulate the CT volume as a continuous intensity field, which can be regarded as a continuous function g(•) of 3D spatial points. The property of a point p in this field represents its intensity value v, i.e., v = g(p). Therefore, the reconstruction problem can be reformulated as regressing the intensity value of an arbitrary 3D point from a stack of 2D projections I, i.e., v = g(I, p). Based on the above formulation, we develop a novel reconstruction framework, namely DIF-Net (Deep Intensity Field Network). Specifically, DIF-Net first extracts feature maps from K given 2D projections. Given a 3D point, we project the point onto the 2D imaging panel of each view i by corresponding imaging parameters (distance, angle, etc.) and query its view-specific features from the feature map of view i . Then, K view-specific features from different views are aggregated by a cross-view fusion module for intensity regression. By introducing the continuous intensity field, it becomes possible to train DIF-Net with a set of sparsely sampled points to reduce memory requirement, and reconstruct the CT volume with any desired resolution during testing. Compared with NeRF-based methods  To summarize, the main contributions of this work include 1.) we are the first to introduce the continuous intensity field for supervised CBCT reconstruction; 2.) we propose a novel reconstruction framework DIF-Net that reconstructs CBCT with high image quality (PSNR: 29.3 dB, SSIM: 0.92) and high spatial resolution (≥256 3 ) from extremely sparse (≤10) views within 1.6 s; 3.) we conduct extensive experiments to validate the effectiveness of the proposed sparse-view CBCT reconstruction method on a clinical knee CBCT dataset."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,2.1,Intensity Field,"We formulate the CT volume as a continuous intensity field, where the property of a 3D point p ∈ R 3 in this field represents its intensity value v ∈ R. The intensity field can be defined as a continuous function g : R 3 → R, such that v = g(p). Hence, the reconstruction problem can be reformulated as regressing the intensity value of an arbitrary point p in the 3D space from K projections I = {I 1 , I 2 , . . . , I K }, i.e., v = g(I, p). Based on the above formulation, we propose a novel reconstruction framework, namely DIF-Net, to perform efficient sparseview CBCT reconstruction, as the overview shown in Fig. "
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,2.2,DIF-Net: Deep Intensity Field Network,"DIF-Net first extracts feature maps {F 1 , F 2 , . . . , F K } ⊂ R C×H×W from projections I using a shared 2D encoder, where C is the number of feature channels and H/W are height/width. In practice, we choose U-Net  View-Specific Feature Querying. Considering a point p ∈ R 3 in the 3D space, for a projection view i with scanning angle α i and other imaging parameters β (distance, spacing, etc.), we project p to the 2D imaging panel of view i and obtain its 2D projection coordinates p i = ϕ(p, α i , β) ∈ R 2 , where ϕ(•) is the projection function. Projection coordinates p i are used for querying view-specific features f i ∈ R C from the 2D feature map F i of view i : where π(•) is bilinar interpolation. Similar to perspective projection, the CBCT projection function ϕ(•) can be formulated as where R(α i ) ∈ R 4×4 is a rotation matrix that transforms point p from the world coordinate system to the scanner coordinate system of view i , A(β) ∈ R 3×4 is a projection matrix that projects the point onto the 2D imaging panel of view i , and H : R 3 → R 2 is the homogeneous division that maps the homogeneous coordinates of p i to its Cartesian coordinates. Due to page limitations, the detailed formulation of ϕ(•) is given in the supplementary material. Cross-View Feature Fusion and Intensity Regression. Given K projection views, K view-specific features of the point p are queried from different views to form a feature list Then, the cross-view feature fusion δ(•) is introduced to gather features from F (p) and generate a 1D vector f = δ(F (p)) ∈ R C to represent the semantic features of p. In general, F (p) is an unordered feature set, which means that δ(•) should be a set function and can be implemented with a pooling layer (e.g., max/avg pooling). In our experiments, the projection angles of the training and test samples are the same, uniformly sampled from 0 • to 180 • (half rotation). Therefore, F (p) can be regarded as an ordered list (K ×C tensor), and δ(•) can be implemented by a 2-layer MLP (K → K 2 → 1) for feature aggregation. We will compare different implementations of δ(•) in the ablation study. Finally, a 4-layer MLP (C → 2C → C 2 → C 8 → 1) is applied to f for the regression of intensity value v ∈ R."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,2.3,Network Training,"Assume that the shape and spacing of the original CT volume are H × W × D and (s h , s w , s d ) mm, respectively. During training, different from previous voxelbased methods that regard the entire 3D CT image as the supervision target, we randomly sample a set of N points {p 1 , p 2 , . . . , p N } with coordinates ranging from (0, 0, 0) to (s h H, s w W, s d D) in the world coordinate system (unit: mm) as the input. Then DIF-Net will estimate their intensity values V = {v 1 , v 2 , . . . , v N } from given projections I. For supervision, ground-truth intensity values V = {v 1 , v2 , . . . , vN } can be obtained from the ground-truth CT image based on the coordinates of points by trilinear interpolation. We choose mean-square-error (MSE) as the objective function, and the training loss can be formulated as Because background points (62%, e.g., air) occupy more space than foreground points (38%, e.g., bones, organs), uniform sampling will bring imbalanced prediction of intensities. We set an intensity threshold 10 -5 to identify foreground and background areas by binary classification and sample N 2 points from each area for training."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,2.4,Volume Reconstruction,"During inference, a regular and dense point set to cover all CT voxels is sampled, i.e., to uniformly sample H × W × D points from (0, 0, 0) to (s h H, s w W, s d D). Then the network will take 2D projections and points as the input and generate intensity values of sampled points to form the target CT volume. Unlike previous voxel-based methods that are limited to generating fixed-resolution CT volumes, our method enables scalable output resolutions by introducing the representation of continuous intensity field. For example, we can uniformly sample H s × W s × D s points to generate a coarse CT image but with a faster reconstruction speed, or sample sH × sW × sD points to generate a CT image with higher resolution, where s > 1 is the scaling ratio."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,3.0,Experiments,"We conduct extensive experiments on a collected knee CBCT dataset to show the effectiveness of our proposed method on sparse-view CBCT reconstruction. Compared to previous works, our DIF-Net can reconstruct a CT volume with high image quality and high spatial resolution from extremely sparse (≤ 10) projections at an ultrafast speed."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,3.1,Experimental Settings,"Dataset and Preprocessing. We collect a knee CBCT dataset consisting of 614 CT scans. Of these, 464 are used for training, 50 for validation, and 100 for testing. We resample, interpolate, and crop (or pad) CT scans to have isotropic voxel spacing of (0.8, 0.8, 0.8) mm and shape of 256 × 256 × 256. 2D projections are generated by digitally reconstructed radiographs (DRRs) at a resolution of 256 × 256. Projection angles are uniformly selected in the range of 180 • . Implementation. We implement DIF-Net using PyTorch with a single NVIDIA RTX 3090 GPU. The network parameters are optimized using stochastic gradient descent (SGD) with a momentum of 0.98 and an initial learning rate of 0.01. The learning rate is decreased by a factor of 0.001 1/400 ≈ 0.9829 per epoch, and we train the model for 400 epochs with a batch size of 4. For each CT scan, N = 10, 000 points are sampled as the input during one training iteration. For the full model, we employ U-Net  Baseline Methods. We compare four publicly available methods as our baselines, including traditional methods FDK  Evaluation Metrics. We follow previous works "
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,3.2,Results,"Performance. As shown in Table  In addition, DIF-Net, which benefits from the intensity field representation, has fewer training parameters and requires less computational memory, enabling high-resolution reconstruction. Ablation Study. Tables "
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,4.0,Conclusion,"In this work, we formulate the CT volume as a continuous intensity field and present a novel DIF-Net for ultrafast CBCT reconstruction from extremely sparse (≤10) projection views. DIF-Net aims to estimate the intensity value of an arbitrary point in 3D space from input projections, which means 3D CNNs are not required for feature decoding, thereby reducing memory requirement and computational cost. Experiments show that DIF-Net can perform efficient and high-quality CT reconstruction, significantly outperforming previous stateof-the-art methods. More importantly, DIF-Net is a general sparse-view reconstruction framework, which can be trained on a large-scale dataset containing various body parts with different projection views and imaging parameters to achieve better generalization ability. This will be left as our future work."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 2.
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,1.0,Introduction,"Magnetic Resonance Imaging (MRI) is one of the most widely used medical imaging modalities, as it is non-invasive and capable of providing superior soft tissue contrast without causing ionizing radiation. However, it is challenging to acquire high-resolution MR images in practical applications  Super-resolution (SR) techniques are a promising way to improve the quality of MR images without upgrading hardware facilities. Clinically, multi-contrast MR images, e.g., T1, T2 and PD weighted images are obtained from different pulse sequences  In recent years, several methods have been explored for arbitrary scale super-resolution tasks on natural images, such as Meta-SR  To address these issues, we propose an arbitrary-scale multi-contrast MRI SR framework. Specifically, we introduce the implicit neural representation to multi-contrast MRI SR and extend the concept of arbitrary scale SR to the reference image domain. Our contributions are summarized as follows: Our Dual-ArbNet outperforms several state-of-the-art approaches on two benchmark datasets: fastMRI "
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,2.1,Background: Implicit Neural Representations,"As we know, computers use 2D pixel arrays to store and display images discretely. In contrast to the traditional discrete representation, the Implicit Neural Representation (INR) can represent an image I ∈ R H×W in the latent space F ∈ R H×W ×C , and use a local neural network (e.g., convolution with kernel 1) to continuously represent the pixel value at each location. This local neural network fits the implicit function of the continuous image, called Implicit Decoding Function (IDF). In addition, each latent feature represents a local piece of continuous image "
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,2.2,Network Architecture,"The overall architecture of the proposed Dual-ArbNet is shown in Fig.  Encoder. In the image encoder, Residual Dense Network (RDN)  Since the resolution of target LR and reference image are different, we have to align them to target HR scale for further fusion. With the target image shaped H tar × W tar and reference image shaped  where z ∈ {ref, tar} indicates the reference and target image, I tar and I ref are the input target LR and reference image. In this way, we obtain the latent feature nearest to each HR pixel for further decoding, and our method can handle Arbitrary scale SR for target images with Arbitrary resolution of reference images (Dual-Arb). Decoder. As described in Sect. 2.1, the INR use a local neural network to fit the continuous image representation, and the fitting can be referred to as Implicit Decoding Function (IDF). In addition, we propose a fusion branch to efficiently fuse the target and reference latent features for IDF decoding. The overall decoder includes a fusion branch and a shared IDF, as shown in Fig.  Inspired by  where L i indicates the i-th fusion layer. Then, we equally divide the fused feature F The IDF in our method is stacked by convolution layer with kernel size 1 (conv 1 ) and sin activation function sin(•). The conv 1 and sin(•) are used to transform these inputs to higher dimension space  where (x, y) is the coordinate of each pixel, and z ∈ {ref, tar} indicates the reference and target image. denotes element-wise multiplication, and cat is the concatenate operation. W (i) and b (i) are weight and bias of i-th convolution layer. Moreover, we use the last layer's output f (5) (•) as the overall decoding function f (•). By introducing the IDF above, the pixel value at any coordinates I z,SR (x, y) can be reconstructed: where Skip(•) is skip connection branch with conv 1 and sin(•), z ∈ {ref, tar}. Loss Function. An L1 loss between target SR results I target,SR and HR images I HR is utilized as reconstruction loss to improve the overall detail of SR images, named as L rec . The reconstructed SR images may lose some frequency information in the original HR images. K-Loss  To this end, the full objective of the Dual-ArbNet is defined as: We set λ K = 0.05 empirically to balance the two losses."
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,2.3,Curriculum Learning Strategy,"Curriculum learning  In the warm-up stage, we fix the integer SR scale to integer (2×, 3× and 4×) and use HR-Ref to stable the training process. Then, in the pre-learning stage, we use arbitrary scale target images and HR reference images to quickly improve the network's migration ability by learning texture-rich HR images. Finally, in the full-training stage, we train the model with a random scale for reference and target images, which further improves the generalization ability of the network."
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,3.0,Experiments,"Datasets. Two public datasets are utilized to evaluate the proposed Dual-ArbNet network, including fastMRI  We compared our Dual-ArbNet with several recent state-of-the-art methods, including two multi-contrast SR methods: McMRSR  Experimental Setup. Our proposed Dual-ArbNet is implemented in PyTorch with NVIDIA GeForce RTX 2080 Ti. The Adam optimizer is adopted for model training, and the learning rate is initialized to 10 -4 at the full-training stage for all the layers and decreases by half for every 40 epochs. We randomly extract 6 LR patches with the size of 32×32 as a batch input. Following the setting in  Quantitative Results. Table  Qualitative Evaluation. Figure "
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,4.0,Conclusion,"In this paper, we proposed the Dual-ArbNet for MRI SR using implicit neural representations, which provided a new paradigm for multi-contrast MRI SR tasks. It can perform arbitrary scale SR on LR images at any resolution of reference images. In addition, we designed a new training strategy with reference to the idea of curriculum learning to further improve the performance of our model. Extensive experiments on multiple datasets show that our Dual-ArbNet achieves state-of-the-art results both within and outside the training distribution. We hope our work can provide a potential guide for further studies of arbitrary scale multi-contrast MRI SR."
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_27.
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,1.0,Introduction,"Cortical thickness (CTh) is a crucial biomarker of various neurological and psychiatric disorders, making it a primary focus in neuroimaging research. The cortex, a thin ribbon of grey matter at the outer surface of the cerebrum, plays a vital role in cognitive, sensory, and motor functions, and its thickness has been linked to a wide range of neurological and psychiatric conditions, including Alzheimer's disease, multiple sclerosis, schizophrenia, and depression, among others. Structural magnetic resonance imaging (MRI) is the primary modality used to investigate CTh, and numerous computational methods have been developed to estimate this thickness on the sub-millimeter scale. Among these, surface-based methods like Freesurfer  The long running time of methods for determining CTh remains a barrier to application in clinical routine: a running time of one hour, while a substantial improvement over Freesurfer and ANTs cortical thickness, is still far beyond the real-time processing desirable for on-demand cortical morphometry in clinical applications. In terms of both the speed and performance, VoxelMorph and related models are known to outperform classical deformable registration methods, suggesting that a DiReCT-style CTh algorithm based on unsupervised registration models may enable faster CTh estimation.  Fig. "
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,2.1,DiReCT Cortical Thickness Estimation,"The estimation of CTh using the DiReCT method  The absence of a reliable gold-standard ground truth for CTh makes comparisons between methods difficult. This situation has recently been improved by the publication of a synthetic cortical atrophy phantom: a dataset generated using a GAN conditioned on subvoxel segmentations, consisting of 20 synthetic subjects with 19 induced sub-voxel atrophy levels per subject (ten evenly spaced atrophy levels from 0 to 0.1 mm, and a further nine evenly spaced atrophy levels from 0.1 mm to 1 mm). "
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,2.2,CortexMorph: VoxelMorph for DiReCT,"The original VoxelMorph architecture, introduced in  The setup of our VoxelMorph architecture, CortexMorph, is detailed in Fig. "
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,2.3,Data and WM/GM Segmentation,Training data and validation for our VoxelMorph model was derived from two publicly available sources: images from 200 randomly selected elderly individuals from the ADNI dataset 
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,2.4,Training and Model Selection,"Our network was implemented and trained in Pytorch (1.13.1). We utilized a standard Unet (derived from the nnUnet framework  The training regime was fully unsupervised with respect to cortical thickness: neither the deformation fields yielded by ANTs-DiReCT nor the CTh results computed from those deformation fields were used in the objective function. Since we are interested in replacing the iterative implementation of DiReCT with a deep learning counterpart, we used the 80 validation examples for model selection, selecting the model which showed best agreement in mean global CTh with the results of ANTs-DiReCT. The metric for agreement chosen is intraclass correlation coefficient, specifically ICC(2,1) (the proportion of variation explained by the individual in a random effects model, assuming equal means of the two CTh measurement techniques), since this method is sensitive to both absolute agreement and relative consistency of the measured quantity. ICC was calculated using the python package Pingouin "
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,2.5,Testing,"The VoxelMorph model which agreed best with ANTs-DiReCT on the validation set was applied to segmentations of the OASIS-3 dataset, to confirm whether model selection on a small set of validation data would induce good agreement with ANTs-DiReCT on a much larger test set (metric, ICC(2,1)) and to the synthetic CTh phantom of Rusak et al., to determine whether the VoxelMorph model is able to distinguish subvoxel changes in CTh (metric, coefficient of determination (R 2 ))."
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,3.0,Results,"The best performing model on the validation set (in terms of agreement with DiReCT) was the model trained with MSE loss and a λ of 0.02. When used to measure mean global CTh, this model scored an ICC(2,1) of 0.91 (95% confidence interval [0.9, 0.92]) versus the mean global CTh yielded by ANTs-DiReCT on the OASIS-3 dataset. For comparison, on the same dataset the ICC between Freesurfer and the ANTs-DiReCT method was 0.50 ([95% confidence interval -0.08, 0.8]). A breakdown of the ICC by cortical subregion can be seen in Fig.  Calculating regional CTh took between 2.5 s and 6.4 s per subject (mean, 4.3 s, standard deviation 0.71 s) (Nvidia A6000 GP, Intel Xeon(R) W-11955M CPU)."
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,4.0,Conclusion,"Our experiments suggest that the classical, iterative approach to cortical thickness estimation by diffeomorphic registration can be replaced with a VoxelMorph network, with ∼ 800 fold reduction in the time needed to calculate CTh from a partial volume segmentation of the cortical grey and white matter. Since such segmentations can also be obtained in a small number of seconds using a CNN or other deep neural network, we have demonstrated for the first time reliable CTh estimation running on a timeframe of seconds. This level of acceleration offers increased feasibility to evaluate CTh in the clinical setting. It would also enable the application of ensemble methods to provide multiple thickness measures for an individual: given an ensemble of, say, 15 segmentation methods, a plausible distribution of CTh values could be reported for each cortical subregion within one minute: this would allow better determination of the presence of cortical atrophy in an individual than is provided by point estimates. We are currently investigating the prospect of leveraging the velocity field to enable fast calculation of other morphometric labels such as grey-white matter contrast and cortical curvature: these too could be calculated with error bars via ensembling. This work allows the fast calculation of diffeomorphisms for DiReCT on the GPU. We did not consider the possibility of directly implementing/accelerating the classical DiReCT algorithm on a GPU in this work. Elements of the ANTs-DiReCT pipeline implement multithreading, yielding for example a 20 min runtime with 4 threads: however, since some parts of the pipeline cannot be parallelized it is unlikely that iterative methods can approach the speed of direct regression by CNN. Given the lack of a gold standard ground truth for CTh, it is necessary when studying a new definition of CTh to compare to an existing silver standard method: this would typically be Freesurfer, but recent results suggest that this may not be the optimal method when studying small differences in CTh. "
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,1.0,Introduction,"Tissue elasticity holds enormous diagnostic value for detecting pathological conditions such as liver fibrosis  Physics-informed neural networks (PINNs) are a recent deep learning framework that uses neural networks to solve PDEs  In this work, we develop a method for enhanced tissue elasticity reconstruction in MR elastography using physics-informed learning. We use PINNs to solve the equations of linear elasticity as an optimization problem for a given wave image. Our model simultaneously learns continuous representations of the measured displacement field and the latent elasticity field. We evaluate the method on a numerical simulation and on patient liver MRE data, where we demonstrate improved noise robustness and overall accuracy than AHI or FEM-based inversion. In addition, we show that augmenting our method with an anatomicallyinformed loss function further improves reconstruction quality."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,2.0,Background,Magnetic Resonance Elastography (MRE). An MRE procedure involves placing the patient in an MRI scanner and using a mechanical actuator to induce shear waves in the region of interest (Fig. 
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,,Assumptions Equation,"signal based on the tissue displacement  Linear Elasticity. Physical models of MRE typically assume there is harmonic motion and a linear, isotropic stress-strain relation. Then tissue displacement is a complex vector field u : Ω → C 3 defined on spatial domain Ω ⊂ R 3 , and shear elasticity is characterized by the shear modulus, a complex scalar field μ : The first equation of motion translates into the general form PDE shown in Table "
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,3.0,Proposed Method,"We use physics-informed neural networks (PINNs) to encode the solution space of the inverse problem. Our PINN framework (Fig.  Physics-Informed Neural Networks. We use a dual-network approach to reconstruct tissue elasticity with PINNs. First, we train a neural network û(x; θ u ) to learn a mapping from spatial coordinates x to displacement vectors u in the wave image by minimizing the mean squared error L wave . The continuous representation of the displacement field enables automatic spatial differentiation. Then, we train a second neural network μ(x; θ µ ) to map from spatial coordinates to the shear modulus μ by minimizing the residual of a PDE, defined by some differential operator D. The PINNs and their spatial derivatives are used to evaluate the differential operator, which is minimized as a loss function L P DE to recover the elasticity field. We combine the loss functions as follows: We train PINNs using either the Helmholtz equation (PINN-HH) or the heterogeneous PDE (PINN-het) as the differential operator D in our experiments. The loss weight hyperparameters λ wave and λ P DE control the contribution of each loss function to the overall objective. We initialized λ P DE to a very low value and slowly stepped it up as the quality of û improved during training."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,,Incorporating Anatomical Information.,"Prior work has demonstrated that tissue elasticity can be accurately predicted from anatomical MRI  We designed our models based on the SIREN architecture, which uses sine activation functions to better represent high spatial frequencies "
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,4.0,Related Work,Algebraic Helmholtz Inversion (AHI). One of the most common methods for elasticity reconstruction is algebraic inversion of the Helmholtz equation (AHI) 
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,5.0,Experiments and Results,"We compare our methods (PINN-HH, PINN-het) to algebraic Helmholtz inversion (AHI) and direct FEM inversion (FEM-HH, FEM-het) on simulated data and liver data from a cohort of patients with NAFLD. We evaluate the overall reconstruction fidelity of each method and the impact of the homogeneity assumption. We assess their robustness to noise on the simulated data, and we study whether incorporating anatomical images enhances performance on patient liver data."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,5.1,Robustness to Noise on Simulated Data,We obtained a numerical FEM simulation of an elastic wave in an incompressible rectangular domain containing four stiff targets of decreasing size from the BIOQIC research group (described in  Experiment Results. Figure  Figure  Figure 
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,5.2,Incorporating Anatomical Information on Patient Data,"For our next experiment, we obtained abdominal MRE from a study at the University of Pittsburgh Medical Center that included patients at least 18 years old who were diagnosed with non-alcoholic fatty liver disease (NAFLD) and underwent MRE between January 2016-2019 (demographic and image acquisition details can be found in Pollack et al. 2021  We performed elasticity reconstruction on each of the 155 patient wave images using each method. We investigated the influence of anatomical information when training PINNs by varying the anatomical loss weight λ anat . Reconstruction fidelity was assessed using the Pearson correlation (R) between the predicted elasticity and the gold standard elasticity in the segmented liver regions. Experiment Results. Figure  Figure "
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,6.0,Conclusion,"PINNs have several clinically significant advantages over conventional methods for tissue elasticity reconstruction in MRE. They are more robust to noise, which is pervasive in real MRE data. Furthermore, they can leverage anatomical information from other MRI sequences that are standard practice to collect during an MRE exam, and doing so significantly improves reconstruction fidelity. Limitations of this work include the use of the incompressibility assumption to simplify the training framework, and the relatively poor contrast on simulated data. This underscores how accurate reconstruction on simulated data does not always translate to real data, and vice versa. In future work, we will evaluate PINNs for solving the general form of the PDE to investigate the effect of the incompressibility assumption. We will also extend to an operator learning framework in which the model learns to solve the PDE in a generalizable fashion without the need to retrain on each wave image. This would reduce the computation cost and enable further integration of physics-informed and data-driven learning."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 32.
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,1.0,Introduction,"During endoscopic procedures, such as colonoscopy, the camera light source produces abundant specular highlight reflections on the visualised anatomy. This is due to its very close proximity to the scene coupled with the presence of wet tissue. These reflections can occlude texture and produce salient artifacts, which may reduce the accuracy of surgical vision algorithms aiming at scene understanding, including depth estimation and 3D reconstruction  Alternatively, videos can be pre-processed to inpaint specular highlights with its hidden texture inferred from neighbouring frames  In this paper, we propose to learn robustness to specular reflections via data augmentation. We use a CycleGAN  -We propose the CycleSTTN training pipeline as an extension of STTN to a cyclic structure. -We use CycleSTTN to train a model for synthetic generation of temporally consistent and realistic specularities in endoscopy videos. We compare results of our method against CycleGAN. -We demonstrate CycleSTTN as a data augmentation technique that improves the performance of SuperPoint feature detector in endoscopy videos."
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,2.0,Related Work,"Barbed et al. investigated the challenge of specular reflections when performing feature detection in endoscopy images  Single-step approaches have also been developed that augment real data directly, but the generated images have different structures and thus, do not create paired data "
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,3.0,Methods,We use the STTN model as our video-to-video translation architecture and T-PatchGAN  1. Paired Dataset Generation We generate a dataset of paired videos with and without specularities. We train a generator for specularity removal fol-lowing the same methodology as in 
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,2.0,ST T N A Pre-training,"Using the paired dataset (V A , V R ) we train a new model to add specularities. We denote it as ST T N A0 with D A0 as its discriminator. This is shown in Fig. "
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,3.0,"ST T N R , ST T N A Joint Training By initializing with the models from",Step  
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,3.1,Model Inputs,"The STTN architecture receives as input a paired sequence of frames and masks. Originally masks are meant to represent occluded image regions that should be inpainted, however, in this work, we do not always use them in this way. When training ST T N R we define the mask inputs as regions to be inpainted (to remove specularities). However, when training ST T N A , input masks are set to 1 for all pixels, since we do not want to enforce specific locations for specularity generation; we want to encourage the model to learn these patterns from data."
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,3.2,Losses,"The loss function of the T-PatchGAN discriminators are shown below, such that E is the expected value of the data distributions as done in  (1) where F ake A = ST T N A (V R ) represents fake videos with added specularities, and analogously F ake R = ST T N R (V A ) represents fake videos with removed specularities. Further, we also define F ake R = M.F ake R + V A (1 -M ) for the discriminator loss, where inpainted occluded regions from F ake R are overlaid over V A . M denotes masks with 1 values in specular regions of V A and 0 otherwise. For the generators, an adversarial loss was used as done in  The identity loss was the only loss modified from the original STTN model  Here L idtR ensures that if a video does not have specularities, it would stay the same when fed into the model that removes specularities. Whereas, L idtA ensures predicted videos with specularities, F ake A , resemble real specular videos V A . Finally, we added cycle loss terms: The total generator losses L R and L A for removing and adding specularities are shown below, such that the loss weights λ are all set to 1 except for λ adv , which is set to 0.01 as advised by  In summary, we adopted the original STTN model "
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,4.1,Datasets and Parameters,"To evaluate our pipeline, we use 373 videos from the Hyper Kvasir dataset  In our experimental analysis, we use our proposed models shown in Fig. "
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,4.2,Pseudo Evaluation Experiments,"We input the pseudo ground truth V R to our models, ST T N A0 and ST T N A1 . We compare the output F ake A to real videos V A . We conduct non-temporal testing by using single frame inputs, as opposed to video inputs, to demonstrate the temporal effect. We report the Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Mean Square Error (MSE) metrics. We show visual results for different models in Fig.  The pseudo evaluation shows that generated specularities closely resemble real ones in appearance and location. While not fully enforcing physical realism, this augmentation improves upon traditional warping methods (Sect. 4.3)."
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,4.3,Relative Pose Estimation Experiments,We use our models as data augmentation to re-train the feature detector proposed in  In Table 
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,5.0,Conclusion,"In conclusion, we introduce CycleSTTN, a temporal CycleGAN applied to generate temporally consistent and realistic specularities in endoscopy. Our model outperforms CycleGAN, as demonstrated by mean PSNR, MSE, and SSIM metrics using a pseudo ground truth dataset. We also observe a positive effect of our model as augmentation for training a feature extractor, resulting in improved inlier precision and rotation errors. However, our evaluation relies on SFM generated ground truth, different testing and training datasets, and indirect metrics, which may introduce some uncertainty. Nevertheless, augmentation shows great promise as an addition for training various endoscopic computer vision tasks to enhance performance and provide insights into the impact of specific artifacts."
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,,Acknowledgments,". This research was funded in part, by the "
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,1.0,Introduction,"Magnetic resonance imaging (MRI) consists of a series of pulse sequences, e.g. T1-weighted (T1), contrast-enhanced (T1Gd), T2-weighted (T2), and T2-fluidattenuated inversion recovery (Flair), each showing various contrast of water and fat tissues. The intensity contrast combination of multi-sequence MRI provides clinicians with different characteristics of tissues, extensively used in disease diagnosis  Many studies have demonstrated the potential of deep learning methods for image-to-image synthesis in the field of both nature images  In this work, we propose an explainable task-specific fusion sequence-tosequence (TSF-Seq2Seq) network, which has adaptive weights for specific synthesis tasks with different input combinations and targets. Specially, this framework can be easily extended to other tasks, such as segmentation. Our primary contributions are as follows: "
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,2.0,Methods,"Figure  To leverage shared information between sequences, we use E and G from Seq2Seq "
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,2.1,Multi-sequence Fusion,"Define a set of N sequences MRI: X = {X i |i = 1, ..., N } and corresponding available indicator A ⊂ {1, ..., N } and A = ∅. Our goal is to predict the target set ∈ A} by giving the available set X A = {X i |i ∈ A} and the corresponding task-specific code c = {c src , c tgt } ∈ Z 2N . As shown in Fig.  Task-Specific Weighted Average. The weighted average is an intuitive fusion strategy that can quantify the contribution of different sequences directly. To learn the weight automatically, we use a trainable fully connected (FC) layer to predict the initial weight ω 0 ∈ R N from c. where W and b are weights and bias for the FC layer, = 10 -5 to avoid dividing 0 in the following equation. To eliminate distractions and accelerate training, we force the weights of missing sequences in ω 0 to be 0 and guarantee the output where • refers to the element-wise product and •, • indicates the inner product. With the weights ω, we can fuse multi-sequence features as f by the linear combination. Specially, f ≡ E(X i ) when only one sequence i is available, i.e. A = {i}. It demonstrates that the designed ω can help the network excellently inherit the synthesis performance of pre-trained E and G. In this work, we use ω to quantify the contribution of different input combinations. Task-Specific Attention. Apart from the sequence-level fusion of f , a taskspecific attention module G A is introduced to refine the fused features at the pixel level. The weights of G A can adapt to the specific fusion task with the given target code. To build a conditional attention module, we replace convolutional layers in convolutional block attention module (CBAM)  As shown in Fig.  Loss Function. To force both f and f + f A can be reconstructed to the target sequence by the conditional G, a supervised reconstruction loss is given as, where • 1 refers to a L 1 loss, and L p indicates the perceptual loss based on pre-trained VGG19. λ r and λ p are weight terms and are experimentally set to be 10 and 0.01."
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,2.2,Task-Specific Enhanced Map,"As f A is a task-specific contextual refinement for fused features, analyzing it can help us understand more what the network tried to do. Many studies focus on visualizing the attention maps to interpret the principle of the network, especially for the transformer modules  3 Experiments"
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,3.1,Dataset and Evaluation Metrics,"We use brain MRI images of 1,251 subjects from Brain Tumor Segmentation 2021 (BraTS2021) "
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,3.2,Implementation Details,"The models are implemented with PyTorch and trained on the NVIDIA GeForce RTX 3090 Ti GPU. E comprises three convolutional layers and six residual blocks. The initial convolutional layer is responsible for encoding intensities to features, while the second and third convolutional layers downsample images by a factor of four. The residual blocks then extract the high-level representation. The 28.5 ± 2.5 0.883 ± 0.040 9.65 ± 3.57 DiamondGAN "
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,3.3,Quantitative Results,"We compare our method with one-to-one translation, image-level fusion, and feature-level fusion methods. One-to-one translation methods include Pix2Pix "
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,3.4,Ablation Study,"We compare two components of our method, including (1) task-specific weighted average and (2) task-specific attention, by conducting an ablation study between Seq2Seq, TSF-Seq2Seq (w/o f A ), and TSF-Seq2Seq. TSF-Seq2Seq (w/o f A ) refers to the model removing the task-specific attention module. As shown in Table "
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,3.5,Interpretability Visualization,"The proposed method not only achieves superior synthesis performance but also has good interpretability. In this section, we will visualize the contribution of different input combinations and TSEM.  Sequence Contribution. We use ω in Eq. 2 to quantify the contribution of different input combinations for synthesizing different target sequences. Figure  TSEM vs. Attention Map. Figure "
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,4.0,Conclusion,"In this work, we introduce an explainable network for multi-to-one synthesis with extensive experiments and interpretability visualization. Experimental results based on BraTS2021 demonstrate the superiority of our approach compared with the state-of-the-art methods. And we will explore the proposed method in assisting downstream applications for multi-sequence analysis in future works."
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 5.
Inverse Consistency by Construction for Multistep Deep Registration,1.0,Introduction,"Image registration, or finding the correspondence between a pair of images, is a fundamental task in medical image computing. One desirable property for registration algorithms is inverse consistency -the property that the transform found registering image A onto image B, composed with the transform found by registering image B onto image A, yields the identity map. Inverse consistency is useful for several reasons. Practically, it is convenient to have a single transform and its inverse associating two images instead of two transforms of unknown relationship. For within-subject registration, inverse consistency is often a natural assumption as long as images are consistent with each other, e.g., did not undergo surgical removal of tissue. For time series analysis, inverse consistency prevents bias "
Inverse Consistency by Construction for Multistep Deep Registration,2.0,Related Work,"Inverse consistency in deep image registration approaches is commonly promoted via a penalty  Mok et al.  . This registration network is inverseconsistent by construction, but only supports one step. Our approach will provide a general inverse consistent multi-step framework. Iglesias et al.  ). This symmetrization retains inverse consistency and is conceptually similar to our approach. However, their approach, unlike ours, does not directly extend to N steps and is not trained end to end. There is extensive literature on deep multi-step approaches. The core idea is to conduct the registration in multiple steps with the warped image produced by the previous step being the input to the latter step. Thus, the original input image pairs can be registered progressively. AVSM  To address this issue, we propose a novel operator for multi-step registration to obtain inverse consistent registration by construction. Definitions and Notation. We use subscripted capital letters, e.g., N θ , to represent neural networks that return arrays of numbers, and capital Greek letters Φ, Ψ, and Ξ to represent registration neural networks, i.e., neural networks that return transforms. A transform is a function R D → R D with D denoting the dimension of the images we are registering. N AB θ is shorthand for N θ called on the images I A and I B , likewise For a Lie group G and associated algebra g, exp is the (Lie-)exponential map from g → G "
Inverse Consistency by Construction for Multistep Deep Registration,3.0,Lie-Group Based Inverse Consistent Registration,"To design a registration algorithm, one must pick a class of transforms that the algorithm will return. Many types of transforms that are useful for practical medical registration problems happen to also be Lie groups. We describe a procedure for designing a neural network that outputs a member of a specified Lie group in an inverse consistent manner and provide several examples. Recall that a Lie group G is always associated with a Lie algebra g. Create a neural network N θ (of arbitrary design) with two input images and an output that can be considered an element of g. A registration network Φ defined to act as follows on two images is inverse consistent, because g(I A , I B ) = -g(I B , I A ) by construction. We explore how this applies to several Lie groups. Rigid Registration. The Lie algebra of rigid rotations is skew-symmetric matrices. N θ outputs a skew-symmetric matrix R and a vector t, so that where Φ (rigid) will output a rigid transformation in an inverse consistent manner. Here, the exponential map is just the matrix exponent. Affine Registration. Relaxing R to be an arbitrary matrix instead of a skewsymmetric matrix in the above construction produces a network that performs inverse consistent affine registration. Nonparametric Vector Field Registration. In the case of the group of diffeomorphisms, the corresponding Lie algebra which is an inverse consistent nonparametric registration network. This is equivalent to the standard SVF technique for image registration, with a velocity field represented as a grid of vectors equal to MLP Registration. An ongoing research question is how to represent the output transform as a multi-layer perceptron (MLP) applied to coordinates. One approach is to reshape the vector of outputs of a ConvNet so that the vector represents the weight matrices defining an MLP (with D inputs and D outputs). This MLP is then a member of the Lie algebra of vector-valued functions, and the exponential map to the group of diffeomorphisms can be computed by solving the following differential equation to t = 1 using an integrator such as fourthorder Runge-Kutta. Again, by defining the velocity field to flip signs when the input image order is flipped, we obtain an inverse consistent transformation:"
Inverse Consistency by Construction for Multistep Deep Registration,4.0,Multi-step Registration,"The standard approach to composing two registration networks is to register the moving image to the fixed image, warp the moving image and then register the warped moving image to the fixed image again and compose the transforms. This is formalized in  Unfortunately, TwoStep  The inverses are interleaved so that even if they were exact, they can not cancel. Our contribution is an operator, TwoStepConsistent, that is inverse consistent if its components are inverse consistent. We assume that our component networks Φ and Ψ are inverse consistent, and that Φ returns a transform that we can explicitly find the square root of, such that √ Φ AB • √ Φ AB = Φ AB . Note that for transforms defined by Φ AB = exp(g) , √ Φ AB = exp(g/2). Since each network is inverse consistent, we have access to the inverses of the transforms they return. We begin with the relationship that Φ will be trained to fulfill  We isolate the transform in the left half of Eq. (  In fact, we can verify that Notably, This Procedure Extends to N -Step Registration. With the operator of Eq. ( "
Inverse Consistency by Construction for Multistep Deep Registration,5.0,Synthetic Experiments,"Inverse Consistent Rigid, Affine, Nonparametric, and MLP Registration. We train networks on MNIST 5 s using the methods in Sects. 3 and 4, demonstrating that the resulting networks are inverse-consistent. Our TwoStepConsistent (TSC) operator can be used on any combination of the networks defined in Sect. 3. For demonstrations, we join an MLP registration network to a vector field registration network, and join two affine networks to two vector field networks. Figure  Affine Registration Convergence. In addition to being inverse consistent, our method accelerates convergence and stability of affine registration, compared to directly predicting the matrix of an affine transform. Here, we disentangle whether this happens for any approach that parameterizes an affine transform by taking the exponent of a matrix, or whether this acceleration is Fig.  unique to our inverse consistent method. We also claim that multi-step registration is important for registration accuracy and convergence time and that an inverse consistent multi-step operator, TwoStepConsistent, is thus beneficial. To justify these claims, we investigate training for affine registration on the synthetic Hollow Triangles and Circles dataset from  We observe that parameterizing an affine registration using the exp(N AB θ -N BA θ ) construction speeds up the first few epochs of training and gets even faster when combined with any multi-step method. In Fig.  Finally, as expected, the only two approaches that are inverse consistent are the single-step inverse consistent by construction network, and the network using two inverse-consistent by construction subnetworks, joined by the TwoStepConsistent operator. (Fig. "
Inverse Consistency by Construction for Multistep Deep Registration,6.0,Evaluation on 3-D Medical Datasets,"We evaluate on several datasets, where we can compare to earlier registration approaches. We use the network Φ := TSC{Ψ 1 , TSC{Ψ 2 , TSC{Ξ 1 , Ξ 2 }}} with Ξ i inverse-consistent SVF networks backed by U-Nets and Ψ i inverseconsistent affine networks backed by ConvNets"
Inverse Consistency by Construction for Multistep Deep Registration,6.1,Datasets,COPDGene/Dirlab Lung CT. We follow the data selection and preprocessing of 
Inverse Consistency by Construction for Multistep Deep Registration,,OASIS Brain MRI.,We use the OASIS-1 
Inverse Consistency by Construction for Multistep Deep Registration,6.2,Comparisons,We use publicly-available pretrained weights and code for ANTs 
Inverse Consistency by Construction for Multistep Deep Registration,7.0,Conclusion,"The fundamental impact of this work is as a recipe for constructing a broad class of exactly inverse consistent, multi-step registration algorithms. We also are pleased to present registration results on four medically relevant datasets that are competitive with the current state of the art, and in particular are more accurate than existing inverse-consistent-by-construction neural approaches."
Geometric Ultrasound Localization Microscopy,1.0,Introduction,"Ultrasound Localization Microscopy (ULM) has revolutionized medical imaging by enabling sub-wavelength resolution from images acquired by piezo-electric transducers and computational beamforming. However, the necessity of beamforming for ULM remains questionable. Our work challenges the conventional assumption that beamforming is the ideal processing step for ULM and presents an alternative approach based on geometric reconstruction from Time-of-Arrival (ToA) information. The discovery of ULM has recently surpassed the diffraction-limited spatial resolution and enabled highly detailed visualization of the vascularity  While Contrast-Enhanced Ultra-Sound (CEUS) is used in the identification of musculoskeletal soft tissue tumours  for ULM to investigate its potential to refine MB localization  To this end, we propose an alternative approach for ULM, outlined in Fig. "
Geometric Ultrasound Localization Microscopy,2.0,Method,Geometric modeling is a useful approach for locating landmarks in space. One common method involves using a Time-of-Flight (ToF) round-trip setup that includes a transmitter and multiple receivers 
Geometric Ultrasound Localization Microscopy,2.1,Feature Extraction,"Feature extraction of acoustic signals has been thoroughly researched  where t ∈ R T denotes the time domain with a total number of T samples and angular frequency ω k and phase φ k for each echo k. Note that erf(•) is the error function. To estimate these parameters iteratively, the cost function is given by, where y n (t) is the measured signal from waveform channel n ∈ {1, 2, . . . , N} and the sum over k accumulates all echo components mn = [m 1 , m 2 , . . . , m K ] . We get the best echo feature set m n over all iterations j via, for which we use the Levenberg-Marquardt solver. Model-based optimization requires initial estimates to be nearby the solution space. For this, we detect initial ToAs via gradient-based analysis of the Hilbert-transformed signal to set m(1) n as in "
Geometric Ultrasound Localization Microscopy,2.2,Ellipse Intersection,"While ellipse intersections can be approximated iteratively, we employ Eberly's closed-form solution  where the virtual transmitter ûs ∈ R 2 and each receiver u n ∈ R 2 with channel index n represent the focal points of an ellipse, respectively. For the intersection, we begin with the ellipse standard equation. Let any point s ∈ R 2 located on an ellipse and displaced by its center c n ∈ R 2 such that, where M contains the ellipse equation with v n and v ⊥ n as a pair of orthogonal ellipse direction vectors, corresponding to their radial extents (r 0 , r 1 ) as well as the squared norm • 2  2 and vector norm | • |. For subsequent root-finding, it is the goal to convert the standard Eq. (  where B and b carry high-order polynomial coefficients b j found via matrix factorization  Let two intersecting ellipses be given as quadratic equations A(x, y) and B(x, y) with coefficients a j and b j , respectively. Their intersection is found via polynomial root-finding of the equation, where ∀j, d j = a jb j . When defining y = w -(a 2 + a 4 x)/2 to substitute y, we get A(x, w) = w 2 + (a 0 + a 1 x + a 3 x 2 ) -(a 2 + a 4 x) 2 /4 = 0 which after rearranging is plugged into "
Geometric Ultrasound Localization Microscopy,2.3,Clustering,"Micro bubble reflections are dispersed across multiple waveform channels yielding groups of location candidates for the same target bubble. Localization deviations result from ToA variations, which can occur due to atmospheric conditions, receiver clock errors, and system noise. Due to the random distribution of corresponding ToA errors  Here, the bandwidth of the kernel is set to λ/4. The Mean Shift algorithm updates the estimate p (j) by setting it to the weighted mean density on each iteration j until convergence. In this way, we obtain the position of the target bubble."
Geometric Ultrasound Localization Microscopy,3.0,Experiments,"Dataset: We demonstrate the feasibility of our geometric ULM and present benchmark comparison outcomes based on the PALA dataset  Metrics: For MB localization assessment, the minimum Root Mean Squared Error (RMSE) between the estimated p and the nearest ground truth position is computed. To align with the PALA study  For a realistic analysis, we employ the noise model used in  where σ p = √ B × 10 P/10 and N (0, σ 2 p ) are normal distributions with mean 0 and variance σ 2 p . Here, L C and L A are noise levels in dB, and n(t) is the array of length T containing the random values drawn from this distribution. The additive noise model is then used to simulate a waveform channel y n (t) = y n (t)+ n(t) g(t, σ f ) suffering from noise, where represents the convolution operator, and g(t, σ f ) is the one-dimensional Gaussian kernel with standard deviation σ f = 1.5. To mimic the noise reduction achieved through the use of sub-aperture beamforming with 16 transducer channels  Baselines: We compare our approach against state-of-the-art methods that utilize beamforming together with classical image filterings  Results: Table  Table "
Geometric Ultrasound Localization Microscopy,4.0,Summary,"This study explored whether a geometric reconstruction may serve as an alternative to beamforming in ULM. We employed an energy-based model for feature extraction in conjunction with ellipse intersections and clustering to pinpoint contrast agent positions from RF data available in the PALA dataset. We carried out a benchmark comparison with state-of-the-art methods, demonstrating that our geometric model provides enhanced resolution and detection reliability with fewer transducers. This capability will be a stepping stone for 3-D ULM reconstruction where matrix transducer probes typically consist of 32 transducers per row only. It is essential to conduct follow-up studies to evaluate the high potential of our approach in an extensive manner before entering a pre-clinical phase. The promising results from this study motivate us to expand our research to more RF data scenarios. We believe our findings will inspire further research in this exciting and rapidly evolving field."
Geometric Ultrasound Localization Microscopy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_21.
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,1.0,Introduction,"The GE Discovery NM Alcyone 530c/570c  Previous works have attempted to address this problem by using deeplearning-based image-to-image networks. Xie et al. proposed a 3D U-net-like network to directly synthesize dense-view images from few-view counterparts  There are a few previous studies proposed to learn the mapping between raw data and images. AUTOMAP  Here, we propose a novel 3D Transformer-based Dual-domain (projection & image) Network (TIP-Net) to address these challenges. The proposed method reconstructs high-quality few-view cardiac SPECT using a two-stage process. First, we develop a 3D projection-to-image transformer reconstruction network that directly reconstructs 3D images from the projection data. In the second stage, this intermediate reconstruction is combined with the original few-view reconstruction for further refinement, using an image-domain reconstruction network. Validated on physical phantoms, porcine, and human studies acquired on GE Alcyone 570c SPECT/CT scanners, TIP-Net demonstrated superior performance than previous baseline methods. Validated by cardiac catheterization (Cath) images, diagnostic results from nuclear cardiologists, and cardiac defect quantified by an FDA-510(k)-cleared software, we also show that TIP-Net produced images with higher resolution and cardiac defect contrast on human studies, as compared to previous baselines. Our method could be a clinically useful tool to improve cardiac SPECT imaging."
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,2.0,Methodology,Following the acquisition protocol described by Xie et al. 
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,2.1,Network Structure,"The overall network structure is presented in Fig.  To reconstruct IMG p in P-net, we may simply project the output from the Transformer block to the size of 3D reconstructed volume (i.e., 70 × 70 × 50). However, such implementation requires a significant amount of GPU memory. To alleviate the memory burden, we proposed to learn the 3D reconstruction in a slice-by-slice manner. The output from the Transformer block is projected to a single slice (70 × 70), and the whole Transformer network (red rectangular in Fig.  Both 3D-CNN 1 and 3D-CNN 2 use the same network structure as the network proposed in "
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,2.2,"Optimization, Training, and Testing","In this work, the TIP-Net was trained using a Wasserstein Generative Adversarial Network (WGAN) with gradient penalty  where X and Y represent two image volumes used for calculations. MAE and SSIM represent MAE and SSIM loss functions, respectively. The Sobel operator (SO) was used to obtain edge images, and the MAE between them was included as the loss function. λ a = 0.1, λ b = 0.005, λ c = 0.8, and λ d = 0.1 were fine-tuned experimentally. Network D shares the same structure as that proposed in "
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,2.3,Evaluations,"Reconstructed images were quantitatively evaluated using SSIM, RMSE, and PSNR. Myocardium-to-blood pool (MBP) ratios were also included for evaluation. For myocardial perfusion images, higher ratios are favorable and typically represent higher image resolution. Two ablated networks were trained and used for comparison. One network, denoted as 3D-CNN, shared the same structure as either 3D-CNN 1 or 3D-CNN 2 but only with IMG mlem as input. The other network, denoted as Dual-3D-CNN, used the same structure as the TIP-Net outlined in Fig.  To compare the performance of cardiac defect quantifications, we used the FDA 510(k)-cleared Wackers-Liu Circumferential Quantification (WLCQ) software  Cath images and cardiac polar maps are also presented. Cath is an invasive imaging technique used to determine the presence/absence of obstructive lesions that results in cardiac defects. We consider Cath as the gold standard for the defect information in human studies. The polar map is a 2D representation of the 3D volume of the left ventricle. All the metrics were calculated based on the entire 3D volumes. All the clinical descriptions of cardiac defects in this paper were confirmed by nuclear cardiologists based on SPECT images, polar maps, WLCQ quantification, and Cath images (if available)."
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,3.1,Porcine Results,"Results from one sample porcine study are presented in Fig.  Quantitative results for 8 porcine and 2 physical phantom studies are included in Table  Average defect size measurements for porcine and physical phantom studies with cardiac defects are 35.9%, 42.5%, 42.3%, 43.6%, 47.0%, 46.2% for one-angle, 3D-CNN, Dual-3D CNN, TIP-Net, and four-angle image volumes, respectively. Images reconstructed by TIP-Net present overall higher defect contrast and the measured defect size is closest to the four-angle images. For normal porcine and physical phantom studies without cardiac defect, these numbers are 16.0%, 12.0%, 14.7%, 11.2%, 11.5%, and 10.1%. All neural networks showed improved uniformity in the myocardium with lower measured defect size. Both transformer-based methods, TIP-Net and SSTrans-3D, showed better defect quantification than other methods on these normal subjects. "
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,3.2,Human Results,"Chosen by a nuclear cardiologist, results from one representative human study with multiple cardiac defects are presented in Fig.  14 patients in the testing data have cardiac defects, according to diagnostic results. The average defect size measurements for these patients are 16.8%, 22.6%, 21.0%, 22.4%, and 23.6% for one-angle, 3D-CNN, Dual-3D-CNN, SSTrans-3D, and TIP-Net results. The higher measured defect size of the TIP-Net indicates that the proposed TIP-Net produced images with higher defect contrast. For the other 6 patients without cardiac defects, these numbers are 11.5%, 12.8%, 13.8%, 12.4%, and 11.8%. These numbers show that TIP-Net did not introduce undesirable noise in the myocardium, maintaining the overall uniformity for normal patients. However, other deep learning methods tended to introduce non-uniformity in these normal patients and increased the defect size."
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,3.3,Intermediate Network Output,"To further show the effectiveness of P-net in the overall TIP-Net design, outputs from P-net (images reconstructed by the network directly from projections) are presented in Fig.  The human study presented in Fig. "
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,4.0,Discussion and Conclusion,"We proposed a novel TIP-Net for 3D cardiac SPECT reconstruction. To the best of our knowledge, this work is the first attempt to learn the mapping from 3D realistic projections to 3D image volumes. Previous works in this direction  The proposed method was tested for myocardial perfusion SPECT imaging. Validated by nuclear cardiologists, diagnostic results, Cath images, and defect size measured by WLCQ, the proposed TIP-Net produced images with higher resolution and higher defect contrast for patients with perfusion defects. For normal patients without perfusion defects, TIP-Net maintained overall uniformity in the myocardium with higher image resolution. Similar performance was observed in porcine and physical phantom studies."
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 16.
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,1.0,Introduction,"The substantial reduction of scanning radiation dose and its accurate reconstruction are of great clinical significance for multiphase contrast-enhanced computed tomography (CECT) imaging. 1) Multiphase CECT requires multiple scans at different phases, such as arterial phase, venous phase, delayed phase and etc., to demonstrate the anatomy and lesion with the contrast agent evolution intra human body over time  Multi-phase low-dose CT reconstruction is still ignored, though single-phase methods behave promising results on their issues  In this paper, guided with Joint Condition, a novel Circle-Supervision based Poisson Flow Generative Model (JCCS-PFGM) is proposed to make the progressive low-dose reconstruction for multiphase CECT. It deeply explores the correlation among multiphase and the mapping learning of PFGM, to progressively reduce the scanning radiation dose of multiphase CECT to the ultra low level of 5% dose, and achieve the high-quality reconstruction with noise reduction and structure maintenance. It thus significantly reduces the radiation risk of multiple CT scans in a short time, accompanied with clear multiphase CECT examination images. The main contributions of JCCS-PFGM can be summarized as: 1) an effectively progressive low-dose reconstruction mechanism is developed to leverages the imaging consistency and radiocontrast evolution along formerlatter phases, so that enormously reduces the radiation dose needs and improve the reconstruction effect, even for the latter-phase scanning with extremely low dose; 2) a newly-designed circle-supervision strategy is proposed in PFGM to enhance the refactoring capabilities of normalized poisson field learned from the perturbed space to the specified CT image space, so that boosts the explicit reconstruction for noise reduction; 3) a novel joint condition is designed to explore correlation between former phases and current phase, so that extracts the complementary information for current noisy CECT and guides the reverse process of diffusion jointly with multiphase condition for structure maintenance."
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,2.0,Methodology,As shown in Fig. 
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,2.1,Progressive Low-Dose Reconstruction Mechanism,"The progressive low-dose reconstruction mechanism effectively promotes highlevel base from former-phase to latter-phase for successively multiphase CECT reconstruction, instead of the casually equal dose reduction seriously breaking the structure in each phase. It further exploits the inherent consistency traceable along multiphase CECT to reduce the burden of multiphase reconstruction. As show in Fig. "
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,2.2,Circle-Supervision Strategy Embedded in PFGM,"The circle-supervision strategy robustly boosts the refactoring capabilities of normalized Poisson field learned by PFGM. So that it further promotes the explicit reconstruction for noise reduction, instead of just CT-similar image generation. PFGM is good at mapping a uniform distribution on a high-dimensional hemisphere into any data distribution  Here, we further propose the circle-supervision strategy on the normalized Poisson field which reflects the mapping direction from the perturbed space to the specified CT image space. It remedies the precise perception on the target initial CECT besides mapping direction learning, to enhance the crucial field components. As shown in Fig. "
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,2.3,Joint Condition Fusing Multiphase Consistency and Evolution,"The joint condition comprehensively fuses the consistency and evolution from the previous phases to enhance the current ultra low-dose CECT. With such multiphase fusion, the reverse process of diffusion is able to get the successive guide to perceive radiocontrast evolution for structure maintenance As shown in Fig.  3 Experiments  "
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,3.2,Results and Analysis,Overall Performance. As the last column shown in  Comparison with Competing Methods. As shown in Table 
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,4.0,Conclusion,"In this paper, we propose JCCS-PFGM to make the progressive low-dose reconstruction for multiphase CECT. JCCS-PFGM t creatively consists of 1) the progressive low-dose reconstruction mechanism utilizes the consistency along the multiphase CECT imaging; 2) the circle-supervision strategy embedded in PFGM makes further self-inspection on normal poisson field prediction; 3) the joint condition integrates the multi-phase consistency and evolution in guiding the reverse process of diffusion. Extensive experiments with promising results from both quantitative evaluations and qualitative assessments reveal our method a great clinical potential in CT imaging."
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,1.0,Introduction,"Computed tomography (CT) has been widely adopted in clinical applications. To reduce the radiation dose and shorten scanning time, sparse-view CT has drawn much attention in the community  For the sparse-view (SV) reconstruction, the existing deep-learning (DL)based methods can be roughly divided into three categories based on the information domain exploited, e.g., sinogram domain, image domain, and dual domains. Specifically, for the sinogram-domain methods, sparse-view sinograms are firstly repaired based on deep networks, such as U-Net  For the metal artifact reduction (MAR) task, similarly, the current DL-based approaches can also be categorized into three types. To be specific, sinogramdomain methods aim to correct the sinogram for the subsequent CT image reconstruction  Albeit achieving promising performance, these aforementioned methods are sub-optimal for the SVMAR task. The main reasons are: 1) Most of them do not consider the joint influence of sparse data sampling and MAR, and do not fully embed the physical imaging constraint between the sinogram domain and CT image domain under the SVMAR scenario; 2) Although a few works focus on the joint SVMAR task, such as "
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,,1-⋃ ⋃,"Fig.  To alleviate these issues, in this paper, we propose a model-driven equivariant proximal network, called MEPNet, which is naturally constructed based on the CT imaging geometry constraint for this specific SVMAR task, and takes into account the inherent prior structure underlying the CT scanning procedure. Concretely, we first propose a dual-domain reconstruction model and then correspondingly construct an unrolling network framework based on a derived optimization algorithm. Furthermore, motivated by the fact that the same organ can be imaged at different angles making the reconstruction task equivariant to rotation "
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,2.0,Preliminary Knowledge About Equivariance,"Equivariance of a mapping w.r.t. a certain transformation indicates that executing the transformation on the input produces a corresponding transformation on the output  where f is any input feature map in the input feature space; T g and T g represent the actions of g on the input and output, respectively. The prior work "
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,3.0,Dual-Domain Reconstruction Model for SVMAR,"In this section, for the SVMAR task, we derive the corresponding dual domain reconstruction model and give an iterative algorithm for solving it. Dual-Domain Reconstruction Model. Given the captured sparse-view metal-affected sinogram Y svma ∈ R N b ×Np , where N b and N p are the number of detector bins and projection views, respectively, to guarantee the data consistency between the reconstructed clean CT image X ∈ R H×W and the observed sinogram Y svma , we can formulate the corresponding optimization model as  where D ∈ R N b ×Np is the binary sparse downsampling matrix with 1 indicating the missing region; T r ∈ R N b ×Np is the binary metal trace with 1 indicating the metal-affected region; P is forward projection; R(•) is a regularization function for capturing the prior of X; ∪ is the union set; is the point-wise multiplication; H and W are the height and width of CT images, respectively; μ is a trade-off parameter. One can refer to Fig.  To jointly reconstruct sinogram and CT image, we introduce the dual regularizers R 1 (•) and R 2 (•), and further derive Eq. (2) as: where S is the to-be-estimated clean sinogram; λ is a weight factor. Following  As observed, given Y svma , we need to jointly estimate S and X. For R 1 (•) and R 2 (•), the design details are presented below. Iterative Optimization Algorithm. To solve the model (4), we utilize the classical proximal gradient technique  where η i is stepsize; prox μiηi (•) is proximal operator, which relies on the regularization term R i (•). For any variable, its iterative rule in Eq. ( "
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,4.0,Equivariant Proximal Network for SVMAR,"By unfolding the iterative rules  x , we carefully investigate that during the CT scanning, the same body organ can be imaged at different rotation angles. However, the conventional CNN for modeling proxNet θ (k) x in  Specifically, from Eq. (  where π θ is a rotation operator. Due to its solid theoretical foundation, the Fourier-series-expansion-based method  Fig.  where x = [x i , x j ] T is 2D spatial coordinates; a mn and b mn are learnable expansion coefficients; ϕ c mn x and ϕ s mn x are 2D fixed basis functions as designed in  Based on the parameterized filter in Eq. (  x in Eq. ( "
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,5.1,Details Description,"Datasets and Metrics. Consistent with  The proposed method is tested on three datasets including DeepLesion-test (2000 pairs), Pancreas-test (50 pairs), and CLINIC-test (3397 pairs). Specifically, DeepLesion-test is generated by pairing another 200 clean CT images from DeepLesion "
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,5.2,Performance Evaluation,Working Mechanism. Figure  Figure 
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_11.
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,1.0,Introduction,"Magnetic resonance imaging (MRI) has seen a tremendous growth over the past three decades as a preferred diagnostic imaging modality. Starting with the 0.25T MRI scanners in 19801980ss  Since the past decade, there is a growing interest towards improving the quality of images acquired with low FS MRI scanners by learning the features responsible for image quality from high FS MRI scanners  In this work, we address the problem of estimating 3T-like images (x) from 1.5T images (y) in an unsupervised manner. To our best knowledge, this is the first work to develop a method for improving ≤1.5T images to estimate HF images without requiring example images. The proposed method formulates the estimation of 3T HF images as an inverse problem: y = f (x), where f (.) is the unknown degradation model. The novel contributions of our work are as follows: (i) the alternate minimization (AM) framework is formulated to estimate x as well as the mapping kernel f (.) relating x and y, (ii) Acquisition physics based signal scaling is proposed to synthesize the desired image contrast similar to HF image, (iii) the simulated contrast image is used as a regularizer while estimating x from y. The experimental results demonstrate that the proposed approach provides improved quality of MRI images in terms of image contrast and sharp tissue boundaries that is similar to HF images. The experiments further demonstrate the successful application of the improved quality images provided by proposed method in improved tissue segmentation and volume quantification."
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,2.0,Proposed Method,"The proposed work formulates the estimation of HF image (x) from LF image (y) in an alternate minimization framework: Here x ∈ R m×n and y ∈ R m×n represent the HF and LF MRI images, respectively. The matrix h ∈ R p×p represents transformation kernel convolved using * operator with each patch of y of size p × p and p is empirically chosen as 5. The matrix c ∈ R m×n represent the pixel wise scale when multiplied with y generates the image with contrast similar to HF image. Here, c y represents the Hadamard product of pixel wise scale c and y the 1.5T (or 0.25T) image."
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,2.1,Physics Based Regularizer,"The physics based regularizer exploits the fact that the T1 relaxation time increases with FS. The differences among T1 relaxation times of gray matter (GM), white matter (WM) and cereberospinal fluid (CSF) also increase with FS, leading to increased contrast in the T1-weighted images in the order 3T ≥1.5T ≥ 0.25T. This factor is used to simulate an image from y which convey similar information as x, and is obtained by scaling the signal intensities of y based on changes in signal due to changes in T1 relaxation time with respect to FS-denoted by r. The acquired signal for y denoted by s l can be corrected with relaxation time to simulate the signal s h that would have been acquired for x as: For example, in the spin echo (SE) pulse sequence, acquired signal can be represented as, s = AB 2 0 (1-e (-T R/T 1 ) sinθ) (1-e (-T R/T 1 ) cosθ) e (-T E/T2)  Here, T 1 and T 2 represent the T1/T2 relaxation times, T R-repetition time, T Eecho time and θ-flip angle (FA). The parameters with subscript h and l represent the parameters used for HF and LF MRI acquisitions, respectively. Similarly, respective mathematical formulations can be used for other pulse sequences such as fast SE (FSE) and gradient echo (GRE). After computing r using Eq. (  The values of T 1 /T 2 relaxation times differ with tissues, hence different values of r should be computed for every voxel considering the tissues present in that voxel. However, if we assign single tissue per voxel using any tissue segmentation technique such as FAST "
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,,Compute Relaxation Times for Voxels with More Than One Tissue:,"In real practice there exist many voxels with more than one tissue kind present in them, and is the reason for discontinuities present in Fig.  After approximating T 1,l and T 1,h for all voxels (and empirically choosing the T R h and θ h ) the r is computed using Eq. ( "
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,3.0,Experimental Results,"The proposed work has been demonstrated for (i) estimating 3T-like from 1.5T images, (ii) estimating 1.5T-like from 0.25T images, (iii) evaluating accuracy of tissue segmentation using improved images in (i) and (ii), and (iv) comparing (i), (ii) and (iii) with existing methods. The results related to (ii) are summarized in SM due to space constraints. The values for λ 1 and λ 2 are chosen as 1.2 and 0.4, respectively."
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,3.1,Data,"The MRI images used to demonstrate the efficacy of proposed work were acquired from five healthy subjects of age 25 ± 10 years. Three different MRI scanners were used in this study: 0.25T (G-scan Brio, Esaote), 1.5T (Aera Siemens) and 3T (Verio, Siemens). The first three subjects were scanned using each of the three scanners while the other two subjects were scanned with only 1.5T and 3T scanners. The three scanners were located in Post Graduate Institute of Medical Education & Research (PGIMER) Chandigarh, India. Scanning was performed using the standard clinical protocols-optimized for both clinical requirement and work-load of clinical site. All the scans were performed according to the guidelines of the Declaration of Helsinki. The details of pulse sequence and scan parameters used to acquire data in each of the three scanners is mentioned in SM Table "
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,3.2,Analysis/Ablation Study of Proposed Approach,The HF image is estimated at different stages of proposed approach to demonstrate the significance of each term in Eq. ( 
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,3.3,Comparison with Existing Approaches,The performance of proposed approach is compared with existing methods that either address the contrast synthesis or estimation of HF images ScSR 
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 13.
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,3.4,Application to Tissue Segmentation and Volume Quantification,"The segmentation labels for WM, GM and CSF were computed using FAST toolbox in FSL software "
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,4.0,Summary,"We propose a method to estimate HF images from ≤1.5T images in an unsupervised manner. Here, the knowledge of acquisition physics to simulate HF image is exploited, and used it in a novel way to regularize the estimation of HF image. The proposed method demonstrates the benefits over state of the art supervised methods that are severely effected by the inaccuracies if present in image registration process. Lower the FS image is, harder is to get accurate image registration, and thus proposed method proves to be a better choice. Further, it is also demonstrated that the proposed approach provides statistically significant accurate tissue segmentation. The code for this work is publicly shared on https://drive.google.com/drive/folders/1WbzkBJS1BWAje8aF0ty2SWYTQ9i0 B7Yr?usp=sharing."
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,1.0,Introduction,Magnetic Resonance Imaging (MRI) has revolutionized medical diagnosis by providing a non-invasive imaging tool with multiple contrast options  Recent studies have shown that multi-contrast data routinely acquired in MRI examinations can be used to develop more powerful super-resolution methods tailored for MRI by using fully sampled images of one contrast as a reference (Ref) to guide the recovery of high-resolution (HR) images of another contrast from low-resolution (LR) inputs  Recent advances in super-resolution techniques have led to the development of hard-attention-based texture transfer methods (such as TTSR  As shown in Fig. 
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,2.0,Methodology,"Overall Architecture. Our goal is to develop a neural network that can restore an HR image from an LR image and a Ref image. Our approach consists of several modules, including an encoder, a dual cross-attention transformer (DCAT) and a decoder, as shown in Fig.  Encoder. To extract features from the up-sampled LR, we employ an encoder consisting of four stages. The first stage uses the combination of a depth-wise convolution and a residual block. In stages 2-4, we utilize a down-sampling layer and a residual block to extract multi-scale features. In this way, the multiscale features for the LR ↑ are extracted as  The core of DCAT is dual cross-attention mechanism, which is diagrammed in Fig.  where q share ,k share ,v spatial and v channel are the parameter weights for shared queries, shared keys, spatial value layer, and channel value layer, respectively. In spatial cross-attention, we further project k share and v spatial to k project and v project through linear layers, to reduce the computational complexity. The spatial and channel attentions are calculated as: Finally, X spatial and X channel are reduced to half channel via 1 × 1 convolutions, and then concatenate to obtain the final feature: For the whole DCAT, the normalized features LN (F LR ) and LN (F Ref ) are fed to the DCA and added back to F LR . The obtained feature is then processed by the FFN in a residual manner to generate the texture feature. Specifically, the DCAT is summarized as: Feeding the multi-scale features of LR ↑ and Ref to DCAT, we can generate the texture features in multi-scales, denoted as T exture H×W , T exture Decoder. In the decoder, we start from the feature F is then up-sampled and feed to Fusion along with T exture is up-sampled and feed to Fusion along with T exture H×W , generating F used H×W . Finally, F used H×W is processed with a 1 × 1 convolution to generate SR. In the Fusion module, following  Loss Function. For simplicity and without loss of generality, L 1 loss between the restored SR and ground-truth is employed as the overall reconstruction loss."
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,3.0,Experiments,"Datasets and Baselines. We evaluated our approach on two datasets: 1) fastMRI, one of the largest open-access MRI datasets. Following the settings of SANet  Implementation Details. All the experiments were conducted using Adam optimizer for 50 epochs with a batch size of 4 on 8 Nvidia P40 GPUs. The initial learning rate for SANet was set to 4 × 10 -5 according to  Quantitative Results. The quantitative results are summarized in Table  Qualitative Evaluation. Visual comparison is shown in Fig.  Ablation Study. We conducted ablation experiments on the M4Raw dataset and the results are shown in Table  Discussion. Our reported results on M4Raw contain instances of slight interscan motion "
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,4.0,Conclusion,"In this study, we propose a Dual Cross-Attention Multi-contrast Super Resolution (DCAMSR) framework for improving the spatial resolution of MRI images. As demonstrated by extensive experiments, the proposed method outperforms existing state-of-the-art techniques under various conditions, proving a powerful and flexible solution that can benefit a wide range of medical applications."
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 30.
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,1.0,Introduction,"Near-infrared (NIR) fluorescence imaging can allow the detection of fluorophores up to 4 cm depth in tissue  A majority of fluorescence imaging applications including Fluorescence Guided Surgery (FGS) rely upon visible 2D surface imaging  The prime motivation of our work is to enable an efficient 3D tumor shape reconstruction for FGS in an operating room environment, where we do not have full control of the ambient light and we cannot rely on sophisticated time or frequency domain imaging instrumentation and setup. In these situations, one has to use clinical cameras producing rapid Continuous Wave (CW) fluorescence boundary measurements  We propose an Incremental Fluorescent Target Reconstruction (IFTR) scheme, based on the recent advances in quadratic and conic convex optimization and sparse regularization, which can recover a relatively large 3D target in tissuelike media. In our experiments, IFTR scheme demonstrates accurate reconstruction of 3D targets from reflectance mode CW-measurements collected at the top surface of the domain. To our best knowledge, this is the first report where the 3D shape of tumor-like target has been recovered from reflectance mode steady-state CW measurements. Previously such results were reported in FDOT literature only for time-consuming frequency-domain or time-domain measurements "
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,2.0,Methods,"Figure  In this section we briefly describe the mathematical formulation of the FDOT problem and introduce the IFTR scheme for solving it. Forward and Inverse Problems. Photon propagation in tissue-like media is described by a coupled system of elliptic Partial Differential Equations (PDEs) for determining photon fluence φ (W/cm 2 ) at excitation and fluorescence emission wavelengths through out the domain. Wavelength and space dependent absorption and scattering coefficients and fluorophore properties comprise the coefficients of this PDE system (see Appendix A). The discretization of coupled diffusion PDEs is obtained by applying a standard FEM methodology  where the first equation describes the excitation photon fluence φ x ∈ R N , and the second describes photon emission fluence φ m ∈ R N ; subscripts x and m indicate excitation and emission respectively. Vectors f , χ ∈ R N are the source of excitation light and target's shape indicator, i.e., a binary vector such that χ i = 1 if x i belongs to the target and 0 otherwise. S x/m (•) ∈ R N ×N are the stiffness matrices obtained by discretizing the diffusion terms of excitation/emission PDEs respectively and additionally S x depends on χ. M ∈ R N ×N is the mass matrix and denotes Hadamard (elementwise) product such that M χ = M diag(χ) and M χφ x = M φ x χ. Finally, vector of measurements y ∈ R K is related to the emission fluence φ m as follows Here T ∈ R K×N is a binary matrix that selects components of φ m corresponding to the observed grid nodes and K is a number of observed nodes. In the following if target indicator χ is given then the system (1) is referred to as the forward FDOT problem to compute unknown excitation and emission fluence φ x , φ m . If vector χ is unknown but measurements of emission fluence are present then the system (1)-( "
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,,Search Space Regularization.,"In what follows we propose an algorithm that estimates target's indicator χ from data y, i.e. solves the inverse FDOT problem. To reduce the ill-posedness of the inverse problem (  The first regularization represents an assumption that the correct χ is a binary vector. Since binary constraints are not convex, we adopt a more relaxed condition on χ referred to as the box constraints: 0 ≤ χ ≤ 1. The second regularization describes the piece-wise constant structure of the indicator χ, and is referred to as the piece-wise total variation (PTV). It is obtained by extending the notion of total variation which has been successfully applied in optical tomography. To this end, assume m(j), n(j), and j ∈ I are indices corresponding to the j-th pair of neighboring nodes. Let the domain Ω be split into N ptv non-overlapping subdomains, e.g., cuboids, and the index I is correspondingly split into non-overlapping sub-indices I i , i = 1, . . . , N ptv of nodes pairs that belong to Ω i . PTV is obtained as a sum of total variations computed using sub-indices I i : and is also written in a matrix form assuming matrix V encodes subtraction across node pairs across all sub-indices. The third regularization aims to reduce a null space of the inverse problem in the boundary layer of a thickness , reflecting the assumption that the target is under the surface. It is referred to as the boundary regularization and is defined as W χ = 0 where W selects components of χ that belong to the boundary layer. Finally, the fourth regularization referred to as the minimum volume regularization requires that χ has at least m 0 non-zero components: Optimization Framework. In this subsection we present an Incremental Fluorescent Target Reconstruction (IFTR) scheme solving the inverse problem (1)-  x as the unique solution of linear excitation equation: then (ii) fix the obtained φ n x and compute χ n+1 as the unique solution of one of the 3 convex optimization problems:"
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,,Variant I. This variant relies upon direct inversion of the emission equation matrix S -1,"m to find χ n+1 : Variant II. This variant imposes the emission equation as an inequality constraint: Here, depending on the value of p we take x 1 = x 2 or x 2 = 1 T x and E m is a parameter defining emission equation constraint tolerance. Variant III. This variant uses the emission equation as a term of the loss function: We note that all the three variants depend on parameter p = 1, 2 which defines the type of optimization problem that should be solved: i) if p = 1 we get conic optimization problems of the loss function in the form • 2 which would be treated as conic constraints); ii) if p = 2 we get quadratic optimization problems. To get a good initial guess for χ 0 we borrow from the Born approximation which suggests that excitation field φ x can be approximated by the background excitation obtained by solving excitation equation with no ICG, i.e., χ 0 = 0. Iterating this splitting method for n = 0, 1, 2, . . . we obtain a sequence of updates χ n that converge into a vicinity of the true χ provided data is ""representative enough"". We conclude the presentation of IFTR scheme with stopping criteria of the iterative process. For this we use a standard Dice coefficient d(•, •) and a binary projector b(•): the scheme stops once the following condition is met"
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,3.0,Data Collection,"To validate the IFTR scheme, we performed an experiment capturing the essential elements of FGS applications. Figure  Figure  We collected 3 sets of experimental measurements (see Fig. "
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,4.0,Experimental Validation,"Performance of the proposed IFTR scheme is characterized by a set of numerical experiments. IFTR scheme was implemented using the FEniCS package for FEM matrices computation and CVXPY for the construction of the loss functions and constraints. Additionally CVXPY provides a common interface to various state-of-the-art optimization solvers making it very easy to switch between them. Although we tested IFTR with 4 commonly used solvers: OSQP, SCS and ECOS (distributed together with CVXPY) and MOSEK (required an additional installation) we report results for the solvers that performed the best. Thus, for the quadratic optimization formulation we selected OSQP and for the conic optimization formulation we selected MOSEK. The resulting configurations were compared in terms of Dice coefficient d(χ est , χ true ) comparing estimated target and the true target as well as execution time. The results of the performed experiments are summarised in Fig.  The third experiment demonstrates the consistency of IFTR scheme: adding side measurements allows all variants to obtain good reconstructions and further increases Dice coefficients. This, however, comes at a price of increased computational demands, particularly for variant I solved with quadratic OSQP solver. The performed experiments reveal that it is difficult to pick a single winning configuration of IFTR scheme but there are several considerations: i) variant I provides the lowest errors but is the slowest variant with MOSEK solver has been consistently faster than OSQP; ii) variant II is the fastest variant but it is more sensitive to the amount of measurement compared to others; iii) variant III is less sensitive to the amount of measurements compared to Variant II has similar execution time but is less accurate. We also note that IFTR scheme is robust with respect to PTV regularization parameter. This was achieved by scaling the data misfit and PTV term to similar magnitude: we normalised the misfit term by the norm of the observations vector and rescaled PTV term by the number of subdomains and each local total variation weight by the number of nodes in that subdomain. The robustness to regularization parameter choice was confirmed by our experiments with several different values of such parameter. Another relevant consideration is that PTV impacts the loss function in a different way compared to a standard L1 or L2 regularization: the latter has the unique global minimizer (0-vector) while the former has many global minimizers and IFTR benefits from this."
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,5.0,Conclusions,"In this work we proposed novel IFTR scheme for solving FDOT problem. It performs a splitting of the bi-linearity of the original non-convex problem into a sequence of convex ones. Additionally, IFTR restricts the search space by a set of regularizers promoting piece-wise constant structure of target's indicator function which in turn allows to recover fluorescent targets from only the reflectance mode CW measurements collected by a consumer grade camera. Although the scheme was tested using proof-of-concept experimental data and cubical shape target the method is general and depending on mesh discretization level, scalable to arbitrary domain and target shapes. Thus, the obtained results suggest strong potential for adoption of IFTR scheme in FGS related applications."
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,,A Continuous formulation of the FDOT problem,"Near-infrared photon propagation in tissue like media is described by the following coupled system of PDEs: Here (  3 μ ax/mi + μ ax/mf + μ sx/m , k x/m = μ ax/mi + μ ax/mf  The system (  where γ = 2.5156 -dimensionless constant depending on the optical reflective index mismatch at the boundary."
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 49.
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,1.0,Introduction,"Compared with 2D MRI, 3D MRI has superior volumetric spatial resolution and signal-to-noise ratio. However, 3D MRI, especially high resolution (HR) 3D MRI (e.g., at least 1 mm 3 voxel size), often takes much longer acquisition time than 2D scans. Therefore, it is necessary to accelerate 3D MRI by acquiring sub-sampled k-space. However, it is more challenging to reconstruct HR 3D MRI images than 2D images. For example, HR 3D MRI data can be as large as 380×294×138×64, which is more than 100X larger than common 2D MRI data  The previous works on 3D MRI reconstruction have several limitations. First, all these methods are based on CNN. In the context of 3D reconstruction, deep CNN networks require significant GPU memory and are difficult to scale. As a result, many models are designed to be relatively small to fit within available resources  To tackle these problems, we proposed Recon3DMLP for 3D MRI reconstruction, a hybrid of CNN modules with small kernels for low-frequency reconstruction and adaptive MLP (dMLP) modules with large kernels to boost the high-frequency reconstruction. The dMLP improves the model fitting ability with almost the same GPU memory usage and a minor increase in computation time. We utilized the circular shift operation "
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,2.1,Recon3DMLP for 3D MRI Reconstruction,"The MRI reconstruction problem can be solved as where y is the acquired measurements, x u is the under-sampled image, M and S are the sampling mask and coil sensitivities, F denotes FFT and λ is a weighting scalar. g θ is a neural network with the data fidelity (DF) module  The proposed Recon3DMLP adapts the memory-friendly cascaded structure. Previous work has shown that convolutions with small kernels are essential for low-level tasks "
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,2.2,Adaptive MLP for Flexible Image Resolution,The dMLP module includes the following operations (Fig. 
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,2.3,Memory Efficient Data Fidelity Module,"In the naive implementation of the DF module the coil combined image z is broadcasted to multi-coil data (I -M )F Sz and it increases memory consumption. Instead, we can process the data coil-by-coil where c is the coil index. Together with eDF, we also employed RO cropping and gradient checkpointing for training and half-precision for inference."
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,2.4,Experiments,"We collected a multi-contrast HR 3D brain MRI dataset with IRB approval, ranging from 224×220×96×12 to 336×336×192×32  We started with a small 3D CNN model (Recon3DCNN) with an expansion factor e = 6, where the channels increase from 2 to 12 in the first convolution layer and reduce to 2 in the last layer in each cascade. We then enlarged Recon3DCNN with increased width (e = 6,12,16,24) and depth (double convolution layers in each cascade). We also replaced the 3D convolution in Recon3DCNN with depth separable convolution "
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,3.0,Results,"We first demonstrate the benefit of eDF and FP16 inference with a small CNN model Recon3DCNN (e = 6) (first and second panels in Table  The performance of Recon3DCNN improves when becoming larger (i.e., more parameters), which indicates CNN models lack fitting power for HR 3D MR reconstruction. Therefore, we performed an overfitting experiment where models were trained and tested on one data. Figure  To investigate the source of the gain, we perform ablation studies on Recon3DMLP (last panel in Table "
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,4.0,Discussion and Conclusion,"Although MLP has been proposed for vision tasks on natural images as well as 2D MRI reconstruction with fixed input size, we are the first to present a practical solution utilizing the proposed dMLP and eDF to overcome the computational constraint for HR 3D MRI reconstruction with various sizes. Compared with CNN based models, Recon3DMLP improves image quality with a little increase in computation time and similar GPU memory usage. One limitation of our work is using the same shift and patch size without utilizing the multi-scale information. dMLP module that utilizes various patch and shift sizes will be investigated in future work. MLP-based models such as Recon3DMLP may fail if the training data is small."
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 19.
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,1.0,Introduction,"Multimodal imaging has become increasingly popular in healthcare due to its ability to provide complementary anatomical and functional information. However, to fully exploit its benefits, it is crucial to perform accurate and robust registration of images acquired from different modalities. Multimodal image registration is a challenging task due to differences in image appearance, acquisition protocols, and physical properties of the modalities. This holds in particular if ultrasound (US) is involved, and has not been satisfactorily solved so far. While simple similarity measures directly based on the images' intensities such as sum of absolute (L1) or squared (L2) differences and normalized crosscorrelation (NCC)  As an alternative to directly assessing similarity on the original images, various groups have proposed to first compute intermediate representations, and then align these with conventional L1 or L2 metrics  More recently, multimodal registration has been approached using various Machine Learning (ML) techniques. Some of these methods involve the utilization of Convolutional Neural Networks (CNN) to extract segmentation volumes from the source data, transforming the problem into the registration of label maps  In contrast, we propose in this work to use a small CNN to approximate an expensive similarity metric with a straightforward dot product in its feature space. Crucially, our method does not necessitate to evaluate the CNN at every optimizer iteration. This approach combines ML and classical multimodal image registration techniques in a novel way, avoiding the common limitations of ML approaches: ground truth registration is not required, it is differentiable and computationally efficient, and generalizes well across anatomies and imaging modalities."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,2.0,Approach,"We formulate image registration as an optimization problem of a similarity metric s between the moving image M and the fixed image F with respect to the parameters α of a spatial transformation T α : Ω → Ω. Most multi-modal similarity metrics are defined as weighted sums of local similarities computed on patches. Denoting M • T α the deformed image, the optimization target can be expressed in the following way: where w(p) is the weight assigned to the point p, s(•, •) defines a local similarity and the [•] operator extracts a patch (or a pixel) at a given spatial location. This definition encompasses SSD but also other more elaborate metrics like LC 2 or MIND. The function w is typically used to reduce the impact of patches with ambiguous content (e.g. with uniform intensities), or can be chosen to encode prior information on the target application. The core idea of our method is to approximate the similarity metric s(P 1 , P 2 ) of two image patches with a dot product φ(P 1 ), φ(P 2 ) where φ(•) is a function that extracts a feature vector, for instance in R 16 , from its input patch. When φ is a fully convolutional neural network (CNN), we can simply feed it the entire volume in order to pre-compute the feature vectors of every voxel with a single forward pass. The registration objective (Eq. 1) is then approximated as thus converting the original problem into a registration of pre-computed feature maps using a simple and differentiable dot product similarity. This approximation is based on the assumption that the CNN is approximately equivariant to the transformation, i.e. Our experiments show that this assumption (implicitly made also by other descriptors like MIND) does not present any practical impediment. Our method exhibits a large capture range and can converge over a wide range of rotations and deformations. Advantages. In contrast to many existing methods, our approach doesn't require any ground truth registration and can be trained using patches from unregistered pairs of images. This is particularly important for multi-modal deformable registration as ground truths are harder to define, especially on ultrasound. The simplicity of our training objective allows the use of a CNN with a limited number of parameters and a small receptive field. This means that the CNN has a negligible computational cost and can generalize well across anatomies and modalities: a single network can be used for all types of images and does not need to be retrained for a new task. Furthermore, the objective function (Eq. 2) can be easily differentiated without backpropagating the gradient through the CNN. This permits efficient gradient-based optimization, even when the original metric is either non-differentiable or costly to differentiate. Finally, we quantize the feature vectors to 8-bit precision further increasing the computational speed of registration without impacting accuracy."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,3.0,Method,"We train our model to approximate the three-dimensional LC 2 similarity, as it showed good performance on a number of tasks, including ultrasound  Dataset. Our neural network is trained using patches from the ""Gold Atlas -Male Pelvis -Gentle Radiotherapy""  Patch Sampling from Unregistered Datasets. For each pair of volumes (M, F ) we repeat the following procedure 5000 times: (1) Select a patch from M with probability proportional to its weight w; (2) Compute the similarity with all the patches of F ; (3) Uniformly sample t ∈ [0, 1]; (4) Pick the patch of F with similarity score closest to t. Running this procedure on our training data results in a total of 510000 pairs of patches."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,,Architecture and Training.,"We use the same feed-forward 3D CNN to process all data modalities. The proposed model is composed of residual blocks  Augmentation on the training data is used to make the model as robust as possible while leaving the target similarity unchanged. In particular, we apply the same random rotation to both patches, randomly change the sign and apply random linear transformation on the intensity values. We train our model for 35 epochs using the L2 loss and batch size of 256. The training converges to an average patch-wise L2 error of 0.0076 on the training set and 0.0083 on the validation set. The total training time on an NVIDIA RTX4090 GPU is 5 h, and inference on a 256 3 volume takes 70 ms. We make the training code and preprocessed data openly available online"
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.0,Experiments and Results,"We present an evaluation of our approach across tasks involving diverse modalities and anatomies. Notably, the experimental data utilized in our analysis differs significantly from our model's training data in terms of both anatomical structures and combination of modalities. To assess the effectiveness of our method, we compare it against LC 2 , which is the metric we approximate, and MIND-SSC  As will be demonstrated in the next subsections, our method is capable of achieving comparable levels of accuracy as LC 2 while retaining the speed and flexibility of MIND-SSC. In particular, on abdominal US registration (Sect. 4.3) our method obtains a significantly larger capture range, opening new possibilities for tackling this challenging problem."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.1,Affine Registration of Brain US-MR,"In this experiment, we evaluate the performance of different methods for estimating affine registration of the REtroSpective Evaluation of Cerebral Tumors (RESECT) MICCAI challenge dataset "
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.2,Deformable Registration of Abdominal MR-CT,Our second application is the Abdomen MR-CT task of the Learn2Reg challenge 2021 
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.3,Deformable Registration of Abdominal US-CT and US-MR,"As the most challenging experiment, we finally use our method to achieve deformable registration of abdominal 3D freehand US to a CT or MR volume. We are using a heterogeneous dataset of 27 cases, comprising liver cancer patients and healthy volunteers, different ultrasound machines, as well as optical vs. electro-magnetic external tracking, and sub-costal vs. inter-costal scanning of the liver. All 3D ultrasound data sets are accurately calibrated, with overall system errors in the range of commercial ultrasound fusion options. Between 4 and 9 landmark pairs (vessel bifurcations, liver gland borders, gall bladder, kidney) were manually annotated by an expert. In order to measure the capture range, we start the registration from 50 random rigid poses around the ground truth and calculate the Fiducial Registration Error (FRE) after optimization. For local optimization, LC 2 is used in conjunction with BOBYQA  From the results shown in Table  Note that this registration problem is much more challenging than the prior two due to difficult ultrasonic visibility in the abdomen, strong deformations, and ambiguous matches of liver vasculature. Therefore, to the best of our knowledge, these results present a significant leap towards reliable and fully automatic fusion, doing away with cumbersome manual landmark placements."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,5.0,Conclusion,"We have discovered that a complex patch-based similarity metric can be approximated with feature vectors from a CNN with particularly small architecture, using the same model for any modality. The training is unsupervised and merely requires unregistered data. After features are extracted from the volumes, the actual registration comprises a simple iterative dot-product computation, allowing for global and derivative-based optimization. This novel combination of classical image processing and machine learning elevates multi-modal registration to a new level of performance, generality, but also algorithm simplicity. We demonstrate the efficiency of our method on three different use cases with increasing complexity. In the most challenging scenario, it is possible to perform global optimization within seconds of both pose and deformation parameters, without any organ-specific distinction or successive increase of parameter sizes. While we specifically focused on developing an unsupervised and generic method, a sensible extension would be to specialize our method by including global information, such as segmentation maps, into the approximated measure or by making use of ground-truth registration during training. Finally, the cross-modality feature descriptors produced by our model could be exploited by future research for tasks different from registration such as modality synthesis or segmentation."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 72.
Differentiable Beamforming for Ultrasound Autofocusing,1.0,Introduction,"Ultrasound images are reconstructed by time sampling the reflected pressure signals measured by individual transducer elements in order to focus at specific spatial locations. The sample times are calculated so as to compensate for the time-of-flight from the elements to the desired spatial locations, often by assuming a constant speed of sound (SoS) in the medium, e.g., 1540 m m/s. However, the human body is highly heterogeneous, with slower SoS in adipose layers than in fibrous and muscular tissues. If unaccounted for, these differences lead to phase aberration, geometric distortions, and loss of focus and contrast  Historically, phase aberration has been described using simplified phasescreen models  Recent developments in artificial intelligence have been facilitated by the release of open-source tensor libraries, which can perform automatic differentiation of composable transformations on vector data. These libraries are the backbone of complex neural network architectures that use automatic reversemode differentiation (back-propagation) to iteratively optimize weights based on a set of training instances. These libraries also simplify and optimize portability to high-performance computing platforms. We hypothesize that such libraries can likewise be extended to model the pipeline of ultrasound image reconstruction as a composition of differentiable operations, allowing optimization based on a single data instance. In this work, we propose an ultrasound imaging paradigm that jointly achieves sound speed estimation and image quality enhancement via differentiable beamforming. We formulate image reconstruction as a differentiable function of a spatially heterogeneous SoS map, and optimize it based on quality metrics extracted from the final reconstructed images (Fig. "
Differentiable Beamforming for Ultrasound Autofocusing,2.1,Beamforming Multistatic Synthetic Aperture Data,"In ultrasound imaging, radiofrequency data (RF) represents the time series signal proportional to the pressure measured by each probe array sensor. A multistatic synthetic aperture dataset contains the RF pulse-echo responses of every pair of transmit and receive elements. We denote the signal due to the i-th transmit element, and j-th receive element as u ij (t). This signal can be focused to an arbitrary spatial location x k by sampling u ij (t) at the time corresponding to the time-of-flight τ from the transmit element at x i to x k and back to the receive element at x j , achieved via 1D interpolation of the RF signal: ( (We describe our time-of-flight model in greater detail below in Sect. 2.3.) The interpolated signals are then summed across the transmit (N t ) and receive (N r ) apertures to obtain a focused ultrasound image: This process of interpolation and summation is called delay-and-sum (DAS) beamforming."
Differentiable Beamforming for Ultrasound Autofocusing,2.2,Differentiable Beamforming,"DAS is composed of elementary differentiable operations and is consequently itself differentiable. Therefore, DAS can be incorporated into an automatic differentiation (AD) framework to allow for differentiation with respect to any desired input parameters θ. For a given loss function L(u(x k ; θ)) that measures the ""quality"" of the beamforming, θ can be optimized using gradient descent to identify the optimal θ using update steps Δθ: This differentiable framework is flexible, providing many ways to parameterize the beamforming. In this work, we will show the promise of differentiable beamforming on the task of sound speed estimation by optimizing for slowness s in a time of flight delay model (i.e. θ = s)."
Differentiable Beamforming for Ultrasound Autofocusing,2.3,Time of Flight Model,"Here, we parameterize the slowness (i.e. the reciprocal of the sound speed) as a function of space. Specifically, we define the slowness at a set of control points as s = {s(x k )} k , which can be interpolated to obtain the slowness at arbitrary x. The time-of-flight from x 1 to x 2 is the integral of the slowness along the path: For simplicity and direct comparison with previous sound speed estimation models "
Differentiable Beamforming for Ultrasound Autofocusing,2.4,Loss Functions for Sound Speed Optimization,"Speckle Brightness Maximization. Diffuse ultrasound scattering produces an image texture called speckle. Speckle brightness can be used as a criterion of focus quality  Coherence Factor Maximization. Coherence factor  Phase-Error Minimization. The van Cittert Zernike theorem of optics  We estimate the phase shift as the complex angle between DAS signals u a and u b of the respective subapertures (T a , R a ) and (T b , R b ), calculated using (2): The phase shift error (PE) is defined for a set of all aperture pairs (a, b) with common midpoint as Fig. "
Differentiable Beamforming for Ultrasound Autofocusing,3.1,Implementation of Differentiable Beamformer,A differentiable DAS beamformer was implemented in Python using JAX
Differentiable Beamforming for Ultrasound Autofocusing,3.2,Comparison with State-of-the-Art Methods,"As a baseline for performance comparison, the Computed Ultrasound Tomography in Echo Mode (CUTE) method developed by Stähli et al. "
Differentiable Beamforming for Ultrasound Autofocusing,3.3,Datasets,"In-Silico. The CUDA-accelerated binaries of the k-Wave simulation suite  To compare with the baseline  A linear 128 element linear probe was simulated, with a pitch of 0.3 mm and a center frequency of 4.8 MHz with a 100% bandwidth. The simulation domain was 60 × 51 x 7.4 mm 3 . Iso-echoic phantoms were generated whereby the sound speed was modulated relative to the density of a region so the average brightness remained constant while the sound-speed variation introduced phase aberration. In-Vivo. In-vivo data was collected on a Verasonics Vantage research system with a L12-3v linear transducer (192 elements, 0.2 mm pitch, 5 MHz center frequency). Three abdominal liver views, which contained subcutaneous adipose, musculoskeletal tissue and liver parenchyma, were collected from a healthy volunteer under a protocol approved by an institutional review board."
Differentiable Beamforming for Ultrasound Autofocusing,4.0,Results,Figure  The sound speed distributions generated with differentiable beamforming are in general agreement with the ground truth sound speed distributions. Table  Figure 
Differentiable Beamforming for Ultrasound Autofocusing,5.0,Discussion and Conclusion,"Differentiable beamforming can be used to solve for unknown quantities with gradient descent. Here, we parameterized beamforming as a function of the slowness and optimized with respect to several candidate loss functions, showing that phase error was best for heterogeneous targets. The differentiable beamformer simultaneously provided B-mode image correction and quantitative sound speed characterization beyond the state-of-the-art across several challenging cases. Preliminary in-vivo quantitative SoS data for liver was shown, which has direct clinical applications such as in the noninvasive assessment of non-alcoholic fatty liver disease, as well as image enhancement in general. Importantly, the differentiable beamformer allows us to incorporate fundamental physics principles like wave propagation, reducing the number of parameters to optimize. In the future, more complex wave propagation physics, such as refraction models, can be added to SoS optimization. In addition to sound speed, this work can be readily adapted to a broad set of applications such as beamforming with flexible arrays, where element positions are unknown, or passive cavitation mapping, where the origin of the signal is uncertain. Because the gradients flow through the entire imaging pipeline, the differentiable beamformer is also highly compatible with deep learning techniques. For instance, a model can be trained in a self-supervised fashion to identify optimal sound speed updates to accelerate convergence. Differentiable beamforming also enables the end-toend optimization of imaging parameters for downstream tasks in computer-aided medical diagnostics."
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,1.0,Introduction,"Panoramic radiography (panoramic X-ray, or PX) is a commonly used technique for dental examination and diagnosis. While PX produces 2D images from panoramic scanning, Cone-Beam Computed Tomography (CBCT) is an alternative imaging modality which provides 3D information on dental, oral, and maxillofacial structures. Despite providing more comprehensive information than PX, CBCT is more expensive and exposes patients to a greater dose of radiation  Previous 3D teeth reconstruction methods from 2D PX have relied on additional information such as tooth landmarks or tooth crown photographs. For example,  In this paper, we propose Occudent, an end-to-end model to reconstruct 3D teeth from 2D PX images. Occudent consists of a multi-label 2D segmentation followed by 3D teeth reconstruction using neural implicit functions "
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,2.0,Methods,"The proposed model, Occudent, consists of two main components: 2D teeth segmentation and 3D teeth reconstruction. The former performs the segmentation of 32 teeth from PX using UNet++ model "
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,2.1,2D Teeth Segmentation,"The teeth in input PX are segmented into 32 teeth classes. The 32 classes correspond to the traditional numbering of teeth, which includes incisors, canines, premolars and molars in both upper and lower jaws. We pose 2D teeth segmentation as a multi-label segmentation problem  The input of the model is H ×W size PX image. The segmentation output has dimension C × H × W , where channel dimension C = 33 represents the number of tooth classes: one class for the background and 32 classes for teeth similar to "
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,2.2,3D Teeth Reconstruction,"Neural Implicit Representation. Typical representations of 3D shapes are point-based  In practice, o A can be estimated only by a set of observations of object A, denoted by X A . Examples of observations are projected images or point cloud data obtained from the object. Our objective is to estimate the occupancy function conditioned on X A . Specifically, we would like to find function f θ which estimates the occupancy probability of a point in 3D space based on X A  Inspired by the aforementioned framework, we leverage segmented tooth patch and tooth class as observations denoted by condition vector c. Specifically, the input to the function is a set of T randomly sampled locations within a unit cube, and the function outputs the occupancy probability of the input. Thus, the function is given by f The model for f θ is depicted in Fig. "
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,,Class-Specific Conditional Features.,"A distinctive feature of the tooth reconstruction task is that teeth with the same number share properties such as surface and root shapes. Hence, we propose to use tooth class information in combination with a segmented tooth patch from PX. The tooth class is processed by a learnable embedding layer which outputs a class embedding vector. Next, we create a square patch of the tooth using the segmentation output as follows. A binary mask of the segmented tooth is generated by applying thresholding to the segmentation output. A tooth patch is created by cropping out the tooth region from the input PX, i.e., the binary mask is applied (bitwise AND) to the input PX to obtain the patch. The segmented tooth patch is subsequently encoded using a pre-trained ResNet18 model  Our approach differs from previous approaches, such as Occupancy Networks  Conditional eXcitation. To effectively inject 2D observations into the reconstruction network, we propose Conditional eXcitation (CX) inspired by Squeezeand-Excitation Network (SENet)  where c is the condition vector, σ is a gating function, W is a learnable weight matrix, α is a hyperparameter for the excitation result, and F ext is the excitation function. We use sigmoid function for σ, and component-wise multiplication for the excitation, F ext (e, x) = e ⊗ x. The CX module is depicted in Fig. "
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,3.0,Experiments,"Dataset. Implementation Details. For the pre-training of the segmentation model, we utilized a combination of cross-entropy and dice loss. For the main segmentation training, we used only dice loss. The segmentation and reconstruction models were trained separately. Following the completion of the segmentation model training, we fixed this model to predict its output for the reconstruction model. Each 3D tooth label was fit in 144 × 80 × 80 size tensor which was then regarded as [-0.5, 0.5] 3 normalized cube in 3D space. For the training of the neural implicit function, a set of T = 100, 000 points was sampled from the unit cube. The preprocessing was consistent with that used in  Baselines. We considered several state-of-the-art models as baselines, including 3D-R2N2  Quantitative Comparison. Table  Qualitative Comparison. Figure "
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,4.0,Conclusion,"In this paper, we present a framework for 3D teeth reconstruction from a single PX. To the best of our knowledge, our method is the first to utilize a neural implicit function for 3D teeth reconstruction. The performance of our proposed framework is evaluated quantitatively and qualitatively, demonstrating its superiority over state-of-the-art techniques. Importantly, our framework is capable of accommodating two distinct modalities, PX, and CBCT. Our framework has the potential to be valuable in clinical practice and also can support virtual simulation or educational tools. In the future, further improvements can be made, such as incorporating additional imaging modalities or exploring neural architectures for more robust reconstruction."
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,,Acknowledgements,. This work was supported by the 
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 36.
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,1.0,Introduction,"Deformable medical image registration has many applications in clinic, including but not limited to clinical case tracking, surgical navigation. In recent years, many advanced learning-based medical image registration methods  Recently, several joint learning methods have been proposed. These methods can be generally divided into two categories in terms of model structure (see Fig.  In comparison, the one-two models are more in line with the general paradigm of multi-task learning, which can be regarded as a process of inductive bias  Based on the framework of one-two model, we propose a progressively coupling network (PGCNet) as shown in Fig.  The main contributions can be summarized as follows: 1) We propose a novel registration learning framework to establish the entangled relationship between registration and segmentation by progressively coupling the moving and fixed segmentation, which promotes both registration and segmentation performance in few-shot situation. 2) To effectively measure the coordinate correspondence of voxels, we design the position correlation calculation, which provides spatial coordinate information for field estimator while measuring feature similarity. 3) Experimental results on the general brain MRI datasets, OASIS and IXI, show that our method outperforms the state-of-the-art methods."
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,2.0,Method,"Our model aims to learn registration and segmentation tasks simultaneously. Let M , F be moving and fixed images, respectively. We parameterize our model as a function with parameter θ, f θ (M, F ) = φ, S M , S F , where φ is the deformation field, S M and S F represent the segmentation results of M and F . Registration task completes the transformation from moving images to fixed ones, and segmentation task generates the segmentation maps of moving and fixed images."
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,2.1,Overall Network Structure,The overall pipeline of our method is shown in Fig. 
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,2.2,Coupling Decoder,"As shown in Fig.  where x, y and z represent the coordinate indexes of feature map in three directions, respectively. We move feature map point by point along x, y and z directions with radius r and do dot product to generate a correlation map. The dimension of correlation matrix (cost volume) is (2r + 1) 3 , and we set r = 1 in this work. Then, Field and Inv-Field estimation modules utilize three residual blocks to compute the increment of forward and inverse displacement fields according to the correlation and inverse correlation matrices, respectively. The parameters of the two modules are shared."
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,2.3,Loss Function,"The loss functions of our framework comprise three parts, which regularize registration, segmentation, and joint optimization, respectively. The registration loss has similarity and smoothness penalty terms to align the moving and fixed images and ensure the smoothness of the deformation field. We use local normalized cross-correlation  where λ 1 is a balance hyperparameter and is set as 1 in the experiments. For segmentation, we use the combination of weighted cross entropy and Dice loss, which is formulated as: where N represents the total number of semantic classes, n indicates the channel of the corresponding category, and P is the number of voxels in each channel. S is the segmentation result and S * is the voxel-wise manual segmentation label. In Eq. 3, w n indicates the inverse proportion of voxels in category n to all voxels. For these two segmentation loss functions, their weights are fixed to the same value which means L seg = L wce + L dice . The joint loss function is defined as L dice (S F , S M • φ). It regularizes registration network to provide additional training data for segmentation task. Meanwhile, registration network can pay more attention to the ROI regions."
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,3.1,Experimental Settings,"Datasets and Preprocessing. We validate our method by using 414 T1weighted brain MRI scans from OASIS  1 IXI dataset is available in https://brain-development.org/ixi-dataset/. The subcortical structures in OASIS and IXI are labeled as 35 and 44 categories by FreeSurfer for evaluation. We randomly select 5 scans as atlas from OASIS dataset, then the remaining scans are randomly divided into 255, 22, 132 for training, validation and testing. We do not use the labels of the training scans, only 5 labels of atlas are used to simulate a few-shot scenario. A total of 1,275, 110, 660 pairs of scans are used for training, validation and testing. Similarly, the IXI dataset is randomly split into 397, 58, 115, and 5 labeled scans are randomly taken as atlas for few-shot situation. The training, validation and testing include 1,985, 290, 575 pairs of scans, respectively. Implementation. All trainings use Adam optimizer with learning rate 1e -4 and the batch size is set as 1. We first train the network by optimizing L reg + L seg for 5,000 iterations and then by optimizing L reg + L seg + L joint for 80,000 iterations. All experiments are performed on 1 Nvidia RTX 3090 GPU. Baseline Methods. We compare our method with a series of registration models. SYN "
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,3.2,Experimental Results,"Registration Performance. As shown in Table  Ablation Study. We conduct a series of ablation experiments on OASIS dataset to verify the effectiveness of each module, as shown in Table "
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,,OASIS IXI,"Joint training (JTrain) introduces semi-supervised loss for registration. And the coupling feature connection (Couple) improves the performance of registration by guiding the deformation field in advance to map semantic features. Position embedding (PEmbedding) assists the field estimator in understanding the correlation between voxels. In addition, we further analyse the factors that affect the performance of segmentation (see Table "
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,4.0,Conclusion,"We propose a progressively coupling network (PGCNet), which employs the deformation fields to couple the registration and segmentation branches. This is a novel mode of regularization for registration and segmentation, which promotes their performance in few-shot situation. In addition, position embedding provides additional spatial coordinate information for registration branch. The experimental results indicate that our work is superior to existing methods."
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,1.0,Introduction,"Statistical shape models (SSMs) are a powerful tool to characterize anatomical variations across a population. They have been widely used in medical image analysis and computational anatomy to represent organ structures, with numerous clinically relevant applications such as clustering, classification, and shape regression  Creating SSMs is cumbersome and intricate, as significant manual human annotation is necessary. Domain experts typically first segment images in 3D. The labeled 3D organ surfaces must then be aligned and brought into correspondence, typically achieved through deformable image registration methods using manual landmark annotations  Unsupervised methods have been proposed to estimate correspondences for SSMs  To address these challenges, we propose S3M, which leverages unsupervised deep geometric features while incorporating a global shape structure. Geometric Deep Learning (GDL) provides techniques to process 3D shapes and geometries, which are robust to noise, 3D rotations, and global deformations. We utilize graph neural networks (GNN) and functional mappings to establish dense surface correspondences of samples without supervision. This approach has significant clinical implications as it enables automatically representing anatomical shapes across large patient populations without requiring manual expert landmark annotations. We demonstrate that our proposed method creates objectively superior SSMs from shapes with noisy surface topologies. Moreover, it accurately corresponds regions of complex anatomies with mesh bifurcations such as the heart, which could ease the modeling of inter-organ relations  Our contributions can be summarized as follows: -We propose a novel unsupervised correspondence method for SSM curation based on geometric deep learning and functional correspondence. -S3M exhibits superior performance on two challenging anatomies: thyroid and heart. It generates objectively more suitable SSMs from noisy thyroid labels and challenging multi-chamber heart reconstructions. -To pave the way for unsupervised SSM generation in other medical domains, we open-source the code of our pipeline."
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,2.0,Related Work,"Point Distribution Models. A population of shapes must be brought into correspondence to construct a PDM. This has been traditionally achieved through pair-wise registration methods  Graph Neural Networks (GNNs) have been used to enable structural feature extraction through message passing. They constitute a powerful tool to process 3D data and extract geometric features  Functional Correspondence. Functional maps abstract the notion of pointto-point correspondences by corresponding arbitrary functions, such as texture or curvature, across shapes. They are extensively used to estimate dense correspondences across deformable shapes  They employ handcrafted features to extract representations from shapes with a relatively smooth surface topology; however, they fail for shapes with high degrees of surface noise or label inconsistencies. To scale SSM curation to larger datasets encompassing population variance, our method must be robust to a more variable and complex surface topology."
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,3.0,Method,"In the following, we propose a method to establish an SSM as a Point Distribution Model (PDM), illustrated in Fig.  Geometric Feature Description. Handcrafted descriptors "
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,Deformable Correspondence Estimation.,"PDMs require correspondences between samples to model the statistical distribution of the organ. Inspired by methods for geometric shape correspondence, we propose to estimate a functional mapping T to correspond high-level semantics from two input shapes, X i and X j . The LBO extends the Laplace operator to Riemannian manifolds, capturing intrinsic characteristics of the shape independent of its position and orientation in Euclidean space. It can be efficiently computed on a surface mesh using, for example, the cotangent weight discretization scheme  Here, A T φ (Di) ∈ R m×m denotes the transformed descriptor, written in the basis of the LBO eigenfunctions of shape X i and C ij ∈ R m×m represents the optimal functional mapping from the descriptor space of X i to the one of X j . Inspired by existing works on shape correspondence "
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,Training and Inference.,"During training, two shapes are sampled from the dataset, and the pipeline is optimized with Eq. 1. We increase model robustness by augmenting with rotations and small surface deformations. The point cloud is sub-sampled in each training iteration using farthest point sampling with random initialization. During inference, our model predicts pairwise correspondences. To accumulate these over an entire dataset of N shapes, we choose a template shape X T = arg min Xi N j=0 1 i =j L (C ij , C ji ) as the instance with the lowest average loss to all other shapes in the dataset. As in  Statistical Shape Modeling. We use the PDM  then s ∼ N X, S , which is the desired distribution of the model. For the above, the points must be in correspondence across the samples. We thus use the correspondences generated in Sect. 3 to construct the PDM."
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,4.0,Experiments,All experiments are carried out using two publicly available datasets: thyroid ultrasound scans and heart MRI acquisitions. Our model is implemented in PyTorch 1.12 using CUDA 11.6. Training takes between 2.5-3 h on an Nvidia A40 GPU and inference about 0.71 s for a pair of shapes. We use publically available implementations for all baseline methods. Thyroid Dataset (SegThy)  Heart Dataset 
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,5.0,Results and Discussion,"Experiment 1: Thyroid SSM. Table  Our proposed method significantly outperformed Shapeworks in all metrics except the specificity of pseudo-labeled thyroids, where the results are not statistically significant. This was despite the advantage of optimizing correspondences across all shapes in training and validation. S3M can better cope with topological noise and generalizes to unseen samples, demonstrating potential in scaling SSM generation to larger datasets. Furthermore, it does not suffer from increasing memory requirements with the number of samples.   From the qualitative results in Fig. "
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,Experiment 3: Thyroid Pseudo-label Generalization,"To further highlight the proposed methods' robustness to network-generated segmentation labels, we additionally measure the reconstruction ability of SSMs created from pseudo-labels on manually annotated thyroid labels under the Chamfer distance (Ours: 1.05 ± 0.10 mm, Shapeworks: 1.84 ± 0.40 mm). Notably, the proposed PDM on pseudo-labels generalizes better than the SSM built on few manual labels (1.25 ± 0.11 mm; see Table "
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,6.0,Conclusion,"We present an unsupervised approach for learning correspondences between shapes that exhibit noisy and irregular surface topologies. Our method leverages the strengths of geometric feature extractors to learn the intricacies of organ surfaces, as well as high-level functional bases of the Laplace-Beltrami operator to capture more extensive organ semantics. S3M outperforms existing methods on both manual labels, and label predictions from a network, demonstrating the potential to scale existing SSM pipelines to datasets that encompass more substantial population variance without additional annotation burden. Finally, we show that our model has the potential to learn correspondences between complex multi-organ shape hierarchies such as chambers of the heart, which would ease the manual burden of SSM curation for structures that currently still require meticulous manual landmark annotations."
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 44.
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,1.0,Introduction,"Magnetic Resonance Imaging (MRI) is one of the most widely used imaging modalities due to its excellent soft tissue contrast, but it has prolonged and costly scan sessions. Therefore, accelerated MRI methods are needed to improve its clinical utilization. Acceleration through undersampled acquisitions of a subset of kspace samples (i.e., Fourier domain coefficients) results in aliasing artifacts  A recently emergent framework for learning data distributions in computer vision is based on diffusion models  To overcome mentioned limitations, we propose a novel self-supervised accelerated MRI reconstruction method, called SSDiffRecon. SSDiffRecon leverages a conditional diffusion model that interleaves linear-complexity cross-attention transformer blocks for denoising with data-consistency projections for fidelity to physical constraints. It further adopts self-supervised learning by prediction of masked-out k-space samples in undersampled acquisitions. SSDiffRecon achieves on par performance with supervised baselines while outperforming selfsupervised baselines in terms of inference speed and image fidelity."
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,2.1,Accelerated MRI Reconstruction,"Acceleration in MRI is achieved via undersampling the acquisitions in the Fourier domain as follows where F p is the partial Fourier operator, C denotes coil sensitivity maps, I is the MR image and y p is partially acquired k-space data. Reconstruction of fully sampled target MR image I from y p is an ill-posed problem since the number of unknowns are higher than the number of equations. Supervised deep learning methods try to solve this ill-posed problem using prior knowledge gathered in the offline training sessions as follows where I is the reconstruction, and λ(I) is the prior knowledge-guided regularization term. In supervised reconstruction frameworks, prior knowledge is induced from underlying mapping between under-and fully sampled acquisitions. "
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,2.2,Denoising Diffusion Models,"In diffusion models  where β t refers to the fixed variance schedule. After a sufficient number of forward diffusion steps (T ), x t follows a Gaussian distribution. Then, the backward diffusion process is deployed to gradually denoise x T to get x 0 using a deep neural network as a denoiser as follows where σ 2 t = βt = 1-ᾱt-1 1-ᾱt β t and θ represents the denoising neural network parametrized during backward diffusion and trained using the following loss  where ᾱt = t m=1 α m , α t = 1 -β t and ∼ N (0, I)."
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,3.0,SSDiffRecon,"In SSDiffRecon, we utilize a conditional diffusion probabilistic model to reconstruct fully-sampled MR images given undersampled acquisitions as input. The reverse diffusion steps are parametrized using an unrolled transformer architecture as shown in Fig. "
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,,Self-Supervised Training:,"For self-supervised learning, we adopt a k-space masking strategy for diffusion models  where • 1 denotes the L1-norm, F denotes 2D Fourier Transform, C are coil sensitivities, x us is the image derived from undersampled acquisitions, and M l is the random sub-mask within the main undersampling mask M. Here x t recon is the output of the unrolled denoiser network (R θ ) at time instant t ∈ {T, T -1, ..., 0} where M p is the sub-mask of the remaining points in M after excluding M l . Inference: To speed up image sampling, inference starts with zero-filled Fourier reconstruction of the undersampled acquisitions as opposed to a pure noise sample. Conditional diffusion sampling is then performed with the trained diffusion model that iterates through cross-attention transformers for denoising and data-consistency projections. For gradual denoising, we introduce a descending random noise onto the undersampled data within data-consistency layers. Accordingly, the reverse diffusion step at time-index t is given as where low ∼ N (0, 0.1I) and z ∼ N (0, I). Unrolled Denoising Network R θ (.): SSDiffRecon deploys an unrolled physics-guided denoiser in the diffusion process instead of UNET as is used in  where β u,v j ∈ R 3×3 is the convolution kernel for the u th input channel and the v th output channel, and m is the channel index. Then, the output of modulated convolution goes into the cross-attention transformer where the attention map att t j is calculated using local latent variables w t l at time index t as follows where Q j (.), K j (.), V j (.) are queries, keys and values, respectively where each function represents a dense layer with input inside the parenthesis, and P.E. is the positional encoding. Then, x t output,j is normalized to zero-mean unit variance and scaled with a learned projection of the attention maps att t j as follows x t output,j = α j (att j ) where α j (.) is the learned scale parameter. After repeating the sequence of crossattention layer twice, lastly the data-consistency is performed. To perform dataconsistency the number of channels in x t output,j is decreased to 2 with an additional convolution layer. Then, 2-channel images are converted, where channels represent real and imaginary components, to complex and data-consistency is applied as follows where F -1 represents the inverse 2D Fourier transform and x t us = x us during training. Then, using another extra convolution, the number of feature maps are increased to n again for the next denoising block. Implementation Details : Adam optimizer is used for self-supervised training with β = (0.9, 0.999) and learning rate 0.002. Default noise schedule paramaters are taken from "
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,4.1,Datasets,Experiments are performed using the following multi-coil and single-coil brain MRI datasets: 1. fastMRI: Reconstruction performance illustrated in multi-coil brain MRI dataset  Acquisitions are retrospectively undersampled using variable-density masks. Undersampling masks are generated based on a 2D Gaussian distribution with variance adjusted to obtain acceleration rates of R = 
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,4.2,Competing Methods,We compare the performance of SSDiffRecon with the following supervised and self-supervised baselines: 1. DDPM: Supervised diffusion-based reconstruction baseline. DDPM is trained with fully sampled MR images and follows a novel k-space sampling approach during inference introduced by Peng et al. 
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,4.3,Experiments,"We compared the reconstruction performance using Peak-Signal-to-Noise-Ratio (PSNR, dB) and Structural-Similarity-Index (SSIM, %) between reconstructions and the ground truth images. Hyperparameter selection for each method is performed via cross-validation to maximize PSNR. Ablation Experiments. We perform the following four ablation experiments to show the relative effect of each component in the model on the reconstruction quality as well as the effect of self-supervision in Table  1. Supervised: Supervised training of SSDiffRecon using paired under-and fully sampled MR images and pixel-wise loss is performed. Other than training, inference sampling procedures are the same as the SSDiffRecon. 2. UNET: Original UNET architecture in DDPM "
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,5.0,Results,The results are presented in two clusters using a single figure and table for each dataset; fastMRI results are presented in Fig. 
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,6.0,Conclusion,We proposed a novel diffusion-based unrolled architecture for accelerated MRI reconstruction. Our model performs better than self-supervised baselines in a relatively short inference time while performing on-par with the supervised reconstruction methods. Inference time and model complexity analyses are presented in the supplementary materials.
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 47.
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,1.0,Introduction,"Deformable image registration is the process of accurately estimating non-rigid voxel correspondences, such as the deformation field, between the same anatomical structure of a moving and fixed image pair. Fast, accurate, and realistic image registration algorithms are essential to improving the efficiency and accuracy of clinical practices. By observing dynamic changes, such as lesions, physicians can more comprehensively design treatment plans for patients  Classical registration methods  Recently, Kim et al.  To address these issues, we present two novel modules, namely Featurewise Diffusion-Guided Module (FDG) and Score-wise Diffusion-Guided Module (SDG) in the registration network. FDG introduces a direct feature-wise diffusion guidance technique for generating deformation fields by utilizing crossattention to integrate the intermediate features of the diffusion model into the hidden layer of the registration network's decoder. Furthermore, we embed the feature-wise guidance into multiple layers of the registration network and produce the feature-level deformation fields in multiple scales. Finally, after obtaining deformation fields at multiple scales, we upsample and average them to generate the full-resolution deformation field for registration. Our SDG introduces explicit score-wise diffusion guidance for deformation topology preservation by reweighing the similarity-based unsupervised registration loss based on the diffusion score. Through this reweighing scheme, direct attention is given during the optimization process to ensure the preservation of the deformation topology. Our main contribution can be summarized as follows: -We propose a novel feature-wise diffusion-guided module (FDG), which utilizes multi-scale intermediate features from the diffusion model to effectively guide the registration network in generating deformation fields. -We also propose a score-wise diffusion-guided module (SDG), which leverages the diffusion model's score function to guide deformation topology preservation during the optimization process without incurring any additional computational burden. -Experimental results on the cardiac dataset validated the effectiveness of our proposed method."
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,2.1,Baseline Registration Model,"Figure  where where 0 < β s < 1 is the variance of the noise, t is the noise level. Then we perform the registration training task. Given an input x in consisting of a fixed reference image f , a moving unaligned image m, and the perturbed noisy image x t , we feed this input x in = {f, m, x t } into the registration network's shared encoder   .., N from the i-th layer of the decoder. Of note, we generate the registration decoder's feature map by incorporating the guidance from the diffusion decoder, which can be formulated as Eq. 2: where r i is the i-th layer of the registration decoder, and F i E is the skip connection of features from the shared encoder layer at the same depth. After obtaining the feature map pairs, our FDG module estimates the i-th feature level deformation field φ i from the feature map pair (F i G , F i R ) using linear cross attention  After obtaining all feature-level deformation fields from the shallowest layer to the deepest layer, we generate the final deformation field φ by enlarging and averaging all feature-level deformation fields. This is a commonly adopted method for multi-scale deformation field merging so as to merge features which attend different scales and granularity. The final φ is then fed into the spatial transformation layer with the moving image m to generate the registered image m(φ)."
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,2.3,Score-Wise Diffusion-Guided Module,"Given the representation z encoded by the shared encoder z = E β (x in ), the diffusion decoder G σ outputs a diffusion score estimation S = G σ (z). Then, the Score-wise Diffusion-Guided Module (SDG) uses this score to reweigh the similarity-based normalized cross-correlation loss function, formulated as Eq. 4: where m(φ) is the warped moving image, defines the Hadamard product, and ⊗ defines the local normalized cross-correlation function. γ is a hyperparameter to amplify the reweighing effect. By this means, SDG utilizes the diffusion score to explicitly indicate the hardto-register areas, i.e., areas where deformation topology is hard to preserve, then assigning higher weights in the loss function for greater attention, and vice versa for easier-to-register areas. Therefore, the information on deformation topology is effectively incorporated into the optimization process without additional constraints by the SDG module."
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,2.4,Overall Training and Inference,"Loss Function. Our network predicts the deformable fields at the feature level and then outputs the registered image. The total loss function of our method is defined as Eq. 5: ,where ∼ N (0, I ) where L dif f usion is the auxiliary loss function for training the diffusion decoder G σ (Eq. 6), and t is the noise level of x t , following the method in  ||∇ φ || 2 is the conventional smoothness penalty on the deformation field. λ and λ φ are hyperparameters, and we empirically set them to 20 in our experiments. Inference. In the inference stage, we perform image registration in the same style as "
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,3.0,Experiments and Results,"Dataset and Preprocessing. Following the previous work  Evaluation Metrics. We employed three evaluation metrics, i.e., DICE, |J| ≤ 0(%), and SD(|J|) to measure the image registration performance, following existing registration methods  Compare with the State-of-the-Art Methods. Table  Table  Optimization guided by diffusion score led to better preservation of deformation topology. It is worth noticing that the results without FDG or SDG showed only marginal improvement over baseline results, indicating the importance of feature-level deformation field generation and the reweighing scheme. The ablative study of hyperparameter λ is illustrated in Supp. Fig. "
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,4.0,Conclusion,"This work proposes two novel modules for unsupervised deformable image registration: the Feature-wise Diffusion-Guided module (FDG) and the Score-wise Diffusion-Guided module (SDG). Among these modules, FDG can effectively guide the deformation field generation by utilizing the multi-scale intermediate diffusion features. SDG demonstrates its ability to guide the optimization process for better deformation topology preservation using the diffusion score. Extensive experiments show that the proposed framework brings impressive improvements over all baselines. The proposed work models the non-linear deformation semantics using the diffusion model. Therefore, it is sound to generalize to other registration tasks and images, which may be one of the future research directions."
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 62.
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,1.0,Introduction,"Computed Tomography (CT) is one of the most widely used technologies in medical imaging, which can assist doctors for diagnosing the lesions in human internal organs. Due to harmful radiation exposure of standard-dose CT, the low dose CT is more preferable in clinical application  Solving the inverse problem of (  where • p denotes the p-norm and R(x) denotes the penalty item from some prior knowledge. In the past years, a number of methods have been proposed for designing the regularization R. The traditional model-based algorithms, e.g., the ones using total variation  A major drawback of the aforementioned reconstruction methods is that they deal with the input CT 2D slices independently (note that the goal of CT reconstruction is to build the 3D model of the organ). Namely, the neighborhood correlations among the 2D slices are often ignored, which may affect the reconstruction performance in practice. In the field of computer vision, ""optical flow"" is a common technique for tracking the motion of object between consecutive frames, which has been applied to many different tasks like video generation  In this paper, we propose a novel optical flow based generative adversarial network for 3D CT reconstruction. Our intuition is as follows. When a patient is located in a CT equipment, a set of consecutive cross-sectional images are generated. If the vertical axial sampling space of transverse planes is small, the corresponding CT slices should be highly similar. So we apply optical flow, though there exist several technical issues waiting to solve for the design and implementation, to capture the local coherence of adjacent CT images for reducing the artifacts in low-dose CT reconstruction. Our contributions are summarized below: 1. We introduce the ""local coherence"" by characterizing the correlation of consecutive CT images, which plays a key role for suppressing the artifacts. 2. Together with the local coherence, our proposed generative adversarial networks (GANs) can yield significant improvement for texture quality and stability of the reconstructed images. 3. To illustrate the efficiency of our proposed approach, we conduct rigorous experiments on several real clinical datasets; the experimental results reveal the advantages of our approach over several state-of-the-art CT reconstruction methods."
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,2.0,Preliminaries,"In this section, we briefly review the framework of the ordinary generative adversarial network, and also introduce the local coherence of CT slices."
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,,Generative Adversarial Network.,"Traditional generative adversarial network  Local Coherence. As mentioned in Sect. 1, optical flow can capture the temporal coherence of object movements, which plays a crucial role in many videorelated tasks. More specifically, the optical flow refers to the instantaneous velocity of pixels of moving objects on consecutive frames over a short period of time  Based on these assumptions, the brightness of optical flow can be described by the following equation: where v = (v w , v h ) represents the optical flow of the position (w, h) in the image. ∇I = (∇I w , ∇I h ) denotes spatial gradients of image brightness, and ∇I t denotes the temporal partial derivative of the corresponding region. Following the Eq. ( "
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,3.0,GANs with Local Coherence,"In this section, we introduce our low-dose CT image generation framework with local coherence in detail. The Framework of Our Network. The proposed framework comprises three components, including a generator G, a discriminator D and an optical flow estimator F. The generator is the core component, and the flow estimator provides auxiliary warping images for the generation process. Suppose we have a sequence of measurements y 1 , y 2 , • • • , y n ; for each y i , 1 ≤ i ≤ n, we want to reconstruct its ground truth image x r i as the Eq. (  Discriminator. The discriminator D assigns the label ""1"" to real standarddose CT images and ""0"" to generated images. The goal of D is to maximize the separation between the distributions of real images and generated images: where x g i is the image generated by G (the formal definition for x g i will be introduced below). The discriminator includes 3 residual blocks, with 4 convolutional layers in each residual block. Generator. We use the generator G to reconstruct the high-quality CT image for the ground truth x r i from the low-dose image s i . The generated image is obtained by where W(•) is the warping operator. Before generating x g i , N (x g i ) is reconstructed from N (s i ) by the generator without considering local coherence. Subsequently, according to the optical flow F(N (s i ), s i ), we warp the reconstructed images N (x g i ) to align with the current slice by adjusting the brightness values. The warping operator W utilizes bi-linear interpolation to obtain W(N (x g i )), which enables the model to capture subtle variations in the tissue from the generated N (x g i ); also, the warping operator can reduce the influence of artifacts for the reconstruction. Finally, x g i is generated by combining s i and W(N (x g i )). Since x r i is our target for reconstruction in the i-th batch, we consider the difference between x g i and x r i in the loss. Our generator is mainly based on the network architecture of Unet  In  where D j (•) refers to the feature extraction performed on the j-th hidden layer. Through capturing the high frequency differences in CT images, L percept can enhance the sharpness for edges and increase the contrast for the reconstructed images. L pixel and L adv are designed to recover global structure, and L percept is utilized to incorporate additional texture details into the reconstruction process."
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,4.0,Experiment,"Datasets. First, our proposed approaches are evaluated on the ""Mayo-Clinic low-dose CT Grand Challenge"" (Mayo-Clinic) dataset of lung CT images  The dataset contains 2250 two dimensional slices from 9 patients for training, and the remaining 128 slices from 1 patient are reserved for testing. The lowdose measurements are simulated by parallel-beam X-ray with 200 (or 150) uniform views, i.e., N v = 200 (or N v = 150), and 400 (or 300) detectors, i.e., N d = 400 (or N d = 300). In order to further verify the denoising ability of our approaches, we add the Gaussian noise with standard deviation σ = 2.0 to the sinograms after X-ray projection in 50% of the experiments. To evaluate the generalization of our model, we also consider another dataset RIDER with nonsmall cell lung cancer under two CT scans  Baselines and Evaluation Metrics. We consider several existing popular algorithms for comparison. (  We set λ pix = 1.0, λ adv = 0.01 and λ per = 1.0 for the optimization objective in Eq. (  Results. a similar increasing trend with our approach across different settings but has worse reconstruction quality. To evaluate the stability and generalization of our model and the baselines trained on Mayo-Clinic dataset, we also test them on the RIDER dataset. The results are shown in Table  To illustrate the reconstruction performances more clearly, we also show the reconstruction results for testing images in Fig. "
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,5.0,Conclusion,"In this paper, we propose a novel approach for low-dose CT reconstruction using generative adversarial networks with local coherence. By considering the inherent continuity of human body, local coherence can be captured through optical flow, which is small deformations and structural differences between consecutive CT slices. The experimental results on real datasets demonstrate the advantages of our proposed network over several popular approaches. In future, we will evaluate our network on real-world CT images from local hospital and use the reconstructed images to support doctors for the diagnosis and recognition of lung nodules. Our code is publicly available at https://github.com/lwjie595/GANLC."
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 50.
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,1.0,Introduction,"Metal artifact could significantly affect the clinical diagnoses with computed tomography (CT) images, and how to effectively reduce it is a critical but challenging issue. Specifically, metal artifact is caused by metallic implants and often exhibits as bright and dark streaks in the reconstructed images  Since the metal artifacts are structured and non-local, they are tough to be removed from images directly  In contrast to the traditional methods mentioned above, deep learning-based MAR methods are undergoing more intensive studies and become a dominant approach to MAR. Depending on the performing domain, they can be categorized into three types, i.e., sinogram domain-based, image domain-based, and dual-domain-based. Specifically, i). the sinogram domain-based methods leverage the advantage that the signals of metal artifacts are concentrated in form of metal trace(s) and can be easily separated from the informative image contents in the sinogram domain  To address the issues of the existing methods aforementioned, we propose a Multi-perspective Adaptive Iteration Network(MAIN), and our main contributions are as follows: 1) Multi-perspective Regularizations: Based on the insightful analysis on the limitations of using sinograms in MAR, this paper innovatively identifies that the desirable properties of wavelet transform could well address the issues of sinograms in MAR. i.e., the spatial distribution characteristics of metal artifacts under different domains and resolutions. Based on this, we integrate multi-domain, multi-frequency band, and multi-constraint into our scheme by exploiting wavelet transform. Therefore, we explicitly formulate such knowledge as a multi-perspective optimization model as shown in Fig. is the element-wise multiplication; and W denotes the adaptive wavelet transform. technique  3) Adaptive Wavelet Transform: In order to increase the flexibility and adaptivity of the proposed model, the proposed model conducts wavelet transforms with neural technology rather than the traditional fixed wavelet transform."
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,2.0,Method,"Mathematical Model. In the image domain, it is easy to segment metal regions with much higher CT values  where Y ∈ R H×W is a metal-corrupted image, A and I denote the metal artifact and binary non-metal mask, respectively. is the element-wise multiplication. To obtain a promising solution, various regularization terms representing prior constrains are introduced as: where W denotes the wavelet transform, f 1 (.), f 2 (.) and f 3 (.) are regularization functions, and λ 1 , λ 2 and λ 3 are regularization weights. Specifically, f 1 (.) and f 2 (.) represent wavelet domain constraints of X and A respectively, and f 3 (.) introduces prior knowledge of X in the image domain. ε is an arbitrarily small number, and . 2 F is the Frobenius norm. When the wavelet components are transformed back to the image domain, the image will be blurred due to information loss. To recover a more precise image, let U = I X and introduce an error feedback item  where β is an adaptive weight. Optimization Algorithm. In this paper, an alternating minimization strategy is used to solve Eq. (  The Proximal Gradient Descent algorithm(PGD)  , the quadratic approximation of A (k+1) can be expressed as: Assuming that ∇ 2 g(A (k) ) is a non-zero constant that is replaced by 1 η1 . Besides, g(A (k) ) is replaced by another constant (η 1 ∇g(A (k) )) 2 to form a perfect square trinomial since changing constant does not affect the minimization of our objective function. Therefore, Eq. (  In code implementation, a trainable parameter is used to represent η 1 . It is excepted that the application of adaptive parameter can assist network fitting A more preferably. In order to reveal the iterative procedure more apparently, an intermediate variable A (k+0.5) is introduced: Based on the above derivations, an iteration equation can be formulated as  where prox η1 is the proximal operator  where prox η2 and prox η3 are the proximal operator related to f 2 (.) and f 3 (.) respectively. Network Design.  "
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,3.0,Experiments,"Synthetic Data. A synthetic dataset is generated by following the simulation procedure in  Implementation Details. The MAIN network is implemented using PyTorch  Baseline and Evaluation Metric. Six state-of-the-art methods for metal artifact reduction are compared, including traditional method (NMAR  Experimental Results on Clinical Data. Figure  Ablation Studies. We re-configure our network and retrain the model in image domain and wavelet domain, respectively. We also utilize 'db3' wavelet to reconfigure the network and compare the corresponding model with the adaptive wavelet. It can be observed from Fig. "
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,1.0,Introduction,"Radiotherapy (RT) is one of the cornerstones of cancer patients. It utilizes ionizing radiation to eradicate all cells of a tumor. The total radiation dose is typically divided over 3-30 daily fractions to optimize its effect. As the surrounding normal tissue is also sensitive to radiation, highly accurate delivery is vital. Image guided RT (IGRT) is a technique to capture the anatomy of the day using in room imaging in order to align the treatment beam with the tumor location  A major challenge especially for CBCT imaging of the thorax and upperabdomen is the respiratory motion that introduces blurring of the anatomy, reducing the localization accuracy and the sharpness of the image. A technique used to alleviate motion artifacts is Respiratory Correlated CBCT (4DCBCT)  Several traditional methods based on iterative reconstruction algorithms and motion compensation techniques are used to reduce view-aliasing in 4DCBCTs  Deep learning has been proposed as a way to address view-aliasing with accelerated reconstruction  A different method, called Noise2Inverse, uses an unsupervised approach to reduce measurement noise in the traditional CT setting  We propose Noise2Aliasing to address these limitations. The method can be used to provably train models to reduce both view-aliasing artifacts and stochastic noise from 4DCBCTs in an unsupervised way. Training deep learning models for medical applications often needs new data. This was not the case for Noise2Aliasing, and historical clinical data sufficed for training. We validated our method on publicly available data "
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,2.0,Theoretical Background,"In this section, we will introduce the concepts and the notation necessary to understand the method and the choices made during implementation. Unsupervised noise reduction with Noise2Noise. Given input-target pairs x, y ∈ R we can define the regression problem in the one-dimensional setting as finding f * : R → R which satisfies the following: which can be minimized point-wise  In Noise2Noise  Then f * will recover the input image without any noise: Denoising for Tomography with Noise2Inverse. During a CT scan, a volume x is imaged by acquiring projections y = Ax using an x-ray source and a detector placed on the opposite side of the volume. The projections can then be used by an algorithm that computes a linear operator R to obtain an approximation of the original distribution of x-ray attenuation coefficients x = Ry. The algorithm can also operate on a subset of the projections. Let J = {1, 2, . . . } be the set of all projections and J ⊂ J , then xJ = R J y J is the reconstruction obtained using only projections y J . Let us now assume that the projections have some meanzero noise ỹi = y i + with E (ỹ i ) = y i . Then, in Noise2Inverse  where J is a random variable that picks subsets of projections at random and J is its complementary. Given Eq. 2, we observe that function f * which minimizes L is: When using reconstructions from a subset of noisy projections as input and reconstructions from their complementary as its output, a neural network will learn to predict the expected reconstruction without the noise. Property of Expectation over Subsets of Projections Using FDK. Now let J be a random variable that selects subsets of projections J ⊂ J at random such that each projection is selected at least once. Define R J : R D d ×|J| → R Dv to be the FDK reconstruction algorithm "
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,3.0,Noise2Aliasing,"Here, we propose Noise2Aliasing, an unsupervised method capable of reducing both view-aliasing and projection noise in 4DCBCTs. At the core of this method is the following proposition. Proposition. Given the projection set J = {1, 2, . . . }, the FDK reconstruction algorithm R, and the noisy projections ỹ = Ax+ with mean-zero element-wise independent noise. Let J 1 , J 2 be two random variables that pick different subsets at random belonging to a partition of J , and be the input-target pairs in dataset D of reconstructions using disjoint subsets of noisy projections. Let L be the expected MSE over D with respect to a function f : R Dv → R Dv and the previously-described input-target pairs. Then, we find that the function f * that minimizes L for any given J ∈ J will reconstruct the volume using all the projections and remove the noise : Proof. The loss function L is defined in the following way: Additionally, J 1 , J 2 are disjoint, the noise is mean-zero element-wise, and we are using the FDK reconstruction algorithm which defines a linear operator R. These allow us to use Eq. 5 to find that the function f * that minimizes L is the following: This is sufficient to reduce stochastic noise but we need to further manipulate this expression to address view aliasing. Simplifying notation and using the properties of conditional expectations, we can write: now assume that xj1 is the clean reconstruction that is consistent with the observed noisy reconstruction z obtained from each disjoint subset j 2 , then: Finally, we use the property of the FDK from Eq. 6:"
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,3.1,Design Choices Based on the Proposition,"The proposition guided the choice of reconstruction method to be FDK and the design of the subset selection method from considerations that are now explained. Equation 12 holds true only when the same underlying clean reconstruction x can be determined from the noisy reconstruction using any subset from a partition of the projections J . This means that, in our dataset, we should have at our disposal reconstructions of the same underlying volume x using disjoint subsets of projections. In 4DCBCTs this is not the case, as separate respiratory phases are being reconstructed, where the organs are in different positions. We can address this problem by carefully choosing subsets of projections that result in respiratory-uncorrelated reconstructions. The reconstructions will display organs in their average position and, therefore, have the same underlying structure. When the projections are selected with the same sampling pattern as the one used in respiratory-correlated reconstructions, then the view-aliasing artifacts display will have the same pattern as the ones present in the 4DCBCTs. Compared to previous work, to obtain the additional effect of reducing projection noise, the respiratory-uncorrelated reconstructions must use non-overlapping subsets of projections. Coincidentally, a previously proposed subset selection method utilized for supervised aliasing reduction fits all these requirements and will, therefore, be used in this work "
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,4.0,Experiments,"First, we used the SPARE Varian dataset to study whether Noise2Aliasing can match the performance of the supervised baseline and if it can outperform it when adding noise to the projections. Then, we use the internal dataset to explore the requirements for the method to be applied to an existing clinical dataset. These required around 64 GPU days on NVIDIA A100 GPUs. Training of the model is done on 2D slices. The projections obtained during a scan are sub-sampled according to the pseudo-average subset selection method described in  The Datasets used in this study are two: 1. The SPARE Varian dataset was used to provide performance results on publicly available patient data. To more closely resemble normal respiratory motion per projection image, the 8 min scan has been used from each patient (five such scans are available in the dataset). Training is performed over 4 patients while 1 patient is used as a test set. The hyperparameters are optimized over the training dataset. 2. An internal dataset (IRB approved) of 30 lung cancer patients' 4DCBCTs from 2020 to 2022, originally used for IGRT, with 25 patients for training and 5 patients for testing. The scans are 4 min 205 • scans with 120keV source and 512 × 512 sized detector, using Elekta LINACs. The data were anonymized prior to analysis. Projection Noise was added using the Poisson distribution to the SPARE Varian dataset to evaluate the ability of the unsupervised method to reduce it. Given a projected value of p and a photon count π (chosen to be 2500), the rate of the Poisson distribution is defined as πe -p and given a sample q from this distribution, then the new projected value is p =log q π . The Architecture used in this work is the Mixed Scale Dense CNN (MSD)  The Baselines we compare against are two. The first is the traditional FDK obtained using RTK  The Metrics used in this work are the Root Mean Squared Error (RMSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM) "
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,5.0,Results and Discussion,"SPARE Varian. Inference speed with the NVIDIA A100 GPU averages 600ms per volume made of 220 slices. From the qualitative evaluation of the methods in Fig.  Noise2Aliasing is capable of reducing the artifacts present in reconstructions caused by stochastic noise in the projections used, outperforming the supervised baseline. Internal Dataset. Noise2Alisting trained on 25 patients and tested on 5 achieved mean PSNR of 35.24 and SSIM of 0.91, while the clinical method achieved mean PSNR of 29.97 and 0.74 SSIM with p-value of 0.048 for the PSNR and 0.0015 for the SSIM, so Noise2Aliasing was significantly better according to both metrics. Additionally, from Fig.  especially between fat tissue and skin and around the bones. However, the model also tends to remove small anatomical structures as high-frequency objects that cannot be distinguished from the noise. When applied to a clinical dataset, Noise2Aliasing benefits from more patients being included in the dataset, however, qualitatively good performance is already achieved with 5 patients. No additional data collection was required and the method can be applied without major changes to the current clinical practice. "
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,6.0,Conclusion,"We have presented Noise2Aliasing, a method to provably remove both viewaliasing and stochastic projection noise from 4DCBCTs using an unsupervised deep learning method. We have empirically demonstrated its performance on a publicly available dataset and on an internal clinical dataset. Noise2Aliasing outperforms a supervised approach when stochastic noise is present in the projections and matches its performance on a popular benchmark. Noise2Aliasing can be trained on existing historical datasets and does not require changing current clinical practices. The method removes noise more reliably when the dataset size is increased, however further analysis is required to establish a good quantitative measurement of this phenomenon. As future work, we plan to study Noise2Aliasing in the presence of changes in the breathing frequency and amplitude between patients and during a scan."
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 46.
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,1.0,Introduction,"Positron Emission Tomography (PET) is a widely used modality in functional imaging for oncology, cardiology, neurology, and medical research  As a result, the quality of PET images can be compromised, leading to difficulties in accurate diagnosis. Deep learning (DL) techniques, especially supervised learning, have recently garnered considerable attention and show great promise in PET image reconstruction compared with traditional analytical methods and iterative methods. Among them, four primary approaches have emerged: DL-based postdenoising  DL-based post denoising methods are relatively straightforward to implement but can not reduce the lengthy reconstruction time and its results are significantly affected by the pre-reconstruction algorithm. End-to-end direct learning methods utilize deep neural networks to learn the directing mapping from measurement sinogram to PET image. Without any physical constraints, these methods can be unstable and extremely data-hungry. Deep learning regularized iterative reconstruction methods utilize a deep neural network as a regularization term within the iterative reconstruction process to regularize the image estimate and guide the reconstruction process towards a more accurate and stable solution. Despite the incorporation of deep learning, the underlying mathematical framework and assumptions of deep learning regularized iterative methods still rely on the conventional iterative reconstruction methods. Deep unrolled methods utilize a DNN to unroll the iterative reconstruction process and to learn the mapping from sinogram to the reconstructed PET images, which potentially result in more accurate and explainable image reconstruction. Deep unrolled methods have demonstrated improved interpretabillity and yielded inspiring outcomes. However, the aforementioned approaches for PET image reconstruction depend on high quality ground truths as training labels, which can be diffi-cult and expensive to obtain. This challenge is further compounded by the high dose exposure associated with PET imaging. Unsupervised/self supervised learning has gained considerable interest in medical imaging, owing to its ability to mitigate the need for high-quality training labels. Gong et al. proposed a PET image reconstruction approach using the deep image prior (DIP) framework  In this paper, we propose a dual-domain unsupervised learned descent algorithm for PET image reconstruction, which is the first attempt to combine unsupervised learning and deep unrolled method for PET image reconstruction. The main contributions of this work are summarized as follows: 1) a novel model based deep learning method for PET image reconstruction is proposed with a learnable l 2,1 norm for more general and robust feature sparsity extraction of PET images; 2) a dual domain unsupervised training strategy is proposed, which is plug-and-play and does not need paired training samples; 3) without any anatomic priors, the proposed method shows superior performance both quantitatively and visually."
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,2.1,Problem Formulation,"As a typical inverse problem, PET image reconstruction can be modeled in a variational form and cast as an optimization task, as follows: where y is the measured sinogram data, y is the mean of the measured sinogram. x is the PET activity image to be reconstructed, L(y|x) is the Poisson loglikelihood of measured sinogram data. P (x; θ) is the penalty term with learnable parameter θ. A ∈ R I×J is the system response matrix, with A ij representing the probabilities of detecting an emission from voxel j at detector i. We expect that the parameter θ in penalty term P can be learned from the training data like many other deep unrolling methods. However, most of these methods directly replace the penalty term "
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,2.2,Parametric Form of Learnable Regularization,"We choose to parameterize P as the l 2,1 norm with a feature extraction operator g(x) to be learned in the training data. The smooth nonlinear mapping g is used to extract sparse features and the l 2,1 norm is used as a robust and effective sparse feature regularization. Specifically, we formulate P as follows  where g i,θ (x) is i-th feature vector. We choose g as a multi-layered CNN with nonlinear activation function σ, and σ is a smoothed ReLU: In this case, the gradient ∇g can be computed directly. The Nesterov's smoothing technique is used in P for the derivative calculation of the l 2,1 norm through smooth approximation: where parameter ε controls how close the approximation P ε to the original P ."
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,,Algorithm 1. Learned Descent Algorithm for PET image reconstruction,"Input: Image initialization x0, ρ, γ ∈ (0, 1), ε0, σ, τ > 0, maximum number of iteration I, total phase numbers K and measured Sinogram y 1:"
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,2.3,Learned Descent Algorithm for PET,"With the parametric form of learnable regularization given above, we rewrite Eq. 1 as the objective function: min φ(x; y, θ) = -L(y|x) + P ε (x; θ)  In each phase k -1, we apply the proximal gradient step in Eq. 8: where the proximal operator is defined as: In order to have a close form solution of the proximal operator, we perform a Taylor approximation of P ε k-1 : After discarding higher-order constant terms, we can simplify the Eq. 10 as: where α k-1 and β k-1 are two parameters greater than 0 and We also calculate a close-form safeguard v k as: The line search strategy is used by shrinking α k-1 to ensure objective function decay. We choose the u k or v k with smaller objection function value φ ε k-1 to be the next x k . The smoothing parameter ε k-1 is shrinkage by γ ∈ (0, 1) if the ||∇φ ε k-1 (x k )|| < σγε k-1 is satisfied. The whole flow is shown in Algorithm 1. "
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,2.4,Dual-Domain Unsupervised Training,"The whole reconstruction network is indicated by f θ with learned parameter θ. Inspired by Deep image prior  where λ is the parameter that controls the ratio of different domain loss function, which was set to 0.1 in the experiments. For image domain loss L image , the equivariance constraint is used. For example, if the test sample x t first undergoes an equivariant transformation, such as rotation, we obtain x tr . Subsequently, we perform a PET scan to obtain the sinogram data of x tr and x t . The image reconstructed by the f θ of these two sinogram should also keep this rotation properties. The L image is formulate as: where T r denotes the rotation operator, A is the forward projection which also can be seen as a measurement operator. For sinogram domain loss L measure , the data argumentation with random noise ξ is performed on y:"
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,2.5,Implementation Details and Reference Methods,"We implemented DULDA using Pytorch 1.7 on a NVIDIA GeForce GTX Titan X. The Adam optimizer with a learning rate of 10 -4 was used and trained for 100 epochs with batch size of 8. The total unrolled phase was 4. The image x 0 was initialized with the values of one. The smoothing parameter ε 0 and δ were initialized to be 0.001 and 0.002. The step-size α 0 and β 0 were initialized to be 0.01 and 0.02. The system matrix was computed by using Michigan Image Reconstruction Toolbox (MIRT) with a strip-integral model  For DIP, we used random noise as input and trained 14000 epochs with the same training settings as DULDA to get the best results before over-fitting. The proposed method can also be trained in a fully supervised manner (we call it SLDA). The loss is the mean square error between the output and the label image. To further demonstrate the effectiveness, we compared SLDA with DeepPET "
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,3.1,Experimental Evaluations,Forty 128 × 128 × 40 3D Zubal brain phantoms 
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,3.2,Results,Figure 
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,4.0,Discussion,"To test the robustness of proposed DULDA, we forward-project one patient brain image data with different dose level and reconstructed it with the trained DULDA model. The results compared with MLEM are shown in Fig. "
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,5.0,Conclusions,"In this work, we proposed a dual-domain unsupervised model-based deep learning method (DULDA) for PET image reconstruction by unrolling the learned descent algorithm. Both quantitative and visual results show the superior performance of DULDA when compared to MLEM, EM-TV and DIP based method. Future work will focus more on clinical aspects."
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,1.0,Introduction,"Following the ""as low as reasonably achievable"" (ALARA) principle  In the past decades, image post-processing techniques attracted much attention from researchers because they did not rely on the vendor-specific parameters  With the development of deep learning (DL), various learning-based methods have been proposed, such as EDSR  The aforementioned methods still have drawbacks: (1) They treated the regions of interest (ROI) and regions of uninterest equally, resulting in the extra cost in computing source and inefficient use for hierarchical features. (2) Most of them extracted the features with a fixed resolution, failing to effectively leverage multi-scale features which are essential to image restoration task  (3) They connected the SR task and the LDCT denoising task stiffly, leading to smooth texture, residual artifacts and unclear edges. To deal with those issues, as shown in Fig. "
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,2.1,Overall Architecture,"The pipeline of our proposed method is shown in Fig.  Dual-Guidance Feature Distillation Backbone. To decrease the redundant computation and make full use of the above-mentioned extra information, we design a dual-guidance feature distillation backbone consisting of a dual-guidance fusion module (DGFM) and sampling attention block(SAB). Firstly, we use a 3 × 3 convolutional layer to extract the shallow features of the three input images. Then, those features are fed into 10 DGFM-SAB blocks to obtain the deep visual features. Especially, the DGFM-SAB block is composed of DGFM concatenated with SAB. Considering the indicative function of ROI, we calculate the correlation matrix between LDCT and its mask and then acquire the response matrix between the correlation matrix and the average CT image by multi-heads attention mechanism: where, F SAB i are the output of i-th SAB. F mask and F AV G represent the shallow features of the input ROI mask and the average CT image respectively. Meanwhile, P rj(•) is the projection function, Sof tmax[•] means the softmax function and F i are the output features of the i-th DGFM. The DGFM helps the backbone to focus on the ROI and tiny structural information by continuously introducing additional guidance information. Furthermore, to take advantage of the multi-scale information which is essential for obtaining the response matrix containing the connections between different levels of features, as shown in Fig.  where, "
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,2.2,Target Function,"Following the multiple supervision strategy, the target function L total is calculated as: where, I gt is the ground truth, BI(•) means bicubic interpolation, • 1 represents the L1 norm and λ 1 , λ 2 are the weight parameters for adjusting the losses."
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,3.1,Datasets and Experiment Setup,"Datasets. Two widely-used public CT image datasets, 3D-IRCADB "
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,3.2,Ablation Study,"Table  Introducing the average CT image guidance alone degrades performance compared with the model without guidance for both the scale factor of 2 and 4. And introducing mask guidance alone could improve the reconstruction effect. When the average CT image guidance and the mask guidance are both embedded, the performance will be promoted further. Table "
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,3.3,Comparison with State-of-the-Art Methods,"We compare the performance of our proposed method with other state-of-theart methods, including Bicubic interpolation  Figure  Table "
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,4.0,Conclusion,"In this paper, we propose an LDCT image SR network with dual-guidance feature distillation and dual-path content communication. Facing the existing problem that reconstructed results suffer from residual artifacts, we design a dualguidance feature distillation backbone which consists of DGFM and SAB to extract deep visual information. Especially, the DGFM could fuse the average CT image to take the advantage of the 3D spatial information of CT volume and the segmentation mask to focus on the ROI, which provides pixel-wise shallow information and deep semantic features for our backbone. The SAB leverages the essential multi-scale features to enhance the ability for feature extraction. Then, our shared heads mechanism reconstructs the deep features obtained by our backbone to satisfactory results. The experiments compared with 6 state-ofthe-art methods on 2 public datasets demonstrate the superiority of our method."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,1.0,Introduction,"Image registration is a fundamental requirement for medical image analysis and has been an active research focus for decades  Many deep registration methods perform coarse-to-fine registration to improve registration accuracy, where the registration is decoupled into multiple coarse-to-fine registration steps that are iteratively performed by using multiple cascaded networks  Firstly, existing NICE registration methods merely focus on deformable coarseto-fine registration, while affine registration, a common prerequisite, is still reliant on traditional registration methods  In this study, we propose a Non-Iterative Coarse-to-finE Transformer network (NICE-Trans) for joint affine and deformable registration. Our technical contributions are two folds: (i) We extend the existing NICE registration framework to affine registration, where multiple steps of both affine and deformable coarse-to-fine registration are performed with a single network in a single iteration. (ii) We explore the benefits of transformers for NICE registration, where Swin Transformer "
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,2.0,Method,"Image registration aims to find a spatial transformation φ that warps a moving image I m to a fixed image I f , so that the warped image I m•φ = I m • φ is spatially aligned with the I f . In this study, we assume the I m and I f are two single-channel, grayscale volumes defined in a 3D spatial domain ⊂ R 3 , which is consistent with common medical image registration studies "
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,2.1,Non-iterative Coarse-to-fine Transformer Networks (NICE-Trans),"The architecture of the proposed NICE-Trans is presented in Fig.  The encoder has two identical, weight-shared paths P m and P f that take I m and I f as input, respectively. Each path consists of L successive Conv modules with 2 × 2 × 2 max pooling applied between two adjacent modules, which produces two L-level feature pyramids where the F f i and F m i are the output of the i th Conv module in the P f and P m . Each Conv module consists of two 3 × 3 × 3 convolutional layers followed by LeakyReLU activation with parameter 0.2. This dual-path design can learn uncoupled image features of I m and I f , which enables the NICE-Trans to reuse the learned features at multiple registration steps, thereby discarding the requirement for repeated feature learning. The decoder consists of L-1 SwinTrans modules and a Conv module, with a patch expanding layer  , which is then fed into its later decoder module. The decoder performs finer registration after each decoder module, where the φ L is the final output φ. Detailed architecture settings (e.g., feature dimensions, head numbers of self-attention) are presented in the supplementary materials. Our NICE-Trans differs from the existing NICE-Net "
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,2.2,Joint Affine and Deformable Registration,"The output features of the first L a decoder modules are fed into L a affine registration heads, where the features are mapped to a 3 × 4 affine matrix through global average pooling and two fully-connected layers, which are then sampled as a dense displacement field. After the first L a steps of affine registration, the output features of the last L d decoder modules are fed into L d deformable registration heads, where the features are directly mapped to a dense displacement field via a 3 × 3 × 3 convolutional layer. At the beginning of coarse-to-fine registration, the φ 1 is the output of the first registration head. Then, the φ 1 is upsampled (×2) and voxel-wisely added to the output of the second registration head to derive φ 2 . This process is repeated until the φ L is derived, which realizes joint affine and deformable coarse-to-fine registration. In our experiments, we set L a and L d as 1 and 4 (illustrated in Fig. "
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,2.3,Unsupervised Learning,"The learnable parameters θ are optimized using an unsupervised loss L that does not require labels. The L is defined as L = L sim + σ L reg , where the L sim is an image similarity term that penalizes the differences between the warped image I m•φ and the fixed image I f , the L reg is a regularization term that encourages smooth and invertible transformations φ, and the σ is a regularization parameter. We adopt negative local normalized cross-correlation (NCC) as the L sim , which is a widely used similarity metric in image registration methods "
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,3.1,Dataset and Preprocessing,"We evaluated the proposed NICE-Trans on the task of inter-patient brain MRI registration, which is a common benchmark task in medical image registration studies  We performed brain extraction and intensity normalization for each MRI image with FreeSurfer "
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,3.2,Implementation Details,"We implemented our NICE-Trans using PyTorch on a NVIDIA Titan V GPU with 12 GB memory. We used an ADAM optimizer with a learning rate of 0.0001 and a batch size of 1 to train the NICE-Trans for 100,000 iterations. At each iteration, two images were randomly picked from the training data as the fixed and moving images. A total of 100 image pairs, randomly picked from the validation data, were used to monitor the training process and to optimize hyper-parameters. We set σ as 1 to ensure that the L sim and σ L reg have close values, while the λ was set as 10 -4 to ensure that the percentage of voxels with negative Jacobian determinants is less than 0.05% (refer to the supplementary materials for detailed regularization analysis). Our code will be available in https://github.com/ MungoMeng/Registration-NICE-Trans."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,3.3,Comparison Methods,"Our NICE-Trans was compared with nine image registration methods, including two traditional methods and seven deep registration methods. The compared traditional methods are SyN "
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,3.4,Experimental Settings,"We compared the NICE-Net to the nine comparison methods for subject-to-subject registration. For testing, we randomly picked 100 image pairs from each of the Mindboggle, Buckner, and LPBA testing sets. We used standard evaluation metrics for medical image registration  We also performed an ablation study to explore the benefits of transformers. We built a baseline method that has the same architecture as the NICE-Trans but only uses Conv modules. After that, we embedded Swin Transformer into the baseline method, where SwinTrans modules replaced the Conv modules in the encoder (Trans-Encoder), decoder (Trans-Decoder), or both (Trans-All)."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,4.0,Results and Discussion,Table  Table 
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,5.0,Conclusion,"We have outlined a Non-Iterative Coarse-to-finE Transformer network (NICE-Trans) for medical image registration. Unlike the existing image registration methods, our NICE-Trans performs joint affine and deformable coarse-to-fine registration with a single network in a single iteration. The experimental results show that our NICE-Trans can outperform the state-of-the-art coarse-to-fine or transformer-based deep registration methods on both registration accuracy and runtime. Our study also suggests that transformers benefit registration in modeling inter-image spatial relevance while having limited benefits in learning intra-image representations."
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,1.0,Introduction,"Image-guided navigation plays a crucial role in modern surgical procedures. In the field of orthopedics, many surgical procedures such as total hip arthro-plasty, total knee arthroplasty, and pedicle screw injections utilize intraoperative fluoroscopy for surgical navigation "
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,2.1,Problem Formulation,"The problem of 2D-3D registration can be formulated a finding the rigid transformation that transforms the 3D model defined in the anatomical or world coordinate system into the camera coordinate system. Specifically, given a CTscan volume V CT (x w ) where x w is defined in the world coordinate system, the registration problem is concerned with finding T c w = [R|t] such that the following holds.  where {•} is the X-ray transform that can be applied to volumes in the camera coordinate system given an intrinsic matrix K and I, the target X-ray image."
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,2.2,Registration,"The proposed registration pipeline overview is shown in Fig.  Scene Coordinates. The scene coordinates are defined as the points of intersection between the camera's back-projected rays and the 3D model in a world coordinate system (i.e., only the first intersection and the last intersections are considered). The same concept was adapted for X-ray images and their underlying 3D models obtained from the CT-scans. Specifically, given an arbitrary point x ij in the image plane, the scene coordinates X ij satisfy the following conditions. where R and t are the rotation matrix and translation vector that maps points in the world coordinate system to the camera coordinate system, K is the intrinsic matrix, d is the depth, as seen from the camera, of the point X on the 3D model. Uncertainty Estimation. The task of scene coordinate regression is to estimate these X ij for every pixel ij, given an X-ray image I. However, the existence of X ij is not guaranteed for all pixels because back-projected rays may not intersect the 3D model. One of the many ways to address such a case is to prepare a mask (i.e., 1 if the bone area, 0 otherwise) in advance so that only the pixels that lie inside the mask are estimated. As this approach requires an explicit method for estimating the mask image, an alternative approach was adopted in this study. Instead of estimating a single X ij , the mean and variance of the scene coordinates are estimated. The non-intersecting scene coordinates were identified by applying thresholding to the estimated variance (i.e., points with high variance were considered non-existent scene coordinates and were filtered out). This approach assumes that the observed scene coordinates are corrupted with a zero mean, non-zero and non-constant variance, and isotropic Gaussian noise. where u(I, x ij ) and σ(I, x ij ) are the functions that produce the mean and standard deviation of the scene coordinates, respectively. This work represents these functions using a fully convolutional neural network. Loss Function. A U-Net architecture was used to estimate the mean and standard deviation of the scene coordinates at every pixel in a given image. The loss function for the intersecting scene coordinates is derived from the maximum likelihood estimates using the likelihood X ij . This can be expressed as follows: Because it is desirable to have a high variance for non-existent scene coordinates, the loss function for non-existent coordinates is designed as follows: 2D-3D Registration. An iterative PnP implementation from OpenCV was run using RANSAC with maximum iteration of 1000 and reprojection error of 10px and 20px for the simulated and real X-ray images respectively. An example of a successful registration is shown in the left part of Fig. "
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,3.1,Dataset,"A dataset containing six annotated CT scans, each with several registered real X-ray images from  The offset vector was sampled from a normal distribution with a mean of zero and standard deviations of 90 mm in the lateral direction, and 30 mm in the other two directions. Images intentionally included partially visible structures. A selection of randomly sampled images is displayed on the right side of Fig.  In total, 8100 simulated X-rays were generated for each specimen. Among these, 5184 images were randomly assigned to the training set, 1296 to the validation set, and the remaining 1620 to the test set."
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,3.2,Implementation Details,"The input image and output scene coordinates had a size of 512 × 512 pixels. The U-Net model had eight output channels, which consisted of three channels for scene coordinates and one channel for standard deviation, multiplied by two for the entry and exit points. Patient-specific models were trained individually for each dataset. Adam optimizer with a constant learning rate of 0.0001 and a batch size of 16 was used. Online data augmentation which includes random inversion, color jitter, and random erasing was applied. The scene coordinates were filtered using a log variance threshold of 0.0 for simulated images and -2.0 for real X-ray images."
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,3.3,Baselines and Evaluation Metrics,The proposed method was compared with two other baseline methods: PoseNet 
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,3.4,Registration Results,Simulated X-Ray Images. Table  Real X-Ray Images. Table  A visualization of the overlays is presented in the Supplemental Material.   
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,4.0,Limitations,"As the proposed method was designed to give initial estimates of the pose parameters, a further refinement step using an intensity-based optimization method would be required to obtain clinically relevant registration accuracy. Although the proposed method provided a good initial estimate, the average runtime for the entire pipeline was 1.75 s which was approximately two orders of magnitude greater than that of PoseNet, which had an average runtime of 0.06 s. This is because RANSAC must determine a good pose from a dense set of correspondences. This issue can be addressed by heuristically selecting a good variance threshold per image that filters out bad correspondences."
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,5.0,Conclusion,"This paper presented a scene coordinate regression-based approach for the Xray to CT-scan model registration problem. Experiments with simulated and real X-ray images showed that the proposed method performed well even under partially visible structures and extreme view angles, compared with direct pose regression and landmark estimation methods. Testing the model trained solely on simulated X-ray images, on real X-ray images did not result in catastrophic failure. Instead, the results were positive for instantiating further refinement steps."
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 74.
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,1.0,Introduction,"Liver cancer is the most prevalent indication for liver surgery, and although there have been notable advancements in oncologic therapies, surgery remains as the only curative approach overall  Liver laparoscopic resection has demonstrated fewer complications compared to open surgery  Performing IOUS during laparoscopic liver surgery poses significant challenges, as laparoscopy has poor ergonomics and narrow fields of view, and on the other hand, IOUS demands skills to manipulate the probe and analyze images. At the end, and despite its real-time capabilities, IOUS images are intermittent and asynchronous to the surgery, requiring multiple iterations and repetitive steps (probe-in -→ instruments-out -→ probe-out -→ instruments-in). Therefore, any method enabling a continuous and synchronous US assessment throughout the surgery, with minimal iterations required would significantly improve the surgical workflow, as well as its efficiency and safety. To overcome these limitations, the use of intravascular ultrasound (IVUS) images has been proposed, enabling continuous and synchronous inside-out imaging during liver surgery  Several approaches have been proposed to address this limitation by proposing a trackerless ultrasound volume reconstruction. Physics-based methods have exploited speckle correlation models between different adjacent frames  Our method improves upon previous solutions in terms of robustness and accuracy, particularly in the presence of rotational motion. Such motion is predominant in the context highlighted above and is the source of additional nonlinearity in the pose estimation problem. To the best of our knowledge, this is the first work that provides a clinically sound and efficient 3D US volume reconstruction during minimally invasive procedures. The paper is organized as follows: Sect. 2 details the method and its novelty, Sect. 3 presents our current results on ex vivo porcine data, and finally, we conclude in Sect. 4 and discuss future work."
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,2.0,Method,"In this work, we make the assumption that the organ of interest does not undergo deformation during the volume acquisition. This assumption is realistic due to the small size of the probe. Let I 0 , I 1 ...I N -1 be a sequence of N frames. Our aim is to find the relative spatial transformation between each pair of frames I i and I j with 0 ≤ i ≤ j ≤ N -1. This transformation is denoted T (i,j) and is a six degrees of freedom vector representing three translations and three Euler angles. To achieve this goal, we propose a Siamese architecture that leverages the optical flow in the sequences in addition to the frames of interest in order to provide a mapping with the relative frames spatial transformation. The overview of our method is presented in Fig.  We consider a window of 2k + 3 frames from the complete sequence of length N , where 0 ≤ k ≤ N -3 2 is a hyper-parameter that denotes the number of frames between two frames of interest. Our method predicts two relative transformations between the pairs of frames (I 1 , I k+2 ) and (I k+2 , I 2k+3 ). The input window is divided into two equal sequences of length k + 2 sharing a common frame. Both deduced sequences are used to compute a sparse optical flow allowing to track the trajectory of M points. Then, Gaussian heatmaps are used to describe the motion of the M points in an image-like format(see Sect. 2.2). Finaly, a Siamese architecture based on two shared weights Sequence to Vector (Seq2Vec) network takes as input the Gaussian heatmaps in addition to the first and last frames and predicts the relative transformations. In the following we detail our pipeline."
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,2.1,Sparse Optical Flow,"Given a sequence of frames I i and I i+k+1 , we aim at finding the trajectory of a set of points throughout the sequence. We choose the M most prominent points from the first frame using the feature selection algorithm proposed in "
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,2.2,Gaussian Heatmaps,"After obtaining the trajectory of M points in the sequence we only keep the first and last position of each point, which corresponds to the positions in our frames of interest. We use Gaussian heatmaps H ∈ R H×W with the same dimension as the ultrasound frames to encode these points, they are more suitable as input for the convolutional networks. For a point with a position (x 0 , y 0 ), the corresponding heatmap is defined in the Eq. 2. Thus, each of our M points are converted to a pair of heatmaps that represent the position in the first and last frames of the ultrasound sequence. These pairs concatenated with the ultrasound first and last frames form the recurrent neural network sequential input of size (M + 1, H, W, 2), where M + 1 is the number of channels (M heatmaps and one ultrasound frame), H and W are the height and width of the frames and finally 2 represents the temporal dimension."
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,2.3,Network Architecture,The Siamese architecture is based on a sequence to vector network. Our network maps a sequence of two images having M + 1 channel each to a six degrees of freedrom vector (three translations and three rotation angles). The architecture of Seq2Vec is illustrated in the Fig. 
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,2.4,Loss Function,"In the training phase, given a sequence of 2k + 3 frames in addition to their ground truth transformations T(1,k+2) , T(k+2,2k+3) and T(1,2k+3) , the Seq2Vec's weights are optimized by minimising the loss function given in the Eq. 3. The loss contains two terms. The first represents the mean square error (MSE) between the estimated transformations (T (1,k+2) , T (k+2,2k+3) ) at each corner point of the frames and their respective ground truth. The second term represents the accumulation loss that aims at reducing the error of the volume reconstruction, the effectiveness of the accumulation loss have been proven in the literature  3 Results and Discussion"
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,3.1,Dataset and Implementation Details,"To validate our method, six tracked sequences were acquired from an ex vivo swine liver. A manually manipulated IVUS catheter was used (8 Fr lateral firing AcuNav TM 4-10 MHz) connected to an ultrasound system (ACUSON S3000 HELX Touch, Siemens Healthineers, Germany), both commercially available. An electromagnetic tracking system (trakSTAR TM , NDI, Canada) was used along with a 6 DoF sensor (Model 130) embedded close to the tip of the catheter, and the PLUS toolkit  We have converted each to a vector of six degrees of freedom that corresponds to three translations in mm and three Euler angles in degrees. For each clip, relative frame to frame transformations were computed for the frames number 0, 3 and 6. The distribution of the relative transformation between the frames in our clips is illustrated in the Fig.  The data was split into train, validation and test sets by a ratio of 7:1.5:1.5. Our method is implemented in Pytorch"
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,3.2,Evaluation Metrics and Results,"The test data was used to evaluate our method, it contains 2060 clips over which our method achieved a translation error of translation of 0.449 ± 0.189 mm, and an orientation error of orientation 1.3 ± 1.5 • . We have evaluated our reconstruction with a commonly used in state-of-the-art metric called final drift error, which measures the distance between the center point of the final frame according to the real relative position and the estimated one in the sequence. On this basis, each of the following metrics was reported over the reconstructions of our method. Final drift rate (FDR): the final drift divided by the sequence length. Average drift rate (ADR): the average cumulative drift of all frames divided by the length from the frame to the starting point of the sequence. Table "
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,4.0,Conclusion,"In this paper, we proposed the first method for trackerless ultrasound volume reconstruction in the context of minimally invasive surgery. Our method does not use any additional sensor data and is based on a Siamese architecture that leverages the ultrasound image features and the optical flow to estimate relative transformations. Our method was evaluated on ex vivo porcine data and achieved translation and orientation errors of 0.449±0.189 mm and 1.3±1.5 • respectively with a fair drift error. In the future work, we will extend our work to further improve the volume reconstruction and use it to register a pre-operative CT image in order to provide guidance during interventions. Aknowledgments. This work was partially supported by French state funds managed by the ANR under reference ANR-10-IAHU-02 (IHU Strasbourg)."
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,1.0,Introduction,"Magnetic Resonance Imaging (MRI) is the primary management tool for brain disorders  Super-resolution (SR) techniques promise to enhance the spatial resolution of LR-MRI and restore tissue contrast. Traditional SR methods, e.g., bicubic interpolation  Earlier DL-based SR methods  Conditional diffusion models are a class of deep generative models that have achieved competitive performance in natural image SR  However, current diffusion-based SR methods are mainly single-contrast models. Several challenges remain for developing multi-contrast methods: 1) Integrating multi-contrast MRI into diffusion models increases the number of conditions. Traditional methods integrate multiple conditions via concatenation, which may not effectively leverage complementary information in multiple MRI contrasts, resulting in high-redundancy features for SR; 2) The noise and outliers in MRI can compromise the performance of standard diffusion models that use Mean Squared Error (MSE) loss to estimate the variational lower bound, leading to suboptimal results; 3) Diffusion models are often large-scale, and so are primarily intended for the generation of 2D images, i.e., treating MRI slices separately. Varied anatomical complexity across MRI slices can result in inconsistent diffusion processes, posing a challenge to efficient learning of SR-relevant features. To address the challenges, we propose a novel conditional disentangled diffusion model (DisC-Diff). To the best of our knowledge, this is the first diffusionbased multi-contrast SR method. The main contribution of our work is fourfold: -We propose a new backbone network disentangled U-Net for the conditional diffusion model, a U-shape multi-stream network composed of multiple encoders enhanced by disentangled representation learning. -We present a disentanglement loss function along with a channel attentionbased feature fusion module to learn effective and relevant shared and independent representations across MRI contrasts for reconstructing SR images. -We tailor a Charbonnier loss  Our extensive experiments on the IXI and in-house clinical datasets demonstrate that our method outperforms other state-of-the-art methods."
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,2.1,Overall Architecture,"The proposed DisC-Diff is designed based on a conditional diffusion model implemented in  For sufficiently large T , the perturbed HR x T can be considered a close approximation of isotropic Gaussian distribution. On the other hand, the reverse diffusion process p aims to generate a new HR image from x T . This is achieved by constructing the reverse distribution p θ (x t-1 | x t , y, v), conditioned on its associated LR image y and MRI contrast v, expressed as follows: where p θ denotes a parameterized model, θ is its trainable parameters and σ 2 t can be either fixed to t t=0 β t or learned. It is challenging to obtain the reverse distribution via inference; thus, we introduce a disentangled U-Net parameterized model, shown in Fig.  Disentangled U-Net. The proposed Disentangled U-Net is a multi-stream net composed of multiple encoders, separately extracting latent representations. We first denote the representation captured from the HR-MRI x t as Z xt ∈ R H×W ×2C , which contains a shared representation S xt and an independent representation I xt (both with 3D shape H × W × C) extracted by two 3 × 3 convolutional filters. The same operations on y and v yield S y , I y and S v , I v , respectively. Effective disentanglement minimizes disparity among shared representations while maximizing that among independent representations. Therefore, S xt/y/v are as close to each other as possible and can be safely reduced to a single representation S via a weighted sum, followed by the designed Squeezeand-Excitation (SE) module (Fig.  where α t = 1β t and ᾱt = t s=0 α s ."
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,2.2,Design of Loss Functions,"To effectively learn disentangled representations with more steady convergence in model training, a novel joint loss is designed in DisC-Diff as follows. Disentanglement Loss. L disent is defined as a ratio between L shared and L indep , where L shared measures the L 2 distance between shared representations, and L indep is the distance between independent representations: Charbonnier Loss. L charb is a smoother transition between L 1 and L 2 loss, facilitating more steady and accurate convergence during training  where γ is a known constant. The total loss is the weighted sum of the above two losses: where λ 1 , λ 2 ∈ (0, 1] indicate the weights of the two losses."
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,2.3,Curriculum Learning,"Our curriculum learning strategy improves the disentangled U-Net's performance on MRI data with varying anatomical complexity by gradually increasing the difficulty of training images, facilitating efficient learning of relevant features. All MRI slices are initially ranked based on the complexity estimated by Shannon-entropy values of their ground-truth HR-MRI, denoted as an ordered set E = {e min , . . . , e max }. Each iteration samples N images whose entropies follow a normal distribution with e min < μ < e max . As training progresses, μ gradually increases from e min to e max , indicating increased complexity of the sampled images. The above strategy is used for the initial M iterations, followed by uniform sampling of all slices."
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,3.0,Experiments and Results,"Datasets and Baselines. We evaluated our model on the public IXI dataset We compare our method with three single-contrast SR methods (bicubic interpolation, EDSR  Quantitative Comparison. The results show that DisC-Diff outperforms other evaluated methods on both datasets at 2× and 4× enlargement scales. Specifically, on the IXI dataset with 4× scale, DisC-Diff achieves a PSNR increment of 1.44 dB and 0.82 dB and an SSIM increment of 0.0191 and 0.0134 compared to state-of-the-art single-contrast and multi-contrast SR methods  Visual Comparison and Uncertainty Estimation. Figure "
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,4.0,Conclusion,"We present DisC-Diff, a novel disentangled conditional diffusion model for robust multi-contrast MRI super-resolution. While the sampling nature of the diffusion model has the advantage of enabling uncertainty estimation, proper condition sampling is crucial to ensure model accuracy. Therefore, our method leverages a multi-conditional fusion strategy based on representation disentanglement, facilitating a precise and high-quality HR image sampling process. Also, we experimentally incorporate a Charbonnier loss to mitigate the challenge of MRI noise and outliers on model performance. Future efforts will focus on embedding DisC-Diff's diffusion processes into a compact, low-dimensional latent space to optimize memory and training. We plan to integrate advanced strategies (e.g., DPM-Solver++ "
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,1.0,Introduction,"Magnetic resonance imaging (MRI) is critical to the diagnosis, treatment, and follow-up of brain tumour patients  management  Despite the success, GAN-based models are challenged by the limited capability of adversarial learning in modelling complex multi-modal data distributions  Diffusion model (DM) has achieved state-of-the-art performance in synthesizing natural images, promising to improve MRI synthesis models. It shows superiority in model training  However, current DM-based methods focus on one-to-one MRI translation, promising to be improved by many-to-one methods, which requires dedicated design to balance the multiple conditions introduced by multi-modal MRI. Moreover, as most DMs operate in original image domain, all Markov states are kept in memory  We propose a DM-based multi-modal MRI synthesis model, CoLa-Diff, which facilitates many-to-one MRI translation in latent space, and preserve anatomical structure with accelerated sampling. Our main contributions include: -present a denoising diffusion probabilistic model based on multi-modal MRI. As far as we know, this is the first DM-based many-to-one MRI synthesis model. -design a bespoke architecture, e.g., similar cooperative filtering, to better facilitate diffusion operations in the latent space, reducing the risks of excessive information compression and high-dimensional noise. -introduce structural guidance of brain regions in each step of the diffusion process, preserving anatomical structure and enhancing synthesis quality. -propose an auto-weight adaptation to balance multi-conditions and maximise the chance of leveraging relevant multi-modal information."
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,2.0,Multi-conditioned Latent Diffusion Model,"Figure  The t-th intermediate representation is denoted as κ t , expressed as: where ᾱt = t i=1 α i , α i denotes hyper-parameters related to variance. The reverse diffusion is modelled by a latent space network with parameters θ, inputting intermediate perturbed feature maps κ t and y (compressed b) to predict noise level θ (κ t , t, y) for recovering feature maps κt-1 from previous, To enable effective learning of the underlying distribution of κ 0 , the noise level needs to be accurately estimated. To achieve this, the network employs similar cooperative filtering and auto-weight adaptation strategies. κ0 is recovered by repeating Eq. 2 process for t times, and decoding the final feature map to generate synthesis images x0 ."
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,2.1,Latent Space Network,"We map multi-condition to the latent space network for guiding noise prediction at each step t. The mapping is implemented by N transformer blocks (Fig.  (3) To mitigate the excessive information losses that latent spaces are prone to, we replace the simple convolution operation with a residual-based block (three sequential convolutions with kernels 1 * 1, 3 * 3, 1 * 1 and residual joins  Similar Cooperative Filtering. The approach has been devised to filter the downsampled features, with each filtered feature connected to its respective upsampling component (shown in Fig.  A and high frequency components A , where i is the number of wavelet transform layers. Previous work "
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,2.2,Structural Guidance,"Unlike natural images, medical images encompass rich anatomical information. Therefore, preserving anatomical structure is crucial for MRI generation. However, DMs often corrupt anatomical structure, and this limitation could be due to the learning and sampling processes of DMs that highly rely on the probability density function  The combined loss function for our multi-conditioned latent diffusion is defined as where KL is the KL divergence loss to measure similarity between real q and predicted p θ distributions of encoded images. where D KL is the KL divergence function."
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,2.3,Auto-Weight Adaptation,"It is critical to balance multiple conditions, maximizing relevant information and minimising redundant information. For encoded conditions y ∈ R h×w×c , c is the number of condition channels. Set the value after auto-weight adaptation to ỹ, the operation of this module is expressed as (shown in Fig.  The embedding outputs are adjusted by embedding weight μ. The autoactivation is governed by the learnable weight ν and bias o.  where is a small constant added to the equation to avoid the issue of derivation at the zero point. The normalization method can establish stable competition between channels, G = {G c } S c=1 . We use L 2 normalization for cross-channel operations: where S denotes the scale. We use an activation mechanism for updating each channel to facilitate the maximum utilization of each condition during diffusion model training, and further enhance the synthesis performance. Given the learnable weight which gives new representations ỹc of each compressed conditions after the automatic weighting. S(•) denotes the Sigmoid activation function."
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,3.1,Comparisons with State-of-the-Art Methods,"Datasets and Baselines. We evaluated CoLa-Diff on two multi-contrast brain MRI datasets: BRATS 2018 and IXI datasets. The BRATS 2018 contains MRI scans from 285 glioma patients. Each includes four modalities: T1, T2, T1ce, and FLAIR. We split them into (190:40:55) for training/validation/testing. For each subject, we automatically selected axial cross-sections based on the perceptible effective area of the slices, and then cropped the selected slices to a size of 224 × 224. The IXI The noise variances were in the range of β 1 = 10 -4 and β T = 0.02. An exponential moving average (EMA) over model parameters with a rate of 0.9999 was employed. The model is trained on 2 NVIDIA RTX A5000, 24 GB with Adam optimizer on PyTorch. An acceleration method "
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,3.2,Ablation Study and Multi-modal Exploitation Capabilities,"We verified the effectiveness of each component in CoLa-Diff by removing them individually. We experimented on BRATS T1+T1ce+FLAIR→T2 task with four absence scenarios (Table  To test the generalizability of CoLa-Diff under the condition of varied inputs, we performed the task of generating T2 on two datasets with progressively increasing input modalities (Table "
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,4.0,Conclusion,"This paper presents CoLa-Diff, a DM-based multi-modal MRI synthesis model with a bespoke design of network backbone, similar cooperative filtering, structural guidance and auto-weight adaptation. Our experiments support that CoLa-Diff achieves state-of-the-art performance in multi-modal MRI synthesis tasks. Therefore, CoLa-Diff could serve as a useful tool for generating MRI to reduce the burden of MRI scanning and benefit patients and healthcare providers."
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,1.0,Introduction,"End-to-end convolutional neural networks (CNNs) have shown remarkable performance compared to classical algorithms  Building image priors through generative models has recently become a popular approach in the field of image SR, for both computer vision  In this study, we propose solving the MRI SR problem by building powerful, 3D-native image priors through a recently proposed HR image generative model, the latent diffusion model (LDM) "
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,1.1,Related Work,MRI Super-Resolution. End-to-end deep training  Solving Inverse Problems Using Generative Models. A common way to solve the inverse problem using an LDM is to use the encoder E to first encode the given image x into the latent space z 0 = E(x)  Our work is also similar to the optimization-based generative adversarial network (GAN) inversion approach 
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,2.0,Methods,3D Brain Latent Diffusion Models. We leverage a state-of-the-art LDM 
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,,Deterministic DDIM Sampling.,"In order to obtain a latent representation z T capable of reconstructing a given noisy sample into a high-resolution image, we employ deterministic DDIM sampling  where α 1:T ∈ (0, 1] T is a time-dependent decreasing sequence, zt- represents the ""predicted x 0 "", and √ 1α t-1 • θ (z t , C, t) can be understood as the ""direction pointing to x t ""  Corruption Function f . We assume a corruption function f known a-priori that is applied on the HR image x obtained from the generative model, and compute the loss function based on the corrupted image f • x and the given LR input image I. In clinical practice, a prevalent method for acquiring MR images is prioritizing high in-plane resolution while sacrificing through-plane resolution to expedite the acquisition process and reduce motion artifacts "
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,,InverseSR(LDM):,"In the case of high sparsity MRI SR, we optimize the noise latent code z * T and its associated conditional variables C * to restore the HR image from the given LR input image I using the optimization method: where DDIM(z T , C, T ) represents T deterministic DDIM sampling steps on the latent z 0 in Eq. 2. We follow the brain LDM model to use the perceptual loss L perc and the L1 pixelwise loss. The loss function is computed on the corrupted image generated from the generative model and the given LR input. A detailed pseudocode description of this method can be found in Algorithm 1."
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,,InverseSR(Decoder):,"For low sparsity MRI SR, we directly find the optimal latent code z * T using the decoder D:"
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,3.0,Experimental Design,"Dataset for Validation: We use 100 HR T1 MRIs from the IXI dataset (http://brain-development.org/ixi-dataset/) to validate our method, after filtering out those scans where registration failed. We note that subjects in the IXI dataset are around 10 years younger on average than those in UK Biobank. The MRI scans from UK Biobank also had the faces masked out, while the scans from IXI did not. This caused the faces of our reconstructions to appear blurred."
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,,Implementation:,"Conditional variables are all initialized to 0.5. Voxels in all input volumes are normalized to [0,1]. When sampling the pre-trained brain LDM with the DDIM sampler, we run T = 46 timesteps due to computational limitations on our hardware. For InverseSR(LDM), z T is initialized with random gaussian noise. For InverseSR(Decoder), we compute the mean latent code z0 as z0 = S i=1 1 S DDIM(z i T , C, T ) by first sampling S = 10, 000 z i T samples from N (0, I), then passing them through the DDIM model. N = 600 gradient descent steps are used for InverseSR(LDM) to guarantee converging (Algorithm 1, line 5). 600 optimization steps are also utilized in InverseSR(Decoder). We use the Adam optimizer with α = 0.07, β 1 = 0.9 and β 2 = 0.999."
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,4.0,Results,Figure  Table 
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,5.0,Limitations,"One key limitation of our method is the need for large computational resources to perform the image reconstruction, in particular the long Markov chain of sampling steps required by the diffusion model to generate samples. An entire pass through the diffusion model (lines 6-8 in Algorithm 1) is required for every step in the gradient descent method. Another limitation of our method is that it is limited by the capacity and output heterogeneity of the LDM generator. "
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,6.0,Conclusions,"In this study, we have developed an unsupervised technique for MRI superresolution. We leverage a recent pre-trained Brain LDM  Experimental results have shown that our approach achieves superior performance compared to the unsupervised baselines, and could create smooth HR images with fine detail even on an external dataset (IXI). Experiments in this paper focus on slice imputation, but our method could be adapted to other MRI under-sampling problems by implementing different corruption functions f . For instance, for reconstructing k-space under-sampled MR images, a new corruption function could be designed by first converting the HR image into k-space, then masking a chosen set of k-space measurements, and then converting back to image space. Instead of estimating a single image, future work could also estimate a distribution of reconstructed images through either variational inference (like the BRGM model "
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,1.0,Introduction,"Currently, the golden standard of gastrointestinal (GI) examination is endoscope screening, which can provide direct vision signals for diagnosis and analysis. Benefiting from its characteristics of being non-invasive, painless, and low physical burden, wireless capsule endoscopy (WCE) has the potential to overcome the shortcomings of conventional endoscopy "
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,,Angiectasia Erosion,"Foreign body Angiectasia Erosion Foreign body Many traditional algorithms (e.g., intensity transformation  Recently, denoising diffusion probabilistic model (DDPM)  -We design a Low-Light image enhancement framework for Capsule endoscopy (LLCaps "
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,2.1,Preliminaries,Multi-scale Residual Block. Multi-scale Residual Block (MSRB)  Denoising Diffusion Probabilistic Models. Denoising Diffusion Probabilistic Models (DDPMs) 
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,2.2,Proposed Methodology,"Curved Wavelet Attention. Curved Wavelet Attention (CWA) block is the core component of our CNN branch, which is constructed via a curved dual attention mechanism and wavelet transform, as shown in Fig.  However, literature  The detailed CurveA layer is presented in the top of Fig.  Reverse Diffusion Process. Some works  In our formulation, we assume that i 0 is the learning target Y * and i T is the output shallow image from the CNN branch. Therefore, we only need to engage the reverse process in our LLIE task. The reverse process is modeled using a Markov chain: p θ (i t-1 | i t ) are parameterized Gaussian distributions whose mean µ θ (i t , t) and variance Σ θ (i t , t) are given by the trained network. Meanwhile, we simplify the network and directly include the reverse diffusion process in the end-toend training of the entire network. Shallow output is therefore optimized by the reverse diffusion branch to get the predicted image Y . We further simplify the optimization function and only employ a pixel-level loss on the final output image, which also improves the training and convergence efficiency."
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,,Overall Network,"Architecture. An overview of our framework can be found in Fig.  The shallow output F OP M ∈ R 3×W ×H shall further be propagated through the reverse diffusion process and achieve the final enhanced image Y ∈ R 3×W ×H . The whole network is constructed in an end-to-end mode and optimized by Charbonnier loss  in which Y and Y * denote the input and ground truth images, respectively."
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,3.1,Dataset,"We conduct our experiments on two publicly accessible WCE datasets, the Kvasir-Capsule  Red Lesion Endoscopy dataset "
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,3.2,Implementation Details,We compare the performance of our LLCaps against the following state-of-theart (SOTA) LLIE methodologies: LIME 
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,3.3,Results,"We compare the performance of our LLCaps to the existing approaches, as demonstrated in Table  our method on the Kvasir-Capsule and RLE datasets are visualized in Fig.  Furthermore, a downstream red lesion segmentation task is conducted to investigate the usefulness of our LLCaps on clinical applications. As illustrated in Table  Besides, an ablation study is conducted on the Kvasir-Capsule dataset to demonstrate the effectiveness of our design and network components, as shown in Table  Table "
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 4.
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,1.0,Introduction,"CINE Cardiac Magnetic Resonance (CMR) imaging is widely recognized as the gold standard for evaluating cardiac morphology and function  On a different principle, k-space interpolation methods first attempt to estimate the full k-space leveraging redundancy in sampled frequency components before Fourier transform. Image domain methods rely on artifacts-specific image priors to denoise the corrupted pixels, making them susceptible to variability in artifact types arising from different undersampling factors. Unlike image domain methods, k-space-based methods have a consistent task of interpolating missing data from reliable sampled ones, even though the undersampling factor may vary. This makes k-space interpolation methods simple, robust and generic over multiple undersampling factors. In this work, we are interested in learning an entirely k-space-based interpolation for the Cartesian undersampled dynamic MR data. An accurate learnable k-space interpolator can be achieved via (1) a rich representation of the sampled k-space data, which can facilitate the exploitation of the limited available samples, and (2) global dependency modeling of k-space to interpolate unsampled data from the learned representation. Modeling global dependencies are beneficial because a local structure in the image domain is represented by a wide range of frequency components in k-space. Furthermore, in the context of dynamic MR, the interpolator also has to exploit temporal redundancies. In the recent past, masked image modeling  1. We propose a novel k-space Global Interpolation Network, termed k-GIN, leveraging masked image modeling for the first time in k-space. To the best of our knowledge, our work enables the first Transformer-based k-space interpolation for 2D+t MR reconstruction. 2. Next, we propose k-space Iterative Refinement Module, termed k-IRM that refines k-GIN interpolation by efficiently gathering spatio-temporal redundancy of the MR data. Crucially, k-IRM specializes in learning high-frequency details with the aid of customized High-Dynamic-Range (HDR) loss. 3. We evaluate our approach on 92 in-house CMR subjects and compare it to model-based reconstruction baselines using image priors. Our experiments show that the proposed k-space interpolator outperforms baseline methods with superior qualitative and quantitative results. Importantly, our method demonstrates improved robustness and generalizability regarding varying undersampling factors than the model-based counterparts."
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,2.0,Related Work,Reconstruction with image priors is broadly used with either image-only denoising  k-space-Domain Interpolation. Methods include works  Hybrid Approaches. Combine information from both k-space and imagedomain. KIKI-Net 
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,3.0,Method,"Fully sampled complex 2D+t dynamic MR k-space data can be expressed as y ∈ C XY T , X and Y are the height (k x ) and the width (k y ) of the k-space matrix, and T is the number of frames along time. In this work, we express k-space as 2 channels (real and imaginary) data y ∈ R 2XY T . For the MR acquisition, a binary Cartesian sampling mask M ∈ Z Y T |M ij ∈ {0, 1} is applied in the k y -t plane, i.e. all k-space values along the k x (readout direction) are sampled if the mask is 1, and remains unsampled if the mask is 0. Figure "
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,3.1,k-space Global Interpolation Network (k-GIN),"In our proposed approach, we work on the k y -t plane and consider k x as the channel dimension. Further, we propose each point in the k y -t plane to be an individual token. In total, we have Y T number of tokens, out of which Y T/R are sampled tokens for an undersampling factor of R. Our objective is to contextualize global dependencies among every sampled token. For that, we use a ViT/MAE "
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,3.2,k-space Iterative Refinement Module (k-IRM),Using 1 loss in k-GIN makes it focus more on the low-frequency components learning but the high-frequency estimation is still sub-optimal. Inspired by the iterative refinement strategy  Inspired by  where s(•) is the stop-gradient operator preventing the network back-propagation of estimation in the denominator and controls the operational range of the logarithmic approximation.
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,3.3,Inference,"The inference is identical to the training till obtaining the refined output from the k-IRM. Then we replace k-space estimation at the sampled position with ground-truth k-space values, ensuring the data-consistency. Note that this step is not done during the training as it deteriorates learning k-space representation. Once full k-space has been estimated, we use Fourier transform to obtain the image reconstruction during the inference. Note that image reconstruction is not needed during the training since our framework is entirely based in k-space."
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,4.0,Data and Experiments,"Dataset. The training was performed on 81 subjects (a mix of patients and healthy subjects) of in-house acquired short-axis 2D CINE CMR, whereas testing was carried out on 11 subjects. Data were acquired with 30/34 multiple receiver coils and 2D balanced steady-state free precession sequence on a 1.5T MR (Siemens Aera with TE=1.06 ms, TR=2.12 ms, resolution=1.9×1.9mm 2 with 8mm slice thickness, 8 breath-holds of 15 s duration). The MR data were acquired with a matrix size of 192×156 with 25 temporal cardiac phases (40ms temporal resolution). Afterwards, these data were converted to single-coil MR imaging and k-space data using coil sensitivity maps, simulating a fully sampled single-coil acquisition. A stack of 12 slices along the long axis was collected, resulting in 415/86 image sequence (2D+t) for training/test. Implementation Details. We use an NVIDIA A6000 GPU to train our framework. The batch size was set to 1 with a one-cycle learning-rate scheduler (max. learning rate 0.0001). We use 8 layers, 8 heads and 512 embedding dimensions for all of our Transformer blocks. We train our network with joint 1 and HDR-loss with tuned to 0.5. Training and inference were carried out on retrospectively undersampled images with masks randomly generated by VISTA  Baseline Methods and Metrics. We compare the proposed framework with three single-coil MR reconstruction methods that apply image priors: TVnorm Optimization (TV-Optim) which is widely used in reconstruction "
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,5.0,Results and Discussion,"The quantitative results in Table  Outlook. Previous works  Limitation. We also acknowledge some limitations of the work. First, we have not evaluated our method on prospectively collected data, which would be our focus in future work. Second, the current study only investigates the single coil setup due to hardware memory limitations. In the future, we will address the multi-coil scenario by applying more memory-efficient Transformers backbones e.g. "
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,6.0,Conclusion,"In this work, we proposed a novel Transformer-based method with mask image modeling to solve the dynamic CMR reconstruction by only interpolating the kspace without any image-domain priors. Our framework leverages Transformers' global dependencies to exploit redundancies in all three k x -, k y -and t-domain. Additionally, we proposed a novel refinement module (k-IRM) to boost highfrequency learning in k-space. Together, k-GIN and k-IRM not only produce high-quality k-space interpolation and superior CMR reconstruction but also generalize significantly better than baselines for higher undersampling factors."
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_22.
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,1.0,Introduction,"Image registration is critical for estimating cardiac motion, aiming to estimate the displacements between cardiac anatomical tissues at different time points. Generative models focus on data distribution and tend to model the underlying patterns or data distribution. They make it possible to train the network using fewer data, improve robustness when data is missing, and, most importantly, allow quantifying the uncertainty associated with the output  Gan et al.  To address the above issues, we propose a probabilistic model based on variational Bayesian using non-uniformly spaced control points for cardiac image registration. Details of our contributions include: -Employing nonuniformly spaced control points in a variational Bayesian image registration model improves registration accuracy. The control points are spaced on the contours of objects, and their intensity and spatial features are extracted using a network. We addressed the inherent disorder challenge in the control-points-based image registration model using CNNs, which can locate control points freely instead of only on grids. Additionally, our approach is not sensitive to location errors of control points. -The global prior is partitioned into several independent priors, which correspond to different control points. We analyzed the KL divergence between the variational posterior and the factorized prior in theory and found that properly factorized priors can close the gap with the variational posterior and increase the evidence of a lower bound in the VB model. -Our approach can provide more available information about registration uncertainty. Our uncertainty maps concentrate on the boundaries of objects instead of spreading over everywhere. It is favorable in real applications, where surgeons only pay attention to regions of interest."
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,2.1,Posterior Estimation and Prior in Variational Registration Model,"Given the source image S and the target image T , the goal of image registration is estimating the spatial transformation f z : R d → R d between S and T . The VB model used a variational posterior q(z|T, S) to approximate the intractable registration posterior p(z|T, S) by maximizing the evidence lower bound (ELBO) L(T, S) of log-likelihood p(T, S), In Eq. (  ), where z = {z i } n i=1 is the latent variable, u is a pixel. ψ is the CSRBF with support r. The value of r is obtained by the distance between control points  is the distribution parameter of the kth element of the latent variable z. Control points {p i } n i=1 influence the DVF greatly. The network NetGI proposed by Gan et al.  Since GPs and NuPs are used, the latent variable can be represented as z = z u z nu , where z u and z nu correspond to GPs and NuPs, respectively. Distribution parameters of z u and z nu , denoted as (μ u , σ 2 u ) and (μ nu , σ 2 nu ), respectively, are estimated by a VAE. Since NuPs locate disorderly, it is challenging for CNNs to extract corresponding features. A specially designed VAE network NuNet  deals with this issue, as shown in Fig.  Gan et al. "
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,2.2,Registration Uncertainty,"Our registration network predicts the variance of latent variables, which corresponds to the deviation of parameters of elastic transformation. It is a kind of data uncertainty. We estimate the uncertainty of DVF using its variance. The displacement of pixel u is , where A u is the local region centered at u with radius r. We found that σ 2 i of NuPs is larger than that of GPs in a statistical sense. The reason is that NuPs locate at the boundaries of objects, where large displacements occur in these areas for cardiac motion. However, cardiac motion varies subject to subject, resulting in the different displacements of points located at the boundaries of LV or RV. On the contrary, GPs distribute uniformly and locate mainly in the background with small motion in general. Correspondingly, the displacement variances of GPs are relatively small compared with that of NuPs. Moreover, when the pixel u is close to NuPs, ψ( u-pi r ) 2 is relatively large. Then, it can be concluded that the region where the NuPs are gathered generally has significant variances, such as the corner of the RV and thin myocardium. On the contrary, the regions with sparse control points, such as the background, usually have low uncertainty."
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,3.1,Datasets and Implement Details,"Four public datasets are used to evaluate our NuNet in experiments, including the York dataset "
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,3.2,Registration Results,"To compare the performance of our proposed approach, five deep learning networks, KrebsDiff  Registration results of different networks on two datasets are listed in Table "
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,3.3,Uncertainty,"We provide uncertainty using different hyperparameters λ for NetGI and our NuNet, as shown in Fig. "
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,3.4,Ablation Study,"To verify the effectiveness of the different modules of our network, we employ different variants of our network to conduct an ablation study. ""GPs"", ""LPs"",  ""FN"" and ""PN"" represent global control points, local control points, FeatureNet, and PointNet, respectively. Three priors with different covariance matrices are compared. The experimental ablation results are listed in Table "
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,4.0,Conclusion,This paper addressed the issue of non-uniformly spaced control points in the VB-based image registration model for cardiac motion estimation. We employed the FPS algorithm to sample control points from the contour of the heart. The PointNetis embedded in our network to learn the intensity and spatial features. We found that the factorized prior leads to small KL divergence and is beneficial to produce more flexible DVFs. Experimental results on four datasets show that our proposed approach achieves optimal performance compared to state-ofart networks. The uncertainty estimated by our network focuses on important regions and provides more information about uncertainty in applications.
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 60.
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,1.0,Introduction,"Magnetic Resonance Imaging (MRI) is widely recognized as a pivotally important neuroimaging technique, which provides rich information about brain tissue anatomy in a non-invasive manner. Several studies have shown that multi-modal MR images offer complementary information on tissue morphology, which help conduct more comprehensive brain region analysis  Relevant literature in this area can be divided into cross-modality synthesis (CMS), super-resolution (SR), or cross-modality super-resolution (CMSR). Most of the works concerning CMS model modality-invariant and coherent high-frequency details to translate between modalities. For example, pGAN  However, these works have two main limitations. On the one hand, these works are limited to specified fields, and cannot be flexibly switched to other tasks or situations. For instance, CMS models described above will fail to produce consistent 3D results due to the lack of modeling inter-plane correlations. Moreover, CMSR models only produce MR images with fixed thickness, which may constrain their further applications. On the other hand, high-frequency details with alias frequencies are often treated carelessly in these generative networks. This may lead to unnatural details and even aliasing artifacts, especially for the ill-posed inverse tasks (SR or CMSR) that require restoring high-frequency details based on low-resolution images. In this paper, we propose an Alias-Free Co-Modulated network (AFCM) to address the aforementioned issues. AFCM is inspired by the recent advances in foundation models "
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,2.1,Co-modulated Network,"Here we propose a novel style-based co-modulated network to accomplish CMS, SR, and CMSR consistently. As shown in Fig.  Different from original stochastic style representation, we design both positionembedded representation and image-conditioned representation as the modulations, which control the generation process consistently. Remind that we aim to translate low-resolution MR images x i with modality i and thickness p into high-resolution MR images y j with modality j and thickness q. The positionembedded representation w is transformed from the latent code l ∼ N (0, I) and the relative slice index δ = kq/p -kq/p that controls the position k for each output slice y k j . This is accomplished with a mapping network M where the relative slice index is embedded using the class-conditional design  where n = kq/p ) with the encoder E. In this paper, m is experientially set to 2 following  The style vector s is then used to modulate the weights of the convolution kernels in the synthesis network D "
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,2.2,Alias-Free Generator,"We notice that results produced by the vanilla generator are contaminated by ""texture sticking"", where several fine-grained details are fixed in pixel coordinates when δ changes, as illustrated in our ablation study. The phenomenon was found to originate from aliasing caused by carelessly designed operators (i.e., convolution, resampling, and nonlinearity) in the network  where X r is the two-dimensional Dirac comb with sampling rate r , and φ r is the interpolation filter with a bandlimit of r/2 so that z can be represented as φ r * Z. The operator denotes pointwise multiplication and * denotes continuous convolution. The operation F is alias-free when any frequencies higher than r /2 in the output signal, also known as alias frequencies, are efficiently suppressed. In this way, we re-design the generator by incorporating the aliasfree mechanism based on the analysis above, which consists of the antialiasing decoder and encoder which are further illustrated as follows. Antialiasing Decoder. Considering that any details with alias frequencies need to be suppressed, the resampling and nonlinearity operators in the decoder are carefully redesigned following Alias-Free GAN  Antialiasing Encoder. Given that our network has a U-shaped architecture, the encoder also needs to be alias-free so that the skip connections would not introduce extra content with undesired frequency. The encoder consists of 14 layers, each of which is further composed of a convolution, a nonlinear operation, and a downsampling filter. The parameters of the filter are designed to vary with the resolution of the corresponding layer. Specifically, the cutoff frequency geometrically decreases from f c = r N /2 in the first non-critically sampled layer to f c = 2 in the last layer, where r N is the image resolution. The minimum acceptable stopband frequency starts at f t = 2 0.3 • r N /2 and geometrically decreases to f t = 2 2.1 , whose value determines the resolution r = min(ceil(2 • f t ), r N ) and the transition band half-width f h = max(r/2, f t ) -f c . The target feature map of each layer is surrounded by a 10-pixel margin, and the final feature map is resampled to 4 × 4 before formulating the image-conditioned representation."
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,2.3,Optimization,"The overall loss is composed of an adversarial loss and a pixel-wise L 1 loss: where λ is used to balance the losses. The adversarial loss is defined as nonsaturation loss with R 1 regularization, and the discriminator preserves the architecture of that in StyleGAN2 "
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,3.1,Experimental Settings,"Datasets. We evaluate the performance of AFCM on three brain MRI datasets: CSDH is an in-house dataset comprising 100 patients diagnosed with chronic subdural hematoma (CSDH). It includes T1-weighted images with spatial resolution of 0.75×0.75×1 mm 3 and T2-weighted flair images with spatial resolution of 0.75 × 0.75 × 8 mm 3 . Pixel-wise annotations of liquefied blood clots made by an experienced radiologist are used for segmentation accuracy evaluation. ADNI Implementation Details. We use the translation equivariant configuration of the Alias-Free GAN, and discard the rotational equivariance to avoid generating overly-symmetric images "
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,3.2,Comparative Experiments,"Cross-Modality Synthesis. In this section, we report three subtasks, namely CMS from T1 to T2, T2 to PD, and PD to T1-weighted MR images on the IXI dataset in Table  Cross-Modality and Super-Resolution. As previously addressed, it is more challenging to implement CMS and SR simultaneously. One possible approach is to perform CMS and SR in a two-stage manner, which is accomplished with a combination of ResViT and DeepResolve. We also compare our approach with SynthSR "
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,4.0,Conclusion,"We propose a novel alias-free co-modulated network for CMS, SR, and CMSR of MR images. Our method addresses the problems of task inconsistency between CMS and SR with a novel co-modulated design, and suppresses aliasing artifacts by a redesigned alias-free generator. AFCM is also flexible enough to reconstruct MR images with various non-integer target thickness. The experiments on three independent datasets demonstrate our state-of-the-art performance in CMS, SR, and CMSR of MR images."
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_7.
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,1.0,Introduction,"Computed tomography (CT) is a prevalent imaging modality with applications in biology, disease diagnosis, interventional imaging, and other areas. Highresolution CT (HRCT) is beneficial for clinical diagnosis and surgical planning because it can provide detailed spatial information and specific features, usually employed in advanced clinical routines  While DL-based methods can generate promising results, there can still be geometric distortions and artifacts along with structural edges in the superresolved results  In this paper, we propose a novel dual-stream conditional diffusion model for CT scan super-resolution to generate UHRCT results with high image quality and structure fidelity. The conditional diffusion model takes the form p(y|x), where x is the LRCT, and y is the targeted UHRCT  1) We proposed a novel dual-stream diffusion model framework for CT superresolution. The framework incorporates a dual-stream structure-preserving network in the denoising process to realize better physiological structure restoration. 2) We designed a new image enhancement operator to model the vascular and blob structures in medical images. To avoid non-derivative operations in image enhancement, we proposed a novel enhancement module consisting of lightweight convolutional layers to replace the filtering operation for faster and easier back-propagation in structural domain optimization. 3) We established an ultra-high-resolution CT scan dataset with a spatial resolution of 0.34 × 0.34 mm 2 and an image size of 1024 × 1024 for training and testing the SR task. 4) We have conducted extensive experiments and demonstrated the excellent performance of the proposed SR methods in both the image and structure domains. In addition, we have evaluated our proposed method on high-level tasks, including vascular-system segmentation and lesion detection on the SRCT, indicating the reliability of our SR results. "
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,2.1,Dual-Stream Diffusion Model for Structure-Preserving Super-Resolution,"To preserve the structure and topology relationship in the denoising progress, we designed a novel Dual-Stream Diffusion Model (DSDM) for better superresolution and topology restoration (Fig.  The optimization is defined as In the denoising process, we used a dual-stream structure-preserving (DSSP) network for supervised structure restoration. The DSSP network optimizes the denoised results in the image domain and the structure domain, respectively. The structural domain, obtained with the image enhancement operator, is concatenated with the LRCT slice as the input of the structure branch. The final SR results are obtained after the feature fusion model between the image map and the structure map."
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,2.2,Imaging Enhancement Operator for Vascular and Blob Structure,"We introduced an enhancement operator in the DSSP network to model the vascular and blob structures, which can represent important physiological information according to clinical experience, and provide the prior structural information for the DSSP network. For one pixel T in the CT slice, let I (x) denote the imaging intensity at this point. The 2 × 2 Hessian matrix at the scale s is defined as  where G (x, s) is the 2D Gaussian kernel. The two eigenvalues of the Hessian matrix are denoted as λ = (λ 1 , λ 2 ) and here we agree that |λ 1 | <= |λ 2 |. The eigenvalues of the Hessian matrix can reflect the geometric shape, curvature, and brightness of the local images. For the blob-like structures, the three eigenvalues are about the same, λ 1 ≈ λ 2 ; for the vascular-like structures, λ 2 can be much larger than the absolute value of λ 1 , |λ 2 | >> |λ 1 |  κ 1 and κ 2 are the parameters to control the sensitivity for the vascular-like structures and blob-like structures, respectively. λ τ is the self-regulating factor. When λ 1 is about the same with λ 2 , λ τ is closed to λ 1 ; and when λ 1 is much smaller to λ 2 , λ τ is closed to λ 2 , which can achieve a balance between two conditions."
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,2.3,Objective Function,"We designed a new loss function to ensure that the final SR result can be optimized in both the image domain and the structure domain. Denoting the reference image as y t , the pixel-wise loss in the imaging domain is formulated as G image DSSP (y t ) is the recovered image from the image-domain branch. L1 loss yields a significantly higher consistency and lower diversity, while L2 loss can better capture the outliers. Here we used a parameter λ L1 to balance these two losses. In the meantime, a structure-constraint loss is also necessary to help the network achieve better performance in structure consistency. The loss function consists of two parts, which measure the consistency of the image-domain branch and the structure-domain branch. Denoting the structure-domain output as G struct DSSP (y t ), the structure-constraint loss can be presented as However, the image enhancement described above involves overly complex calculations, making back-propagation difficult in the training process. Here we utilized a convolution-based operator O Fc (•) to simplify the calculation, which consists of several lightweight convolutional layers to simulate the operation of image enhancement. In this way, we transform the complex filtering operation into a simple convolution operation, thus back-propagation can be easily processed. The loss function is then modified as The total objective function is the sum of two losses. 3 Experiments and Conclusion"
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,3.1,Datasets and Evaluation,"We constructed three datasets for framework training and evaluation; two of them were in-house data collected from two CT scanners(the ethics number is 20220359), and the other was the public Luna16 dataset  We evaluated our SR model on three CT datasets: • Dataset 1: 2D super-resolution from 256×256 to 1024×1024, with the spatial resolution from 1.36 × 1.36 mm 2 to 0.34 × 0.34 mm 2 . • Dataset 2: 3D super-resolution from 256 × 256 × 1X to 512 × 512 × 5X, with the spatial resolution from 1.60 × 1.60 × 5.00 mm 3 to 0.80 × 0.80 × 1.00 mm 3 . • Dataset 3: 2D super-resolution from 256 × 256 to 512 × 512 on the Luna16 dataset. We compare our model with other SOTA super-resolution methods, including bicubic interpolation, SRCNN "
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,3.2,Qualitative and Quantitative Results,"Qualitative comparisons are shown in Fig.  Lesion Detection and Vessel Segmentation on Super-Resolved CT. To further evaluate the information maintenance of our SR methods, we conducted some high-level tasks, including lung nodules detection and pulmonary airway and blood vessel segmentation on the super-resolved CT scans. We compared the performance of different methods on SRCT and the ground truth. For nodule detection, these methods included U-Net, V-Net "
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,4.0,Conclusion,"In this paper, we have established a dual-stream diffusion model framework to address the problem of topology distortion and artifact introduction that generally exists in the medical super-resolution results. We first propose a novel image enhancement operator to model the vessel and blob structures in the CT slice, which can provide a structure prior to the SR framework. Then, we design a dualstream diffusion model that employs a dual-stream ream structure-preserving network in the denoising process. The final SR outputs are optimized not only by convolutional image-space losses but also by the proposed structure-space losses. Extensive experiments have shown that our SR methods can achieve high performance in both image restoration and structure fidelity, demonstrating the promising performance of information preservation and the potential of applying our SR results to downstream tasks."
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 25.
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,1.0,Introduction,"Colorectal cancer (CRC) is the third most commonly diagnosed cancer and is the second most common cause of cancer death  To overcome these difficulties, we rely on two properties of endoscopic images: -Endoluminal cavities such as the gastrointestinal tract, and in particular the human colon, are watertight surfaces. To account for this, we represent its surface in terms of a signed distance function (SDF), which by its very nature presents continuous watertight surfaces. -In endoscopy the light source is co-located with the camera. It illuminates a dark scene and is always close to the surface. As a result, the irradiance decreases rapidly with distance t from camera to surface; more specifically it is a function of 1/t 2 . In other words, there is a strong correlation between light and depth, which remains unexploited to date. To take advantage of these specificities, we build on the success of Neural implicit Surfaces (NeuS)  NeuS training selects a pixel from an image and samples points along its projecting ray. However, the network is agnostic to the sampling distance. In LightNeuS, we explicitly feed to the renderer the distance of each one of these sampled points to the light source, as shown in Fig.  Our results show that exploiting the illumination is key to unlocking implicit neural surface reconstruction in endoscopy. It delivers accuracies in the range of 3 mm, whereas an unmodified NeuS is either 5 times less accurate or even fails to reconstruct any surface at all. Earlier methods "
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,2.0,Related Works,"3D Reconstruction from Endoscopic Images. It can help with the effective localization of lesions, such as polyps and adenomas, by providing a complete representation of the observed surface. Unfortunately, many state-of theart SLAM techniques based on feature matching  In monocular dense reconstructions, it is common practice to encode shape priors in terms of smooth rigid surfaces  Recent methods for dense reconstruction rely on neural networks to predict per-pixel depth in the 2D space of each image and fuse the depth maps by using multi-view stereo (MVS)  Neural Radiance Fields (NeRFs) were first proposed to reconstruct novel views of non-Lambertian objects  Neural Implicit Surfaces (NeuS) were introduced in  The SDF formulation makes it possible to estimate the surface normal as n = ∇f (x). The reflectance of a material is usually determined as a function of the incoming and outgoing light directions with respect to the surface normal. Therefore, the normal is added as an input to the MLP that estimates color c : (x, d, n), as shown in Fig. "
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,3.0,LightNeuS,"In this section, we present the key contributions that make LightNeuS a neural implicit reconstruction method suitable for endoscopy in endoluminal cavities. In this context, the light source is located next to the camera and moves with it. Furthermore, it is close to the surfaces to be modeled. As a result, for any surface point x = o+td, the irradiance decreases with the square of the distance to the camera t. Hence, we can write the color of the corresponding pixel as  where L e is the radiance emitted by the light source to the surface point, that was modeled and calibrated in the EndoMapper dataset "
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,3.1,Using Illumination Decline as a Depth Cue,"The NeuS formulation of Sect. 2 assumes distant and fixed lighting. However, in endoscopy inverse-square light decline is significant, as quantified in Eq. (  Accounting for this is done by modifying the original NeuS formulation as follows. Figure  This conceptually simple change, using illumination decline while training, unlocks all the power of neural surface reconstruction in endoscopy."
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,3.2,Endoscope Photometric Model,"Apart from illumination decline, there are several significant differences between the images captured by endoscopes and those conventionally used to train NeRFs and NeuS: fish-eye lenses, strong vignetting, uneven scene illumination, and postprocessing. Endoscopes use fisheye lenses to cover a wide field of view, usually close to 170 • . These lenses produce strong deformations, making it unwise to use the standard pinhole camera model. Instead, specific models  The light sources of endoscopes behave like spotlights. In other words, they do not emit with the same intensity in all directions, so L e in Eq. (  The post-processing software of medical endoscopes is designed to always display well-exposed images, so that physicians can see details correctly. An adaptive gain factor g is applied by the endoscope's internal logic and gamma correction is also used to adapt to non-linear human vision, achieving better contrast perception in mid tones and dark areas. Endoscope manufacturers know the post-processing logic of their devices, but this information is proprietary and not available to users. Again, gamma correction can be calibrated assuming it is constant  All these factors must be taken into account during network training. Thus, our photometric loss is computed using a normalized image: (5)"
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,4.0,Experiments,"We validate our method on the C3VD dataset  During training, we follow the NeuS paper approach of using a few informative frames per scene, as separated as possible, by sampling each video uniformly. For each sequence, we train both the vanilla NeuS and our LighNeuS using 20 frames each time. They are extracted uniformly over the duration of the video. We use the same batch size and number of iterations as in the original NeuS paper, 512 and 300k respectively. Once the network is trained, we can extract triangulated meshes from the reconstruction. Since the C3VD dataset comprises a ground-truth triangle mesh, we compute point-to-triangle distances from all the vertices in the reconstruction to the closest ground-truth triangle. In the first rows of Table  In contrast, vanilla NeuS assumes constant illumination. The strong light changes typical of endoscopy fatally mislead the method. We only report numerical results of NeuS in two sequences because in all the rest, the SDF diverges and ends up blown out of the rendering volume, giving no result at all. The NeuS reconstruction exhibits multiple artifacts that make it unusable. Bottom: Our reconstruction is much closer to the ground truth shape. The error is shown in blue if the reconstruction is inside the surface, and in red otherwise. A fully saturated red or blue denotes an error of more than 1 cm and grey denotes no error at all. We provide a qualitative result in Fig.  We hypothesize that this desirable behavior stems from the fact that the network learns an empirical shape prior from the observed anatomy of the colon. However, we don't expect this behavior to hold for distant unseen parts, but only for regions closer than 20 mm to one observation. In the last rows of Table "
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,5.0,Conclusion,"We have presented a method for 3D dense multi-view reconstruction from endoscopic images. We are the first to show that neural radiance fields can be used to obtain accurate dense reconstructions of colon sections of significant length. At the heart of our approach, is exploiting the correlation between depth and brightness. We have observed that, without it, neural reconstruction fails. The current method could be used offline for post-exploration coverage analysis and endoscopist training. But real-time performance could be achieved in the future as the new NeuS2 "
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 48.
