{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib2\n",
    "\n",
    "\n",
    "_ANO = '2013/'\n",
    "_MES = '01/'\n",
    "_MATERIAS = 'matematica/'\n",
    "_CONTEXT = 'wp-content/uploads/' + _ANO + _MES\n",
    "_URL = 'http://www.desconversa.com.br/' + _MATERIAS + _CONTEXT\n",
    "\n",
    "# functional\n",
    "r = requests.get(_URL)\n",
    "soup = bs(r.text)\n",
    "urls = []\n",
    "names = []\n",
    "for i, link in enumerate(soup.findAll('a')):\n",
    "    _FULLURL = _URL + link.get('href')\n",
    "    if _FULLURL.endswith('.pdf'):\n",
    "        urls.append(_FULLURL)\n",
    "        names.append(soup.select('a')[i].attrs['href'])\n",
    "\n",
    "names_urls = zip(names, urls)\n",
    "\n",
    "for name, url in names_urls:\n",
    "    print url\n",
    "    rq = urllib2.Request(url)\n",
    "    res = urllib2.urlopen(rq)\n",
    "    pdf = open(\"pdfs/\" + name, 'wb')\n",
    "    pdf.write(res.read())\n",
    "    pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 16:26:47 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): conferences.miccai.org:80\n",
      "2024-02-11 16:26:47 [urllib3.connectionpool] DEBUG: http://conferences.miccai.org:80 \"GET /2023/papers/ HTTP/1.1\" 200 33490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup as bs\n",
    "import regex as re\n",
    "\n",
    "# specify the URL of the archive here \n",
    "archive_url = \"http://conferences.miccai.org/2023/papers/\"\n",
    "url = \"http://conferences.miccai.org\"\n",
    "\n",
    "# functional\n",
    "r = requests.get(archive_url)\n",
    "soup = bs(r.content, 'html.parser')\n",
    "urls = []\n",
    "names = []\n",
    "for i, link in enumerate(soup.findAll('a', attrs={'href': re.compile(\"rdcu.be\")})):\n",
    "    _FULLURL = url + link.get('href')\n",
    "    if _FULLURL.endswith('.html'):\n",
    "        urls.append(_FULLURL)\n",
    "        names.append(soup.select('a')[i].attrs['href'])\n",
    "\n",
    "\n",
    "print(urls)\n",
    "names_urls = zip(names, urls)\n",
    "\n",
    "\n",
    "#for name, url in names_urls:\n",
    "#    print(url)\n",
    "#    rq = urllib2.Request(url)\n",
    "#    res = urllib2.urlopen(rq)\n",
    "#    pdf = open(\"pdfs/\" + name, 'wb')\n",
    "#    pdf.write(res.read())\n",
    "#    pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import regex as re\n",
    "\n",
    "# specify the URL of the archive here \n",
    "archive_url = \"http://conferences.miccai.org/2023/papers/\"\n",
    "url = \"http://conferences.miccai.org\"\n",
    "\n",
    "# create response object \n",
    "r = requests.get(archive_url) \n",
    "\n",
    "# create beautiful-soup object \n",
    "soup = BeautifulSoup(r.content,'html')\n",
    "\n",
    "# find all links on web-page \n",
    "links = soup.findAll('a') \n",
    "\n",
    "# filter links ending with .html \n",
    "list_of_urls = [url + link['href'] for link in links if link['href'].endswith('html')] \n",
    "# print(len(paper_links)) gives us 730 papers in total\n",
    "\n",
    "list_of_pdfs = []\n",
    "# now that we have all the links to the papers, we can go through each paper and find the pdf link\n",
    "for link in list_of_urls:\n",
    "\t# create response object per link from the total list of links\n",
    "\tr = requests.get(link) \n",
    "\tsoup = BeautifulSoup(r.content,'html')\n",
    "\n",
    "\t# find all urls with 'rdcu.be' which contains the specific paper\n",
    "\tlinks = soup.findAll(attrs={'href': re.compile(\"rdcu.be\")})\n",
    "\tpdfs =  [link['href'] for link in links]\n",
    "\tlist_of_pdfs.extend(pdfs)\n",
    "\tprint(list_of_pdfs)\n",
    "\n",
    "\n",
    "\n",
    "\tfor link in list_of_pdfs:\n",
    "\t\t'''iterate through all links in links\n",
    "\t\tand download them one by one'''\n",
    "\t\t\n",
    "\t\t# obtain filename by splitting url and getting \n",
    "\t\t# last string \n",
    "\t\tfile_name = link.split('/')[-1]\t\t\n",
    "\n",
    "\t\tprint( \"Downloading file:%s\"%file_name) \n",
    "\t\t\n",
    "\t\t# create response object \n",
    "\t\tr = requests.get(link, stream = True)\n",
    "\t\t\n",
    "\t\t# download started \n",
    "\t\twith open(file_name, 'wb') as f: \n",
    "\t\t\tfor chunk in r.iter_content(chunk_size = 1024*1024): \n",
    "\t\t\t\tif chunk: \n",
    "\t\t\t\t\tf.write(chunk) \n",
    "\t\t\n",
    "\t\tprint( \"%s downloaded!\\n\"%file_name ) \n",
    "\n",
    "\tprint (\"All links are downloaded!\") \n",
    "\t\n",
    "\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import regex as re\n",
    "\n",
    "# specify the URL of the archive here \n",
    "archive_url = \"http://conferences.miccai.org/2023/papers/\"\n",
    "url = \"http://conferences.miccai.org\"\n",
    "\n",
    "def get_url_links(): \n",
    "\t# create response object \n",
    "\tr = requests.get(archive_url) \n",
    "\t\n",
    "\t# create beautiful-soup object \n",
    "\tsoup = BeautifulSoup(r.content,'html')\n",
    "\t\n",
    "\t# find all links on web-page \n",
    "\tlinks = soup.findAll('a') \n",
    "\n",
    "\t# filter links ending with .html \n",
    "\tlist_of_urls = [url + link['href'] for link in links if link['href'].endswith('html')] \n",
    "\t# print(len(paper_links)) gives us 730 papers in total\n",
    "\n",
    "\n",
    "\t# now that we have all the links to the papers, we can go through each paper and find the pdf link\n",
    "\tfor link in list_of_urls:\n",
    "\t\t# create response object per link from the total list of links\n",
    "\t\tr = requests.get(link) \n",
    "\t\tsoup = BeautifulSoup(r.content,'html')\n",
    "\n",
    "\t\t# find all urls with 'rdcu.be' which contains the specific paper\n",
    "\t\tlinks = soup.findAll(attrs={'href': re.compile(\"rdcu.be\")})\n",
    "\t\tlist_of_pdfs = []\n",
    "\t\tpdfs = [link['href'] for link in links]\n",
    "\t\tlist_of_pdfs.extend(pdfs)\n",
    "\n",
    "\treturn list_of_pdfs\n",
    "\n",
    "def download_pdf_series(pdf_links):\n",
    "\tprint(pdf_links) \n",
    "\n",
    "\tfor link in pdf_links: \n",
    "\t\tprint(link)\t\t\n",
    "\n",
    "\t\t'''iterate through all links in links\n",
    "\t\tand download them one by one'''\n",
    "\t\t\n",
    "\t\t# obtain filename by splitting url and getting \n",
    "\t\t# last string \n",
    "\t\tfile_name = link.split('/')[-1] \n",
    "\t\t\n",
    "\n",
    "\t\tprint( \"Downloading file:%s\"%file_name) \n",
    "\t\t\n",
    "\t\t# create response object \n",
    "\t\tr = requests.get(link, stream = True) \n",
    "\t\t\n",
    "\t\t# download started \n",
    "\t\twith open(file_name, 'wb') as f: \n",
    "\t\t\tfor chunk in r.iter_content(chunk_size = 1024*1024): \n",
    "\t\t\t\tif chunk: \n",
    "\t\t\t\t\tf.write(chunk) \n",
    "\t\t\n",
    "\t\tprint( \"%s downloaded!\\n\"%file_name ) \n",
    "\n",
    "\tprint (\"All links are downloaded!\") \n",
    "\treturn\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "\n",
    "\t# getting all video links \n",
    "\tpdf_links = get_url_links()\n",
    "\tprint(pdf_links)\n",
    "\n",
    "\t# download all videos \n",
    "\tdownload_pdf_series(pdf_links) \n",
    "\t\n",
    "\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import regex as re\n",
    "\n",
    "# specify the URL of the archive here \n",
    "archive_url = \"http://conferences.miccai.org/2023/papers/\"\n",
    "url = \"http://conferences.miccai.org\"\n",
    "\n",
    "def get_url_links(): \n",
    "\tlist_of_pdfs = []\n",
    "\t# create response object \n",
    "\tr = requests.get(archive_url) \n",
    "\t\n",
    "\t# create beautiful-soup object \n",
    "\tsoup = BeautifulSoup(r.content,'html')\n",
    "\t\n",
    "\t# find all links on web-page \n",
    "\tlinks = soup.findAll('a') \n",
    "\n",
    "\t# filter links ending with .html \n",
    "\tlist_of_urls = [url + link['href'] for link in links if link['href'].endswith('html')] \n",
    "\t# print(len(paper_links)) gives us 730 papers in total\n",
    "\n",
    "\n",
    "\t# now that we have all the links to the papers, we can go through each paper and find the pdf link\n",
    "\tfor link in list_of_urls:\n",
    "\t\t# create response object per link from the total list of links\n",
    "\t\tr = requests.get(link) \n",
    "\t\tsoup = BeautifulSoup(r.content,'html')\n",
    "\n",
    "\t\t# find all urls with 'rdcu.be' which contains the specific paper\n",
    "\t\tlinks = soup.findAll(attrs={'href': re.compile(\"rdcu.be\")})\n",
    "\t\tpdfs = [link['href'] for link in links]\t\n",
    "\t\tlist_of_pdfs.append(pdfs)\n",
    "\n",
    "\treturn list_of_pdfs\n",
    "\n",
    "\n",
    "def download_pdf_series(pdf_links): \n",
    "\n",
    "\tfor link in pdf_links: \n",
    "\t\tprint(link)\t\t\n",
    "\n",
    "\t\t'''iterate through all links in links\n",
    "\t\tand download them one by one'''\n",
    "\t\t\n",
    "\t\t# obtain filename by splitting url and getting \n",
    "\t\t# last string \n",
    "\t\tfile_name = link.split('/')[-1] \n",
    "\t\t\n",
    "\n",
    "\t\tprint( \"Downloading file:%s\"%file_name) \n",
    "\t\t\n",
    "\t\t# create response object \n",
    "\t\tr = requests.get(link, stream = True) \n",
    "\t\t\n",
    "\t\t# download started \n",
    "\t\twith open(file_name, 'wb') as f: \n",
    "\t\t\tfor chunk in r.iter_content(chunk_size = 1024*1024): \n",
    "\t\t\t\tif chunk: \n",
    "\t\t\t\t\tf.write(chunk) \n",
    "\t\t\n",
    "\t\tprint( \"%s downloaded!\\n\"%file_name ) \n",
    "\n",
    "\tprint (\"All links are downloaded!\") \n",
    "\treturn\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "\n",
    "\t# getting all video links \n",
    "\tpdf_links = get_url_links() \n",
    "\n",
    "\t# download all videos \n",
    "\tdownload_pdf_series(pdf_links) \n",
    "\t\n",
    "\n",
    "\t\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
