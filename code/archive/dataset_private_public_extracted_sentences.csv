,title,extracted_keyword_sent
3,Anatomy-Driven Pathology Detection on Chest X-rays,"-we train
our models on the chest imagenome [21] dataset and evaluate on nih chestx-ray 8
[20]"
7,Anatomy-Driven Pathology Detection on Chest X-rays,"we train on the chest imagenome dataset [4,21,22]1 ,
consisting of roughly 240 000 frontal chest x-ray images with corresponding
scene graphs automatically constructed from free-text radiology reports."
8,Anatomy-Driven Pathology Detection on Chest X-rays,"it is
derived from the mimic-cxr dataset [9,10], which is based on imaging studies
from 65 079 patients performed at beth israel deaconess medical center in
boston, us."
9,Anatomy-Driven Pathology Detection on Chest X-rays,"we consider the image-level label for a pathology to be positive if
any region is positively labeled with that pathology.we use the provided
jpg-images [11] 2 and follow the official mimic-cxr training split but only keep
samples containing a scene graph with at least five valid region bounding boxes,
resulting in a total of 234 307 training samples.during training, we use random
resized cropping with size 224 × 224, apply contrast and brightness jittering,
random affine augmentations, and gaussian blurring.evaluation dataset and class
mapping."
10,Anatomy-Driven Pathology Detection on Chest X-rays,"we evaluate our method on the subset of 882 chest x-ray images with
pathology bounding boxes, annotated by radiologists, from the nih chestxray-8
(cxr8) dataset [20] 3 from the national institutes of health clinical center in
the us."
17,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"using the multi-clamp function, each
channel of the logits is restricted to the following parameter spaces:the limits
of the ranges were defined based on the meaning of the parameter (as in v b ),
mathematical requirements (as in the minimum values of k 2 and k 3 , whose sum
can not be zero) [6] or previous knowledge on the dataset derived by the work of
sari et al."
18,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,the dataset is composed of 23 oncological patients with different tumor types.
19,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"the dataset included the label maps of 7 organs
(bones, lungs, heart, liver, kidneys, spleen, aorta) and one image-derived input
function a(t) [bq/ml] from the descending aorta per patient."
20,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"further details on
the dataset are presented elsewhere [16].the pet frames and the label map were
resampled to an isotropic voxel size of 2.5 mm."
21,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"then, the dataset was split
patient-wise into training, validation, and test set, with 10, 4, and 9 patients
respectively."
22,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,"details on the dataset split are available in the supplementary
material (table 1)."
23,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"however, most deep
learning approaches for segmentation require fully or partially labeled training
datasets, which can be time-consuming and expensive to annotate."
24,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"we evaluate our method on the brain tumor segmentation challenge (brats) dataset
[1,2,14], which contains 2,000 cases, each of which includes four 3d volumes
from four different mri modalities: t1, post-contrast enhanced t1 (t1-ce), t2,
and t2 fluid attenuated inversion recovery (t2-flair), as well as a
corresponding segmentation ground-truth mask."
25,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"finally, the proposed ame-cam achieves optimal performance in all modalities of
the brats dataset.compared to the unsupervised baseline (ul), c&f is unable to
separate the tumor and the surrounding tissue due to low contrast, resulting in
low dice scores in all experiments."
26,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"2 shows the
visualization of the cam and segmentation results from all six cam-based
approaches under four different modalities from the brats dataset."
27,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"we aim to demonstrate the superiority of the proposed
attention-based aggregation approach for segmenting tumor regions in t1 mri of
the brats dataset."
28,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"note that we only report the results for t1 mri in the brats
dataset."
29,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,the experiments are done on the t1-ce mri of brats dataset.
30,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"1 and the
multipleexit using results from m 2 and m 3 , and using all exits (ame-cam) on
t1-ce mri in the brats dataset.the comparisons show that the activation map
obtained from the shallow layer m 1 and the deepest layer m 4 result in low dice
scores, around 0.15."
31,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"we then use an
attention model to hierarchically aggregate these activation maps, learning
pixel-wise weighted sums.experimental results on the four modalities of the 2021
brats dataset demonstrate the superiority of our approach compared with other
cam-based weakly-supervised segmentation methods."
32,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,"specifically, ame-cam achieves
the highest dice score for all patients in all datasets and modalities."
33,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"these works implicitly assume a certain threshold on depth to define
positive and negative samples, which may be difficult to define and may be
different among applications and datasets."
34,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"this is the public lihc
dataset from the cancer genome atlas [9], which presents a histological score,
the ishak score, designated as y 2 histo , that differs from the metavir score
present in d 1 histo ."
35,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"similarly to the metavir score in d 1 histo
, we also binarize the ishak score, as proposed in [16,20], which results in two
cohorts of 34 healthy and 15 pathological patients.in all datasets, we select
the slices based on the liver segmentation of the patients."
36,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"for the
latter pretraining dataset, it presents an average slice spacing of 3.23 mm with
a standard deviation of 1.29 mm."
37,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"then, we train a regularized logistic regression on the frozen
representations of the datasets d 1 histo and d 2 histo ."
38,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"as a baseline, we train a classification algorithm from
scratch (supervised) for each dataset, d 1 histo and d 2 histo , using both
backbone encoders and the same 5-fold crossvalidation strategy."
39,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"finally, we report the
cross-validated results for each model on the aggregated dataset we present in
table 1 the results of all our experiments."
40,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"first, we can
notice that our method outperforms all other pretraining methods in d 1 histo
and d 1+2 histo , which are the two datasets with more patients."
41,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"for the second dataset d 2 histo , our method
is on par with byol and supcon when using a small encoder and outperforms the
other methods when using a larger backbone.to illustrate the impact of the
proposed method, we report in fig."
42,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,"we tested it
on a challenging clinical application, cirrhosis prediction, using three
different datasets, including the lihc public dataset."
43,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"furthermore,
experts may focus on annotating objects they are already aware of, thereby
restricting the possibility of new structural discoveries in large datasets
using deep learning."
44,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"3) our approach outperforms previous
3d unsupervised discovery methods on challenging synthetic datasets and on a
real-world brain tumor segmentation (brats'19) dataset."
45,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"we draw γ from the uniform distribution: to compare with state-of-the-art
unsupervised 3d segmentation methods we follow
[13] and evaluate our method on challenging biologically inspired 3d synthetic
datasets and a real-world brain tumor segmentation (brats'19) dataset.the
synthetic dataset of [13], consists of 120 volumes (80-20-20 split) of size 50 ×
50 × 50."
46,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"the regular variant of the dataset contains
cubical and spherical objects, while the irregular variant contains more complex
shapes."
47,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"figure 3 shows sample slices of both
variants.the brats'19 dataset [2,3,21] is an established benchmark for 3d tumor
segmentation of brain mris."
48,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"we compare our method with state-of-the-art unsupervised 3d structure discovery
approaches including clustering using 3d feature learning [23], a 3d
convolutional autoencoder [24], and self-supervised hyperbolic representations
[13].for the synthetic datasets, we used k = 2 (background and cell) for level
1, k = 4 (background, cell, vesicle, mitochondria) for level 2, and k = 8
(background, cell, vesicle, mitochondria, and 4 small protein aggregates) for
level 3 predictions."
49,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"table 1 shows the results for the regular and irregular variants of the
cryo-et-inspired synthetic dataset."
50,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"we also show in table 2 that features from early decoder stages of the
u-net-based diffusion models better discover larger objects in the hierarchy,
features at intermediate stages better capture intermediate objects, and
features at later stages better find smaller objects.for the brain tumor
segmentation (brats'19) dataset, we use the whole tumor (wt) segmentation mask
for evaluation, which is detectable based on the flair images alone."
51,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"our predictions look smoother and do not capture fine details
of tumor segmentations.we perform ablation studies on the brats'19 dataset
(table 3: below the line)."
52,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"to evaluate the
significance of the diffusion features, we replaced our diffusion feature
extractor with a 3d resnet from med3d [5] trained on 23 medical datasets."
53,Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,"our method outperforms
existing unsupervised segmentation approaches and discovers meaningful
hierarchical concepts on challenging biologically-inspired synthetic datasets
and on the brats brain tumor dataset."
54,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"the effectiveness of deep
learning models in medical applications is usually based on large,
well-annotated datasets, which in turn necessitates a time-consuming and
expertise-driven annotation process."
55,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"therefore,
we comprehensively evaluate our approach on multiple datasets from various
medical sites to showcase its viability and effectiveness across different
contexts.dual-branch learning has been widely adopted in annotation-efficient
learning to encourage mutual consistency through co-teaching."
56,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"an extensive assessment of our approach through the examination of
four publicly accessible datasets establishes its superiority and clinical
significance."
57,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,datasets.
58,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"we employ the sun-seg [10] dataset with scribble annotations for
training and assessing the in-distribution performance."
59,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"this dataset is based on
the sun database [16], which contains 100 different polyp video cases."
60,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"for out-of-distribution evaluation, we utilize
three public datasets, namely kvasir-seg [9], cvc-clinicdb [2], and polypgen [1]
with 1000, 612, and 1537 polyp frames, respectively."
61,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"these datasets are
collected from diversified patients in multiple medical centers with various
data acquisition systems."
62,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"2, our s 2 me achieves superior in-distribution performance quantitatively and
qualitatively compared with other baselines on the sun-seg [10] dataset."
63,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"regarding generalization and robustness, as indicated in table 2, our method
outperforms other weakly-supervised methods by a significant margin on three
unseen datasets, and even exceeds the fully-supervised upper bound on two of
them 4 ."
64,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"notably, the encouraging performance on unseen datasets exhibits
promising clinical implications in deploying our method to real-world scenarios."
65,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,"with extensive in-domain and
out-ofdomain evaluation on four public datasets, our method shows superior
accuracy, generalization, and robustness, indicating its clinical significance
in alleviating data-related issues such as data shift and corruption which are
commonly encountered in the medical field."
66,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"this idea is supported for example by
recent efforts on collecting new public dataset, to further advance in this
field, such as endoscopic recordings from endoslam [16] and endomapper [1]
datasets."
67,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"our training set contains short sequences (4-7 s) from the
complete colonoscopy recordings in endomapper dataset where colmap software was
able to obtain a 3d reconstruction."
68,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"the following experiments demonstrate the proposed feature detection efficacy to
obtain 3d models on real colonoscopy videos, comparing different variations of
our approach and relevant baseline methods.dataset."
69,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"we seek techniques that are
applicable to real medical data, so we train and evaluate with subsequences from
the endomapper dataset [1], which contains a hundred complete endoscopy
recordings obtained during regular medical practice."
70,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"we use colmap 3d
reconstructions obtained from subsequences from this dataset (11260 frames from
65 reconstructions obtained along 14 different videos for training, and 838
frames from 7 reconstructions from 6 different videos for testing)."
71,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"for both metrics, our detector achieves significantly better
results, showcasing the better properties of our detector for 3d
reconstruction.to provide quantitative evaluation of the camera motion
estimation, we use a simulated dataset [3] to have ground truth available for
the camera trajectory."
72,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,"we took 5 sequences of 100-150 frames from this dataset,
and we tested the baselines and our model."
73,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"moreover, pathological images from different tissues or
cancer types often show significant domain shifts, which hamper the
generalization of models trained on one dataset to others."
74,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"dataset and setting: we collect four pathology image datasets to validate our
proposed approach."
75,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"[10] publish a dataset of nucleus segmentation containing 5,060
segmented slides from 10 tcga cancer types."
76,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"we have also included 463 images of
kidney renal clear cell carcinoma (kirc) in our dataset, which are made publicly
available by irshad et al [11]."
77,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"[2] publicly release a dataset
containing tissue slide images and associated clinical data on colorectal cancer
(crc), from which we randomly select 200 patches for our study."
78,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,"we
demonstrate the effectiveness of our method on multiple public datasets and
believe it can be readily applied to other domains and adaptation scenarios."
79,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,"yet, acquiring large training datasets and
their corresponding labels, especially from a cohort of patients, can be costly
or even infeasible, which poses a significant challenge in developing a dl model
with high performance [7]."
80,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,"second, even when large-scale datasets are available
through collaborative research from multiple sites, dl models trained on such
datasets may yield sub-optimal solutions due to domain gaps caused by
differences in images acquired from different sites [20]."
81,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,"third, due to the
small number of datasets from each domain, the images for each individual domain
may not capture representative features, limiting the ability of dl models to
generalize across domains [3].domain adaptation (da) has been extensively
studied to alleviate the aforementioned limitations, the goal of which is to
reduce the domain gap caused by the diversity of datasets from different domains
[12,20,26,29,33]."
82,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"the keys of
our upete include 1) compressing the 3d pet images into a lower dimensional
space for reducing the computational cost of diffusion model, 2) adopting the
poisson noise, which is the dominant noise in pet imaging [20], to replace the
gaussian noise in the diffusion process for avoiding the introduction of details
that are not existing in pet images, and 3) designing ct-guided cross-attention
to incorporate additional ct images into the inverse process for helping the
recovery of structural details in pet.our work had three main
features/contributions: i) proposing a clinicallyapplicable unsupervised pet
enhancement framework, ii) designing three targeted strategies for improving the
diffusion model, including pet image compression, poisson diffusion, and
ct-guided cross-attention, and iii) achieving better performance than
state-of-the-art methods on the collected pet datasets."
83,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"3 experiments our dataset consists of 100 spet images for training and 30 paired
lpet and spet
images for testing."
84,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"validated by
extensive experiments, our upete achieved better performance than both
state-of-the-art unsupervised and fully-supervised pet enhancement methods, and
showed stronger generalizability to the tracer dose changes.despite the advance
of upete, our current work still suffers from a few limitations such as (1)
lacking theoretical support for our poisson diffusion, which is just an
engineering attempt, and 2) only validating the generalizability of upete on a
simulated dataset."
85,PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,"in our future work, we will complete the design of poisson
diffusion from theoretical perspective, and collect more real pet datasets
(e.g., head datasets) to comprehensively validate the generalizability of our
upete."
86,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"[4] train a vessel
segmentation model from unsupervised 2d labels transferred from a publicly
available dataset, however, there is still a gap to be closed between
unsupervised and supervised model performance."
87,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,dataset.
88,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"we use an in-house dataset of contrast-enhanced abdominal computed
tomography images (cts) in the arterial phase to segment the peripancreatic
arteries [6]."
89,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"the dataset contains binary 3d
annotations of the peripancreatic arteries carried out by two radiologists, each
having annotated half of the dataset."
90,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"for more information about
the dataset, see [6].image augmentation and transformation."
91,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"we
implement [13] as a baseline on our dataset, training on up to 3 fixed
orthogonal projections."
92,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"we
theorize that this is because the dataset itself contains noisy annotations and
fully supervised models better overfit to the type of data annotation, whereas
our models converge to following the contrast and segmenting more vessels, which
are sometimes wrongfully labeled as background in the ground truth."
93,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"msd are not
very telling in our dataset due to the noisy annotations and the nature of
vessels, as an under-or over-segmented vessel branch can quickly translate into
a large surface distance.the effect of dataset size."
94,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"we vary the size of the
training set from |d tr | = 80 to as little as |d tr | = 10 samples, while
keeping the size of the validation and test sets constant, and train models on
single random viewpoints.in table 2, we compare single random projections
trained with and without depth information at varying dataset sizes to ilustrate
the usefulness of the depth information with different amounts of training data."
95,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"our depth loss offers consistent improvement across multiple dataset sizes and
reduces the overall performance variance."
96,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"the smaller the dataset
size is, the greater the performance boost from the depth."
97,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"we perform a wilcoxon
rank-sum statistical test comparing the individual sample predictions of the
models trained at various dataset sizes with single random orthogonal viewpoints
with or without depth information, obtaining a statistically significant
(p-value of < 0.0001)."
98,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"using a labeled dataset consisting of
single, randomly selected, orthogonal 2d annotations for each training sample
and additional depth information obtained at no extra cost, we obtain accuracy
almost on par with fully supervised models trained on 3d data at a mere fraction
of the annotation cost."
99,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"limitations of our work are that the depth information
relies on the assumption that the vessels exhibit minimal intensity fluctuations
within local neighborhoods, which might not hold on other datasets, where more
sophisticated ray-tracing methods would be more effective in locating the front
and back of projected objects."
100,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,"furthermore, careful preprocessing is performed
to eliminate occluders, which would limit its transferability to datasets with
many occluding objects of similar intensities."
101,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"here, we focus on the automated construction of pdms because,
compared to deformation fields, point correspondences are easier to interpret by
clinicians, are computationally efficient for large datasets, and less sensitive
to noise and outliers than deformation fields [5].ssm performance depends on the
underlying process used to generate shape correspondences and the quality of the
input data."
102,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"this vae
branch serves two purposes: (a) serves as a shape analysis module for the
non-linear shape variations and (b) learns a data-specific template from the
latent space of the correspondences that is fed back to the correspondence
generation network.to motivate the need for the mesh feature encoder and study
the effect of the template selection, we considered the box-bump dataset, a
synthetic dataset of 3d shapes of boxes with a moving bump."
103,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"dataset: we use the publicly available decath-pancreas dataset of 273
segmentations from patients who underwent pancreatic mass resection [24]."
104,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"although the dgcnn mesh autoencoder used in mesh2ssm does not require
the same number of vertices, uniformity across the dataset makes it
computationally efficient; hence, we pad the smallest mesh by randomly repeating
the vertices (akin to padding image for convolutions)."
105,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"similar to the
observations made box-bump dataset, flowssm is affected by the choice of the
template, and the modes of variation differ as the template changes."
106,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"pancreatic cancer mainly presents itself on the head of the
structure [20] and for the decath dataset, we can see the first mode identifies
the change in the shape of the head."
107,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"figure 4.a shows the metrics for the
pancreas dataset."
108,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"using the analysis module of
mesh2ssm, we visualized the top three modes of variation identified by sorting
the latent dimensions of sp-vae based on the standard deviations of the latent
embeddings of the training dataset."
109,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"for the pancreas dataset
with the medoid as the initial template, mesh2ssm with the template feedback
produced more precise models."
110,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"like most deep learning models,
performance of mesh2ssm could be affected by small dataset size, and it can
produce overconfident estimates."
111,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,"the method
is demonstrated to have superior performance in identifying shape variations
using fewer parameters on synthetic and clinical datasets."
112,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"4) we conduct comprehensive
experiments on adcd [1] and la [26] datasets, showing that our cross-ald
regularization achieves state-of-the-art performance against existing solutions
[8,11,12,14,21,22,28]."
113,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"let d l and d ul be the labeled and unlabeled dataset, respectively, with p d l
and p d ul being the corresponding data distribution."
114,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"the labeled image x l and segmentation
groundtruth y are sampled from the labeled dataset d l (x l , y ∼ p d l ), and
the unlabeled image sampled from d ul is x ∼ p d ul .given an input x ∼ p d ul
(i.e., the unlabeled data distribution), let us denote the ball constraint
around the image x aswhere is a ball constraint radius with respect to a norm ||
• || p , and x is an adversarial example2 ."
115,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"the
first term is the dice loss, where labeled image x l and segmentation
ground-truth y are sampled from labeled dataset d l ."
116,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"in this section, we conduct several comprehensive experiments using the acdc4
dataset [1] and the la5 dataset [26] for 2d and 3d image segmentation tasks,
respectively."
117,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"the cross-ald uses
the u-net [18] and v-net [13] architectures for the acdc and la dataset,
respectively."
118,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"we then illustrate the cross-ad outperforms other recent methods
on acdc and la datasets in sect."
119,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"φ is the decoder of
u-net in acdc dataset, while φ is the decoder of v-net in la dataset."
120,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"we randomly pick three
images from the datasets to generate adversarial particles."
121,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,"we adapt cross-ald to semi-supervised medical image
segmentation to achieve start-of-the-art performance on the acdc and la datasets
compared to many recent methods such as vat [14], ua-mt [28], sassnet [8], dtc
[11], urpc [12] , mc-net [22], and ss-net [21]."
122,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"we then present the training and test datasets and finish the section by
demonstrating the network architecture."
123,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,"we selected 600 rf frame
pairs of this dataset for the training of the networks.two well-known metrics of
contrast to noise ratio (cnr) and strain ratio (sr) are utilized to evaluate the
compared methods."
124,SLPD: Slide-Level Prototypical Distillation for WSIs,"meanwhile, this clustering strategy ignores the
hierarchical structure ""region→wsi→whole dataset"" underlying the data, where the
id of the wsi can be served as an extra learning signal."
125,SLPD: Slide-Level Prototypical Distillation for WSIs,"specifically, for a region embedding z belonging to the slide w and
assigned to the prototype c, we first search the top-k nearest neighbors of w in
the dataset based on the semantic similarity, denoted as { ŵk } k k=1 ."
126,SLPD: Slide-Level Prototypical Distillation for WSIs,datasets.
127,SLPD: Slide-Level Prototypical Distillation for WSIs,"we conduct experiments on two public wsi datasets for downstream
tasks."
128,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,"(4) ablation and evaluation studies on two public datasets demonstrate our
model's ability to outperform state-of-the-art techniques not only with ideal
labels but also with shifted labels."
129,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,dataset.
130,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,"to validate the effectiveness of our model, we use two public nuclei
segmentation datasets i.e., cpm17 [26] & monuseg [12]."
131,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,"monuseg is a multi-organ nuclei
segmentation dataset consisting of 30 h&e stained images (1000×1000) extracted
from seven different organs."
132,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,"we obtain statistically significant (p-value <0.05) for the aji of all
comparison methods on two datasets in all scenarios."
133,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,"compared to other variants, our proposed
model is more robust to the point shift in both datasets."
134,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,"according to our experimental results, we
established a new state-of-art on two publicly available datasets across
different levels of point annotation imperfections."
135,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"to the best of our knowledge, this is the first work to
train models to directly identify anomalies on tasks that are deformation-based,
tasks that use poisson blending with patches extracted from external datasets,
and tasks that perform efficient poisson image blending in 3d volumes, which is
in itself a new contribution of our work."
136,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"also, all tasks share a common recipe: the target anomaly mask m h is always a
randomly sized and rotated ellipse or rectangle (ellipsoids/cuboids in 3d); all
anomalies are positioned such that at least 50% of the mask intersects with the
foreground of the image; and after one augmentation is applied, the process is
randomly repeated (based on a fair coin toss, p = 0.5), for up to a maximum of 4
anomalies per image.the intra-dataset blending task."
137,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"the intra-dataset blending task therefore
results from xintra = p oissonblend(x, x , m h ) with x, x ∈ d with samples from
a common dataset d and is therefore similar to the self-supervision task used in
[23] for 2d images.the inter-dataset blending task follows the same process as
intra-dataset blending but uses patches extracted from an external dataset d ,
allowing for a greater variety of structures."
138,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"data: we evaluate our method on t2-weighted brain mr and chest x-ray datasets to
provide direct comparisons to state-of-the-art methods over a wide range of real
anomalies."
139,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"for brain mri we train on the human connectome project (hcp) dataset
[28] which consists of 1113 mri scans of healthy, young adults acquired as part
of a scientific study."
140,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"to evaluate, we use the brain tumor segmentation
challenge 2017 (brats) dataset [1], containing 285 cases with either high or low
grade glioma, and the ischemic stroke lesion segmentation challenge 2015 (isles)
dataset [13], containing 28 cases with ischemic stroke lesions."
141,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"the hcp dataset was
resampled to have 1mm isotropic spacing to match the test datasets."
142,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"lastly, samples are
downsampled by a factor of two.for chest x-rays we use the vindr-cxr dataset
[18] including 22 different local labels."
143,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"in particular, we achieve a
pixel-wise ap of 76.2 and 45.9 for brats and isles datasets respectively."
144,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,"for the brain datasets, all metrics generally decrease as the
number of training tasks increases."
145,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"the label encoder
encodes the text labels in the dataset into n label features, denoted as l ∈ r n
×c l , where n represents the number of classes in the dataset and c l
represents the dimension of label features."
146,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"here we use
medclip1 as our label encoder, which is a model fine-tuned on the roco dataset
[12] based on clip [14].knowledge encoder."
147,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"clinicalbert is a language
model that has been fine-tuned on the mimic-iii [8] dataset based on biobert
[9].adaptive layer."
148,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"we freeze the label and knowledge encoders for training
efficiency but add an adaptive layer after the text encoders to better tailor
the text features to our dataset."
149,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"we first perform min-max normalization
on it, the formula is as followswhere 1 ≤ c ≤ n means c th class in the dataset."
150,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"cam [20] and grad-cam [15] were
evaluated using the same resnet38 [16] classifier, and the results showed that
cam [20] outperformed grad-cam [15], with miou values of 70.44% and 56.52% on
the luad-histoseg and bcss-wsss datasets, respectively."
151,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"this could be due to the design of transws [18] for single-label
image segmentation, with the segmentation branch simplified to binary
segmentation to reduce the difficulty, while our dataset consists of multilabel
images."
152,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"our proposed method outperformed all
previous methods on both luad-histoseg and bcss-wsss datasets, with improvements
of 2.64% and 5.42% over the second-best method, respectively (table 2)."
153,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"due to its heavy reliance on dataset-specific
post-processing steps, histosegnet [5] failed to produce the desired results on
our datasets."
154,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"as we have previously analyzed since the datasets we used are all
multi-label images, it was challenging for the segmentation branch of transws
[18] to perform well, and it failed to provide an overall benefit to the model."
155,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"specifically, our miou scores exceeded the second-best method by 3.17%
and 3.09% on luad-histoseg and bcss-wsss datasets, respectively."
156,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,"the proposed method
achieves the best results on two public datasets, luad-histoseg and bcss-wsss,
demonstrating the superiority of our method."
157,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,"manual editing of vessel geometry
is a tedious and error prone task that requires expert medical knowledge, which
explains the scarcity of curated datasets."
158,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,"dataset and pre-processing overview: the raw
meshes from the intraa 3d collection undergo pre-processing using the vmtk
toolkit."
159,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,"by using these metrics,
we can determine how well the generated 3d models of blood vessels match the
original dataset distribution, as well as the diversity of the generated output."
160,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"the validation dataset {(x v i , y v i ) | i = 1, ..., m } is
assumed to be independent and identically distributed as the training dataset."
161,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"3, the update of
model parameter θ t j and lr α t j is performed using different datasets to
ensure that the updated θ t j can be evaluated for generalization without being
influenced by the seen data."
162,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"to ensure the reproducibility of the results, all pre-trained models
(uscl [9], imagenet [11], c2l [28], models genesis [29]) and target datasets
(pocus [5], busi [1], chest x-ray [17], lits [4]) are publicly available."
163,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"in our
work, we consider models pre-trained on both natural and medical image datasets,
with three target modalities and three target organs, which makes our
experimental results more credible."
164,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,"the
validation set for the lits segmentation dataset comprises 23 samples from the
training set of size 111."
165,DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,"our proposal has proven its
effectiveness on two well-known histological datasets, camelyon16 and tcga lung
cancer, obtaining state-of-the-art results on wsi classification."
166,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"to address this problem, the common paradigm of
transfer learning, which first pre-trains a model on upstream image datasets and
then fine-tunes it on various target tasks, has been widely investigated in
recent years [10,21,30]."
167,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"with the increasing number of pre-trained networks provided by the
community, model repositories like hugging face [25] and pytorch hub [18] enable
researchers to experiment across a large number of downstream datasets and
tasks."
168,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"a brute-force method is to
fine-tune a set of pretrained models with target datasets to find the optimal
one."
169,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"existing methods also measured
the task-relatedness between source and target datasets [6,7,22,28]."
170,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"however,
most of these works require source information available while medical images
have more privacy and ethical issues and fewer datasets are publicly available
than natural images.considering the issues mentioned above, this work focused on
source-free pre-trained model selection for segmentation tasks in the medical
image."
171,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"the main idea is to directly measure the transferability of the
pre-trained models without fully training based on the downstream/target
dataset."
172,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"extensive experiments
have proved the superiority of our method compared with baseline methods.2
methodology in our work, a model bank m consisting of pre-trained models {m i }
k i=1 are
available to be fine-tuned and evaluated with a target dataset, where x j is the
image and y j is the ground truth of segmentation."
173,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"our work is to directly
estimate the transferability score t i s→t without fine-tuning the model on
target datasets."
174,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"the proposed method is intuitive and straightforward: features extracted by the
pre-trained model should be consistent within the class of the target dataset
while representative and various globally."
175,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"the pre-trained models are trained with
specific pretext tasks based on the upstream dataset."
176,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"we calculate the
wasserstein distance of the distribution with voxels of the same class in a
sample pair comprised of every two samples in the dataset, and obtained the
following definition of class consistency c consgiven that 3d medical images are
computationally intensive, and prone to causing out-of-memory problems, in the
sliding window inference process for each case, we do not concatenate the output
of each patch into the final prediction result, but directly sample from the
patched output and concatenate them into the final sampled feature matrix."
177,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"for the dataset with n cases, we choose s = 1 and the feature variety f v is
formulated asoverall estimation."
178,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"besides, we
decrease the sampling ratio in the decoder layer close to the bottleneck to
avoid feature redundancy.the final transferability of pre-trained model m to
dataset t t m→t iswhere d is the number of decoder layers used in the
estimation."
179,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"the medical segmentation decathlon (msd) [2] dataset is composed of ten
different datasets with various challenging characteristics, which are widely
used in the medical image analysis field."
180,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"to evaluate the effectiveness of
cc-fv, we conduct extensive experiments on 5 of the msd dataset, including
task03 liver(liver and tumor segmentation), task06 lung(lung nodule
segmentation), task07 pancreas(pancreas and pancreas tumor segmentation), task09
spleen(spleen segmentation), and task10 colon(colon cancer segmentation)."
181,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"all of
the datasets are 3d ct images."
182,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"the public part of the msd dataset is chosen for
our experiments, and each dataset is divided into a training set and a test set
at a scale of 80% and 20%."
183,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,"for each dataset, we use the other four datasets to
pre-train the model and fine-tune the model on this dataset to evaluate the
performance as well as the transferability using the correlation between two
ranking sequences of upstream pre-trained models."
184,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"pre-training the backbone in a
self-supervised manner enables scaling to larger datasets across multiple data
and task domains."
185,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"in medical imaging, this is particularly useful given the
growing number of available datasets.in this work, we focus on contrastive
learning [8,12], one of the most effective approaches to ssl in computer vision."
186,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"second, we
employ vox2vec to pre-train a fpn architecture on a diverse collection of six
unannotated datasets, totaling over 6,500 ct images of the thorax and abdomen."
187,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"finally, we compare the
pretrained model with the baselines on 22 segmentation tasks on seven ct
datasets in three setups: linear probing, non-linear probing, and fine-tuning."
188,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"for pre-training, we use 6 public ct datasets [1,3,5,15,21,27], totaling more
than 6550 cts, covering abdomen and thorax domains."
189,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"we do not use the
annotations for these datasets during the pre-training stage."
190,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"we evaluate our method on the beyond the cranial vault abdomen (btcv) [19] and
medical segmentation decathlon (msd) [4] datasets."
191,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"the btcv dataset consists of
30 ct scans along with 13 different organ annotations."
192,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"we test our method on 6
ct msd datasets, which include 9 different organ and tumor segmentation tasks."
193,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"the segmentation performance of each model
on btcv and msd datasets is evaluated by the dice score.for our method, the
pre-processing steps are the same for all datasets, as at the pre-training
stage, but in addition, intensities are clipped to (-1350, 1000) hu window and
rescaled to (0, 1).we compare our results with the current state-of-the-art
self-supervised methods [2,26] in medical imaging."
194,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"the mean value and standard deviation of dice score across 5 folds on the btcv
dataset for all models in all evaluation setups are presented in table 1."
195,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"we demonstrate an example of the
excellent performance of vox2vec-fpn in both linear and non-linear probing
regimes in supplementary materials.we reproduce the key results on msd challenge
ct datasets, which contain tumor and organ segmentation tasks."
196,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"by pre-training a fpn backbone to
extract informative representations from unlabeled data, our method scales to
large datasets across multiple task domains."
197,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"we plan to investigate further how the performance of vox2vec
scales with the increasing size of the pre-training dataset and the pre-trained
architecture size."
198,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,"another interesting research direction is exploring the
effectiveness of vox2vec with regard to domain adaptation to address the
challenges of domain shift between different medical imaging datasets obtained
from different sources."
199,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"we
evaluate our proposed method on two multi-domain datasets: 1)."
200,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"the infant brain
mri dataset for cross-age segmentation; 2)."
201,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"the brats2018 dataset for
cross-grade tumor segmentation."
202,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"our proposed method was evaluated using two medical image segmentation da
datasets."
203,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"the first dataset, i.e., cross-age infant segmentation [20], was used
for cross-age infant brain image segmentation, while the second dataset, i.e.,
brats2018 [21], was used for hgg to lgg domain adaptation.the first dataset is
for infant brain segmentation (white matter, gray matter and cerebrospinal
fluid)."
204,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"to build the cross-age dataset, we take advantage 10 brain mris of
6-month-old from iseg2019 [20], and also build 3-month-old and 12-month-old
in-house datasets."
205,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"in this dataset, we collect 11 brain mri for both the
3-month-old and 12-month-old infants."
206,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"we take the 6-month-old data as the source
domain, the 3-month-old and 12-month-old as the target domains.the 2nd dataset
is for brain tumor segmentation (enhancing tumor, peritumoral edema and necrotic
and non-enhancing tumor core), which has 285 mri samples (210 hgg and 75 lgg)."
207,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"as observed, our method demonstrates very good da
ability on the crossage infant segmentation task, which improves about 5.46 dice
and 4.75 dice on 12-month-old and 3-month-old datasets, respectively."
208,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"also, the proposed method shows considerable improvements
over adda and cycada, but very subtle improvements to the sifa and adr methods
(although adr shows a small advantage on the whole category).we also visualize
the segmentation results on a typical test sample of the infant brain dataset in
fig."
209,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"is
prompt-da compatible with adv-da?the corresponding experiments are conducted on
the infant brain dataset and experimental results are shown in table 2."
210,Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,"experiments on two da datasets with two different segmentation backbones
demonstrate that our proposed method works well on da problems."
211,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"to solve this problem, many unsupervised domain
adaptation (uda) methods [6] have been developed for adapting a model to a new
site with only unlabeled data (target domain) by transferring the knowledge
learned from the original dataset (source domain)."
212,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"previous studies [7,10,24,25] have demonstrated that transferring
the amplitude spectrum of target domain images to a source domain can
effectively convey image style information and diversify training dataset."
213,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"the sensitivity at frequency (i, j) of a
model f trained on the source domain is defined as the prediction error rate
over the whole dataset x s as in (1), where acc denotes the prediction accuracy
using the dodiss map m s and an adversarially learned parameter λ * as a
weighting factor, samix mixes the amplitude spectrum of each source image with
the spectrum of a target image."
214,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,we evaluated samix on two medical image datasets.
215,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,"to assess the efficacy of the components in samix, we conducted an ablation
study with adaptseg+samix and daln+samix (full model) on fundus and camelyon
datasets."
216,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"this makes it expensive and
non-viable for curating large datasets for training large dnn models."
217,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"gallbladder cancer detection in ultrasound images: we use the public gbc us
dataset [3] consisting of 1255 image samples from 218 patients."
218,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"the dataset
contains 990 non-malignant (171 patients) and 265 malignant (47 patients) gb
images (see fig."
219,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"the dataset contains image labels as
well as bounding box annotations showing the malignant regions.note that, we use
only the image labels for training."
220,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"polyp detection in colonoscopy images: we use the publicly available kvasir-seg
[17] dataset consisting of 1000 white light colonoscopy images showing polyps
(see fig."
221,Gall Bladder Cancer Detection from US Images with only Image Level Labels,"since kvasir-seg does not contain any control images, we add 600
non-polyp images randomly sampled from the polypgen [1] dataset.since the
patient information is not available with the data, we use random stratified
splitting for 5-fold cross-validation."
222,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"extensive experiments on three publicly available datasets show the potential of
such models for the processing of gigapixel-sized images, under both weakly and
multi-task schemes."
223,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"camelyon16 [16] is a dataset that consists of resections of lymph nodes, where
each wsi is annotated with a binary label indicating the presence of tumour
tissue in the slide, and all slides containing tumors have a pixel-level
annotation indicating the metastatic region."
224,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"in our experiments, the average patch sequence length
arising from camelyon16 is 6129 (ranging from 127 to 27444).tcga-luad is a tcga
lung adenocarcinoma dataset that contains 541 wsis along with genetic
information about each patient."
225,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"the average sequence length is 10557 (ranging from 85 to
34560).tcga-rcc is a tcga dataset for three kidney cancer subtypes (denoted
kich, kirc, and kirp)."
226,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"we evaluate our method on each dataset by
accuracy and area under receiver operating characteristic curve (auroc)."
227,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"for the
camelyon16 dataset, our method performs on par with trans-mil and the clam
models, while it clearly outperforms the other methods."
228,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"similarly, in the
tcga-luad dataset the proposed model achieves comparable performance with both
clam models, while outperforming transmil and the other methods."
229,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"we note that
tcga-luad proves to be a more challenging dataset for all models."
230,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"moreover, our
method outperforms clam models on the tcga-rcc dataset, while reporting very
similar performance with respect to transmil."
231,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"overall, looking at the average
metrics per model across all three datasets, our proposed method achieves the
highest accuracy and the second highest auroc, only behind clam-mb."
232,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"models a and b show that stacking multiple ssm layers results in
lower accuracy, which was observed over all three datasets, while models c and d
show that modifying the state dimension of the ssm module can have an impact on
the accuracy."
233,Structured State Space Models for Multiple Instance Learning in Digital Pathology,the optimal state space dimension varies depending on the dataset.
234,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"we explored the ability of our model to combine slide-and patch-level
information on the cameylon16 dataset."
235,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"in order to highlight the inherent
ability of ssm models to effectively model long sequences, we performed an
experiment on only the largest wsis of the tcga-rcc dataset."
236,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"indeed, this
dataset contains particularly long sequences (up to 62235 patches at 20x)."
237,Structured State Space Models for Multiple Instance Learning in Digital Pathology,"finally, we demonstrated
that on the longest sequences in our datasets, state space models offer better
performance than competing models, confirming their power in modeling long-range
dependencies."
238,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"this information can be leveraged to assess treatment response, e.g., by
analyzing the evolution of size and morphology for a given tumor [1], but also
for adaptation of (re-)treatment radiotherapy plans that take into account new
tumors.in practice, the development of automatic and reliable lesion tracking
solutions is hindered by the complexity of the data (over different modalities),
the absence of large, annotated datasets, and the difficulties associated with
lesion identification (i.e., varying sizes, poses, shapes, and sparsely
distributed locations)."
239,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"furthermore, a significant focus and contribution of our research is
the experimental study at a very large scale: we (1) train a pixel-wise
self-supervised system using a very large and diverse dataset of 52,487 ct
volumes and (2) evaluate on two publicly available datasets."
240,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"notably, one of the
datasets, nlst, presents challenging cases with 68% of lesions being very small
(i.e., radius < 5 mm)."
241,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"1, given an image x ∈ r d×h×w from the training dataset
d, we randomly select two overlapping 3d patches (anchor and query), namely x a
and x q ."
242,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"datasets: we train the universal and fine-grained anatomical point matching
model using an in-house ct dataset (variousct)."
243,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"the training dataset contains
52,487 unlabeled 3d ct volumes capturing various anatomies, including chest,
head, abdomen, pelvis, and more.the evaluation is based on two datasets, the
publicly released deep longitudinal study (dls) dataset [8] and the national
lung screening trial (nlst) dataset [12]."
244,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"the dls dataset is a subset of the
deeplesion [11] medical imaging dataset, containing 3891 pairs of lesions with
information on their location and size."
245,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"the dataset covers various types of
lesions across different organs."
246,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"we follow the official data split for dls
dataset and perform evaluation on the testing dataset which comprises 480 lesion
pairs."
247,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"the nlst
testing dataset has a distinctive feature wherein nodules are relatively small,
68% of annotated lesions have a radius of less than 5 mm (compared to 6% in dls
dataset)."
248,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"for the lesion tracking task on dls dataset, we quantitatively compare our
system against existing trackers in table 1."
249,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"hence, for
performance comparison against self-supervised anatomical embedding tracker, we
retrain sam [5] with images from variousct dataset."
250,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"when imposing a maximum distance limit of 10 mm between the ground truth
and prediction, our method increases performance by 1.46%, showing the
importance of the multi-scale approach in lesion on the nlst dataset, our
proposed method obtains a center point matching accuracy of 92.12% (table 2)."
251,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,"through large-scale experiments and validation on two longitudinal datasets, we
highlight the superiority of the proposed method in comparison to
state-of-theart."
252,Geometry-Invariant Abnormality Detection,"211 scans
from nsclc radiogenomics [2,3,10,16] combined with 83 scans from a proprietary
dataset constitute our lower resolution dataset with voxel dimensions of 3.6 ×
3.6×3 mm."
253,Geometry-Invariant Abnormality Detection,"our higher resolution dataset uses autopet [10,15]
(1014 scans) with voxel dimensions of 2.036 × 2.036 × 3 mm."
254,Geometry-Invariant Abnormality Detection,"from this, 850 scans
are used for training, 64 for validation and 100 for testing.all baseline models
work in a single space with constant dimensions, obtained by registering the
autopet images to the space of the nsclc dataset.for evaluation, we use four
testing sets: a lower resolution set derived from both the nsclc and the private
dataset; a higher resolution set from autopet; a testing set with random crops
of the same nsclc/private testing dataset and finally a testing set that has
been rotated through 90 • using the high resolution testing data."
255,Geometry-Invariant Abnormality Detection,"as the cropped
and rotated dataset cannot be fed into the baseline models, we pad the images to
the common image sizing before inference."
256,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"besides using the
additional knowledge to improve performance, it can also help to increase
explainability, as has already been shown using the lidc-idri dataset [3]."
257,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"in addition to the enhanced
explainability offered by the proposed approach, to our knowledge the proposed
method outperforms existing studies on the lidc-idri dataset.the main
contributions of our work are:-a novel method that, for the first time to our
knowledge, combines privileged information and prototype learning to provide
increased explanatory power for medical classification tasks."
258,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"-an explainable solution outperforming state-of-the-art
explainable and nonexplainable methods on the lidc-idri dataset.we provide the
code with the model architecture and training algorithm of proto-caps on github."
259,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"randomly initialized, the prototypes are a
representative subset of the training dataset for each attribute after the
training."
260,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"the proposed approach is evaluated using the publicly available lidc-idri
dataset consisting of 1018 clinical thoracic ct scans from patients with
non-small cell lung cancer (nsclc) [2,3]."
261,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,"this
opens the door for application to other datasets by reducing the additional
annotation overhead."
262,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"while recent advances
in datadriven deep learning (dl) have achieved superior segmentation performance
[29], the segmentation task is often constrained by the availability of costly
pixel-wise labeled training datasets."
263,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"in addition, even if static dl models are
trained with extraordinarily large amounts of training datasets in a supervised
learning manner [29], there exists a need for a segmentor to update a trained
model with new data alongside incremental anatomical structures [24].in
real-world scenarios, clinical databases are often sequentially constructed from
various clinical sites with varying imaging protocols [19][20][21]23]."
264,Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,"simply enforcing the same statistics across domains
as [5,30,33] can weaken the model expressiveness [36].the recent brn [10]
proposes to rectify the data shift between each batch and the dataset by using
the moving average μ and σ along with the training:where η ∈ [0, 1] is applied
to balance the global statistics and the current batch.in addition, γ = σb σ and
β = μb -μ σ are used in both training and testing."
265,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"(2) large-scale colonoscopy
generation: the proposed approach can be used to generate large-scale datasets
with no/arbitrary annotations, which significantly benefits the medical image
society, laying the foundation for large-scale pre-training models in automatic
colonoscopy analysis."
266,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"through extensive
experiments, we found inaccurate sample images with coarse polyp boundary that
is not aligned properly with the original masks may introduce large biases and
noises to the datasets."
267,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"the detailed procedure of one training iteration is shown in algorithm
1 and the overall loss function is defined as:3 experiments we conducted our
experiments on five public polyp segmentation datasets:
endoscene [20], cvc-clincdb/cvc-612 [1], cvc-colondb [18], etis [14] and kvasir
[7]."
268,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"the evaluations are conducted on the
five datasets separately to verify the learning and generalization capability."
269,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"automatic generation of annotated data is essential for colonoscopy image
analysis, where the scale of existing datasets is limited by the expertise and
time required for manual annotation."
270,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,"to evaluate our
approach comprehensively, we conduct polyp segmentation and detection
experiments on five widely used datasets, where experimental results demonstrate
the effectiveness of our approach, in which model performances are greatly
enhanced with little synthesized data."
271,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"large-scale well-annotated
datasets are one of the key components for training deep learning models to
achieve satisfactory results [3,17]."
272,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"although conventional augmentation techniques [23] such as flipping and
cropping can be directly applied to medical images, they merely improve the
diversity of datasets, thus leading to marginal performance gains [1]."
273,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"another
group of studies employ conditional generative adversarial networks (cgans) [10]
to synthesize visually appealing medical images that closely resemble those in
the original datasets [36,37]."
274,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"we propose a novel synthetic
augmentation method, named histodiffusion, which can be pre-trained on
large-scale unannotated datasets and adapted to smallscale annotated datasets
for augmented training."
275,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"specifically, we first employ a latent diffusion model
(ldm) and train it on a collection of unlabeled datasets from multiple sources."
276,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"second, given a
small labeled dataset that does not exist in the pre-training datasets, the
decoder of the ldm is fine-tuned using annotations to adapt to the domain shift."
277,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"we
evaluate our proposed method on a histopathology image dataset of colorectal
cancer (crc)."
278,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"our experimental results show
that once histodiffusion is well pre-trained using large datasets, it can be
applied to any future incoming small dataset with minimal fine-tuning and may
substantially improve the flexibility and efficacy of synthetic augmentation."
279,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"first, we
train an ldm on a large-scale set of unlabeled datasets collected from multiple
sources."
280,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"we then fine-tune the decoder of this pretrained ldm on a small labeled
dataset."
281,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"to enable conditional image synthesis, we also train a latent
classifier on the same labeled dataset to guide the diffusion model in ldm."
282,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"to ensure the
latent space z can cover features of various data types, we first pre-train our
proposed his-todiffusion on large-scale unlabeled datasets."
283,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"specifically, we
gather unlabeled images from m different sources to construct a large-scale set
of datasets s = {s 1 , s 2 , ."
284,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"to generalize our histodiffusion to the
small-scale labeled dataset s collected from a different source (i.e., s ⊂ s),
we further fine-tune histodiffusion using the labeled data from s ."
285,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"to enable conditional image generation with our histodiffusion, we further apply
the classifier-guided diffusion sampling proposed in [4,29,30,33] using the
labeled data (x, y) from small-scale labeled dataset s ."
286,Synthetic Augmentation with Large-Scale Unconditional Pre-training,datasets.
287,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"we employ three public datasets of histopathology images during the
large-scale pre-training procedure."
288,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"the first one is the h&e breast cancer
dataset [2], containing 312,320 patches extracted from the hematoxylin & eosin
(h&e) stained human breast cancer tissue micro-array (tma) images [18]."
289,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"the second dataset is pannuke [9], a
pan-cancer histology dataset for nuclei instance segmentation and
classification."
290,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"the pannuke dataset includes 7,901 patches of 19 types of h&e
stained tissues obtained from multiple data sources, and each patch has a
unified size of 256×256 pixels."
291,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"the third dataset is tcga-brca-a2/e2 [34], a
subset derived from the tcga-brca breast cancer histology dataset [20]."
292,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"as for fine-tuning and
evaluation, we employ the nct-crc-he-100k dataset that contains 100,000 patches
from h&e stained histological images of human colorectal cancer (crc) and normal
tissue."
293,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"the resolution of each patch is 224 × 224.to
replicate a scenario where only a small annotated dataset is available for
training, we have opted to utilize a subset of 5,000 (5%) samples for
finetuning."
294,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"by ensuring that the fine-tuning
process is representative of the entire dataset through even sampling from each
tissue type, we can eliminate bias towards any particular tissue type."
295,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"when augmenting the training
dataset with different numbers of images synthesized from histodiffusion and
stylegan2, one can observe that when increasing the ratio of synthesized data to
100%, the fid score of stylegan2 increases quickly and can become even worse
than the one without using image selection strategy."
296,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"histodiffusion leverages multiple unlabeled datasets for large-scale,
unconditional pre-training, while employing a labeled dataset for small-scale
conditional fine-tuning."
297,Synthetic Augmentation with Large-Scale Unconditional Pre-training,"experiment results on a histopathology image dataset
excluded from the pre-training demonstrate that given limited labels,
histodiffusion with image selection remarkably enhances the classification
performance of the baseline model, and can potentially handle any future
incoming small dataset for augmented training using the same pre-trained model."
298,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"common da
strategies create new samples by using predefined transformations such as
rotation, translation, and colour jitter to existing data, where the performance
gains heavily relies on the choice of augmentation operations and parameters
[1].to mitigate this reliance, recent efforts have focused on learning optimal
augmentation operations for a given task and dataset [3,8,11,15]."
299,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"it thus will be challenging to transfer the
learned shape variations to even the same objects across different locations,
orientations, or sizes in the image, let alone transferring across dataset
(e.g., to transfer the learned shape variations of an organ from one image
modality to another).intuitively, object-centric transformations and
augmentations have the potential to overcome the challenges associated with
global image-level transformations."
300,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"this allows us
to add shape diversity to the objects of interest in an image regardless of
their positions or sizes, eventually facilitating transferring the learned
variations across datasets."
301,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"we demonstrated the effectiveness of the presented
object-centric diffeomorphic augmentation in kidney tumour segmentation,
including using shape variations of kidney tumours learned from the same dataset
(kits [7]), as well as transferring those learned from a larger liver tumour
dataset (lits [2])."
302,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"we used two publicly available datasets, lits [2] and kits [7], for our
experiments."
303,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"we considered two da scenarios: augment with transformations learned from kits
(within-data augmentation) versus from lits (cross-data augmentation).models:
for the base segmentation network, we adopted nnu-net [9] as it contains state
of the art (sota) pipeline for medical image segmentation on most datasets."
304,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"note that, as the transformations are learned as variations in object shapes,
they can be transferred easily across datasets surprisingly, the improvements
achieved by the presented augmentation strategy were the most prominent when the
segmentation was trained on 50% and 75% of the kits training set."
305,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"as demonstrated by the
experimental results, this allows us to not only introduce new variations to
unfixed objects like tumours in an image but also transfer the knowledge of
shape variations across datasets."
306,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,"in the long term, it would be interesting to explore ways to
transfer knowledge about more general forms of variations across datasets."
307,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"formally, we have dataset d ehr-pulmonary = {l k | k = 1, ."
308,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"we
represent our multimodal datasets d image-ehr and }, where t is the maximum
sequence length."
309,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,datasets.
310,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"next, ehr-pulmonary was the
unlabeled dataset used to learn clinical signatures in an unsupervised manner."
311,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"additionally, image-ehr was a labeled dataset
with paired imaging and ehrs."
312,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"we release our implementation at
https://github.com/masilab/lmsignatures.the lack of longitudinal multimodal
datasets has long been a limiting factor [24] in conducting studies such as
ours."
313,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"one of our contributions is demonstrating training strategies in a
small-dataset, incomplete-data regime."
314,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,"we were able to overcome our small cohort
size (image-ehr-spn) by leveraging unsupervised learning on datasets without
imaging (ehr-pulmonary), pretraining on public datasets without ehrs (nlst), and
pretraining on paired multimodal data with noisy labels (image-ehr) within a
flexible transformer architecture.our approach of sampling cross-sections where
clinical decisions are likely to be made scales well with long, multi-year
observation windows, which may not be true for bert-based embeddings [20,25]."
315,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"therefore, existing fully
labeled datasets (termed as flds) are very few and often low in sample size [1]."
316,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"while there exist many publicly available partially labeled datasets (plds)
[2,3], each with one or a few out of the many organs annotated."
317,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"this has
motivated the development of various partially-supervised multi-organ
segmentation (psmos) methods that aim to learn a unified model from a union of
such datasets."
318,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"-we demonstrate on
five datasets collected from different sites that our method can effectively
learn a unified mos model from multi-source datasets, achieving superior
performance over the state-of-the-art (sota) methods."
319,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"each dataset can then be
formally defined as either, where i f j,i is the i-th pixel of the j-th image in
the fld d f , and y f j,i is its corresponding label."
320,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"since
foreground organ in one pld may be labeled as background in another dataset,
such a background ambiguity brings challenges to joint training on multiple
plds."
321,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,datasets and implementation details.
322,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"we use five abdominal ct datasets (malbcvwc
[1], decathlon spleen [3], kits [2], decathlon liver [3] and decathlon pancreas
[3] datasets respectively) to evaluate the effectiveness of our method
[1][2][3]."
323,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"the spatial resolution of all these datasets are resampled to (1 × 1
× 3)mm 3 ."
324,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"we randomly split each dataset into training (60%), validation (20%)
and testing (20%)."
325,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"in the mmd calculation, for each dataset, we first
generate features from the penultimate layer."
326,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"as shown, by introducing the csfa module, the feature
distribution discrepancy in terms of mmd can be effectively alleviated across
all the ""full vs partial"" dataset pairs.comparison with the state-of-the-art
(sota) methods."
327,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"for fair comparison, all the sota methods
were trained/tested on our own dataset splits."
328,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"we reported the dsc values for each organ across test sets from all the
datasets."
329,Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,"therefore, we pay more attention to the performance on those hard organs (in our
datasets, pancreas and kidneys are deemed to be more difficult due to their
relatively small sizes)."
330,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"the experimental results
demonstrate that cafenet achieves the state-of-the-art cancer grading
performance in colorectal cancer grading datasets."
331,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"3 experiments and
results two publicly available colorectal cancer datasets [9] were employed to
evaluate
the effectiveness of the proposed cafenet."
332,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"we conducted a series of comparative experiments to evaluate the effectiveness
of cafenet for cancer grading, in comparison to several existing methods: 1)
three dcnnbased models: resnet [13], densenet [14], efficientnet [12], 2) two
metric learningbased models: triplet loss (triplet) [15] and supervised
contrastive loss (sc) [16], 3) two transformer-based models: vision transformer
(vit) [17] and swin transformer (swin) [18], and 4) one (pathology)
domain-specific model (m mae-ce o ) [9], which demonstrates the state-of-the-art
performance on the two colorectal cancer datasets under consideration."
333,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"we initialized all models using the pre-trained weights on the imagenet dataset,
and then trained them using the adam optimizer with default parameter values (β
1 = 0.9, β 2 = 0.999, ε = 1.0e-8) for 50 epochs."
334,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"to increase the variability of the dataset during the training phase, we
applied several data augmentation techniques, including affine transformation,
random horizontal and vertical flip, image blurring, random gaussian noise,
dropout, random color saturation and contrast conversion, and random contrast
transformations."
335,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"this is ascribable to the difference between the test datasets (c testi and c
testii ) and the training and validation datasets (c train and c validation )."
336,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"in the experiments on colorectal cancer datasets
against several competing models, the proposed network demonstrated that it has
a better learning capability as well as a generalizability in classifying
pathology images into different cancer grades."
337,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"however, the experiments were
only conducted on two public colorectal cancer datasets from a single institute."
338,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,shows the details of the datasets.
339,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,"both datasets provide colorectal pathology
images with ground truth labels for cancer grading."
340,Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,"each slice of the ct volumes in
the dataset has a matrix size of 512 × 512 pixels, with in-plane pixel sizes of
0.60-1.00 mm and thicknesses of 0.20-0.70 mm."
341,Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,"the dataset consists of the
original hepatic ct image with the liver mask and the ""gold-standard"" liver
tumor region manually segmented by a radiologist, as illustrated in fig."
342,Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,"also, the patches were extracted from input images from
both channels: a 5 × 5 × 5 sized patch in the same spatial position was
extracted to form a training patch with a size of 2 × 5 × 5 × 5 pixels.seven
cases and 24 cases in the dynamic contrast-enhanced ct scans dataset were used
for training and testing, respectively."
343,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"despite its success, festa
requires pretraining the vit body on a large dataset prior to its utilization in
the sl and fl training process."
344,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,datasets.
345,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,we conduct our experiments on three medical imaging datasets.
346,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"the
first dataset is ham10000 [26], a multi-class dataset comprising of 10, 015
dermoscopic images from diverse populations."
347,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"the second dataset [2] termed ""bloodmnist""
is a multi-class dataset consisting of 17, 092 blood cell images for 8 different
imbalanced cell types."
348,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"we followed [29] and split the dataset into 70% training,
10% validation, and 20% testing."
349,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"finally, the fed-isic2019 dataset consists of
23, 247 dermoscopy images for 8 different melanoma classes."
350,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"this dataset was
prepared by flamby [25] from the original isic2019 dataset [6,7,26] and the data
was collected from 6 centers, with significant differences in population
characteristics and acquisition systems, representing real-world domain shifts."
351,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"the training
samples in all datasets are divided among 6 clients, whereas the testing set is
shared among them all."
352,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,the distribution of each dataset is depicted in fig.
353,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"following [25], we used balanced accuracy in all experiments to evaluate the
performance of the classification task across all datasets."
354,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"fesvibs consistently
outperforms other methods on the three datasets with both iid and non-iid
settings."
355,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"on the other hand, svibs shows dominant performance across datasets, where the
sampling of vit blocks provides augmented representations of the input images at
different rounds and improves the generalizability."
356,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"4
(left) show consistent performance for different sets of blocks across different
datasets."
357,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"for other methods, the communication cost
per client per collaboration round: (i) fedavg/fedprox: ∼ 97m, (ii) slvit/svibs:
∼ 197m values for ham10000 dataset, and (iii) festa/fesvibs: ∼ 197m values +12m
parameters per client per unifying round."
358,FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,"we evaluate fesvibs framework under
iid and non-iid settings on three real-world medical imaging datasets and
demonstrate consistent performance."
359,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"meanwhile, vision
transformers (vit) [4] have been shown to replace cnn with a transformer encoder
in computer vision tasks and can achieve obvious advantages on large-scale
datasets."
360,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"second, no complete open liver lesion classification
datasets exist."
361,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"most relevant studies are based on private datasets, which tend
to be small in size and cause overfitting in learning models.in this paper, we
construct a hybrid framework with vit backbone for liver lesion classification,
transliver."
362,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"while most
multi-phase liver lesion classification studies use datasets with no more than
three phases (without dl phase for its difficulty of collection) or no more than
six lesion classes, we validate the whole framework on an in-house dataset with
four phases of abdominal ct and seven classes of liver lesions."
363,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"considering the
disproportion of axial lesion slice number and the relatively small scale of the
dataset, we adopt a 2-d network in classification part instead of 3-d in
pre-processing part and achieve a 90.9% accuracy."
364,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"vision transformers can get excellent performance on large-scale datasets such
as imagenet [4], but they are also prone to overfit on small datasets such as
private hospital datasets."
365,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,dataset.
366,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"the employed single-phase annotated dataset is collected from sir run
run shaw hospital (srrsh), affiliated with the zhejiang university school of
medicine, and has received the ethics approval of irb."
367,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"to
handle the imbalance of dataset, we randomly select 586 lesions as the training
and validation set with no more than 700 axial slices in each lesion type, and
the rest 175 lesions constitute the test set."
368,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"in the results of our method, hm has a relatively low performance of 62.5%,
mainly due to its low proportion in our dataset."
369,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"the details can be found in
supplementary materials.because the sources of data are different among the
methods compared above and to the best of our knowledge, no relevant study based
on transformers was found, we further train some sota normal classification
models on our dataset."
370,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"most of lesions in our dataset having few slices weakens the
redundancy between slices in 2-d pipeline, while the number of slices is still
obviously larger than the number of lesions, alleviating the overfitting issue."
371,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,"we report performance of an overall 90.9% classification
accuracy on a four-phase seven-class dataset through quantitative experiments
and show obvious improvement compared with sota classification methods."
372,Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,"for instance, in the
camelyon16 breast cancer metastases dataset [10], 49.5% of wsis contain
metastases that are smaller than 1% of the tissue, requiring a high level of
expertise and long inspection time to ensure exhaustive tumor localization;
whereas other wsis have large tumor lesions and require a substantial amount of
annotation time for boundary delineation [18]."
373,Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,"we test our method using a breast
cancer metastases segmentation task on the public camelyon16 dataset and
demonstrate that determining the selected regions individually provides greater
flexibility and efficiency than selecting regions with a uniform predefined
shape and size, given the variability in histological tissue structures."
374,Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,"we used the publicly available camelyon16 challenge dataset [10] training
schedules."
375,Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,"since the camelyon16 dataset is fully annotated, we perform al by assuming all
wsis are unannotated and revealing the annotation of a region only after it is
selected during the al procedure."
376,Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,"future work will involve the development of a wsi dataset with comprehensive
documentation of annotation time to evaluate the proposed method and an
investigation of potential combination with self-supervised learning."
377,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"to date, data-driven deep
learning (dl) methods have shown prominent segmentation performance when trained
on fully-annotated datasets [8]."
378,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"however, data annotation is a significant
bottleneck for dataset creation."
379,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"first, cold-start al aims to study the general question of
constructing a training set for an organ that has not been labeled in public
datasets."
380,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"this is a very common scenario (whenever a dataset is collected for a
new application), especially when iterative al is not an option."
381,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"we train and validate our models on five
3d medical image segmentation tasks from the publicly available medical
segmentation decathlon (msd) dataset [1], which covers two of the most common 3d
image modalities and the segmentation tasks for both healthy tissue and
tumor/pathology.our contributions are summarized as follows:• we offer the first
cold-start al benchmark for 3d medical image segmentation."
382,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"we use the medical segmentation decathlon (msd) collection [1] to define our
benchmark, due to its public accessibility and the standardized datasets
spanning across two common 3d image modalities, i.e., ct and mri."
383,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"with a low budget of 5
volumes (except for heart, where 3 volumes are used because of the smaller
dataset and easier segmentation task), we assess the performance of the
uncertainty-based and diversity-based approaches against the random selection."
384,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"1) follow the same trends.our results explain why random
selection remains a strong competitor for 3d segmentation tasks in cold-start
scenarios, as no strategy evaluated in our benchmark consistently outperforms
the random selection average performance.however, we observe that typiclust
(shown as orange) achieves comparable or superior performance compared to random
selection across all tasks in our benchmark, whereas other approaches can
significantly under-perform on certain tasks, especially challenging ones like
the liver dataset."
385,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"in this paper, we presented the colossal benchmark for cold-start al strategies
on 3d medical image segmentation using the public msd dataset."
386,COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,"we believe our findings and
the open-source benchmark will facilitate future cold-start al studies, such as
the exploration of different uncertainty estimation/feature extraction methods
and evaluation on multi-modality datasets."
387,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,dataset.
388,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,"the developed explainability framework has been validated on an in vivo
and ex vivo pcle dataset of meningioma, glioblastoma and metastases of an
invasive ductal carcinoma (idc)."
389,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,"our dataset includes 38 meningioma
videos, 24 glioblastoma and 6 idc."
390,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,"this resulted in a training dataset of 2500
frames per class (7500 frames in total) and a testing dataset of the same size."
391,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,"the dataset is split into a training and testing subset, with the division done
on the patient level.implementation."
392,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"we
evaluate our continual learning method using three datasets: btcv [8], lits [1]
and jhh [25] (a private dataset at johns hopkins hospital) 1 ."
393,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"on the public
datasets, the learning trajectory is to first segment 13 organs in the btcv
dataset, then learn to fig."
394,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"these kernels, when applied to the decoder (dec) feature, yield the
mask for the respective class.segment liver tumors in the lits dataset."
395,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"on the
private dataset, the learning trajectory is to first segment 13 organs, followed
by continual segmentation of three gastrointestinal tracts and four
cardiovascular system structures."
396,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"we formulate the continual organ segmentation as follows: given a sequence of
partially annotated datasets {d 1 , d 2 , ."
397,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,", c n }, we learn a single multi-organ segmentation model
sequentially using one dataset at a time."
398,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"when training on the i-th dataset d t
, the previous datasets {d 1 , ."
399,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"the model is
required to predict the accumulated organ labels for all seen datasets {d 1 , ."
400,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"in the context of continual organ segmentation, the model's inability to access
the previous dataset presents a challenge as it often results in the model
forgetting the previously learned classes."
401,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"formally, the label lc t for class c in current
learning step t can be expressed as:where l c t represents the ground truth
label for class c in step t obtained from dataset d t ."
402,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"datasets: we empirically evaluate the proposed model under two data settings: in
one setting, both training and continual learning are conducted on the inhouse
jhh dataset."
403,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"in the other setting, we first train on the btcv dataset and then do
continual learning on the lits dataset."
404,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"the btcv dataset contains 47 abdominal
ct images delineating 13 organs."
405,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"the lits dataset contains 130 contrast-enhanced
abdominal ct scans for liver and liver tumor segmentation."
406,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"in each
learning step, we report the average dsc for the classes that are used at the
current step as well as the previous steps (e.g., in step 2 of the jhh dataset,
we report the average dice of the gastrointestinal tracts and the abdominal
organs)."
407,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"the initial the continual segmentation results using the jhh dataset and public
datasets are
shown in tables 1 and2, respectively."
408,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"notably, by simply using the pseudo
labeling technique (lwf), we are able to achieve reasonably good performance in
remembering the old classes (dice of 0.777 in step 2 and 0.767 in step 3 for
abdominal organs in the jhh dataset; dice of 0.770 in step 2 for btcv organs)."
409,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"specifically, the proposed method exhibits the least forgetting in old classes
and a far better ability to adapt to new data and new classes.to evaluate the
proposed model designs, we also conduct the ablation study on the jhh dataset,
shown in table 3."
410,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"finally, we show the qualitative segmentation results
of the proposed method together with the best baseline method ilt on the jhh
dataset."
411,Continual Learning for Abdominal Multi-organ and Tumor Segmentation,"numerical results on an in-house dataset and two public
datasets demonstrate that the proposed method outperforms the continual learning
baseline methods in the challenging multiple organ and tumor segmentation tasks."
412,Efficient Subclass Segmentation in Medical Images,"moreover, in some
cases, a dataset may have already been annotated with superclass labels, but the
research focus has shifted towards finer-grained categories [9,24]."
413,Efficient Subclass Segmentation in Medical Images,"in such
cases, re-annotating an entire dataset may not be as cost-effective as
annotating only a small amount of data with subclass labels.here, the primary
challenge is to effectively leverage superclass annotations to facilitate the
learning of fine-grained subclasses."
414,Efficient Subclass Segmentation in Medical Images,"our experiments on the brats 2021 [3] and
acdc [5] datasets demonstrate that our model, with sufficient superclass and
very limited subclass annotations, achieves comparable accuracy to a model
trained with full subclass annotations."
415,Efficient Subclass Segmentation in Medical Images,"2, for each sample (x, y) in the dataset that does not
have subclass labels, we pair it with a randomly chosen fine-labeled sample (x ,
y , z )."
416,Efficient Subclass Segmentation in Medical Images,dataset and preprocessing.
417,Efficient Subclass Segmentation in Medical Images,we conduct all experiments on two public datasets.
418,Efficient Subclass Segmentation in Medical Images,"the first one is the acdc1 dataset [5], which contains 200 mri images with
segmentation labels for left ventricle cavity (lv), right ventricle cavity (rv),
and myocardium (myo)."
419,Efficient Subclass Segmentation in Medical Images,"the second is the brats20212 dataset [3],
which consists of 1251 mpmri scans with an isotropic 1 mm 3 resolution."
420,Efficient Subclass Segmentation in Medical Images,"we randomly split the dataset into 876, 125, and 250
cases for training, validation, and testing, respectively."
421,Efficient Subclass Segmentation in Medical Images,"for both datasets,
image intensities are normalized to values in [0, 1] and the foreground
superclass is defined as the union of all foreground subclasses for both
datasets.implementation details and evaluation metrics."
422,Efficient Subclass Segmentation in Medical Images,"to augment the data
during training, we randomly cropped the images with a patch size of 256 × 256
for the acdc dataset and 96 × 96 × 96 for the brats2021 dataset."
423,Efficient Subclass Segmentation in Medical Images,"we used a
batch size of 24 for the acdc dataset and 4 for the brats2021 dataset, where
half of the samples are labeled with subclasses and the other half only labeled
with superclasses."
424,Efficient Subclass Segmentation in Medical Images,"the first u-net
was trained on the complete subclass dataset {(x l , y l , z l )} n l=1 , while
the second was trained on its subset {(x l , y l , z l )} n l=1 ."
425,Efficient Subclass Segmentation in Medical Images,"mean dice score (%, left) and hd95 (mm,
right) of different methods on acdc and brats2021 datasets."
426,Efficient Subclass Segmentation in Medical Images,"in contrast, our
proposed method achieved the best performance among all compared methods on both
the acdc and brats2021 datasets."
427,Efficient Subclass Segmentation in Medical Images,"our experiments on
the acdc and brats2021 datasets demonstrated that our proposed approach
outperformed other compared methods in improving the segmentation accuracy."
428,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"five
expert physicians evaluated the edited images from a clinical perspective using
two datasets: a pelvic mri dataset and chest ct dataset."
429,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"ssim, and psnr were 1.41 × 10 -2 ± 1.04 × 10 -2 , 7.40 ×
10 -1 ± 0.57 × 10 -1 , and 22.5 ± 2.7 in the pelvic mri testing dataset and 5.03
× 10 -4 ± 3.03 × 10 -4 , 9.08 × 10 -1 ± 0.34 × 10 -1 , and 38.6 ± 1.7 in the
chest ct testing dataset."
430,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"the accuracies (i.e., the ratio of images correctly identified as real or
synthetic) were 0.69 ± 0.11 and 0.65 ± 0.11, for the pelvic mri and chest ct
testing datasets, respectively."
431,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"0.14 ± 0.13) for the pelvic mri testing dataset, and 0.74 ± 0.28 (vs."
432,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"0.12 ± 0.14) for the
chest ct testing dataset."
433,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"future challenges
include improving scalability with fewer manual operations, validating
segmentation maps from a more objective perspective, and comparing our proposed
algorithm with existing methods, such as those based on superpixels [10].data
use declaration and acknowledgment: the pelvic mri and chest ct datasets were
collected from the national cancer center hospital."
434,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"implementation and datasets: all neural networks were implemented in python 3.8
using the pytorch library 1.10.0 [12] on an nvidia tesla a100 gpu running cuda
10.2."
435,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"the pelvic mri dataset with
rectal cancer contained 289 image series for training and 100 image series for
testing."
436,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"the chest ct dataset with lung cancer contained 500 image
series for training and 100 image series for testing."
437,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"both were in-house datasets collected
from a single hospital."
438,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"by comparing different settings on
the pelvic mri training dataset (see supplementary information), the number of
segmentation classes of 10, the combination of t 1 , t 2 , and t 3 with moderate
magnitude, the weakly imposed reconstruction loss, and a certain value of the
margin parameter were considered suitable for self-supervised segmentation."
439,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"a similar configuration was
applied to the chest ct training dataset."
440,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,"the anatomical substructures, including the histological
structure of the colorectal wall and subregions within the lung, corresponded
well with the segmentation maps in both the pelvic mri and chest ct testing
datasets."
441,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"since unlabeled medical images are comparatively easier to obtain in larger
quantities, an alternative strategy is to perform self-supervised learning and
generate pre-trained models from unlabeled datasets."
442,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"during the
fine-tuning stage, we simply concatenate the local and global contrast models
and fine-tune the resulting model on a small target dataset."
443,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"in the fine-tuning stage, we fine-tune the model with a
limited number of labelled images x f ∈ {x 1 , x 2 , ..., x f }, where f is the
size of the fine-tuning dataset.besides the two pre-trained encoders and one
decoder, a randomly initialized decoder d g is appended to the pre-trained e g
to ensure that the embeddings have the same dimensions prior to concatenation."
444,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"we combine local and global contrast models by concatenating the output of d g
and d l 's last convolutional layer, and fine-tune on the target dataset in an
end-to-end fashion."
445,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"during both global and local pre-training stages, we pre-train the encoders on
the abdomen-1k [17] dataset."
446,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"the ct images have been curated from 12
medical centers and include multi-phase, multi-vendor, and multi-disease
cases.although segmentation masks for liver, kidney, spleen, and pancreas are
provided in this dataset, we ignore these labels during pre-training since we
are following the self-supervised protocol."
447,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"during the fine-tuning stage, we perform extensive experiments on three datasets
with respect to different regions of the human body."
448,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"abd-110 is an abdomen
dataset from [25] that contains 110 ct images from patients with various
abdominal tumors and these ct images were taken during the treatment planning
stage."
449,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"we report the average dsc on 11 abdominal organs (large bowel, duodenum,
spinal cord, liver, spleen, small bowel, pancreas, left kidney, right kidney,
stomach and gallbladder).thorax-85 is a thorax dataset from [5] that contains 85
thoracic ct images."
450,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"through extensive experiments on 3 different datasets, we demonstrate lrc
is capable of enhancing these pre-training algorithms in a consistent way."
451,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"2, we show segmentation results on abd-110, thorax-85, and han datasets
respectively."
452,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"all the results are provided by models trained with target dataset
size |x t | = 10."
453,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"by comparing (c) with (g) and (d) with (h), our method shows
significant improvement, particularly on the challenging han dataset."
454,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,"through extensive experiments on three multi-organ
segmentation datasets, we demonstrated that our approach consistently boosts
current supervised and unsupervised pre-training methods."
455,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"however, accompanied by the
promulgation of data acts and the strengthening of data privacy, it has become
increasingly challenging to train models in large-scale centralized medical
datasets."
456,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"fedavg
[1], as pioneering work, was a simple and effective aggregation algorithm, which
makes the proportions of local datasets size as the aggregation weights of local
models."
457,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"but in the real world, not only the numbers of datasets held by clients
is different, but also their data distribution may be diverse, which leads to
the fact that the data in the federated learning is non-independent identically
distribution (non-iid)."
458,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"(3) we propose an aggregation algorithm that introduces the concept of
affinity and graph into federated learning, and the aggregation weights can be
adjusted adaptively; (4) the superior performance is achieved by the proposed
method, on the public cifar-10 and fets challenge datasets."
459,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"comparisons with other state-of-the-art methods on
the cifar-10 dataset."
460,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,the first dataset to verify the validity of our algorithm is cifar-10.
461,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"the real-world dataset used in experiments is provided by the
fets challenge organizer, which is the training set of the whole dataset about
brain tumor segmentation."
462,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"in order to evaluate the performance of fedgrav, we
partition the dataset composed of 341 data samples experiment results on the
cifar-10."
463,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"we first validate the proposed method on the
cifar-10 dataset."
464,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"as can be seen from the table, the proposed
fedgrav method outperforms the other competing fl aggregation methods including
auto-fedavg, a learning-based aggregation method, which indicates the potential
and superiority of fedgrav.experiment results on miccai fets2021 training
dataset."
465,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"in order to verify the robustness of our method and its performance in
real-world data, we conduct the experiment on the miccai fets2021 training
dataset."
466,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"to evaluate the effectiveness and find the better configuration of fedgrav, we
conduct the ablation study on the fets datasets, and the results are shown in
fig."
467,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,"we evaluated our
method on cifar-10 and real-world miccai federated tumor segmentation challenge
(fets) datasets, and the superior results demonstrated the effectiveness and
robustness of our fedgrav."
468,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"as
with all deep learning-based approaches, the availability of large datasets is
essential, which is problematic in the considered case since the additional ce
low-dose scan is not acquired in clinical routine exams."
469,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"hence, there are no
public datasets to easily benchmark and compare different algorithms or evaluate
their performance."
470,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"the generator learns a non-linear transformation of a predefined
noise distribution to fit the distribution of a target dataset, while the
discriminator provides feedback by simultaneously approximating a distance or
divergence between the generated and the target distribution."
471,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"using this dataset, we aim at the semantic
interpolation of the gbca signal at various fractional dose levels."
472,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"to this end,
we use gans to learn the contrast enhancement behavior from the dataset
collective and thereby enable the synthesis of contrast signals at various dose
levels for individual cases."
473,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"to focus the generation on the contrast agent signal,
our model predicts residual images ŷld ; the corresponding low-dose can be
obtained by xld = x na + ŷld .for training and evaluation, we consider samples
(x na , x sd , y ld , d, b) of a dataset ds, where y ld = x ld -x na is the
residual image of a real ce low-dose scan x ld with dose level d ∈ d and b ∈
{1.5, 3} is the field-strength in tesla of the used scanner."
474,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"further details of the dataset and the preprocessing are
in the supplementary material."
475,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"finally,
using a distance c , the content loss l c (θ) := e (xna,xsd,yld,d,b)∼u (ds),z∼n
(0,id) c g θ (z, c) , y ld guides the generator g θ towards residual images in
the dataset."
476,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"further
details of the dataset, model and training can be found in the supplementary."
477,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,"4 visualizes
synthesized ld images on the brats dataset [6] along with the associated vcm
outputs."
478,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,"we conduct extensive
experiments on a public breast lesion ultrasound video dataset, named bluvd-186
[9]."
479,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,dataset.
480,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,"we conduct the experiments on the public bluvd-186 dataset [9],
comprising 186 videos including 112 malignant and 74 benign cases."
481,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,"the dataset
has totally 25,458 ultrasound frames, where the number of frames in a video
ranges from 28 to 413."
482,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,"we adopt the same dataset splits as in table
1."
483,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,"state-of-the-art quantitative comparison of our approach with existing
methods in literature on the bluvd-186 dataset."
484,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,"the experiments conducted on a public
breast lesion ultrasound video dataset show the efficacy of our stnet, resulting
in a superior detection performance while operating at a fast inference speed."
485,DeDA: Deep Directed Accumulator,"for fair and consistent comparison, the dataset applied in the previous work
[24] was asked for and used to demonstrate the performance of the proposed
dedabased rim parameterization da-tr."
486,DeDA: Deep Directed Accumulator,"a total of 172 subjects were included in
the dataset, and 177 lesions were identified as rim+ lesions and 3986 lesions
were identified as rim-lesions, please refer to [24] for more details about the
image acquisition and pre-processing."
487,DeDA: Deep Directed Accumulator,"transformer-based networks with fewer inductive biases rely heavily on the
use of a large training dataset or depends strongly on the feature reuse [19],
as a result, these networks as well as cnns with deeper structures are prone to
overfit small datasets.implementation details: a stratified five-fold
cross-validation procedure was applied to train and validate the performance,
and all experiments including ablation study were carried out within this
setting."
488,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"after that, we train the classifier again
using all the fine-grained labeled target class samples.we conducted two
experiments with different matching ratios (ratio of the number of target class
samples to the total number of samples) on a public 9-class colorectal cancer
pathology image dataset."
489,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"to validate the effectiveness of openal, we conducted two experiments with
different matching ratios (the ratio of the number of samples in the target
class to the total number of samples) on a 9-class public colorectal cancer
pathology image classification dataset (nct-crc-he-100k) [6]."
490,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"the dataset
contains a total of 100,000 patches of pathology images with fine-grained
labeling, with nine categories including adipose (adi 10%), background (back
11%), debris (deb 11%), lymphocytes (lym 12%), mucus (muc 9%), smooth muscle
(mus 14%), normal colon mucosa (norm 9%), cancer-associated stroma (str 10%),
and colorectal adenocarcinoma epithelium (tum, 14%)."
491,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"to construct the openset
datasets, we selected three classes, tum, lym and norm, as the target classes
and the remaining classes as the non-target classes."
492,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"we
visualize the cumulative sampling ratios of openal for the target classes in
each round on the original dataset with a 33% matching ratio, as shown in fig."
493,OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,"therefore, this framework can be applied to both datasets containing
only target class samples and datasets also containing a large number of
non-target class samples during testing."
494,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"-we provide qualitative and quantitative performance
evaluations on activity recognition with the shl [22] dataset and brain tumor
segmentation with the brats2020 [1] dataset."
495,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"the shl (sussex-huawei locomotion) challenge 2019 [22] dataset provides
data from seven sensors of a smartphone to recognize eight modes of locomotion
and transportation (activities), including still, walking, run, bike, car, bus,
train, and subway."
496,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,"the brats2020 [1] dataset provide four modality scans: t1ce, t1, t2, flair for
brain tumor segmentation."
497,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"in fss, a training dataset d tr = {(x i , y i (l))} ntr i=1 , l ∈ l tr , and a
testing dataset d te = {(x i , y i (l))} nte i=1 , l ∈ l te are available, where
(x i , y i (l)) denotes an imagemask pair of the binary class l."
498,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"however, the effect of this hyper-parameter is investigated in the supplementary
materials.dataset."
499,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"also, the performance of visa-fss was evaluated using hausedorff
distance and surface dice metrics on ct and mri datasets."
500,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,"on the ct dataset,
visa-fss reduced sslalpnet's hausedorff distance from 30.07 to 23.62 effect of
task generation."
501,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"however, as data-hungry
approaches, deep learning models require large balanced and high-quality
datasets to meet the in scl, head classes are overtreated leading to
optimization concentrating on head classes."
502,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"thus,
existing public skin datasets usually suffer from imbalanced problems which then
results in class bias of classifier, for example, poor model performance
especially on tail lesion types.to tackle the challenge of learning unbiased
classifiers with imbalanced data, many previous works focus on three main ideas,
including re-sampling data [1,18], re-weighting loss [2,15,22] and re-balancing
training strategies [10,23]."
503,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"(4) experimental
results demonstrate that the proposed framework outperforms other
state-of-theart methods on two imbalanced dermoscopic image datasets and the
ablation study shows the effectiveness of each element."
504,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"however, when dealing with an imbalanced
dataset, tail samples in a batch contribute little to the update of their
corresponding proxies due to the low probability of being sampled."
505,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"moreover, as the skin datasets are often small, richer
relations can effectively help form a high-quality distribution in the embedding
space and improve the separation of features."
506,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,dataset and evaluation metrics.
507,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"we evaluate the ecl on two publicly available
dermoscopic datasets isic2018 [5,19] and isic2019 [5,6,19]."
508,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"to ensure fairness, we re-train all methods by rerun their released codes
on our divided datasets with the same experimental settings."
509,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"it can be seen that ecl has a significant advantage with
the highest level in most metrics on two datasets."
510,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"noticeably, our ecl
outperforms other imbalanced methods by great gains, e.g., 2.56% in pre on
isic2018 compared with scl and 4.33% in f1 on isic2019 dataset compared with
tsc."
511,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,"extensive experiments on isic2018 and isic2019 datasets have demonstrated the
effectiveness and superiority of ecl over other compared methods."
512,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"since we do not have any labels in the initial
and our pre-trained model is task-agnostic, we select diverse samples to cover
the entire dataset."
513,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"these selected samples are then annotated
and serve as the initial dataset for downstream tasks.step 1: supervised
uncertainty selection."
514,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"after prompt tuning with the initial dataset, we obtain a
task-specific model that can be used to evaluate data value under supervised
training."
515,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,datasets and pre-trained model.
516,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"although there are publicly available liver
tumor datasets [1,24], they only contain major tumor types and differ in image
characteristics and label distribution from our hospital's data."
517,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"we
collected a dataset from our in-house hospital comprising 941 ct scans with
eight categories: hepatocellular carcinoma, cholangioma, metastasis,
hepatoblastoma, hemangioma, focal nodular hyperplasia, cyst, and others."
518,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"we utilized a pre-trained model for liver segmentation using
supervised learning on two public datasets [24] with no data overlap with our
downstream task."
519,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"since we aim to evaluate the efficacy of prompt
tuning on limited labeled data in table 1, we create a sub-dataset of
approximately 5% (40/752) from the original dataset."
520,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"using this sub-dataset, we evaluated various tuning methods for limited
1."
521,SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,"moreover, we
presented a diversified visual prompt tuning and a tesla strategy that combines
unsupervised and supervised selection to build annotated datasets for downstream
tasks."
522,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"(2) most segmentation tasks face the limitation of a small
labeled dataset, especially for 3d segmentation tasks, since pixel-wise 3d image
annotation is labor-intensive, time-consuming, and susceptible to operator bias."
523,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,train one model on n datasets using task-specific prompts.
524,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"although they benefit from the encoder parameter-sharing scheme and the rich
information provided by multiple training datasets, multi-head networks are
less-suitable for multi-task co-training, due to the structural redundancy
caused by the requirement of preparing a separate decoder for each task."
525,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"we collected 3237 volumetric data with three
modalities (ct, mr, and pet) and various targets (eight organs, vertebrae, and
tumors) from 11 datasets as the upstream dataset."
526,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"on this dataset, we evaluated
our uniseg model against other universal models, such as dodnet and the
clip-driven universal model."
527,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"we also compared uniseg to seven advanced
single-task models, such as cotr [26], nnformer [30], and nnunet [12], which are
trained independently on each dataset."
528,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"furthermore, to verify its generalization
ability on downstream tasks, we applied the trained uniseg to two downstream
datasets and compared it to other pre-trained models, such as mg [31], desd
[28], and unimiss [27]."
529,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"let {d 1 , d 2 , ..., d n } be n datasets."
530,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"here,j=1 represents that the i-th
dataset has a total of n i image-label pairs, and x ij and y ij are the image
and the corresponding ground truth, respectively."
531,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"straightforwardly, n tasks can
be completed by training n models on n datasets, respectively."
532,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"after training uniseg on upstream datasets, we transfer the pre-trained
encoderdecoder and randomly initialized segmentation heads to downstream tasks."
533,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"the model is fine-tuned in a fully supervised manner to minimize the sum of the
dice loss and cross-entropy loss.3 experiments and results datasets."
534,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"for this study, we collected 11 medical image segmentation datasets as
the upstream dataset to train our uniseg and single-task models."
535,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"the liver and
kidney datasets are from lits [3] and kits [11], respectively."
536,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"the hepatic
vessel (hepav), pancreas, colon, lung, and spleen datasets are from medical
segmentation decathlon (msd) [1]."
537,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"verse20 [19], prostate [18], brats21 [2], and
autopet [8] datasets have annotations of the vertebrae, prostate, brain tumors,
and whole-body tumors, respectively."
538,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"we used the binary version of the verse20
dataset, where all foreground classes are regarded as one class."
539,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"moreover, we
dropped the samples without tumors in the autopet dataset."
540,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"meanwhile, we use
btcv [14] and vs datasets [20] as downstream datasets to verify the ability of
uniseg to generalize to other medical image segmentation tasks."
541,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"the vs
dataset contains the annotations of the vestibular schwannoma."
542,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"both pre-training on eleven upstream datasets and fine-tuning on two downstream
datasets were implemented based on the nnunet framework [12]."
543,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"moreover, we
adopted a uniform sampling strategy to sample training data from upstream
datasets."
544,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"during fine-tuning, we set the batch size to
2, the initial learning rate to 0.01, the default patch size to 48 × 192 × 192,
and the maximum training iterations to 25,000 for all downstream datasets."
545,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"as shown in table 2, our uniseg achieves the highest
dice on eight datasets, beating the second-best models by 1.9%, 0.7%, 0.8%,
0.4%, 0.4%, 1.0%, 0.3%, 1.2% on the liver, kidney, hepav, pancreas, colon, lung,
prostate, and autopet datasets, respectively."
546,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"moreover, uniseg also presents
superior performance with an average margin of 1.0% and 1.6% on eleven datasets
compared to the second-best universal model and single-task model, respectively,
demonstrating its superior performance.comparing to other pre-trained models."
547,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"the former are officially
released with different backbones while the latter are trained using the
datasets and backbone used in our uniseg."
548,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"to verify the benefit of training on
multiple datasets, we also report the performance of the models per-trained on
autopet and brats21, respectively."
549,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"more important, thanks to the powerful baseline and small
gap between the pretext and downstream tasks, uniseg achieves the best
performance and competitive performance gains on downstream datasets,
demonstrating that it has learned a strong representations ability."
550,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"thanks to both designs,
our uniseg achieves superior performance on 11 upstream datasets and two
downstream datasets, setting a new record."
551,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,of single-task models and universal models on eleven datasets.
552,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"we use dice (%)
on each dataset and mean dice (%) on all datasets as metrics."
553,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,"the best results
on each dataset are in bold."
554,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"the corresponding dataset [14] used in this study is widely employed
for multi-class texture classification in colorectal cancer histology and
comprises eight types of tissue: tumor epithelium, simple stroma, complex
stroma, immune cells, debris, normal mucosal glands, adipose tissue and
background (no tissue)."
555,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,we conduct experiments on human colorectal cancer dataset [14].
556,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"the dataset
consist of 8 categories of colorectal tissues, tumor, stroma, lymph, complex,
debris, mucosa, adipose, and empty with 625 per categories."
557,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,"1, we compare our xm-gan approach for fs
tissue image generation with state-of-the-art lofgan [7] on [14] dataset.our
proposed xm-gan that utilizes dense aggregation of relevant local information at
a global receptive field along with controllable feature modulation outperforms
lofgan with a significant margin of 30.1, achieving fid score of 55.8."
558,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,table 3 shows the baseline comparison on the [14] dataset.
559,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"for example, the chestxray dataset
[18] contains about 112,120 chest x-rays used to classify common thoracic
diseases."
560,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"chestxr dataset [1] contains 17,955 chest x-rays used for covid-19
recognition."
561,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"we advocate using chest x-ray datasets such as chestxray and
chestxr may benefit covid-19 segmentation using ct scans because of three
reasons: (1) supplement limited ct data and contribute to training a more
accurate segmentation model; (2) provide large-scale chest x-rays with labeled
features, including pneumonia, thus can help the segmentation model to recognize
patterns and features specific to covid-19 infections; and (3) help improve the
generalization of the segmentation model by enabling it to learn from different
populations and imaging facilities."
562,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"we also
used two chest x-ray-based classification datasets including chestx-ray14 [18]
and chestxr [1] to assist the uci training."
563,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"the chestx-ray14 dataset comprises
112,120 x-ray images showing positive cases from 30,805 patients, encompassing
14 disease image labels pertaining to thoracic and lung ailments."
564,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"the chestxr dataset consists of 21,390 samples,
with each sample classified as healthy, pneumonia, or covid-19."
565,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"we employ the online data
argumentation, including random cropping and zooming, random rotation, and
horizontal/vertical flip, to enlarge the x-ray training dataset."
566,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"notably, despite chestxr being more focused on covid-19
recognition, the uci model aided by the chestx-ray14 dataset containing 80k
images performs better than the uci model using the chestxr dataset with only
16k images."
567,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,"this suggests that having a larger auxiliary dataset can improve the
segmentation performance even if it is not directly related to the target task."
568,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,"the diversity of
object appearance, size, and shape makes the task challenging.dataset and
evaluation metric."
569,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,"we use the gland segmentation challenge dataset [17] that
contains colored light microscopy images of tissues with a wide range of
histological levels from benign to malignant."
570,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,"we also compare with suggestive annotation (sa) [23], and sa with
model quantization (qsa) [20], which use multiple fcn models to select
informative training samples from the dataset."
571,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,"our sdt framework achieves state-of-the-art performance on 5 out of 6 evaluation
metrics on the gland segmentation dataset (table 1)."
572,DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,none
573,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"the success of deep neural networks heavily relies on the availability of large
and diverse annotated datasets across a range of computer vision tasks."
574,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"to learn
a strong data representation for robust and performant medical image
segmentation, huge datasets with either many thousands of annotated data
structures or less specific self-supervised pretraining objectives with
unlabeled data are needed [29,33]."
575,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"this results in a
situation where a zoo of partially labeled datasets is available to the
community."
576,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"recent efforts have resulted in a large dataset of >1000 ct images
with >100 annotated classes each, thus providing more than 100,000 manual
annotations which can be used for pre-training [30]."
577,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"focusing on such a dataset
prevents leveraging the potentially precious additional information of the above
mentioned other datasets that are only partially annotated."
578,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"integrating
information across different datasets potentially yields a higher variety in
image acquisition protocols, more anatomical target structures or details about
them as well as information on different kinds of pathologies."
579,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"consequently,
recent advances in the field allowed utilizing partially labeled datasets to
train one integrated model [21]."
580,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"early approaches handled annotations that are
present in one dataset but missing in another by considering them as background
[5,27] and penalizing overlapping predictions by taking advantage of the fact
that organs are mutually exclusive [7,28]."
581,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"trained
one network with a shared encoder and separate decoders for each dataset to
generate a generalized encoder for transfer learning [2]."
582,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"so far, all previous methods do not convincingly leverage
cross-dataset synergies."
583,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"pointed out, one common caveat is that
many methods force the resulting model to average between distinct annotation
protocol characteristics [22] by combining labels from different datasets for
the same target structure (visualized in fig."
584,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"hence, they all fail to
reach segmentation performance on par with cutting-edge single dataset
segmentation methods."
585,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"to this end, we introduce multitalent (multi dataset
learning and pre-training), a new, flexible, multi-dataset training method: 1)
multitalent can handle classes that are absent in one dataset but annotated in
another during training."
586,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"overall, mul-titalent can include all kinds of new datasets
irrespective of their annotated target structures.multitalent can be used in two
scenarios: first, in a combined multi-dataset (md) training to generate one
foundation segmentation model that is able to predict all classes that are
present in any of the utilized partially annotated datasets, and second, for
pre-training to leverage the learned representation of this foundation model for
a new task."
587,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"in experiments with a large collection of abdominal ct datasets, the
proposed model outperformed state-of-the-art segmentation networks that were
trained on each dataset individually as well as all previous methods that
incorporated multiple datasets for training."
588,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"in comparison to an ensemble of single dataset solutions, multitalent comes with
shorter training and inference times."
589,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"additionally, at the example of three
challenging datasets, we demonstrate that fine-tuning multitalent yields higher
segmentation performance than training from scratch or initializing the model
parameters using unsupervised pretraining strategies [29,33]."
590,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"we introduce multitalent, a multi dataset learning and pre-training method, to
train a foundation medical image segmentation model."
591,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"it comes with a novel
dataset and class adaptive loss function."
592,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"furthermore, we introduce a
training schedule and dataset preprocessing which balances varying dataset size
and class characteristics."
593,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"we begin with a dataset collection of k datasets, where c (k) ⊆ c is the label
set associated to dataset d (k) ."
594,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"even if classes from different datasets refer
to the same target structure we consider them as unique, since the exact
annotation protocols and labeling characteristics of the annotations are unknown
and can vary between datasets: c (k) ∩ c (j) = ∅, ∀k = j."
595,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"to solve the label contradiction problem we decouple the
segmentation outputs for each class by applying a sigmoid activation function
instead of the commonly used softmax activation function across the dataset."
596,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"this modification allows the
network to assign multiple classes to one pixel and thus enables overlapping
classes and the conservation of all label properties from each dataset."
597,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"we modify the loss function to be calculated only for
classes that were annotated in the corresponding partially labeled dataset
[5,27], in the following indicated by 1and 0 otherwise."
598,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"this
compensates for the varying number of annotated classes in each dataset."
599,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"however, the automatic pipeline configuration from nnu-net was not used in favor
of a manually defined configuration that aims to reflect the peculiarities of
each of the datasets, irrespective of the number of training cases they contain."
600,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"for the swinunetr, we
adopted the default network topology.multi-dataset training setup."
601,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"we trained
multitalent with 13 public abdominal ct datasets with a total of 1477 3d images,
including 47 classes (multi-dataset (md) collection)
[1,3,9,11,[18][19][20]25,26]."
602,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"detailed information about the datasets, can be
found in the appendix in table 3 and fig."
603,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"to compensate
for the varying number of training images in each dataset, we choose a sampling
probability per case that is inversely proportional to √ n, where n is the
number of training cases in the corresponding source dataset."
604,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"we used the btcv
(small multi organ dataset [19]), amos (large multi organ dataset [16]) and
kits19 (pathology dataset [11]) datasets to evaluate the generalizability of the
multitalent features in a pre-training and fine tuning setting."
605,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"naturally, the
target datasets were excluded from the respective pre-training."
606,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"as a baseline for the multitalent, we applied the 3d u-net generated by the
nnu-net without manual intervention to each dataset individually."
607,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"furthermore,
we trained a 3d u-net, a resenc u-net and a swinunetr with the same network
topology, patch and batch size as our multitalent for each dataset."
608,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"as supervised baseline, we used the weights resulting
from training the three model architectures on the totalsegmentator dataset,
which consists of 1204 images and 104 classes [30], resulting in more than 10 5
annotated target structures."
609,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"as unsupervised baseline
for the cnns, we pre-trained the networks on the multi-dataset collection based
on the work of zhou et al."
610,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,multi-dataset training results are presented in fig.
611,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"multitalent improves the performance of the purely convolutional
architectures (u-net and resenc u-net) and outperforms the corresponding
baseline models that were trained on each dataset individually."
612,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"since a simple
average over all classes would introduce a biased perception due to the highly
varying numbers of images and classes, we additionally report an average over
all datasets."
613,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"for example, dataset 7 consists of only 30 training images but has
13 classes, whereas 4 in the appendix provides all results for all classes."
614,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"averaged over all datasets, the multitalent gains 1.26 dice points for the
resenc u-net architecture and 1.05 dice points for the u-net architecture."
615,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"compared to the default nnu-net, configured without manual intervention for each
dataset, the improvements are 1.56 and 0.84 dice points."
616,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"for the official btcv test set in table 1, multitalent outperforms all related
work that have also incorporated multiple datasets during training, proving that
multitalent is substantially superior to related approaches."
617,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"the training is 6.5 times faster and the inference is around 13
times faster than an ensemble of models trained on 13 datasets.transfer learning
results are found in table 2, which compares the finetuned 5-fold
cross-validation results of different pre-training strategies for three
different models on three datasets."
618,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"especially for the small multi-organ
dataset, which only has 30 training images (btcv), and for the kidney tumor
(kits19), the multi-talent pre-training boosts the segmentation results."
619,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"in
general, the results show that supervised pre-training can be beneficial for the
swinunetr as well, but pre-training on the large totalsegmentator dataset works
better than the md pre-training."
620,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"for the amos dataset, no pre-training scheme
has a substantial impact on the performance."
621,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"we suspect that it is a result of
the dataset being saturated due to its large number of training cases."
622,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,"allows including any publicly available datasets
(e.g."
623,Co-assistant Networks for Label Correction,"in particular,
obtaining high-quality labels needs professional experience so that corrupted
labels can often be found in medical datasets, which can seriously degrade the
effectiveness of medical image analysis."
624,Co-assistant Networks for Label Correction,"based on this, given two
outputs of the gmm model for the i-th sample, its output with a larger mean
value and the output with a smaller mean value, respectively, are denoted as m
i,1 and m i,2 , so the following definition v i is used to determine if the i-th
samples is noise:hence, the noise rate r of training samples is calculated
by:where n represents the total number of samples in training dataset."
625,Co-assistant Networks for Label Correction,"the used datasets are breakhis [13], isic [3], and nihcc [16]."
626,Co-assistant Networks for Label Correction,"in particular, the
random selection in our experiments guarantees that three datasets (i.e., the
training set, the testing set, and the whole set) have the same ratio for each
class."
627,Co-assistant Networks for Label Correction,"moreover, we assume that all labels in the used raw datasets are clean,
so we add corrupted labels with different noise rates = {0, 0.2, 0.4} into these
datasests, where = 0 means that all labels in the training set are clean.we
compare our proposed method with six popular methods, including one fundamental
baseline (i.e., cross-entropy (ce)), three robustness-based methods (i.e.,
co-teaching (ct) [6], nested co-teaching (nct) [2] and self-paced resistance
learning (sprl) [12]), and two label correction methods (i.e., co-correcting
(cc) [9] and self-ensemble label correction (selc) [11])."
628,Co-assistant Networks for Label Correction,table 1 presents the classification results of all methods on three datasets.
629,Co-assistant Networks for Label Correction,"first, our method obtains the best results, followed
by ct, nct, sprl, celc, cc, and ce, on all datasets in terms of four evalua-
this might be because our proposed method not only utilizes a robust method to
train a cnn for distinguishing corrupted labels from clean labels, but also
corrects them by considering their relationship among the samples within the
same class."
630,Co-assistant Networks for Label Correction,"experiments on three
medical image datasets demonstrate the effectiveness of the proposed framework."
631,Co-assistant Networks for Label Correction,"although our method has achieved promising performance, its accuracy might be
further boosted by using more powerful feature extractors, like pre-train models
on large-scale public datasets or some self-supervised methods, e.g.,
contrastive learning."
632,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"for example,
openai's improved diffusion models [21] took 1600-16000 a100 hours to be trained
on the imagenet dataset with one million images, which is prohibitively
expensive."
633,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,2: for t = t to 1 do 3: dataset.
634,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"to mimic the real-world setting, the diffusion models were
trained on a diverse dataset, including images from different centers and
scanners."
635,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"specifically, the denoising task is based on
the aapm low dose ct grand challenge abdominal dataset [19], which can be also
used for sr [33]."
636,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"the heart mr sr task is based on three datasets: acdc [1],
m&ms1-2 [3], and cmrxmotion [27]."
637,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"for the ct image enhancement task, we trained a
diffusion model [21] based on the full-dose dataset that contains 5351 images,
and the hold-out quarter-dose images were used for testing."
638,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,"for the mr
enhancement task, we used the whole acdc [1] and m&ms1-2 [3] for training the
diffusion model and the cmrxmotion [27] dataset for testing."
639,Robust T-Loss for Medical Image Segmentation,"despite efforts to obtain labels through
automated mining [31] and crowd-sourcing methods [11], the quality of datasets
gathered using these methods remains challenging due to often high levels of
label noise.for instance, the fitzpatrick 17k dataset, commonly used in
dermatology research, contains non-skin images and noisy annotations."
640,Robust T-Loss for Medical Image Segmentation,"the dataset was scraped from online atlases, which makes it vulnerable to
inaccuracies and noise [10]."
641,Robust T-Loss for Medical Image Segmentation,"noisy labels are and will continue to be, a problem
in medical datasets."
642,Robust T-Loss for Medical Image Segmentation,"the t-loss, whose simplest formulation features a
single parameter, can adaptively learn an optimal tolerance level to label noise
directly during backpropagation, eliminating the need for additional
computations such as the expectation maximization (em) steps.to evaluate the
effectiveness of the t-loss as a robust loss function for medical semantic
segmentation, we conducted experiments on two widely-used benchmark datasets in
the field: one for skin lesion segmentation and the other for lung segmentation."
643,Robust T-Loss for Medical Image Segmentation,"we injected different levels of noise into these datasets that simulate typical
human labeling errors and trained deep learning models using various robust loss
functions."
644,Robust T-Loss for Medical Image Segmentation,"section 3 covers the datasets
used in our experiments, the implementation and training details of t-loss, and
the metrics used for comparison."
645,Robust T-Loss for Medical Image Segmentation,"section 4 presents the main findings of our
study, including the results of the t-loss and the baselines on both datasets
and an ablation study on the parameter of t-loss."
646,Robust T-Loss for Medical Image Segmentation,"the isic 2017 dataset [5] is a well-known public benchmark of dermoscopy images
for skin cancer detection."
647,Robust T-Loss for Medical Image Segmentation,"the dataset also includes a list of
lesion attributes, such as size, shape, and color."
648,Robust T-Loss for Medical Image Segmentation,"we resized the images to 256
× 256 pixels for our experiments.shenzhen [4,13,25] is a public dataset
containing 566 frontal chest radiographs with corresponding lung segmentation
masks for tuberculosis detection."
649,Robust T-Loss for Medical Image Segmentation,"all images were resized to 256 × 256
pixels.without a public benchmark with real noisy and clean segmentation masks,
we artificially inject additional mask noise in these two datasets to test the
model's robustness to low annotation quality."
650,Robust T-Loss for Medical Image Segmentation,"the nnu-net was trained for 100 epochs using the adam optimizer with a
learning rate of 10 -3 and a batch size of 16 for the isic dataset and 8 for the
shenzhen dataset."
651,Robust T-Loss for Medical Image Segmentation,"we present experimental results for the skin lesion segmentation task on the
isic dataset in table 1."
652,Robust T-Loss for Medical Image Segmentation,"similar to the isic dataset, all considered robust losses perform well at low
noise levels."
653,Robust T-Loss for Medical Image Segmentation,"to shed light on this mechanism, we study the
behavior of ν during training for different label noise levels and
initializations on the isic dataset."
654,Robust T-Loss for Medical Image Segmentation,"our evaluation on public medical
datasets for skin lesion and lung segmentation demonstrates that the t-loss
outperforms other robust losses by a statistically significant margin."
655,Robust T-Loss for Medical Image Segmentation,"our
loss function also features remarkable independence to different noise types and
levels.it should be noted that other methods, such as [15] offer better
performance for segmentation on the isic dataset with the same synthetic noisy
labels, while the t-loss offers a simple alternative."
656,Robust T-Loss for Medical Image Segmentation,"we declare that we have used the isic dataset [5] under the apache license 2.0,
publicly available, and the shenzhen dataset [4,13,25] public available under
the cc by-nc-sa 4.0 license."
657,Multi-Head Multi-Loss Model Calibration,"a binary classifier in a
balanced dataset, randomly predicting always one class with c = 0.5 +
confidence, has a perfect calibration and 50% accuracy."
658,Multi-Head Multi-Loss Model Calibration,"we now describe the data we used for experimentation, carefully analyze
performance for each dataset, and end up with a discussion of our findings."
659,Multi-Head Multi-Loss Model Calibration,"we conducted experiments on two datasets: 1) the chaoyang dataset1 , which
contains colon histopathology images."
660,Multi-Head Multi-Loss Model Calibration,"2) kvasir2 , a dataset for the task of endoscopic
image classification."
661,Multi-Head Multi-Loss Model Calibration,"the annotated part of this dataset contains 10,662 images,
and it represents a challenging classification problem due a high amount of
classes (23) and highly imbalanced class frequencies [2]."
662,Multi-Head Multi-Loss Model Calibration,"for the sake of
readability we do not show measures of dispersion, but we add them to the
supplementary material (appendix b), together with further experiments on other
datasets.we implement the proposed approach by optimizing several popular neural
network architectures, namely a common resnet50 and two more recent models: a
convnext [23] and a swin-transformer [23]."
663,Multi-Head Multi-Loss Model Calibration,"finally we would ideally observe
improved performance as we increase the diversity (comparing 2hsl to 2hml) and
as we add heads (comparing 2hml to 4hml).chaoyang: in table 1 we report the
results on the chaoyang dataset."
664,Multi-Head Multi-Loss Model Calibration,"overall, accuracy is relatively low, since this
dataset is challenging due to label ambiguity, and therefore calibration
analysis of aleatoric uncertainty becomes meaningful here."
665,Multi-Head Multi-Loss Model Calibration,"kvasir: next, we show in table 2 results for the kvasir dataset."
666,Multi-Head Multi-Loss Model Calibration,"comprehensive
experiments on two challenging datasets with three different neural networks
show that multi-head multi-loss models consistently outperform other
learning-based calibration techniques, matching and sometimes surpassing the
calibration of deep ensembles."
667,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"deep learning models have achieved remarkable success in segmenting anatomy and
lesions from medical images but often rely on large-scale manually annotated
datasets [1][2][3]."
668,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"we compare 5 existing guidance signals on the autopet [1] and
msd spleen [2] datasets and vary various hyperparameters."
669,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"we
implemented our experiments with monai label [23] and will release our code.we
trained and evaluated all of our models on the openly available autopet [1] and
msd spleen [2] datasets."
670,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"geodesic maps exhibit lower dice
scores for small σ < 5 and achieve the best performance for σ = 5 on both
datasets."
671,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"differences in results for different σ values are more pronounced in autopet [1]
as it is a more challenging dataset [17][18][19]25].(h2) theta."
672,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"1d) also confirm that θ = 10 is the optimal parameter for both
datasets and that not truncating values on msd spleen [2], i.e."
673,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"this holds true for
both datasets and the difference in performance is substantial.(h4) probability
of interaction."
674,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"although the concrete values for msd spleen [2] autopet [1] are
different, the five metrics follow the same trend on both datasets.(m1) initial
and (m2) final dice."
675,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"moreover, geodesic-based
signals have lower initial scores on both datasets and require more
interactions.(m3) consistent improvement."
676,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"the consistent improvement is ≈ 65%
for both datasets, but it is slightly worse for autopet [1] as it is more
challenging."
677,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"the guidances
are ranked in the same order in (m3) and in (m4) for both datasets."
678,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,"distance
transform-based guidances are the slowest on both datasets due to their
complexity, but all guidance signals are computed in a reasonable time (<1
s).adaptive heatmaps: results."
679,Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,"we tested our model using the synapse dataset [13], which comprises 30 cases of
contrast-enhanced abdominal clinical ct scans (a total of 3,779 axial slices)."
680,Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,"we also followed [2] experiments to evaluate
our method on the isic 2018 skin lesion dataset [6] with 2,694 images."
681,Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,"our approach outperforms other competitors across most evaluation
metrics, indicating its excellent generalization ability across different
datasets."
682,Understanding Silent Failures in Medical Image Classification,"[3] studied failure detection on several biomedical
datasets, but only assessed the performance of csfs in isolation without
considering the classifier's ability to prevent failures."
683,Understanding Silent Failures in Medical Image Classification,"we compare various csfs under a wide range
of distribution shifts on four biomedical datasets."
684,Understanding Silent Failures in Medical Image Classification,"to this end, we present sf-visuals, a visualization tool that
facilitates identifying silent failures in a dataset and investigating their
causes (see fig."
685,Understanding Silent Failures in Medical Image Classification,"we follow the
spirit of recent robustness benchmarks, where existing datasets have been
enhanced by various distribution shifts to evaluate methods under a wide range
of failure sources and thus simulate real-world application [19,27]."
686,Understanding Silent Failures in Medical Image Classification,"specifically, we introduce corruptions of various intensity levels to
the images in four datasets in the form of brightness, motion blur, elastic
transformations and gaussian noise."
687,Understanding Silent Failures in Medical Image Classification,"dermoscopy
dataset: we combine data from isic 2020 [26], derma 7 point [17], ph2 [24] and
ham10000 [30] and map all lesion sub-types to the super-classes ""benign"" or
""malignant""."
688,Understanding Silent Failures in Medical Image Classification,"chest x-ray dataset: we pool the data from chexpert [14], nih14 [31] and
mimic [16], while only retaining the classes common to all three."
689,Understanding Silent Failures in Medical Image Classification,"fc-microscopy dataset: the rxrx1 dataset [28] represents
the fluorescence cell microscopy domain."
690,Understanding Silent Failures in Medical Image Classification,"lung nodule ct dataset: we create a simple 2d
binary nodule classification task based on the 3d lidc-idri data [1] by
selecting the slice with the largest annotation per nodule (±two slices
resulting in 5 slices per nodule)."
691,Understanding Silent Failures in Medical Image Classification,"we emulate two manifestation shifts by defining nodules with high
spiculation (rating > 2), and low texture (rating < 3) as target domains.the
datasets consist only of publicly available data, our benchmark provides scripts
to automatically generate the combined datasets and distribution shifts.the
sf-visuals tool: visualizing silent failures."
692,Understanding Silent Failures in Medical Image Classification,"the proposed tool is based on
three simple operations, that enable effective and intuitive analysis of silent
failures in datasets across various csfs: 1) interactive scatter plots: see
example in fig."
693,Understanding Silent Failures in Medical Image Classification,"for corruption shifts, we further allow
investigating the predictions on a fixed input image over varying intensity
levels.based on these visualizations, the functionality of sf-visuals is
three-fold: 1) visual analysis of the dataset including distribution shifts."
694,Understanding Silent Failures in Medical Image Classification,"2)
visual analysis of the general behavior of various csfs on a given task 3)
visual analysis of individual silent failures in the dataset for various csfs."
695,Understanding Silent Failures in Medical Image Classification,"training settings: on each dataset, we
employ the classifier behind the respective leading results in literature: for
chest xray data we use densenet121 [12], for dermoscopy data we use
efficientnet-b4 [29] and for fluorescence cell microscopy and lung nodule ct
data we us densenet161 [12]."
696,Understanding Silent Failures in Medical Image Classification,"however, the method is
not reliable across all settings, falling short on manifestation shifts and
corruptions on the lung nodule ct dataset."
697,Understanding Silent Failures in Medical Image Classification,"when looking beyond the averages
displayed in table 1 and analyzing the results of individual clinical centers,
corruptions and manifestation shifts, one remarkable pattern can be observed: in
various cases, the same csf showed opposing behavior between two variants of the
same shift on the same dataset."
698,Understanding Silent Failures in Medical Image Classification,"outperforms all
other csfs for one clinical site (mskcc) as target domain, but falls short on
the other one (hcb).on the chest x-ray dataset, mcd worsens the performance for
darkening corruptions across all csfs and intensity levels, whereas the opposite
is observed for brightening corruptions."
699,Understanding Silent Failures in Medical Image Classification,"further, on the lung nodule ct dataset,
dg-mcd-res performs best on bright/dark corruptions and the spiculation
manifestation shift, but worst on noise corruption and falls behind on the
texture manifestation shift."
700,Understanding Silent Failures in Medical Image Classification,"1b, left) provides an overview of the mskcc acquisition shift
on the dermoscopy dataset and reveals a severe change of the data distribution."
701,Understanding Silent Failures in Medical Image Classification,"this
pattern of contrary class characteristics is also observed on the dermoscopy
dataset (2c)."
702,Understanding Silent Failures in Medical Image Classification,"the failure example at the top is particularly severe, and
localization in the scatter plot reveals a position deep inside the 'benign'
cluster indicating either a severe sampling error in the dataset (e.g."
703,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"extensive experiments on both
isic2019 dataset and in-house pancreas tumor dataset demonstrate that the
proposed ernn significantly improves the reliability and accuracy of ood
detection for clinical applications."
704,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,datasets.
705,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"we conduct experiments on isic 2019 dataset [3,4,19] and an inhouse
dataset."
706,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"the in-house pancreas
tumor dataset collected from a cooperative hospital is composed of eight
classes: pdac (302), ipmn (71), net (43), scn (37), asc (33), cp (6), mcn (3),
and panin (1)."
707,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"• posterior network described in
[2], where density estimators are used for generating the parameters of
dirichlet distributions.inspired by [14], we further compare the proposed method
with mixup-based methods:• mixup: as described in [23], mix up is applied to all
samples.• mt-mixup: mix up is only applied to mid-class and tail-class samples.•
mtmx-prototype: on the basis of mt mixup, prototype network is also applied to
estimate uncertainty.the results on two datasets are shown in table 1."
708,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"even
with using mixup, ernn exhibits near performance with the best method
(mtmx-prototype) on isic 2019 and outperforms the other methods on in-house
datasets."
709,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"in this section, we conduct a detailed ablation study to clearly demonstrate the
effectiveness of our major technical components, which consist of evaluation of
evidential head, evaluation of the proposed evidence reconcile block on both
isic 2019 dataset and our in-house pancreas tumor dataset."
710,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"as shown in
table 2, it is clear that a network with an evidential head can improve the ood
detection capability by 6% and 1% on isic dataset and in-house pancreas tumor
dataset respectively."
711,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"furthermore, introducing erb further improves the ood
detection performance of ernn by 1% on isic dataset."
712,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,"and on the more challenging
inhouse dataset, which has more similarities in samples, the proposed method
improves the auroc by 2.3%, demonstrating the effectiveness and robustness of
our model on more challenging tasks."
713,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"(b): statistics of the number of nodules at different scales in three
datasets."
714,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"the main reason is that the lesion scale in the
two public datasets are relatively small, which matches the fact few patients
have very large nodule or mass."
715,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"experimental results on two public datasets and one
in-house dataset demonstrate that the proposed method outperforms existing
methods with different backbones."
716,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"to determine the super parametric values for the mapping function
r, we perform cross-validation on three datasets."
717,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,we experiment on two public datasets and one in-house dataset.
718,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"all three
datasets are divided into training, validation, and test sets using a 7:1:2
ratio."
719,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"the lidc dataset is a publicly available lung ct image database containing 1018
scans, developed by the lung image database consortium (lidc)."
720,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"all pulmonary
nodules and masses in the dataset have been annotated by multiple raters."
721,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"overall, we selected a total of 1625 nodules
and masses that were annotated by more than three raters from the lidc dataset
for the experiment."
722,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"the lndb dataset published in 2019, comprises 294 ct scans collected between
2016 and 2018."
723,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"each ct scan in the dataset has been segmented by at least one
radiologist."
724,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,the nodules included in this dataset are larger than 3 mm.
725,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"the mean
scale of the lesion in lndb dataset is the shortest among the three datasets."
726,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"we
adopt 1968 nodules and masses from the lndb dataset."
727,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"we
exclude nodules and masses with diameters larger than 64 mm or smaller than 2
mm, as the diameter of the largest mass in the public dataset is no more than 64
mm."
728,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"the experimental results presented in table 1, consistently
demonstrate that the cnn-based network can achieve better results in multiscale
pulmonary nodule and mass segmentation tasks across all three datasets."
729,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"in datasets such as lidc and in-house, where the number imbalance of
multi-scale lesion phenomena is more notable, the multi-input method
consistently outperforms the other two baselines."
730,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"firstly, we present the quantitative comparison in table 2, where we
group the nodules and masses in each dataset at 10 mm intervals and calculate
the average segmentation performance differences of the nodules in each scale
group."
731,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,"extensive experiments on two public datasets and one in-house dataset
demonstrate that though sattca increases inference time for each sample by about
one second, it outperforms the corresponding baseline and click-based methods
with different backbones."
732,WeakPolyp: You only Look Bounding Box for Polyp Segmentation,"more meaningfully, weakpolyp can take existing large-scale polyp detection
datasets to assist the polyp segmentation task."
733,WeakPolyp: You only Look Bounding Box for Polyp Segmentation,datasets.
734,WeakPolyp: You only Look Bounding Box for Polyp Segmentation,"two large polyp datasets are adopted to evaluate the model
performance, including sun-seg [9] and polyp-seg."
735,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"the work presented in [10]
proposed the first pixel-wise annotated benchmark dataset for breast lesion
segmentation in us videos, but it has some limitations."
736,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"although their efforts
were commendable, this dataset is private and contains only 63 videos with 4,619
annotated frames."
737,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"the small dataset size increases the risk of overfitting and
limits the generalizability capability."
738,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"in this work, we collected a
larger-scale us video breast lesion segmentation dataset with 572 videos and
34,300 annotated frames, of which 222 videos contain aln metastasis, covering a
wide range of realistic clinical scenarios."
739,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"please refer to table 1 for a
detailed comparison between our dataset and existing datasets.although the
existing benchmark method dpstt [10] has shown promising results for breast
lesion segmentation in us videos, it only uses the ultrasound image to read
memory for learning temporal features."
740,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"the experimental results
unequivocally showcase that our network surpasses state-of-the-art techniques in
the realm of both breast lesion segmentation in us videos and two video polyp
segmentation benchmark datasets (fig."
741,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"to support advancements in breast lesion segmentation and aln metastasis
prediction, we collected a dataset containing 572 breast lesion ultrasound
videos with 34,300 annotated frames."
742,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"table 1 summarizes the statistics of
existing breast lesion us video datasets."
743,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"unlike previous datasets [10,12], our
dataset has a reserved validation set to avoid model overfitting."
744,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"the entire
dataset is partitioned into training, validation, and test sets in a proportion
of 4:2:4, yielding a total of 230 training videos, 112 validation videos, and
230 test videos for comprehensive benchmarking purposes."
745,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"moreover, apart from
the segmentation annotation, our dataset also includes lesion bounding box
labels, which enables benchmarking breast lesion detection in ultrasound videos."
746,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,more dataset statistics are available in the supplementary.
747,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"to initialize the backbone of our network, we pretrained
res2net-50 [6] on the imagenet dataset, while the remaining components of our
network were trained from scratch."
748,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"additionally, we retrain these networks on our dataset and fine-tune their
network parameters to attain their optimal segmentation performance, enabling
accurate and meaningful comparisons.quantitative comparisons."
749,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"following the experimental protocol
employed in a recent study on video polyp segmentation [8], we retrain our
network and present quantitative results on two benchmark datasets, namely
cvc-300-tv [2] and cvc-612-v [3]."
750,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"table 4 showcases the dice, iou, s α , e φ ,
and mae results achieved by our network in comparison to state-of-the-art
methods on these two datasets."
751,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"our method demonstrates clear superiority over
state-ofthe-art methods in terms of dice, iou, e φ , and mae on both the
cvc-300-tv and cvc-612-v datasets."
752,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"specifically, our method enhances the dice
score from 0.840 to 0.874, the iou score from 0.745 to 0.789, the e φ score from
0.921 to 0.969, and reduces the mae score from 0.013 to 0.010 for the cvc-300-tv
dataset."
753,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"similarly, for the cvc-612-v dataset, our method achieves improvements
of 0.012, 0.014, 0.019, and 0 in dice, iou, e φ , and mae scores, respectively."
754,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"in this study, we introduce a novel approach for segmenting breast lesions in
ultrasound videos, leveraging a larger dataset consisting of 572 videos
containing a total of 34,300 annotated frames."
755,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,"our proposed method surpasses existing
state-of-the-art techniques in terms of performance on our annotated dataset as
well as two publicly available video polyp segmentation datasets."
756,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"extensive experiments on 5 benchmark datasets suggest that our proposed
modifications have the potential to improve unet models."
757,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"in order to evaluate acc-unet, we conducted experiments on 5 public datasets
across different tasks and modalities."
758,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"for the glas dataset, we considered the original test split as the test
data, for the other datasets we randomly selected 20% of images as test data."
759,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"apparently, for the comparatively
larger datasets (isic-18) transformer-based swin-unet was the 2nd best method,
as transformers require more data for proper training [2]."
760,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"on the other end of
the spectrum, lightweight convolutional model (multiresunet) achieved the 2nd
best score for small datasets (glas)."
761,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"for the remaining datasets, hybrid model
(uctransnet) seemed to perform as the 2 nd best method."
762,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"smeswin-unet fell behind
in all the cases, despite having such a large number of parameters, which in
turn probably makes it difficult to be trained on small-scale datasets.however,
our model combining the design principles of transformers with the inductive
bias of cnns seemed to perform best in all the different categories with much
lower parameters."
763,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"compared to much larger state-of-the-art models, for the 5
datasets, we achieved 0.13%, 0.10%, 0.63%, 0.90%, 0.27% improvements in dice
score, respectively."
764,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"each row of the figure comprises one example
from each of the datasets and the segmentation predicted by acc-unet and the
ground truth mask are presented in the rightmost two columns."
765,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"for the 1 st
example from the isic-18 dataset, our model did not oversegment but rather
followed the lesion boundary."
766,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"similarly, in the 4 th sample from the covid dataset, we were capable to model
the gaps in the consolidation of the left lung visually better, which in turn
resulted in 2.9% higher dice score than the 2 nd best method."
767,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"again, in the
final example from the glas dataset, we not only successfully predicted the
gland at the bottom right corner but also identified the glands at the top left
individually, which were mostly missed or merged by the other models,
respectively."
768,ACC-UNet: A Completely Convolutional UNet Model for the 2020s,"we performed an ablation study on the cvc-clinicdb dataset to analyze the
contributions of the different design choices in our roadmap (fig."
769,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"third, our methodology
advances state-of-the-art performance via extensive experiments on both
realworld and benchmark datasets."
770,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"to evaluate our method, we use a large private dataset with 400 ct scans and a
large public dataset with 300 ct scans (amos [9])."
771,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"as far as we know, the amos
dataset is the only publicly available ct dataset including prostate ground
truth."
772,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"we randomly split the private dataset with 280 scans for training, 40 for
validation, and 80 for testing."
773,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"the amos dataset has 200 scans for training and
100 for testing [9]."
774,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"although the amos dataset includes the prostate class, it
mixes the prostate (in males) and the uterus (in females) into one single class
labeled pro/ute."
775,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"we interpolate
all ct scans into an isotropic voxel spacing of [1.0 × 1.0 × 1.5] mm for both
datasets."
776,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"for the private
dataset, we train models for 200 epochs using the adamw optimizer with an
initial learning rate of 5e -4 ."
777,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"however, we did not get improved performance compared
with directly applying the training parameters learned from tuning the private
dataset."
778,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"detailed information regarding the number of parameters, flops, and average
inference time can be found in the supplementary materials.quantitative results
are presented in table 1, which shows that the proposed focalunetr, even without
co-training, outperforms other fcn and transformer baselines (2d and 3d) in both
datasets for most of the metrics."
779,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"the amos dataset mixes the
prostate(males)/uterus(females, a relatively small portion)."
780,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"thus, the overall performance of focalunetr is overshadowed by this
challenge, resulting in only moderate improvement over the baselines on the amos
dataset."
781,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"however, the performance margin significantly improves when using the
real-world (private) dataset."
782,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"to better examine the efficacy of the auxiliary task for
focalunetr, we selected different settings of λ 1 and λ 2 for the overall loss
function l tol on the private dataset."
783,FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,"extensive experiments on two large ct
datasets have shown that the focalunetr outperforms state-ofthe-art methods for
the prostate segmentation task."
784,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"the latter
however requires a dataset containing multiple annotations per image to obtain
optimal results.previous methods provide pixel-wise uncertainty estimates."
785,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"let's consider a dataset made of n pairs {x i , y k i } n i=1 , each pair
consisting of an image x i ∈ r h×w of height h and width w , and a series of k
ordered points y k i , drawn by an expert."
786,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"the camus dataset [20] contains cardiac ultrasounds from 500 patients,
for which two-chamber and four-chamber sequences were acquired.manual
annotations for the endocardium and epicardium borders of the left ventricle
(lv) and the left atrium were obtained from a cardiologist for the end-diastolic
(ed) and end-systolic (es) frames."
787,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"the dataset is split into 400 training
patients, 50 validation patients, and 50 testing patients."
788,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"this is a proprietary multi-site multi-vendor dataset
containing 2d echocardiograms of apical two and four chambers from 890 patients."
789,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"the dataset is split into a training/validation set (80/20)
and an independent test set from different sites, comprised of 994
echocardiograms from 684 patients and 368 echocardiograms from 206 patients,
respectively."
790,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"the japanese society of radiological technology (jsrt) dataset
consists of 247 chest x-rays [26]."
791,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"we used a network based on enet [24] for the ultrasound data and on deeplabv3
[7] for the jsrt dataset to derive both the segmentation maps and regress the
per-landmark heatmaps."
792,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"reliability diagrams [22]
for the 3 datasets."
793,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"as can be seen, our uncertainty
estimation method is globally better than the other approaches except for the
correlation score on the camus dataset which is slightly larger for tta."
794,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"4 show that our method is systematically better
aligned to perfect calibration (dashed line) for all datasets, which explains
why our method has a lower mce."
795,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"with the exception of the private cardiac us
dataset, the skewed normal distribution model shows very similar or improved
results for both correlation and mutual information compared to the univariate
and bivariate models."
796,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"it can be noted, however, that in specific instances, the
asymmetric model performs better on private cardiac us dataset (c.f."
797,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"for instance, structures such as the left ventricle and the myocardium wall in
the ultrasound datasets have large components of their contour oriented along
the vertical direction which allows the univariate and bivariate models to
perform as well, if not better, than the asymmetric model."
798,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,"the contrast between the left ventricle and myocardium in the images of
the private cardiac us dataset is small, which explains why the simpler
univariate and bivariate models perform well."
799,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"however, most of these methods do not
consider the class imbalance issue, which is common in medical image datasets."
800,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"however,
the proposed indicators can not model the difficulty well, and the benefits may
be overestimated due to the non-representative datasets used (fig."
801,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"1(c)), which satisfies the design ethos of a heterogeneous framework.the
key contributions of our work can be summarized as follows: 1) we first state
the homogeneity issue of cps and improve it with a novel dual-debiased
heterogeneous co-training framework targeting the class imbalance issue; 2) we
propose two novel weighting strategies, distdw and diffdw, which effectively
solve two critical issues of ssl: data and learning biases; 3) we introduce two
public datasets, synapse [9] and amos [7], as new benchmarks for
class-imbalanced semi-supervised medical image segmentation."
802,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"these datasets
include sufficient classes and significant imbalance ratios (> 500 : 1), making
them ideal for evaluating the effectiveness of class-imbalance-targeted
algorithm designs."
803,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"assume that the whole dataset consists of n l labeled samples {(x l i , y i )}
nl i=1 and n u unlabeled samples {x u i } nu i=1 , where x i ∈ r d×h×w is the
input volume and y i ∈ r k×d×h×w is the ground-truth annotation with k classes
(including background)."
804,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"the difficulty-aware weights
for all classes are dataset and implementation details."
805,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"we introduce two new benchmarks on the
synapse [9] and amos [7] datasets for class-imbalanced semi-supervised medical
image segmentation."
806,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"the synapse dataset has 13 foreground classes, including
spleen (sp), right kidney (rk), left kidney (lk), gallbladder (ga), esophagus
(es), liver(li), stomach(st), aorta (ao), inferior vena cava (ivc), portal &
splenic veins (psv), pancreas (pa), right adrenal gland (rag), left adrenal
gland (lag) with one background and 30 axial contrast-enhanced abdominal ct
scans."
807,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"compared with synapse, the amos dataset excludes psv but
adds three new classes: duodenum(du), bladder(bl) and prostate/uterus(p/u)."
808,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"furthermore,
our method outperforms sota methods on synapse by larger margins than the amos
dataset, demonstrating the more prominent stability and effectiveness of the
proposed dhc framework in scenarios with a severe lack of data."
809,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,"more results on datasets with different labeled
ratios can be found in the supplementary material."
810,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"extensive experiments on two publicly available datasets named
3dircadb [20] and lits [2] demonstrate that our proposed framework achieves
state-of-the-art (sota) performance, outperforming cutting-edge methods
quantitatively and qualitatively."
811,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"we evaluated the proposed framework on two publicly available datasets, 3dircadb
[20] and lits [2]."
812,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"the 3dircadb dataset [20] contains 20 ct images with spacing
ranging from 0.56 mm to 0.87 mm, and slice thickness ranging from 1 mm to 4 mm
with liver and liver vessel segmentation labels."
813,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"the lits dataset [2] consists
of 200 ct images, with a spacing of 0.56 mm to 1.0 mm and slice thickness of
0.45 mm to 6.0 mm, and has liver and liver tumour labels, but without vessels."
814,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"we annotated the 20 subjects of the 3dircadb dataset [20] with the couinaud
segments and randomly divided 10 subjects for training and another 10 subjects
for testing."
815,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"for lits dataset [2], we observed the vessel structure on ct
images, annotated the couinaud segments of 131 subjects, and randomly selected
66 subjects for training and 65 for testing.we have used three widely used
metrics, i.e., accuracy (acc, in %), dice similarity metric (dice, in %), and
average surface distance (asd, in mm) to evaluate the performance of the
couinaud segmentation."
816,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"based on
the liver mask has been extracted, we train a 3d unet [6] on the 3diradb dataset
[20] to generate the vessel attention map of two datasets."
817,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"by comparing the first two rows, we can see that point-net2plus [16]
and 3d unet [6] have achieved close performance in the lits dataset [2], which
demonstrates the potential of the point-based methods in the couinaud
segmentation task."
818,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"especially on the 3diradb dataset [20] with only 10 training subjects, the acc
and dice achieved by our method exceed pointnet2plus [16] and 3d unet [6] by
nearly 10 points, and the asd is also greatly reduced, which demonstrates the
effectiveness of the combining point-based and voxel-based methods.qualitative
comparison."
819,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"figure 4 shows the ablation experimental results obtained on all the
couinaud segments of two datasets, under the dice and the asd metrics."
820,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,"it can be
seen that our full method is significantly better than the cnn branch joint
decoder method on both metrics of two datasets, which demonstrates the
performance gain by the combined point-based branch."
821,HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,dataset and evaluation metrics.
822,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,"following [23,41],
we use qubiq 2020, which contains 7 segmentation tasks in 4 different ct and mr
datasets: prostate (55 cases, 2 tasks, 6 raters), brain growth (39 cases, 1
task, 7 raters), brain tumor (32 cases, 3 tasks, 3 raters), and kidney (24
cases, 1 task, 3 raters)."
823,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,"for each dataset, we calculate the average dice score
between each rater and the majority votes in table 1."
824,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,"in some datasets, such as
brain tumor t2, the inter-rater disagreement can be quite substantial."
825,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,"in our method, we average annotations
with uniform weights for brain tumor t2 and with each rater's dice score for all
other datasets."
826,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"datasets: we evaluate our work on two different da tasks to evaluate its
generalizability: (1) polyp segmentation from colonoscopy images in kvasir-seg
[11] and cvc-endoscene still [20], and (2) brain tumor segmentation in mri
images from brats2018 [16]."
827,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"act [14] simply
ignores the domain gap and only learns content semantics, resulting in
substandard performance on the brats dataset that has a significant domain gap."
828,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"the
margins are even higher for less labeled data (1l) on the brats dataset, which
is promising considering the difficulty of the task."
829,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"similar results are observed for the brats dataset in
table 2, where our work achieved a margin of upto 2.4% dsc than its closest
performer."
830,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,"we have
similar ablation study observations on the brats2018 dataset, which is provided
in the supplementary file, along with some qualitative examples along with
available ground truth."
831,BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,"3) experimental results on lidc-idri and brats 2021
datasets demonstrate that our berdiff outperforms other state-of-the-art
methods."
832,BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,dataset and preprocessing.
833,BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,"the data used in this experiment are obtained from
lidc-idri [2,7] and brats 2021 [4] datasets."
834,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"to obtain
the final prediction, we average the obtained maps and obtain one soft map.we
evaluate the performance of the proposed method on a dataset of medical images
annotated by multiple annotators."
835,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"the total number of diffusion steps t is set by the user, and c is
the number of different annotators in the dataset."
836,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"our experiments were carried out on
datasets of the qubiq benchmark1 ."
837,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,datasets.
838,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"the quantification of
uncertainties in biomedical image quantification challenge (qubiq), is a
recently available challenge dataset specifically for the evaluation of
inter-rater variability."
839,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"qubiq comprises four different segmentation datasets
with ct and mri modalities, including brain growth (one task, mri, seven raters,
34 cases for training and 5 cases for testing), brain tumor (one task, mri,
three raters, 28 cases for training and 4 cases for testing), prostate (two
subtasks, mri, six raters, 33 cases for training and 15 cases for testing), and
kidney (one task, ct, three raters, 20 cases for training and 4 cases for
testing)."
840,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"based on the intuition that the
more rrdb blocks, the better the results, we used as many blocks as we could fit
on the gpu without overly reducing batch size.following [13], for all datasets
of the qubiq benchmark the input image resolution, as well as the test image
resolution, was 256 × 256."
841,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"as can be seen in table
1, our method outperforms all other methods across all datasets of qubiq
benchmark.ablation study."
842,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"we
also note that our ""no annotator"" variant outperforms the analog amis model in
four out of five datasets, indicating that our architecture is somewhat
preferable."
843,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"for
example, for the brain and the prostate 1 datasets, optimal performance is
achieved using 5 generated images, while on prostate 2 the optimal performance
is achieved using 25 gen- erated images."
844,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"figure 4 depicts samples from multiple
datasets and presents the progression as the number of generated images
increases."
845,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"in order to investigate the relationship between the annotator agreement and the
performance of our model, we conducted an analysis by calculating the average
dice score between each pair of annotators across the entire dataset."
846,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"we observed that
our proposed method demonstrated improved performance on datasets with higher
agreement among annotators, specifically the kidney and prostate 1 datasets."
847,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,"conversely, the performance of the other methods significantly deteriorated on
the kidney dataset, leading to a lower correlation between the dice score and
the overall performance."
848,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,dataset.
849,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"we conducted experiments on two representative public polyp
segmentation datasets: kvasir-seg [11] and etis-larib polyp db [25] (etis)."
850,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"4, we visualize data augmentation results with (anti-) adversarial
perturbations on kvasir-seg dataset."
851,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,"extensive
experiments with various backbones and datasets confirm the effectiveness of our
method."
852,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,image datasets.
853,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"the proposed methodology was evaluated on two publicly available
datasets: our recently released han-seg dataset [14] and the pddca dataset [15]."
854,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"the han-seg dataset comprises ct and t1-weighted mr images of 56 patients, which
were deformably registered with the simpleelastix registration tool, and
corresponding curated manual delineations of 30 oars (for details, please refer
to [14])."
855,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"on the other hand, to evaluate the generalization ability of our
method, we also conducted experiments on the ct-only pddca dataset (for details,
please refer to [15]), from which we collected 15 images from the offand on-site
test sets of the corresponding challenge for our evaluation."
856,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"as this dataset is
widely used for evaluating the performance of automatic han oar segmentation
methods, it serves as a valuable benchmark for comparison with other
state-of-the-art methods."
857,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"note that none of the images from the ct-only pddca
dataset were used for training, and as our model expects two inputs, we
substituted the missing mr modality with an empty matrix (i.e."
858,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"to address the challenge of a relatively small dataset, we adopted a
4-fold cross-validation strategy without using any external training images."
859,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"the overall good
performance on the han-seg dataset suggests that all models are close to the
maximal performance, which is bounded by the quality of reference segmentations."
860,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,"however, the performance on the pddca dataset that consists only of ct images
allows us to test how the models handle the missing modality scenario and
perform on an out-ofdistribution dataset, as images from this dataset were not
used for training."
861,Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,"-we conducted extensive
experiments using the brats 2021 [12] dataset for brain tumor segmentation and a
liver mri dataset for liver tumor segmentation."
862,Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,dataset.
863,Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,"we used two mri datasets that brats 2021 [1,2,12] and a liver mri
dataset."
864,Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,"for the number of ddpm paths, brats 2021 dataset is
equal to 4 corresponding to the input 4 mri modalities and the liver mri dataset
is equal to 3 corresponding to the input 3 mri modalities."
865,Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,"table 3 and table 4 show the learned reliability
coefficients η on brats 2021 dataset and liver mri dataset."
866,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"however, the effectiveness
of vit-based approaches heavily relies on access to large datasets for learning
meaningful representations of input images."
867,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"moreover,
multitask learning acts as a regularizer by introducing inductive bias and
prevents overfitting [25] (particularly with vits), and with that, can mitigate
the challenges posed by small bus dataset sizes."
868,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"we evaluated the performance of hybrid-mt-estan using four public datasets, hmss
[9], busi [10], busis [20], and dataset b [6]."
869,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"we combined all four datasets to
build a large and diverse dataset with a total of 3,320 b-mode bus images, of
which 1,664 contain benign tumors and 1,656 have malignant tumors."
870,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"table 1 shows
the detailed information for each dataset."
871,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"hmss dataset does not provide the
segmentation ground-truth masks, and for this study we arranged with a group of
experienced radiologists to prepare the masks for hmss."
872,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"refer to the original
publications of the datasets for more details."
873,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"all bus images in the
dataset were zero-padded and reshaped to form square images."
874,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,"the proposed approach utilizes the building blocks of resnet50 and
swin-transformer-v2, pretrained on imagenet dataset."
875,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"among them, swinunetr [23] has
achieved the new top performance in the msd challenge and beyond the cranial
vault (btcv) segmentation challenge by pretraining on large datasets."
876,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"swinunetr reaches top performances
on several large benchmarks, making itself the current sota, but without
effective pretraining and excessive tuning, its performance on new datasets and
challenges is not as high-performing as expected.a straightforward direction to
improve transformers is to combine the merits of both convolutions and
self-attentions."
877,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"the network is evaluated extensively on a
variety of benchmarks and achieved top performances on the word [17], flare2021
[18], msd prostate, msd lung cancer, and msd pancreas cancer datasets [1]."
878,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"compared to the original swin-unetr which needs extensive recipe tuning on a new
dataset, we utilized the same training recipe with minimum changes across all
benchmarks, showcasing the straightforward applicability of swinunetr-v2 to
reach state-of-the-art without extensive hyperparameter tuning or pretraining."
879,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"to make fair comparisons with baselines, we
did not use any pre-trained weights.datasets."
880,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"the network is validated on five
datasets of different sizes, targets and modalities:1) the word dataset [17] the
challenge comes from segmenting small tumors from large full 3d ct images."
881,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"the
pancreas dataset contains 281 3d ct scans with annotated pancreas and tumors (or
cysts)."
882,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"for msd datasets, we perform 5-fold cross-validation and ran the baseline
experiments with our codebase using exactly the same hyperparameters as
mentioned."
883,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"we perform
the study on the word dataset, and the mean test dice and hd95 scores are shown
in table 5."
884,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,"extensive experiments are performed on a variety of challenging datasets, and
swinunetr-v2 achieved promising improvements."
885,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,"the method obtained a dice value
of 83% using the interval-slice annotation, on a testing dataset containing only
28 patients.in this study, we propose a simple yet effective weakly-supervised
strategy, by using extreme points as annotations (see fig."
886,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,"we
evaluate our method on a collected dce-mri dataset containing 206 subjects."
887,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,dataset.
888,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,"we evaluated our method on an in-house breast dce-mri dataset collected
from the cancer center of sun yat-sen university."
889,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,"we randomly divided
the dataset into 21 scans for training and the remaining scans for testing 1 ."
890,Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,dataset and implementation.
891,Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,"we evaluate the our uml network on two datasets
refuge [14] and ispy-1 [13]."
892,Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,"a total of 157 patients who suffer
the breast cancer are considered -43 achieve pcr and 114 non-pcr.for each case,
we cut out the slices in the 3d image and totally got 1,570 2d images, which are
randomly divided into the train, validation, and test datasets with 1,230, 170,
and 170 slices, respectively."
893,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"our approach
achieves state-of-the-art performance on both datasets, demonstrating the
effectiveness of the proposed framework."
894,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"this task is conducted on
dataset from isbi 2019 chaos challenge [12], which contains 20 volumes of
t2-spir mr abdominal scans."
895,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"for both datasets, we repeat the evaluation protocols for four times and
report the average metrics and their standard deviation on test set."
896,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"the batch sizes for both
datasets are 2."
897,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"we benchmark our methods against previous
wsss works on two datasets in table 1 & 2, in terms of dice score, mean
intersection over union (miou), and hausdorff distance (hd95)."
898,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"firstly, our proposed method, even without classifier guidance,
outperform all other wsss methods including the classifier guided diffusion
model cg-diff on both datasets for all three metrics."
899,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"secondly, all wsss methods have performance
drop on kidney dataset compared with brats dataset."
900,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"this demonstrates that the
kidney segmentation task is a more challenging task for wsss than brain tumor
task, which may be caused by the small training size and diverse appearance
across slices in the chaos dataset."
901,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"the default setting is cg-cdm on brats dataset
with q = 400, r = 10, τ = 0.95, and s = 10."
902,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,"when compared with
latest wsss methods on two public medical image segmentation datasets, our
method shows superior performance regarding both segmentation accuracy and
inference efficiency."
903,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,"and for semantic segmentation tasks, many methods, such as setr
[20] and segformer [21], use vit as the direct backbone network and combine it
with a taskspecific segmentation head for prediction results, reaching excellent
performance on some 2d natural image datasets."
904,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,datasets.
905,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,"the
brats 2021 dataset reflects real clinical diagnostic species and has four
spatially aligned mri modality data, namely t1, t1ce, t2, and flair, which are
obtained from different devices or according to different imaging protocols."
906,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,"the
dataset contains three distinct sub-regions of brain tumors, namely peritumoral
edema, enhancing tumor, and tumor core."
907,HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,"this provides useful insights that are usually
unavailable in other studies.experimental results on the brats'19 dataset
[2,3,22] show that the proposed models have superior robustness to training
image resolution than other tested models with less than 1% of their model
parameters."
908,HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,"the dataset of brats'19 with 335 cases of gliomas was used, each with four
modalities of t1, post-contrast t1, t2, and t2-flair images with 240 × 240 × 155
voxels [3]."
909,HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,"there is also an official validation dataset of 125 cases in the
same format without given annotations."
910,HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,"in training, we split the training dataset (335 cases) into
90% for training and 10% for validation."
911,HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,"in testing, each model was tested on
the official validation dataset (125 cases) with 240 × 240 × 155 voxels
regardless of the downsampling factor."
912,HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,"although only the results of a dataset are shown because of
the page limit, the characteristics of the proposed models can be demonstrated
through this challenging multi-modal brain tumor segmentation problem."
913,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"meanwhile, it is common to have access to multiple,
diverse, yet small-sized datasets (100 s to 1000 ss of images per dataset) for
the same mis task, e.g., ph2 [25] and isic 2018 [11] in dermatology, lits [6]
and chaos [18] in liver ct, or oasis [24] and adni [17] in brain mri."
914,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"as each
dataset alone is too small to properly train a vit, the challenge becomes how to
effectively leverage the different datasets."
915,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"nevertheless, one notable limitation in these approaches is
that they are not universal, i.e., they rely on separate training for each
dataset rather than incorporate valuable knowledge from related domains."
916,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"as a
result, they can incur additional training, inference, and memory costs, which
is especially challenging when dealing with multiple small datasets in the
context of mis tasks."
917,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"multi-domain learning, which trains a single universal
model to tackle all the datasets simultaneously, has been found promising for
reducing computational demands while still leveraging information from multiple
domains [1,21]."
918,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"[4,21], directly mixing all the datasets for
training, i.e., joint training, may improve a model's performance on one dataset
while degrading performance on other datasets with non-negligible unrelated
domain-specific information, a phenomenon referred to as negative knowledge
transfer (nkt) [1,38]."
919,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"however, those mat
techniques are built based on cnn rather than vit or are scalable, i.e., the
models' size at the inference time increases linearly with the number of
domains.to address vits' data-hunger, in this work, we propose mdvit, a novel
fixedsize multi-domain vit trained to adaptively aggregate valuable knowledge
from multiple datasets (domains) for improved segmentation."
920,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"besides, for better representation learning across domains, we propose a novel
mutual knowledge distillation approach that transfers knowledge between a
universal network (spanning all the domains) and additional domain-specific
network branches.we summarize our contributions as follows: (1) to the best of
our knowledge, we are the first to introduce multi-domain learning to alleviate
vits' data-hunger when facing limited samples per dataset."
921,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"(3) the experiments on 4 skin
lesion segmentation datasets show that our multi-domain adaptive training
outperforms separate and joint training (st and jt), especially a 10.16%
improvement in iou on the skin cancer detection dataset compared to st and that
mdvit outperforms state-of-the-art data-efficient vits and multi-domain learning
strategies."
922,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"training samples {(x, y )} come from m datasets, each
representing a domain."
923,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"we aim to build and train a single vit that performs well
on all domain data and addresses the insufficiency of samples in any of the
datasets."
924,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"the universal
network experiences all the domains and grasps the domain-shared knowledge,
which is beneficial for peer learning.each auxiliary peer is trained on a small,
individual dataset specific to that peer (fig."
925,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"to achieve a rapid training
process and prevent overfitting, particularly when working with numerous
training datasets, we adapt a lightweight multilayer perception (mlp) decoder
designed for vit encoders [36] to our peers' architecture."
926,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"we study 4 skin lesion segmentation datasets collected from varied sources: isic
2018 (isic) [11], dermofit image library (dmf) [3], skin cancer detection (scd)
[14], and ph2 [25], which contain 2594, 1300, 206, and 200 samples,
respectively."
927,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"to facilitate a fairer performance comparison across datasets, as
in [4], we only use the 1212 images from dmf that exhibited similar lesion
conditions as those in other datasets."
928,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"1, to train all the models from
scratch on the skin datasets."
929,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"we deploy
models on a single titan v gpu and train them for 200 epochs with the adamw [23]
optimizer, a batch size of 16, ensuring 4 samples from each dataset, and an
initial learning rate of 1×10 -4 , which changes through a linear decay
scheduler whose step size is 50 and decay factor γ = 0.5."
930,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"in table 2-a,b, compared with base in st, base in jt improves the segmentation
performance on small datasets (ph2 and scd) but at the expense of diminished
performance on larger datasets (isic and dmf)."
931,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"this is expected given the
non-negligible inter-domain heterogeneity between skin lesion datasets, as found
by bayasi et al."
932,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"the above results demonstrate that shared knowledge in
related domains facilitates training a vit on small datasets while, without a
well-designed multi-domain algorithm, causing negative knowledge transfer (nkt)
due to inter-domain heterogeneity, i.e., the model's performance decreases on
other datasets."
933,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"nevertheless, in jt, these models also suffer from nkt: they perform better than
models in st on some datasets, like scd, and worse on others, like isic."
934,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"though bat and transfuse in st have better results on
some datasets like isic, they require extra compute resources to train m models
as well as an m -fold increase in memory requirements."
935,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"the above results
indicate that domain-shared knowledge is especially beneficial for training
relatively small datasets such as scd.we employ the two fixed-size (i.e.,
independent of m ) multi-domain algorithms proposed by rundo et al."
936,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"we propose a new algorithm to alleviate vision transformers (vits)' datahunger
in small datasets by aggregating valuable knowledge from multiple related
domains."
937,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"the experiments on 4 skin lesion segmentation datasets show that mdvit
outperformed sota data-efficient medical image segmentation vits and
multi-domain learning methods."
938,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,"previous mis
vits mitigated the data-hunger in one dataset by adding inductive bias, e.g.,
swinunet the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8 43."
939,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"we evaluate m-genseg on a modified version of the brats
2020 dataset, in which each type of sequence (t1, t2, t1ce and flair) is
considered as a distinct modality."
940,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"experiments were performed on the brats 2020 challenge dataset [23][24][25],
adapted for the cross-modality tumor segmentation problem where images are known
to be diseased or healthy."
941,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"datasets were then assembled from each distinct pair of the four
mri contrasts available (t1, t2, t1ce and flair)."
942,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"however, this modified
version of the dataset provides an excellent study case for the evaluation of
any modality adaptation method for tumor segmentation."
943,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,"3 the dice performance on the target modality
for (i) supervised segmentation on source data without domain adaptation, (ii)
domain adaptation methods and (iii) uagan [26], a model designed for unpaired
multi-modal datasets, trained on all source and target data."
944,Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,"for the first time, eamtnet has achieved high performance with the dice
similarity coefficient (dsc) up to 90.01 ± 1.23%, and the mean absolute error
(mae) of the md, x o , y o and area are down to 2.72 ± 0.58 mm,1.87±0.76 mm,
2.14 ± 0.93 mm and 15.76 ± 8.02 cm 2 , respectively.dataset and configuration."
945,Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,"an axial dataset includes 250 distinct subjects, each underwent initial standard
clinical liver mri protocol examinations with corresponding pre-contrast images
(t2fs [4mm]) and dwi [4mm]) was collected."
946,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,"to evaluate the proposed rcs-yolo model, we used the brain tumor detection 2020
dataset (br35h) [3], with a total of 701 images in the 'train' and 'val' two
folders, 500 images of which are the 'train' folder were selected as the
training set, while the other 201 images in the 'val' folder as the testing set."
947,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,"the
small object is defined as the object whose pixel size is less than 32 × 32
defined by the ms coco dataset [18], so there are no small objects in the brain
tumor medical image data sets, and the scale change of the target boxes is
smooth, almost square."
948,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,"evaluation of the brain mri dataset
shows superior performance for brain tumor detection in terms of both speed and
precision, as compared to yolov6, yolov7, and yolov8 models."
949,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,"evaluation on a publicly
available brain tumor detection annotated dataset shows superior detection
accuracy and speed compared to other state-of-the-art yolo architectures."
950,Certification of Deep Learning Models for Medical Image Segmentation,"by extension, we
show that current diffusion models, trained on 'classical images' generalize
well to medical datasets for denoising tasks."
951,Certification of Deep Learning Models for Medical Image Segmentation,"extensive experiments on five
public medical datasets of chest x-rays [21,31], skin lesions [10], and
colonoscopies [6], and different popular segmentation models, prove the
potential of our method."
952,Certification of Deep Learning Models for Medical Image Segmentation,"in this paper, we propose to leverage randomized smoothing and
diffusion models for certified segmentation on medical datasets, setting the
first baseline for this challenging problem and certifying popular segmentation
architectures."
953,Certification of Deep Learning Models for Medical Image Segmentation,datasets: we perform experiments on 5 different publicly available datasets.
954,Certification of Deep Learning Models for Medical Image Segmentation,"all
datasets were divided to 70% for training, 10% for validation, and 20% for
testing."
955,Certification of Deep Learning Models for Medical Image Segmentation,"the testing set is the one used to compute certified results.chest
x-rays datasets: jsrt dataset [31] with annotations of lung, heart, and
clavicles provided by [35] is used."
956,Certification of Deep Learning Models for Medical Image Segmentation,this dataset contains 247 images.
957,Certification of Deep Learning Models for Medical Image Segmentation,"for lung
segmentation only, we use both the montgomery and shenzen datasets [21]."
958,Certification of Deep Learning Models for Medical Image Segmentation,this dataset consists of 2694 rgb dermatoscopy images.
959,Certification of Deep Learning Models for Medical Image Segmentation,"colonoscopy images: cvc-clinicdb dataset [6] containing 612 colonoscopy images
in rgb together with their annotations were utilized.implementation details: we
train three different segmentation models namely, a unet [28], a resunet++ [22],
and a deeplabv2 [9] with and without noise."
960,Certification of Deep Learning Models for Medical Image Segmentation,"our code is made publicly available at:
https://github.com/othmanela/medical_cert_seg.results and discussion: for all
five datasets, we compute a certified dice score and certified mean intersection
over union (iou)."
961,Certification of Deep Learning Models for Medical Image Segmentation,"we also report the percentage of abstentions (% ) representing
the mean number of pixels on which the model's prediction confidence was
insufficient with respect to the radius r.the lower the percentage of
abstentions the better the segmentation model is.in table 1, we compare our
method using 3 different and popular architectures (unet, resunet++, and
deeplabv2) on the chest x-rays datasets."
962,Certification of Deep Learning Models for Medical Image Segmentation,"a comparison of our method and segcertify using the
resunet++ architecture is presented in table 2 for the three chest x-ray
datasets."
963,Certification of Deep Learning Models for Medical Image Segmentation,"1 for our proposed
method and segcertify for the different datasets and different levels of noise."
964,Certification of Deep Learning Models for Medical Image Segmentation,"we train
three unet models (one for each noise level) on the jsrt dataset."
965,Certification of Deep Learning Models for Medical Image Segmentation,"in this paper, we present the first work on certified segmentation for medical
imaging, and extensively evaluate it on five different datasets and three deep
learning segmentation models."
966,Certification of Deep Learning Models for Medical Image Segmentation,"our technique leverages off-the-shelf denoising
and segmentation models and provides the highest certified dice and miou on
multi-class and binary segmentation of five different datasets."
967,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,"in our scenario of ms-ssl, we have access to a local target dataset d local
(consisted of a labeled sub-set d l local and an unlabeled sub-set d u local )
and the external unlabeled support datasets d u e = m j=1 d u,j e , where m is
the number of support centers."
968,A Sheaf Theoretic Perspective for Robust Prostate Segmentation,"the total loss to train our entire
framework is: in the task of anatomical segmentation, the first two columns of
table 1 show
the results for the domain shift from runmc in the decathlon dataset to
bmc.here, we demonstrate that our method improves segmentation performance in
all evaluation metrics compared to the baseline, nn-unet and the other sdg
methods."
969,A Sheaf Theoretic Perspective for Robust Prostate Segmentation,"similar findings are noted for the domain shift from the internal
dataset to the runmc data in the prostatex2 dataset (second two columns of table
1).in table 2, we note our method significantly improves tumour segmentation and
localisation performance."
970,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"however, transformers are plagued by
the necessity of large annotated datasets to maximize performance benefits owing
to their limited inductive bias."
971,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"while such datasets are common to natural
images (imagenet-1k [6], imagenet-21k [26]), medical image datasets usually
suffer from the lack of abundant high quality annotations [19]."
972,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"the authors
paired large kernel con-vnext networks with enormous datasets to outperform
erstwhile state-of-the-art transformer-based networks."
973,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"however, 3d-ux-net only uses these blocks partially in a
standard convolutional encoder, limiting their possible benefits.in this work,
we maximize the potential of a convnext design while uniquely addressing
challenges of limited datasets in medical image segmentation."
974,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"compression layer: convolution
layer with 1 × 1 × 1 kernel and c output channels performing channel-wise
compression of the feature maps.mednext is convolutional and retains the
inductive bias inherent to conv-nets that allows easier training on sparse
medical datasets."
975,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"convnext architectures
in classification of natural images, despite the benefit of large datasets such
as imagenet-1k and imagenet-21k, are seen to saturate at kernels of size 7 × 7 ×
7 [22]."
976,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"we use 4 popular tasks, encompassing organ as well as tumor segmentation tasks,
to comprehensively demonstrate the benefits of the mednext architecture -1)
beyond-the-cranial-vault (btcv) abdominal ct organ segmentation [16], 2) amos22
abdominal ct organ segmentation [14] 3) kidney tumor segmentation challenge 2019
dataset (kits19) [11], 4) brain tumor segmentation challenge 2021 (brats21) [1]."
977,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"btcv, amos22 and kits19 datasets contain 30, 200 and 210 ct volumes with 13, 15
and 2 classes respectively, while the brats21 dataset contains 1251 mri volumes
with 3 classes."
978,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"we ablate the mednext-b configuration on amos22 and btcv datasets to highlight
the efficacy of our improvements and demonstrate that a vanilla convnext is
unable to compete with existing segmentation baselines such as nnunet."
979,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"we further
establish the performance of the mednext architecture against our baselines
-comprising of convolutional, transformer-based and large kernel baselines -on
all 4 datasets."
980,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"in 5-fold cv scores in table 2,
med-next, with 3 × 3 × 3 kernels, takes advantage of depth and width scaling to
provide state-of-the-art segmentation performance against every baseline on all
4 datasets with no additional training data."
981,MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,"in this work, mednext is presented as
a scalable transformer-inspired fully-convnext 3d segmentation architecture
customized for high performance on limited medical image datasets."
982,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"more
specifically, we use the classic unet [17] as the cnn backbone and evaluate our
kspc-net on the publicly available miccai hecktor (head and neck tumor
segmentation) challenge 2021 dataset."
983,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"the dataset is from the hecktor challenge in miccai 2021 (head and neck tumor
segmentation challenge)."
984,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"the hecktor training dataset consists of 224 patients
diagnosed with oropharyngeal cancer [1]."
985,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"table 1 shows the quantitative
comparison of different approaches on hecktor dataset."
986,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,"promising performance was achieved by
our proposed kspc-net compared to the state-of-the-art approaches on the miccai
2021 challenge dataset (hecktor)."
987,A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,"although our fusion idea is quite simple, a2fseg achieves
state-of-the-art (sota) performance in the incomplete multimodal brain tumor
image segmentation task on the brats2020 dataset."
988,A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,"-we conduct
experiments on the brats 2020 dataset and achieve the sota segmentation
performance, having a mean dice core of 89.79% for the whole tumor, 82.72% for
the tumor core, and 66.71% for the enhancing tumor."
989,A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,"the dataset is randomly
split into 70% for training, 10% for validation, and 20% for testing, and all
methods are evaluated on the same dataset and data splitting."
990,A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,"figure 2 visualizes the segmentation results of samples from the brats2020
dataset."
991,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"in this way, inconsistency between the augmented features and the corresponding
labels can be effectively reduced.our method is evaluated using pre-operative
multimodal mr brain images of 1726 diffuse glioma patients collected from
cooperation hospitals and a public dataset brats2019 [12] containing multimodal
mr brain images of 210 patients."
992,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"assume that d = {x 1 , ...,
x n } is the dataset containing pre-operative multimodal mr brain images of
diffuse glioma patients, and n is the number of patients."
993,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"in the in-house dataset, the proportions of the three tumor types are
20.9% (oligodendroglioma), 28.7% (astrocytoma), and 50.4% (glioblastoma), which
is consistent with the statistical report in [13]."
994,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"in our experiment, both in-house and public datasets are used to evaluate our
method."
995,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"specifically, the in-house dataset collected in cooperation hospitals
contains pre-operative multimodal mr images, including t1, t1 contrast enhanced
(t1c), t2, and flair, of 1726 patients (age 49.7 ± 13.1) with confirmed diffuse
glioma types."
996,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"besides the inhouse
dataset, a public dataset brats2019, including pre-operative multimodal mr
images of 210 non-censored patients (age 61.4 ± 12.2), is adopted as the
external independent testing dataset."
997,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"all images of the in-house and brats2019
datasets go through the same pre-processing stage, including image normalization
and affine transformation to mni152 [17]."
998,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"concordance index (c-index) is adopted to quantify the
prediction accuracy:where d = {x 1 , ..., x n } is the dataset containing all
patients, t i and t j are ground truth of survival times of the i-th and j-th
patients, r i and r j are the days predicted by rf, mcsp, and pgsp or risks
predicted by the deep cox proportional hazard models (i.e., deepconvsurv and our
method), 1 x<y = 1 if x < y, else 0, and δ i = 0 or 1 when the i-th patient is
censored or non-censored."
999,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"as rf, mcsp, and pgsp cannot use the censored data in
the in-house dataset, 80% of the non-censored data (594 patients) are randomly
selected as the training data, and the rest 20% non-censored data (149 patients)
are for testing."
1000,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"so besides the 80%
non-censored patients, all censored data (983 patients) are also included in the
training data.table 1 shows the evaluation results of the in-house and the
external independent (brats2019) testing datasets using all methods under
evaluation."
1001,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"for the in-house dataset, the
resulting c-indices are 0.744 (baseline-1) and 0.735 (baseline-2)."
1002,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"for the
external independent testing dataset brats2019, the resulting c-indices are
0.738 (baseline-1) and 0.714 (baseline-2), and our method still has more than 6%
improvement comparing with baseline-2.figure 3 shows the distributions of
tumor-type-related features (after the gap) of the in-house testing data in
baseline-2, and our method."
1003,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,"both in-house and public
datasets containing 1936 patients were used in the experiment."
1004,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"we evaluate our
model on two popular skin lesion segmentation datasets, isic-2016 and ph 2
datasets, and find that it performs significantly better than existing models."
1005,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"finally, the segmentation map is generated as3 experiment datasets: we use two
publicly available skin lesion segmentation datasets from
different institutions in our experiments: the isic-2016 dataset and the ph 2
dataset."
1006,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"the isic-2016 dataset [8] is provided by the international skin imaging
collaboration (isic) archive and consists of 900 samples in the public training
set and 379 samples in the public validation set."
1007,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"as the annotation for its
public test set is not currently available, we additionally collect the ph 2
dataset [13], which contains 200 labeled samples and is used to evaluate the
generalization performance of our methods.evaluation metrics: to comprehensively
compare the segmentation results, particularly the boundary delineations, we
employ four commonly used metrics to quantitatively evaluate the performance of
our segmentation methods."
1008,Medical Boundary Diffusion Model for Skin Lesion Segmentation,regarding dataset.
1009,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"the quantitative results are shown in table
1, which reports four evaluation scores for two datasets."
1010,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"moreover, our method shows a larger improvement
in generalization performance on the ph 2 dataset, indicating its better ability
to handle new data."
1011,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"2, including three samples from the isic-2016 validation set and three from
the ph 2 dataset."
1012,Medical Boundary Diffusion Model for Skin Lesion Segmentation,"our
method is evaluated on two well-known skin lesion segmentation datasets, and the
results demonstrate superior performance and generalization ability in unseen
domains."
1013,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,"with the emergence of multimodal datasets (e.g., brats [25]
and hecktor [1]), various deep-learning-based multimodal image segmentation
methods have been proposed [3,10,13,27,29,31]."
1014,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,"extensive experimental results on two publicly available datasets
demonstrate the effectiveness of our proposed method."
1015,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,"hecktor21 is a dualmodality dataset
for head and neck tumor segmentation, containing 224 pet-ct image pairs."
1016,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,"we randomly select 180 samples
for each dataset as the training set and the rest as the independent test set
(44 cases for hecktor21 and 40 cases for pi-cai22)."
1017,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,"it is worth noting that
the performance of hybrid models such as unetr is not as good as expected, even
worse than 3d u-net, perhaps due to the small size of the dataset."
1018,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,dataset.
1019,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"we evaluated the applicability of our approach across multiple
modalities by conducting evaluations on microscopy and histology datasets."
1020,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"the
private dataset contains 300 images sized at 512 × 512 tessellated from 50 wsis
scanned at 20×, and meticulously labeled by five pathologists according to the
labeling guidelines of the monuseg [10]."
1021,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"for both datasets, we randomly split
80% of the samples on the patient level as the training set and the remaining
20% as the test set."
1022,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"more
importantly, our method can outperform swinunet and the previous methods on both
datasets."
1023,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"for example, in the histology image dataset, transnuseg improves the
dice score, f1 score, accuracy, and iou by 2.08%, 3.41%, 1.25%, and 2.70%
respectively, over the second-best models."
1024,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"similarly, in the fluorescence
microscopy image dataset, our proposed model improves dsc by 0.96%, while also
leading to 1.65%, 1.03% and 1.91% increment in f1 score, accuracy, and iou to
the second-best performance."
1025,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"to further show the effectiveness of these schemes, as well as
consistency self distillation, we conduct a comprehensive ablation study on both
datasets."
1026,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,"experimental results on two datasets demonstrate the excellence of our
transnuseg against state-of-the-art counterparts for potential real-world
clinical deployment."
1027,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"1.multi-modal segmentation is composed not only of multiple modalities, but also
of multiple tasks, such as the three types of tumours in brats2018 dataset that
represent the three tasks."
1028,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"our model and competing methods are evaluated on the brats2018 segmentation
challenge dataset [1,14]."
1029,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"the dataset consists of 3d multi-modal brain mris, including flair, t1, t1
contrast-enhanced (t1c), and t2, with ground-truth annotations."
1030,Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,"the dataset
comprises 285 cases for training, and 66 cases for evaluation."
1031,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"in this framework, the test
image is registered to a preselected reference dataset with known ground-truth
segmentation."
1032,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"the quality of a query segmentation is assessed by warping the
query image to the reference dataset."
1033,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"the
two parts are combined using a weight parameter λ to balance the different loss
components:3 experimentsfor this study, pre-operative multimodal mri scans of
varying grades of glioma were obtained from the 2021 brain tumor segmentation
(brats) challenge [1] training dataset (n = 1251)."
1034,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"pre-contrast t1-weighted (t1), t2-weighted (t2), post-contrast
t1-weighted (t1c), and fluid attenuated inversion recovery (flair) are included
in the dataset."
1035,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"the initial dataset was expanded by producing segmentation results at different
levels of quality to provide an unbiased estimation of segmentation quality."
1036,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"second, to further
enrich our dataset with segmentations of diverse quality, we sampled
segmentations along the training routines at different iterations."
1037,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"third, we devised a method called seggen that applied
image transformations, including random rotation (angle = [-15 • , 15 • ]),
random scaling (scale = [0.85, 1.25]), random translation (moves = [-20, 20]),
and random elastic deformation (displacement = [0, 20]), to the ground-truth
segmentations with a probability of 0.5, resulting in three segmentations for
each subject.the original brats 2021 training dataset was split into training (n
= 800), validation (n = 200), and testing (n = 251) sets."
1038,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"however, this generated dataset suffered from
imbalance (fig."
1039,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"training using such an imbalanced dataset is prone to
producing biased models that do not generalize well."
1040,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,"next, the generated samples closest to the transformed
uniform distribution in terms of euclidean distance were chosen to form the
resampled dataset."
1041,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,"b) respectively show the visualization of comparative experimental
results on the isic2017 and isic2018 datasets."
1042,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,datasets and implementation details.
1043,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,"to assess the efficacy of our model, we
select two public skin lesion segmentation datasets, namely isic2017 [1,3] and
isic2018 [2,6], containing 2150 and 2694 dermoscopy images, respectively."
1044,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,"consistent with prior research [19], we randomly partition the datasets into
training and testing sets at a 7:3 ratio."
1045,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,"to evaluate our method, we
employ mean intersection over union (miou), dice similarity score (dsc) as
metrics, and we conduct 5 times and report the mean and standard deviation of
the results for each dataset."
1046,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,"table 1 reveal that our ege-unet exhibits a comprehensive state-of-the-art
performance on the isic2017 dataset."
1047,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,"for the isic2018 dataset, the performance of our
model also outperforms that of the best-performing model."
1048,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"in this manner, our latent kinetic code can be interpreted to
provide tic information and hemodynamic characteristics for accurate cancer
segmentation.we verify the effectiveness of our proposed diffusion kinetic model
(dkm) on dce-mri-based breast cancer segmentation using breast-mri-nact-pilot
dataset [13]."
1049,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"dataset: to demonstrate the effectiveness of our proposed dkm, we evaluate our
method on 4d dce-mri breast cancer segmentation using the breast-mri-nact-pilot
dataset [13], which contains a total of 64 patients with the contrastenhanced
mri protocol: a pre-contrast scan, followed by 2 consecutive postcontrast time
points (as shown in fig."
1050,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"we divided the
original dataset into training (70%) and test set (30%) based on the scans."
1051,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,"ground truth segmentations of the data are provided in the dataset for tumor
annotation."
1052,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"however, such methods typically rely on large-scale annotated
image datasets, which usually require significant effort and expertise from
pathologists and can be prohibitively expensive [28].to reduce the annotation
cost, developing annotation-efficient methods for semantic-level gland
segmentation has attracted much attention [10,18,23,37]."
1053,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"(3) we validate the efficacy of our mssg on two public glandular
datasets (i.e., the glas dataset [27] and the crag dataset [13]), and the
experiment results demonstrate the effectiveness of our mssg in unsupervised
gland segmentation."
1054,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"we evaluate our mssg on the gland segmentation challenge (glas) dataset [27] and
the colorectal adenocarcinoma gland (crag) dataset [13]."
1055,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"the glas dataset
contains 165 h&e-stained histopathology patches extracted from 16 wsis."
1056,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"the crag
dataset owns 213 h&e-stained histopathology patches extracted from 38 wsis."
1057,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"the
crag dataset has more irregular malignant glands, which makes it more difficult
than glas, and we would like to emphasize that the results on crag are from the
model trained on glas without retraining."
1058,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"on the glas dataset, the end-to-end clustering methods (denoted by ""
* "") end up with limited improvement over a randomly initialized network."
1059,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"on crag dataset, even in
the absence of any hints, mssg still outperforms all unsupervised methods and
even some of the fully-supervised methods."
1060,Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,"on
both datasets, mssg obtains more accurate and complete results."
1061,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"our model has been evaluated on both the publicly brats 2020 dataset and
a private medulloblastoma segmentation dataset."
1062,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"in the inference stage, the output feature f is produced
by a normal 3 × 3 × 3 convolution as follows: in order to validate the
performance of eoformer, we conduct extensive
experiments on both the publicly available brats 2020 dataset and a private
medulloblastoma segmentation dataset (medseg)."
1063,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"the brats 2020 dataset [14]
consists of mri image data from 369 patients, with each patient having four
modalities (t1, t1ce, t2 and t2-flair) of skull-striped mri, which are aligned
to a standard brain template."
1064,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"the training/validation/test split follows
315/16/37 according to recent works [10,23].the medseg dataset includes mri
images of t1, t1ce, t2, and t2 flair modalities from 255 patients with
medulloblastoma."
1065,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"the dataset includes manual annotations of the wt and et
regions."
1066,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,four-fold cross-validation is performed on this dataset.
1067,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,"the results are reproduced
on our data split.table 1 displays the performance comparison of eoformer
against other methods on the brats 2020 dataset."
1068,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"first, there lacks of a well-segmented
dataset with manual labels on lyme disease."
1069,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"on one hand, some datasets-such as
ham10000 [10] and isbi challenges [11]-have manual annotated segmentations for
diseases like melanoma, but they do not have lyme disease lesions."
1070,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"on the other
hand, some datasets-such as groh et al."
1071,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"therefore, existing skin disease segmentation [13] as
well as existing general segmentation works, such as u-net [14], polar training
[15], vit-adapter [16], and mfsnet [17], usually suffer from relatively low
performance and reduced fairness [2,18,19].in this paper, we present the first
lyme disease dataset that contains labeled segmentation and skin tones."
1072,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"our lyme
disease dataset contains two parts: (i) a classification dataset, composed of
more than 3,000 diseased skin images that are either obtained from public
resources or clinicians with patient-informed consent, and (ii) a segmentation
dataset containing 185 samples that are manually annotated for three
regions-i.e., background, skin (light vs."
1073,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"our
dataset with manual labels is available at this url [20].secondly, we design a
simple yet novel data preprocessing and alternation method, called edgemixup, to
improve lyme disease segmentation and diagnosis fairness on samples with
different skin-tones."
1074,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"note that not all skin disease datasets are
carefully processed either due to the large amount of work required or the
scarcity of data samples collected, e.g., sd-198 [23] contains samples that are
taken under variant environments."
1075,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"specifically, we train two resnet-34 models
using the same dataset with and without edgemixup for a classification task of
skin disease."
1076,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"that is, even if the original
dataset is imbalanced, as long as one sample from a subpopulation exists, the
color range of the sample's lesion is considered in the initial detection."
1077,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"we present two datasets: (i) a dataset collected and annotated by us (called
skin), and (ii) a subset of sd-198 [23] with our annotation (called sd-sub)."
1078,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"first, we collect and annotate a dataset with 3,027 images containing three
types of disease/lesions, i.e., tinea corporis (tc), herpes zoster (hz), and
erythema migrans (em)."
1079,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"second, we select five classes from sd-198
[23], a benchmark dataset for skin disease classification, as another dataset
for both segmentation and classification tasks."
1080,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"we choose 30 samples in each class for segmentation task, and we
split them into 0.7, 0.1, and 0.2 ratio for training, validation, and testing,
respectively.table 1 show the characteristics of these two datasets for both
classification and segmentation tasks broken down by the disease type and skin
tone, as calculated by the individual typology angle (ita) [24]."
1081,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,take our skin-seg dataset for example.
1082,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"as for skin lesion segmentation tasks, few works has been
proposed due to the lack of datasets with ground-truth segmentation masks."
1083,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,"however, official datasets released, e.g., ham10000 [10] only
contains melanoma samples and all of the samples are with light skins according
to our inspection using ita scores.bias mitigation: researchers have addressed
bias and heterogeneity in deep learning models [18,29]."
1084,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,"ablation study (in ""mean (std)"") on mdc dataset using
regnetx40 [26] as the backbone."
1085,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,"the feature maps obtained from the ensemble can be decoded using lightweight 2d
decoders to generate segmentation masks.3 experimental results we conducted
experiments on the public multi-dimensional choledoch (mdc) dataset
[31] with 538 scenes and hyperspectral gastric carcinoma (hgc) dataset [33]
(data provided by the author) with 414 scenes, both with highquality labels for
binary mhsi segmentation tasks."
1086,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,"following [23,27], we partition the datasets into training, validation, and
test sets using a patient-centric hard split approach with a ratio of 3:1:1."
1087,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,"69.89) lower dsc, possibly because transformers are difficult
to optimize on small datasets."
1088,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,"table 3 shows
comparisons on mdc and hgc datasets."
1089,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,"we evaluate our approach on two
mhsi datasets."
1090,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"experimental results on lung (83 cts, 19 patients) and liver (77 cects, 18
patients) datasets show that our method yields high classification accuracy.to
the best of our knowledge, ours is the first method to perform longitudinal
lesion matching and lesion changes pattern detection."
1091,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"similarity-based methods pair two lesions
with similar features, e.g., intensity, shape, location [13][14][15][16] with an
84-96% accuracy on the deeplesion dataset [14]."
1092,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"we evaluated our method with two studies on retrospectively collected patient
datasets that were manually annotated by an expert radiologist.dataset: lung and
liver ct studies were retrospectively obtained from two medical centers
(hadassah univ hosp jerusalem israel) during the routine clinical examination of
patients with metastatic disease."
1093,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"dliver consists of
77 abdominal cect scans from 18 patients with a mean 4.3 ± 2.0 scans/patient, a
mean time interval between consecutive scans of 109.7 ± 93.5 days, and voxel
sizes of 0.6-1.0 × 0.6-1.0 × 0.8-5.0 mm 3 .lesions in both datasets were
annotated by an expert radiologist, yielding a total of 1,178 lung and 800 liver
lesions, with a mean of 14.2 ± 19.1 and 10.4 ± 7.9 lesions/scan (lesions with
<20 voxels were excluded)."
1094,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"ground-truth lesion matching graphs and lesion
changes labeling were produced by running the method on the datasets and then
having the radiologist review and correct the resulting node labels and
edges.study 1: lesion changes labeling, lesion matching, evaluation of patterns
of lesion changes."
1095,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"for the dlungs dataset, 25 visible
and 5 faintly visible or surmised to be present unmarked lesions were found for
27 nonconsecutive edges."
1096,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"for the dliver dataset, 20 visible and 21 faintly
visible or surmised to be present unmarked lesions were found for 25
non-consecutive edges.after reviewing the 42 and 37 lesions labeled as lone in
dlungs and dliver with > 5mm diameter, the radiologist determined that 1 and 8
of them had been wrongly identified as a cancerous lesion."
1097,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,"in total, 45 and 62 missing lesions were added to the
ground truth dlungs and dliver datasets, respectively."
1098,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,"the experimental results on a benchmark npsle dataset demonstrate the
proposed method outperforms comparing methods in terms of early noninvasive
biomarker discovery and early diagnosis."
1099,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,"dataset and preprocessing: the t2-weighted mr images of 39 participants
including 23 patients with npsle and 16 hcs were gathered from our affiliated
hospital."
1100,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,"the
classification performances of the npsle dataset contaminated by attribute noise
are shown in fig."
1101,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,"we validate our approach by
using a publicly available nsclc dataset from the cancer imaging archive (tcia)."
1102,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,"-we conduct experiments on a
publicly available dataset and achieve superior performance compared to the most
advanced methods currently available."
1103,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,"our dataset nsclc-tcia for lung cancer histological subtype classification is
sourced from two online resources of the cancer imaging archive (tcia) [5]:
nsclc radiomics [1] and nsclc radiogenomics [2]."
1104,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,"we evaluate the performance of nsclc classification in
five-fold cross validation on the nsclc-tcia dataset, and measure accuracy
(acc), sensitivity (sen), specificity (spe), and the area under the receiver
operating characteristic (roc) curve (auc) as evaluation metrics."
1105,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"although deep neural network-based object detectors
achieve tremendous success within the domain of natural images, directly
training generic object detectors on gld datasets performs below expectations
for two reasons: 1) the scale of labeled data in gld datasets is limited in
comparison to natural images due to the annotation costs."
1106,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"moreover, we propose the first large-scale gld datasets (lgldd),
which contains 10,083 gastroscopic images with 12,292 well-annotated lesion
bounding boxes of four categories of lesions (polyp, ulcer, cancer, and
sub-mucosal tumor)."
1107,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"-a large-scale gastroscopic lesion detection
datasets (lgldd) -experiments on lgldd demonstrate that ssl can bring
significant enhancement compared with baseline methods."
1108,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"we
set τ u = 0.5 and τ s = 0.5 we contribute the first large-scale gstroscopic
lesion detection datasets
(lgldd) in the literature."
1109,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"for map, we
follow the popular object detection datasets coco [11] and calculate the mean of
11 aps of iou from 0.5 to 0.95 with stepsize 0.05 (map @[.5:.05:.95]).we also
report ap under some specific iou threshold (ap 50 for .5, ap 75 for .75) and ap
of different scale lesions (ap s , ap m , ap l ) like coco [11]."
1110,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"experimental results in table 2.d show that ssl can bring
significant improvements to publicly available datasets."
1111,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"moreover, we contribute the first
large-scale gld datasets (lgldd)."
1112,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,"since annotation cost always limits
of datasets scale of such tasks, we hope ssl and lgldd could fully realize its
potential, as well as kindle further research in this direction."
1113,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"(3) the proposed method achieves
state-of-the-art results in five polyp segmentation datasets and outperforms the
previous cutting-edge approach by a large margin (3%) on cvc-colondb and etis
datasets."
1114,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,datasets.
1115,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"we conduct extensive experiments on five polyp segmentation datasets,
including kvasir [6], cvc-clinicdb [1], cvc-colondb [13], etis [12] and cvc-t
[14]."
1116,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"according to the experimental settings, the results on
cvc-clinicdb and kvasir demonstrate the learning ability of the proposed model,
while the results on cvc-t, cvc-colondb, and etis demonstrate the model's
ability for cross-dataset generalization."
1117,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"furthermore, our proposed
method demonstrates strong cross-dataset generalization capability on cvc-t,
cvc-colondb, and etis datasets, with particularly good performance on the latter
two due to their larger and more representative datasets."
1118,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"specifically, across the five datasets, our proposed model improves
the mdice score by at least 1.4% and up to 3.4% on cvc-t, compared to the
baseline."
1119,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"for miou, the improvements are 1.5% and 3.5% on the corresponding
datasets."
1120,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"experimental results on
five popular polyp datasets demonstrate the effectiveness and superiority of our
proposed method."
1121,Revisiting Feature Propagation and Aggregation in Polyp Segmentation,"specifically, it outperforms the previous cutting-edge approach
by a large margin (3%) on cvc-colondb and etis datasets."
1122,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","we train and validate
our methods on two retrospective rectal cancer datasets, grampian and aristotle."
1123,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","grampian and aristotle are used in both
training and validation, with a 70/30% training-validation split, keeping any
wsis from a single patient in the same dataset."
1124,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","the datasets are unbalanced, since in grampian only 61/244 slides have
complete response, and in aristotle only 24/121 slides have complete response."
1125,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","there are 365 slides total in our dataset, from 249
patients."
1126,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","prior to
fitting the graph model we normalize the node features relative to the whole
dataset."
1127,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","we use
weighted metrics due to the class imbalance in our dataset."
1128,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","the
prediction performance of the model could be improved by utilising a larger
training dataset and performing more exhaustive parameter searches, however the
current performance of the model is sufficient to demonstrate the impact of this
approach.the predicted response to radiotherapy can now be viewed in the context
of disease biology as captured by cms4."
1129,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies","the focus of this research is not to achieve the best possible
metrics, but to develop robust methods which can add context and explanation to
clinical black box deep learning model predictions, with the view to ease
clinical translation of such models.to explore the effects of the noisy cms4
ground truth labels, we remove from our dataset any wsis classified as
'unmatched' for the cms call, which for the main results of this paper we
defined as 'not cms4'."
1130,Automatic Bleeding Risk Rating System of Gastric Varices,"in
cram, the varices features are extracted using the segmentation results and
combined with an attention mechanism to learn the intra-class correlation and
cross-region correlation between the target area and the context.to learn from
experienced endoscopists, gv datasets with bleeding risks annotation is needed."
1131,Automatic Bleeding Risk Rating System of Gastric Varices,"while most works and public datasets focus on colonoscopy [13,15] and esophagus
[5,9], with a lack of study on gastroscopy images."
1132,Automatic Bleeding Risk Rating System of Gastric Varices,"in the public dataset of
endocv challenge [2], the majority are colonoscopies while only few are
gastroscopy images."
1133,Automatic Bleeding Risk Rating System of Gastric Varices,"in this work, we collect a gv bleeding risks rating dataset
(gvbleed) that contains 1678 gastroscopy images from 411 patients with different
levels of gv bleeding risks."
1134,Automatic Bleeding Risk Rating System of Gastric Varices,"three senior clinical endoscopists are invited to
grade the bleeding risk of the retrospective data in three levels and annotated
the corresponding segmentation masks of gv areas.in sum, the contributions of
this paper are: 1) a novel gv bleeding risk rating framework that constructively
introduces segmentation to enhance the robustness of representation learning; 2)
a region-constraint module for better feature localization and a cross-region
attention module to learn the correlation of target gv with its context; 3) a gv
bleeding risk rating dataset (gvbleed) with high-quality annotation from
multiple experienced endoscopists."
1135,Automatic Bleeding Risk Rating System of Gastric Varices,"baseline methods have been evaluated on the
newly collected gvbleed dataset."
1136,Automatic Bleeding Risk Rating System of Gastric Varices,"the gvbleed dataset contains 1678 endoscopic
images with gastric varices from 527 cases."
1137,Automatic Bleeding Risk Rating System of Gastric Varices,"to ensure the quality of our dataset, senior endoscopists are
invited to remove duplicates, blurs, active bleeding, chromoendoscopy, and nbi
pictures.criterion of gv bleeding risk level rating."
1138,Automatic Bleeding Risk Rating System of Gastric Varices,"based on the clinical
experience in practice, the gv bleeding risks in our dataset are rated into
three levels, i.e., mild, moderate, and severe."
1139,Automatic Bleeding Risk Rating System of Gastric Varices,"note that the diameter is only one reference for the final risk rating since the
gv is with 1 please refer to the supplementary material for more detailed
information about our dataset."
1140,Automatic Bleeding Risk Rating System of Gastric Varices,"to
ensure the accuracy of our annotation, three senior endoscopists with more than
10 years of clinical experience are invited to jointly label each sample in our
dataset."
1141,Automatic Bleeding Risk Rating System of Gastric Varices,"the gvbleed dataset is partitioned into training and testing sets for
evaluation, where the training set contains 1337 images and the testing set has
341 images."
1142,Automatic Bleeding Risk Rating System of Gastric Varices,"the dataset is planned to be released in the
future."
1143,Automatic Bleeding Risk Rating System of Gastric Varices,"in addition, we collected the gvbleed
dataset with high-quality annotation of three-level of gv bleeding risks."
1144,Automatic Bleeding Risk Rating System of Gastric Varices,"the
experiments on our dataset demonstrated the effectiveness and superiority of our
framework."
1145,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"survival
analysis is often used in pe to assess how survival is affected by different
variables, using a statistical method like kaplan-meier method and cox
proportional-hazards regression model [7,12,14].however, one issue with
traditional survival analysis is bias from single modal data that gets
compounded when curating multimodal datasets, as different combinations of modes
and datasets create with a unified structure."
1146,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"in addition,
it has been found that creating multimodal datasets without any debiasing
techniques does not improve performance significantly and does increase bias and
reduce fairness [5]."
1147,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"overall, a holistic approach to model development would be
beneficial in reducing bias aggregation in multimodal datasets."
1148,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"we then implemented methods
to remove racial bias in our dataset and model and output unbiased pe outcomes
as a result."
1149,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"we will first introduce our pulmonary embolism multimodal
datasets, including survival and race labels."
1150,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"then, we evaluate the baseline
survival learning framework without de-biasing in the various racial
groups.dataset."
1151,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"the pulmonary embolism dataset used in this study from 918
patients (163 deceased, median age 64 years, range 13-99 years, 52% female),
including 3978 ctpa images and 918 clinical reports, which were identified via
retrospective review across three institutions."
1152,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"the penet is
pre-trained on large-scale ctpa studies and shows excellent pe detection
performance with an auroc of 0.85 on our entire dataset."
1153,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,"we detected indications of racial bias in our dataset and
conducted an analysis of the multimodal diversity."
1154,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"our
method is based on a transformer model that uses attention [30], similar to how
radiologists would compare current and prior mammograms.the method is trained
and evaluated on a large and diverse dataset of over 9,000 patients and shown to
outperform a model based on state-of-the art risk prediction techniques for
mammography [33]."
1155,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"we compiled an in-house mammography dataset comprising 16,113 exams (64,452
images) from 9,113 patients across institutions from the united states, gathered
between 2010 and 2021."
1156,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"the
dataset has 3,625 biopsy-proven cancer exams, 5,394 biopsyproven benign exams,
and 7,094 normal exams."
1157,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"we partitioned the dataset by patient to create
training, validation, and test sets."
1158,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"the cindex
measures the performance of a model by evaluating how well it correctly predicts
the relative order of survival times for pairs of individuals in the dataset."
1159,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,"our extensive
experiments on a dataset of 16,113 exams show that prime+ outperformed a model
based on the state-of-the-art for breast cancer risk prediction [33]."
1160,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"in pare, a nodule is diagnosed from two levels: first parsing the contextual
information contained in the nodule itself, and then recalling the previously
learned nodules to look for related clues.one of the major challenges of lung
nodule malignancy prediction is the quality of datasets [6]."
1161,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"[16] collated a pathological gold standard dataset of 990
ct scans."
1162,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"to fulfill both ldct and ncct screening needs, we curate a large-scale
lung nodule dataset with pathology-or follow-up-confirmed benign/malignant
labels."
1163,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"for the ldct, we annotate more than 12,852 nodules from 8,271 patients
from the nlst dataset [14]."
1164,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"experimental results on
several datasets demonstrate that our method achieves outstanding performance on
both ldct and ncct screening scenarios.our contributions are summarized as
follows: (1) we propose context parsing to extract and aggregate rich contextual
information for each nodule."
1165,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"(3) we curate the largest-scale lung nodule dataset with
high-quality benign/malignant labels to fulfill both ldct and ncct screening
needs."
1166,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"data collection and curation: nlst is the first large-scale ldct dataset for
low-dose ct lung cancer screening purpose [14]."
1167,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"unlike nlst, this dataset
is noncontrast chest ct, which is used for routine clinical care."
1168,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"we additionally evaluate our method on the lungx [2] challenge
dataset, which is usually used for external validation in previous work
[6,11,24]."
1169,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"segmentation: we also evaluate the
segmentation performance of our method on the public nodule segmentation dataset
lidc-idri [3], which has 2,630 nodules with nodule segmentation mask."
1170,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"due to the lack of manual annotation of nodule masks for the nlst dataset, we
can only optimize the segmentation task using our in-house dataset, which has
manual nodule masks."
1171,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"our model is trained on a mix of ldct and ncct datasets, which can perform
robustly across low-dose and regular-dose applications."
1172,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"we find that the models
trained on either ldct or ncct dataset alone cannot generalize well to other
modalities, with at least a 6% auc drop."
1173,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"besides, we curate a large-scale pathological-confirmed
dataset with up to 13,000 nodules to fulfill the needs of both ldct and ncct
screening scenarios."
1174,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,"with the support of a high-quality dataset, our pare
achieves outstanding malignancy prediction performance in both scenarios and
demonstrates a strong generalization ability on the external validation."
1175,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"we utilize
optimam [7], a large dataset with a significant proportion of negatives (table
1), for training and evaluation."
1176,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"ultimately, each
component contributes to our goal of reducing false positives.we validate m&m
through evaluation on five datasets: two in-house datasets, two public datasets
-ddsm [8] and cbis-ddsm [9], and optimam [7]."
1177,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"furthermore, m&m can
provide breast-level classification predictions, achieving aucs of more than
0.88 on four different datasets (table 3)."
1178,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"we employ a 1:1 sampling
ratio between unannotated and annotated images.datasets."
1179,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"we utilize three 2d
digital mammography datasets: (1) optimam : a development dataset derived from
the optimam database [7], which is funded by cancer research uk."
1180,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"we split the
data into train/val/test with an 80:10:10 ratio at the patient level; (2)
inhouse-a: an evaluation dataset collected from a u.s."
1181,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"multi-site mammography
operator; (3) inhouse-b : an evaluation dataset collected from a u.s."
1182,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,2.2 for more details on the inhouse datasets).
1183,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"we also
utilize two film mammography datasets: (4) ddsm: a dataset maintained at the
university of south florida [8]."
1184,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"dataset statistics are reported
in table 1.metrics."
1185,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"table 3a reports
m&m's breast-level and exam-level classification results on optimam and the two
inhouse datasets."
1186,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"both baseline models suffer large
generalization drops of approximately 3b compares m&m with recent literature
reporting on the public cbis-ddsm dataset."
1187,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,"this is more advantageous than previous
pipelines that require additional stages or classifiers to reduce false
positives [15,22,29].as a classifier, m&m establishes strong performance on
several datasets (table 3)."
1188,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"our method can use both expert
annotatedor unsupervised generated masks to reverse and segment anomalies
annotated datasets for training and tend to generalize poorly beyond the learned
labels [21]."
1189,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"adapting the noise distribution to the diversity
and heterogeneity of pathology is inherently difficult, and even if achieved,
the noising process disrupts the structure of both healthy and anomalous regions
throughout the entire image.in related computer vision areas, such as industrial
inspection [3], the topperforming methods do not focus on reversing anomalies,
but rather on detecting them by using large nominal banks [7,20], or pre-trained
features from large natural imaging datasets like imagenet [4,22]."
1190,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"they include the variability
and complexity of normal data, subtlety of anomalies, limited size of datasets,
and domain shifts.this work aims to combine the advantages of constrained latent
restoration for understanding healthy data distribution with generative
in-painting networks."
1191,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"we compute
the final anomaly maps based on residual and perceptual differences: datasets."
1192,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"we trained our model using two publicly available brain t1w mri
datasets, including fastmri+ (131 train, 15 val, 30 test) and ixi (581 train
samples), to capture the healthy distribution."
1193,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"performance evaluation was done
on a large stroke t1-weighted mri dataset, atlas v2.0 [14], containing 655
images with manually segmented lesion masks for training and 355 test images
with hidden lesion masks."
1194,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"the lack of healthy data from the same
scanner and the limited size of the healthy datasets limit the successful
application of such methods, with a maximum achievable dice score of just under
6%."
1195,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,"generally, unsupervised methods tend to have
lower dice scores partly due to unlabeled artefacts in the dataset."
1196,SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,"dataset: this work uses the breast cancer histopathological image database
(breakhis)1 [20]."
1197,SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,"the images in the dataset have four magnification factors the
model is trained using the adam optimizer [25] with the learning rate set to
1x10 -3 ."
1198,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"to enable language comprehension, cite makes use of large language
models pretrained on biomedical text datasets [10,11] with rich and professional
biomedical knowledge."
1199,Text-Guided Foundation Model Adaptation for Pathological Image Classification,dataset.
1200,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"we adopt the patchgastric [25] dataset, which includes
histopathological image patches extracted from h&e stained whole slide images
(wsi) of stomach adenocarcinoma endoscopic biopsy specimens."
1201,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"the
dataset contains 9 subtypes of gastric adenocarcinoma."
1202,Text-Guided Foundation Model Adaptation for Pathological Image Classification,"figure 3
shows the classification accuracy on the patchgastric dataset of our approach
compared with baseline methods and related works, including (1) r50-21k:
fine-tune the whole resnet50 [27] backbone pre-trained on imagenet-21k [26].(2)
linear probe: train a classification head while freezing the backbone
encoder.(3) fine-tune: train a classification head together with the backbone
encoder."
1203,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"our results show that on a real-world dataset, drl can
significantly improve the stroke classification performance of erm and other
baseline defensive training methods, when the signal sparsity and noise in
accelerated mri are generated by the cartesian undersampling (cu) method [20]
and white gaussian noise (wgn)."
1204,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"our dataset included mri brain scans from 226 patients performed at an urban
tertiary referral academic medical center that is a comprehensive stroke center."
1205,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"while the whole dataset includes 4,883 (74.7%) normal slices and 1,650 (25.3%)
stroke slices, we further randomly split them into training/validation/test sets
using the ratio 80%/10%/10%."
1206,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"for the cnn model, we used a
resnet-18 [9] architecture, while for the vit model, we first pre-trained a
4-layer vit using a self-supervised pre-training method called masked
autoencoder (mae) [8], using the t1/t2-weighted brain mr images in the ixi
dataset [1]."
1207,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,"as our
dataset is unbalanced, we also considered the area under precision-recall curve
(auprc)."
1208,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"with
the decoupled snippet-level feature ingredients, our cfd employs both the normal
and abnormal feature ingredients via a contrastive learning paradigm to
concurrently optimize video-level and snippetlevel disease scores for pursuing
more accurate detection.to assess the proposed contrastive feature decoupling
network, we conduct experiments on two datasets, i.e., polyp and panda-mil."
1209,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"given dataset d comprising normal sub-dataset d 0 and abnormal sub-dataset d 1 ,
we first encode all instances per bag b ∈ d into instance-level feature set f =
{f t } t t=1 ∈ r t ×c via a pre-trained feature extractor e."
1210,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"we then collect all normal instance-level features f ∈ r 1×c from d 0
to learn the memory bank m by using the dictionary learning technique [7] where
d 0 is the normal sub-dataset collected from the training split, w t is the
learned weights within the memory bank learning process, and λ is a
hyperparameter to constrain the memory bank sparsity."
1211,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"we evaluate our model against sotas on the existing polyp [14] dataset and the
panda-mil dataset introduced in this work."
1212,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"please refer to the
supplementary material for the statistics of the two datasets.polyp."
1213,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"this
dataset collects colonoscopy videos from hyper-kvasir [1] and ldpolypvideo [10]."
1214,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"each bag/video is encoded into t = 32 snippets among both datasets
via linear interpolation."
1215,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"the results in table 1
demonstrate that our cfd consistently outperforms all the other methods on two
datasets."
1216,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"precisely, our model achieves the new sota by 1.1% auc and 1.5% ap
improvements on the polyp dataset and 1.09% auc and 2.45% ap improvements on the
panda-mil dataset."
1217,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"figure 2 visualizes one
disease detection result of our cfd model on the panda-mil dataset."
1218,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"the ablation study in
table 2 is conducted on the panda-mil dataset to evaluate the effectiveness of
the memory bank and loss functions in our model."
1219,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"besides, we introduce a new
dataset of prostate cancer detection, i.e., panda-mil, to provide a biomedical
imaging dataset concerning a different pathological modality."
1220,Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,"experiments
demonstrate that our cfd network achieves new sota performance on the polyp and
panda-mil datasets, indicating that our method effectively addresses the disease
detection task across different pathological modalities."
1221,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"in addition, we also examine the language syntax based prompt
fusion approach as a comparison, and explore several fusion strategies by first
grouping the prompts either with described attributes or categories and then
repeating the fusion process.we evaluate the proposed approach on a broad range
of public medical datasets across different modalities including photography
images for skin lesion detection isic 2016 [2], endoscopy images for polyp
detection cvc-300 [21], and cytology images for blood cell detection bccd."
1222,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"we collect three public medical image datasets across various modalities
including skin lesion detection dataset isic 2016 [2], polyp detection dataset
cvc-300 [21], and blood cell detection dataset bccd to validate our proposed
approach for zero-shot medical lesion detection."
1223,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"more details on
the dataset and implementation are described in the appendix."
1224,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"as illustrated in table 1, our ensemble guided fusion rivals the
glip [11] with single prompt and other fusion baselines across all datasets."
1225,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"table 2 shows that our method outperforms yolov5, which indicates
fullysupervised models such as yolo may not be suitable for medical scenarios
where a large labeled dataset is often not available."
1226,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"the misclassification problem in some of the single prompts is
corrected (i.e., malignant to benign) on the first dataset."
1227,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"for all datasets,
the candidate boxes are more precise and associated with higher confidence
scores.fine-tuned models can further improve the detection performance."
1228,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"figure 3 shows the
visualization of the zero-shot results across three datasets."
1229,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"fusing prompts by category is
specifically for multi-category datasets to first gather the prompts belonging
to the same category and make further fusion."
1230,Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,"as
shown in table 5, we perform ablation studies on three datasets."
1231,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"we validate our
framework on two brain medical datasets, demonstrating the effectiveness of the
framework components and showing more accurate detection results of anomaly
regions."
1232,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"brats 2020 [2] is a brain tumor segmentation dataset containing the mr sequences
of t1, t1gd, t2, and flair."
1233,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"isles 2022 [19] is an mr image dataset for stroke
lesions segmentation."
1234,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"we train them on
brats2020 and isles datasets."
1235,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"our fndm
outperforms the existing methods in all metrics on both datasets."
1236,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"fndm also
outperforms the previous state-of-theart diffusion method, diffano, by a large
gap of +9.56% dice, -0.98% hdis, and +0.54% vsim on brats dataset, and +19.98%
dice, -1.39% hdis, and +26.73% vsim on isles dataset, revealing that our fndm is
effective to reconstruct the healthy image from diseased to detect the anomaly
regions in brain mr images."
1237,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"thanks to non-markovian procedure and pixel-wise
hybrid guidance, the performance improvement of our method is larger on the
isles dataset where stroke lesions are more challenging due to smaller sizes and
irregular shapes."
1238,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"from table 2, we decompose the overall pixel-wise hybrid
condition into classifier gradient (cg), non-markovian (nm), and memory bank
(mb), comparing all possible combinations on brats2020 dataset."
1239,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"4, we further evaluate the
segmentation performance of diffano [27] and ours across diverse steps on
brats2020 dataset."
1240,Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,"extensive experiments on two brain datasets reveal the effectiveness and
superiority of our approach for anomaly detection."
1241,MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,"then, we utilize scaled dot-product to compute the attention weights
of v cls as:after temporal feature aggregation, f temp is fed into a multilayer
perceptron head to predict the class of tumor.3 experimental results we collect
a renal tumor us dataset of 179 cases from two medical centers, which
is split into the training and validation sets."
1242,MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,"in this paper, we create the first multi-modal ceus video dataset and propose a
novel attention-based multi-modal video fusion framework for renal tumor
diagnosis using b-mode and ceus-mode us videos."
1243,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"we conducted rigorous experiments on two
datasets and demonstrated the effectiveness of our method."
1244,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"this innovative approach not only improves the classification
performance at the patient level but also at the slide level, showcasing its
effectiveness and versatility; 3) conducting extensive experiments on two
separate datasets."
1245,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,cd-itb dataset.
1246,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"cd-itb is a private dataset consisting of 853 slides from 163
patients, with binary patient-level labels of cd or itb in a ratio of 103:60 and
tri-class slide-level labels of cd, itb, and normal slides in a ratio of
436:121:296, respectively."
1247,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"the dataset comprises an average
of 2.3k instances per bag, with the largest bag containing over 16k
instances.camelyon17 dataset."
1248,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"camelyon17 [1] is a publicly dataset, and its
training set comprises 500 slides from 100 breast cancer patients with lymph
node metastases."
1249,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"the data folding method is the
same as the cd-itb dataset."
1250,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"abmil with p&sre improves the f1 score from 0.565 to 0.579 for the
cd-itb dataset and from 0.529 to 0.571 for the camelyon17 dataset at the
slide-level, and improves the f1 score from 0.522 to 0.599 for the cd-itb
dataset and from 0.842 to 0.861 for the camelyon17 dataset at the patient-level."
1251,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,"by
introducing a transformer, the framework enables iterative interaction and
correction of information between patients and slides, resulting in better
performance at both the patient level and slide level compared to existing
state-of-the-art algorithms on two validation datasets."
1252,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"although single-source dg (sdg) using only one source dataset has been applied
to medical images [12,14,32], very few studies focus on sdg with pet imaging and
the current sdg methods may not be suitable for lesion identification on pet
data."
1253,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"trained with a
single-source synthesized dataset, the proposed method provides superior
performance of hepatic lesion detection in multiple cross-scanner real clinical
pet image datasets, compared with the reference baseline and recent
state-of-the-art sdg methods."
1254,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"given a source-domain dataset of
list mode-synthesized 3d pet images and corresponding lesion labels (x s , y s
), the goal of the framework is to learn a lesion detection model h , composed
of e and d, which generalizes to real clinical pet image data."
1255,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"in the synthesized pet image dataset, each subject have multiple simulated
lesions of varying size with known boundaries [11], and thus no human annotation
is required."
1256,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"however, this synthesized dataset presents a significant domain
shift from real clinical data, as they have markedly different image textures
and voxel intensity values (see fig."
1257,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"inspired by previous domain
generalization work [39], we introduce a specific data augmentation module to
generate out-of-domain samples from this single-source synthesized dataset for
generalizable model learning (see fig."
1258,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,datasets.
1259,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"we evaluate the proposed method with multiple 68 ga-dotatate pet liver
net image datasets that are acquired using different pet/ct scanners and/or
imaging protocols."
1260,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"the synthesized source-domain dataset contains 103 simulated
subjects, with an average of 5 lesions and 153 transverse slices per subject."
1261,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"this dataset is synthesized using list mode data from a single real, healthy
subject acquired on a ge discovery mi pet/ct scanner with list mode
reconstruction [11,37]."
1262,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"we collect two additional real 68 ga-dotatate pet liver
net image datasets that serve as unseen domains."
1263,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"the first dataset (real1) has
123 real subjects with about 230 hepatic lesions in total and is acquired using
clinical reconstructions with a photomultiplier tube-based pet/ct scanner (ge
discovery ste)."
1264,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"the second real-world dataset (real2) consists of 65 cases with
around 113 lesions and is acquired from clinical reconstructions using a digital
pet/ct scanner (ge discovery mi)."
1265,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"following [28,38], we randomly split the
synthesized dataset and the real1 dataset into 60%, 20% and 20% for training,
validation and testing, respectively."
1266,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"due to the relatively small size of real2,
we use a two-fold cross-validation for model evaluation on this dataset."
1267,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"here we
split the real datasets to learn fully supervised models for a comparison with
the proposed method."
1268,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,table 1 presents the comparison results on the two unseen-domain datasets.
1269,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"our
method significantly outperforms the state-of-the-art approaches in terms of f 1
score, with p-value < 0.05 in student's t-test for almost all cases on both
datasets."
1270,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"this indicates that compared with the competitor approaches, our method
is relatively more effective and stable in learning generalizable
representations for lesion detection in a very challenging situation, i.e.,
learning with a single-source synthesized pet image dataset to generalize to
real clinical data.the qualitative results are provided in the supplementary
material.ablation study."
1271,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"the u pper-bound means training with real-world images
and gold-standard labels from the testing datasets."
1272,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"we note that using the data
augmentation module a can significantly improve the lesion detection performance
compared with the baseline on the real1 dataset, and combining data augmentation
and patch gradient reversal can further close the gap to the u pper-bound model."
1273,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"our method also outperforms the baseline model by a large margin on the real2
dataset, suggesting the effectiveness of our method."
1274,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"in addition, we observe a similar trend of the f 1 curve for the λ cls ,
especially for the real1 dataset, and this indicates the necessity of the domain
classification pretext task."
1275,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"we propose a novel sdg framework that uses only a single dataset for hepatic
lesion detection in real clinical pet images, without any human data
annotations."
1276,Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,"the experiments show that our method outperforms
the reference baseline and recent state-of-the-art sdg approaches on
cross-scanner or -protocol real pet image datasets."
1277,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"given
a dataset x = {x 1 , .., x n } we optimize the encoder and decoder with
parameters θ, φ to minimize the mse loss between the input and its
reconstruction."
1278,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"in contrast, the detection of pathology on chest
radiographs is much more difficult due to the high variability and complexity of
nominal features and the diversity and irregularity of abnormalities.datasets."
1279,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"we used the rsna
dataset [35], which contains 10k cxr images of normal subjects and 6k lung
opacity cases."
1280,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,"for the detection of covid-19, we used the padchest dataset [3]
containing cxr images manually annotated by trained radiologists."
1281,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,dataset.
1282,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"1 experimental results on two public tumor
segmentation datasets show that dcaug improves the tumor segmentation accuracy
compared with state-of-theart tumor augmentation methods."
1283,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"-experimental results on two public tumor segmentation
datasets demonstrate that dcaug improves the diversity and quality of synthetic
tumor images."
1284,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"atlas dataset [11]: the atlas dataset consists of 229 t1-weighted mr images from
220 subjects with chronic stroke lesions."
1285,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"kits19 dataset [4]: the kits19 consists of 210 3d abdominal ct
images with kidney tumor subtypes and segmentation of kidney and kidney tumors."
1286,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"the maximum number of
training epochs was set to 500 for the two datasets."
1287,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"4 show that compared with other
stateof-the-art methods, including mixup [16], cutmix [15], carvemix [17],
selfmix [19], stylemix [5], nnunet combined with dcaug achieves the highest
improvement on the two datasets, which convincingly demonstrates the innovations
and contribution of dcaug in generating higher quality tumor."
1288,DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,"dataset num means and standard deviations of the dice coefficients (%) tda mixup
cutmix carvemix selfmix stylemix dcaug atlas 25% 49.87 ± 32.19 49.18 ± 32.72
41.19 ± 33.98 55.16 ± 32.16 57.89 ± 31.05 52.84 ± 34.36 56.43 ± 32.33 50% 56.72
± 30.74 58.40 ± 29.35 54.25 ± 30.24 58.34 ± 31.32 58.81 ± 31.75 58.04 ± 30.39
59.75 ± 31.41 100% 59.39 ± 32.45 59.33 ± 33.06 56.11 ± 32.44 62.32 ± 31.10 63.5
± 31.06 64.00 ± 28.89 64.64 ± 29.91 cross-domain consistency learning (cdcl)
strategy can preserve the
domaininvariant content information of tumor in the synthesized images x b→a a ,
x a→b b for avoiding content distortion."
1289,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,"the entire dataset is denoted by, where x i is the
i-th non-contrast ct volume, with y i being the voxel-wise label map of the same
size as x i and k channels."
1290,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,dataset and ground truth.
1291,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,"our study analyzed a dataset of ct scans collected
from guangdong province people's hospital between years 2018 and 2020, with
2,139 patients consisting of 787 gastric cancer and 1,352 normal cases."
1292,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,"readers were
informed that the dataset might contain more tumor cases than the standard
prevalence observed in screening, but the proportion of case types was not
disclosed."
1293,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,"we obtain the 95% confidence interval of auc,
sensitivity, and specificity values from 1000 bootstrap replicas of the test
dataset for statistical analysis."
1294,Self-supervised Polyp Re-identification in Colonoscopy,"moreover,
self-supervised techniques using extensive unannotated datasets has exhibited
substantial advantages within the medical domain [12].hence, we turn to simclr
[5], a contrastive self-supervised learning technique, which requires no manual
labeling."
1295,Self-supervised Polyp Re-identification in Colonoscopy,dataset.
1296,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"1(a), we show the target
motion speed [26] 1 on imagenetvid [14] (natural) and ldpolypvideo [9]
(colonoscopy) dataset."
1297,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"figure 1(b) shows the performance of fgfa [26] on two datasets with increasing
reference frames."
1298,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"(3) extensive experiments demonstrate that our yona
achieves new state-of-the-art performance on three large-scale public video
polyp detection datasets."
1299,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"for the fairness of the experiments, we
keep the same dataset settings for yona and all other methods."
1300,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"besides,
yona achieves the best trade-off between accuracy and speed compared with all
other image-based sotas across all datasets."
1301,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"second, for video-based
competitors, previous video object detectors with multiple frame collaborations
lack the ability for accurate detection on challenging datasets."
1302,YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,"specifically,
yona surpasses the second-best stft [19] by 2.2%, 3.0%, and 1.3% on f1 score on
three datasets and 33.8 on fps."
1303,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"consequently, while there are many breast ultrasound image datasets [1,28],
breast ultrasound video datasets remain scarce, with only one relatively small
dataset [15] containing 188 videos available currently.given the difficulties in
collecting ultrasound video data, we investigate the feasibility of enhancing
the performance of ultrasound video classification using a static image dataset."
1304,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"the images in the ultrasound dataset are keyframes of a lesion that
exhibit the clearest appearance and most typical symptoms, making them more
discriminative for diagnosis."
1305,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"our approach leverages both
image (keyframes) and video datasets to train the network."
1306,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"due
to the feature centers being generated by the larger scale image dataset, it
provides more accurate and discriminative feature centers which can guide the
video frame attention to focus on important frames, and finally leads to better
video classification.our experimental results on the public busv dataset [15]
show that our kga-net significantly outperforms other video classification
models by using an external ultrasound image dataset."
1307,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"we propose kga-net, which adopts a static image dataset to
boost the performance of ultrasound video classification."
1308,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"kga-net significantly
outperforms other video baselines on the busv dataset."
1309,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"however, all of them are based on
image datasets, such as busi [1], while few works focus on the video modality."
1310,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"fortunately, the recent publicly available dataset busv [15] has
made the research on the task of bus video-based classification possible."
1311,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"in
this paper, we build our model based on this dataset.video recognition based on
neural networks."
1312,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,datasets.
1313,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"we use the public busv dataset [15] for video classification and the
busi dataset [1] as the image dataset."
1314,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"for the busv dataset, we use the official data split in [15]."
1315,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,all images of the busi dataset are adopted to train our kga-net.
1316,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"evaluation
metrics are reported on the busv test set for performance assessment.as shown in
table 1, by leveraging the guidance of the image dataset, our kga-net
significantly surpasses all other models on all of the metrics."
1317,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"the feature centers
formed by the image dataset with larger data size and clear appearance
effectively improve the accuracy of frame attention hence boosting the video
classification performance."
1318,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"to portray the effect of using the
image dataset, we train the kga-net using busv dataset alone in the first row of
table 2."
1319,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"without the image dataset, we generate the feature centers from the
video frames."
1320,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"as a result, the performance significantly drops due to the
decrease in dataset scale."
1321,Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,"it also shows that the feature centers generated by
the image dataset are more discriminative than that of the video dataset."
1322,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"to train our proposed model, we utilize a dataset consisting of
image and text pairs."
1323,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"2, we multiply the learnable parameter (l ∈ r 1×(hw )
) by the global text representation (v t ) to match the dimension of the text
feature with that of the image feature map as the text feature map f t is used
as key and value, and the image feature map f i is used as a query to perform
self-attention aswhere h q , h k , and h v are convolution layers with a kernel
size of 1, and q, k, and v are queries, keys, and values for
self-attention.finally, by upsampling the low-dimensional cp am t g obtained
through crossattention of text and image together with skip-connection, more
accurate segmentation prediction can express the detailed information of an
object.3 experiments medical datasets."
1324,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"we evaluated cp am t g using three datasets: monuseg [8]
dataset, qata-cov19 [6] dataset, and sacroiliac joint (sij) dataset."
1325,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"the first
two datasets are the same benchmark datasets used in [10]."
1326,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"sij is the dataset privately prepared for this study
which consists of 804 mri slices of nineteen healthy subjects and sixty patients
diagnosed with axial spondyloarthritis."
1327,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"we randomly rotated images by -20 • ∼ +20
• and conducted a horizontal flip with 0.5 probability for only the monuseg and
qata-cov19 datasets."
1328,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"furthermore, cp am
t g achieves a better performance by 1 to 3% than lvit [10] on all datasets."
1329,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"figure 3 also shows that even on the qata-cov19 and
monuseg datasets, cp am t g predicted the most accurate segmentation masks (see
the red box areas)."
1330,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,"specifically, for the sij dataset, we
examined the effect of attention in extracting feature maps through comparison
with backbone networks (u-net) and pam."
1331,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"hence, it is important
to provide indications of the expected performance on a target dataset without
requiring annotations [5,25]."
1332,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"another related topic is out-of-distribution (ood)
detection [33] which aims to detect individual samples that are ood, in contrast
to our objective of estimating a difference of expected performances between
some datasets."
1333,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"alternatively, a drop in performance can be estimated by comparing the model's
softmax outputs [8] or some hidden features [24,28] acquired on in-domain and
domain shift datasets."
1334,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"inspired by fid, we propose a
metric named fréchet domain distance (fdd) for evaluating if a model is
experiencing a drop in performance on some new dataset."
1335,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"the fréchet distance
(fd) between two multivariate gaussian variables with means µ 1 , µ 2 and
covariance matrices c 1 , c 2 is defined as [3]:we are interested in using the
fd for measuring the domain shift between different wsi datasets x d ."
1336,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"68 wsis with lobular carcinoma
(28 wsis with metastases): potentially large shift as it is a rare type of
carcinoma and relatively difficult to diagnose.the datasets of lobular and
ductal carcinomas each contain 50 % of wsis from sentinel and axillary lymph
node procedures."
1337,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"all datasets are
publicly available to be used in legal and ethical medical diagnostics research."
1338,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"whereas extremely large
variations in label prevalence could reduce the reliability of the mcc metric,
this is not the case here as label prevalence is similar (35-45%) in our test
datasets."
1339,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"as there is no related work on domain shift detection in the mil setting, we
selected methods developed for supervised algorithms as baselines:-the model's
accumulated uncertainty between two datasets."
1340,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"-the accumulated confidence of a model across two
datasets."
1341,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,doc [8] can be measured on the mean softmax scores of two datasets.
1342,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"2.2) with both methods.for all possible pairs of camelyon and the other
test datasets, and for the 10 cv models, we compute the domain shift measures
and compare them to the observed drop in performance."
1343,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"while showing similar trends, there
is some discrepancy in the level of domain shift represented by the datasets due
to the differences between the mcc and roc-auc measures.as we deemed mcc to
better represent the clinical use situation (see sect."
1344,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"we observe the largest domain shift in terms of mcc on
axillary nodes followed by lobular carcinoma and full brln datasets."
1345,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"clam models
achieved better performance on ductal carcinoma compared to the in-domain
camelyon test data.table 2 summarises the pearson correlation between the change
in performance, i.e., the mcc difference between camelyon and other test
datasets, and the domain shift measures for the same pairs."
1346,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"figure 1
shows how individual drop in performance of model-dataset combinations are
related to the f dd 64 metric."
1347,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"for most models detecting larger drop in
performance (> 0.05) is easier on axillary lymph nodes data than on any other
analysed dataset."
1348,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,"however, the drop is easier to detect on axillary and lobular datasets
compared to others."
1349,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,"for
example, public datasets such as the liver tumor segmentation benchmark (lits)
[1] fostered a series of works aiming to segment liver tumors with improved
convolutional neural network (cnn) backbones [9,13] and lesion edge information
[15]."
1350,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,"we collect a large-scale dataset with both tumor and
non-tumor subjects, where the non-tumor subjects includes not only healthy ones,
but also patients with various diffuse liver diseases such as steatosis and
hepatitis to improve the robustness of the algorithm."
1351,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,"it contains three branches with bottomup cooperation: the segmentation map from
the pixel branch helps to initialize the lesion branch, which is an improved
mask transformer aiming to segment and classify each lesion; the patient branch
aggregates information from the whole image and predicts image-level labels of
each lesion type, with regularization terms to encourage consistency with the
lesion branch.we collected a large-scale multi-phase dataset containing 810
non-tumor subjects and 939 tumor patients."
1352,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,"our dataset contains 810 normal subjects and 939 patients with liver
tumors."
1353,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,"we first train an nnu-net on public datasets to segment
liver and surrounding organs (gallbladder, hepatic vein, spleen, stomach, and
pancreas), and then crop the liver region to train plan."
1354,Self-supervised Learning for Endoscopic Video Analysis,"we first experiment solely on public datasets,
cholec80 [32] and polypsset [33], demonstrating performance on-par with the top
results reported in the literature."
1355,Self-supervised Learning for Endoscopic Video Analysis,"therefore, to exploit msns to their full extent, we collect and build
two sizable unlabeled datasets for laparoscopy and colonoscopy with 7, 700
videos (>23m frames) and 14, 000 videos (>2m frames) respectively."
1356,Self-supervised Learning for Endoscopic Video Analysis,"furthermore, the proposed approach
exhibits robust generalization, yielding better performance with only 50% of the
annotated data, compared with standard supervised learning using the complete
labeled dataset."
1357,Self-supervised Learning for Endoscopic Video Analysis,"ai solutions have shown remarkable
performance in recognizing surgical phases of cholecystectomy procedures
[17,18,32]; however, they typically require large labelled training datasets."
1358,Self-supervised Learning for Endoscopic Video Analysis,"a recent work [27] presented an extensive
analysis of modern ssl techniques for surgical computer vision, yet on
relatively small laparoscopic datasets.optical polyp characterization."
1359,Self-supervised Learning for Endoscopic Video Analysis,"however,
training such automatic optical biopsy systems relies on a large body of
annotated data, while ssl has not been investigated in this context, to the best
of our knowledge.3 self-supervised learning for endoscopy ssl approaches have
produced impressive results recently [5][6][7][8], relying on two key factors:
(i) effective algorithms for unsupervised learning and (ii) training on
large-scale datasets."
1360,Self-supervised Learning for Endoscopic Video Analysis,"4, we show that training
msns on these substantial datasets unlocks their potential, yielding effective
representations that transfer well to public laparoscopy and colonoscopy
datasets."
1361,Self-supervised Learning for Endoscopic Video Analysis,"this is of great
interest for us since our downstream datasets are typically of small size
[32,33]."
1362,Self-supervised Learning for Endoscopic Video Analysis,"applying msns on the large datasets described
below, generates representations that serve as a strong basis for various
downstream tasks, as shown in the next section."
1363,Self-supervised Learning for Endoscopic Video Analysis,"we compiled a dataset of laparoscopic procedures videos exclusively
performed on patients aged 18 years or older."
1364,Self-supervised Learning for Endoscopic Video Analysis,"the dataset consists of 7,877
videos recorded at eight different medical centers in israel."
1365,Self-supervised Learning for Endoscopic Video Analysis,"the dataset
predominantly consists of the following procedures: cholecystectomy (35%),
appendectomy (20%), herniorrhaphy (12%), colectomy (6%), and bariatric surgery
(5%)."
1366,Self-supervised Learning for Endoscopic Video Analysis,"the remaining 21% of the dataset encompasses various standard laparoscopic
operations."
1367,Self-supervised Learning for Endoscopic Video Analysis,"each video recording was sampled at a rate of 1 frame
per second (fps), resulting in an extensive dataset containing 23.3 million
images."
1368,Self-supervised Learning for Endoscopic Video Analysis,"we
have curated a dataset comprising 13,979 colonoscopy videos of patients aged 18
years or older."
1369,Self-supervised Learning for Endoscopic Video Analysis,"our experimental protocol is the following: (i) first, we perform ssl
pretraining with msns over our unlabeled private dataset to learn informative
and generic representations, (ii) second we probe these representations by
utilizing them for different public downstream tasks."
1370,Self-supervised Learning for Endoscopic Video Analysis,"(b) polypsset [33]: a unified dataset of
155 colonoscopy videos (37,899 frames) with labeled polyp classes (hyperplastic
or adenoma) and bounding boxes."
1371,Self-supervised Learning for Endoscopic Video Analysis,"table 1 compares the results of pretraining with different datasets
(public and private) and model sizes."
1372,Self-supervised Learning for Endoscopic Video Analysis,"we present results for the cholecystectomy
phase recognition task based on fine-tuned models and for the optical polyp
characterization task based on linear evaluation, due to the small size of the
public dataset."
1373,Self-supervised Learning for Endoscopic Video Analysis,"as baselines, we report fully-supervised resnet50 results,
trained on public datasets."
1374,Self-supervised Learning for Endoscopic Video Analysis,"ssl pretraining
on public datasets (without labels) provides comparable or better results than
fully supervised baselines."
1375,Self-supervised Learning for Endoscopic Video Analysis,"importantly, we see that the performance gap becomes prominent when
using the large scale private datasets for ssl pretraining."
1376,Self-supervised Learning for Endoscopic Video Analysis,"when using the private colonoscopy dataset the macro f1 improves by 11.5%
compared to the fully supervised baseline."
1377,Self-supervised Learning for Endoscopic Video Analysis,"this study showcases the use of masked siamese networks to learn informative
representations from large, unlabeled endoscopic datasets."
1378,Self-supervised Learning for Endoscopic Video Analysis,"moreover, this methodology displays strong generalization, achieving comparable
performance with just 50% of labeled data compared to standard supervised
training on the complete labeled datasets."
1379,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,"extensive
experiments on the public dataset of hecktor 2022 [7] demonstrate that our xsurv
outperforms state-of-the-art survival prediction methods, including the
top-performing methods in hecktor 2022."
1380,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,"we adopted the training dataset of hecktor 2022 (refer to
https://hecktor.grand-cha llenge.org/), including 488 h&n cancer patients
acquired from seven medical centers [7], while the testing dataset was excluded
as its ground-truth labels are not released."
1381,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,"extensive
experiments have shown that the proposed framework and blocks enable our xsurv
to outperform state-of-the-art survival prediction methods on the
well-benchmarked hecktor 2022 dataset."
1382,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,"therefore, the
contributions of this work can be summarized as: 1) a novel graph-based model
for predicting survival that extracts both local and global properties by
identifying morphological super-nodes; 2) introducing a fine-coarse feature
distillation module with 3 various strategies to aggregate interactions at
different scales; 3) outperforming sota approaches in both risk prediction and
patient stratification scenarios on two datasets; 4) publishing two large and
rare prostate cancer datasets containing more than 220 graphs for active
surveillance and 240 graphs for brachytherapy cases."
1383,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,"we utilize two prostate cancer (pca) datasets to evaluate the performance of our
proposed model."
1384,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,"although majority of patients in our cohort are classified as low-risk based on
nccn guidelines [21], a significant subset of them experienced disease upgrade
that triggered definitive therapy (range: 6.2 to 224 months after diagnosis).the
second dataset (pca-bt) includes 105 pca patients with low to high risk disease
who went through brachytherapy."
1385,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,"we also utilized the prostate
cancer grade assessment (panda) challenge dataset [7] that includes more than
10,000 pca needle biopsy slides (no outcome data) as an external dataset for
training the encoder of our model."
1386,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,"while none of the baselines are capable of assigning patients into
risk groups with statistical significance, our distillation policies achieve
significant separation in both pca-as and pca-bt datasets; suggesting that
global histo-morphological properties improve patient stratification
performance."
1387,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,"however, the best baseline with vit still has poorer performance
compared to our model in both datasets, while the number of parameters (reported
for vit embeddings' size in table 1) in our full-model is about half of this
baseline."
1388,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,"the proposed method is evaluated on four datasets, including two h&e stained
image datasets consep [3] and cpm17 [28] and two ihc stained datasets deepliif
[29] and bc-deepliif [29,32]."
1389,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,"in
the experiments, models trained on one of the datasets will be evaluated on the
three unseen ones."
1390,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,"to avoid the influence of the different sample numbers of the
datasets, we calculate the average scores within each unseen domain respectively
and then average them across domains.in this paper, we re-implement some
existing popular domain generalization algorithms for comparisons under the same
training conditions."
1391,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"this is also a
big cause of concern for publicly available h&e/ihc cell segmentation datasets
with immune cell annotations from single pathologists."
1392,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"this requires only affine registration to align the digitized
restained images to obtain non-occluded signal intensity profiles for all the
markers, similar to mif staining/scanning.in this paper, we introduce a new
dataset that can be readily used out-ofthe-box with any artificial intelligence
(ai)/deep learning algorithms for spatial characterization of tumor immune
microenvironment and several other use cases.to date, only two denovo stained
datasets have been released publicly: bci h&e and singleplex ihc her2 dataset
[7] and deepliif singleplex ihc ki67 and mif dataset [2], both without any
immune or tumor markers."
1393,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"in contrast, we release the first denovo mif/mihc
stained dataset with tumor and immune markers for more accurate characterization
of tumor immune microenvironment."
1394,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"the complete staining protocols for this dataset are given in the accompanying
supplementary material."
1395,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"it provides a standardized dataset to demonstrate the
equivalence of the two methods and a source that can be used to calibrate other
methods."
1396,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"in this section, we demonstrate some of the use cases enabled by this
high-quality ai-ready dataset."
1397,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"we
extracted 268 tiles of size 512×512 from this final segmented and co-registered
dataset."
1398,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"we randomly extracted tiles from
the lyon19 challenge dataset [14] to use as style ihc images."
1399,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"using these
images, we created a dataset of synthetically generated ihc images from the
hematoxylin and its marker image as shown in fig."
1400,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"3.we evaluated the
effectiveness of our synthetically generated dataset (stylized ihc images and
corresponding segmented/classified masks) using our generated dataset with the
nuclick training dataset (containing manually segmented cd3/cd8 cells) [6]."
1401,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"we
randomly selected 840 and 230 patches of size 256 × 256 from the created dataset
for training and validation, respectively."
1402,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"nuclick training and validation sets
[6] comprise 671 and 200 patches, respectively, of size 256 × 256 extracted from
lyon19 dataset [14]."
1403,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"only the total number of
lymphocytes in each image patch are reported in this dataset."
1404,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"to evaluate the
performance of trained models on this dataset, we counted the total number of
marked lymphocytes in a predicted mask and calculated the difference between the
reported number of lymphocytes in each image with the total number of
lymphocytes in the predicted mask by the model."
1405,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"in table 2, the average
difference value (diffcount) of lymphocyte number for the whole dataset is
reported for each model."
1406,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"as seen, the trained models on our dataset outperform
the models trained solely on nuclick data."
1407,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"there are several public h&e/ihc cell segmentation datasets with manual immune
cell annotations from single pathologists."
1408,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"sample images/results taken from the
testing dataset are shown in fig."
1409,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"we have released the first ai-ready restained and co-registered mif and mihc
dataset for head-and-neck squamous cell carcinoma patients."
1410,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"this dataset can be
used for virtual phenotyping given standard clinical hematoxylin images, virtual
clinical ihc dab generation with ground truth segmentations (to train
highquality segmentation models across multiple cancer types) created from
cleaner mif images, as well as for generating standardized clean mif images from
neighboring h&e and ihc sections for registration and 3d reconstruction of
tissue specimens."
1411,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,"in the future, we will release similar datasets for additional
cancer types as well as release for this dataset corresponding whole-cell
segmentations via impartial https://github.com/nadeemlab/impartial."
1412,Detection of Basal Cell Carcinoma in Whole Slide Images,"existing skin cancer detection methods [7][8][9] typically employs
models like inception net and resnet, designed for natural images like those in
the imagenet dataset."
1413,Detection of Basal Cell Carcinoma in Whole Slide Images,"we
evaluate the searched architectures on the skin cancer dataset."
1414,Detection of Basal Cell Carcinoma in Whole Slide Images,"the original dataset is typically
split into training d t and validation datasets dv."
1415,Detection of Basal Cell Carcinoma in Whole Slide Images,"classification accuracy) on the validation dataset, i.e.,where
f p is the resource budget of flops."
1416,Detection of Basal Cell Carcinoma in Whole Slide Images,"3 experiments the dataset, comprised of 194 skin slides acquired from the
southern sun pathology laboratory, includes 148 bcc cases and 46 other types
(common nevus, scc), all manually annotated by a dermatopathologist."
1417,Detection of Basal Cell Carcinoma in Whole Slide Images,"we validated our algorithm using the curated skin cancer dataset and sc-net as a
supernet, testing both heavy and light models."
1418,Detection of Basal Cell Carcinoma in Whole Slide Images,"to
ensure a fair comparison on our dataset, we selected several papers in the field
of pathological image analysis, such as [9,22,23], as well as others using the
ua principle, such as [18,24].evaluation metrics."
1419,Detection of Basal Cell Carcinoma in Whole Slide Images,"with scnet and
evolutionary search, we obtained optimal architectures, achieving 96.2% top-1
and 96.5% accuracy on a skin cancer dataset, improvements of 4.8% and 4.7% over
baselines."
1420,Detection of Basal Cell Carcinoma in Whole Slide Images,"future work will apply our approach to larger datasets for
wider-scale validation."
1421,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,"2) we develop a label-disambiguation module that leverages
prototypes and confidence bank to tackle the weakly supervised nature of
instance-level supervision and reduce the impact of assigned noisy labels.3) the
proposed framework outperforms state-of-the-art (sota) methods on public
datasets and in a practical clinical task, demonstrating its superiors in wsi
analysis."
1422,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,we evaluate our model with three datasets.
1423,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,"(1) luad-gm dataset: the objective is
to predict the epidermal growth factor receptor (egfr) gene mutations in
patients with lung adenocarcinoma (luad) using 723 whole slide image (wsi)
slices, where 47% of cases have egfr mutations."
1424,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,"(2) tcga-nsclc and tcga-rcc
datasets: cancer type classification is performed using the cancer genome atlas
(tcga) dataset."
1425,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,"the tcga-nsclc dataset comprised two subtypes, lung squamous
cell carcinoma (lusc) and lung adenocarcinoma (luad), while the tcga-rcc dataset
included three subtypes: renal chromophobe cell carcinoma (kich), renal clear
cell carcinoma (kirc), and renal papillary cell carcinoma (kirp)."
1426,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,"the dataset was randomly split into three parts: training, validation, and
testing, with 60%, 20%, and 20% of the samples, respectively."
1427,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,"experimental
results demonstrate that iib-mil surpasses current sota techniques on publicly
available datasets, and holds significant potential for addressing more complex
clinical applications, such as predicting gene mutations."
1428,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"a wsi dataset t can be defined as:where x i denotes a
patient, y i the label of x i , i j i is the j-th instance of x i , n is the
number of patients and n is the number of instances."
1429,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"however, the wsi dataset generally has
a long sequence of instances, which makes the clustering algorithms
computationally expensive and slow down as the size of the bag increases.to
solve the issue above, we propose to apply the self-attention (sa) mechanism in
transformer to re-calibrate these cluster prototypes."
1430,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"to evaluate the effectiveness of mspt, we conducted experiments on two public
dataset, namely camelyon16 [24] and tcga-nsclc."
1431,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"camelyon16 is a wsi dataset for
the automated detection of metastases in lymph node tissue slides."
1432,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"the tcga-nsclc dataset includes two
sub-types of lung cancer, i.e., lung squamous cell carcinoma (tgca-lusc) and
lung adenocarcinoma (tcga-luad)."
1433,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"the dataset yields 4.3 million patches at 20× magnification, 1.1 million patches
at 10× magnification, and 0.30 million patches at 5× magnification with an
average of about 5000, 1200, and 350 patches per bag."
1434,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"for camelyon16 dataset,
we reported the results of the official testing set."
1435,Multi-scale Prototypical Transformer for Whole Slide Image Classification,[9] for the camelyon16 and tcga datasets.
1436,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"but [9] only trained simclr encoders
at 20× and 5× magnification, to align with that setting, we used the same
settings to train the simclr encoder at 10× magnification on both datasets."
1437,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"1 shows the comparison results on the camelyon16 and
tcga-nsclc datasets."
1438,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"to evaluate the effectiveness of the pt, we first changed the number of
prototypes k in the range of {1, 2, 4, 8, 16, 32} to get the optimal k for each
dataset."
1439,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"in the camelyon16 dataset, the performance
of both pt and prototype-bag increases with the increase of k value, and
achieves the best results with k = 16."
1440,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"in the tcga-nsclc dataset, pt always
outperforms the fullbag and prototype-bag."
1441,Multi-scale Prototypical Transformer for Whole Slide Image Classification,"(3)
ms-attention: this variant used attention-pooling [8] on the cluster prototypes
for each magnification, and then added them.table 2 gives the results on the
camelyon16 and tcga-nsclc datasets."
1442,Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,dataset.
1443,Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,"our dataset contained 282 consecutive patients who underwent thyroid
nodule examination at nanjing drum tower hospital."
1444,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"finally, we apply the learned ssimnet to produce the final
nuclei segmentation.to validate the effectiveness of our method, we conduct
extensive experiments on the monuseg dataset [16,17] based on ten existing
unsupervised segmentation methods [9,[11][12][13][14][15][18][19][20]."
1445,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"our
method outperforms all comparison methods with an average dice score of 0.792
and aggregated jaccard index of 0.498 on the monuseg dataset which is close to
the supervised method."
1446,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"the monuseg dataset consists of 44 h&e stained histopathology
images with 28,846 manually annotated nuclei."
1447,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,the cpm17 dataset [24] is also derived from tcga repository.
1448,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"our experiment repeated ten times on
monuseg dataset and only once on cpm17 dataset for an augmented convenience."
1449,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"to evaluate the effectiveness of ssimnet, we compare it with several deep
learning based and conventional unsupervised segmentation methods on the
mentioned datasets, including minibatch k-means (termed as mkmeans), gaussian
mixture model [9] (termed as gmm), invariant information clustering [12] (termed
as iic), double dip [18], deep clustering via adaptive gmm model [19] (termed as
dcagmm), deep image clustering [13] (termed as dic), kim's work [20], kanezaki's
work [11], deep conditional gmm [14] (termed as dcgmm), and deep constrained
gaussian network [15] (termed as dcgn).for the methods without public codes, we
report the results from the original publications for a fair comparison."
1450,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"it also conforms the effectiveness of our
method on eliminating the model confusion in the region between adjacent nuclei
and the ability in capturing nuclei shape.besides, we conduct an additional
comparison experiment based on cpm17 dataset to demonstrate the generalization
of our method."
1451,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"moreover, as the image size of cpm17 is smaller than that of
monuseg, the performance gain is not as big as on the monuseg dataset."
1452,Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,"comprehensive
experimental results demonstrate that ssimnet achieves the best performances on
the benchmark monuseg and cpm17 datasets, outperforming other unsupervised
segmentation methods."
1453,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,"the graphs are defined by g =
(v, e), where v is the set of vertices (nodes) v = {v 1 , ...v n } with τ n
vertex types, and e is the collection of pairs of vertices from v, e = {e 1 ,
...e m }, which are called edges and φ n is the mapping function that maps every
vertex to one of n differential marker expressions in this dataset φ n : v → τ n
."
1454,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"as such, generative models can be sampled to emphasize each
disease subtype equally and generate more balanced datasets, thus preventing
dataset biases getting amplified by the models [7]."
1455,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"synthetic datasets can also tackle privacy
concerns surrounding medical data sharing."
1456,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"in this work, (1) we leverage recently
discovered capabilities of ddpms to design a first-of-its-kind nuclei-aware
semantic diffusion model (nasdm) that can generate realistic tissue patches
given a semantic mask comprising of multiple nuclei types, (2) we train our
framework on the lizard dataset [5] consisting of colon histology images and
achieve state-of-the-art generation capabilities, and (3) we perform extensive
ablative, qualitative, and quantitative analyses to establish the proficiency of
our framework on this tissue generation task."
1457,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"it is also challenging to capture long-tailed distributions
and synthesize rare samples from imbalanced datasets using gans."
1458,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,we use the lizard dataset [5] to demonstrate our framework.
1459,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"this dataset
consists of histology image regions of colon tissue from six different data
sources at 20× objective magnification."
1460,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"a generative model trained on this dataset can be used to
effectively synthesize the colonic tumor micro-environments."
1461,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"the dataset
contains 238 image regions, with an average size of 1055 × 934 pixels."
1462,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"[26] to transform all the slides to match the stain distribution of an
empirically chosen slide from the training dataset."
1463,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,"in all
following experiments, we synthesize images using the semantic masks of the
held-out dataset at the concerned objective magnification."
1464,NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,we demonstrate the model on a colon dataset and qualitatively fig.
1465,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,"(3) the
proposed diffdp is extensively evaluated on a clinical dataset consisting of 130
rectum cancer patients, and the results demonstrate that our approach
outperforms other state-of-the-art methods."
1466,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,dataset and evaluations.
1467,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,"we measure the performance of our model on an in-house
rectum cancer dataset which contains 130 patients who underwent volumetric
modulated arc therapy (vmat) treatment at west china hospital."
1468,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,"extensive experiments on
an in-house dataset with 130 rectum cancer patients demonstrate the superiority
of our method."
1469,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"the proposed approach was evaluated on a public tcga-lung dataset and an
in-house endometrial dataset and compared with 6 state-of-the-art methods."
1470,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"(3) the
experiments on two datasets show our pama can achieve competitive performance
compared with sota mil methods and ssl methods."
1471,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"we evaluated the proposed method on two datasets, the public tcga-lung and the
in-house endometrial dataset, which are introduced as follows.algorithm 1:
kernel reorientation algorithm.input: p (n) ∈ n h×n k ×np : the relative polar
angle matrix of n-th block, where h is the head number of multi-head attention,
n k is the number of anchors in the wsi, np is the number of patches in the wsi;
a (n) ∈ r h×n k ×np : the attention matrix from anchors to patches, defined asd
score : a dictionary taking the angle as key for storing attention scores;
output: p (n+1) ∈ r h×n k ×np : the updated polar angle matrix.h,i,max = arg max
d score ; // find the orientation that has the highest attention score."
1472,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,tcga-lung dataset is collected from the cancer genome atlas (tcga) data portal.
1473,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"the dataset includes a total of 3,064 wsis, which consist of three categories,
namely tumor-free (normal), lung adenocarcinoma (luad), and lung squamous cancer
(lusc), endometrial dataset includes 3,654 wsis of endometrial pathology, which
includes 8 categories, namely well/moderately/low-differentiated endometrioid
adenocarcinoma, squamous differentiation carcinoma, plasmacytoid carcinoma,
clear cell carcinoma, mixed-cell adenocarcinoma, and benign tumor.each dataset
was randomly divided into training, validation and test sets according to 6:1:3
while keeping each category of data proportionally."
1474,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"we conducted wsi multi-type
classification experiments on the two datasets."
1475,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"we first conducted experiments on the endometrial dataset to verify the
effectiveness of self-supervised learning for wsi analysis under label-limited
conditions."
1476,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"in
comparison with the second-best methods, pama achieves an increase of
0.015/0.011 and 0.025/0.009 in aucs on tcga and endometrial datasets,
respectively, by using 35%/100% labeled wsis."
1477,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"moreover, pama reveals the most
robust capacity when reducing the training data from 100% to 35%, with auc
decreasing slightly from 0.988 to 0.982 and from 0.851 to 0.829 on the two
datasets."
1478,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"the experiments on two large-scale datasets have
demonstrated the effectiveness of pama in the condition of limited-label."
1479,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,"future work will focus on training the wsi
representation model based on datasets across multiple organs, thus promoting
the generalization ability of the model for different downstream tasks."
1480,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"cervical cancer
cell detection datasets involve labeling individual and small bounding boxes in
a large number of cells."
1481,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"generally, large-scale pre-training usually requires
a massive dataset and a suitable loss function."
1482,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"therefore, we only need
2,000-3,000 wsi samples to obtain a dataset that can even be compared to
imagenet in quantity."
1483,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,dataset and experimental setup.
1484,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"first, we label a
dataset with cell-level bounding boxes to train a detection model."
1485,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"the detection
dataset has 3761 images and 7623 cell-level annotations."
1486,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,"thus, we conclude that a detection model trained with
an expensive annotated dataset is not necessary to build a cad pipeline for
cervical abnormality.ablation study."
1487,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"our proposed
method was trained and tested on the 2015 mic-cai gland segmentation (glas)
challenge dataset [20] and colorectal adenocarcinoma gland (crag) dataset [6]
(as shown in fig."
1488,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"we evaluated the effectiveness of the proposed model on two datasets: the glas
dataset and the crag dataset."
1489,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"the glas dataset comprises 85 training and 80
testing images, divided into 60 images in test a and 20 images in test b."
1490,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"the
crag dataset consists of 173 training and 40 testing images."
1491,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"furthermore, to enhance the
training dataset and mitigate the risk of overfitting, we employed random
combinations of image flipping, translation, gaussian blur, brightness
variation, and other augmentation techniques.we assessed the segmentation
results using three metrics from the glas challenge: (1) object f1, which
measures the accuracy of detecting individual glands, (2) object dice, which
evaluates the volume-based accuracy of gland segmentation, and (3) object
hausdorff, which assesses the shape similarity between the segmentation result
and the ground truth."
1492,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"we trained on the glas and crag
datasets in a python 3.8.3 environment on ubuntu 18.04, using pytorch 1.10 and
cuda 11.4."
1493,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"table 1 provides an overview of the average performance of these
models.our proposed model demonstrated a enhancement in performance, surpassing
the second-best method on both test a and test b datasets."
1494,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"the proposed model was additionally evaluated on the crag dataset by comparing
it against the gcsba-net, doubleu-net, dse model, mild-net, and dcan."
1495,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"these results
demonstrate the effectiveness of our method in segmenting different
datasets.ablation studies: our network utilizes the mask branch and conditional
encoding to enhance performance and segmentation quality."
1496,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"ablation studies on
the glas and crag datasets confirm the effectiveness of these modules (table 3)."
1497,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,"experimental results on the glas dataset and
crag dataset show that our method surpasses state-of-the-art approach,
demonstrating its effectiveness."
1498,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,"high inconsistencies, our proposed asp loss helps the network
learn more robustly.lastly, to support further research in virtual
ihc-restaining, we present the multi-ihc stain translation (mist) as a new
public dataset."
1499,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,"the mist dataset contains 4k+ training and 1k testing aligned
h&e-ihc patches for each of the following ihc stains that are critical for
breast cancer diagnostics: her2, ki67, er (estrogen receptor) and pr
(progesterone receptor)."
1500,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,datasets.
1501,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,"the following datasets are used in our experiments: the breast cancer
immunohistochemical (bci) challenge dataset [7] and our own mist dataset that is
now in the public domain."
1502,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,"due to the page limit, from the mist dataset, here we only present
detailed results on her2 and er."
1503,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,"overall, it can be observed that the
proposed framework with the asp loss consistently outperforms existing methods
across all three datasets."
1504,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,"finally, we have made public our multi-ihc stain translation dataset
with the hope to assist further research towards accurate h&e-to-ihc stain
translation."
1505,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"the
extensive experiments with promising results on two public wsi datasets from
tcga projects, i.e., kidney carcinoma (kica) and esophageal carcinoma (esca),
validate the effectiveness and efficiency of our framework on both tumor
subtyping and staging tasks."
1506,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,datasets and evaluation metrics.
1507,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"we assess the efficacy of the proposed higt
framework by testing it on two publicly available datasets (kica and esca) from
the cancer genome atlas (tcga) repository."
1508,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"the datasets are described below in
more detail:-kica dataset."
1509,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"the kica dataset consists of 371 cases of kidney
carcinoma, of which 279 are classified as early-stage and 92 as late-stage."
1510,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,-esca dataset.
1511,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"the esca dataset comprises 161 cases of esophageal carcinoma, with 96 cases
classified as early-stage and 65 as late-stage."
1512,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"the results for esca and kica datasets
are summarized in table 1 and table 2, respectively."
1513,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"even for the non-hierarchical graph-transformer baseline la-mil and
hierarchical transformer model hipt, our model approaches at least around 3% and
2% improvement on auc and acc in the classification of the staging of the kica
dataset."
1514,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,"extensive experimentation on two public wsi datasets demonstrates the
effectiveness and efficiency of our designed framework, yielding promising
results."
1515,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"although some classification methods achieve promising performance on balanced
and clean medical datasets, balanced datasets with high-accuracy annotations are
time-consuming and expensive."
1516,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"besides, pruning clean and balanced datasets
require a large amount of crucial clinical data, which is insufficient for
large-scale deep learning."
1517,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"noisy
imbalanced datasets arise due to the lack of high-quality annotations [11] and
skewed data distributions [18] where the number of instances largely varies
across different classes."
1518,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"therefore, noisy-labeled, imbalanced datasets with various class
hardness remain a persistent challenge in medical classification.existing
approaches for non-ideal medical image classification can be summarized into
noisy classification, imbalanced recognition, and noisy imbalanced
identification."
1519,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"the main
contributions of our work include: 1) we decompose the negative effects in
practical medical image classification, 2) we minimize the invariant risk to
tackle noise identification influenced by multiple factors, enabling the
classifier to learn causal features and be distribution-invariant, 3) a
re-scaling class-aware gaussian mixture modeling (cgmm) approach is proposed to
distinguish noise labels under various class hardness, 4) we evaluate our method
on two medical image datasets, and conduct thorough ablation studies to
demonstrate our approach's effectiveness."
1520,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"in the noisy imbalanced classification setting, we denote a medical dataset as
{(x i , y i )} n i=1 where y i is the corresponding label of data x i and n is
the total amount of instances."
1521,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"further, we split the
dataset according to class categories."
1522,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"we aim to train a robust medical image classification model composed of a
representation backbone and a classifier head on label noise and imbalance
distribution, resulting in a minimized loss on the testing dataset: we decompose
the non-linear mapping p(y = c|x) as a product of two space
mappings p g (y = c|z) • p h (z|x)."
1523,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"we evaluated our approach on two medical image datasets with imbalanced class
distributions and noisy labels."
1524,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"the first dataset, ham10000 [22], is a
dermatoscopic image dataset for skin-lesion classification with 10,015 images
divided into seven categories."
1525,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"the second dataset, chaoyang [29], is a histopathology
image dataset manually annotated into four cancer categories by three
pathological experts, with 40% of training samples having inconsistent
annotations from the experts."
1526,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"consequently, chaoyang dataset consists of a training set with 2,181 images, a
validation set with 713 images, and a testing set with 1,426 images, where the
validation and testing sets have clean labels."
1527,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"figure 3a and 3b show that only using mer or rcgm
achieves better performance than our strong baseline on both datasets."
1528,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"further, our multi-stage noise removal technique outperforms single mer and
rcgm, revealing that the decomposition for noise effect and hardness effect
works on noisy imbalanced datasets."
1529,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"we find that the combination of mer and rcgm
improves more on chaoyang dataset."
1530,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,"to solve these difficulties, we
conduct multi-environment risk minimization (mer) and rescaling class-aware
gaussian mixture modeling (rcgm) together for robust feature learning.extensive
results on two public medical image datasets have verified that our framework
works on the noisy imbalanced classification problem."
1531,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"patients with colorectal cancer typically undergo contrast-enhanced computed
tomography (cect) scans multiple times during follow-up visits after surgery for
early detection of crlm, generating a 5d dataset."
1532,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"e3d-lstm [12] shows
uni-directional lstm works well on natural videos while several other works show
bi-directional lstm is needed in certain medical image segmentation tasks
[2,7].in this paper, we investigate how state-of-art deep learning models can be
applied to the crlm prediction task using our 5d cect dataset."
1533,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"we evaluate the
effectiveness of bi-directional lstm and explore the possible method of
incorporating different phases in the cect dataset."
1534,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"our dataset follows specific inclusion criteria:-no tumor appears on the ct
scans."
1535,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"-patients have two or
more times of cect scans.-we already determined whether or not the patients had
liver metastases within 2 years after the surgery, and manually labeled the
dataset based on this."
1536,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"-no potential focal infection in the liver before the
colorectal radical surgery.-no metastases in other organs before the liver
metastases.-no other malignant tumors.our retrospective dataset includes two
cohorts from two hospitals."
1537,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"additional statistics on our dataset are presented in table 1 and examples of
representative images are shown in fig."
1538,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"the dataset is available upon
request."
1539,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"4) simvp
[4], introduced by gao et al., uses cnn as the translator instead of lstm.all of
these models need to be modified to handle 5d cect datasets."
1540,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"a straightforward
way to extend them is simply concatenating the a phase and v phase together,
thus collapsing the 5d dataset to 4d."
1541,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"after this, the output y v,t0 is passed into the
bi-directional lstm module in the next layer and viewed as input for this
module.figure 2(a) illustrates how mpbd-lstm uses these 3d-lstm building blocks
to handle the multiple phases in our ct scan dataset."
1542,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"when the spatiotemporal dataset
enters the model, it is divided into smaller groups based on timestamps and
phases."
1543,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"we selected 170 patients who underwent three or more cect scans from our
original dataset, and cropped the images to only include the liver area, as
shown in fig."
1544,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"to handle the imbalanced training dataset, we selected and
duplicated 60% of positive cases and 20% of negative cases by applying standard
scale jittering (ssj) [5]."
1545,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"we used the a and v
phases of cect for our crlm prediction task since the p phase is only relevant
when tumors are significantly present, which is not the case in our dataset."
1546,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"as the data size is limited, 10-fold cross-validation is adopted, and the ratio
of training and testing dataset is 0.9 and 0.1, respectively."
1547,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"table 2 shows the auc scores of all
models tested on our dataset."
1548,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"by replacing the bi-directional connection with a uni-directional
connection, the mpbd-lstm model's performance decreased to 0.768 on the original
dataset."
1549,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"how to effectively address
inter-patient variability in the dataset, perhaps by better fusing the 5d
features, requires further research from the community in the future."
1550,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"in this paper, we put forward a 5d cect dataset for crlm prediction."
1551,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,"based on
the popular e3d-lstm model, we established mpbd-lstm model by replacing the
uni-directional connection with the bi-directional connection to better capture
the temporal information in the cect dataset."
1552,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,none
1553,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,datasets and experimental setup.
1554,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"at the wsi
level, we use two datasets."
1555,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"dataset-small is balanced with 100 positive wsis and
100 negative wsis."
1556,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"we further validate upon an
imbalanced dataset-large of 7654 wsis."
1557,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"there are only 140 positive wsis in this
dataset, which is closer to real world."
1558,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"these two wsi-level datasets have no
overlay with the data used to train the above detection and classification
tasks.for implementation details, the models are implemented by pytorch and
trained on 4 nvidia tesla v100s gpus."
1559,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"to
save computation, we did not verify the performance of the methods that
performed too poorly on dataset-small."
1560,Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,"transformer shows a better capacity of feature aggregation
than other wsi-level classifiers, raising the auc on dataset-large to 84.18%."
1561,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,"combining all the loss functions above, the total objective l
total to train the proposed cellgan in an adversarial manner can be expressed
as:where λ reg is empirically set to 0.01 in our experiments.3 experimental
results dataset."
1562,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"such distinct
morphological differences can be characterized by the tbsrtc category, which
thus inspires us to utilize the handy image-wise grading labels to guide the
nuclei segmentation model learning from unbalanced datasets."
1563,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"(3) we establish a dataset of
thyroid cytopathology image patches of 224 × 224, where 4,965 image labels are
provided following tbsrtc, and 1,473 of them are densely annotated [3] (to be on
github upon acceptance)."
1564,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"to the best of our knowledge, it is the first
publicized thyroid cytopathology dataset of both image-wise and pixel-wise
labels."
1565,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"the annotated dataset well alleviates the insufficiency of an open
cytopathology dataset for computer-assisted analysis (fig."
1566,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"in tcsegnet, we introduce a tbsrtc-category label guidance block to
address the learning issue from unbalanced routine datasets."
1567,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"correspondingly,
to train this block, we use a cross-entropy loss function (ce) that provides an
extra supervision signal to help the network learn from unbalanced datasets,
defined as follows:where y cls is the image-wise tbsrtc-category label, and the
balancing coefficient γ cls is set to 3, as the global feature captured by the
transformer branch is tightly correlated with the image-level classification
tag."
1568,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,image dataset.
1569,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"we construct a clinical thyroid cytopathology dataset with images
of both image-wise and pixel-wise labels as a benchmark (appear in github upon
acceptance) some representative images are presented in fig."
1570,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"2, together with
the profile of the dataset."
1571,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"the dataset comprises 4,965 h&e stained image
patches and labels of tbsrtc, where a subset of 1,473 images was densely
annotated for nuclei boundaries by three experienced cytopathologists and
reached a total number of 31,064 elaborately annotated nuclei."
1572,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"we divided the dataset with image-wise labels into 80%
training samples and the remaining 20% testing samples."
1573,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"importantly, it addresses the challenge of
distinguishing nuclei across different cell scales in an unbalanced dataset."
1574,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,"moreover, we construct the first
thyroid cytopathology dataset with both image-wise and pixel-wise labels, which
we believe can it facilitate future research in this field."
1575,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"we evaluate the proposed method on two public datasets as
herohe challenge and bci challenge, which shows that our method achieves
state-of-theart performance."
1576,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"we pre-train our mmp-mae on the acrobat dataset with adamw [17] and the
learning rate of 1e -4 ."
1577,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"three methods on bci datasets are compared in
our experiments, as shown in table 1."
1578,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"the visualization on the
acrobat dataset also shows our model could learn the modality-related
information, as shown in fig."
1579,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"most of these
methods use the multinetwork ensemble strategy and extra datasets."
1580,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"team macaroon
uses the came-lyon dataset [4] for tumor classification."
1581,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"team mitel uses bach
dataset [1] for tumor classification."
1582,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,"both
the experiment results on bci and herohe datasets show our pretrained mmp-mae
demonstrates strong transfer ability."
1583,Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,"recently, deep learning has achieved remarkable performance in pathological
image segmentation when trained with a large and well-annotated dataset
[6,13,20]."
1584,Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,"in addition, we apply an
uncertainty minimization-based regularization to the average probability
prediction across the decoders, which not only increases the network's
confidence, but also improves the inter-decoder consistency for leveraging
labeled images.the contribution of this work is three-fold: 1) a novel framework
named cdma based on mtnet is introduced for semi-supervised pathological image
segmentation, which leverages different attention mechanisms for generating
diverse and complementary predictions for unlabeled images; 2) a cross decoder
knowledge distillation method is proposed for robust and efficient learning from
noisy pseudo labels, which is combined with an average prediction-based
uncertainty minimization to improve the model's performance; 3) experimental
results show that the proposed cdma outperforms eight state-of-the-art ssl
methods on the public digestpath dataset [3]."
1585,Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,dataset and implementation details.
1586,Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,"we used the public digestpath dataset [3]
for binary segmentation of colonoscopy tumor lesions from whole slide images
(wsi) in the experiment."
1587,Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,"experimental results on a colonoscopy tissue
segmentation dataset demonstrated that our cdma outperformed eight
state-of-the-art ssl methods."
1588,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"we conducted extensive experiments on the dermoscopy dataset isic
2019, and the experimental results show that our method outperforms other
state-of-the-art comparison algorithms by a large margin."
1589,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"given an unlabeled dataset {x u i } n u i=1 with n u images, where x u i is the
ith unlabeled image."
1590,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"in addition, we also have access to a labeled dataset {x l i
, y l i } n l i=1 with n l images, where x l i is the ith labeled image and y l
i ∈ y = 1, ."
1591,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,dataset.
1592,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"to validate the effectiveness of the proposed algorithm, we conduct
experiments on the widely used public dermoscopy challenge dataset isic 2019
[4,5]."
1593,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"the dataset contains a total of 25,331 dermoscopic images from eight
categories: melanoma (mel), melanocytic nevus (nv), basal cell carcinoma (bcc),
actinic keratosis (ak), benign keratosis (bkl), dermatofibroma (df), vascular
lesion (vasc), and squamous cell carcinoma (scc)."
1594,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"since the dataset suffers from
severe category imbalance, we randomly sampled 500 samples from those major
categories (mel, nv, bcc, bkl) to maintain category balance."
1595,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,"following [9,23,24], we report the clustering performance on
the unlabeled unknown category dataset."
1596,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"meanwhile, the size of multimodal
medical datasets is not as large as natural vision-language datasets, which
necessitates the need for data-efficient analytics to address the training
difficulty.to tackle above challenges, we propose a pathology-and-genomics
multimodal framework (i.e., pathomics) for survival prediction (fig."
1597,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"as a result, the task-specific finetuning
broadens the dataset usage (fig 1b andc), which is not limited by data modality
(e.g., both singleand multi-modal data)."
1598,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"our approach could achieve comparable performance even with fewer
finetuned data (e.g., only use 50% of the finetuned data) when compared with
using the entire finetuning dataset."
1599,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,datasets.
1600,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"we collected wsis
from the cancer genome atlas colon adenocarcinoma (tcga-coad) dataset
(cc-by-3.0) [8,21] and rectum adenocarcinoma (tcga-read) dataset (cc-by-3.0)
[8,20], which contain 440 and 153 patients."
1601,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"we implement two types of
settings that involve internal and external datasets for model pretraining and
finetuning."
1602,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"as shown in fig 2a, we pretrain and finetune the model on the same
dataset (i.e., internal setting)."
1603,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"for the external setting, we implement pretraining and
finetuning on the different datasets, as shown in fig 2b ; we use tcga-coad for
pretraining; then, we only use tcga-read for finetuning and final evaluation."
1604,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"in table 1, our approach shows improved survival prediction performance on both
tcga-coad and tcga-read datasets."
1605,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"in
table 1, our method could yield better performance compared with baselines on
the small dataset across the combination of images and multiple types of
genomics data."
1606,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"approach broadens the scope of dataset inclusion, particularly
for model finetuning and evaluation, while enhancing model efficiency on
analyzing multimodal clinical data in real-world settings."
1607,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"for
tcga-coad dataset, we include 50%, 25%, and 10% of the finetuning data."
1608,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,"for the
tcga-read dataset, as the number of uncensored patients is limited, we use 75%,
50%, and 25% of the finetuning data to allow at least one uncensored patient to
be included for finetuning."
1609,Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,"during inference, only the optimized detector is used
to output the final detection results without any additional modules.3
experimental results dataset."
1610,Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,"for cervical cell detection, our dataset includes 3761 images of 1024 ×
1024 pixels cropped from wsis."
1611,Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,"our private dataset was collected and
qualitycontrolled according to a standard protocol involving three pathologists:
a, b, and c."
1612,Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,"we also collect a new dataset of
5000 positive and negative 224 × 224 cell patches to train the
pcn.implementation details."
1613,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,dataset-a: mitos-atypia 14 challenge1 .
1614,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,"this dataset aims to measure the style
transfer performance on 284 histology frames."
1615,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,"( 2) dataset-b: the cancer
genome atlas (tcga)."
1616,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,"this dataset evaluates the performance of stain
normalization quantified by the downstream nine-category tissue structure
classification accuracy [27]."
1617,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,"to further investigate the effect of ensemble number m, we
conduct ablation on dataset-a."
1618,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,none
1619,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,datasets.
1620,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,"we conducted our experiments on the breast invasive carcinoma (brca)
dataset from the cancer genome atlas (tcga)."
1621,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,"specifically, the brca dataset
includes 661 patients with hematoxylin and eosin (he)-stained pathological
imaging and corresponding survival information."
1622,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"given a multimodal dataset d consisting of pairs of wsi pathological images and
genomic data (x i , x g ), our gimp learns feature representations via
accomplishing masked patch modeling and triplets learning."
1623,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"we use a simple multi-layer perceptron (mlp) head to map
cls pat to the final class predictions p , which can be written as p =
softmax(mlp(cls pat )).3 experiments datasets."
1624,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"we verify the effectiveness of our method on the caner genome atlas
(tcga) non-small cell lung cancer (nsclc) dataset, which contains two cancer
subtypes, i.e., lung squamous cell carcinoma (lusc) and lung adenocarcinoma
(luad)."
1625,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"firstly, we compare our proposed patch aggregator with the current
state-of-the-art deep mil models on unimodal tcga-nsclc dataset, i.e., only
pathological wsis are included as input."
1626,Gene-Induced Multimodal Pre-training for Image-Omic Classification,"we can
observe in the table that, our gimp raises acc from 91.05% to 99.47% on
tcga-nsclc dataset."
1627,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"pcl [9] and hcsc [5] integrated the
k-means clustering and contrastive learning model by introducing prototypes as
latent variables and assigning each sample to multiple prototypes to learn the
hierarchical semantic structure of the dataset."
1628,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"the dataset for the former task was collected from 168 patients
with 332 wsis from seoul national university hospital."
1629,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"the liver cancer dataset for the latter task was composed of 323 wsis,
in which the wsis can be further classified into hepatocellular carcinomas
(hccs) (collected from pathology ai platform [1]) and ihccs."
1630,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"the performance of different models from two different datasets is reported in
this section."
1631,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"red dots represent sdt samples and blue dots represent ldt samples
from the ihccs dataset (corresponding histology thumbnail images are shown on
the right)."
1632,Histopathology Image Classification Using Deep Manifold Contrastive Learning,"in
the future, we plan to optimize the algorithm and apply our method to other
datasets and tasks, such as multi-class classification problems and natural
image datasets."
1633,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"-we developed a comprehensive
pipeline for constructing tumor-associated stroma datasets across multiple data
sources, and employed adversarial training and neighborhood consistency
regularization techniques to learn robust multimodal-invariant image
representations."
1634,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"in our study, we utilized three datasets for tumor-associated stroma
analysis.(1) dataset a comprises 513 tiles extracted from the whole mount slides
of 40 patients, sourced from the archives of the pathology department at
cedars-sinai medical center (irb# pro00029960)."
1635,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"the tiles were
annotated at the pixel-level by expert pathologists to generate stroma tissue
segmentation masks and were cross-evaluated and normalized to account for stain
variability.(2) dataset b included 97 whole mount slides with an average size of
over 174,000×142,000 pixels at 40x magnification."
1636,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"(3) dataset c comprised 6134 negative biopsy slides obtained from 262
patients' biopsy procedures, where all samples were diagnosed as negative."
1637,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"dataset a was utilized for
training the stroma segmentation model."
1638,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"this model was then applied to generate stroma
masks for all slides in datasets b and c."
1639,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"to precisely isolate stroma tissues
and avoid data bleeding from epithelial tissues, we only extracted patches where
over 99.5% of the regions were identified as stroma at 40x magnification to
construct the stroma classification dataset.for positive tumor-associated stroma
patches, we sampled patches near tumor glands within annotated tumor region
boundaries, as we presumed that tumor regions represent zones in which the
greatest amount of damage has progressed."
1640,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"to
incorporate multi-modal information, we randomly sampled negative stroma patches
from all biopsy slides in dataset c."
1641,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,"future research can focus on validating our
approach on larger and more diverse datasets and expanding the method to a
patient-level prediction system, ultimately improving prostate cancer diagnosis
and treatment."
1642,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,"furthermore, due to
the large size of crc dataset and relatively high model complexity, patch-gcn
and transmil encountered a memory overflow when processing the crc dataset,
which limits their clinical application."
1643,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,"in addition, the feature aggregation of the lower levels (i.e., patch
and tissue) are guided by the priors, and the mhsa is only executed on
pathological components, resulting in high efficiency even on the crc dataset."
1644,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,"[13] 0.580 ± 0.005 0.634 ± 0.005 0.617 ± 0.094 deepattnmisl [26] 0.570 ± 0.001
0.644 ± 0.009 0.584 ± 0.019 clam [18] 0.575 ± 0.010 0.641 ± 0.002 0.635 ± 0.006
dsmil [16] 0.550 ± 0.016 0.626 ± 0.005 0.603 ± 0.022 patchgcn [ we selected the
crc dataset for further interpretable analysis, as it is one of
the leading causes of mortality in industrialized countries, and its
prognosis-related factors have been widely studied [3,8]."
1645,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,"we trained an encoded
feature based classification model (i.e., a mlp) on a open-source colorectal
cancer dataset (i.e., nct-crc-he-100k [14]), which is annotated with 9 classes,
including: adipose tissue (adi); background (back); debris (deb); lymphocytes
(lym); mucus (muc); muscle (mus); normal colon mucosa (norm); stroma (str);
tumor (tum)."
1646,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"experiments on cva-bus dataset [9] demonstrate that ultra-det,
with real-time inference speed, significantly outperforms previous works,
reducing about 50% fps at a recall rate of 0.90.our contributions are four-fold."
1647,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"(4) we release high-quality labels of the cva-bus
dataset [9] to facilitate future research."
1648,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"we use the open source cva-bus dataset that consists of 186
valid videos, which is proposed in cva-net [9]."
1649,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"we split the dataset into
train-val (154 videos) and test (32 videos) sets."
1650,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"we focus on the lesion detection task and do not utilize the
benign/malignant classification labels provided in the original
dataset.high-quality labels."
1651,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"the bounding box labels provided in the original
cva-bus dataset are unsteady and sometimes inaccurate, leading to jiggling and
inaccurate model predictions."
1652,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,"we use flownets [3] as the fixed flownet in iof align and
share the same finding with previous works [4,12,13] that the flownet trained on
natural datasets generalizes well on ultrasound datasets."
1653,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"as a result,
many methods focus solely on improving a(•) or f (•), leaving g(•) untrained on
the wsi dataset (as shown in fig."
1654,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"(3) we conduct extensive experiments on two
datasets using three different backbones and demonstrate the effectiveness of
our proposed framework."
1655,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"to begin with, we first employ a traditional approach to
train a bag-level classifier f (•) on a given dataset, with patch embeddings
generated by a fixed resnet50 [6] pre-trained on imagenet [19] (step 1 in fig."
1656,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"after this, g(•) is fine-tuned for the specific wsi
dataset, which allows it to generate improved representations for each instance,
thereby enhancing the performance of f (•)."
1657,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"our experiments utilized two datasets, with the first being the publicly
available breast cancer dataset, camelyon16 [1]."
1658,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"this dataset consists of a
total of 399 wsis, with 159 normal and 111 metastasis wsis for the training set,
and the remaining 129 for test."
1659,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"although patch-level labels are officially
provided in camelyon16, they were not used in our experiments.the second dataset
is a private hepatocellular carcinoma (hcc) dataset collected from sir run run
shaw hospital, hangzhou, china."
1660,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"this dataset comprises a total of 1140 valid
tumor wsis scanned at 40× magnification, and the objective is to identify the
severity of each case based on the edmondson-steiner (es) grading."
1661,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"for camelyon16, we tiled the wsis into 256×256 patches on 20× magnification
using the official code of [25], while for the hcc dataset the patches are
384×384 on 40× magnification following the pathologists' advice."
1662,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"for both
datasets, we used an imagenet pre-trained resnet50 to initialize g(•)."
1663,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"camelyon16 results
are reported on the official test split, while the hcc dataset used a 7:1:2
split for training, validation and test."
1664,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"since the number of
instances is very large in wsi datasets, we empirically recommend to choose to
run icmil one iteration for fine-tuning g(•) to achieve the balance between
performance gain and time consumption."
1665,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"results on the hcc dataset also proves the effectiveness of icmil, despite
the minor difference on the relative performance of baseline methods."
1666,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"mean
pooling performs better on this dataset due to the large area of tumor in the
wsis (about 60% patches are tumor patches), which mitigates the impact of
average pooling on instances."
1667,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"also, the performance differences among different
vanilla mil methods tends to be smaller on this dataset since risk grading is a
harder task than camelyon16."
1668,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"as a result, after applying icmil on the mil baselines, these
methods all gain great performance boost on the hcc dataset."
1669,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,"5
displays the instance-level and bag-level representations of camelyon16 dataset
before and after applying icmil on ab-mil backbone.the results indicate that one
iteration of g(•) fine-tuning in icmil significantly improves the instance-level
representations, leading to a better aggregated baglevel representation
naturally."
1670,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,"extensive evaluations
on real-world histology datasets and downstream tasks demonstrate the
superiority of our framework in artifact removal performance, which can generate
reliable restored images while preserving the stain style."
1671,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,dataset.
1672,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,"the test set uses another public histology image dataset [6]
with 462 artifact-free images 2 , where we obtain the paired artifact images by
the manually-synthesized artifacts [18]."
1673,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,"for a fair compaison, we train the
cyclegan with two configurations, namely (#1) using the entire dataset, and (#2)
using only half the dataset, where the latter uses the same number of the
training samples as artifusion."
1674,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,"to this end, we use the public dataset nct-crc-he-100k for training and
crc-val-he-7k for testing, which together contains 100, 000 training samples and
7, 180 test samples."
1675,Artifact Restoration in Histology Images with Diffusion Probabilistic Models,"experimental results on a
public histological dataset demonstrate the superiority of our proposed method
over the state-of-the-art gan counterpart."
1676,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,none
1677,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,"3)
experiments results show our proposed method outperforms five existing
scribble-supervised methods on the public dataset word [17] for multiple
abdominal organ segmentation."
1678,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,"we used the publicly available abdomen ct dataset word [17] for experiments,
which consists of 150 abdominal ct volumes from patients with rectal cancer,
prostate cancer or cervical cancer before radiotherapy."
1679,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,"following
the default settings in [17], the dataset was split into 100 for training, 20
for validation and 30 for testing, respectively, where the scribble annotations
for foreground organs and background in the axial view of the training volumes
had been provided and were used in model training."
1680,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,"figure 2 shows a visual comparison between our method
and the other weakly supervised methods on the word dataset (word 0014.nii)."
1681,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,"experiments on a public abdominal ct dataset
word demonstrated the effectiveness of the proposed method, which outperforms
five existing scribble-based methods and narrows the performance gap between
weakly-supervised and fully-supervised segmentation methods."
1682,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"• we demonstrate the effectiveness of
our approach through experiments using mammography datasets, which show the
superiority of mammo-net.2 proposed method the pipeline of mammo-net is
illustrated in fig."
1683,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"the overall loss function is defined as the sum of these three loss
functions, with coefficients λ and μ used to adjust their relative weights:3
experiments and results mammogram dataset."
1684,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"the cbis-ddsm dataset contains 1249 exams that have been divided based on
the presence or absence of masses, which we used to perform mass classification."
1685,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"the inbreast dataset contains 115 exams with both masses and
micro-calcifications, on which we performed benign and malignant classification."
1686,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,we split the inbreast dataset into training and testing sets in a 7:3 ratio.
1687,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"it
is worth noting that the official inbreast dataset does not provide image-level
labels, so we obtained these labels following shen et al."
1688,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,[20].eye gaze dataset.
1689,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"considering the
relatively small size of our dataset, we used resnet-18 as the backbone of our
network."
1690,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"figure 2 illustrates the visualization of our proposed model on
three representative exams from the inbreast dataset that includes masses,
calcifications, and a combination of both."
1691,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,"our experimental results on mammography datasets demonstrate the
superiority of our proposed model."
1692,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"this study used two unique datasets: (1) the ucla low-dose chest ct dataset, a
collection of 186 exams acquired using siemens ct scanners at an equivalent dose
of 2 mgy following an institutional review board-approved protocol."
1693,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"the dataset was split into 80 scans for training, 20 for validation,
and 86 for testing."
1694,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"(2) aapm-mayo clinic low-dose ct ""grand challenge"" dataset,
a publicly available grand challenge dataset consisting of 5,936 abdominal ct
images from 10 patient cases reconstructed at 1.0 mm slice thickness."
1695,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"this dataset was only
used for evaluating image quality against other harmonization techniques."
1696,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"using the grand challenge dataset, we assessed image
quality and compared it with other previously published low-dose ct denoising
techniques."
1697,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"the model was trained and validated on the
lidc-idri dataset, a public de-identified dataset of diagnostic and low-dose ct
scans with annotations from four experienced thoracic radiologists."
1698,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"as part of
the training process, we only considered nodules annotated by at least three
readers in the lidc dataset."
1699,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"on the grand challenge dataset, ctflow took 3 days to train on an nvidia rtx
8000 gpu."
1700,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,"hoffman, nastaran emaminejad, and
michael mcnitt-gray for providing access to the ucla low-dose ct dataset."
1701,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"our contributions are threefold:• we propose a novel
gaussian-probabilistic guided semantic fusion method for polyp segmentation,
which improves the decoder's global perception of polyp locations and
discrimination capability for polyps in complex scenarios.• we evaluate the
performance of petnet on five widely adopted datasets, demonstrating its
superior ability to identify polyp camouflage and small polyp scenes, achieving
state-of-the-art performance in locating polyps with high precision."
1702,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"in our
evaluation, we also examine the decoder cfm utilized in [3], which shares the
same input features (excluding the first level) as the fus.3 experiments to
evaluate models fairly, we completely follow p ranet [4] and use five public
datasets, including 548 and 900 images from clinicdb [2] and kvasir-seg [5] as
training sets, and the remaining images as validation sets."
1703,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"we also test the
generalization capability of all models on three unseen datasets (etis [13] with
196 images, cvc-colondb [8] with 380 images, and endoscene [15] with 60 images)."
1704,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"our model achieves comparable performance to
the sota model on the kvasir-seg and clinicdb datasets."
1705,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"we conduct three
unseen datasets to test models' generalizability."
1706,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"we selected images from two unseen datasets with 0∼2% polyp
labeled area to perform the test."
1707,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"as shown, p et net demonstrates great strength
in both datasets, which indicates that one of the major advantages of our model
lies in detecting small polyps with lower false-positive rates.ablation
analysis."
1708,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"furthermore, the git method
significantly enhances instance-level evaluation without incurring performance
penalty in pixel-level evaluation, especially in unseen datasets."
1709,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,"experiments show that p et net consistently outperforms
most current cutting-edge models on five challenging datasets, demonstrating its
solid robustness in distinguishing other intestinal analogs."
1710,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the tumor set t is
collected from real-world datasets."
1711,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"specifically, to maintain the rule of
weaklysupervised learning of segmentation and localization tasks, we collect the
tumors from the ddsm dataset as t and train the model on the inbreast
dataset.when training the model on other datasets, we use the tumor set
collected from the inbreast dataset."
1712,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,this study reports experiments on four mammography datasets.
1713,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the inbreast
dataset [7] consists of 115 exams with bi-rads labels and pixel-wise
anno-tations, comprising a total of 87 normal (bi-rads = 1) and 342 abnormal
(bi-rads = 1) images."
1714,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the ddsm dataset [3] consists of 2,620 cases, encompassing
6,406 normal and 4,042 (benign and malignant) images with outlines generated by
an experienced mammographer."
1715,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the vindr-mammo dataset [8] includes 5,000 cases
with bi-rads assessments and bounding box annotations, consisting of 13,404
normal (bi-rads = 1) and 6,580 abnormal (bi-rads = 1) images."
1716,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the in-house
dataset comprises 43,258 mammography exams from 10,670 women between 2004-2020,
collected from a hospital with irb approvals."
1717,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"in this study, we randomly select
20% women of the full dataset, comprising 6,000 normal (bi-rads = 1) and 28,732
abnormal (bi-rads = 1) images."
1718,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"due to a lack of annotations, the in-house
dataset is only utilized for classification tasks."
1719,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"each dataset is randomly
split into training, validation, and testing sets at the patient level in an
8:1:1 ratio, respectively (except for that inbreast which is split with a ratio
of 6:2:2, to keep enough normal samples for the test).table 1."
1720,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"comparison of
asymmetric and abnormal classification tasks on four mammogram datasets."
1721,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the training process on the
inbreast dataset is conducted for 50 epochs with a lr decay of 0.1 every 20
epochs."
1722,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"for the other three datasets, the training is conducted separately on
each one with 20 epochs and a lr decay of 0.1 per 10 epochs."
1723,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the
training takes 3-24 h (related to the size of the dataset) on each dataset.to
assess the performance of different models in classification tasks, we calculate
the area under the receiver operating characteristic curve (auc) metric."
1724,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"our method outperforms all the single-based and mv-based
methods in these classification tasks across all datasets."
1725,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,"the extensive
experiments on four datasets demonstrate the robustness of our disasymnet
framework for improving performance in classification, segmentation, and
localization tasks."
1726,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"to achieve this goal, we
create a synthetic dataset, which has separate annotations for normal kidneys
and protruded regions, and train a segmentation network to separate the
protruded regions from the normal kidney regions."
1727,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"verify
that the proposed framework achieves a higher dice score compared to the
standard 3d u-net using a publicly available dataset."
1728,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"the release of two public ct image datasets with kidney and tumor masks from the
2019/2021 kidney and kidney tumor segmentation challenge [8] (kits19, kits21)
attracted researchers to develop various methods for segmentation.looking at the
top 3 teams from each challenge [6,11,13,17,21], all teams utilized 3d u-net [3]
or v-net [16], which bears a similar architecture."
1729,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,we train this network using synthetic datasets.
1730,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"the details of the
dataset and training procedures are described in sect."
1731,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,synthetic dataset.
1732,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"alternatively, we create a synthetic dataset that mimics a kidney with
protrusions."
1733,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,the synthetic dataset is created through the following steps:1.
1734,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"if both of the following conditions are met, append to
the dataset.where k i is a voxel value (0 or 1) in the kidney mask and t i is a
voxel value in the tumor mask."
1735,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"although our network
is fully differentiable, since there is no separate annotation for protruded
regions other from the synthetic dataset, we freeze the parameters in
protuberance detection network.the output of the protuberance detection network
will likely have more false positives than the base network since it has no
access to the input image."
1736,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,we used a dataset from kits19 [8] which contains both cect and ncct images.
1737,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"cysts are not
annotated separately and included in the kidney label on this dataset."
1738,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"we applied
random rotation, random scaling and random noise addition as data
augmentation.during the step2 phase of the training, where we used the synthetic
dataset, we created 10,000 masks using the method from sect."
1739,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"to cope with isodensity tumors, which have similar intensity values
to their surrounding tissues, we created a synthetic dataset to train a network
that extracts protuberance from the kidney masks."
1740,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,"we evaluated our method using the publicly
available kits19 dataset, and showed that the proposed method can achieve a
higher sensitivity than existing approach."
1741,Skin Lesion Correspondence Localization in Total Body Photography,we evaluated our methods on two datasets.
1742,Skin Lesion Correspondence Localization in Total Body Photography,"the first dataset is from skin3d [26]
(annotated 3dbodytex [18,19])."
1743,Skin Lesion Correspondence Localization in Total Body Photography,"the second dataset comes from a 2d imaging-rich
total body photography system (irtbp), from which the 3d textured meshes are
derived from photogrammetry 3d reconstruction."
1744,Skin Lesion Correspondence Localization in Total Body Photography,"the number of vertices is on
average 300k and 600k for skin3d and irtbp datasets respectively."
1745,Skin Lesion Correspondence Localization in Total Body Photography,example data of the two datasets can be found in the supplement.
1746,Skin Lesion Correspondence Localization in Total Body Photography,"to interpret cle in a clinical application,
the localized correspondence is successful if its cle is less than a threshold
criterion.we measured the success rate as the percentage of the correctly
localized skin lesions over the total number of skin lesion pairs in the
dataset.to compare our result to the existing method [26], we compute our
success rates with the threshold criterion at 10 mm."
1747,Skin Lesion Correspondence Localization in Total Body Photography,"the qualitative result of the localized correspondence in the skin3d
dataset is shown in fig."
1748,Skin Lesion Correspondence Localization in Total Body Photography,"comparison of the success rate on skin3d
dataset."
1749,Skin Lesion Correspondence Localization in Total Body Photography,"the framework is evaluated on a private dataset and a public dataset with
success rates that are comparable to those of the state-of-the-art method.the
proposed method assumes that the local texture enclosing the lesion and its
surroundings should be similar from scan to scan."
1750,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,"it plays a crucial role in
medical image analysis [8] where annotated datasets are only available with
limited size."
1751,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,"this technique allows us
to simulate different physiological states during the training and enrich our
dataset with a wider range of organ and lesion shapes."
1752,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,"a possible explanation for this result is that less
than 30% of the lesions are located close to the bladder, and our dataset did
not contain enough training examples for more improvements.realistic modeling of
organ deformation."
1753,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,"the success of
anatomy-informed da opens the research question of whether it enhances
performance across diverse datasets and model backbones."
1754,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,none
1755,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,the overall loss function is then:3 experiments dataset.
1756,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,"we use a dataset of 172 patients containing 94 paaf and 78 peaf cases
collected from the sun yat-sen memorial hospital in china."
1757,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,"cross-validation is implemented by
splitting the dataset into five equal subsets and using three subsets for
training, one subset for validation, and one subset for testing."
1758,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,"experiments on larger
datasets or alternative tasks can also be done to provide more empirical
support, since current results show only slight improvements over baseline."
1759,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,"based on the ratio of 8:2, the training set and independent test set of the
in-house dataset have 612 and 153 cases, respectively."
1760,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"this dataset included 303 biopsy-proven (stage i-ivb) npc patients who
received radiation treatment during 2012-2016."
1761,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"the use of this dataset was approved by the institutional review board
of the university of hong kong/hospital authority hong kong west cluster (hku/ha
hkw irb) with reference number uw21-412, and the research ethics committee
(kowloon central/kowloon east) with reference number kc/ke-18-0085/er-1."
1762,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"the details of patient
characteristics and the number split for training and testing of each dataset
were illustrated in table 1."
1763,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"prior to model training, mri images were resampled
to 256*224 by bilinear interpolation [14] due to the inconsistent matrix sizes
of the three datasets."
1764,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"different from the original
study, which used single institutional data for model development and utilized
min-max value of the whole dataset for data normalization, in this work, we used
mean and standard deviation of each individual patient to normalize mri
intensities due to the heterogeneity of the mri intensities across institutions
[15]."
1765,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"ji measures
similarity of two datasets, which ranges from 0% to 100%."
1766,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"due to both real patients and synthetic
patients were involved in delineation, to erase the delineation memory of the
same patient, we separated the patients to two datasets, each with the same
number of patients, both two datasets with mixed real patients and synthetic
patients without overlaps (i.e., the ce-mri and vce-mri from the same patient
are not in the same dataset).when finished the first dataset delineation, there
was a one-month interval before the delineation of the second dataset."
1767,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,"the average ji obtained from institution-1,
institution-2, and institution-3 dataset were similar with a result of 71.54%,
74.78% and 75.85%, respectively."
1768,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"our ct scan dataset consists of 62, 420 patients from 16 different sites across
north america, asia and europe."
1769,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"our 3d camera dataset consists of 2, 742 pairs
of depth image and ct scan from 2, 742 patients from 6 different sites across
north america and europe acquired using a ceiling-mounted kinect 2 camera."
1770,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"we trained our autodecoder model on our unpaired ct scan dataset of 62, 420
patients with a latent vector of size 32."
1771,Automated CT Lung Cancer Screening Workflow Using 3D Camera,"the encoder was trained on our paired
ct scan and depth image dataset of 2, 742 patients."
1772,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"our method finds common patterns of disease progression in datasets of
longitudinal images."
1773,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"clinicians
suspect that this is due to the grading system's reliance on static biomarkers
that are unable to capture temporal dynamics which contain critical information
for assessing progression risk.in their search for new biomarkers, clinicians
have annotated known biomarkers in longitudinal datasets that monitor patients
over time and mapped them against disease progression [2,16,19]."
1774,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"however, these
approaches neglect temporal relationships between images and the obtained
biomarkers are by definition static and cannot capture the dynamic nature of the
disease.our contribution: in this work, we present a method to automatically
propose biomarkers that capture temporal dynamics of disease progression in
longitudinal datasets (see fig."
1775,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,we use two retinal oct datasets curated in the scope of the pinnacle study [20].
1776,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"we first design and test our method on a development dataset, which was
collected from the southampton eye unit."
1777,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"afterwards, we test our method on a
second independent unseen dataset, which was obtained from moorfields eye
hospital."
1778,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"after strict quality control, the development
dataset consists of 46,496 scans of 6,236 eyes from 3,456 patients."
1779,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"the
unseen dataset is larger, containing 114,062 scans of 7,253 eyes from 3,819
patients."
1780,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"models were trained on the entire dataset for
120,000 steps using the adam optimiser with a momentum of 0.9, weight decay of
1.5 • 10 -6 and a learning rate of 5 • 10 -4 ."
1781,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"initially, we tune the hyperparameters, λ, φ and k, on the development dataset
by heuristically selecting values that result in higher uniformity between
subtrajectories within each cluster."
1782,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"next, using the
same hyperparameters we apply the method directly to the unseen dataset."
1783,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"the
ophthalmologists then review these clusters and confirm whether they capture the
same temporal biomarkers observed in the development dataset.in addition to the
qualitative evaluation, we also validate the utility of our clusters as
biomarkers that stratify risk of disease progression."
1784,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"sub-trajectory clusters are candidate temporal biomarkers: by first applying our
method to the development dataset we found that using λ = 0.75, φ = 0.75 and k =
30 resulted in the most uniform and homogeneous clusters while still limiting
the total number of clusters to a reasonable amount."
1785,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"using the same
hyperparameters our method generalised to the unseen dataset which yielded
clusters with equivalent dynamics and quality (see fig."
1786,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"ophthalmologists
identified clusters capturing the same variants of temporal progression in both
datasets."
1787,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"we applied our method to two large longitudinal datasets, cataloguing 3,218
total years of disease progression."
1788,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,"as late stage patients were overrepresented in our datasets, we
also intend to apply our method to datasets with greater numbers of patients
progressing from earlier disease stages."
1789,Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,database: we use publicly available tcga gbm-lgg dataset [6].
1790,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"moreover, the automatic delineation of the gtv in the esophagus poses a
significant difficulty, primarily attributable to the low contrast between the
esophageal gtv and the neighboring tissue, as well as the limited
datasets.recently, advances in deep learning [21] have promoted research in
automatic esophageal gtv segmentation from computed tomography (ct) [18,19]."
1791,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"meanwhile, an ideal method
for automatic esophageal gtv segmentation in the second course of rt should
consider three key aspects: 1) changes in tumor volume after the first course of
rt, 2) the proliferation of cancerous cells from a tumor to neighboring healthy
cells, and 3) the anatomical-dependent our training approach leverages
multi-center datasets containing relevant annotations, that challenges the
network to retrieve information from e1 using the features from e2."
1792,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"our training strategy leverages three datasets that introduce prior
knowledge to the network of the following three key aspects: 1) tumor volume
variation, 2) cancer cell proliferation, and 3) reliance of gtv on esophageal
anatomy.nature of gtv on esophageal locations."
1793,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"to achieve this, we efficiently
exploit knowledge from multi-center datasets that are not tailored for
second-course gtv segmentation."
1794,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"in this study, we use a paired first-second course gtv dataset
s p , an unpaired gtv dataset s v , and a public esophagus dataset s e .in order
to fully leverage both public and private datasets, the training objective
should not be specific to any tasks."
1795,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"1, our strategy is to challenge the network to retrieve information from
augmented inputs in e 1 using the features from e 2 , which can incorporate a
wide range of datasets that are not tailored for second-course gtv segmentation."
1796,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"to adequately
monitor changes in tumor volume and integrate information from the initial
course into the subsequent course, a paired first-second courses dataset s p =
{i 1 p , i 2 p , g 1 p ; g 2 p } is necessary for training."
1797,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"the paired dataset s p for the first and second courses is limited, whereas an
unpaired gtv dataset s v = {i v ; g v } can be easily obtained in a standard
clinical workflow with a substantial amount."
1798,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"to make full use of the datasets of relevant tasks, we incorporate a public
esophagus segmentation dataset, denoted as s e = {i e ; g e }, where i e /g e
represent the ct images and corresponding annotations of the esophagus
structure."
1799,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"similarly, data from the paired s p is also augmented by p 1/2 to increase the
network's robustness.in summary, our training strategy is not dataset-specific
or target-specific, thus allowing the integration of prior knowledge from
multi-center esophageal gtv-related datasets, which effectively improves the
network's ability to retrieve information for the second course from the three
key aspects stated in sect."
1800,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,datasets.
1801,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"the paired first-second course dataset, s p , is collected from sun
yat-sen university cancer center (ethics approval number: b2023-107-01),
comprising paired ct scans of 69 distinct patients from south china."
1802,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"we
collected the gtv dataset s v from medmind technology co., ltd., which has ct
scans from 179 patients."
1803,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"to demonstrate the presence of a domain gap between the first and second
courses, we train sota methods with datasets s train p and s v , by feeding the
data sequentially into the network."
1804,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"notably, the paired first-second course dataset s test p pertains
to the same group of patients, thereby ensuring that any performance drop can be
attributed solely to differences in courses of rt, rather than variations across
different patients.figure 2 illustrates the reduction in the gtv area after the
initial course of rt, where the transverse plane is taken from the same location
relative to the vertebrae (yellow lines)."
1805,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,combination of various datasets.
1806,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"table 2 presents the information gain derived
from multi-center datasets using quantified metrics for segmentation
performance."
1807,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"our proposed training strategy fully exploits the datasets s p ,
s v , and s e , and further improve the dsc to 74.54% by utilizing comprehensive
knowledge of both the tumor morphology and esophageal
structures.region-preserving attention module."
1808,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,"besides, to efficiently leverage prior
knowledge contained in various medical ct datasets, we train the network in an
information-querying manner."
1809,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"experimental results on
lidc-idri [1] dataset demonstrate the effectiveness of learning with textual
knowledge for improving lung nodule malignancy prediction.the contributions of
this paper are summarized as follows.1) we propose clip-lung for lung nodule
malignancy prediction, which leverages clinical textual knowledge to enhance the
image encoder and classifier."
1810,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"for the attribute annotations, all the lung nodules in the lidc-idri dataset are
annotated with the same eight attributes: ""subtlety"", ""internal structure"",
""calcification"", ""sphericity"", ""margin"", ""lobulation"", ""spiculation"", and
""texture"" [4,8], and the annotated value for each attribute ranges from 1 to 5
except for ""calcification"" that is ranged from 1 to 6."
1811,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,dataset.
1812,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"lidc-idri [1] is a dataset for pulmonary nodule classification or
detection based on low-dose ct, which involves 1,010 patients."
1813,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"in this paper, we construct
three sub-datasets: lidc-a contains three classes of nodules both in training
and test sets; according to [11], we construct the lidc-b, which contains three
classes of nodules only in the training set, and the test set contains benign
and malignant nodules; lidc-c includes benign and malignant nodules both in
training and test sets.experimental settings."
1814,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"in table 1, we compare the classification performances
on the lidc-a dataset, where we regard the benign-unsure-malignant we argue that
this is due to the indistinguishable textual annotations, such as similar
attributes of different nodules."
1815,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"in addition, we verify the effect of textual
branch of clip-lung using mv-dar [12] on lidc-a dataset."
1816,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"table 2
presents a performance comparison of clip-lung on the lidc-b and lidc-c
datasets."
1817,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,"in table 3, we
verify the effectiveness of different loss components on the three constructed
datasets."
1818,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"additionally, we devise a domain
mixup strategy to resolve the problem of co-occurring artifacts in dermoscopic
images and mitigate the resulting noisy domain label assignments.our
contributions can be summarized as: (1) we resolve an artifacts-derived biasing
problem in skin cancer diagnosis using a novel environment-aware prompt
learning-based dg algorithm, epvt; (2) epvt takes advantage of a vitbased
domain-aware prompt learning and a novel domain prompt generator to improve
domain-specific and cross-domain knowledge learning simultaneously;(3) a domain
mixup strategy is devised to reduce the co-artifacts specific to dermoscopic
images; (4) extensive experiments on four out-of-distribution skin datasets and
six biased isic datasets demonstrate the outperforming generalization ability
and robustness of epvt under heterogeneous distribution shifts."
1819,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"in domain generalization (dg), the training dataset d train consists of m source
domains, denoted as d train = {d k |k = 1, ..., m }."
1820,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"we
train and validate all algorithms on isic2019 [6] dataset, following the split
of [3]."
1821,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"we evaluate models on four
out-of-distribution (ood) datasets, including derm7pt-dermoscopic [14],
derm7pt-clinical [14], ph2 [18], and pad-ufes-20 [21]."
1822,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"(2) trap set debiasing: we train
and test our epvt with its baseline on six trap sets [3] with increasing bias
levels, ranging from 0 (randomly split training and testing sets from the
isic2019 dataset) to 1 (the highest bias level where the correlation between
artifacts and class label is in the opposite direction in the dataset splits)."
1823,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"more details about these datasets and splits are provided in the complementary
material.implementation details: for a fair comparison, we train all models
using vit-base/16 [8] backbone pre-trained on imagenet and report the roc-auc
with five random seeds."
1824,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"the results clearly demonstrate the superiority
of our approach, with the best performance on three out of four ood datasets and
remarkable improvements over the erm algorithm, especially achieving 4.1% and
8.9% improvement on the pad and ph2 datasets, respectively."
1825,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"although some
algorithms may perform similarly to our model on one of the four datasets, none
can consistently match the performance of our method across all four datasets."
1826,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"particularly, our approach showcases the highest average performance, with a
2.05% improvement over the second-best algorithm across all four datasets."
1827,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"when we
combine the adapter, the model's average performance improves by 1.37%, but it
performs worse than erm on pad dataset."
1828,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"the consistently better performance than the baseline on all four
datasets also highlights the importance of addressing coartifacts and
cross-domain learning for dg in skin lesion recognition.trap set debiasing: in
fig."
1829,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"3a, we present the performance of the erm baseline and our epvt on six
biased isic2019 datasets."
1830,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"notably, our epvt outperforms the erm baseline by 9.4% on the bias 1
dataset.prompt weights analysis: to verify whether our model has learned the
correct domain prompts for target domain prediction, we analyze and plot the
results in fig."
1831,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"firstly, we extract the features of each domain from
our training set and extract the feature from one target dataset,
derm7pt-clin.we then calculate the frechet distance [9] between each domain and
the target dataset using the extracted feature, representing the domain distance
between them."
1832,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,"compared to other competitive domain generalization algorithms, our
method achieves outstanding results on three out of four ood datasets and the
second-best on the remaining one."
1833,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"most cad studies were developed on regular and selected
datasets in the laboratory environment, which avoided the problems (data noise,
missing data, etc.) in the clinical scenarios [3,6,9,13,18]."
1834,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"in addition, none of the current multi-label cad studies have
considered the problem of missing labels and noisy labels.considering these
real-world challenges, we propose a multi-label model named self-feedback
transformer (sft), and validate our method on a realworld pnens dataset."
1835,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,real-world pnens dataset.
1836,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"we validated our method on a real-world pnens dataset
from two centers."
1837,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"the dataset contained 264 and 28 patients in center 1 and
center 2, and a senior radiologist annotated the bounding boxes for all 408 and
28 lesions."
1838,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"it is
obvious that the real-world dataset has a large number of labels with randomly
missing data, thus, we used an adjusted 5-fold cross-validation."
1839,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"taking a
patient as a sample, we chose the dataset from center 1 as the internal dataset,
of which the samples with most of the main labels were used as dataset 1 (219
lesions) and was split into 5 folds, and the remaining samples are randomly
divided into the training set dataset 2 (138 lesions) and the validation set
dataset 3 (51 lesions), the training set and the validation set of the
corresponding folds were added during cross-validation, respectively."
1840,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"details of each dataset are in
the supplementary material."
1841,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,dataset evaluation metrics.
1842,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"we evaluate the
performance of our method on the 10 main tasks for internal dataset, and due to
missing labels and too few sstr2 labels, only the performance of predicting rt,
pfs, os, gd, mtf are evaluated for external dataset."
1843,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"both sf and ea perform better than regular mode,
and emc outperforms other modes with a mauc of 0.72 (0.82 on external dataset)."
1844,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,"when using 100 percent labels, the mauc of the internal
dataset decreased from 0.71 (noise ratio = 0.0) to 0.53 (noise ratio = 1.0), a
decrease of 0."
1845,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"the main contributions of our
work are as follows: (1) we propose a well-pruned simple but effective network
for breast tumor segmentation, which shows remarkable and solid performance on
large clinical dataset; (2) our large pretrained model is evaluated on two
additional public datasets without fine-tuning and shows extremely stabilized
improvement, indicating that our model has outstanding generalizability and good
robustness against multi-site data data."
1846,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"cn θcn (g) represents the probability for the
input of cn coming from the original dataset.in the implementation, we update
the segmentation network and all the discriminators alternatingly in each
iteration until both the generator and discriminators are converged."
1847,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"five-fold cross
validation is performed on the dataset in all experiments to verify our proposed
network."
1848,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"for external validation, we further test our model on two independent
publicly-available datasets collected by stu-hospital (dataset 1) [22] and
syu-university (dataset 2) [23]."
1849,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"the
comparison experiments are carried on a large-scale clinical breast ultrasound
dataset, and the quantitative results are reported in table 1."
1850,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"four groups of frameworks (stage i, stage ii, stage iii and stage iv)
are designed, with the numerals denoting the level of deep supervision counting
from the last deconvolutional layer.we test these four frameworks on the
in-house breast ultrasound dataset, and verify their segmentation performance
using the same five evaluation criteria."
1851,Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,"using a large clinical
dataset, our proposed model demonstrates not only state-of-the-art segmentation
performance, but also the outstanding generalizability to new ultrasound data
from different sites."
1852,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"here,
we present the corresponding segmentation predictions of the baseline and our
approach along with the ground truth.our stt-unet approach achieves superior
segmentation performance by accurately segmenting 16% more cell instances in
these examples, compared to res-unet-r.segmentation performance on all three
datasets."
1853,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"set, stt-unet achieves ap-75 score of 0.842 and outperforms the recent 3d
res-unet [16] by 3.0%.figure 1 shows a qualitative comparison between our
stt-unet and 3d res-unet [16] on examples from mitoem-r and mitoem-h datasets."
1854,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"dataset: we evaluate our approach on three datasets: mitoem-r [36], mitoem-h
[36] and lucchi [22]."
1855,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"the mitoem [36] is a dense mitochondria instance
segmentation dataset from isbi 2021 challenge."
1856,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"the dataset consists of 2 em
image volumes (30 μm 3 ) of resolution of 8 × 8 × 30 nm, from rat tissues
(mitoem-r) and human tissue (mitoem-h) samples, respectively."
1857,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"lucchi [22] is a sparse mitochondria semantic segmentation dataset with training
and test volume size of 165 × 1024 × 768.implementation details: we implement
our approach using pytorch1.9 [27] (rcom env) and models are trained using 2 amd
mi250x gpus."
1858,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"for fair comparison with previous works, we use the same
evaluation metrics as in the literature for both datasets."
1859,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"we use 3d ap-75
metric [36] for mitoem-r and mitoem-h datasets."
1860,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"the best
results are obtained with deformable-conv on both datasets."
1861,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"experiments on three datasets demonstrate the effectiveness of
our approach, leading to state-of-the-art segmentation performance."
1862,3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,"our method sets a new
state-of-the-art on this dataset in terms of both jaccard and dsc."
1863,Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,none
1864,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"a question
is raised naturally: can we expand the training dataset with a small proportion
of images labeled to reach or even exceed the segmentation performance of the
fully-supervised baseline?"
1865,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"intuitively, since the labeled images are samples
from the population of histopathology images, if the underlying distribution of
histopathology images is learned, one can generate infinite images and their
pixel-level labels to augment the original dataset."
1866,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"our goal is to augment a dataset containing a limited number of labeled images
with more samples to improve the segmentation performance."
1867,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,datasets.
1868,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,we conduct experiments on two datasets: monuseg [13] and kumar [14].
1869,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"the monuseg dataset has 44 labeled images of size 1000 × 1000, 30 for training
and 14 for testing."
1870,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"the kumar dataset consists of 30 1000×1000 labeled images
from seven organs of the cancer genome atlas (tcga) database."
1871,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"the dataset is
splited into 16 training images and 14 testing images."
1872,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"to validate the effectiveness of the proposed augmentation method, we create 4
subsets of each training dataset with 10%, 20%, 50% and 100% nuclei instance
labels."
1873,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"precisely, we first crop all images of each dataset into 256 × 256
patches with stride 128, then obtain the features of all patches with pretrained
resnet50 [6] and cluster the patches into 6 classes by kmeans."
1874,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"for the diffusion process of both steps, we set the total diffusion timestep
t to 1000 with a linear variance schedule {β 1 , ..., β t } following [8].for
monuseg dataset, we generate 512/512/512/1024 synthetic samples for
10%/20%/50%/100% labeled subsets; for kumar dataset, 256/256/256/512 synthetic
samples are generated for 10%/20%/50%/100% labeled subsets."
1875,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"third, the synthetic nuclei structures and images show great diversity:
the synthetic samples resemble different styles of the real ones but with
apparent differences.we then train segmentation models on the four labeled
subsets of monuseg and kumar dataset and corresponding augmented subsets with
both real and synthetic labeled images."
1876,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"for monuseg
dataset, it is clear that the segmentation metrics drop with fewer labeled
images."
1877,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"interestingly,
augmenting the full dataset also helps: dice increases by 1.3% and aji increases
by 1.6% compared with the original full dataset."
1878,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"for kumar dataset, by augmenting 10% labeled subset, aji
increases to a level comparable with that using 100% labeled images; by
augmenting 20% and 50% labeled subset, ajis exceed the fully-supervised
baseline."
1879,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"these results demonstrate the effectiveness of the proposed
augmentation method that we can achieve the same or higher level segmentation
performance of the fully-supervised baseline by augmenting a dataset with a
small amount of labeled images.generalization of the proposed data augmentation."
1880,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"for both monuseg and
kumar datasets, all the four labeling proportions metrics notably improve with
synthetic samples."
1881,Diffusion-Based Data Augmentation for Nuclei Image Segmentation,"by augmenting datasets
with a small amount of labeled images, we achieved even better segmentation
results than the fully-supervised baseline on some benchmarks."
1882,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"however, there is a practical need to be
able to guide the deep clustering model towards the identification of grouping
structures in a given dataset that have not been already annotated."
1883,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"using a mixture-of-gaussians (mog) prior distribution
for the latent representations z, we examine subgroups or domains within the
dataset, revealed by the individual gaussians within the learned latent space,
and how z affects the generation of x."
1884,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"the colored mnist is an extension to the classic mnist dataset [3], which
contains binary images of handwritten digits."
1885,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"this simple dataset can be used to investigate whether a given
clustering algorithm will categorize the images by color or by the digit label
and whether the proposed conditioning mechanism of cdvade can successfully guide
the clustering away from the categorization we want to avoid (e.g., condition
the model to avoid clustering by color, in order to distinguish the digits in an
unsupervised fashion)."
1886,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"2 a summary of the results for the
experiments on the colored mnist dataset is presented."
1887,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,her2 dataset.
1888,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"the dataset consists of 241
patches extracted from 64 digitized slides of breast cancer tissue which were
stained with her2 antibody."
1889,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"we use a subset of this dataset consisting of 672 images (the remainder is
held out for future research)."
1890,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"because the intended purpose is finding subgroups
in the given dataset only, a separate test set is not used."
1891,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"we refer to [4,8] for more
details about this dataset."
1892,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"this retrospective human subject dataset has been
made available to us by the authors of the prior studies [4,8], who are not
associated with this paper."
1893,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"appropriate ethical approval for the use of this
material in research has been obtained.deep clustering models applied to the
her2 dataset."
1894,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"we evaluate the performance and behavior of the dec, vade, and
cdvade models on the her2 dataset."
1895,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"to investigate the
clustering abilities of cdvade on the her2 dataset, we inject the her2 class
labels into the latent embedding space."
1896,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"we investigated deep clustering models for the identification of meaningful
subgroups within medical image datasets."
1897,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"our experimental
findings on the her2 digital pathology dataset surmise that vade and dec are
capable of finding, in an unsupervised fashion, image subgroups related to the
her2 class labels, while cdvade (conditioned on the her2 labels) identifies
visually distinct subgroups that have a weaker association to the her2 labels."
1898,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,"while cdvade can be used as an exploratory tool to unveil unknown subgroups in a
given dataset, developing specialized quantitative evaluation metrics for this
unsupervised task is inherently difficult and will also be a focus in our future
work."
1899,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"this paper constructed a bp dataset with the most commonly used bp
mris in our clinical practice."
1900,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"the major contributions of this study include 1) directed
triangle construction idea for tpp, 2) huge number of tpp matrices as the
heterogeneity representations of bp, 3) tppnet with 15 layers and huge number of
channels, 4) the bp dataset containing mr images and their corresponding roi
masks."
1901,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"the final dataset for radiomic analysis was
constructed by merging the datasets for each sequence type."
1902,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"only patients that
had all three sequences segmented (t2, t1 and post-gadolinium) were included in
the dataset."
1903,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,table 1 shows a breakdown of the final dataset.
1904,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"the whole dataset is divided into three subsets according to the mr
sequence, i.e."
1905,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"since all images in our dataset are 3d images, therefore, the initial channel is
set 325 which is equal to the tpp number."
1906,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"we evaluated our proposed tppnet by comparing it to the recent state-of-the-art
approaches over our bp dataset including vgg16 [28], inceptionnet [29],
mobilenet [30], glcm-cnn [13], vit [31]."
1907,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,"to testify our proposed tppnet, a bp
dataset is constructed with 452 series including three most commonly used mr
sequences in clinical practice, i.e."
1908,CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,"we evaluate our circleformer on the
public monuseg dataset for nuclei detection in whole slide images."
1909,CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,monuseg dataset.
1910,CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,"monuseg dataset is a public dataset from the 2018 multi-organ
nuclei segmentation challenge [6]."
1911,CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,"since the maximum number of objects per image in the dataset is
close to 1000, we set the number of queries to 1000."
1912,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"the resulting datasets can consist of terabytes
of raw videos that require automatic methods for downstream tasks such as
classification, segmentation, and tracking of objects (e.g."
1913,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"note that instead of creating image pairs from consecutive video frames we
can as well choose a custom time step δt ∈ n and sample x 1 ⊂ i t and x 2 ⊂ i
t+δt , which we empirically found to work better for datasets with high frame
rate."
1914,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"to demonstrate the utility of tap for a diverse set of specimen and microscopy
modalities we use the following four different datasets:hela."
1915,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"the dataset consists of four videos with overall 368 frames of
size 1100 × 700."
1916,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"the dataset consists of a single video with 1200
frames of size 1600×1200."
1917,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"the dataset consists of three videos with overall
410 frames of size 3900 × 1900.we use δt = 1.yeast."
1918,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"the dataset
consists of five videos with overall 600 frames of size 1024 × 1024."
1919,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"we use δt ∈
{1, 2, 3}.for each dataset we heuristically choose δt to roughly correspond to
the time scale of observable biological processes (i.e."
1920,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"50% irrespective of the used augmentations, suggesting the absence of
predictive cues in the background for this dataset."
1921,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"3 we show example
attribution maps on top of single raw frames for three different datasets."
1922,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"to that end, we generate a dataset of 97k crops
of size 2 × 96 × 96 from flywing and label them as mitotic/nonmitotic (16k/81k)
based on available tracking data [20]."
1923,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"we use the same dataset as for flywing mitosis
classification, but now densely label post-mitotic cells."
1924,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"interestingly, fine-tuning tap only slightly outperforms the supervised baseline
for this task even for moderate amounts of training data, suggesting that fixed
tap representations generalize better for limited-size datasets.emerging bud
detection on yeast: finally, we test tap on the challenging task of segmenting
emerging buds in phase contrast images of yeast colonies."
1925,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"we train tap networks
on yeast and generate a dataset of 1205 crops of size 5 × 192 × 192 where we
densely label yeast buds in the central frame (defined as buds that appeared
less than 13 frames ago) based on available segmentation data [17]."
1926,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"surprisingly,
training with fixed tap representations performs slightly worse than the
baseline for this dataset (fig."
1927,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"furthermore, we
demonstrate on a variety of datasets that the learned features can substantially
reduce the required amount of annotations for downstream tasks."
1928,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,"although in this
work we focus on 2d+t image sequences, the principle of tap should generalize to
3d+t datasets, for which dense ground truth creation is often prohibitively
expensive and therefore the benefits of modern deep learning are not fully
tapped into."
1929,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"for example, on the bright dataset [2], the accuracy drops more
than 5% compared to the conventional mil approaches."
1930,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"indeed, especially
for weakly supervised wsi classification, where annotated data for downstream
tasks is significantly less compared to natural image datasets, conventional
finetuning schemes can prove to be quite challenging.to address the subpar
performance of ssl-pretrained vision transformers, we utilize the prompt tuning
techniques."
1931,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"this avoids
potential overfitting while still injecting task-specific knowledge into the
learned representations.extensive experiments on three public wsi datasets,
tcga-brca, tcga-crc, and bright demonstrate the superiority of prompt-mil over
conventional mil methods, achieving a relative improvement of 1.49%-4.03% in
accuracy and 0.25%-8.97% in auroc by using only less than 0.3% additional
parameters."
1932,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"we assessed prompt-mil using three histopathological wsi datasets: tcga-brca
[14], tcga-crc [19], and bright [2]."
1933,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"these datasets were utilized for both the
self-supervised feature extractor pretraining and the end-to-end finetuning
(with or without prompts), including the mil component."
1934,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"we pretrained separate vit models on the tcga-crc datasets for 50
epochs, on the bright dataset for 50 epochs, and on the brca dataset for 30
epochs."
1935,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,this training strategy is optimized using the validation datasets.
1936,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"compared to the full fine-tuning method, our method achieved
a relative improvement of 1.29% to 13.61% in accuracy and 3.22% to 27.18% in
auroc on the three datasets."
1937,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"we evaluated the training speed and memory consumption
of our method and compared to the full fine-tuning baseline on four different
sized wsis in the bright dataset."
1938,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"foundational models refer to those trained on large-scale
pathology datasets (e.g."
1939,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,the entire tcga pan-cancer dataset [28]).
1940,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"table 4 shows the accuracy and auroc of our prompt-mil model
with 1, 2 and 3 trainable prompt tokens (k = 1, 2, 3) on the tcga-brca and the
bright datasets."
1941,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"on the tcga-brca dataset, our prompt-mil model with 1 to 3
prompt tokens reported similar performance."
1942,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"on the bright dataset, the
performance of our model dropped with the increased number of prompt tokens."
1943,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,"we applied our proposed method to three
publicly available datasets."
1944,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"however, the transformer framework has a
relatively large number of parameters, which could cause high costs in
fine-tuning the whole model on large datasets."
1945,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"in
the prompt-tuning phase, grouping prompts are added to the input of the backbone
and gtc, while the backbone parameters are frozen.3 experiments and results
consep 1 [10] is a colorectal nuclear dataset with three types, consisting of 41
h&e stained image tiles from 16 colorectal adenocarcinoma whole-slide images
(wsis)."
1946,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"we split them following the official partition [1,10].is a breast cancer
dataset with three types and consists of 120 image tiles from 113 patients."
1947,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"we follow the work [1] to apply the slic [2] algorithm to generate
superpixels as instances and split them into 80/10/30 slides for
training/validation/testing.lizard 3 [9] has 291 histology images of colon
tissue from six datasets, containing nearly half a million labeled nuclei in h&e
stained colon tissue."
1948,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"our method is trained with
pytorch on a 48 gb gpu (nvidia a100) for 12-24 h (depending on the dataset
size)."
1949,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"2
https://github.com/topoxlab/dataset-brca-m2c/."
1950,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"specifically, on the
consep dataset, our approach achieves 1.6% higher f-score on the detection (f d
) and 1.8% higher f-score on the classification (f c ) than the second best
methods mcspatnet [1] and upernet [22]."
1951,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"on brca-m2c dataset, our method has 0.5%
higher f d and 3.9% higher f c , compared with the second best models mcspatnet
[1] and dab-detr [19]."
1952,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"besides, on lizard dataset, our method outperforms
upernet [22] by more than 1.5% and 6.4% on f d and f c , respectively."
1953,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"meanwhile, we conduct t-test on consep dataset for statistical significance
test."
1954,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"the strengths of the grouping transformer based classifier and the grouping
prompts are verified on consep dataset, as shown in table 2.prompt-based
grouping transformer (pgt) is our proposed detection and classification
architecture with grouping prompts and the gtc (in fig."
1955,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,"table 3 shows the effect of different numbers of grouping
prompts on consep dataset."
1956,Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,"the proposed me algorithms were applied to an in vivo rdmri dataset acquired in
our previous work [16]."
1957,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,"acquiring such a dataset requires
modification of the standard imaging protocol and involves additional training
of the mr technicians."
1958,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,"to this effect, we introduce a vision transformer based dl model1
that can synthesize brain 2 mri images that correspond to arbitrary dose levels,
by training on a highly imbalanced dataset with only t1w pre-contrast, t1w 10%
low-dose, and t1w ce standard dose images."
1959,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,"we
first utilize this design paradigm for the dose simulation task and train an
end-to-end model on a highly imbalanced dataset where only t1w pre-contrast, t1w
low-dose, and t1w post-contrast are available.as shown in fig."
1960,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,"dataset: with irb approval and informed consent, we retrospectively used 126
clinical cases (113 training, 13 testing) from a internal private dataset3 using
gadoterate meglumine contrast agent (site a)."
1961,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"finally, computational cost can pose a challenge for
the parcellation of large tractography datasets that can include thousands of
subjects with millions of streamlines per subject.in this work, we propose a
novel point-cloud-based strategy that leverages neighboring and whole-brain
streamline information to learn local-global streamline representations."
1962,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"avoiding
image registration can also reduce computational time and cost when processing
very large tractography datasets with thousands of subjects."
1963,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"we utilized a high-quality and large-scale dataset of 1 million labeled
streamlines for model training and validation."
1964,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"the dataset was obtained from a
wm tractography atlas [42] that was curated and annotated by a neuroanatomist."
1965,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"on average, the 42 anatomical tracts have 2539
streamlines with a standard deviation of 2693 streamlines.for evaluation, we
used a total of 120 subjects from four public datasets and one private dataset."
1966,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"these five datasets were independently acquired with different imaging protocols
across ages and health conditions."
1967,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"the
twotensor unscented kalman filter (ukf) [22,25,27] method, which is consistent
across ages, health conditions, and image acquisitions [42], was utilized to
create whole-brain tractography for all subjects across the datasets mentioned
above."
1968,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"in detail, we applied 30 random transformations
to each subject tractography in the training dataset to obtain 3000 transformed
subjects and 30 million streamlines."
1969,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"many methods are capable of
tractography parcellation after affine registration [12,42]; therefore, with sta
applied to the training dataset, our framework has the potential for
registration-free parcellation.module for local-global streamline representation
learning."
1970,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"training of
our registrationfree framework (tractcloud reg-free ) with the large sta dataset
took about 22 h and 10.9 gb gpu memory with pytorch (v1.13) on an nvidia rtx
a5000 3 experiments and results we evaluated our method on the original labeled
training dataset (registered and
aligned) and its synthetic transform augmented (sta) data (unregistered and
unaligned)."
1971,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"we performed experiments on five independently acquired, unlabeled testing
datasets (dhcp, abcd, hcp, ppmi, btp) to evaluate the robustness and
generalization ability of our tractcloud reg-free framework on unseen and
unregistered data."
1972,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"furthermore, we also provide a visualization of identified tracts in an
example individual subject for every dataset across methods (fig."
1973,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"2).as shown in
table 2, all methods achieve high tirs on all datasets, and the tir metric does
not have significant differences across methods."
1974,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"however, our registration-free
framework (tractcloud reg-free ) obtains significantly lower tda values (better
quality of identified tracts) than all compared methods on abcd, hcp, and ppmi
datasets, where ages of test subjects are from 9 to 75 years old."
1975,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"on the very
challenging dhcp (baby brain) dataset, tractcloud reg-free still significantly
outperforms two sota methods."
1976,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"results of tract identification rate (tir) and tract distance to atlas (tda) on
five independently acquired testing datasets as well as computation time on a
randomly selected subject."
1977,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"the tract spatial overlap (wdice) is over 0.965
on all datasets, except for the challenging dhcp (wdice is 0.932) (table 3)."
1978,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,all methods can successfully identify these tracts across datasets.
1979,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"it
is visually apparent that the tractcloud reg-free framework obtains results with
fewer outlier streamlines, especially on the challenging dhcp dataset."
1980,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"the fast inference speed and robust ability to
parcellate data in original subject space will allow tractcloud to be useful for
analysis of large-scale tractography datasets."
1981,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,"in the challenging btp (tumor patients) dataset, tractcloud reg-free
obtains significantly lower tda values than sota methods and comparable
performance to tractcloud regist ."
1982,Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,none
1983,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"however, their satisfactory performance
depends on the appropriate annotated source dataset."
1984,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"to evaluate the effectiveness of psm, we evaluated our method on
two datasets."
1985,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,dataset.
1986,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"we validated the proposed method on the public dataset of multi-organ
nuclei segmentation (monuseg) [13] and breast tumor cell dataset (bcdata) [11]."
1987,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"bcdata is a public large-scale breast tumor dataset
containing 1338 immunohistochemically ki-67 stained images of size 640 × 640."
1988,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"in monuseg dataset, four fully-supervised methods unet [20], medt
[24], cdnet [8], and the competition winner [13] are adopted to estimate the
upper limit as shown in the first four rows of table 1."
1989,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"unlike
natural image datasets containing diverse samples, the minor inter class
differences in biomedical images may not fully exploit the superiority of
contrastive learning."
1990,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,"our unsupervised method
was evaluated on two publicly available datasets and obtained competitive
results compared to the methods with annotations."
1991,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"considering that there are a large number of unlabeled mris in
existing large-scale datasets [12,13], several deep learning methods propose to
extract brain anatomical features from mri without requiring specific category
labels."
1992,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"while it
is often challenging to annotate mris in practice, there are a large number of
mris (without task-specific category labels) in existing large-scale datasets."
1993,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,"this implies
that the brain anatomical mri features learned by our pretext model on
large-scale datasets would be more discriminative, compared with those used in
the competing methods."
1994,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"the method is implemented as the tool attractive in
mitk diffusion1 , enabling researchers to quickly and intuitively segment tracts
in pathological datasets or other situations not covered by automatic
techniques, simply by annotating a few but informative streamlines."
1995,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"the proposed technique was tested on a healthy-subject dataset and on a dataset
containing tumor cases."
1996,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"we focused on the left optic radiation (or), the left
cortico-spinal tract (cst), and the left arcuate-fasciculus (af), representing a
variety of established tracts.to test the proposed method on pathological data,
we used an in-house dataset containing ten presurgical scans of patients with
brain tumors."
1997,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"additionally, reproducible
simulations on the freely available hcp and the internal tumor dataset were
created (algorithmic evaluation)."
1998,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"the code used for these
experiments is publicly available2 .for the algorithmic evaluation, the initial
training dataset was created with 20 randomly selected streamlines from the
whole-brain tractogram, which have been shown to be a decent number to start
training."
1999,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"since some tracts contain only a fraction of streamlines from the
entire tractogram, it might be unlikely that the training dataset will contain
any streamline belonging to the target tract."
2000,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"therefore, two streamlines of the
specific tract were further added to the training dataset, and class weights
were used to compensate for the class unbalance."
2001,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"in each iteration, the ten streamlines with the
highest entropy are added to the training dataset, which has been determined to
be a good trade-off between annotation effort and prediction improvement."
2002,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"to ensure that the initial
dataset s rand contained streamlines from the target tract, the expert initiated
the active learning workflow by defining a small roi that included fibers of the
tract."
2003,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"to allow comparison between the proposed and traditional
roi-based techniques, the or of subjects from the tumor dataset were segmented
using both approaches by an expert familiar with the respective tool, and the
time required was reported to measure efficiency.note, in all experiments, the
classifier is trained from scratch every iteration, prototypes are generated for
each subject individually, and the classifier predicts on data from the same
subject it is trained with, as it performs subject-individual tract segmentation
and is not used as a fully automated method."
2004,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"to ensure a stable active learning
setup that generalizes across different datasets, the whole method was developed
on the hcp and applied with fixed settings to the tumor data [10]."
2005,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"qualitative results of the algorithmic evaluation of the af of a randomly chosen
subject of the hcp dataset are shown in fig."
2006,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"when accessing qualitative results of the pathological dataset
visual inspection revealed particularly poorly performance of tractseg in cases
where or fibers were in close proximity to tumor tissue, leading to fragmented
segmentations, while complete segmentations were reached with active learning
even for these challenging tracts after a few iterations, as shown in fig."
2007,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"the algorithmic evaluation
yielded consistent results from the fifth to the tenth iterations on both the
hcp and tumor datasets."
2008,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"as expected, outcomes obtained from the tumor dataset
were not quite as good as those of the hcp dataset."
2009,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,"this trend is generally
observed in clinical datasets, which tend to exhibit lower performance levels
compared to high-quality datasets, which could be responsible for the decline in
the results."
2010,B-Cos Aligned Transformers Learn Human-Interpretable Features,"• we
extensively evaluate both models on three public datasets: nct-crc-he-100k [18],
tcga-coad-20x [19], munich-aml-morphology [25]."
2011,B-Cos Aligned Transformers Learn Human-Interpretable Features,"we classify image patches from the public
colorectal cancer dataset nct-crc-he-100k [18]."
2012,B-Cos Aligned Transformers Learn Human-Interpretable Features,this dataset is highly unbalanced and not color normalized compared fig.
2013,B-Cos Aligned Transformers Learn Human-Interpretable Features,to the first dataset.
2014,B-Cos Aligned Transformers Learn Human-Interpretable Features,"additionally, we demonstrate that the
b-cos vision transformer is adaptable to domains beyond histopathology by
training the model on the single white blood cell dataset munich-aml-morphology
[25], which is also highly unbalanced and also publicly available.domain-expert
evaluation: our primary objective is to develop an extension of the vision
transformer that is more transparent and trusted by medical professionals."
2015,B-Cos Aligned Transformers Learn Human-Interpretable Features,"additional details on training,
optimization, and datasets can be found in the appendix."
2016,B-Cos Aligned Transformers Learn Human-Interpretable Features,"a third expert
points out that vit might overfit certain patterns in this dataset, which could
aid the model in improving its performance."
2017,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"we extensively evaluate our method on multiple
brain mri datasets and show that it achieves high visual quality for different
contrasts and views and preserves pathological details, highlighting its
potential clinical usage.related work."
2018,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"however, all current mcsr approaches are limited by their need
for a large training dataset."
2019,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,datasets.
2020,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"we
conduct experiments on two public datasets, brats [16], and msseg [4], and an
in-house clinical ms dataset (cms)."
2021,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"in each dataset, we select 25 patients that
fulfill the isotropic acquisition criteria for both ground truth hr scans."
2022,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"for mcsr from single-subject scans, we achieve
encouraging results across all metrics for all datasets, contrasts, and views."
2023,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"lastly, given their similar physical acquisition
and lesion sensitivity, we note that dir/flair benefit to the same degree in the
cms dataset.qualitative analysis."
2024,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,"figure 2 shows the typical behavior of our
models on cms dataset, where one can qualitatively observe that the split-head
inr pre-serves the lesions and anatomical structures shown in the yellow boxes,
which other models fail to capture."
2025,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"second, resting-state fmri data are not routinely collected for gbm clinical
practices, which restricts the size of annotated datasets such that it is
infeasible to train a reliable prediction model based on deep learning for
survival prediction."
2026,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"similar to data augmentation schemes, we can artificially
boost data volume (i.e., fln maps) up to m times through producing m fln maps
for each patient in the a-lnm, which helps to mitigate the risk of over-fitting
and improve the performance of overall survival time prediction when learning a
deep neural network from a small sized dataset.for this reason, we propose the
name ""augmented lnm (a-lnm)"", compared to the traditional lnm where only one fln
map is generated per patient by averaging all the n fdc maps."
2027,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"to evaluate the predictive
power of the fln maps generated by our a-lnm, we conduct extensive experiments
on 235 gbm patients in the training dataset of brats 2020 [18] to classify the
patients into three overall survival time groups viz."
2028,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"it provided an open-access pre-operative imaging training dataset to
segment brain tumors of glioblastoma (gbm, belonging to high grade glioma) and
low grade glioma (lgg) patients, as well as to predict overall survival time of
gbm patients [18]."
2029,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"this training dataset contained 133 lgg and 236 gbm patients,
and each patient had four mri modalities, including t1, post-contrast
t1-weighted, t2-weighted, and t2 fluid attenuated inversion recovery."
2030,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"in this paper, we propose to investigate the feasibility of the novel
neuroimaging features, i.e., fln maps, for overall survival time prediction of
gbm patients in the training dataset of the brats 2020, in which one patient
alive was excluded, and the remaining 235 patients consisted of 89 short-term
survivors (less than 10 months), 59 mid-term survivors (between 10 and 15
months), and 87 long-term survivors (more than 15 months)."
2031,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"we evaluated
the classification performance of our proposed method using 235 gbm patients in
the brats 2020 training dataset, because only these 235 patients had both
overall survival time and manual expert segmentation labels of lesions."
2032,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,"experimental results on the brats 2020 training dataset validated the
effectiveness of the a-lnm derived fln maps for gbm survival prediction."
2033,A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,"training on 18-videos (367-images), the model achieved
statistically significant results (p < 0.001) on the hold-out testing dataset of
5-videos (182-images) when compared to a location prior baseline model [12]."
2034,A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,"images from a singular video were present in either the training or validation
dataset.each model was run for with a batch size of 5 for 20 epochs, where the
epoch with the best primary evaluation metric on the validation dataset was
kept."
2035,A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,"this model, resnet18 with mse loss, outperforms the
more sophisticated models, as these models over-learn image features in the
training dataset."
2036,Intraoperative CT Augmentation for Needle-Based Liver Interventions,none
2037,Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,none
2038,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"(3) we established a comprehensive pelvic
fracture ct dataset and provided ground-truth annotations."
2039,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"our dataset and
source code are publicly available at https://github.com/yzzliu/fracsegnet."
2040,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"with
a cascaded 3d nn-unet architecture, the network is pre-trained on a set of
healthy pelvic ct images [5,13] and further refined on our fractured dataset."
2041,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"although large-scale datasets on pelvic segmentation have been studied in some
research [13], to the best of our knowledge, currently there is no
well-annotated fractured pelvic dataset publicly available."
2042,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"therefore, we
curated a dataset of 100 preoperative ct scans covering all common types of
pelvic fractures."
2043,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,"we evaluated our method on 100 pelvic fracture ct scans and
made our dataset and ground truth publicly available."
2044,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,"we employed the publicly available easy-resect (retrospective evaluation of
cerebral tumors) dataset [10] (https://archive.sigma2.no/pages/ public/dataset
detail.jsf?id=10.11582/2020.00025) to train and evaluate our proposed method."
2045,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,"this dataset is a deep-learning-ready version of the original resect database,
and was released as part of the 2020 learn2reg challenge [24]."
2046,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,"to train our dl model, we made subject-wise division of the entire dataset into
70%:15%:15% as the training, validation, and testing sets, respectively."
2047,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,"table 1 lists the mean and standard deviation of landmark identification errors
(in mm) between the predicted position and the ground truth in intra-operative
us for each patient of the resect dataset."
2048,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,"in the resect dataset, eligible anatomical landmarks were defined
as deep grooves and corners of sulci, convex points of gyri, and vanishing
points of sulci."
2049,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,"in order to assemble a large and diverse focal-time scan
dataset, we choose to simulate focal-time scans using existing in-focus video
data."
2050,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,"we use this
technique to create a training and testing dataset consisting of 1000 and 200
simulated focal-time scans based on 200 10-second video clips sampled from
cholec80 [13], a popular endoscopic dataset."
2051,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,"these act as a validation dataset to help prevent over fitting and aid
generalisation."
2052,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,"as a testing dataset similar to our
intended use case, we chose to approximate a real focal-time scan by controlling
conditions during capture of the individual focal stacks."
2053,Surgical Video Captioning with Mutual-Modal Concept Alignment,"extensive experiments
are performed on neurosurgery video and nephrectomy image datasets, and
demonstrate the effectiveness of our sca-net by remarkably outperforming the
state-of-the-art captioning works."
2054,Surgical Video Captioning with Mutual-Modal Concept Alignment,neurosurgery video captioning dataset.
2055,Surgical Video Captioning with Mutual-Modal Concept Alignment,"to evaluate the effectiveness of surgical
video captioning, we collect a large-scale dataset with 41 surgical videos of
endonasal skull base neurosurgery."
2056,Surgical Video Captioning with Mutual-Modal Concept Alignment,"we split these video
clips at patientlevel, where the video clips of 31 patients are used for
training and the rest of 10 patients are utilized for test.endovis image
captioning dataset."
2057,Surgical Video Captioning with Mutual-Modal Concept Alignment,"we further compare our method with state-of-the-arts on the
public endovis-2018 image captioning dataset [1,23]."
2058,Surgical Video Captioning with Mutual-Modal Concept Alignment,"this dataset reveals
robotic nephrectomy procedures acquired by the da vinci x or xi system, and is
annotated with surgical actions between 9 possible tools and surgical targets
[23]."
2059,Surgical Video Captioning with Mutual-Modal Concept Alignment,"in this way, these two datasets can comprehensively evaluate
the captioning tasks under both surgical videos and images.implementation
details."
2060,Surgical Video Captioning with Mutual-Modal Concept Alignment,"we optimize the sca-net and compared captioning
methods using adam with the batch size of 12 for both captioning datasets."
2061,Surgical Video Captioning with Mutual-Modal Concept Alignment,"all
models are trained for 20 and 50 epochs in neurosurgery and endovis datasets,
respectively."
2062,Surgical Video Captioning with Mutual-Modal Concept Alignment,"to further confirm the effectiveness of surgical captioning, we perform the
comparison on the public endovis image captioning dataset."
2063,Surgical Video Captioning with Mutual-Modal Concept Alignment,"moreover, we
propose the mc-align to mutually coordinate visual and text representations with
surgical concept representations of the other modality for multi-modal decoding,
thereby generating more accurate captions with aligned multi-modal
knowledge.extensive experiments on neurosurgery and nephrectomy datasets confirm
the advantage of our sca-net over state-of-the-arts on the surgical captioning."
2064,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"for
experimental evaluation, we collected a surgical video dataset of esd
procedures, and preprocessed 1032 short clips with dissection trajectories
labelled."
2065,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,", i t }, i t ∈ r h×w ×3 and the output is an action distribution of a
sequence of 2d coordinates a = {y t+1 , y t+2 , ..., y t+n }, y t ∈ r 2
indicating the future dissection trajectory projected to the image space.in
order to obtain the demonstrated dissection trajectories from the expert video
data, we first manually annotate the dissection trajectories on the video frame
according to the moving trend of the instruments observed from future frames,
then create a dataset d = {(s, a) i } m i=0 containing m pairs of video clip
(state) and dissection trajectory (action).to precisely predict the expert
dissection behaviors and effectively learn generalizable features from the
expert demonstrations, we use the implicit model as our imitation policy."
2066,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,dataset.
2067,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"we evaluated the proposed approach on a dataset assembled from 22
videos of esd surgery cases, which are collected from the endoscopy centre of
the prince of wales hospital in hong kong."
2068,Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,"experimental results show that our method outperforms
state-of-the-art approaches on the evaluation dataset, demonstrating the
effectiveness of our approach for learning dissection skills in various surgical
scenarios."
2069,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"establishing point cloud correspondences using machine learning has
been demonstrated on liver and prostate datasets [8,9]."
2070,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"sensitivity to
regularized kelvinlet function hyperparameters is explored on a supine mr breast
imaging dataset."
2071,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"for the breast imaging datasets in this work, we
used simulated intraoperative data features that realistically could be
collected in a surgical environment visualized in fig."
2072,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"the first explores sensitivity
to regularized kelvinlet function hyperparameters k grab , k twist , ε grab ,
and ε twist and establishes optimal hyperparameters in a training dataset of 11
breast deformations."
2073,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"this dataset consists of supine breast mr images simulating surgical
deformations of 11 breasts from 7 healthy volunteers."
2074,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,"three
hyperparameter sweeps were used: this dataset consists of supine breast mr
images simulating surgical
deformations from one breast cancer patient."
2075,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"we choose to use a neural image analogy method that combines
the texture of a source image with a high-level content representation of a
target image without the need for a large dataset [1]."
2076,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,dataset.
2077,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"we tested our method retrospectively on 6 clinical datasets from 6
patients (cases) (see fig."
2078,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"we generated 100 poses for each 3d mesh (i.e.: each
case) and used a total of 15 unique textures from human brain surfaces
(different from our 6 clinical datasets) for synthesis using s θ ."
2079,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,"because a conventional train/validation/test split would lead to texture
contamination, we created our validation dataset so that at least one texture is
excluded from the training set."
2080,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"we present results on a clinical
dataset comprising fifty post-operative glioblastoma (gbm) patients."
2081,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"we divide the descriptions of the two separate datasets used for the dose
prediction and segmentation models."
2082,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"the dose prediction model was trained on an in-house dataset comprising a total
of 50 subjects diagnosed with post-operative gbm."
2083,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"we
divided the dataset into training (35 cases), validation (5 cases), and testing
(10 cases)."
2084,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"we refer the reader to [9] for further details.segmentation models:
to develop and test the proposed approach, we employed a separate in-house
dataset (i.e., different cases than those used to train the dose predictor
model) of 50 cases from post-operative gmb patients receiving standard rt
treatment."
2085,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"we divided the dataset into training (35 cases), validation (5
cases), and testing (10 cases)."
2086,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"3 the tumor presents a non-convex
shape alongside the skull's parietal lobe, which was not adequately modeled by
the training dataset used to train the segmentation models."
2087,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,"these first results on a dataset of post-operative gbm
patients show the ability of the proposed doselo to deliver improved
dosimetric-compliant segmentation results."
2088,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,none
2089,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"for methodological development and assessment, we used the resect
(retro-spective evaluation of cerebral tumors) dataset [16], which has
pre-operative mri, and ius scans at different surgical stages from 23 subjects
who underwent low-grade glioma resection surgeries."
2090,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"to create the silver registration ground truths, we
used the homologous landmarks between mri and ius in the resect dataset to
perform landmark-based 3d b-spline nonlinear registration to register ius to the
corresponding mri for all 22 cases."
2091,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"the two dl models were trained with the same dataset and
procedure, and their prediction accuracies, measured as the absolute error
between the predicted and ground truths mis-registration on the test set were
compared with two-sided pairedsamples t-tests to confirm the superiority of the
proposed method, in addition to correlations between their estimated and ground
truth errors."
2092,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,"one limitation of our work
lies in the limited patient data, as public ius datasets are scarce, while the
settings and properties of us scanners can vary, potentially affecting the dl
model designs."
2093,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"therefore, we can establish a dataset with an image pair (rgb image and laser
image) that shares the same intersection point ground truth with the laser image
(see fig."
2094,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"to validate our proposed solution for the newly formulated problem, we acquired
and publicly released two new datasets."
2095,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,2 shows a sample from our dataset.
2096,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"we acquired the dataset on a silicone
tissue phantom which was 30 × 21 × 8 cm and was rendered with tissue color
manually by hand to be visually realistic."
2097,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"therefore, our first newly
acquired dataset, named jerry, contains 1200 sets of images."
2098,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"since it is
important to report errors in 3d and in millimeters, we recorded another dataset
similar to jerry but also including ground truth depth map for all frames by
using structured-lighting system [8]-namely the coffbee dataset.these datasets
have multiple uses such as:-intersection point detection: detecting intersection
points is an important problem that can bring accurate surgical cancer
visualization."
2099,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"we partitioned the jerry dataset into three subsets, the
training, validation, and test set, consisting of 800, 200, and 200 images,
respectively, and the same for the coffbee dataset."
2100,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"quantitative results on the released datasets are shown in table 1 and table 2
with different backbones for extracting image features, resnet and vit."
2101,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"for the
2d error on two datasets, among the different settings, the combination of
resnet and mlp gave the best performance with a mean error of 70.5 pixels and a
standard deviation of 56.8."
2102,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"both the hardware and software
design of the proposed solution were illustrated and two newly acquired datasets
were publicly released."
2103,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,"we believe that our problem
reformulation and dataset release, together with the initial experimental
results, will establish a new benchmark for the surgical vision community."
2104,A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,"vision transformers require large
datasets for training, so we initialize the encoder with weights pretrained on
imagenet and further train it on our in-house database."
2105,Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,"additionally, four public datasets including
kvasir, etis-laribpolypdb, cvc-colondb, and cvc-clinicdb were also used to
evaluate our network model."
2106,Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,"furthermore, we also summarizes the average three metrics computed from
all the five databases (the in-house dataset and four public datasets)."
2107,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"we evaluated our method on a dataset of 66
consecutive adult patients with brain gliomas who were surgically treated at the
brigham and women's hospital, boston usa, where both pre-operative 3d t2-space
and pre-dural opening intraoperative us (ius) reconstructed from a tracked
handheld 2d probe were acquired."
2108,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"the dataset was randomly split into a
training set (n = 56) and a testing set (n = 10)."
2109,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,"for example, synthetic ultrasound images
could be generated from the brats dataset [1], the largest collection of
annotated brain tumor mr scans."
2110,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,dataset and preprocessing.
2111,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,"our clinical dataset consists of 108 patients for
whom were acquired both a pre-operative h&n ct scan and 4 to 11 wsis after
laryngectomy (with a total amount of 849 wsis)."
2112,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,"we split the dataset patient-wise into three
groups for training (64), validation (20), and testing (24)."
2113,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,"to demonstrate the
performance of our model on another application, we also retrieved the datasets
from [14] for pelvis 3d ct/2d mr."
2114,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,"according to the mr/ct application in rt, we compared our model against
the state-of-the-art results of msv-regsynnet which were computed on the same
dataset."
2115,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,"we also compared against
msv-regsynnet on its own validation dataset for generalization assessment: we
yielded comparable results for the first cohort and significantly better ones
for the second, which proves that structuregnet behaves well on other modalities
and that the structure awareness is an essential asset for better registration,
as pelvis is a location where organs are moving."
2116,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,"we trained our model with a large dataset of 3500 cts of
patients with head-and-neck cancer, more exactly 2297 patients from the publicly
available the cancer imaging archive (tcia) [1,6,16,17,28,32] and 1203 from
private internal data, after obtention of ethical approbations."
2117,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,"for
reference, compared to 3dstylegan [10], our model achieved half their fid score
on another brain mri dataset, with comparable ms-ssim."
2118,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,"this approach may provide coarse
reconstructions for patients with rare abnormalities, as most learning methods,
but a larger dataset or developing a prior including tissue abnormalities could
improve robustness."
2119,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,the dataset comprises brain mr and ct volumes from 262 subjects.
2120,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,"the dataset is divided into
249, 1 and 12 subjects for training, validating and testing set."
2121,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,"unlike adult datasets [4,14], pediatric datasets are
easily misaligned due to children's rapid growth between scans."
2122,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,"experimental results on a clinical dataset show that maskgan
significantly outperforms existing methods and produces synthetic ct with more
consistent mappings of anatomical structures."
2123,FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,"we conduct experiments on the dataset of ""the 2016 nih-aapm mayo clinic low dose
ct grand challenge"" [8], which contains 5,936 ct slices in 1 mm image thickness
from 10 anonymous patients, where a total of 5,410 slices from 9 patients,
resized to 256 × 256 resolution, are randomly selected for training and the 526
slices from the remaining one patient for testing without patient overlap."
2124,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"the non-correspondence
detection approach, which typically relies on a sophisticated designed loss
function, is very sensitive to the dataset [1] and difficult to find a set of
unified parameters.therefore, to effectively address the non-correspondence
problem in registering pathology images, it is necessary to incorporate both a
data-independent segmentation module and a modality-adaptive inpainting module
into the registration pipeline."
2125,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"our experimental design focuses on two common clinical tasks: atlas-based
registration, which involves warping pathology images to a standard atlas
template, and longitudinal registration, which involves registering
pre-operative images to post-operative images for the purpose of tracking
changes over time.dataset and pre-processing."
2126,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"to create
such a mapping, we created a pseudo dataset by utilizing images from the oasis-1
and brats2020."
2127,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"from the resulting t1 sequences, a pseudo dataset of 300 images
was randomly selected for further analysis."
2128,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"appendix b provides a detailed
process for creating the pseudo dataset.real data with landmarks."
2129,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,"after creating the pseudo dataset, we warped brain mr images without tumors to
the atlas and used the resulting deformation field as the gold standard for
evaluation."
2130,Fast Reconstruction for Deep Learning PET Head Motion Correction,"multi-subject studies were conducted on a dataset of 20 subject and its results
were quantitatively and qualitatively evaluated by molar reconstruction studies
and corresponding brain region of interest (roi) standard uptake values (suv)
evaluation."
2131,Fast Reconstruction for Deep Learning PET Head Motion Correction,"we split our dataset of 20 subjects into distinct
subsets for training and testing with 14 and 6 subjects, respectively."
2132,Fast Reconstruction for Deep Learning PET Head Motion Correction,"we plan to apply the proposed method to other datasets, developing a
generalized model for multi-tracer and multi-scanner pet data."
2133,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,"experiments
on four publicly collected pet/ct datasets demonstrate that our aseg outperforms
existing methods by preserving better anatomical structures in generated pseudo
ct images and achieving better visual similarity in corrected pet images."
2134,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,"the data used in our experiments are collected from the cancer image archive
(tcia) [4] (https://www.cancerimagingarchive.net/collections/), where a series
of public datasets with different types of lesions, patients, and scanners are
open-access."
2135,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,"we use
these samples in hnscc for training and in other three datasets for
evaluation.each sample contains co-registered (acquired with pet-ct scans) ct,
pet, and nac-pet whole-body scans."
2136,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,"because we cannot
access the original scatters [5], inspired by [11], we propose to resort cgan to
simulate the ac process, denoted as acgan and trained on hnscc dataset."
2137,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,"four metrics, including the peak signal to noise ratio
(psnr), mean absolute error (mae), normalized cross correlation (ncc), and ssim,
are used to measure acgan with pseudo ct images on test datasets (nsclc,
tcga-hnsc, and tcga-luda)."
2138,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,"experiments on a collection of public datasets demonstrate that
our aseg outperforms existing methods by achieving advanced performance in
anatomical consistency."
2139,An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,none
2140,Geometric Ultrasound Localization Microscopy,"dataset: we demonstrate the feasibility of our geometric ulm and present
benchmark comparison outcomes based on the pala dataset [11]."
2141,Geometric Ultrasound Localization Microscopy,"this dataset is
chosen as it is publicly available, allowing easy access and reproducibility of
our results."
2142,Geometric Ultrasound Localization Microscopy,"to date, it is the only public ulm dataset featuring radio
frequency (rf) data as required by our method."
2143,Geometric Ultrasound Localization Microscopy,"we
obtain the results for classical image processing approaches directly from the
open-source code provided by the authors of the pala dataset [11]."
2144,Geometric Ultrasound Localization Microscopy,"since the u-net-based
localization is a supervised learning approach, we split the pala dataset into
sequences 1-15 for testing and 16-20 for training and validation, with a split
ratio of 0.9, providing a sufficient number of 4500 training frames.results:
table 1 provides the benchmark comparison results with state-of-theart methods."
2145,Geometric Ultrasound Localization Microscopy,"we employed an energy-based model for feature
extraction in conjunction with ellipse intersections and clustering to pinpoint
contrast agent positions from rf data available in the pala dataset."
2146,Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,none
2147,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"note that the network will be trained
only once, on a fixed dataset that is fully independent of the datasets that
will be used in the evaluation (see sect."
2148,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4).dataset.
2149,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"our neural network is
trained using patches from the ""gold atlas -male pelvis -gentle radiotherapy""
[14] dataset, which is comprised of 18 patients each with a ct, mr t1, and mr t2
volumes."
2150,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"we would like to report that, initially, we also made use of a
proprietary dataset including us volumes."
2151,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"consequently, for
the purpose of ensuring reproducibility, all evaluations presented in this paper
exclusively pertain to the model trained solely on the public mr-ct
dataset.patch sampling from unregistered datasets."
2152,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"in this experiment, we evaluate the performance of different methods for
estimating affine registration of the retrospective evaluation of cerebral
tumors (resect) miccai challenge dataset [23]."
2153,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"this dataset consists of 22 pairs
of pre-operative brain mrs and intra-operative ultrasound volumes."
2154,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"the dataset comprises 8 sets of mr and ct volumes, both depicting the
abdominal region of a single patient and exhibiting notable deformations."
2155,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,"as the most challenging experiment, we finally use our method to achieve
deformable registration of abdominal 3d freehand us to a ct or mr volume.we are
using a heterogeneous dataset of 27 cases, comprising liver cancer patients and
healthy volunteers, different ultrasound machines, as well as optical vs."
2156,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"to illustrate the efficiency of our proposed approach, we conduct
rigorous experiments on several real clinical datasets; the experimental results
reveal the advantages of our approach over several state-of-the-art ct
reconstruction methods."
2157,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,datasets.
2158,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"first, our proposed approaches are evaluated on the ""mayo-clinic
low-dose ct grand challenge"" (mayo-clinic) dataset of lung ct images [19].the
dataset contains 2250 two dimensional slices from 9 patients for training, and
the remaining 128 slices from 1 patient are reserved for testing."
2159,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"to evaluate the
generalization of our model, we also consider another dataset rider with
nonsmall cell lung cancer under two ct scans [36] for testing."
2160,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"we randomly
select 4 patients with 1827 slices from the dataset."
2161,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"to evaluate the stability and generalization of our model and the
baselines trained on mayo-clinic dataset, we also test them on the rider
dataset."
2162,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"due to the bias in the datasets
collected from different facilities, the performances of all the models are
declined to some extents."
2163,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,"the experimental results on real datasets demonstrate the
advantages of our proposed network over several popular approaches."
2164,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"this was not the case for noise2aliasing, and
historical clinical data sufficed for training.we validated our method on
publicly available data [15] against a supervised approach [6] and applied it to
an internal clinical dataset of 30 lung cancer patients."
2165,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"we explore different
dataset sizes to understand their effects on the reconstructed images."
2166,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"let j 1 , j 2 be two random variables
that pick different subsets at random belonging to a partition of j , andbe the
input-target pairs in dataset d of reconstructions using disjoint subsets of
noisy projections."
2167,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"this means that, in our dataset,
we should have at our disposal reconstructions of the same underlying volume x
using disjoint subsets of projections."
2168,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"first, we used the spare varian dataset to study whether noise2aliasing can
match the performance of the supervised baseline and if it can outperform it
when adding noise to the projections."
2169,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"then, we use the internal dataset to
explore the requirements for the method to be applied to an existing clinical
dataset."
2170,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"given two volumes (x, y), the
training pairs (x i (k) , y i (k) ) are the same i-th slice along the k-th
dimension of each volume chosen to be the axial plane.the datasets used in this
study are two:1."
2171,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"the spare varian dataset was used to provide performance
results on publicly available patient data."
2172,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"to more closely resemble normal
respiratory motion per projection image, the 8 min scan has been used from each
patient (five such scans are available in the dataset)."
2173,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"the hyperparameters are
optimized over the training dataset.2."
2174,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"an internal dataset (irb approved) of 30
lung cancer patients' 4dcbcts from 2020 to 2022, originally used for igrt, with
25 patients for training and 5 patients for testing."
2175,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"the
data were anonymized prior to analysis.projection noise was added using the
poisson distribution to the spare varian dataset to evaluate the ability of the
unsupervised method to reduce it."
2176,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"for the spare varian dataset, we use the rois defined provided
[15] and used the 3d reconstruction using all the projections available as a
ground truth."
2177,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"for the internal dataset, we deformed the planning ct to each of
the phases reconstructed using the fdk algorithm and evaluate the metric over
only the 4dcbct volume boundaries."
2178,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"1,
noise2aliasing matches the visual quality of the supervised approach on the
low-noise dataset on both soft tissue and bones."
2179,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"1 and table 1, the supervised approach reproduces the noise
that was seen during training, while noise2aliasing manages to remove it
consistently, outperforming the supervised approach, especially in the soft
tissue area around the lungs, where the noise affects attenuation coefficients
the most.noise2aliasing is capable of reducing the artifacts present in
reconstructions caused by stochastic noise in the projections used,
outperforming the supervised baseline.internal dataset."
2180,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"reconstruction using
noise2aliasing with different-sized datasets."
2181,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"however, the model also tends to remove small
anatomical structures as high-frequency objects that cannot be distinguished
from the noise.when applied to a clinical dataset, noise2aliasing benefits from
more patients being included in the dataset, however, qualitatively good
performance is already achieved with 5 patients."
2182,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"we have empirically demonstrated its performance on a publicly available
dataset and on an internal clinical dataset."
2183,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"noise2aliasing can be trained on
existing historical datasets and does not require changing current clinical
practices."
2184,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,"the method removes noise more reliably when the dataset size is
increased, however further analysis is required to establish a good quantitative
measurement of this phenomenon."
2185,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,none
2186,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,datasets.
2187,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,"two widely-used public ct image datasets, 3d-ircadb [5] and pancreas
[5] we augment the data by rotation and flipping first and then randomly crop
them to 128 × 128 patches."
2188,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,"we compare the performance of our proposed method with other state-of-theart
methods, including bicubic interpolation [16], dan [11], realsr [15], spsr [21],
aid-srgan [12] and jdnsr [10].figure 2 shows the qualitative comparison results
on the 3d-ircadb dataset with the scale factor of 2."
2189,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,"figure 3 shows the
qualitative comparison results on the pancreas dataset with the scale factor of
4."
2190,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,"therefore, our method reconstructs more detailed results than other
methods.table 2 shows the quantitative comparison results of different
state-of-theart methods with two scale factors on two datasets."
2191,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,"for the
3d-ircadb and pancreas datasets, our method outperforms the second-best methods
1.6896/0.0157 and 1.7325/0.0187 on psnr/ssim with the scale factor of 2
respectively."
2192,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,"the experiments compared with 6
state-ofthe-art methods on 2 public datasets demonstrate the superiority of our
method."
2193,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"extensive experiments with seven public datasets show that our
nice-trans outperforms state-of-the-art registration methods on both
registration accuracy and runtime."
2194,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"we followed the dataset settings
in [18]: 2,656 brain mri images acquired from four public datasets (adni [27],
abide [28], adhd [29], and ixi [30]) were used for training; two public brain
mri datasets with anatomical segmentation (mindboggle [31] and buckner [32])
were used for validation and testing."
2195,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"the mindboggle dataset contains 100 mri
images and were randomly split into 50/50 images for validation/testing."
2196,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"the
buckner dataset contains 40 mri images and were used for testing only."
2197,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"in
addition to the original settings of [18], we adopted an additional public brain
mri dataset (lpba [33]) for testing, which contains 40 mri images.we performed
brain extraction and intensity normalization for each mri image with freesurfer
[32]."
2198,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,"bold: the best dsc and njd in each testing dataset and the shortest runtime of
completing both affine and deformable registration."
2199,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,bold: the best dsc and njd in each testing dataset.
2200,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,none
2201,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,datasets and baselines.
2202,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,"we evaluated cola-diff on two multi-contrast brain mri
datasets: brats 2018 and ixi datasets."
2203,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,"the ixi1 dataset consists of 200 multi-contrast mris from healthy
brains, plit them into (140:25:35) for training/validation/testing."
2204,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,"seven cases were tested
in two datasets (table 1)."
2205,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,"our results show that each component contributes to the
performance improvement, with auto-weight adaptation bringing a psnr increase of
1.9450db and ssim of 4.0808%.to test the generalizability of cola-diff under the
condition of varied inputs, we performed the task of generating t2 on two
datasets with progressively increasing input modalities (table 2 bottom)."
2206,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,"while the ldm model was
trained on uk biobank, we demonstrate our methods on an external dataset (ixi)
which was inaccessible to the pre-trained generative model."
2207,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,"for low sparsity mri sr, we directly find the optimal latent code z * t using
the decoder d: dataset for validation: we use 100 hr t1 mris from the ixi
dataset
(http://brain-development.org/ixi-dataset/) to validate our method, after
filtering out those scans where registration failed."
2208,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,"we note that subjects in
the ixi dataset are around 10 years younger on average than those in uk
biobank.the mri scans from uk biobank also had the faces masked out, while the
scans from ixi did not."
2209,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,"qualitative results of applying our method on tumour and lesion filling are
available in the supplementary material.table 1 shows quantitative results on
100 hr t1 scans from the ixi dataset, which the brain ldm did not have access to
during training."
2210,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,"we validated our method on 100 brain t1w mris from the
ixi dataset through slice imputation using input scans of 4 and 8 mm slice
thickness, and compared our method with cubic interpolation and unires
[3].experimental results have shown that our approach achieves superior
performance compared to the unsupervised baselines, and could create smooth hr
images with fine detail even on an external dataset (ixi)."
2211,Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,"furthermore, we constructed a new
ultra-high resolution ct scan dataset obtained with the most advanced ct
machines."
2212,Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,"the dataset contained 87 uhrct scans with a spatial resolution of
0.34×0.34 mm 2 and an image size of 1024×1024."
2213,Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,"to avoid non-derivative operations in image enhancement, we proposed a
novel enhancement module consisting of lightweight convolutional layers to
replace the filtering operation for faster and easier back-propagation in
structural domain optimization.3) we established an ultra-high-resolution ct
scan dataset with a spatial resolution of 0.34 × 0.34 mm 2 and an image size of
1024 × 1024 for training and testing the sr task."
2214,Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,"the loss function is then modified
asthe total objective function is the sum of two losses.3 experiments and
conclusion we constructed three datasets for framework training and evaluation;
two of them
were in-house data collected from two ct scanners(the ethics number is
20220359), and the other was the public luna16 dataset [22]."
2215,Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,"more details about
the two in-house datasets are described in the supplementary materials.we
evaluated our sr model on three ct datasets:• dataset 1: 2d super-resolution
from 256×256 to 1024×1024, with the spatial resolution from 1.36 × 1.36 mm 2 to
0.34 × 0.34 mm 2 .• dataset 2: 3d super-resolution from 256 × 256 × 1x to 512 ×
512 × 5x, with the spatial resolution from 1.60 × 1.60 × 5.00 mm 3 to 0.80 ×
0.80 × 1.00 mm 3 ."
2216,Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,"• dataset 3: 2d super-resolution from 256 × 256 to 512 × 512
on the luna16 dataset.we compare our model with other sota super-resolution
methods, including bicubic interpolation, srcnn [7], srresnet [6], cycle-gan
[2], and sr3 [12]."
2217,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"by contrast, we can handle much
longer ones and provide a broad evaluation in a real dataset (c3vd) over
multiple sequences."
2218,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"hence, we can write the color of the corresponding pixel as
[3]:where l e is the radiance emitted by the light source to the surface point,
that was modeled and calibrated in the endomapper dataset [1] according to the
sls model from [16]."
2219,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"thus, our
photometric loss is computed using a normalized image:(5) we validate our method
on the c3vd dataset [4], which covers all different
sections of the colon anatomy in 22 video sequences."
2220,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"this dataset contains
sequences recorded with a medical video colonoscope, olympus evis exera iii
cf-hq190l."
2221,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"the gain values were easily estimated
from the dataset itself."
2222,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"for vignetting, we use the calibration obtained from a
colonoscope of the same brand and series from the endomapper dataset [1].during
training, we follow the neus paper approach of using a few informative frames
per scene, as separated as possible, by sampling each video uniformly."
2223,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,"since the c3vd dataset comprises a ground-truth
triangle mesh, we compute point-to-triangle distances from all the vertices in
the reconstruction to the closest ground-truth triangle.in the first rows of
table 1, we report median (medae), mean (mae), and root mean square (rmse)
values of these distances for all vertices seen in at least one image."
