{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xml.etree import ElementTree as et\n",
    "from lxml import etree\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text(text, width=100):\n",
    "    \"\"\"\n",
    "    A simple function to wrap text at a given width.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return text  # Handle NaN values\n",
    "    \n",
    "    wrapped_lines = []\n",
    "    for paragraph in text.split('\\n'):  # Splitting by existing newlines to preserve paragraph breaks\n",
    "        line = ''\n",
    "        for word in paragraph.split():\n",
    "            if len(line) + len(word) + 1 > width:\n",
    "                wrapped_lines.append(line)\n",
    "                line = word\n",
    "            else:\n",
    "                line += (' ' + word if line else word)\n",
    "        wrapped_lines.append(line)\n",
    "    return '\\n'.join(wrapped_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from grobid_client.grobid_client import GrobidClient\n",
    "\n",
    "\n",
    "#client = GrobidClient(grobid_server='https://kermitt2-grobid.hf.space/')\n",
    "#client = GrobidClient(grobid_server='http://localhost:8081')\n",
    "client = GrobidClient(grobid_server='http://localhost:8070')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol01\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2'\n",
    "#client.process('processFulltextDocument', process_file, output = \"./vol02\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol03\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol04\", force=True)\n",
    "\n",
    "process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5'\n",
    "client.process('processFulltextDocument', process_file, output=\"./vol05\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol06\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol07\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol08\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol09\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol10\", force=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol01'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol02'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol03'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol04'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol05'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol06'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol07'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol08'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol09'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming XML files by folder path\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed 'paper_63.grobid.tei.xml' to 'Unsupervised_Classification_of_Congenital_Inner_Ear_Malformations_Using_DeepDiffusion_for_Latent_Space_Representation.xml'\n",
      "Renamed 'paper_76.grobid.tei.xml' to 'Convolving_Directed_Graph_Edges_via_Hodge_Laplacian_for_Brain_Network_Analysis.xml'\n",
      "Renamed 'paper_12.grobid.tei.xml' to 'Learning_with_Synthesized_Data_for_Generalizable_Lesion_Detection_in_Real_PET_Images.xml'\n",
      "Renamed 'paper_71.grobid.tei.xml' to 'Flexible_Unfolding_of_Circular_Structures_for_Rendering_Textbook-Style_Cerebrovascular_Maps.xml'\n",
      "Renamed 'paper_64.grobid.tei.xml' to 'How_Does_Pruning_Impact_Long-Tailed_Multi-label_Medical_Image_Classifiers?.xml'\n",
      "Renamed 'paper_15.grobid.tei.xml' to 'Cluster-Induced_Mask_Transformers_for_Effective_Opportunistic_Gastric_Cancer_Screening_on_Non-contrast_CT_Scans.xml'\n",
      "Renamed 'paper_65.grobid.tei.xml' to 'Multimodal_Deep_Fusion_in_Hyperbolic_Space_for_Mild_Cognitive_Impairment_Study.xml'\n",
      "Renamed 'paper_70.grobid.tei.xml' to 'Open-Ended_Medical_Visual_Question_Answering_Through_Prefix_Tuning_of_Language_Models.xml'\n",
      "Renamed 'paper_14.grobid.tei.xml' to 'Multi-view_Vertebra_Localization_and_Identification_from_CT_Images.xml'\n",
      "Renamed 'paper_62.grobid.tei.xml' to 'MUVF-YOLOX:_A_Multi-modal_Ultrasound_Video_Fusion_Network_for_Renal_Tumor_Diagnosis.xml'\n",
      "Renamed 'paper_13.grobid.tei.xml' to 'Robust_Exclusive_Adaptive_Sparse_Feature_Selection_for_Biomarker_Discovery_and_Early_Diagnosis_of_Neuropsychiatric_Systemic_Lupus_Erythematosus.xml'\n",
      "Renamed 'paper_11.grobid.tei.xml' to 'Graph-Theoretic_Automatic_Lesion_Tracking_and_Detection_of_Patterns_of_Lesion_Changes_in_Longitudinal_CT_Studies.xml'\n",
      "Renamed 'paper_75.grobid.tei.xml' to 'M&M:_Tackling_False_Positives_in_Mammography_with_a_Multi-view_and_Multi-instance_Learning_Sparse_Detector.xml'\n",
      "Renamed 'paper_60.grobid.tei.xml' to 'Identification_of_Disease-Sensitive_Brain_Imaging_Phenotypes_and_Genetic_Factors_Using_GWAS_Summary_Statistics.xml'\n",
      "Renamed 'paper_68.grobid.tei.xml' to 'Transformer-Based_Tooth_Segmentation,_Identification_and_Pulp_Calcification_Recognition_in_CBCT.xml'\n",
      "Renamed 'paper_19.grobid.tei.xml' to 'Utilizing_Longitudinal_Chest_X-Rays_and_Reports_to_Pre-fill_Radiology_Reports.xml'\n",
      "Renamed 'paper_16.grobid.tei.xml' to 'Detecting_Domain_Shift_in_Multiple_Instance_Learning_for_Digital_Pathology_Using_Fréchet_Domain_Distance.xml'\n",
      "Renamed 'paper_67.grobid.tei.xml' to 'Improved_Flexibility_and_Interpretability_of_Large_Vessel_Stroke_Prognostication_Using_Image_Synthesis_and_Multi-task_Learning.xml'\n",
      "Renamed 'paper_72.grobid.tei.xml' to 'Dynamic_Curriculum_Learning_via_In-Domain_Uncertainty_for_Medical_Image_Classification.xml'\n",
      "Renamed 'paper_17.grobid.tei.xml' to 'Positive_Definite_Wasserstein_Graph_Kernel_for_Brain_Disease_Diagnosis.xml'\n",
      "Renamed 'paper_73.grobid.tei.xml' to 'Joint_Prediction_of_Response_to_Therapy,_Molecular_Traits,_and_Spatial_Organisation_in_Colorectal_Cancer_Biopsies.xml'\n",
      "Renamed 'paper_66.grobid.tei.xml' to 'Hierarchical_Vision_Transformers_for_Disease_Progression_Detection_in_Chest_X-Ray_Images.xml'\n",
      "Renamed 'paper_69.grobid.tei.xml' to 'Treatment_Outcome_Prediction_for_Intracerebral_Hemorrhage_via_Generative_Prognostic_Model_with_Imaging_and_Tabular_Data.xml'\n",
      "Renamed 'paper_18.grobid.tei.xml' to 'A_Reliable_and_Interpretable_Framework_of_Multi-view_Learning_for_Liver_Fibrosis_Staging.xml'\n",
      "Renamed 'paper_10.grobid.tei.xml' to 'DiffULD:_Diffusive_Universal_Lesion_Detection.xml'\n",
      "Renamed 'paper_61.grobid.tei.xml' to 'Revisiting_Feature_Propagation_and_Aggregation_in_Polyp_Segmentation.xml'\n",
      "Renamed 'paper_74.grobid.tei.xml' to 'Distributionally_Robust_Image_Classifiers_for_Stroke_Diagnosis_in_Accelerated_MRI.xml'\n",
      "Renamed 'paper_48.grobid.tei.xml' to 'Contrastive_Masked_Image-Text_Modeling_for_Medical_Visual_Representation_Learning.xml'\n",
      "Renamed 'paper_39.grobid.tei.xml' to 'Uncertainty_Inspired_Autism_Spectrum_Disorder_Screening.xml'\n",
      "Renamed 'paper_3.grobid.tei.xml' to 'SHISRCNet:_Super-Resolution_and_Classification_Network_for_Low-Resolution_Breast_Cancer_Histopathology_Image.xml'\n",
      "Renamed 'paper_31.grobid.tei.xml' to 'SwIPE:_Efficient_and_Robust_Medical_Image_Segmentation_with_Implicit_Patch_Embeddings.xml'\n",
      "Renamed 'paper_24.grobid.tei.xml' to 'Improved_Prognostic_Prediction_of_Pancreatic_Cancer_Using_Multi-phase_CT_by_Integrating_Neural_Distance_and_Texture-Aware_Transformer.xml'\n",
      "Renamed 'paper_55.grobid.tei.xml' to 'Self-supervised_Learning_for_Endoscopic_Video_Analysis.xml'\n",
      "Renamed 'paper_40.grobid.tei.xml' to 'Rad-ReStruct:_A_Novel_VQA_Benchmark_and_Method_for_Structured_Radiology_Reporting.xml'\n",
      "Renamed 'paper_4.grobid.tei.xml' to 'cOOpD:_Reformulating_COPD_Classification_on_Chest_CT_Scans_as_Anomaly_Detection_Using_Contrastive_Representations.xml'\n",
      "Renamed 'paper_23.grobid.tei.xml' to 'Discovering_Brain_Network_Dysfunction_in_Alzheimer’s_Disease_Using_Brain_Hypergraph_Neural_Network.xml'\n",
      "Renamed 'paper_36.grobid.tei.xml' to 'Style-Based_Manifold_for_Weakly-Supervised_Disease_Characteristic_Discovery.xml'\n",
      "Renamed 'paper_47.grobid.tei.xml' to 'Diversity-Preserving_Chest_Radiographs_Generation_from_Reports_in_One_Stage.xml'\n",
      "Renamed 'paper_52.grobid.tei.xml' to 'Text-Guided_Cross-Position_Attention_for_Segmentation:_Case_of_Medical_Image.xml'\n",
      "Renamed 'paper_37.grobid.tei.xml' to 'COVID-19_Pneumonia_Classification_with_Transformer_from_Incomplete_Modalities.xml'\n",
      "Renamed 'paper_22.grobid.tei.xml' to 'Punctate_White_Matter_Lesion_Segmentation_in_Preterm_Infants_Powered_by_Counterfactually_Generative_Learning.xml'\n",
      "Renamed 'paper_53.grobid.tei.xml' to 'Visual-Attribute_Prompt_Learning_for_Progressive_Mild_Cognitive_Impairment_Prediction.xml'\n",
      "Renamed 'paper_46.grobid.tei.xml' to 'Improving_Image-Based_Precision_Medicine_with_Uncertainty-Aware_Causal_Models.xml'\n",
      "Renamed 'paper_5.grobid.tei.xml' to 'YONA:_You_Only_Need_One_Adjacent_Reference-Frame_for_Accurate_and_Fast_Video_Polyp_Detection.xml'\n",
      "Renamed 'paper_25.grobid.tei.xml' to 'Contrastive_Feature_Decoupling_for_Weakly-Supervised_Disease_Detection.xml'\n",
      "Renamed 'paper_30.grobid.tei.xml' to 'What_Do_AEs_Learn?_Challenging_Common_Assumptions_in_Unsupervised_Anomaly_Detection.xml'\n",
      "Renamed 'paper_41.grobid.tei.xml' to 'Xplainer:_From_X-Ray_Observations_to_Explainable_Zero-Shot_Diagnosis.xml'\n",
      "Renamed 'paper_54.grobid.tei.xml' to 'Acute_Ischemic_Stroke_Onset_Time_Classification_with_Dynamic_Convolution_and_Perfusion_Maps_Fusion.xml'\n",
      "Renamed 'paper_49.grobid.tei.xml' to 'Adjustable_Robust_Transformer_for_High_Myopia_Screening_in_Optical_Coherence_Tomography.xml'\n",
      "Renamed 'paper_2.grobid.tei.xml' to 'You_Don’t_Have_to_Be_Perfect_to_Be_Amazing:_Unveil_the_Utility_of_Synthetic_Images.xml'\n",
      "Renamed 'paper_38.grobid.tei.xml' to 'Enhancing_Breast_Cancer_Risk_Prediction_by_Incorporating_Prior_Images.xml'\n",
      "Renamed 'paper_43.grobid.tei.xml' to 'Boosting_Breast_Ultrasound_Video_Classification_by_the_Guidance_of_Keyframe_Feature_Centers.xml'\n",
      "Renamed 'paper_56.grobid.tei.xml' to 'Fast_Non-Markovian_Diffusion_Model_for_Weakly_Supervised_Anomaly_Detection_in_Brain_MR_Images.xml'\n",
      "Renamed 'paper_8.grobid.tei.xml' to 'Liver_Tumor_Screening_and_Diagnosis_in_CT_with_Pixel-Lesion-Patient_Network.xml'\n",
      "Renamed 'paper_27.grobid.tei.xml' to 'Text-Guided_Foundation_Model_Adaptation_for_Pathological_Image_Classification.xml'\n",
      "Renamed 'paper_32.grobid.tei.xml' to 'Smooth_Attention_for_Deep_Multiple_Instance_Learning:_Application_to_CT_Intracranial_Hemorrhage_Detection.xml'\n",
      "Renamed 'paper_51.grobid.tei.xml' to 'Recruiting_the_Best_Teacher_Modality:_A_Customized_Knowledge_Distillation_Method_for_if_Based_Nephropathy_Diagnosis.xml'\n",
      "Renamed 'paper_44.grobid.tei.xml' to 'Learning_with_Domain-Knowledge_for_Generalizable_Prediction_of_Alzheimer’s_Disease_from_Multi-site_Structural_MRI.xml'\n",
      "Renamed 'paper_35.grobid.tei.xml' to 'CARL:_Cross-Aligned_Representation_Learning_for_Multi-view_Lung_Cancer_Histology_Classification.xml'\n",
      "Renamed 'paper_20.grobid.tei.xml' to 'Parse_and_Recall:_Towards_Accurate_Lung_Nodule_Malignancy_Prediction_Like_Radiologists.xml'\n",
      "Renamed 'paper_7.grobid.tei.xml' to 'Patients_and_Slides_are_Equal:_A_Multi-level_Multi-instance_Learning_Framework_for_Pathological_Image_Analysis.xml'\n",
      "Renamed 'paper_28.grobid.tei.xml' to 'Multiple_Prompt_Fusion_for_Zero-Shot_Lesion_Detection_Using_Vision-Language_Models.xml'\n",
      "Renamed 'paper_59.grobid.tei.xml' to 'Visual_Grounding_of_Whole_Radiology_Reports_for_3D_CT_Images.xml'\n",
      "Renamed 'paper_29.grobid.tei.xml' to 'Reversing_the_Abnormal:_Pseudo-Healthy_Generative_Networks_for_Anomaly_Detection.xml'\n",
      "Renamed 'paper_6.grobid.tei.xml' to 'Personalized_Patch-Based_Normality_Assessment_of_Brain_Atrophy_in_Alzheimer’s_Disease.xml'\n",
      "Renamed 'paper_58.grobid.tei.xml' to 'A_Multimodal_Disease_Progression_Model_for_Genetic_Associations_with_Disease_Dynamics.xml'\n",
      "Renamed 'paper_45.grobid.tei.xml' to 'GSDG:_Exploring_a_Global_Semantic-Guided_Dual-Stream_Graph_Model_for_Automated_Volume_Differential_Diagnosis_and_Prognosis.xml'\n",
      "Renamed 'paper_50.grobid.tei.xml' to 'Improving_Outcome_Prediction_of_Pulmonary_Embolism_by_De-biased_Multi-modality_Model.xml'\n",
      "Renamed 'paper_21.grobid.tei.xml' to 'Privacy-Preserving_Early_Detection_of_Epileptic_Seizures_in_Videos.xml'\n",
      "Renamed 'paper_34.grobid.tei.xml' to 'Beyond_the_Snapshot:_Brain_Tokenized_Graph_Transformer_for_Longitudinal_Brain_Functional_Connectome_Embedding.xml'\n",
      "Renamed 'paper_1.grobid.tei.xml' to 'Automatic_Bleeding_Risk_Rating_System_of_Gastric_Varices.xml'\n",
      "Renamed 'paper_57.grobid.tei.xml' to 'Self-supervised_Polyp_Re-identification_in_Colonoscopy.xml'\n",
      "Renamed 'paper_42.grobid.tei.xml' to 'Towards_Generalizable_Diabetic_Retinopathy_Grading_in_Unseen_Domains.xml'\n",
      "Renamed 'paper_33.grobid.tei.xml' to 'DCAug:_Domain-Aware_and_Content-Consistent_Cross-Cycle_Framework_for_Tumor_Augmentation.xml'\n",
      "Renamed 'paper_26.grobid.tei.xml' to 'Uncovering_Heterogeneity_in_Alzheimer’s_Disease_from_Graphical_Modeling_of_the_Tau_Spatiotemporal_Topography.xml'\n",
      "Renamed 'paper_9.grobid.tei.xml' to 'Self-_and_Semi-supervised_Learning_for_Gastroscopic_Lesion_Detection.xml'\n"
     ]
    }
   ],
   "source": [
    "# Rename_xml_files_in_folder(folder_path)\n",
    "def find_title(element):\n",
    "    \"\"\"Recursively search for the title element in the XML structure.\"\"\"\n",
    "    if 'title' in element.tag.lower() and element.text:\n",
    "        return element.text.strip()\n",
    "    for child in element:\n",
    "        title = find_title(child)\n",
    "        if title:\n",
    "            return title\n",
    "    return None\n",
    "\n",
    "def rename_xml_files_in_folder(folder_path):\n",
    "    \"\"\"Rename XML files based on their title tags.\"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.endswith('.xml'):  # Skip non-XML files\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            tree = et.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            paper_title = find_title(root)\n",
    "            if paper_title:\n",
    "                new_filename = paper_title.replace(\" \", \"_\") + '.xml'\n",
    "                new_file_path = os.path.join(folder_path, new_filename)\n",
    "                os.rename(file_path, new_file_path)\n",
    "                print(f\"Renamed '{filename}' to '{new_filename}'\")\n",
    "            else:\n",
    "                print(f\"Title not found in '{filename}'. Skipping.\")\n",
    "        except et.ParseError as e:\n",
    "            print(f\"Error parsing '{filename}': {e}\")\n",
    "\n",
    "# Rename the XML files in this folder\n",
    "folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol05'\n",
    "rename_xml_files_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually renaming the XML files \n",
    "***\n",
    "(some files are wrongly named/not named for some strange reason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/papers_xml/vol02/COLosSAL:_A_Benchmark_for_Cold-Start_Active_Learning_for_3D_Medical_Image_Segmentation.xml'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually renaming the XML files based on their title tags\n",
    "\n",
    "# Load and parse the XML file\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol09/paper_59.grobid.tei.xml'\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/output2/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "\n",
    "''' Paper 44 XML file: title was too long to be used as a file name, removed '/CT Self-supervised Denoising' from the title '''\n",
    "#file_path ='/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/paper_44.grobid.tei.xml'\n",
    "\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/output2/paper_13.grobid.tei.xml'\n",
    "\n",
    "'''FileNotFoundError: [Errno 2] No such file or directory: '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/paper_49.grobid.tei.xml' -> \n",
    "'/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/A_Patient-Specific_Self-supervised_Model_for_Automatic_X-Ray/CT_Registration.xml'\n",
    "Solution: removed '/CT_Registration' from title\n",
    "'''\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/paper_49.grobid.tei.xml'\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/papers_xml/vol04/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/papers_xml/vol01/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/papers_xml/vol02/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "\n",
    "tree = et.parse(file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Since XML namespaces can complicate direct tag access, we find the title tag dynamically.\n",
    "# This approach is based on the assumption that titles are relatively unique in structure.\n",
    "\n",
    "# Attempt to extract the paper title. This might need adjustments based on the actual structure.\n",
    "title = None\n",
    "for elem in root.iter():\n",
    "    if 'title' in elem.tag.lower():\n",
    "        title = elem.text\n",
    "        break\n",
    "\n",
    "title_clean = title.strip().replace(\" \", \"_\") if title else \"Untitled_Document\"\n",
    "title_clean\n",
    "\n",
    "# Attempt a more generic search for the title, considering common patterns in scholarly articles\n",
    "# We'll look for title elements that might be nested within other elements (like \"titleStmt\" or \"fileDesc\" in TEI format)\n",
    "\n",
    "def find_title(element):\n",
    "    \"\"\"\n",
    "    Recursively search for the title element in the XML structure.\n",
    "    \"\"\"\n",
    "    if 'title' in element.tag.lower() and element.text:\n",
    "        return element.text.strip()\n",
    "    for child in element:\n",
    "        title = find_title(child)\n",
    "        if title:\n",
    "            return title\n",
    "    return None\n",
    "\n",
    "# Attempt to find the title using the recursive search\n",
    "paper_title = find_title(root)\n",
    "paper_title_clean = paper_title.replace(\" \", \"_\") if paper_title else \"Untitled_Document\"\n",
    "paper_title, paper_title_clean\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the new file path with the clean title\n",
    "new_file_path = os.path.join(os.path.dirname(file_path), f\"{paper_title_clean}.xml\")\n",
    "\n",
    "# Rename the file\n",
    "os.rename(file_path, new_file_path)\n",
    "\n",
    "new_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming XML files, extracting paper titles and text\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .xpath() method with the string() function in XPath concatenates all text nodes within the current context. This method can retrieve all text within an element, including text in nested child elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 headers in 'Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising'\n",
      "Found 14 headers in 'CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?'\n",
      "Found 15 headers in 'ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer'\n",
      "Found 15 headers in 'CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning'\n",
      "Found 18 headers in '$$\\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment'\n",
      "Found 12 headers in 'A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration'\n",
      "Found 15 headers in 'PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer'\n",
      "Found 15 headers in 'Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations'\n",
      "Found 23 headers in 'SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid'\n",
      "Found 10 headers in 'Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis'\n",
      "Found 19 headers in 'Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain'\n",
      "Found 12 headers in 'DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images'\n",
      "Found 21 headers in 'Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction'\n",
      "Found 16 headers in 'StructuRegNet: Structure-Guided Multimodal 2D-3D Registration'\n",
      "Found 14 headers in 'X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior'\n",
      "Found 16 headers in 'RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform'\n",
      "Found 18 headers in 'Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation'\n",
      "Found 18 headers in 'Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction'\n",
      "Found 19 headers in 'TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms'\n",
      "Found 15 headers in 'MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI'\n",
      "Found 16 headers in 'MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities'\n",
      "Found 20 headers in 'FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction'\n",
      "Found 22 headers in 'ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising'\n",
      "Found 12 headers in 'Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration'\n",
      "Found 16 headers in 'Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior'\n",
      "Found 15 headers in 'Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training'\n",
      "Found 14 headers in 'Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis'\n",
      "Found 12 headers in 'Fast Reconstruction for Deep Learning PET Head Motion Correction'\n",
      "Found 14 headers in 'Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction'\n",
      "Found 14 headers in 'Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty'\n",
      "Found 15 headers in 'GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration'\n",
      "Found 14 headers in 'Motion Compensated Unsupervised Deep Learning for 5D MRI'\n",
      "Found 18 headers in 'An Unsupervised Multispectral Image Registration Network for Skin Diseases'\n",
      "Found 17 headers in 'Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network'\n",
      "Found 10 headers in 'Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction'\n",
      "Found 19 headers in 'Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction'\n",
      "Found 15 headers in 'Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI'\n",
      "Found 12 headers in 'CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph'\n",
      "Found 16 headers in 'Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography'\n",
      "Found 19 headers in 'CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy'\n",
      "Found 17 headers in 'An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis'\n",
      "Found 13 headers in 'Inverse Consistency by Construction for Multistep Deep Registration'\n",
      "Found 13 headers in 'Geometric Ultrasound Localization Microscopy'\n",
      "Found 16 headers in 'Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions'\n",
      "Found 19 headers in 'JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition'\n",
      "Found 14 headers in 'MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images'\n",
      "Found 18 headers in 'Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint'\n",
      "Found 13 headers in 'Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network'\n",
      "Found 14 headers in 'Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras'\n",
      "Found 15 headers in 'Computationally Efficient 3D MRI Reconstruction with Adaptive MLP'\n",
      "Found 15 headers in 'DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration'\n",
      "Found 16 headers in 'Differentiable Beamforming for Ultrasound Autofocusing'\n",
      "Found 13 headers in '3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions'\n",
      "Found 16 headers in 'Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation'\n",
      "Found 16 headers in 'S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences'\n",
      "Found 20 headers in 'Self-supervised MRI Reconstruction with Unrolled Diffusion Models'\n",
      "Found 15 headers in 'FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images'\n",
      "Found 13 headers in 'Solving Low-Dose CT Reconstruction via GAN with Local Coherence'\n",
      "Found 12 headers in 'Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction'\n",
      "Found 11 headers in 'Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT'\n",
      "Found 17 headers in 'DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction'\n",
      "Found 14 headers in 'Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication'\n",
      "Found 16 headers in 'Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration'\n",
      "Found 16 headers in 'X-Ray to CT Rigid Registration Using Scene Coordinate Regression'\n",
      "Found 16 headers in 'Trackerless Volume Reconstruction from Intraoperative Ultrasound Images'\n",
      "Found 12 headers in 'DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution'\n",
      "Found 15 headers in 'CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis'\n",
      "Found 15 headers in 'InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model'\n",
      "Found 18 headers in 'LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion'\n",
      "Found 14 headers in 'Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling'\n",
      "Found 17 headers in 'Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration'\n",
      "Found 16 headers in 'Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images'\n",
      "Found 16 headers in 'Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model'\n",
      "Found 13 headers in 'LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "\n",
    "def parse_xml_and_extract_headers(file_path):\n",
    "    tree = etree.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    # Extract the paper title by XPath in the XML's structure\n",
    "    paper_title_element = root.find('.//tei:title', ns)\n",
    "    paper_title = paper_title_element.text if paper_title_element is not None else \"No Title Found\"\n",
    "\n",
    "    headers = root.xpath('//tei:head', namespaces=ns)\n",
    "    print(f\"Found {len(headers)} headers in '{paper_title}'\")\n",
    "    \n",
    "    data = []\n",
    "    for header in headers:\n",
    "        # Use XPath string() function to get all text within the <p> tags, including nested elements\n",
    "        text_content = ''.join(header.getparent().xpath('.//tei:p//text()', namespaces=ns))\n",
    "        data.append({\n",
    "            'Paper Title': paper_title,\n",
    "            'Header Number': header.get('n'),\n",
    "            'Header Title': header.text,\n",
    "            'Text': text_content  # Updated to use text_content\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Paper Title', 'Header Number', 'Header Title', 'Text'])\n",
    "    return df\n",
    "\n",
    "def process_xml_folder(folder_path):\n",
    "    all_data_frames = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".xml\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = parse_xml_and_extract_headers(file_path)\n",
    "            all_data_frames.append(df)\n",
    "\n",
    "    if all_data_frames:\n",
    "        final_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "    else:\n",
    "        final_df = pd.DataFrame()\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Folder path - where XML files should be stored (gets updated for each volume path)\n",
    "folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/papers_xml/vol10'\n",
    "\n",
    "# Process the folder and create a DataFrame with all headers\n",
    "df_headers = process_xml_folder(folder_path)\n",
    "df_headers.to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol10_headers.csv\", index=False)\n",
    "#df_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see if titles are missing from the volume in the current dataframe \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_headers['Paper Title'].unique())\n",
    "#df_headers['Paper Title'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing dataframes for all 10 volumes within a dictionary\n",
    "***\n",
    "- Cleaning rows by removing non-relevant rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml_folder(folder_path):\n",
    "    df = pd.read_csv(folder_path)\n",
    "    return df\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Remove rows where both 'Header Title' and 'Text' are NaN or just 'Text' is NaN\n",
    "    df_cleaned = df.dropna(subset=['Header Title', 'Text'], how='all')\n",
    "    df_cleaned = df_cleaned.dropna(subset=['Text'], how='any')\n",
    "    return df_cleaned\n",
    "\n",
    "# Base path where all processed volumes are stored\n",
    "base_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/papers_xml/originals_dfs'\n",
    "\n",
    "# Dictionary to store cleaned DataFrames\n",
    "cleaned_dataframes = {}\n",
    "\n",
    "# Loop over the volume directories\n",
    "for i in range(1, 11):\n",
    "    vol_path = os.path.join(base_path, f'vol{str(i)}_headers.csv')\n",
    "    df_headers = process_xml_folder(vol_path)\n",
    "    df_cleaned = clean_dataframe(df_headers)\n",
    "    \n",
    "    # Store the cleaned DataFrame in the dictionary with the volume number as the key\n",
    "    cleaned_dataframes[f'vol{str(i)}'] = df_cleaned\n",
    "\n",
    "# Dictionary with all cleaned DataFrames\n",
    "# cleaned_dataframes['vol1'], cleaned_dataframes['vol2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_dataframes['vol4']['Paper Title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataframes['vol1'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol1_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol2'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol2_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol3'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol3_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol4'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol4_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol5'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol5_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol6'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol6_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol7'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol7_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol8'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol8_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol9'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol9_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol10'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol10_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking to see if papers per volume are missing after cleaning the dataframes\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total papers in 1: 73\n",
      "total papers in 2: 73\n",
      "total papers in 3: 72\n",
      "total papers in 4: 75\n",
      "total papers in 5: 76\n",
      "total papers in 6: 77\n",
      "total papers in 7: 75\n",
      "total papers in 8: 65\n",
      "total papers in 9: 70\n",
      "total papers in 10: 74\n"
     ]
    }
   ],
   "source": [
    "print('total papers in 1:', len(cleaned_dataframes['vol1']['Paper Title'].unique())) # 73\n",
    "print('total papers in 2:', len(cleaned_dataframes['vol2']['Paper Title'].unique())) # 73 \n",
    "print('total papers in 3:', len(cleaned_dataframes['vol3']['Paper Title'].unique())) # 72 \n",
    "print('total papers in 4:', len(cleaned_dataframes['vol4']['Paper Title'].unique())) # 75 \n",
    "print('total papers in 5:', len(cleaned_dataframes['vol5']['Paper Title'].unique())) # 76 \n",
    "print('total papers in 6:', len(cleaned_dataframes['vol6']['Paper Title'].unique())) # 77 \n",
    "print('total papers in 7:', len(cleaned_dataframes['vol7']['Paper Title'].unique())) # 75 \n",
    "print('total papers in 8:', len(cleaned_dataframes['vol8']['Paper Title'].unique())) # 65 \n",
    "print('total papers in 9:', len(cleaned_dataframes['vol9']['Paper Title'].unique())) # 70\n",
    "print('total papers in 10:', len(cleaned_dataframes['vol10']['Paper Title'].unique())) # 74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Merge all volumes into 1 dataframe\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique titles after enhancement: 730\n"
     ]
    }
   ],
   "source": [
    "for vol, df in cleaned_dataframes.items():\n",
    "    volume_number = int(re.search(r'\\d+', vol).group())\n",
    "    # Append volume information to each title to ensure uniqueness across volumes\n",
    "    df['Paper Title'] = df['Paper Title'].astype(str) + ' (vol' + str(volume_number) + ')'\n",
    "    df['Volume'] = volume_number\n",
    "\n",
    "combined_df = pd.concat(cleaned_dataframes.values(), ignore_index=True)\n",
    "\n",
    "# Check unique titles after appending volume information\n",
    "unique_titles_count = len(combined_df['Paper Title'].unique())\n",
    "print(f\"Total unique titles after enhancement: {unique_titles_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#combined_df.to_csv('combined_df.csv')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Lowercase all text in the 'Text' column\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mText\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m      6\u001b[0m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(wrap_text, width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Regular expression with str.replace to remove the volume information\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Text'"
     ]
    }
   ],
   "source": [
    "combined_df = combined_df.fillna(0)\n",
    "#combined_df.to_csv('combined_df.csv')\n",
    "\n",
    "# Lowercase all text in the 'Text' column\n",
    "combined_df['Text'] = combined_df['Text'].str.lower()\n",
    "combined_df['Text'] = combined_df['Text'].apply(wrap_text, width = 80)\n",
    "\n",
    "# Regular expression with str.replace to remove the volume information\n",
    "combined_df['Paper Title'] = combined_df['Paper Title'].str.replace(r'\\s*\\(vol\\d+\\)', '', regex=True)\n",
    "\n",
    "combined_df.rename(columns={'Paper Title': 'title', 'Header Number':'header_no', \n",
    "                                           'Header Title': 'header_title', 'Text':'text', 'Volume': 'volume'}, inplace=True)\n",
    "\n",
    "#combined_df.to_csv('refined_all_papers_extracted_w_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>header_no</th>\n",
       "      <th>header_title</th>\n",
       "      <th>text</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>to reduce radiologists' reading burden and mak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Method</td>\n",
       "      <td>notation. we first formally define the problem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Stage 1-Proxy Task to Detect Synthetic Anomalies</td>\n",
       "      <td>amae starts the first training stage using onl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Stage 2-MAE Inter-Discrepancy Adaptation</td>\n",
       "      <td>the proposed mae adaptation scheme is inspired...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>datasets. we evaluated our method on three pub...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6963</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Using Illumination Decline as a Depth Cue</td>\n",
       "      <td>the neus formulation of sect. 2 assumes distan...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6964</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Endoscope Photometric Model</td>\n",
       "      <td>apart from illumination decline, there are sev...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6965</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>we validate our method on the c3vd dataset [4]...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6966</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Conclusion</td>\n",
       "      <td>we have presented a method for 3d dense multi-...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6967</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>0</td>\n",
       "      <td>Supplementary Information</td>\n",
       "      <td>the online version contains supplementary mate...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6968 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title header_no  \\\n",
       "0     AMAE: Adaptation of Pre-trained Masked Autoenc...       1.0   \n",
       "1     AMAE: Adaptation of Pre-trained Masked Autoenc...       2.0   \n",
       "2     AMAE: Adaptation of Pre-trained Masked Autoenc...       2.1   \n",
       "3     AMAE: Adaptation of Pre-trained Masked Autoenc...       2.2   \n",
       "4     AMAE: Adaptation of Pre-trained Masked Autoenc...       3.0   \n",
       "...                                                 ...       ...   \n",
       "6963  LightNeuS: Neural Surface Reconstruction in En...       3.1   \n",
       "6964  LightNeuS: Neural Surface Reconstruction in En...       3.2   \n",
       "6965  LightNeuS: Neural Surface Reconstruction in En...       4.0   \n",
       "6966  LightNeuS: Neural Surface Reconstruction in En...       5.0   \n",
       "6967  LightNeuS: Neural Surface Reconstruction in En...         0   \n",
       "\n",
       "                                          header_title  \\\n",
       "0                                         Introduction   \n",
       "1                                               Method   \n",
       "2     Stage 1-Proxy Task to Detect Synthetic Anomalies   \n",
       "3             Stage 2-MAE Inter-Discrepancy Adaptation   \n",
       "4                                          Experiments   \n",
       "...                                                ...   \n",
       "6963         Using Illumination Decline as a Depth Cue   \n",
       "6964                       Endoscope Photometric Model   \n",
       "6965                                       Experiments   \n",
       "6966                                        Conclusion   \n",
       "6967                         Supplementary Information   \n",
       "\n",
       "                                                   text  volume  \n",
       "0     to reduce radiologists' reading burden and mak...       1  \n",
       "1     notation. we first formally define the problem...       1  \n",
       "2     amae starts the first training stage using onl...       1  \n",
       "3     the proposed mae adaptation scheme is inspired...       1  \n",
       "4     datasets. we evaluated our method on three pub...       1  \n",
       "...                                                 ...     ...  \n",
       "6963  the neus formulation of sect. 2 assumes distan...      10  \n",
       "6964  apart from illumination decline, there are sev...      10  \n",
       "6965  we validate our method on the c3vd dataset [4]...      10  \n",
       "6966  we have presented a method for 3d dense multi-...      10  \n",
       "6967  the online version contains supplementary mate...      10  \n",
       "\n",
       "[6968 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/databases/refined_all_papers_extracted_w_text.csv'\n",
    "combined_df = pd.read_csv(filename, index_col=0)\n",
    "print(len(combined_df['title'].unique()))\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique titles in vol1: 73\n",
      "Unique titles in vol2: 73\n",
      "Unique titles in vol3: 72\n",
      "Unique titles in vol4: 75\n",
      "Unique titles in vol5: 76\n",
      "Unique titles in vol6: 77\n",
      "Unique titles in vol7: 75\n",
      "Unique titles in vol8: 65\n",
      "Unique titles in vol9: 70\n",
      "Unique titles in vol10: 74\n",
      "Sum of unique titles from individual DataFrames: 730\n"
     ]
    }
   ],
   "source": [
    "# Verify unique title counts in individual dataframes before combining\n",
    "total_unique = 0\n",
    "for vol, df in cleaned_dataframes.items():\n",
    "    unique_in_df = len(df['Paper Title'].unique())\n",
    "    print(f\"Unique titles in {vol}: {unique_in_df}\")\n",
    "    total_unique += unique_in_df\n",
    "\n",
    "print(f\"Sum of unique titles from individual DataFrames: {total_unique}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select papers related to cancer\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n"
     ]
    }
   ],
   "source": [
    "# Search for papers related to cancer\n",
    "df_cancer_related = combined_df[combined_df['text'].str.contains('cancer|tumour|tumor')]\n",
    "\n",
    "# Extract unique paper titles from these rows\n",
    "unique_paper_titles_with_cancer = df_cancer_related['title'].unique()\n",
    "\n",
    "# The total number of unique papers related to cancer\n",
    "print(len(unique_paper_titles_with_cancer))\n",
    "df_cancer_related.to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/databases/cancer_related_papers_w_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix issue: not all text per papers get added to df_cancer_related. Only text rows with cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your combined DataFrame (assuming it's already loaded as combined_df)\n",
    "\n",
    "# Initialize an empty DataFrame for cancer-related papers\n",
    "cancer_related_papers = pd.DataFrame(columns=combined_df.columns)\n",
    "\n",
    "# Keywords to search for in text\n",
    "keywords = ['cancer', 'tumour', 'tumor']\n",
    "\n",
    "# Step 1: Identify Papers\n",
    "for title in combined_df['title'].unique():\n",
    "    # Get all text for the current paper\n",
    "    paper_text = combined_df[combined_df['title'] == title]['text'].str.cat(sep=' ')\n",
    "    \n",
    "    # Check if any of the keywords are in the paper's text\n",
    "    if any(keyword in paper_text.lower() for keyword in keywords):\n",
    "        # Since the paper is related to cancer, append all its rows to the cancer_related_papers DataFrame\n",
    "        cancer_related_papers = pd.concat([cancer_related_papers, combined_df[combined_df['title'] == title]])\n",
    "\n",
    "# Step 2: Remove duplicates in cancer_related_papers if any\n",
    "cancer_related_papers.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "df_cancer_related.to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/databases/cancer_related_papers_w_text.csv\", index=False)\n",
    "# The total number of unique papers related to cancer\n",
    "unique_paper_titles_with_cancer = cancer_related_papers['title'].unique()\n",
    "print(len(unique_paper_titles_with_cancer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract keyword-related sentences from selected papers\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>header_no</th>\n",
       "      <th>header_title</th>\n",
       "      <th>text</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>chest radiographs (chest x-rays) represent the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Self-supervised Learning for Physiologically-B...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>the dataset is composed of 23 oncological pati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AME-CAM: Attentive Multiple-Exit CAM for Weakl...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>deep learning techniques have greatly improved...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AME-CAM: Attentive Multiple-Exit CAM for Weakl...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>we evaluate our method on the brain tumor segm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AME-CAM: Attentive Multiple-Exit CAM for Weakl...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Quantitative and Qualitative Comparison with S...</td>\n",
       "      <td>in this section, we compare the segmentation p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>CoLa-Diff: Conditional Latent Diffusion Model ...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Comparisons with State-of-the-Art Methods</td>\n",
       "      <td>datasets and baselines. we evaluated cola-diff...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>InverseSR: 3D Brain MRI Super-Resolution Using...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>end-to-end convolutional neural networks (cnns...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>InverseSR: 3D Brain MRI Super-Resolution Using...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Results</td>\n",
       "      <td>figure 2 shows the qualitative results on the ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>Topology-Preserving Computed Tomography Super-...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>computed tomography (ct) is a prevalent imagin...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>colorectal cancer (crc) is the third most comm...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>646 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  header_no  \\\n",
       "0    Anatomy-Driven Pathology Detection on Chest X-...        1.0   \n",
       "1    Self-supervised Learning for Physiologically-B...        2.4   \n",
       "2    AME-CAM: Attentive Multiple-Exit CAM for Weakl...        1.0   \n",
       "3    AME-CAM: Attentive Multiple-Exit CAM for Weakl...        3.1   \n",
       "4    AME-CAM: Attentive Multiple-Exit CAM for Weakl...        4.1   \n",
       "..                                                 ...        ...   \n",
       "641  CoLa-Diff: Conditional Latent Diffusion Model ...        3.1   \n",
       "642  InverseSR: 3D Brain MRI Super-Resolution Using...        1.0   \n",
       "643  InverseSR: 3D Brain MRI Super-Resolution Using...        4.0   \n",
       "644  Topology-Preserving Computed Tomography Super-...        1.0   \n",
       "645  LightNeuS: Neural Surface Reconstruction in En...        1.0   \n",
       "\n",
       "                                          header_title  \\\n",
       "0                                         Introduction   \n",
       "1                                              Dataset   \n",
       "2                                         Introduction   \n",
       "3                                              Dataset   \n",
       "4    Quantitative and Qualitative Comparison with S...   \n",
       "..                                                 ...   \n",
       "641          Comparisons with State-of-the-Art Methods   \n",
       "642                                       Introduction   \n",
       "643                                            Results   \n",
       "644                                       Introduction   \n",
       "645                                       Introduction   \n",
       "\n",
       "                                                  text  volume  \n",
       "0    chest radiographs (chest x-rays) represent the...       1  \n",
       "1    the dataset is composed of 23 oncological pati...       1  \n",
       "2    deep learning techniques have greatly improved...       1  \n",
       "3    we evaluate our method on the brain tumor segm...       1  \n",
       "4    in this section, we compare the segmentation p...       1  \n",
       "..                                                 ...     ...  \n",
       "641  datasets and baselines. we evaluated cola-diff...      10  \n",
       "642  end-to-end convolutional neural networks (cnns...      10  \n",
       "643  figure 2 shows the qualitative results on the ...      10  \n",
       "644  computed tomography (ct) is a prevalent imagin...      10  \n",
       "645  colorectal cancer (crc) is the third most comm...      10  \n",
       "\n",
       "[646 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cancer_related = pd.read_csv('/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/databases/cancer_related_papers_w_text.csv')\n",
    "print(len(df_cancer_related['title'].unique()))\n",
    "\n",
    "df_cancer_related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### List of keywords\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords\n",
    "keywords_demographics_long = [\n",
    "    'age', 'gender', 'sex', 'women', 'woman', 'female', 'male',\n",
    "    'geolocation', 'geographical', 'geographic', 'country', 'countries', 'city', 'cities', \n",
    "    'hospital', 'hospitals', 'clinic', 'clinics', 'society', 'societies',\n",
    "    'etnicity', 'etnicities', 'race', \n",
    "    'bias', 'biases', 'fair', 'unfair', 'fairness', 'transparency', 'awareness',\n",
    "    'imbalance', 'imbalanced', 'balance', 'balanced',\n",
    "    'problem', 'problems', 'issue', 'issues', 'challenge', 'challenges', \n",
    "    'difficult', 'difficulty', 'difficulties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords\n",
    "keywords_demographics_short = [\n",
    "    'age', 'gender', 'sex', 'women', 'woman', 'female', 'male',\n",
    "    'etnicity', 'etnicities', 'race', \n",
    "    'bias', 'biases', 'fair', 'unfair', 'fairness', 'transparency', \n",
    "    'imbalance', 'imbalanced', 'balance', 'balanced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dataset = ['data', 'dataset', 'datasets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Split the text into words and extract keyword-matches. Group each keyword-match by relatd paper \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into sentences and search for the keywords\n",
    "\n",
    "def extract_keywords(df, keywords):\n",
    "    # Search for the whole word in the text\n",
    "    pattern = r'\\b(' + '|'.join(re.escape(keyword) for keyword in keywords) + r')\\b'\n",
    "\n",
    "    # Initialize a dictionary to hold sentences organized by paper title\n",
    "    sentences_by_paper = {}\n",
    "\n",
    "    # Loop through each row in the dataframe\n",
    "    for index, row in df_cancer_related.iterrows():\n",
    "        # Find all sentences that contain any of the keywords\n",
    "        sentences = re.findall(pattern, row['text'], flags=re.IGNORECASE | re.DOTALL)\n",
    "        \n",
    "        # If there are matching sentences, add them to the dictionary under the paper title\n",
    "        if sentences:\n",
    "            paper_title = row['title']\n",
    "            if paper_title not in sentences_by_paper:\n",
    "                sentences_by_paper[paper_title] = []\n",
    "            sentences_by_paper[paper_title].extend(sentences)\n",
    "\n",
    "    # Sentences_by_paper contains all the sentences that contain keywords, organized by paper title\n",
    "\n",
    "    # Convert this dictionary into a DataFrame:\n",
    "    # Create a list of tuples (paper title, sentence)        \n",
    "    keywords_data = [(title, keyword_sentence) for title, related_group in sentences_by_paper.items() for keyword_sentence in related_group]\n",
    "    keywords_df = pd.DataFrame(keywords_data, columns=['title', 'keyword']) \n",
    "\n",
    "    return keywords_df       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AME-CAM: Attentive Multiple-Exit CAM for Weakl...</td>\n",
       "      <td>issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AME-CAM: Attentive Multiple-Exit CAM for Weakl...</td>\n",
       "      <td>issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AME-CAM: Attentive Multiple-Exit CAM for Weakl...</td>\n",
       "      <td>issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AME-CAM: Attentive Multiple-Exit CAM for Weakl...</td>\n",
       "      <td>issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>difficulties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>873 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title       keyword\n",
       "0    Anatomy-Driven Pathology Detection on Chest X-...         issue\n",
       "1    AME-CAM: Attentive Multiple-Exit CAM for Weakl...         issue\n",
       "2    AME-CAM: Attentive Multiple-Exit CAM for Weakl...         issue\n",
       "3    AME-CAM: Attentive Multiple-Exit CAM for Weakl...         issue\n",
       "4    AME-CAM: Attentive Multiple-Exit CAM for Weakl...        issues\n",
       "..                                                 ...           ...\n",
       "868  LightNeuS: Neural Surface Reconstruction in En...       problem\n",
       "869  LightNeuS: Neural Surface Reconstruction in En...       problem\n",
       "870  LightNeuS: Neural Surface Reconstruction in En...       problem\n",
       "871  LightNeuS: Neural Surface Reconstruction in En...  difficulties\n",
       "872  LightNeuS: Neural Surface Reconstruction in En...       problem\n",
       "\n",
       "[873 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_df = extract_keywords(df_cancer_related, keywords_demographics_long)\n",
    "keywords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords_df.to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/data/cancer_related_keywords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Split text into sentences, keyword search sentences and extract those with a keyword match\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo code\n",
    "# regex to split the text into sentences. A sentence is defined as a sequence of characters that ends with a period, question mark, or exclamation mark.\n",
    "# iterate through the sentences to find those with a keyword from the list of keywords. \n",
    "# for each match\n",
    "    # option 1) concatentinate the previous and next sentences to the sentence with the keyword (if they haven't been added already)\n",
    "    # option 2) extract sentence with keyword only\n",
    "# keep track of the sentences already added for each paper title.\n",
    "# if no matches are found for a paper title, add 'none'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT SENTS WITH KEYWORDS    \n",
    "# Option 2) Storing keyword sentence only \n",
    "\n",
    "def extract_keyword_sentences(df, keywords):\n",
    "    \"\"\"\n",
    "    Extract sentences containing specified keywords from DataFrame and organize by paper title.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the text to search through.\n",
    "    - keywords: List of keywords to search for in the text.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with paper titles as keys and lists of sentences containing the keywords as values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compile the regular expression for matching sentences containing the keywords\n",
    "    keyword_pattern = re.compile(r'\\b(?:' + '|'.join(keywords) + r')\\b', flags=re.IGNORECASE)\n",
    "\n",
    "    # Initialize a dictionary to hold sentences organized by paper title\n",
    "    sentences_by_paper = {}\n",
    "\n",
    "    # Loop through each paper title in the DataFrame\n",
    "    for title in df['title'].unique():\n",
    "        # Get the full text for the current title\n",
    "        text = ' '.join(df[df['title'] == title]['text'])\n",
    "        # Split the text into sentences\n",
    "        sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "\n",
    "        # List to store sentences that contain the keyword\n",
    "        keyword_sentences_buffer = []\n",
    "\n",
    "        # Iterate through sentences to find and store sentences that contain the keyword\n",
    "        for sentence in sentences:\n",
    "            if keyword_pattern.search(sentence):\n",
    "                # Add only the sentence with the keyword to the buffer\n",
    "                keyword_sentences_buffer.append(sentence)\n",
    "\n",
    "        # Add the sentences to the dictionary, use 'none' if there are no matches\n",
    "        sentences_by_paper[title] = keyword_sentences_buffer if keyword_sentences_buffer else ['none']\n",
    "    \n",
    "    extracted_data = [(title, keyword_sentence) for title, related_group in sentences_by_paper.items() for keyword_sentence in related_group]\n",
    "    extracted_df = pd.DataFrame(extracted_data, columns=['title', 'extracted_keyword_sent'])\n",
    "    \n",
    "    # Wrap title and the extracted sentences to a maximum width of n-characters for better readability\n",
    "    extracted_df['extracted_keyword_sent'] = extracted_df['extracted_keyword_sent'].apply(wrap_text, width=80)\n",
    "\n",
    "    return extracted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = extract_keyword_sentences(df_cancer_related, keywords_demographics_long).to_csv('extracted_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND INFO ABOUT DATASETS    \n",
    "extracted_data = extract_keyword_sentences(df_cancer_related, keywords_dataset).to_csv('dataset_extracted_sentences.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>extracted_keyword_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Self-supervised Learning for Physiologically-B...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AME-CAM: Attentive Multiple-Exit CAM for Weakl...</td>\n",
       "      <td>to address this\\nissue, recent research has fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AME-CAM: Attentive Multiple-Exit CAM for Weakl...</td>\n",
       "      <td>to meet this need, many researchers\\nhave devo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AME-CAM: Attentive Multiple-Exit CAM for Weakl...</td>\n",
       "      <td>our proposed method has the following\\ncontrib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>Trackerless Volume Reconstruction from Intraop...</td>\n",
       "      <td>liver cancer is the most prevalent indication ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>Trackerless Volume Reconstruction from Intraop...</td>\n",
       "      <td>such motion is predominant in the context high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>CoLa-Diff: Conditional Latent Diffusion Model ...</td>\n",
       "      <td>it shows superiority in model training however...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>CoLa-Diff: Conditional Latent Diffusion Model ...</td>\n",
       "      <td>-propose an auto-weight\\nadaptation to balance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>colorectal cancer (crc) is the third most comm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>318 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Anatomy-Driven Pathology Detection on Chest X-...   \n",
       "1    Self-supervised Learning for Physiologically-B...   \n",
       "2    AME-CAM: Attentive Multiple-Exit CAM for Weakl...   \n",
       "3    AME-CAM: Attentive Multiple-Exit CAM for Weakl...   \n",
       "4    AME-CAM: Attentive Multiple-Exit CAM for Weakl...   \n",
       "..                                                 ...   \n",
       "313  Trackerless Volume Reconstruction from Intraop...   \n",
       "314  Trackerless Volume Reconstruction from Intraop...   \n",
       "315  CoLa-Diff: Conditional Latent Diffusion Model ...   \n",
       "316  CoLa-Diff: Conditional Latent Diffusion Model ...   \n",
       "317  LightNeuS: Neural Surface Reconstruction in En...   \n",
       "\n",
       "                                extracted_keyword_sent  \n",
       "0                                                 none  \n",
       "1                                                 none  \n",
       "2    to address this\\nissue, recent research has fo...  \n",
       "3    to meet this need, many researchers\\nhave devo...  \n",
       "4    our proposed method has the following\\ncontrib...  \n",
       "..                                                 ...  \n",
       "313  liver cancer is the most prevalent indication ...  \n",
       "314  such motion is predominant in the context high...  \n",
       "315  it shows superiority in model training however...  \n",
       "316  -propose an auto-weight\\nadaptation to balance...  \n",
       "317  colorectal cancer (crc) is the third most comm...  \n",
       "\n",
       "[318 rows x 2 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_extracted_df = pd.read_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/data/extracted_sentences.csv\", index_col=0)\n",
    "print(len(sents_extracted_df['title'].unique()))\n",
    "\n",
    "sents_extracted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>extracted_keyword_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>these region boxes are easier to annotate -the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Self-supervised Learning for Physiologically-B...</td>\n",
       "      <td>the dataset is composed of 23 oncological pati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Self-supervised Learning for Physiologically-B...</td>\n",
       "      <td>dpet data was acquired on a biograph vision qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Self-supervised Learning for Physiologically-B...</td>\n",
       "      <td>the dataset included the label maps of 7 organ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Self-supervised Learning for Physiologically-B...</td>\n",
       "      <td>further details on\\nthe dataset are presented ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Noise2Aliasing: Unsupervised Deep Learning for...</td>\n",
       "      <td>the data were anonymized prior to analysis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Noise2Aliasing: Unsupervised Deep Learning for...</td>\n",
       "      <td>projection\\nnoise was added using the poisson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>Trackerless Volume Reconstruction from Intraop...</td>\n",
       "      <td>3 presents our current results on ex vivo porc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>CoLa-Diff: Conditional Latent Diffusion Model ...</td>\n",
       "      <td>magnetic resonance imaging (mri) is critical t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>603 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Anatomy-Driven Pathology Detection on Chest X-...   \n",
       "1    Self-supervised Learning for Physiologically-B...   \n",
       "2    Self-supervised Learning for Physiologically-B...   \n",
       "3    Self-supervised Learning for Physiologically-B...   \n",
       "4    Self-supervised Learning for Physiologically-B...   \n",
       "..                                                 ...   \n",
       "598  Noise2Aliasing: Unsupervised Deep Learning for...   \n",
       "599  Noise2Aliasing: Unsupervised Deep Learning for...   \n",
       "600  Trackerless Volume Reconstruction from Intraop...   \n",
       "601  CoLa-Diff: Conditional Latent Diffusion Model ...   \n",
       "602  LightNeuS: Neural Surface Reconstruction in En...   \n",
       "\n",
       "                                extracted_keyword_sent  \n",
       "0    these region boxes are easier to annotate -the...  \n",
       "1    the dataset is composed of 23 oncological pati...  \n",
       "2    dpet data was acquired on a biograph vision qu...  \n",
       "3    the dataset included the label maps of 7 organ...  \n",
       "4    further details on\\nthe dataset are presented ...  \n",
       "..                                                 ...  \n",
       "598        the data were anonymized prior to analysis.  \n",
       "599  projection\\nnoise was added using the poisson ...  \n",
       "600  3 presents our current results on ex vivo porc...  \n",
       "601  magnetic resonance imaging (mri) is critical t...  \n",
       "602                                               none  \n",
       "\n",
       "[603 rows x 2 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_extracted_sents = pd.read_csv('/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/data/dataset_extracted_sentences.csv', index_col=0)\n",
    "print(len(dataset_extracted_sents['title'].unique())) \n",
    "\n",
    "dataset_extracted_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
