<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty</title>
				<funder ref="#_6Xkz26V">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_JQGMcGR #_vuVC4pU #_M3BbEY6 #_SVZ4HCJ">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiazhen</forename><surname>Wang</surname></persName>
							<email>jzwang@stu.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yizhe</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Yang</surname></persName>
							<email>yangyan@xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>jiansun@xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Pazhou Laboratory (Huangpu)</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="293" to="302"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">5AECDEFAB9CF3CECEDEBFA462B60492D</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_28</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Magnetic resonance imaging</term>
					<term>Motion artifacts correction</term>
					<term>Dual domain reconstruction</term>
					<term>K-space uncertainty</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Magnetic resonance imaging (MRI) may degrade with motion artifacts in the reconstructed MR images due to the long acquisition time. In this paper, we propose a dual domain motion correction network (D 2 MC-Net) to correct the motion artifacts in 2D multi-slice MRI. Instead of explicitly estimating the motion parameters, we model the motion corruption by k-space uncertainty to guide the MRI reconstruction in an unfolded deep reconstruction network. Specifically, we model the motion correction task as a dual domain regularized model with an uncertainty-guided data consistency term. Inspired by its alternating iterative optimization algorithm, the D 2 MC-Net is composed of multiple stages, and each stage consists of a k-space uncertainty module (KU-Module) and a dual domain reconstruction module (DDR-Module). The KU-Module quantifies the uncertainty of k-space corruption by motion. The DDR-Module reconstructs motion-free k-space data and MR image in both k-space and image domain, under the guidance of the k-space uncertainty. Extensive experiments on fastMRI dataset demonstrate that the proposed D 2 MC-Net outperforms state-of-the-art methods under different motion trajectories and motion severities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Magnetic resonance imaging (MRI) is a widely used non-invasive imaging technique. However, MRI is sensitive to subject motion due to the long time for k-space data acquisition <ref type="bibr" target="#b15">[16]</ref>. Motion artifacts, appearing as ghosting or blurring artifacts in MR images, degrade the MR image quality <ref type="bibr" target="#b22">[23]</ref> and affect the clinical diagnosis. During the scan, it is hard for subjects to remain still, especially for pediatrics or neuro-degenerative patients. Therefore, the correction of motion artifacts in MRI has a great clinical demand.</p><p>The typical methods for motion artifacts correction in MRI include the prospective and retrospective methods. The prospective methods measure the subject motion using external tracking devices or navigators during the scan for motion correction <ref type="bibr" target="#b10">[11]</ref>. The retrospective motion correction methods either explicitly model and correct the motion in the image reconstruction algorithm, or learn the mapping from MR image with motion artifacts to the motion-free MR image using deep learning approach. Specifically, the methods in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15]</ref> are based on a forward model of subject motion, and jointly estimate the motion parameters and MR image using the optimization algorithm. The methods in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref> introduce convolutional neural networks (CNNs) into the joint optimization procedure to learn the MR image prior. The deep learning methods in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> directly learn the mapping from motion-corrupted MR image to motion-free MR image by designing various deep networks. Some other methods correct the motion artifacts using additional prior information, such as the different contrasts of the same object <ref type="bibr" target="#b12">[13]</ref>, self-assisted adjacent slices priors <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this paper, we propose a dual domain motion correction network (i.e., D 2 MC-Net) to correct the motion artifacts in 2D multi-slice MRI. Instead of explicitly estimating motion parameters, we design a dual domain regularized model with an uncertainty-guided data consistency term, which models the motion corruption by k-space uncertainty to guide the MRI reconstruction. Then the alternating iterative algorithm of the model is unfolded to be a novel deep network, i.e., D 2 MC-Net. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the D 2 MC-Net contains multiple stages, and each stage consists of two key components, i.e., k-space uncertainty module (KU-Module) and dual domain reconstruction module (DDR-Module). The KU-Module measures the uncertainty of k-space data corrupted by the motion. The DDR-Module reconstructs motion-free k-space data and MR image in both k-space and image domain under the guidance of the k-space uncertainty. Extensive experiments on fastMRI dataset demonstrate that the proposed D 2 MC-Net achieves the state-of-the-art results under different motion trajectories and motion severities. For example, under severe corruption with piecewise constant motion trajectory, our result in PSNR is at least 2.11 dB higher than the existing methods, e.g., Autofocusing+ <ref type="bibr" target="#b11">[12]</ref>.</p><p>Different from the optimization-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, our model is based on modeling the motion corruption by k-space uncertainty without explicitly estimating the motion parameters. Different from the deep learning methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>, D 2 MC-Net incorporates an uncertainty-guided data consistency term into the unfolded network to guide MRI reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>In our approach, we model the motion corruption by measuring the uncertainty of k-space data. Specifically, we assume that the distribution of motion-corrupted k-space data ŷ ∈ C N at each position obeys a non-i.i.d. and pixel-wise Gaussian distribution, where N is the number of the k-space data. Specifically, considering the i-th position of the ŷ, we have where w ∈ [0, 1] N represents the k-space uncertainty with the elements w</p><formula xml:id="formula_0">p(ŷ [i] |x, σ [i] ) ∼ N (ŷ [i] |(F x) [i] , σ 2 [i] ),<label>(1)</label></formula><formula xml:id="formula_1">[i] = 1 /σ [i]</formula><p>. p(x) and p(y) are the prior distributions of the motion-free data in image domain and k-space domain. The likelihood distribution log p(ŷ|x, w)</p><formula xml:id="formula_2">= i p(ŷ [i] |x, σ [i]</formula><p>) has been modeled by Eq. ( <ref type="formula" target="#formula_0">1</ref>). Then the solution of Eq. ( <ref type="formula">2</ref>) can be converted to a dual domain regularized model with an uncertainty-guided data consistency term to correct the motion-related artifacts:</p><formula xml:id="formula_3">x * , y * , w * = arg min x,y ,w 1 2 w F x -w ŷ 2 2 + ρ 2 x -H I (x; θ I ) 2 2 + λ 2 y -H K (y; θ K ) 2 2 - N i log w [i] , s.t. y = F x (3)</formula><p>where H I and H K are learnable denoisers with parameters θ I and θ K , which adopt the U-Net <ref type="bibr" target="#b17">[18]</ref> architecture in this paper. λ and ρ are trade-off parameters. The first term is the uncertainty-guided data consistency term corresponding to the log-likelihood log p(ŷ|x, w) which enforces consistency between the k-space data of reconstructed MR image and its motion-corrupted k-space data under the guidance of the uncertainty w. The second and third terms are regularizations for imposing image-space prior p(x) and k-space prior p(y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dual Domain Motion Correction Network</head><p>Our proposed D 2 MC-Net is designed based on the alternating optimization algorithm to solve Eq. ( <ref type="formula">3</ref>). As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, taking the motion-corrupted k-space data as input, it reconstructs the motion-free MR images with T stages. Each stage consists of the k-space uncertainty module (KU-Module) and the dual domain reconstruction module (DDR-Module), respectively corresponding to the sub-problems for optimizing the k-space uncertainty w, and the dual domain data including k-space data y and MR image x. The KU-Module estimates the k-space uncertainty w, quantifying the uncertainty of k-space data corrupted by motion. The DDR-Module is responsible for reconstructing the k-space data y and MR image x, under the guidance of the k-space uncertainty w. Details of these two modules at t-th stage are as follows.</p><p>K-space Uncertainty Module. This module is designed to update k-space uncertainty w in Eq. ( <ref type="formula">3</ref>). If directly optimizing w in Eq. ( <ref type="formula">3</ref>), w t = 1 /|Fxt-1-ŷ| at t-th stage, which depends on the difference between the k-space data of reconstructed image F x t-1 and the motion-corrupted k-space data ŷ. We extend this estimate to be a learnable module defined as:</p><formula xml:id="formula_4">w t H W (F x t-1 , ŷ; θ W ),<label>(4)</label></formula><p>where H W is the sub-network with parameters θ W . When t=1, we only send ŷ into the KU-Module because we do not have the estimate of the reconstructed MR images in such case.</p><p>Dual Domain Reconstruction Module. This module is designed to update k-space data y and MR image x in Eq. ( <ref type="formula">3</ref>) under the guidance of the uncertainty w. Specifically, given the reconstructed MR image x t-1 from (t-1)-th stage and the k-space uncertainty w t , the k-space data at t-th stage is updated by:</p><formula xml:id="formula_5">y t = arg min y 1 2 W t y -W t ŷ 2 2 + λ 2 y -H K (F x t-1 ; θ K ) 2 2 = (W t W t + λI) -1 (W t W t ŷ + λH K (F x t-1 ; θ K )) UDC K • H K (F x t-1 ; θ K ),<label>(5)</label></formula><p>where W t = diag(w t ) ∈ [0, 1] N ×N is a diagonal matrix, thus the matrix inversion in Eq. ( <ref type="formula" target="#formula_5">5</ref>) can be computed efficiently. Equation ( <ref type="formula" target="#formula_5">5</ref>) is defined as k-space reconstruction block (K-Block), solving the sub-problem for optimizing k-space data y in Eq. (3). Equation ( <ref type="formula" target="#formula_5">5</ref>) can be implemented by firstly computing H K , followed by the k-space uncertainty-guided data consistency operator UDC K in Eq. ( <ref type="formula" target="#formula_5">5</ref>). Similarly, given the updated uncertainty w t and k-space data y t , the MR image at t-th stage is updated by:</p><formula xml:id="formula_6">x t = arg min x 1 2 W t F x -W t ŷ 2 2 + ρ 2 x -H I (F H y t ; θ I ) 2 2 = F H (W t W t + ρI) -1 (W t W t ŷ + ρF H I (F H y t ; θ I )) UDC I • H I (F H y t ; θ I ).<label>(6)</label></formula><p>Equation ( <ref type="formula" target="#formula_6">6</ref>) is defined as image reconstruction block (I-Block), solving the subproblem for optimizing MR image x in Eq. ( <ref type="formula">3</ref>). Equation ( <ref type="formula" target="#formula_6">6</ref>) can be implemented by firstly computing H I , followed by the image domian uncertainty-guided data consistency operator UDC I in Eq. ( <ref type="formula" target="#formula_6">6</ref>). The K-Block and I-Block are combined as the dual domain reconstruction module (DDR-Module) to sequentially reconstruct the k-space data y t and MR image x t at t-th stage. In summary, by connecting the k-space uncertainty module and dual domain reconstruction module alternately, we construct a multi-stage deep network (i.e., D 2 MC-Net) for motion artifacts correction as shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Network Details and Training Loss</head><p>In the proposed D 2 MC-Net, we use T = 3 stages for speed and accuracy tradeoff. Each stage has three sub-networks (i.e., H W , H K and H I ) as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. H K and H I adopt U-Net <ref type="bibr" target="#b17">[18]</ref> architecture which contains five encoder blocks and four decoder blocks followed by a 1×1 convolution layer for the final output. Each block consists of two 3 × 3 convolution layers, an instance normalization (IN) layer and a ReLU activation function. The average pooling and bilinear interpolation layers are respectively to reduce and increase the resolution of the feature maps. The number of output feature channels of the encoder and decoder blocks in U-Net are successively 32, 64, 128, 256, 512, 256 128, 64, 32. The structure of H W is Conv→IN→ReLU→Conv→Sigmoid, where Conv denotes a 3 × 3 convolution layer. The number of output feature channels for these two convolution layers are 64 and 2, respectively.</p><p>The overall loss function in image space and k-space is defined as:</p><formula xml:id="formula_7">L = T t=1 γ y t -y gt 1 + x t -x gt 1 + (1 -SSIM(x t , x gt )),<label>(7)</label></formula><p>where x t and y t are the reconstructed MR image and k-space data at t-th stage.</p><p>x gt and y gt are the motion-free MR image and k-space data. SSIM <ref type="bibr" target="#b21">[22]</ref> is the structural similarity loss. γ is a hyperparameter to balance the different losses in dual domain, and we set γ = 0.001. The Adam optimizer with mini-batch size of 4 is used to optimize the network parameters. The initial value of the learning rate is 1 × 10 -4 and divided by 10 at 40-th epoch. We implement the proposed D 2 MC-Net using PyTorch on one Nvidia Tesla V100 GPU for 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset. We evaluate our method on the T2-weighted brain images from the fastMRI dataset <ref type="bibr" target="#b6">[7]</ref>, and we randomly select 78 subjects for training and 39 subjects for testing. The in-plane matrix size of the subjects is resized to 384 × 384, and the number of slices varies from the subjects. Sensitivity maps are estimated using the ESPIRiT algorithm <ref type="bibr" target="#b19">[20]</ref> for coil combination.</p><p>Motion Artifacts Simulation. We simulate in-plane and through-plane motion according to the forward model ŷ = M FT θ x <ref type="bibr" target="#b1">[2]</ref>, where T θ ∈ R N ×N is the rigid-body motion matrix parameterized by a vector of translations and rotations</p><formula xml:id="formula_8">θ ∈ R 3 × [-π, π] 3 . M ∈ {0, 1} N ×N is the diagonal mask matrix in k-space.</formula><p>And we keep 7% of the k-space lines in the center for preventing excessive distortion of the images. The motion vectors are randomly selected from a Gaussian distribution N (0, 10). We follow the motion trajectories (i.e., piecewise constant, piecewise transient and Gaussian) used in the paper <ref type="bibr" target="#b4">[5]</ref> to simulate motion. In addition, to generate various motion severities, each motion level has a series of motion-corrupted k-space lines: 0-30%, 0-50%, and 0-70% of the total of k-space lines for mild, moderate, and severe, respectively. Finally, the motioncorrupted volume k-space data is cut into slice data and sent to the proposed D 2 MC-Net. Performance Evaluation. We compare the proposed D 2 MC-Net with four deep learning methods (i.e., U-Net <ref type="bibr" target="#b17">[18]</ref>, UPGAN <ref type="bibr" target="#b20">[21]</ref>, SU-Net <ref type="bibr" target="#b0">[1]</ref>, and Alternating <ref type="bibr" target="#b16">[17]</ref>), and an optimization-based method (i.e., Autofocusing+ <ref type="bibr" target="#b11">[12]</ref>). The motion-corrupted image without motion correction is denoted as "Corrupted".</p><p>In Table <ref type="table" target="#tab_0">1</ref>, we show the quantitative results of different methods under different motion trajectories and motion severities. Compared with "Corrupted", these deep learning methods improve the reconstruction performance. By explicitly estimating motion parameters, Autofocusing+ produces better results than deep learning methods. Our method achieves the best results in all experiments, mainly because the uncertainty-guided data consistency term is introduced into the unfolded deep network to guide MRI reconstruction. The qualitative comparison results under the severe corruption with piecewise constant motion trajectory are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. In comparison, our method has the smallest reconstruction error and recovers finer image details while suppressing undesired artifacts. The PSNR and SSIM values in Fig. <ref type="figure" target="#fig_1">2</ref> also demonstrate the superiority of our method. For example, the PSNR value of our method is 3.06 dB higher than that of SU-Net <ref type="bibr" target="#b0">[1]</ref>.  Effectiveness of the Key Components. We evaluate the effectiveness of these key components, including KU-Module, K-Block, and I-Block in Fig. <ref type="figure" target="#fig_0">1</ref>, under the moderate corruption with piecewise constant motion trajectory. In Table <ref type="table" target="#tab_2">2</ref>, (A) "Baseline" denotes the reconstruction model  <ref type="table" target="#tab_2">2</ref>, our results are better than all the compared variants, showing the effectiveness of the k-space uncertainty and dual-domain reconstruction. Compared with methods that do not use motion-corrupted k-space data (i.e., "Ours (w = 0)") and fully use motion-corrupted k-space data (i.e., "Ours (w = 1)") in reconstruction, our method selectively uses the motion-corrupted k-space data under the guidance of the learned k-space uncertainty w, and achieves higher performance. Visualization of the Uncertainty w. The estimated k-space uncertainties of all stages are visualized in Fig. <ref type="figure">3(b</ref>). As we can see, the averaged k-space uncertainty w avg over all stages approximates the real motion trajectory mask m uc with ones indicating the un-corrupted k-space lines.</p><formula xml:id="formula_9">x t = H I (x t-1 ; θ I )), t = 1 • • • T . (<label>B</label></formula><p>Effect of Different Loss Functions. We also investigate the effect of the kspace loss by adjusting the values of hyperparameters γ in Eq. <ref type="bibr" target="#b6">(7)</ref>. The PSNR results under the piecewise constant moderate motion are shown in Table <ref type="table">3</ref>. From these results, our method achieves the best performance at γ = 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed a novel dual domain motion correction network (D 2 MC-Net) to correct the motion artifacts in MRI. The D 2 MC-Net consists of KU-Modules and DDR-Modules. KU-Module measures the uncertainty of kspace data corrupted by motion. DDR-Module reconstructs the motion-free MR images in k-space and image domains under the guidance of the uncertainty estimated by KU-Module. Experiments on fastMRI dataset show the superiority of the proposed D 2 MC-Net. In the future work, we will extend the D 2 MC-Net to be a 3D motion correction method for 3D motion artifacts removal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of the proposed dual domain motion correction network, i.e. D 2 MC-Net. Each stage consists of two components, i.e., k-space uncertainty module (KU-Module) and dual domain reconstruction module (DDR-Module).</figDesc><graphic coords="3,71,97,197,24,308,32,117,16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative results of different methods under severe corruption with the piecewise constant motion trajectory.</figDesc><graphic coords="7,98,46,274,97,255,52,188,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) "w/o K-Blcok" denotes our D 2 MC-Net without K-Blocks. (C) "Ours (w = 0)" denotes our D 2 MC-Net without KU-Modules, and the k-space uncertainty w = 0. (D) "Ours (w = 1)" denotes our D 2 MC-Net without KU-Modules and the k-space uncertainty w = 1. (E) "Ours" is our full D 2 MC-Net equipped with KU-Modules, K-Blocks and I-Blocks. As shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .Table 3 .</head><label>33</label><figDesc>Fig. 3. (a) Qualitative results from different stages of the D 2 MC-Net under different severities with the piecewise constant motion trajectory. (b) The visual results of w from different stages under moderate piecewise constant motion.</figDesc><graphic coords="8,79,68,253,10,273,76,156,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison of different methods on fastMRI under different motion trajectories and motion severities, in PSNR (dB), SSIM and NRMSE.</figDesc><table><row><cell>Motion Trajectories Methods</cell><cell>Mild</cell><cell></cell><cell>Moderate</cell><cell>Severe</cell></row><row><cell></cell><cell>PSNR SSIM</cell><cell cols="2">NRMSE PSNR SSIM</cell><cell>NRMSE PSNR SSIM</cell><cell>NRMSE</cell></row><row><cell>Piecewise Constant Corrupted</cell><cell cols="2">33.76 0.9081 0.1344</cell><cell cols="2">30.71 0.8563 0.1895</cell><cell>28.52 0.8134 0.2366</cell></row><row><cell>U-Net</cell><cell cols="2">35.84 0.9571 0.1023</cell><cell cols="2">32.65 0.9345 0.1494</cell><cell>32.14 0.9168 0.1527</cell></row><row><cell>UPGAN</cell><cell cols="2">36.06 0.9537 0.0986</cell><cell cols="2">34.01 0.9287 0.1246</cell><cell>32.19 0.8781 0.1530</cell></row><row><cell>SU-Net</cell><cell cols="2">35.92 0.9541 0.1012</cell><cell cols="2">34.00 0.9378 0.1254</cell><cell>32.97 0.9241 0.1389</cell></row><row><cell>Alternating</cell><cell cols="2">37.08 0.9538 0.0879</cell><cell cols="2">34.51 0.9305 0.1186</cell><cell>32.33 0.9064 0.1506</cell></row><row><cell cols="3">Autofocusing+ 37.43 0.9559 0.0847</cell><cell cols="2">35.57 0.9356 0.1044</cell><cell>33.17 0.9115 0.1360</cell></row><row><cell>Ours</cell><cell>41.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>00 0.9761 0.0567 37.79 0.9594 0.0806 35.28 0.9399 0.1066</head><label></label><figDesc></figDesc><table><row><cell cols="2">Piecewise Transient Corrupted</cell><cell>32.78 0.8317 0.1407</cell><cell>30.04 0.7750 0.1934</cell><cell>28.56 0.7469 0.2301</cell></row><row><cell></cell><cell>U-Net</cell><cell>35.58 0.9511 0.1053</cell><cell>33.67 0.9338 0.1310</cell><cell>32.49 0.9217 0.1494</cell></row><row><cell></cell><cell>UPGAN</cell><cell>37.57 0.9526 0.0809</cell><cell>35.50 0.9339 0.1026</cell><cell>34.20 0.9220 0.1191</cell></row><row><cell></cell><cell>SU-Net</cell><cell>37.50 0.9540 0.0815</cell><cell>35.28 0.9363 0.1052</cell><cell>34.56 0.9335 0.1269</cell></row><row><cell></cell><cell>Alternating</cell><cell>37.09 0.9447 0.0854</cell><cell>35.15 0.9264 0.1068</cell><cell>34.02 0.9170 0.1217</cell></row><row><cell></cell><cell cols="2">Autofocusing+ 37.21 0.9415 0.0850</cell><cell>35.58 0.9271 0.1021</cell><cell>34.37 0.9138 0.1169</cell></row><row><cell></cell><cell>Ours</cell><cell cols="3">38.94 0.9607 0.0691 37.37 0.9493 0.0828 35.96 0.9380 0.0973</cell></row><row><cell>Gaussian</cell><cell>Corrupted</cell><cell>32.71 0.8293 0.1419</cell><cell>30.12 0.7749 0.1915</cell><cell>28.67 0.7444 0.2270</cell></row><row><cell></cell><cell>U-Net</cell><cell>34.78 0.9484 0.1174</cell><cell>33.83 0.9357 0.1299</cell><cell>34.05 0.9268 0.1222</cell></row><row><cell></cell><cell>UPGAN</cell><cell>36.25 0.9477 0.0938</cell><cell>35.94 0.9363 0.0974</cell><cell>34.42 0.9208 0.1160</cell></row><row><cell></cell><cell>SU-Net</cell><cell>37.06 0.9523 0.0864</cell><cell>34.92 0.9402 0.1100</cell><cell>34.49 0.9290 0.1169</cell></row><row><cell></cell><cell>Alternating</cell><cell>37.02 0.9432 0.0861</cell><cell>34.72 0.9194 0.1121</cell><cell>34.43 0.9196 0.1160</cell></row><row><cell></cell><cell cols="2">Autofocusing+ 37.75 0.9425 0.0792</cell><cell>35.50 0.9275 0.1033</cell><cell>34.67 0.9153 0.1129</cell></row><row><cell></cell><cell>Ours</cell><cell cols="3">39.40 0.9615 0.0654 37.58 0.9502 0.0807 36.21 0.9396 0.0945</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of the key components of D 2 MC-Net.</figDesc><table><row><cell>Methods</cell><cell>KU-Module K-Block I-Block PSNR SSIM</cell><cell>NRMSE</cell></row><row><cell>Baseline</cell><cell cols="2">34.53 0.9416 0.1172</cell></row><row><cell>w/o K-Block</cell><cell cols="2">36.17 0.9503 0.0968</cell></row><row><cell>Ours (w = 0)</cell><cell cols="2">35.18 0.9438 0.1089</cell></row><row><cell>Ours (w = 1)</cell><cell cols="2">36.84 0.9521 0.0880</cell></row><row><cell>Ours</cell><cell cols="2">37.79 0.9594 0.0806</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported by <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2022YFA1004201</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">12090021</rs>, <rs type="grantNumber">12125104</rs>, <rs type="grantNumber">61721002</rs>, <rs type="grantNumber">U20B2075</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6Xkz26V">
					<idno type="grant-number">2022YFA1004201</idno>
				</org>
				<org type="funding" xml:id="_JQGMcGR">
					<idno type="grant-number">12090021</idno>
				</org>
				<org type="funding" xml:id="_vuVC4pU">
					<idno type="grant-number">12125104</idno>
				</org>
				<org type="funding" xml:id="_M3BbEY6">
					<idno type="grant-number">61721002</idno>
				</org>
				<org type="funding" xml:id="_SVZ4HCJ">
					<idno type="grant-number">U20B2075</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stacked u-nets with self-assisted priors towards robust correction of rigid motion artifact in brain mri</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Masni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">259</biblScope>
			<biblScope unit="page">119411</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Blind retrospective motion correction of MR images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hannes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pohmannand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bernhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1608" to="1618" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MedGAN: medical image translation using GANs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Armanious</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">101684</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic correction of motion artifacts in magnetic resonance images using an entropy focus criterion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L G</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N R</forename><surname>Stoyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Keevil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="903" to="910" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Retrospective motion artifact correction of structural MRI images using deep learning improves the quality of cortical surface reconstructions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Ben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">230</biblScope>
			<biblScope unit="page">117756</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scout accelerated motion estimation and reduction (SAMER)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="178" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">fastMRI: a publicly available raw k-space and DICOM dataset of knee images for accelerated MR image reconstruction using machine learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiol. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Network accelerated motion estimation and reduction (NAMER): convolutional neural network guided retrospective motion correction using a separable motion model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Haskell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1452" to="1461" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Targeted motion estimation and reduction (TAMER): data consistency based motion mitigation for MRI using a reduced model joint optimization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Haskell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Cauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Wald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1253" to="1265" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Motion artifacts reduction in brain MRI by means of a deep residual network with densely connected multi-resolution blocks (DRN-DCMB)</title>
		<author>
			<persName><forename type="first">L</forename><surname>Junchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehmet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="69" to="79" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Prospective correction of affine motion for arbitrary MR sequences on a clinical scanner</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1130" to="1138" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Autofocusing+: Noise-resilient motion correction in magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kuzmina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Razumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Y</forename><surname>Rogov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adalsteinsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Dylov</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_35</idno>
		<idno>978-3-031-16446-0 35</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="365" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MC2-Net: motion correction network for multi-contrast brain MRI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1077" to="1092" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accelerated motion correction for MRI using score-based generative models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Levac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Tamir</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2211.00199" />
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sensitivity encoding for aligned multishot magnetic resonance reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Lucilio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P A G</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="266" to="280" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Motion artifacts in MRI: a complex problem with many partial solutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Maxim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="887" to="901" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint frequency and image space learning for MRI reconstruction and analysis</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adalsteinsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Biomed. Imaging</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Retrospective correction of motion-affected MR images using deep learning frameworks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiahuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sergios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1527" to="1540" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ESPIRiT-an eigenvalue approach to autocalibrating parallel MRI: where sense meets grappa</title>
		<author>
			<persName><forename type="first">M</forename><surname>Uecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="990" to="1001" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Uncertainty-guided progressive GANs for medical image translation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gatidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_58</idno>
		<idno>978-3-030-87199-4 58</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="614" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MR image artifacts from periodic motion</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Henkelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="151" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
