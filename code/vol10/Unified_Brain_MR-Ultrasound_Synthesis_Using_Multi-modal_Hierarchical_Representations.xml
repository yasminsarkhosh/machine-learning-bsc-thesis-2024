<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations</title>
				<funder ref="#_d6ykTxZ">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_xUF8JXK">
					<orgName type="full">Wellcome/EPSRC</orgName>
				</funder>
				<funder>
					<orgName type="full">McMahon Family Brain Tumor Research Fund</orgName>
				</funder>
				<funder ref="#_QyK8jpQ #_FKyTPfj #_n8TnA5J #_XNvHaNB">
					<orgName type="full">National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Reuben</forename><surname>Dorent</surname></persName>
							<email>rdorent@bwh.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nazim</forename><surname>Haouchine</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fryderyk</forename><surname>Kogl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Joutard</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Parikshit</forename><surname>Juvekar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erickson</forename><surname>Torio</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandra</forename><forename type="middle">J</forename><surname>Golby</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastien</forename><surname>Ourselin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sarah</forename><surname>Frisken</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Vercauteren</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tina</forename><surname>Kapur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">M</forename><surname>Wells</surname><genName>III</genName></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="448" to="458"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">9A31E6AEF7A9276133F49A46247E5C3C</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_43</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Variational Auto-Encoder</term>
					<term>Ultrasound</term>
					<term>Brain Resection</term>
					<term>Image Synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce MHVAE, a deep hierarchical variational autoencoder (VAE) that synthesizes missing images from various modalities. Extending multi-modal VAEs with a hierarchical latent structure, we introduce a probabilistic formulation for fusing multi-modal images in a common latent representation while having the flexibility to handle incomplete image sets as input. Moreover, adversarial learning is employed to generate sharper images. Extensive experiments are performed on the challenging problem of joint intra-operative ultrasound (iUS) and Magnetic Resonance (MR) synthesis. Our model outperformed multi-modal VAEs, conditional GANs, and the current state-of-the-art unified method (ResViT) for synthesizing missing images, demonstrating the advantage of using a hierarchical latent representation and a principled probabilistic fusion operation. Our code is publicly available (https://github.com/ReubenDo/MHVAE).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical imaging is essential during diagnosis, surgical planning, surgical guidance, and follow-up for treating brain pathology. Images from multiple modalities are typically acquired to distinguish clinical targets from surrounding tissues. For example, intra-operative ultrasound (iUS) imaging and Magnetic Resonance Imaging (MRI) capture complementary characteristics of brain tissues that can be used to guide brain tumor resection. However, as noted in <ref type="bibr" target="#b30">[30]</ref>, multi-modal data is expensive and sparse, typically leading to incomplete sets of images. For example, the prohibitive cost of intra-operative MRI (iMRI) scanners often hampers the acquisition of iMRI during surgical procedures. Conversely, iUS is an affordable tool but has been perceived as difficult to read compared to iMRI <ref type="bibr" target="#b4">[5]</ref>. Consequently, there is growing interest in synthesizing missing images from a subset of available images for enhanced visualization and clinical training.</p><p>Medical image synthesis aims to predict missing images given available images. Deep-learning based methods have reached the highest level of performance <ref type="bibr" target="#b29">[29]</ref>, including conditional generative adversarial (GAN) models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref> and conditional variational auto-encoders <ref type="bibr" target="#b2">[3]</ref>. However, a key limitation of these techniques is that they must be trained for each subset of available images.</p><p>To tackle this challenge, unified approaches have been proposed. These approaches are designed to have the flexibility to handle incomplete image sets as input, improving practicality as only one network is used for generating missing images. To handle partial inputs, some studies proposed to fill missing images with arbitrary values <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>. Alternatively, other work aim at creating a common feature space that encodes shared information from different modalities. Feature representations are extracted independently for each modality. Then, arithmetic operations (e.g., mean <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref>, max <ref type="bibr" target="#b1">[2]</ref> or a combination of sum, product and max <ref type="bibr" target="#b32">[32]</ref>) are used to fuse these feature representations. However, these operations do not force the network to learn a shared latent representation of multi-modal data and lack theoretical foundations. In contrast, Multi-modal Variational Auto-Encoders (MVAEs) provide a principled probabilistic fusion operation to create a common representation space <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">30]</ref>. In MVAEs, the common representation space is low-dimensional (e.g., R 256 ), which usually leads to blurry synthetic images. In contrast, hierarchical VAEs (HVAEs) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> allow for learning complex latent representations by using a hierarchical latent structure, where the coarsest latent variable (z L ) represents global features, as in MVAEs, while the finer variables capture local characteristics. However, HVAEs</p><p>have not yet been extended to multi-modal settings to synthesize missing images.</p><p>In this work, we introduce Multi-Modal Hierarchical Latent Representation VAE (MHVAE), the first multi-modal VAE approach with a hierarchical latent representation for unified medical image synthesis. Our contribution is four-fold.</p><p>First, we integrate a hierarchical latent representation into the multi-modal variational setting to improve the expressiveness of the model. Second, we propose a principled fusion operation derived from a probabilistic formulation to support missing modalities, thereby enabling image synthesis. Third, adversarial learning is employed to generate realistic image synthesis. Finally, experiments on the challenging problem of iUS and MR synthesis demonstrate the effectiveness of the proposed approach, enabling the synthesis of high-quality images while establishing a mathematically grounded formulation for unified image synthesis and outperforming non-unified GAN-based approaches and the state-of-the-art method for unified multi-modal medical image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Variational Auto-Encoders (VAEs). The goal of VAEs <ref type="bibr" target="#b15">[16]</ref> is to train a generative model in the form of p(x, z) = p(z)p(x|z) where p(z) is a prior distribution (e.g. isotropic Normal distribution) over latent variables z ∈ R H and where p θ (x|z) is a decoder parameterized by θ that reconstructs data x ∈ R N given z. The latent space dimension H is typically much lower than the image space dimension N , i.e. H N . The training goal with respect to θ is to maximize the marginal likelihood of the data p θ (x) (the "evidence"); however since the true posterior p θ (z|x) is in general intractable, the variational evidence lower bound (ELBO) is instead optimized. The ELBO L VAE (x; θ, φ) is defined by introducing an approximate posterior q φ (z|x) with parameters φ:</p><formula xml:id="formula_0">L VAE (x; θ, φ) := E q φ (z|x) [log(p θ (x|z))] -KL[q φ (z|x)||p(z)] ,<label>(1)</label></formula><p>where KL[q||p] is the Kullback-Leibler divergence between distributions q and p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-modal Variational Auto-Encoders (MVAE).</head><p>Multi-modal VAEs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">30]</ref> introduced a principled probabilistic formulation to support missing data at training and inference time. Multi-modal VAEs assume that M paired images x = (x 1 , ..., x M ) ∈ R M ×N are conditionally independent given a shared representation z as higlighted in Fig. <ref type="figure" target="#fig_0">1</ref>, i.e. p θ (x|z) = M i=1 p(x i |z). Instead of training one single variational network q φ (z|x) that requires all images to be presented at all times, MVAEs factorize the approximate posterior as a combination of unimodal variational posteriors (q φ (z|x i )) M i=1 . Given any subset of modalities π ⊆ {1, ..., M }, MVAEs have the flexibility to approximate the π-marginal posteriors p(z|(x i ) i∈π ) using the |π| unimodal variational posteriors (q φ (z|x i )) i∈π . MVAE <ref type="bibr" target="#b30">[30]</ref> and U-HVED <ref type="bibr" target="#b7">[8]</ref> factorize the π-marginal variational posterior as a product-of-experts (PoE), i.e.:</p><formula xml:id="formula_1">q PoE φ (z|x π ) = p(z) i∈π q φ (z|x i ) .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this paper, we propose a deep multi-modal hierarchical VAE called MHVAE that synthesizes missing images from available images. MHVAE's design focuses on tackling three challenges: (i) improving expressiveness of VAEs and MVAEs using a hierarchical latent representation; (ii) parametrizing the variational posterior to handle missing modalities; (iii) synthesizing realistic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Latent representation</head><formula xml:id="formula_2">Let x = (x i ) M i=1 ∈ R M ×N</formula><p>be a complete set of paired (i.e. co-registered) images of different modalities where M is the total number of image modalities and N the number of pixels (e.g. M = 2 for T 2 MRI and iUS synthesis). The images x i are assumed to be conditionally independent given a latent variable z. Then, the conditional distribution p θ (x|z) parameterized by θ can be written as:</p><formula xml:id="formula_3">p θ (x|z) = M i=1 p θ (x i |z) . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>Given that VAEs and MVAEs typically produce blurry images, we propose to use a hierarchical representation of the latent variable z to increase the expressiveness the model as in HVAEs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Specifically, the latent variable z is partitioned into disjoint groups, as shown in Fig. <ref type="figure" target="#fig_0">1</ref> i.e. z = {z 1 , ...z L }, where L is the number of groups. The prior p(z) is then represented by:</p><formula xml:id="formula_5">p θ (z) = p(z L ) L-1 l=1 p θ l (z l |z &gt;l ) , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where p(z L ) = N (z L ; 0, I) is an isotropic Normal prior distribution and the conditional prior distributions p θ l (z l |z &gt;l ) are factorized Normal distributions with diagonal covariance parameterized using neural networks, i.e. p θ l (z l |z &gt;l ) = N (z l ; μ θ l (z &gt;l ), D θ l (z &gt;l )). Note that the dimension of the finest latent variable z 1 ∈ R H1 is similar to number of pixels, i.e. H 1 = O(N ) and the dimension of the latent representation exponentially decreases with the depth, i.e. H L H 1 . Reusing Eq. 1, the evidence log (p θ (x)) is lower-bounded by the tractable variational ELBO L ELBO MHVAE (x; θ, φ):</p><formula xml:id="formula_7">L ELBO MHVAE (x; θ, φ) = M i=1 E q φ (z|x) [log(p θ (x i |z))] -KL [q φ (z L |x)||p(z L )] - L-1 l=1 E q φ (z &gt;l |x) [KL[q φ (z l |x, z &gt;l )||p θ (z l |z &gt;l )]]<label>(5)</label></formula><p>where q φ (z|x) = L l=1 q φ (z l |x, z &gt;l ) is a variational posterior that approximates the intractable true posterior p θ (z|x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Variational Posterior's Parametrization for Incomplete Inputs</head><p>To synthesize missing images, the variational posterior (q φ (z l |x, z &gt;l )) L l=1 should handle missing images. We propose to parameterize it as a combination of unimodal variational posteriors. Similarly to MVAEs, for any set π ⊆ {1, ..., M } of images, the conditional posterior distribution at the coarsest level L is expressed</p><formula xml:id="formula_8">q PoE φL (z L |x π ) = p(z L ) i∈π q φ i L (z|x i ) . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where p(z L ) = N (z L ; 0, I) is an isotropic Normal prior distribution and q φL (z|x i ) is a Normal distribution with diagonal covariance parameterized using CNNs.</p><p>For the other levels l ∈ {1, .., L -1}, we similarly propose to express the conditional variational posterior distributions as a product-of-experts:</p><formula xml:id="formula_10">q PoE φ l ,θ l (z l |x π , z &gt;l ) = p θ l (z l |z &gt;l ) i∈π q φ i l (z l |x i , z &gt;l )<label>(7)</label></formula><p>where q φ i l (z l |x i , z &gt;l ) is a Normal distribution with diagonal covariance parameterized using CNNs, i.e.</p><formula xml:id="formula_11">q φ i l (z l |x i , z &gt;l ) = N (z l ; μ φ i l (x i , z &gt;l ); D φ i l (x i , z &gt;l )</formula><p>). This formulation allows for a principled operation to fuse content information from available images while having the flexibility to handle missing ones. Indeed, at each level l ∈ {1, ..., L}, the conditional variational distributions q PoE φ l ,θ l (z l |x π , z &gt;l ) are Normal distributions with mean μ φ l ,θ l (x π , z &gt;l ) and diagonal covariance D φ l ,θ l (x π , z &gt;l ) expressed in closed-form solution <ref type="bibr" target="#b11">[12]</ref> as:</p><formula xml:id="formula_12">⎧ ⎪ ⎨ ⎪ ⎩ D φ l ,θ l (xπ, z &gt;l ) = D -1 θ l (z &gt;l ) + i∈π D -1 φ i l (x i , z &gt;l ) -1 µ φ l ,θ l (xπ, z &gt;l ) = D -1 φ l ,θ l (xπ, z &gt;l ) D -1 θ l (z &gt;l )µ θ l (z &gt;l ) + i∈π D -1 φ i l (x i , z &gt;l )µ φ i l (x i , z &gt;l )</formula><p>with D θL (z &gt;L ) = I and μ θL (z &gt;L ) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization Strategy for Image Synthesis</head><p>The joint reconstruction and synthesis optimization goal is to maximize the expected evidence E x∼p data [log(p(x))]. As the ELBO defined in Eq. 5 is valid for any approximate distribution q, the evidence, log(p θ (x)), is in particular lowerbounded by the following subset-specific ELBO for any subset of images π:</p><formula xml:id="formula_13">L ELBO MAVAE (x π ; θ, φ) = M i=1 E q φ (z|xπ) [log(p θ (x i |z 1 ))] reconstruction -KL [q φL (z L |x π )||p(z L )] - L-1 l=1 E q φ l ,θ l (z &gt;l |xπ) [KL[q φ l ,θ l (z l |x π , z &gt;l )||p θ l (z l |z &gt;l )]] .<label>(8)</label></formula><p>Hence, the expected evidence E x∼p data [log(p(x))] is lower-bounded by the average of the subset-specific ELBO, i.e.:</p><formula xml:id="formula_14">L MHVAE := 1 |P| π∈P L ELBO MAVAE (x π ; θ, φ) . (9)</formula><p>Consequently, we propose to average all the subset-specific losses at each training iteration. The image decoding distributions are modelled as Normal with variance σ, i.e. p θ (x i |z 1 ) = N (x i ; μ i (z 1 ), σI), leading to reconstruction losses log(p θ (x i |z 1 )), which are proportional to ||x iμ i (z 1 )|| 2 . To generate sharper images, the L 2 loss is replaced by a combination of L 1 loss and GAN loss via a PatchGAN discriminator <ref type="bibr" target="#b13">[14]</ref>. Moreover, the expected KL divergences are estimated with one sample as in <ref type="bibr" target="#b18">[19]</ref>. Finally, the loss associated with the subsetspecific ELBOs Eq. ( <ref type="formula">9</ref>) is:</p><formula xml:id="formula_15">L = N i=1 (λ L1 L 1 (μ i , x i ) + λ GAN L GAN (μ i )) + KL .</formula><p>Following standard practices <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>, images are normalized in [-1, 1] and the weights of the L 1 and GAN losses are set to λ L1 = 100 and λ GAN = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we report experiments conducted on the challenging problem of MR and iUS image synthesis.</p><p>Data. We evaluated our method on a dataset of 66 consecutive adult patients with brain gliomas who were surgically treated at the Brigham and Women's hospital, Boston USA, where both pre-operative 3D T2-SPACE and pre-dural opening intraoperative US (iUS) reconstructed from a tracked handheld 2D probe were acquired. The data will be released on TCIA in 2023. 3D T2-SPACE scans were affinely registered with the pre-dura iUS using NiftyReg <ref type="bibr" target="#b19">[20]</ref> following the pipeline described in <ref type="bibr" target="#b9">[10]</ref>. Three neurological experts manually checked registration outputs. The dataset was randomly split into a training set (N = 56) and a testing set (N = 10). Images were resampled to an isotropic 0.5 mm resolution, padded for an in-plane matrix of <ref type="bibr">(192,</ref><ref type="bibr">192)</ref>  using SPADE <ref type="bibr" target="#b20">[21]</ref>, Pix2Pix <ref type="bibr" target="#b13">[14]</ref>, MVAE <ref type="bibr" target="#b30">[30]</ref>, ResViT <ref type="bibr" target="#b3">[4]</ref> and MHVAE (ours) without and with GAN loss. As highlighted by the arrows, our approach better preserves anatomy compared to GAN-based approach and produces more realistic approach than the transformer-based approach (ResViT).</p><p>for the encoder and decoder from MobileNetV2 <ref type="bibr" target="#b22">[23]</ref> are used with Squeeze and Excitation <ref type="bibr" target="#b12">[13]</ref> and Swish activation. The image decoders (μ i ) M i=1 correspond to 5 ResNet blocks. Following state-of-the-art bidirectional inference architectures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref>, the representations extracted in the contracting path (from x i to (z l ) l ) and the expansive path (from z L to x i and (z l ) l&lt;L ) are partially shared. Models are trained for 1000 epochs with a batch size of 16. To improve convergence, λ GAN is set to 0 for the first 800 epochs. Network architecture is presented in Appendix, and the code is available at https://github.com/ReubenDo/ MHVAE.</p><p>Evaluation. Since paired data was available for evaluation, standard supervised evaluation metrics are employed: PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity), and LPIPS <ref type="bibr" target="#b31">[31]</ref> (Learned Perceptual Image Patch Similarity). Quantitative results are presented in Table <ref type="table" target="#tab_1">1</ref>, and qualitative results are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Wilcoxon signed rank tests (p &lt; 0.01) were performed.</p><p>Ablation Study. To quantify the importance of each component of our approach, we conducted an ablation study. First, our model (MHVAE) was compared with MVAE, the non-hierarchical multi-modal VAE described in <ref type="bibr" target="#b30">[30]</ref>. It can be observed in Table <ref type="table" target="#tab_1">1</ref> that MHVAE (ours) significantly outperformed MVAE. This highlights the benefits of introducing a hierarchy in the latent representation. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, MVAE generated blurry images, while our approach produced sharp and detailed synthetic images. Second, the impact of the GAN loss was evaluated by comparing our model with (λ GAN = 0) and without (λ GAN = 1) the adversarial loss. Both models performed similarly in terms of evaluation metrics. However, as highlighted in Fig. <ref type="figure" target="#fig_1">2</ref>, adding the GAN loss led to more realistic textures with characteristic iUS speckles on synthetic iUS. Finally, the image similarity between the target and reconstructed images (i.e., target image used as input) was excellent, as highlighted in Table <ref type="table" target="#tab_1">1</ref>. This shows that the learned latent representations preserved the content information from input modalities. State-of-the-Art Comparison. To evaluate the performance of our model (MHVAE) against existing image synthesis frameworks, we compared it to two state-of-the-art GAN-based conditional image synthesis methods: Pix2Pix <ref type="bibr" target="#b13">[14]</ref> and SPADE <ref type="bibr" target="#b20">[21]</ref>. These models have especially been used as synthesis backbones in previous MR/iUS synthesis studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>. Results in Table <ref type="table" target="#tab_1">1</ref> show that our approach statistically outperformed these GAN methods with and without adversarial learning. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, these conditional GANs produced realistic images but did not preserve the brain anatomy. Given that these models are not unified, Pix2Pix and SPADE must be trained for each synthesis direction (T 2 → iUS and iUS → T 2 ). In contrast, MHVAE is a unified approach where one model is trained for both synthesis directions, improving inference practicality without a drop in performance. Finally, we compared our approach with ResViT [4], a transformer-based method that is the current state-of-the-art for unified multi-modal medical image synthesis. Our approach outperformed or reached similar performance depending on the metric. In particular, as shown in Fig. <ref type="figure" target="#fig_1">2</ref> and in Table <ref type="table" target="#tab_1">1</ref> for the perceptual LPIPS metric, our GAN model synthesizes images that are visually more similar to the target images. Finally, our approach demonstrates significantly lighter computational demands when compared to the current SOTA unified image synthesis framework (ResViT), both in terms of time complexity (8G MACs vs. 487G MACs) and model size (10M vs. 293M parameters). Compared to MVAEs, our hierarchical multi-modal approach only incurs a marginal increase in time complexity (19%) and model size (4%).</p><p>Overall, this set of experiments demonstrates that variational auto-encoders with hierarchical latent representations, which offer a principled formulation for fusing multi-modal images in a shared latent representation, are effective for image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>Other Potential Applications. The current framework enables the generation of iUS data using T 2 MRI data. Since image delineation is much more efficient on MRI than on US, annotations performed on MRI could be used to train a segmentation network on pseudo-iUS data, as performed by the top-performing teams in the crossMoDA challenge <ref type="bibr">[9]</ref>. For example, synthetic ultrasound images could be generated from the BraTS dataset <ref type="bibr" target="#b0">[1]</ref>, the largest collection of annotated brain tumor MR scans. Qualitative results shown in Appendix demonstrate the ability of our approach to generalize well to T 2 imaging from BraTS. Finally, the synthetic images could be used for improved iUS and T 2 image registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work.</head><p>We introduced a multi-modal hierarchical variational auto-encoder to perform unified MR/iUS synthesis. By approximating the true posterior using a combination of unimodal approximates and optimizing the ELBO with multi-modal and uni-modal examples, MHVAE demonstrated state-of-the-art performance on the challenging problem of iUS and MR synthesis. Future work will investigate synthesizing additional imaging modalities such as CT and other MR sequences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Graphical models of: (a) variational auto-encoder (VAE); (b) hierarchical VAE (HVAE); (c) Multi-modal VAE (MVAE); (d) Multi-Modal Hiearchical VAE (Ours).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2.Examples of image synthesis (rows 1 and 2: iUS → T2; rows 3 and 4: T2 → iUS) using SPADE<ref type="bibr" target="#b20">[21]</ref>, Pix2Pix<ref type="bibr" target="#b13">[14]</ref>, MVAE<ref type="bibr" target="#b30">[30]</ref>, ResViT<ref type="bibr" target="#b3">[4]</ref> and MHVAE (ours) without and with GAN loss. As highlighted by the arrows, our approach better preserves anatomy compared to GAN-based approach and produces more realistic approach than the transformer-based approach (ResViT).</figDesc><graphic coords="7,45,93,54,62,333,40,187,09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, and normalized in [-1, 1].</figDesc><table><row><cell>Implementation Details. Since raw brain ultrasound images are typically</cell></row><row><cell>2D, we employed a 2D U-Net-based architecture. The spatial resolution and</cell></row><row><cell>the feature dimension of the coarsest latent variable (z</cell></row></table><note><p><p><p>L ) were set to 1 × 1 and 256. The spatial and feature dimensions are respectively doubled and halved after each level to reach a feature representation of dimension 8 for each pixel, i.e. z 1 ∈ R 196×196×8 and z L ∈ R 1×1×256 . This leads to 7 latent variable levels, i.e. L = 7. Following state-of-the-art NVAE architecture</p><ref type="bibr" target="#b26">[27]</ref></p>, residual cells</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison against the state-of-the-art conditional GAN models for image synthesis. Available modalities are denoted by •, the missing ones by •. Mean and standard deviation values are presented. * denotes significant improvement provided by a Wilcoxon test (p &lt; 0.01). Arrows indicate favorable direction of each metric.</figDesc><table><row><cell></cell><cell cols="2">Input</cell><cell></cell><cell>iUS</cell><cell></cell><cell></cell><cell>T2</cell><cell></cell></row><row><cell></cell><cell cols="3">iUS T2 PSNR(dB)↑</cell><cell cols="2">SSIM(%)↑ LPIPS(%)↓</cell><cell>PSNR(dB)↑</cell><cell>SSIM(%)↑</cell><cell>LPIPS(%)↓</cell></row><row><cell>MHVAE (λGAN = 0)</cell><cell>•</cell><cell>•</cell><cell>33.15 (2.48)</cell><cell>91.3 (3.5)</cell><cell>6.3 (2.3)</cell><cell>36.38 (2.40)</cell><cell>95.3 (1.9)</cell><cell>2.2 (0.8)</cell></row><row><cell>MHVAE (λGAN = 1)</cell><cell>•</cell><cell>•</cell><cell>31.54 (2.62)</cell><cell>89.1 (4.3)</cell><cell>7.1 (2.6)</cell><cell>34.35 (2.67)</cell><cell>93.6 (2.7)</cell><cell>2.8 (1.2)</cell></row><row><cell cols="2">Pix2Pix [14] T2 → iUS •</cell><cell>•</cell><cell>20.31 (3.78)</cell><cell>70.2 (12.0)</cell><cell>19.8 (5.7)</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell cols="2">SPADE [21] T2 → iUS •</cell><cell>•</cell><cell>20.30 (3.62)</cell><cell>70.1 (12.1)</cell><cell>21.5 (6.9)</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>MVAE [30]</cell><cell>•</cell><cell>•</cell><cell>21.21 (4.20)</cell><cell cols="2">73.5 (10.9) 26.9 (10.5)</cell><cell>23.23 (4.55)</cell><cell>83.4 (8.1)</cell><cell>21.4 (9.0)</cell></row><row><cell>ResViT [4]</cell><cell>•</cell><cell>•</cell><cell cols="3">21.22 (3.10) 75.2* (9.7) 24.0 (7.5)</cell><cell>37.14 (5.94)</cell><cell>99.1 (0.9)</cell><cell>1.0 (0.5)</cell></row><row><cell>MHVAE (λGAN = 0)</cell><cell>•</cell><cell cols="3">• 21.87* (4.06) 74.9 (10.4)</cell><cell>24.2 (9.1)</cell><cell>36.41 (2.13)</cell><cell>95.5 (1.8)</cell><cell>7.2 (3.0)</cell></row><row><cell>MHVAE (λGAN = 1)</cell><cell>•</cell><cell>•</cell><cell>21.26 (3.93)</cell><cell cols="3">71.9 (11.4) 19.0* (7.6) 34.94 (2.27)</cell><cell>94.4 (2.3)</cell><cell>7.6 (3.2)</cell></row><row><cell cols="2">Pix2Pix [14] iUS → T2 •</cell><cell>•</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>21.01 (3.70)</cell><cell>77.9 (9.2)</cell><cell>17.4 (4.7)</cell></row><row><cell cols="2">SPADE [21] iUS → T2 •</cell><cell>•</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>20.12 (3.20)</cell><cell>74.3 (8.5)</cell><cell>18.6 (3.8)</cell></row><row><cell>MVAE [30]</cell><cell>•</cell><cell>•</cell><cell>23.02 (4.12)</cell><cell>75.3 (10.4)</cell><cell>25.5 (9.9)</cell><cell>21.70 (4.60)</cell><cell>82.6 (8.2)</cell><cell>21.7 (9.1)</cell></row><row><cell>ResViT [4]</cell><cell>•</cell><cell>•</cell><cell>35.09 (3.96)</cell><cell>97.6 (1.0)</cell><cell>3.5 (1.2)</cell><cell cols="2">21.70 (3.40) 82.8* (7.6)</cell><cell>18.9 (6.8)</cell></row><row><cell>MHVAE (λGAN = 0)</cell><cell>•</cell><cell>•</cell><cell>33.07 (2.34)</cell><cell>91.3 (3.4)</cell><cell cols="3">13.2 (4.8) 22.16* (4.13) 82.8* (8.0)</cell><cell>18.3 (7.6)</cell></row><row><cell>MHVAE (λGAN = 1)</cell><cell>•</cell><cell>•</cell><cell>31.58 (2.26)</cell><cell>90.8 (3.6)</cell><cell>12.0 (4.4)</cell><cell>22.12 (4.28)</cell><cell cols="2">81.7 (8.2) 17.4* (7.3))</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">National Institutes of Health</rs> (<rs type="grantNumber">R01EB032387</rs>, <rs type="grantNumber">R01EB027134</rs>, <rs type="grantNumber">P41EB028741</rs>, <rs type="grantNumber">R03EB032050</rs>), the <rs type="funder">McMahon Family Brain Tumor Research Fund</rs> and by core funding from the <rs type="funder">Wellcome/EPSRC</rs> [<rs type="grantNumber">WT203148/Z/16/Z</rs>; <rs type="grantNumber">NS/A000049/1</rs>]. For the purpose of open access, the authors have applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QyK8jpQ">
					<idno type="grant-number">R01EB032387</idno>
				</org>
				<org type="funding" xml:id="_FKyTPfj">
					<idno type="grant-number">R01EB027134</idno>
				</org>
				<org type="funding" xml:id="_n8TnA5J">
					<idno type="grant-number">P41EB028741</idno>
				</org>
				<org type="funding" xml:id="_XNvHaNB">
					<idno type="grant-number">R03EB032050</idno>
				</org>
				<org type="funding" xml:id="_xUF8JXK">
					<idno type="grant-number">WT203148/Z/16/Z</idno>
				</org>
				<org type="funding" xml:id="_d6ykTxZ">
					<idno type="grant-number">NS/A000049/1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 43.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimodal mr synthesis via modality-invariant latent representation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chartsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="803" to="814" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Disentangled representation learning in cardiac image analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chartsias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101535</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Resvit: residual vision transformers for multimodal medical image synthesis</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dalmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2022.3167808</idno>
		<ptr target="https://doi.org/10.1109/TMI.2022.3167808" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2598" to="2614" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Intraoperative ultrasound in brain tumor surgery: a review and implementation guide</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grech-Sollars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurosurg. Rev</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2503" to="2515" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Realistic synthesis of brain tumor resection ultrasound images with a generative adversarial network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Donnez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Carton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Le Lann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Schlichting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chabanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotic Interventions, and Modeling</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="637" to="642" />
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning joint segmentation of tissues and brain lesions from task-specific hetero-modal domain-shifted datasets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dorent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101862</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hetero-modal variational encoder-decoder for joint modality completion and segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dorent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joutard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-89" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crossmoda 2021 challenge: benchmark of cross-modality domain adaptation techniques for vestibular schwannoma and cochlea segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dorent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kujawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ivory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rieke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">102628</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Registration of MRI and iUS data to compensate brain shift using a symmetric block-matching based approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Drobny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CuRIOUS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">HeMIS: hetero-modal image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guizard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_54</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-854" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Balancing flexibility and robustness in machine learning: semi-parametric methods and sparse linear models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appendix C</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Image-To-Image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised ultrasound to mri fetal brain image synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I L</forename><surname>Namburete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Papageorghiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2020.3018560</idno>
		<ptr target="https://doi.org/10.1109/TMI.2020.3018560" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4413" to="4424" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Collagan: collaborative gan for missing image data imputation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2487" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DiamondGAN: unified multi-modal generative adversarial networks for MRI sequences synthesis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_87</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-987" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="795" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">BIVA: a very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liévin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast free-form deformation using graphics processing units</title>
		<author>
			<persName><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Prog. Biomed</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="278" to="224" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical variational models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">MobileNetV2: inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Missing MRI pulse sequence synthesis using multimodal generative adversarial network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1170" to="1183" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Variational mixture-of-experts autoencoders for multi-modal deep generative models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ladder variational autoencoders</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">NVAE: a deep hierarchical variational autoencoder</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">PIMMS: permutation invariant multi-modal segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Varsavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Eaton-Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<editor>Stoyanov, D., et al.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<idno type="DOI">10.1007/978-3-030-00889-5_23</idno>
		<idno>DLMIA/ML-CDS -2018</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-523" />
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="201" to="209" />
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A review on medical imaging synthesis using deep learning and its clinical applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Clin. Med. Phys</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="36" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multimodal generative models for scalable weaklysupervised learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hi-net: hybrid-fusion network for multi-modal mr image synthesis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2772" to="2781" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
