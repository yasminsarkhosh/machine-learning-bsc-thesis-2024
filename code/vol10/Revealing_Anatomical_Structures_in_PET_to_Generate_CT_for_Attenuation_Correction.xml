<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction</title>
				<funder ref="#_yAzjwdv #_gnfnQZ5">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_Xn5fMXM">
					<orgName type="full">The Key R&amp;D Program of Guangdong Province, China</orgName>
				</funder>
				<funder ref="#_Gzf84Rb">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_A2vUN5h">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
					<orgName type="abbreviated">STCSM</orgName>
				</funder>
				<funder ref="#_JVrGMG5">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yongsheng</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feihong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Northwest University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Caiwen</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Xia</surname></persName>
							<email>yxia@nwpu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<email>dgshen@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="24" to="33"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F9E7A319CF4F461ACF4C797E7247CFEA</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>PET</term>
					<term>Attenuation correction</term>
					<term>CT</term>
					<term>Image generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Positron emission tomography (PET) is a molecular imaging technique relying on a step, namely attenuation correction (AC), to correct radionuclide distribution based on pre-determined attenuation coefficients. Conventional AC techniques require additionally-acquired computed tomography (CT) or magnetic resonance (MR) images to calculate attenuation coefficients, which increases imaging expenses, time costs, or radiation hazards to patients, especially for whole-body scanners. In this paper, considering technological advances in acquiring more anatomical information in raw PET images, we propose to conduct attenuation correction to PET by itself. To achieve this, we design a deep learning based framework, namely anatomical skeleton-enhanced generation (ASEG), to generate pseudo CT images from non-attenuation corrected PET images for attenuation correction. Specifically, ASEG contains two sequential modules, i.e., a skeleton prediction module and a tissue rendering module. The former module first delineates anatomical skeleton and the latter module then renders tissue details. Both modules are trained collaboratively with specific anatomical-consistency constraint to guarantee tissue generation fidelity. Experiments on four public PET/CT datasets demonstrate that our ASEG outperforms existing methods by achieving better consistency of anatomical structures in generated CT images, which are further employed to conduct PET attenuation correction with better similarity to real ones. This work verifies the feasibility of generating pseudo CT from raw PET for attenuation correction without acquising additional images. The associated implementation is available at https://github.com/YongshengPan/ASEG-for-PET2CT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Positron emission tomography (PET) is a general nuclear imaging technique, which has been widely used to characterize tissue metabolism, protein deposition, etc. <ref type="bibr" target="#b8">[9]</ref>. According to the PET imaging principle, radioactive tracers injected into the body involve in the metabolism and produce γ decay signals externally. However, due to photoelectric absorption and Compton scattering, the decay signals are attenuated when passing through human tissues to external receivers, resulting in incorrect tracer distribution reasoning (see non-attenuation corrected PET (NAC-PET) in Fig. <ref type="figure" target="#fig_0">1(a)</ref>). To obtain correct tracer distribution (see AC-PET in Fig. <ref type="figure" target="#fig_0">1(a)</ref>), attenuation correction (AC) on the received signals is required. Traditional AC accompanies additional costs caused by the simultaneously obtained MR or CT images which are commonly useless for diagnosis. The additional costs are especially significant for advanced total-body PET/CT scanners <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, which have effective sensitivity and low radiation dose during PET scanning but accumulative radiation dose during CT scanning. In another word, CT becomes a non-negligible source of radiation hazards. To reduce the costs, including expense, time, and radiation hazards, some studies proposed to conduct AC by exploiting each PET image itself. Researchers have been motivated to generate pseudo CT images from NAC-PET images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>, or more directly, to generate AC-PET images from NAC-PET images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>. Since pseudo CT is convenient to be integrated into conventional AC processes, generating pseudo CT images is feasible in clinics for AC.</p><p>The pseudo CT images should satisfy two-fold requests. Firstly, the pseudo CT images should be visually similar in anatomical structures to corresponding actual CT images. Secondly, PET images corrected by pseudo CT images should be consistent with that corrected by actual CT images. However, current techniques of image generation tend to produce statistical average values and patterns, which easily erase significant tissues (e.g., bones and lungs). As a result, for those tissues with relatively similar metabolism but large variances in attenuation coefficient, these methods could cause large errors as they are blind to the correct tissue distributions. Therefore, special techniques should be investigated to guarantee the fidelity of anatomical structures in these generated pseudo CT images.</p><p>In this paper, we propose a deep learning framework, named anatomical skeleton enhanced generation (ASEG), to generate pseudo CT from NAC-PET for attenuation correction. ASEG focuses more on the fidelity of tissue distribution, i.e., anatomical skeleton, in pseudo CT images. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b), this framework contains two sequential modules structure prediction module G 1 and tissue rendering module G 2 }. G 1 devotes to delineating the anatomical skeleton from a NAC-PET image, thus producing a prior tissue distribution map to G 2 , while G 2 devotes to rendering the tissue details according to both the skeleton and NAC-PET image. We regard G 1 as a segmentation network that is trained under the combination of cross-entropy loss and Dice loss and outputs the anatomical skeleton. For training the generative module G 2 , we further propose the anatomical-consistency constraint to guarantee the fidelity of tissue distribution besides general constraints in previous studies. Experiments on four publicly collected PET/CT datasets demonstrate that our ASEG outperforms existing methods by preserving better anatomical structures in generated pseudo CT images and achieving better visual similarity in corrected PET images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We propose the anatomical skeleton enhanced generation (ASEG, as illustrated in Fig. <ref type="bibr" target="#b0">(1)</ref> framework that regards the CT generation as two sequential tasks, i.e., skeleton prediction and tissue rendering, instead of simply mapping pseudo CT from NAC-PET. ASEG composes of two sequential generative modules {G 1 , G 2 } to deal with them, respectively. G 1 devotes itself to decoupling the anatomical skeleton from NAC-PET to provide rough prior information of attenuation coefficients to G 2 , particularly for lungs and bones that have the most influential variances. G 2 then devotes to rendering the tissue details in the CT pattern exploiting both the skeleton and NAC-PET images. In short, the skeleton decoupled by G 1 is a prior guidance to G 2 , and in turn, G 2 can serve as a target supervision for G 1 . These two modules are trained with different constraints according to the corresponding tasks. Specially, the general Dice loss and cross-entropy loss <ref type="bibr" target="#b15">[16]</ref> are employed to guarantee G 1 for the fidelity of tissue distributions while general mean absolute error and feature matching losses are utilized to guarantee G 2 for potential coarse-to-fine semantic constraint. To improve the fidelity of anatomical structures, we further propose the anatomical consistency loss to encourage G 2 to generate CT images that are consistent in tissue distributions with actual CT images in particular.</p><p>Network Architecture. As illustrated in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, our ASEG has two generative modules for skeleton prediction and tissue rendering, respectively, where G 1 and G 2 share the same network structure but G 2 is accompanied by an adversarial network D (not drawn, same structure in <ref type="bibr" target="#b7">[8]</ref>). Each generative network consists of an input convolutional layer, four encoding blocks, two residual blocks (RBs) <ref type="bibr" target="#b7">[8]</ref>, four decoding blocks, and an output convolutional layer. Each encoding block contains a RB and a convolutional layer with strides of 2 × 2 × 2 for downsampling while each decoding block contains an upsampling operation of 2 × 2 × 2 and a convolutional layer. The kernel size for the input and output convolutional layers is 7 × 7 × 7 while for others is 3 × 3 × 3. Skip connections are further used locally in RBs and globally between corresponding layers to empower information transmission. Meanwhile, the adversarial network D consists of five 4 × 4 × 4 convolutional layers with strides of 2 × 2 × 2 for the first four layers and 1 × 1 × 1 for the last layer.</p><p>Model Formulation. Let X nac and X ac denote the NAC-PET and AC-PET images, and Y be the actual CT image used for AC. Since CT image is highly crucial in conventional AC algorithms, they generally have a relationship as</p><formula xml:id="formula_0">X ac = F(X nac , Y ),<label>(1)</label></formula><p>under an AC algorithm F. To avoid scanning an additional CT image, we attempt to predict Y from X nac as an alternative in AC algorithm. Namely, a mapping G is required to build the relationship between Y and X nac , i.e., Ŷ = G(X nac ).</p><p>Then, X ac can be acquired by</p><formula xml:id="formula_1">X ac ≈ Xac = F(X nac , Ŷ ) = F(X nac , G(X nac )).<label>(2)</label></formula><p>This results in a pioneering AC algorithm that requires only a commonly reusable mapping function G for all PET images rather than a corresponding CT image Y for each PET image. As verified in some previous studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7]</ref>, G can be assigned by some image generation techniques, e.g. GANs and CNNs. However, since these general techniques tend to produce statistical average values, directly applying them may lead to serious brightness deviation, for those tissues with large intensity ranges. To overcome this drawback, we propose ASEG as a specialized AC technique, which decouple the CT generation process G in two sequential parts, i.e., G 1 for skeleton prediction and G 2 for tissue rendering, as formulated as</p><formula xml:id="formula_2">Ŷ = G(X nac ) = G 2 (Y as , X nac ) ≈ G 2 ( Ŷas , X nac ) = G 2 (G 1 (X nac ), X nac ). (3) Y. Pan et al.</formula><p>Herein, G 1 devotes to delineating anatomical skeleton Y as from X nac , thus providing a prior tissue distribution to G 2 while G 2 devotes to rendering the tissue details from X nac and Ŷas = G 1 (X nac ).</p><p>To avoid annotating the ground truth, Y as can be derived from the actual CT image by a segmentation algorithm (denoted as S : Y as = S(Y )). As different tissues have obvious differences in intensity ranges, we define S as a simple thresholding-based algorithm. Herein, we first smooth each non-normalized CT image with a small recursive Gaussian filter to suppress the impulse noise, and then threshold this CT image to four binary masks according to the Hounsfield scale of tissue density <ref type="bibr" target="#b5">[6]</ref>, including the air-lung mask (intensity ranges from -950HU to -125HU), the fluids-fat mask (ranges from -125HU to 10HU), the soft-tissue mask (ranges from 10HU to 100HU), and the bone mask (ranges from 100HU to 3000HU), as demonstrated by anatomical skeleton in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. This binarization trick highlights the difference among different tissues, and thus is easier perceived.</p><p>General Constraints. As mentioned above, two generative modules {G 1 , G 2 } work for two tasks, namely the skeleton prediction and tissue rendering, respectively. Thus, they are trained with different target-oriented constraints. In the training scheme, the loss function for G 1 is the combination of Dice loss L dice and cross-entropy loss L ce <ref type="bibr" target="#b15">[16]</ref>, denoted as</p><formula xml:id="formula_3">L 1 ( Ŷas , Y as ) = L dice ( Ŷas , Y as ) + L ce ( Ŷas , Y as ).<label>(4)</label></formula><p>Meanwhile, the loss function for G 2 combines the mean absolute error (MAE) L mae , perceptual feature matching loss L fm <ref type="bibr" target="#b13">[14]</ref>, and anatomical-consistency loss L ac , denoted as</p><formula xml:id="formula_4">L 2 ( Ŷ , Y ) = L mae ( Ŷ , Y ) + L fm ( Ŷ ; D) + L ac ( Ŷ , Y ). (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where the anatomical consistency loss L st is explained below.</p><p>Anatomical consistency. It is generally known that CT images can provide anatomical observation because different tissues have a distinctive appearance in Hounsfield scale (linear related to attenuation coefficients). Therefore, it is crucial to ensure the consistency of tissue distribution in the pseudo CT images, tracking which we propose to use the tissue distribution consistency to guide the network learning. Based on the segmentation algorithm S, both the actual and generated CTs {Y, Ŷ } can be segmented to anatomical structure/tissue distribution masks {S(Y ), S( Ŷ )}, and their consistency can then be measured by Dice coefficient. Accordingly, the anatomical-consistency loss L ac is a Dice loss as</p><formula xml:id="formula_6">L ac (Y, Ŷ ) = L dice (S( Ŷ ), S(Y )).<label>(6)</label></formula><p>During the inference phase, only the NAC-PET image of each input subject is required, where the pseudo CT image is derived by Ŷ ≈ G 2 (G 1 (X nac ), X nac ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Materials</head><p>The data used in our experiments are collected from The Cancer Image Archive (TCIA) <ref type="bibr" target="#b3">[4]</ref> (https://www.cancerimagingarchive.net/collections/), where a series of public datasets with different types of lesions, patients, and scanners are open-access. Among them, 401, 108, 46, and 20 samples are extracted from the Head and Neck Scamorous Cell Carcinoma (HNSCC), Non-Small Cell Lung Cancer (NSCLC), The Cancer Genome Atlas (TCGA) -Head-Neck Squamous Cell Carcinoma (TCGA-HNSC), and TCGA -Lung Adenocarcinoma (TCGA-LUAD), respectively. We use these samples in HNSCC for training and in other three datasets for evaluation.</p><p>Each sample contains co-registered (acquired with PET-CT scans) CT, PET, and NAC-PET whole-body scans. In our experiments, we re-sampled all of them to a voxel spacing of 2×2×2 and re-scaled the intensities of NAC-PET/AC-PET images to a range of [0, 1], of CT images by multiplying 0.001. The input and output of our ASEG framework are cropped patches with the size of 192 × 192 × 128 voxels. To achieve full-FoV output, the consecutive outputs of each sample are composed into a single volume where the overlapped regions are averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with Other Methods</head><p>We compared our ASEG with three state-of-the-art methods, including (i ) a U-Net based method <ref type="bibr" target="#b2">[3]</ref> that directly learns a mapping from NAC-PET to CT image with MAE loss (denoted as U-Net), (ii ) a conventional GAN-based method <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> that uses the U-Net as the backbone and employ the style-content loss and adversarial loss as an extra constraint (denoted as CGAN), and (iii ) an auxiliary GAN-based method <ref type="bibr" target="#b9">[10]</ref> that uses the CT-based segmentation (i.e., the simple thresholding S) as an auxiliary task for CT generation (denoted as AGAN). For a fair comparison, we implemented these methods by ourselves in a TensorFlow platform with an NVIDIA 3090 GPU. All methods share the same backbone structure as G * in Fig. <ref type="figure" target="#fig_0">1(b</ref>) and follow the same experimental settings. Particularly, the adversarial loss of methods (ii ) and (iii ) are replaced by the perceptual feature matching loss. These two methods could be considered as variants of our method without using predicted prior anatomic skeleton.</p><p>Quantitative Analysis of CT. As the most import application of CT that is to display the anatomical information, we propose to measure the anatomical consistency between the pseudo CT images and actual CT images, where the Dice coefficients on multiple anatomical regions that extracted from the pseudo/actual CT images are calculated. To avoid excessive self-referencing in evaluating anatomical consistency, instead of employing the simple thresholding segmentation (i.e., S), we resort to the open-access TotalSegmentator <ref type="bibr" target="#b14">[15]</ref> to finely segment the actual and pseudo CT images to multiple anatomical structures, and compose them to nine independent tissues for simplifying result , where the following conclusions can be drawn. Firstly, U-Net and CGAN generate CT images with slightly better global intensity similarity but worse anatomical consistency in some tissues than AGAN and ASEG. This indicates that the general constraints (MAE and perceptual feature matching) cannot preserve the tissue distribution since they tend to produce statistical average values or patterns, particularly in these regions with large intensity variants. Secondly, AGAN achieves the worst intensity similarity and anatomical consistency for some organs. Such inconsistent metrics suggest that the global intensity similarity may have a competing relationship with anatomical consistency in the learning procedure, thus it is not advisable to balance them in a single network. Thirdly, CGAN achieves better anatomical consistency than U-Net, but worse than ASEG. It implies that the perceptual feature matching loss can also identify the variants between different tissues implicitly but cannot compare to our strategy to explicitly enhance the anatomical skeleton. Fourthly, our proposed ASEG achieves the best anatomical consistency for all tissues, indicating it is reasonable to enhance tissue variations. In brief, the above results supports the strategy to decouple the skeleton prediction as a preceding task is effective for CT generation.</p><p>Effectiveness in Attenuation Correction. As the pseudo CT images generated from NAC-PET are expected to be used in AC, it is necessary to further evaluate the effectiveness of pseudo CT images in PET AC. Because we cannot access the original scatters <ref type="bibr" target="#b4">[5]</ref>, inspired by <ref type="bibr" target="#b10">[11]</ref>, we propose to resort CGAN to simulate the AC process, denoted as ACGAN and trained on HNSCC dataset. The input of ACGAN is a concatenation of NAC-PET and actual CT, while the output is actual AC-PET. To evaluate the pseudo CT images, we simply use them to take place of the actual CT. Four metrics, including the Peak Signal to Noise Ratio (PSNR), Mean Absolute Error (MAE), Normalized Cross Correlation (NCC), and SSIM, are used to measure ACGAN with pseudo CT images on test datasets (NSCLC, TCGA-HNSC, and TCGA-LUDA). The results are reported in Table <ref type="table" target="#tab_0">1</ref>(b), where the fourth column list the ACGAN results with actual CT images. Meanwhile, we also report the results of direct mapping NAC-PET to AC-PET without CT images in the third column ("No CT"), which is trained from scratch and independent from ACGAN.</p><p>It can be observed from Table <ref type="table" target="#tab_0">1</ref>(b) that: (1) ACGAN with actual CT images can predict images very close to the actual AC-PET images, thus is qualified to simulate the AC process; (2) With actual or pseudo CT images, ACGAN can predict images closer to the actual AC-PET images than without CT, demonstrating the necessity of CT images in process of PET AC; (3) These pseudo CTs cannot compare to actual CTs, reflecting that there exist some relative information that can hardly be mined from NAC-PET; (4) The pseudo CTs generated by ASEG achieve the best in three metrics (MAE, PSNR, NCC) and second in the other metric (SSIM), demonstrating the advance of our ASEG. Figure <ref type="figure" target="#fig_1">2</ref> displayed the detailed diversity of the AC-PET corrected by different pseudo CTs. It can be found that the structures of AC-PET are highly dependent on CT, particularly the lung regions. However, errors in corners and shapes are relatively large (see these locations marked by red arrows), which indicates there are still some space in designing more advanced mapping methods. Nonetheless, compared to other pseudo CTs, these generated by ASEG result in more realistic AC-PET with fewer errors, demonstrating the AC usability of ASEG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed the anatomical skeleton-enhance generation (ASEG) to generate pseudo CT images for PET attenuation correction (AC), with the goal of avoiding acquiring extra CT or MR images. ASEG divided the CT generation into the skeleton prediction and tissue rendering, two sequential tasks, addressed by two designed generative modules. The first module delineates the anatomical skeleton to explicitly enhance the tissue distribution which are vital for AC, while the second module renders the tissue details based on the anatomical skeleton and NAC-PET. Under the collaboration of two modules and specific anatomical-consistency constraint, our ASEG can generate more reasonable pseudo CT from NAC-PET. Experiments on a collection of public datasets demonstrate that our ASEG outperforms existing methods by achieving advanced performance in anatomical consistency. Our study support that ASEG could be a promising and lower-cost alternative of CT acquirement for AC. Our future work will extend our study to multiple PET tracers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. ASEG framework. (a) The general process for reconstructing AC-PET by generating CT from NAC-PET. Rather than directly generating CT from NAC-PET, (b) our ASEG first delineates the anatomical skeleton and then renders the tissue details.</figDesc><graphic coords="2,197,00,337,19,55,36,85,46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of different pseudo CT images (top) and their AC effect (bottom). From left to right are actual NAC-PET/AC-PET and PET corrected without CT (no CT) or with actual or pseudo CT generated by U-Net, CGAN, AGAN, and our ASEG.</figDesc><graphic coords="8,56,46,463,49,339,76,79,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of pseudo CT images generated by different methods. (a) Anatomical consistency (%) Method SSIM Dice l Dice h Dice k Dice b Dice d Dicer Dicev Dice il</figDesc><table><row><cell>UNet</cell><cell cols="5">78.40 58.34 78.89 66.91 52.23 43.00 6.83 50.19 41.81</cell></row><row><cell cols="6">CGAN 80.42 80.97 79.25 67.38 52.86 43.91 18.95 52.40 41.19</cell></row><row><cell cols="6">AGAN 74.71 83.40 74.41 60.26 46.71 39.25 22.39 64.58 49.52</cell></row><row><cell>ASEG</cell><cell cols="5">76.31 85.26 80.63 75.89 63.78 52.32 31.60 69.05 52.79</cell></row><row><cell></cell><cell></cell><cell cols="4">(b) Effectiveness in PET attenuation correction (%)</cell></row><row><cell cols="2">CT Source</cell><cell>MAE (%)</cell><cell>PSNR (dB)</cell><cell>NCC (%)</cell><cell>SSIM (%)</cell></row><row><cell>No CT</cell><cell></cell><cell>1.55±0.56</cell><cell>34.83±2.58</cell><cell>97.13±2.04</cell><cell>94.08±3.44</cell></row><row><cell>UNet</cell><cell></cell><cell>1.47±0.50</cell><cell>34.76±2.45</cell><cell>97.20±1.75</cell><cell>95.18±2.49</cell></row><row><cell>CGAN</cell><cell></cell><cell>1.52±0.49</cell><cell>34.51±2.33</cell><cell>97.05±1.77</cell><cell>94.92±2.49</cell></row><row><cell>AGAN</cell><cell></cell><cell>1.45±0.51</cell><cell>35.09±2.38</cell><cell>97.11±1.80</cell><cell>95.22±2.61</cell></row><row><cell>ASEG</cell><cell></cell><cell>1.36±0.49</cell><cell>35.89±2.48</cell><cell>97.66±1.72</cell><cell>95.67±2.49</cell></row><row><cell cols="2">Actual CT</cell><cell>1.20±0.48</cell><cell>37.28±3.37</cell><cell>98.25±1.56</cell><cell>96.10±2.51</cell></row></table><note><p><p><p><p>report, e.g., lung (Dice l ), heart (Dice h ), liver (Dice li ), kidneys (Dice k ), blood vessels (Dice k ), digestive system (Dice d ), ribs (Dice r ), vertebras (Dice v ), and iliac bones (Dice ib ). Additionally, the Structure Similarity Index Measure (SSIM) values are also reported to measure the global intensity similarity.</p>Results of various methods are provided in Table</p>1</p>(a)</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by The <rs type="funder">China Postdoctoral Science Foundation</rs> (Nos. <rs type="grantNumber">2021M703340</rs>, <rs type="grantNumber">BX2021333</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (Nos. <rs type="grantNumber">62131015</rs>, <rs type="grantNumber">62203355</rs>), <rs type="funder">Science and Technology Commission of Shanghai Municipality (STCSM)</rs> (No. <rs type="grantNumber">21010502600</rs>), and <rs type="funder">The Key R&amp;D Program of Guangdong Province, China</rs> (No. <rs type="grantNumber">2021B0101420006</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Gzf84Rb">
					<idno type="grant-number">2021M703340</idno>
				</org>
				<org type="funding" xml:id="_JVrGMG5">
					<idno type="grant-number">BX2021333</idno>
				</org>
				<org type="funding" xml:id="_yAzjwdv">
					<idno type="grant-number">62131015</idno>
				</org>
				<org type="funding" xml:id="_A2vUN5h">
					<idno type="grant-number">62203355</idno>
				</org>
				<org type="funding" xml:id="_Xn5fMXM">
					<idno type="grant-number">21010502600</idno>
				</org>
				<org type="funding" xml:id="_gnfnQZ5">
					<idno type="grant-number">2021B0101420006</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_3.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Independent attenuation correction of whole body [18F] FDG-PET using a deep learning approach with generative adversarial networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Armanious</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EJNMMI Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthetic CT generation from non-attenuation corrected PET images for whole-body PET imaging</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">215016</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Çiçek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8_49" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cancer imaging archive (TCIA): maintaining and operating a public information repository</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1045" to="1057" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using domain knowledge for robust and generalizable deep learning-based CT-free PET attenuation and scatter correction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mingels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">5882</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Häggström</surname></persName>
		</author>
		<ptr target="https://radlines.org/Hounsfield_unit" />
		<imprint/>
	</monogr>
	<note>Hounsfield units</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning MR imaging-based attenuation correction for PET/MR imaging</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kijowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="676" to="684" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Disease-image-specific learning for diagnosisoriented neuroimage synthesis with incomplete multi-modality data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="6839" to="6853" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Association of amyloid positron emission tomography with subsequent change in clinical management among medicare beneficiaries with mild cognitive impairment or dementia</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Rabinovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatsonis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Apgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gareen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1286" to="1294" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards a whole body [18F] FDG positron emission tomography attenuation correction map synthesizing using deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rodríguez Colmeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Verrastro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Minsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grosges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Direct attenuation correction of brain PET images using only emission data via a deep convolutional encoder-decoder (Deep-DAC)</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shiri</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00330-019-06229-1</idno>
		<ptr target="https://doi.org/10.1007/s00330-019-06229-1" />
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6867" to="6879" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Performance evaluation of the uEXPLORER total-body PET/CT scanner based on NEMA NU 2-2018 with additional tests to characterize PET scanners with a long axial field of view</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Schmall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Omidvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nucl. Med</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="861" to="870" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Total-body PET/CT: current applications and future perspectives</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Roentgenol</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="325" to="337" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis and semantic manipulation with conditional GANs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wasserthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Breit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cyriac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Segeroth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.05868</idno>
		<title level="m">TotalSegmentator: robust segmentation of 104 anatomical structures in CT images</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking dice loss for medical image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="851" to="860" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
