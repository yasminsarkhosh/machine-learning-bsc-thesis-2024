<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Unsupervised Multispectral Image Registration Network for Skin Diseases</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Songhui</forename><surname>Diao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenxue</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenchen</forename><surname>Qin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Liao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<settlement>Arlington</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenming</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jianhua</forename><surname>Yao</surname></persName>
							<email>jianhuayao@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Unsupervised Multispectral Image Registration Network for Skin Diseases</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="720" to="729"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2C0A22C6F8BE048DC8C9D75F3F5BAF37</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_68</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image registration</term>
					<term>Multispectral image</term>
					<term>Unsupervised registration</term>
					<term>Registration field refinement</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multispectral imaging has a broad, promising and advantageous application prospect in the diagnosis of skin diseases. However, there are inherent deviations such as rigid or non-rigid deformation among multispectral images (MSI), which makes accurate and robust registration algorithms desirable to extract reliable multispectral features. Existing registration algorithms are susceptible to significant and nonlinear amplitude differences and geometric distortions among MSI, resulting in an unsatisfactory estimation of the registration field (RF). In this study, we propose an end-to-end multispectral image registration (MSIR) network with unsupervised learning for human skin disease diagnosis. First, we propose a basic adjacent-band pair registration (ABPR) model to obtain the corresponding RFs through simultaneously modeling a series of image pairs from adjacent bands. Second, we introduce a multispectral attention module (MAM) for extraction and adaptive weight allocation of the high-level pathological features of multiple MSI pairs. Third, we design a registration field refinement module (RFRM) to rectify and reconstruct a general RF solution. Fourth, we propose an unsupervised center-toward registration loss function, combining a similarity loss for features in the frequency domain and a smoothness loss for RF. In addition, we built a MSI dataset of multi-type skin diseases and conducted extensive experiments. The results show that our method not only outperforms state-of-the-art methods on MSI registration task, but also contributes to the subsequent task of benign and malignant disease classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Skin disease is common in clinic, which is characterized by the complexity of pathological morphology and etiology, as well as the diversity of disease types and locations. Multispectral imaging (MSI) has the characteristics of non-tissue contact puncture, no radiation and no need for exogenous contrast agents. The imaging mechanism characterizes specific, correlated and complementary tissue features, which makes it having a broad, promising and advantageous application prospect in the diagnosis of skin diseases <ref type="bibr" target="#b0">[1]</ref>. Correspondingly, multispectral imaging also has some shortcomings. On one hand, wavelength and focal length vary with frequency, resulting in non-rigid deformation such as scaling deviation among MSI. On the other hand, the motion of imaging device or patient may introduce further deviation among images. Consequently, MSI registration, that is, the identification and mapping of the same or similar structure or content at the pixel level, is a fundamental and critical process for subsequent tasks such as image fusion, pathological analysis, disease identification and diagnosis.</p><p>The difficulties for MSI registration are twofold. Firstly, conventional registration method is to register two images <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, while group image registration (GIR) is the joint registration of a group of related images. Current GIR research focuses on time-series MRI. MSI contains multiple images with significant and nonlinear amplitude differences and geometric distortions, not only making the pair-wise image registration not applicable, but also bringing great challenges to GIR due to the inability to take advantage of image intensity or structural similarity. Secondly, the type and location of diseases both affect the light reflection coefficient of skin tissue, making it challenging to find a general registration field (RF) for GIR.</p><p>In the field of image registration, many inspiring methods based on traditional or deep learning techniques have been developed and applied to computer vision tasks in medical imaging, remote sensing, etc. Whereas, the traditional methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> are not suitable for MSI dataset with significant non-rigid deformation, gray jump, noise and other factors, which will lead to low efficiency and poor accuracy. The supervised deep learning methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> have the limitation of relying on the groundtruth of RF which is difficult to obtain in medical images. For the unsupervised deep learning method, G. Balakrishnan et al. <ref type="bibr" target="#b9">[10]</ref> proposed a VoxelMorph framework for deformable and pairwise brain image registration based on image intensity. Y. Ye et al. <ref type="bibr" target="#b10">[11]</ref> presented a MU-Net framework, which stacks several DNN models on multiple scales to generate a coarse-to-fine registration pipeline. L. Meng et al. <ref type="bibr" target="#b11">[12]</ref> proposed an DSIM network for MSI registration, which utilized pyramid structure similarity loss to optimize the network and regress the homography parameters. Although the existing algorithms can achieve relatively accurate registration for images with weak or repeated texture, they are susceptible to significant and nonlinear amplitude differences and geometric distortions among MSI, and generally have the disadvantages of poor accuracy, low robustness and low efficiency in realizing group image registration, leading to unsatisfactory RF results.</p><p>To address the aforementioned issues, we propose an end-to-end multispectral image registration (MSIR) network with unsupervised learning for multiple types of human skin diseases, which improves the capability of CNN architecture to learn the cross-band transformation relationship among pathological features, so as to obtain an efficient and robust RF solution. First, we design a basic adjacentband pair registration (ABPR) model, which simultaneously models a series of image pairs from adjacent bands based on CNN, and makes full use of the feature transformation relationship between images to obtain their corresponding RFs. Second, we introduce a multispectral attention module (MAM), which is used to achieve extraction and adaptive weight allocation for the high-level pathological features. Third, we design a registration field refinement module (RFRM) to obtain a general RF solution of MSI through rectifying and reconstructing the RFs learned from all adjacent-band MSI pairs. Fourth, we propose an unsupervised center-toward registration loss function, combining a similarity loss for features in the frequency domain and a smoothness loss for RF. We perform extensive experiments on a MSI dataset of multi-type skin diseases. The evaluation results demonstrate that our method not only outperforms prior arts on MSI registration task, but also contributes to the subsequent task of benign and malignant disease classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>As shown in Fig. <ref type="figure" target="#fig_1">1</ref>, the proposed MSIR framework consists of four main components: (1) an ABPR model to extract pixel-wise representations of corresponding features from a pair of images synchronously.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adjacent-Band Pair Registration (ABPR) Model</head><p>The ABPR model is based on Unet <ref type="bibr" target="#b12">[13]</ref> cascaded with 2D residual blocks <ref type="bibr" target="#b13">[14]</ref>. In the feature encoder represented by the gray part, 4 convolution blocks with size of 3 × 3 and stride of 2 are used for down-sampling, and feature maps with channel number of 16, 32, 32 and 32 are obtained. Correspondingly, in the decoder represented by the yellow part, the convolution blocks contain interpolation operations for up-sampling.</p><p>The input to ABPR model is the concatenation of a pair of MSI images in adjacent bands P i ∈ R H×W ×2 (images I i and I i+1 , 1 ≤ i ≤ n -1. n is the number of bands). The module constructs a function Γ i = F ρ (P i ) to synchronously extracts the mutual transformation relationship. F represents the registration function fit by the designed Unet architecture. ρ refers to the weights and bias parameters of the kernels of the convolutional layers. Γ i ∈ R H×W ×2 is the RF between the given P i , where 2 denotes two channels along the x-axis and y-axis.</p><p>Since P i in multi-band imaging has unequal contribution to the solution of the final RF, we introduce a multispectral attention module (MAM) into ABPR model, which can not only facilitate the extraction of mutual complementary non-local features between MSI, but also guide the model to pay more attention to features from specific spectra with high impact.</p><p>Specifically, we carry out global average pooling operation on the high-level features F i of the encoder for image pair P i , and flatten F i into a feature vector V i . Then we obtain a feature map M ∈ R (n-1)×H ×W ×C by concatenating the accumulated n -1 feature vectors in column. The formula for reallocating attention weights is defined as <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_0">M = sof tmax( MW Q MW T K √ d k )MW v (1)</formula><p>where W Q , W K and W V are weights of the fully connection layers. d k represents the dimension of V i . Next, we reshape M to obtain the updated high-level features F i and continue the subsequent decoding calculation in ABPR model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Registration Field Refinement Module (RFRM)</head><p>In order to make full use of the potential complementarity of corresponding features between cross-band images, we design a RFRM to obtain a general RF of MSI through rectifying and reconstructing a series of RFs learned from multiple adjacent-band image pairs, which is more conducive to further improving the accuracy and generalization of the MSI registration network. First, RFRM concatenates all Γ i ∈ R H×W ×2 learned from n -1 adjacentband image pairs. The obtained RF∈ R (n-1)×H×W ×2 is then refined through three 3D residual blocks, and the reliable Γ ∈ R H×W ×2 is finally generated. T Γ means a coordinate mapping that is parameterized by Γ and its spatial transformation. That is, for each pixel p ∈ I i , there is a Γ such that I i (p) and T Γ (I i )(p) are two corresponding points in adjacent bands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Center-Toward Registration Loss Function</head><p>The ultimate goal of image registration is to obtain a RF from MSI with significant amplitude difference and geometric distortion, so that the perceived images corrected by the RF have the best similarity with each other. The pixel similarity in spatial domain among MSI is prone to large difference due to spectral intensity, while the feature similarity in the frequency domain is more stable. In order to optimize the adaptive center-toward registration of a group of images simultaneously, we propose an unsupervised loss function, including a similarity loss for features in the frequency domain and a smoothness loss for RF. The specific scheme is described in detail as follows:</p><p>(1) The first is a similarity loss function based on the residual complexity of features in the frequency domain, which is used to penalize differences in appearance and optimize the registration effect under different lighting conditions. The residual complexity loss function is defined as <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_1">L RC (I, I ) = 1 m m j=1 log[(DCT (I -I )) 2 /α + 1] (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where m is the number of pixels of the images I and I . DCT denotes discrete cosine transform whose weight is regulated by a hyperparameter α with an empirical value of 0.05. It is worth noting that the similarity loss consists of two components. For the image with band i, we first use the fused Γ and its spatial transformation to obtain the warped image I i = T Γ (I i ). Then we evaluate its similarity to the reference image with the adjacent band I i+1 and the warped image I i+1 = T Γ (I i+1 ), which not only ensures that the transformed images do not deviate from the original spatial distribution, but also realizes center-toward registration of a group of images synchronously and uniformly. Then the total similarity loss can be obtained through adding the residual complexity results from n -1 image pairs. The formula is defined as:</p><formula xml:id="formula_3">L sim = 1 n -1 n-1 i=1 [L RC (I i , I i+1 ) + L RC (I i , I i+1 )]<label>(3)</label></formula><p>(2) The second is an auxiliary loss function that constrains the smoothness of RF and penalizes the local spatial variation. L smooth is constructed based on Γ i through differentiable operation on spatial gradient, and the formula is as follows <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_4">L smooth = 1 n -1 n-1 i=1 (Γ i ) 2<label>(4)</label></formula><p>where is the gradient operator calculated along the x-axis and y-axis. Then the loss function of our module L total is the weighted sum of L sim and L smooth , which is defined as:</p><formula xml:id="formula_5">L total = L sim + λL smooth (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where λ is a hyperparameter used to balance the similarity and smoothness of RF, and the empirical value 4 is taken, which is consistent with the Voxelmorph <ref type="bibr" target="#b9">[10]</ref> method. All these components are unified in the MSIR network and trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>We validated the proposed MSIR network on an in-house MSI dataset containing 85 cases with multi-type skin diseases collected from our partner hospital from November 2021 to March 2022, including 36 cases with benign diseases (keloid, fibrosarcoma, cyst, lipoma, hemangioma and nevus) and 49 cases with malignant diseases (eczematoid paget disease, squamous cell carcinoma, malignant melanoma and basal cell carcinoma). Specifically, each case consists of 22 scans with wavelengths ranging from 405 nm to 1650 nm and a uniform size of 640 × 512. For each case, a clinician manually annotated four corresponding landmarks for registration on each scan, and their positions were further reviewed by an experienced medical expert. These landmarks are used to evaluate the registration accuracy. The detailed information for the training set and test set is shown in Table <ref type="table" target="#tab_0">1</ref>. We conducted a 4-fold cross-validation in training set to select models and hyperparameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quantitative and Qualitative Evaluation</head><p>In this paper, we adopt three evaluation indexes to assess the registration performance of different methods, including normalized mutual information (NMI), registration feature error (RFE) and target registration point error (TRE) <ref type="bibr" target="#b16">[17]</ref>.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the quantitative comparisons of the registration performance among our MSIR network and three state-of-the-art competitive methods, including Voxelmorph <ref type="bibr" target="#b9">[10]</ref>, DSIM <ref type="bibr" target="#b11">[12]</ref> framework and MU-Net <ref type="bibr" target="#b10">[11]</ref>. The mean value of the initial TRE is 8.77 pixels. It can be observed that our method achieves the superior performance. Specifically, it improves the best NMI to 0.547, RFE to 1.166 and TRE to 4.984 pixels, which significantly outperforms the other competitors. We further conduct ablation study to verify the contributions of individual components, as shown in Table <ref type="table" target="#tab_2">3</ref>. NCC refers to the normalization cross correlation, which describes the relevance and similarity of targets. Compared with NCC loss, the proposed loss function significantly improves the registration performance (0.142 reduction in TRE, 0.163 reduction in RFE and 0.016 increase in NMI). The introduction of MAM further improves the registration performance. On this basis, we introduce an augmented dataset to expand the training set to 50 times of its original size by performing affine transformation operations on MSI, such as translation, rotation, scaling, clipping, oblique cutting, etc. The adoption of the augmented dataset makes the indexes continuously optimized. Quantitative results validate the effectiveness of MSIR network and augmented dataset in improving registration performance.</p><p>Next, we conduct ablation experiments to explore the registration performance based on images with different subsets of bands, as illustrated in Table <ref type="table" target="#tab_3">4</ref>. VIS and NIR represent the visible bands (405 nm-780 nm) and the near infrared bands (780 nm-1650 nm), respectively. ALL represents the whole bands (405 nm-1650 nm). The results demonstrate that our method can achieve remarkable registration improvement for images of different bands.</p><p>To verify the generalization of the proposed method, we test the model using a synthetic dataset (with |5| degrees of rotation, |0.02| of scaling, |6| and |8| pixels of translation along the x-axis and y-axis). The visualization results are shown in Fig. <ref type="figure" target="#fig_2">2</ref>. It can be seen that MSIR method can achieve accurate registration not only for the MSI of the raw dataset, but also for that of the synthetic dataset with more complex transformation that are challenging for registration.</p><p>In addition, in order to verify the effect of this registration method to subsequent tasks, we further conduct a classification task for benign and malignant diseases based on the established MSI dataset. Table <ref type="table" target="#tab_4">5</ref> shows the quantitative comparisons of different classifiers using single-band and all-bands images. Four classifiers are compared, namely KNN <ref type="bibr" target="#b17">[18]</ref>, SVM <ref type="bibr" target="#b18">[19]</ref>, Resnet18 <ref type="bibr" target="#b19">[20]</ref> and Inception V3 <ref type="bibr" target="#b20">[21]</ref>. Columns 2 and 3 represent the worst and best classification accuracy based on single-band images, which come from the wavelengths of 450 nm and 525 nm, respectively. The fourth column is the classification results based on the original MSI dataset without registration, and the last column shows that based on the images processed by MSIR network. It can be seen that the MSI contains more abundant information than the single-band image, which is more conducive to the subsequent analysis. More importantly, due to the contribution of MSIR network for image registration, the classification accuracy on  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, an efficient and robust framework for multispectral image registration is proposed and validated on a self-established dataset of multiple types of skin diseases, which holds great potentials for the further analysis, such as the classification of benign and malignant diseases. We intend to release the MSI dataset in future. The quantitative results of experiments demonstrate the superiority of our method over the current state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(2) a MAM to reassign the weight of the high-level features. (3) a RFRM to yield a general RF of MSI. (4) a center-toward registration loss function to optimize the effect of GIR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overall framework of our proposed MSIR network for multiple types of human skin diseases.</figDesc><graphic coords="3,44,79,371,63,334,57,194,59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual comparisons of test results on raw dataset and synthetic dataset. A, B and C represent three patient cases respectively. Column (I) shows the example of the original image at 650nm, in which the boxes marked with red crosses indicate the position of zooming for column (II) to column (III). Column (II) and column (III) are the qualitative results of the raw dataset processed without and with our method, respectively. The first row represents the sectioning of the red cross on the x-axis, and the second row represents that on the y-axis. Column (IV) and column (V) are the quantitative results of two indicators in the red cross region of the synthetic dataset, namely, the pixel intensity among bands and the normalized correlation coefficient (NCC) between adjacent bands. The blue and red lines represent the results without and with registration, respectively.</figDesc><graphic coords="8,58,98,53,96,334,54,166,75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Characteristics for in-house dataset.</figDesc><table><row><cell>Characteristic</cell><cell>Entry</cell><cell cols="2">Training Set Test Set</cell></row><row><cell>Patient Demographics</cell><cell cols="2">No. of cases 59</cell><cell>26</cell></row><row><cell></cell><cell cols="2">No. of scans 1298</cell><cell>572</cell></row><row><cell cols="2">Prevalence No. of scans (percentage) with Benign</cell><cell cols="2">528(28.24%) 264(14.12%)</cell></row><row><cell></cell><cell>Malignant</cell><cell cols="2">770(41.18%) 308(16.47%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparisons of different methods.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>NMI</cell><cell>RFE</cell><cell>TRE</cell></row><row><cell cols="3">Voxelmorph [10] Multidimensional Image 0.498(±0.057)</cell><cell>1.909(±0.246)</cell><cell>5.734(±3.265)</cell></row><row><cell>DSIM [12]</cell><cell>Multispectral Image</cell><cell>0.479(±0.064)</cell><cell>2.122(±0.393)</cell><cell>5.763(±3.560)</cell></row><row><cell>MU-Net [11]</cell><cell>Multispectral Image</cell><cell>0.513(±0.027)</cell><cell>1.851(±0.891)</cell><cell>5.692(±3.206)</cell></row><row><cell cols="2">MSIR (Ours) Multispectral Image</cell><cell cols="3">0.547(±0.029) 1.166(±0.642) 4.984(±3.471)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results of ablation study on components and augmented dataset.</figDesc><table><row><cell cols="4">NCC Loss Our Loss MAM Augmented Dataset NMI</cell><cell>RFE</cell><cell>TRE</cell></row><row><cell>×</cell><cell>×</cell><cell>×</cell><cell>0.523(±0.051)</cell><cell>1.331(±0.819)</cell><cell>5.227(±4.441)</cell></row><row><cell>×</cell><cell>×</cell><cell>×</cell><cell>0.539(±0.046)</cell><cell>1.168(±0.674)</cell><cell>5.085(±3.905)</cell></row><row><cell>×</cell><cell></cell><cell>×</cell><cell>0.526(±0.078)</cell><cell>1.245(±0.795)</cell><cell>4.935(±6.422)</cell></row><row><cell>×</cell><cell></cell><cell>×</cell><cell cols="3">0.547(±0.029) 1.166(±0.642) 4.984(±3.471)</cell></row><row><cell>×</cell><cell></cell><cell></cell><cell cols="3">0.576(±0.024) 1.109(±0.547) 4.736(±3.353)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Quantitative mean(±std) results of ablation experiments on bands. ±0.101) 1.324(±1.241) 12.448(±15.093) 0.576(±0.048) 1.013(±0.718) 5.595(±5.406) ALL 0.271(±0.097) 1.323(±0.904) 10.070(±9.544) 0.547(±0.029) 1.166(±0.642) 4.984(±3.471)</figDesc><table><row><cell cols="3">Bands w/o Registration</cell><cell></cell><cell>MSIR (Ours)</cell><cell></cell></row><row><cell></cell><cell>NMI</cell><cell>RFE</cell><cell>TRE</cell><cell>NMI</cell><cell>RFE</cell><cell>TRE</cell></row><row><cell>VIS</cell><cell cols="3">0.295(±0.103) 1.505(±1.197) 8.625(±7.055)</cell><cell cols="3">0.517(±0.089) 1.287(±0.788) 4.632(±3.947)</cell></row><row><cell>NIR</cell><cell>0.227(</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Quantitative comparisons of different classifiers on band and registration operation.MSI dataset has been significantly improved, which verifies the necessity and effectiveness of MSIR network.</figDesc><table><row><cell>Classifier</cell><cell>Accuracy</cell><cell></cell></row><row><cell></cell><cell>Single Band</cell><cell>All Bands</cell></row><row><cell></cell><cell cols="3">Worst (450 nm) Best (525 nm) w/o Registration MSIR (Ours)</cell></row><row><cell>KNN [18]</cell><cell cols="2">0.524(±0.017) 0.567(±0.079) 0.607(±0.063)</cell><cell>0.644(±0.058)</cell></row><row><cell>SVM [19]</cell><cell cols="2">0.539(±0.064) 0.583(±0.072) 0.615(±0.084)</cell><cell>0.672(±0.049)</cell></row><row><cell>Resnet18 [20]</cell><cell cols="2">0.540(±0.072) 0.602(±0.051) 0.647(±0.051)</cell><cell>0.754(±0.037)</cell></row><row><cell cols="3">Inception V3 [21] 0.551(±0.061) 0.606(±0.046) 0.659(±0.022)</cell><cell>0.762(±0.018)</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 68.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multispectral near infrared absorption imaging for histology of skin cancer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Spreinat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Selvaggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Erpenbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kruss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biophotonics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">201960080</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large deformation diffeomorphic image registration with Laplacian pyramid networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C W</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_21</idno>
		<idno>978-3-030-59716-0 21</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="211" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Medical image registration based on uncoupled learning and accumulative enhancement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-11" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Non-iterative coarse-to-fine registration based on single-pass deep cumulative learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-09" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="88" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transmorph: transformer for unsupervised medical image registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Segars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102615</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Remote sensing image registration with modified sift and enhanced feature matching</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="7" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deformable image registration based on similarity-steered CNN regression</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66182-7_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66182-735" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Duchesne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10433</biblScope>
			<biblScope unit="page" from="300" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly-supervised learning of multimodal features for regularised iterative descent in 3D image registration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blendowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101822</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep adaptive registration of multi-modal prostate images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">101769</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Voxelmorph: a learning framework for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1788" to="1800" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multiscale framework with unsupervised learning for remote sensing image registration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Investigation and evaluation of algorithms for unmanned aerial vehicle multispectral image registration</title>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Appl. Earth Obs. Geoinf</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">102403</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Intensity-based image registration by minimizing residual complexity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1882" to="1891" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46475-6_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46475-6" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comparative performance analysis of k-nearest neighbour (KNN) algorithm and its different variants for disease prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Moni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Melanoma detection on skin lesion images using k-means algorithm and SVM classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Therese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Devi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kavya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of Deep Learning in Biomedical Engineering and Health Informatics</title>
		<imprint>
			<biblScope unit="page" from="227" to="251" />
			<date type="published" when="2021">2021</date>
			<publisher>Apple Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective classification of colon cancer using Resnet-18 in comparison with squeezenet</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Pharm. Negat. Results</title>
		<imprint>
			<biblScope unit="page" from="1413" to="1421" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An internet of health things-driven deep learning framework for detection and classification of skin cancer using transfer learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khamparia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhushan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Emerg. Telecommun. Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3963</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
