<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention</title>
				<funder ref="#_WdxmxYW">
					<orgName type="full">Key Research and Development Program of Hubei Province</orgName>
				</funder>
				<funder ref="#_DySXGuV #_WBkjce7">
					<orgName type="full">Bingtuan Science and Technology Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qian</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hua</forename><surname>Zou</surname></persName>
							<email>zouhua@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haifeng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Aier Eye Hospital of Wuhan University</orgName>
								<orgName type="institution" key="instit2">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Aier Eye Hospital of Wuhan University</orgName>
								<orgName type="institution" key="instit2">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="735" to="744"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">1B4005BF1E74557C71AD6A68D1AC4406</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_69</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Incomplete Multimodal Learning</term>
					<term>Visual Acuity Prediction</term>
					<term>Self-Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the primary treatment option for cataracts, it is estimated that millions of cataract surgeries are performed each year globally. Predicting the Best Corrected Visual Acuity (BCVA) in cataract patients is crucial before surgeries to avoid medical disputes. However, accurate prediction remains a challenge in clinical practice. Traditional methods based on patient characteristics and surgical parameters have limited accuracy and often underestimate postoperative visual acuity. In this paper, we propose a novel framework for predicting visual acuity after cataract surgery using masked self-attention. Especially different from existing methods, which are based on monomodal data, our proposed method takes preoperative images and patient demographic data as input to leverage multimodal information. Furthermore, we expand our method to a more complex and challenging clinical scenario, i.e., the incomplete multimodal data. Firstly, we apply efficient Transformers to extract modality-specific features. Then, an attentional fusion network is utilized to fuse the multimodal information. To address the modalitymissing problem, an attention mask mechanism is proposed to improve the robustness. We evaluate our method on a collected dataset of 1960 patients who underwent cataract surgery and compare its performance with other state-of-the-art approaches. The results show that our proposed method outperforms other methods and achieves a mean absolute error of 0.122 logMAR. The percentages of the prediction errors within ± 0.10 logMAR are 94.3%. Besides, extensive experiments are conducted to investigate the effectiveness of each component in predicting visual acuity. Codes will be available at https://github.com/liyiersan/MSA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cataract has been the leading cause of vision loss. As the only treatment option, cataract surgery is one of the most commonly performed surgeries worldwide. Nevertheless, not all patients achieve complete visual recovery after surgery, which can be due to various factors, such as pre-existing eye conditions, surgical complications, and postoperative inflammation. The ability to accurately predict visual recovery can help clinicians identify high-risk patients and provide appropriate interventions to improve their visual outcomes.</p><p>Over the past decades, many efforts have been made to predict the Best Corrected Visual Acuity (BCVA) after cataract surgeries. Most of them are based on traditional approaches like retinometer <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> or visual electrophysiology <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref>. These methods require specialized expertise to perform and are subject to significant variability in their results. Even though some computeraided approaches have been proposed, most of them use traditional machine learning algorithms <ref type="bibr" target="#b0">[1]</ref> and focus on single-modal data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. While they can be effective to some extent, they often have limited predictive power due to the reliance on a single source of information. In addition, traditional machine learning methods (e.g., linear regression, decision trees, and support vector machines) may not be able to capture complex relationships between different modalities, such as clinical data and imaging data. What's more, clinical scenarios are more complex. For example, the multimodal data may be incomplete due to medical conditions, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Therefore, there is a need for more sophisticated computer-aided techniques that can integrate multiple sources of data and leverage the strengths of each to improve predictive accuracy as well as address the challenging modality-missing problem.</p><p>Transformers <ref type="bibr" target="#b11">[12]</ref>, which are a type of neural network architecture originally developed for natural language processing tasks, have recently shown For instance, "two images" denotes the samples with two image modalities. We can see that only one-third of cases have complete multimodal images.</p><p>great promise in computer vision applications, such as image classification <ref type="bibr" target="#b3">[4]</ref>, object detection <ref type="bibr" target="#b2">[3]</ref>, and segmentation <ref type="bibr" target="#b19">[20]</ref>. Transformers have the ability to learn from large amounts of data and can capture complex patterns and relationships in the data, which makes them well-suited for analyzing multimodal learning. Motivated by the tremendous success of Transformers in multimodal learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, we propose a new framework that utilizes incomplete multimodal data for predicting BCVA after cataract surgeries. In particular, our framework contains three stages: modality-specific feature extraction, attentional feature fusion, and visual acuity prediction. Firstly, for each input modality, a pre-trained efficient transformer will be used to extract features. To better leverage the clinical diagnosis keywords, an auxiliary classification loss is added to the image transformer. And to extract text features more efficiently, we apply a CLIP-like <ref type="bibr" target="#b9">[10]</ref> input to combine discrete clinical words (e.g., age, sex, and preoperative visual acuity) into sentences. Secondly, we apply an attentional transformer to fuse multimodal features. Specifically, to address the issue of missing modalities, we introduce modality embeddings and attentional masks to prevent the interference of missing modalities with the remaining modalities. Finally, a prediction head takes the fused features as input to predict the BCVA with the Mean Square Error (MSE) loss.</p><p>The main contributions of this work can be summarized as follows: (1) We develop a novel framework that uses multimodal data to predict BCVA as well as tackle the complex modality-missing issue. (2) An auxiliary classification loss is adopted to extract more comprehensive pathological features for images. Also, discrete textual words are combined into sentences to better fit the text transformer input. <ref type="bibr" target="#b2">(3)</ref> Extensive experiments are conducted to prove the effectiveness of our method. The compared methods include incomplete multimodal learning approaches and monomodal BCVA prediction methods. We also analyze the importance of each component of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Framework Overview</head><p>As shown in Fig. <ref type="figure">2</ref>, our framework contains three parts: modality-specific encoder, multimodal fusion network, and BCVA prediction head. During feature extraction, we take pre-trained transformers as the backbone, i.e., ViT <ref type="bibr" target="#b3">[4]</ref> as the image encoder, and CLIP <ref type="bibr" target="#b9">[10]</ref> as the text encoder. After that, a crossmodal transformer is used to fuse features from multiple modalities, in which an attentional mask is added to tackle the missing modalities. Finally, a fully connected (FC) layer is used as the prediction head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Monomodal Feature Extraction</head><p>Text Encoder. The text encoder is a pre-trained CLIP <ref type="bibr" target="#b9">[10]</ref> model. The discrete physiological information is combined into a sentence format that benefits the Fig. <ref type="figure">2</ref>. Pipeline of the proposed framework. The modality-specific encoders utilize vanilla multi-head self-attention. In contrast, the multimodal fusion network employs masked multi-head self-attention. Notably, the fusion network takes features (i.e., tokens) from all modalities as input, whereas each auxiliary classification network receives features from a single modality as input.</p><p>text encoder. For instance, the text "male, 67 years old, preoperative visual acuity 0.52 logMAR" will be combined into the sentence "A 67-year-old male patient with preoperative visual acuity of 0.52 logMAR". By combining the text in this way, the physiological texts of all patients are fed into the text transformer in a unified manner. Compared with directly concatenating the texts and inputting them into the model, the combined sentences are more in line with the real scene and are easier to be understood by the model to extract key semantic information. Moreover, the CLIP text encoder is trained on a large text corpus, enabling excellent generalization performance. Thus, during the training of the overall model, the weights of the text encoder can be fixed.</p><p>Image Encoder. The image encoder adopts ViT <ref type="bibr" target="#b3">[4]</ref>. Since ViT is trained on natural images and may not be directly applicable to medical images, it can not be fixed during the training. However, the pre-trained weights can be utilized to expedite convergence. In addition, there are diagnostic keywords given by ophthalmologists for each image in the dataset. It is not appropriate to directly use the CLIP text encoder to extract medical features from these keywords since CLIP is trained on natural language texts. To this end, we introduce an auxiliary classification loss in the image encoder. Specifically, for each input image, a multilabel classification network is incorporated after the image encoder to predict the diseases contained in the image. The classification network is composed of an average pooling layer and a fully connected layer. For simplicity, we adopt the binary cross-entropy loss as the auxiliary classification loss as Eq. ( <ref type="formula">1</ref>).</p><formula xml:id="formula_0">L CLS = W i L i BCE (1)</formula><p>where W i equals 1 if the i-th modality is available and equals 0, otherwise. By adding the classification loss, the final loss to train the whole model is:</p><formula xml:id="formula_1">L = L MSE + αL CLS (2)</formula><p>where L MSE is the mean square error loss between the predicted BCVA and the ground truth, α is a hyper-parameter and set to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multimodal Feature Fusion</head><p>We use a cross-modal Transformer as the multimodal fusion network. The obtained modality-specific tokens are projected into the same dimension and concatenated into an input sequence. In contrast to the modality-specific Transformer, besides adding positional embeddings to all input tokens, we also add learnable modality-specific embeddings to all tokens indicating the modality information.</p><p>Complete Multimodal Learning. In the collected dataset, all cases have corresponding text modality, and almost all cases have corresponding OCT modality. Therefore, we built a complete multimodal prediction model based on the text modality and OCT modality. In such a situation, only the OCT encoder and text encoder will be preserved, while the SLO encoder and Ultrasound encoder will be dropped. Since transformers can handle sequences of any input length, we don't need to make any changes to the fusion network. Using complete multimodal learning, we can compare our methods to other BCVA prediction approaches which do not consider the incomplete multimodal scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incomplete Multimodal</head><p>Learning. Not all cases have complete modalities for the three image modalities (i.e., OCT, SLO, and Ultrasound). For the missing modalities, one possible way is to simply represent them by 0 values <ref type="bibr" target="#b17">[18]</ref>. However, the 0 values will be regarded as noise by the model as they do not contain any useful information. To avoid model degradation, we add attentional masks in the vanilla self-attention to exclude the interactions among missing modalities and available modalities.</p><p>The proposed attentional masks are easy to implement. As shown in Eq. ( <ref type="formula">3</ref>), self-attention in transformers is mainly matrix multiplication.</p><formula xml:id="formula_2">Attn(Q, K, V ) = sof tmax( QK T √ d z )V (3)</formula><p>in which, Q, K, and V are queries, keys, and values obtained from tokens, respectively. d z is the projection dimension. To avoid interactions between irrelevant tokens, the masked self-attention is computed as:</p><formula xml:id="formula_3">Attn Mask (Q, K, V ) = sof tmax( QK T √ d z + M )V (4)</formula><p>in which, M is the mask matrix. For each element in M , it will be 0 if the interactions should be included, or it will be negative infinity to avoid unnecessary interactions. By adding negative infinity, the results of softmax will be very close to 0. Figure <ref type="figure" target="#fig_1">3</ref> shows an example of masks when modalities are missing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Implementation Details</head><p>To validate the effectiveness of the proposed method, we have conducted extensive experiments implemented with Pytorch and 8×RTX 3090 GPUs. The input images are resized to 224 × 224. The Adam optimizer is adopted with an initial learning rate of 0.001 and β 1 = 0.9, β 2 = 0.99. The mini-batch size is set to 32. We train the model for 100 epochs in total, and the learning rate will be decayed by 0.1 every 20 epochs. Besides, we randomly split all samples to 80% for training and 20% for testing. All experiments are conducted with 5-fold cross-validation to produce more solid results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation Metrics</head><p>The collected dataset consists of 1960 patients (2685 eyes) having cataract surgeries at Aier eye hospital of Wuhan University. The collected modalities are texts and images. The images contain 2635 Optical Coherence Tomography (OCT), 2615 Ultrasound, and 988 Scanning Laser Ophthalmoscopy (SLO). The textual information includes sex, age, preoperative and postoperative visual acuity. For each image, three ophthalmologists will label it to obtain the diagnosis (i.e., clinical diagnosis keywords) of 14 retinal diseases. The retinal diseases include normal, vitreous opacity, posterior staphyloma, stellate vitreous degeneration, pathological myopia changes, retinal atrophy, macular degeneration, epiretinal membrane, ellipsoid band partially missing, retinoschisis, retinal hemorrhage, macular edema, macular hole, and retinitis pigmentosa. We use Mean Absolute Error (MAE), Symmetric Mean Absolute Percentage Error (SMAPE), and prediction accuracy as the metrics. For simplicity, we consider predictions to be accurate if the prediction errors are within ± 0.10 logMAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quantitative Performance</head><p>We have compared the results on the collected dataset with other approaches.</p><p>The compared methods include state-of-the-art methods which aim to predict BCVA using OCT like CTT-Net <ref type="bibr" target="#b14">[15]</ref>, Wei et al. <ref type="bibr" target="#b15">[16]</ref>, and other algorithms considering incomplete multimodal learning such as Huang et al. <ref type="bibr" target="#b5">[6]</ref>, Ma et al. <ref type="bibr" target="#b6">[7]</ref>, and Zhao et al. <ref type="bibr" target="#b18">[19]</ref>. For the former methods, we compare our method with them using the complete data. As for the latter approaches, they are not proposed for BCVA prediction. Therefore, we finetune them so that they can be applied to incomplete BCVA prediction data. From Table <ref type="table" target="#tab_2">1</ref>, we can see that our proposed framework achieves the best performance. Specifically, we have improved CTT-Net <ref type="bibr" target="#b14">[15]</ref> and shown a sharp rise compared to Wei et al. <ref type="bibr" target="#b15">[16]</ref> on the complete dataset. Even though CCT-Net uses both text and oct modalities, the utilization of text is still limited. Wei et al. <ref type="bibr" target="#b15">[16]</ref> directly ignores the textual information and only uses some simple frameworks to predict BCVA, thus achieving the worst results. When using incomplete data, the performance is also improved greatly, and we still achieve the best performance. Huang et al. <ref type="bibr" target="#b5">[6]</ref> try to apply image synthesis to solve the modality-missing problem. However, the collected images in our dataset are not  <ref type="bibr" target="#b18">[19]</ref> propose to learn common representations of all modalities, and this idea works in cases of minorly missing modalities but not in our dataset. Ma et al. <ref type="bibr" target="#b6">[7]</ref> achieve similar performance to our proposed method due to their robust design. The results show that our model is capable of extracting modality-specific features as well as fusing them in an effective way. Besides, the performance also shows the robustness of our proposed method. Note that almost all results on the incomplete multimodal dataset outperform results on the complete multimodal dataset. This is due to the incomplete multimodal dataset will always contain OCT and text modalities, thus having more information in the input. Figure <ref type="figure" target="#fig_2">4</ref> shows the distribution of preoperative, predictive, and postoperative visual acuity. We can see that the predicted visual acuity largely overlaps with the true postoperative visual acuity, demonstrating the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>As shown in Table <ref type="table" target="#tab_3">2</ref>, the proposed framework mainly benefits from the auxiliary classification loss and attentional fusion mask. Our analysis is as follows: Images provide more valuable information than text, making the auxiliary classification loss more effective than text combining. In missing multimodal learning, feature fusion takes precedence, and the masked self-attention mechanism contributes the most. Additionally, we conducted experiments to evaluate each modality's effectiveness. The results can be seen in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present a new framework for BCVA prediction on the collected incomplete multimodal dataset. We take full advantage of multimodal information through our framework. The text modality is better utilized through the combination of the words. Moreover, image modality is explored effectively by the auxiliary classification loss. The attentional mask addresses the modalitymissing issue. Extensive experiments have proved the effectiveness and superiority of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Statistics for the collected dataset. (a) Number of patients, eyes, and three image modalities. OCT refers to Optical Coherence Tomography, and SLO stands for Scanning Laser Ophthalmoscopy. (b) Number of multimodal or monomodal samples.For instance, "two images" denotes the samples with two image modalities. We can see that only one-third of cases have complete multimodal images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An example of the attentional mask. (a) means only SLO is missing; (b) represents both OCT and SLO are missing. White cells represent a value of 0, and gray cells represent a value of negative infinity. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Distribution of preoperative, predictive, and postoperative visual acuity. Actual vision means the actual postoperative visual acuity.</figDesc><graphic coords="7,152,97,327,14,146,56,113,05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,46,29,54,14,301,36,207,58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>specific encoder Multimodal Fusion Network 12 Layers, C=768 6 Layers, C=384 Tokens Fused Features</head><label></label><figDesc></figDesc><table><row><cell>Transformer Layer Transformer Layer Transformer Layer Transformer Layer Modality-Average Pooling Linear Projection Transformer Layer … Transformer Layer Transformer Layer … Linear Projection Transformer Layer … Linear Projection Transformer Layer … Word Embedding Transformer Layer A 63-year-old male patient with … OCT Ultrasound Text Classification Head Average Pooling Classification Head Average Pooling SLO Classification Head Average Pooling</cell><cell>Prediction Head</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Predicted Diseases Predicted Diseases Predicted Diseases Predicted BCVA Auxiliary Classification Network</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Quantitative prediction results on the collected dataset. Complete means text and OCT modalities are available and complete. Incomplete means text and three image modalities are available, but the image modalities may be incomplete.</figDesc><table><row><cell>Dataset</cell><cell>Methods</cell><cell>MAE (↓)</cell><cell>SMAPE (↓)</cell><cell>Accuracy (↑)</cell></row><row><cell cols="5">Complete CTT-Net [15] (OCT+Text) 0.168 ± 0.014 85.236 ± 3.277 0.887 ± 0.022</cell></row><row><cell></cell><cell>CTT-Net [15] (OCT)</cell><cell cols="3">0.174 ± 0.013 89.635 ± 2.881 0.872 ± 0.016</cell></row><row><cell></cell><cell>Wei et al. [16] (OCT)</cell><cell cols="3">0.237 ± 0.093 93.587 ± 3.236 0.723 ± 0.056</cell></row><row><cell></cell><cell>Ours (OCT)</cell><cell cols="3">0.153 ± 0.012 65.615 ± 1.690 0.901 ± 0.018</cell></row><row><cell></cell><cell>Ours (OCT + Text)</cell><cell cols="3">0.142 ± 0.009 62.550 ± 1.668 0.923 ± 0.014</cell></row><row><cell cols="2">Incomplete Huang et al. [6]</cell><cell cols="3">0.176 ± 0.054 88.672 ± 3.051 0.854 ± 0.017</cell></row><row><cell></cell><cell>Ma et al. [7]</cell><cell cols="3">0.139 ± 0.013 61.722 ± 2.007 0.917 ± 0.015</cell></row><row><cell></cell><cell>Zhao et al. [19]</cell><cell cols="3">0.133 ± 0.021 59.673 ± 2.362 0.921 ± 0.021</cell></row><row><cell></cell><cell>Ours</cell><cell cols="3">0.122 ± 0.007 57.165 ± 1.610 0.943 ± 0.012</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Ablation study results on incomplete datasets.</figDesc><table><row><cell>Model</cell><cell>MAE (↓)</cell><cell>SMAPE (↓)</cell><cell>Accuracy (↑)</cell></row><row><cell>Baseline</cell><cell cols="3">0.176 ± 0.014 87.642 ± 3.023 0.874 ± 0.014</cell></row><row><cell cols="4">Baseline + combined sentences 0.163 ± 0.012 78.932 ± 3.672 0.893 ± 0.011</cell></row><row><cell>Baseline + cls loss</cell><cell cols="3">0.157 ± 0.009 71.023 ± 2.346 0.912 ± 0.015</cell></row><row><cell>Baseline + attentional mask</cell><cell cols="3">0.145 ± 0.010 62.328 ± 2.064 0.925 ± 0.013</cell></row><row><cell>Baseline + all</cell><cell cols="3">0.122 ± 0.007 57.165 ± 1.610 0.943 ± 0.012</cell></row></table><note><p><p>aligned, and image synthesis may not work in such a situation.</p>Zhao et al.  </p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. This work is partially supported by <rs type="funder">Bingtuan Science and Technology Program</rs> (No. <rs type="grantNumber">2022DB005</rs> and <rs type="grantNumber">2019BC008</rs>) and <rs type="funder">Key Research and Development Program of Hubei Province</rs> (<rs type="grantNumber">2022BCA009</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DySXGuV">
					<idno type="grant-number">2022DB005</idno>
				</org>
				<org type="funding" xml:id="_WBkjce7">
					<idno type="grant-number">2019BC008</idno>
				</org>
				<org type="funding" xml:id="_WdxmxYW">
					<idno type="grant-number">2022BCA009</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 69.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Development and validation of machine learning models: electronic health record data to predict visual acuity after cataract surgery</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Alexeeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perm. J</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">188</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The success of cataract surgery and the preoperative measurement of retinal function by electrophysiological techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Ophthalmol</title>
		<imprint>
			<biblScope unit="page">401281</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-813" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fullfield electroretinography in age-related macular degeneration: can retinal electrophysiology predict the subjective visual outcome of cataract surgery?</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R J</forename><surname>Forshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Kjaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Andréasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Sørensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="693" to="700" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multi-modal brain tumor segmentation via missing modality synthesis and modality-level attention fusion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04586</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are multimodal transformers robust to missing modality?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Testuggine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18177" to="18186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Assessing visual function behind cataract: preoperative predictive value of the Heine lambda 100 Retinometer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mimouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jadon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Blumenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="559" to="564" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Prediction of postoperative visual acuity after vitrectomy for macular hole using deep learning-based artificial intelligence. Graefe&apos;s Archive for Clinical and Experimental Ophthalmology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Obata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Prospective comparison of the Heine Retinometer with the mentor Guyton-Minkowski potential acuity meter for the assessment of potential visual acuity before cataract surgery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shoemaker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Retinometer predicts visual outcome in Descemet membrane endothelial keratoplasty</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Unterlauft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Girbardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graefes Arch. Clin. Exp. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2283" to="2290" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Electrophysiology as a prognostic indicator of visual recovery in diabetic patients undergoing cataract surgery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graefes Arch. Clin. Exp. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">259</biblScope>
			<biblScope unit="page" from="1879" to="1887" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CTT-Net: a multi-view cross-token transformer for cataract postoperative visual acuity prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="835" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An optical coherence tomography-based deep learning algorithm for visual acuity prediction of highly myopic eyes after cataract surgery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Weil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Cell Develop. Biol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">652848</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GroupViT: semantic segmentation emerges from text supervision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18134" to="18144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">mmFormer: multimodal medical transformer for incomplete multimodal learning of brain tumor segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-911" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Missing modality imagination network for emotion recognition with uncertain missing modalities</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2608" to="2618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
