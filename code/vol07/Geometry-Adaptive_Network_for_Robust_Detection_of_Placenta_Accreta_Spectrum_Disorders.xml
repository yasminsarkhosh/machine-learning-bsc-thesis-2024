<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders</title>
				<funder ref="#_rBFyy8s">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_7pH7mFt #_gv7VQ2E">
					<orgName type="full">Natural Science Foundation of Hunan Province, China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zailiang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiang</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hailan</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Xiangya Hospital</orgName>
								<orgName type="institution" key="instit2">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yajing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rongchang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feiyang</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="43" to="53"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">68214CF3D870C3791CEE63AE7AF8FFBA</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>PAS detection</term>
					<term>Geometric information</term>
					<term>Inaccurate annotations</term>
					<term>Magnetic resonance imaging</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Placenta accreta spectrum (PAS) is a high-risk obstetric disorder associated with significant morbidity and mortality. Since the abnormal invasion usually occurs near the uteroplacental interface, there is a large geometry variation in the lesion bounding boxes, which considerably degrades the detection performance. In addition, due to the confounding visual representations of PAS, the diagnosis highly depends on the clinical experience of radiologists, which easily results in inaccurate bounding box annotations. In this paper, we propose a geometryadaptive network for robust PAS detection. Specifically, to deal with the geometric prior missing problem, we design a Geometry-adaptive Label Assignment (GA-LA) strategy and a Geometry-adaptive RoI Fusion (GA-RF) module. The GA-LA strategy dynamically selects positive PAS candidates (RoIs) for each lesion according to its shape information. The GA-RF module aggregates the multi-scale RoI features based on the geometry distribution of proposals. Moreover, we develop a Lesionaware Detection Head (LA-Head) to leverage high-quality predictions to iteratively refine inaccurate annotations with a novel multiple instance learning paradigm. Experimental results under both clean and noisy labels indicate that our method achieves state-of-the-art performance and demonstrate promising assistance for PAS diagnosis in clinical applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Placenta accreta spectrum (PAS) refers to the abnormal invasion of trophoblast cells into the myometrium at different depths of infiltration <ref type="bibr" target="#b9">[10]</ref>. With the rising trend of advanced maternal age and cesarean section delivery, the incidence of PAS has increased steadily in recent years <ref type="bibr" target="#b4">[5]</ref>. Undiagnosed PAS may lead to massive obstetric hemorrhage and hysterectomy <ref type="bibr" target="#b20">[21]</ref>. Therefore, accurate detection of PAS prenatally is critical for appropriate treatment planning. Magnetic resonance imaging (MRI) provides valuable position information of placenta and can be an important complementary imaging method when ultrasound (US) diagnosis is inconclusive <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>. Since manually identifying PAS on MR images is time-consuming and labor-intensive, automated detection of PAS is significant in clinical practice.</p><p>However, due to the lack of open-source dataset, the research on computeraided diagnosis of PAS is very limited. Previous studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">29]</ref> mainly focused on the classification task but accurate location cannot be provided. Moreover, existing object detection methods are designed for natural images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">30]</ref> or specific diseases <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">27]</ref>, with no consideration for the characteristics of PAS. Given that the abnormal invasion usually occurs near the uteroplacental interface, the geometric information of lesion regions is highly correlated with the shape of placenta, thereby causing significant variation in the aspect ratios of PAS bounding boxes. Furthermore, MRI in most cases is used after suspecting an abnormality on US, this by itself raises the difficulty of accurate labeling <ref type="bibr" target="#b18">[19]</ref>.</p><p>To address the above issues, we propose a novel geometry-adaptive PAS detection method, which utilizes the shape prior of placenta to predict highquality PAS bounding boxes and further refines inaccurate annotations. The prior knowledge is mainly reflected in the aspect ratio of lesion boxes. Specifically, to take advantage of the geometry prior, we design a Geometry-adaptive Label Assignment (GA-LA) strategy and a Geometry-adaptive RoI Fusion (GA-RF) module. The GA-LA strategy dynamically selects positive proposals by calculating the optimal IoU threshold for each lesion. The GA-RF module fuses multi-scale RoI features from different pyramid layers. To reduce the impact of lesions with large aspect ratio, the module generates fusion weights through the geometry distribution of proposals. Furthermore, in order to alleviate the reliance on accurate annotations, we construct a Lesion-aware Detection Head (LA-Head), which leverages the geometry-guided predictions to iteratively refine bounding box labels by multiple instance learning. To the best of our knowledge, this is the first work to automatically detect PAS disorders on MR images. The contributions of this paper can be summarized as follows: (1) A Lesion-aware Detection Head (LA-Head) is designed, which employs a new multiple instance learning approach to improve the robustness to inaccurate annotations. (2) A flexible Geometry-adaptive Label Assignment (GA-LA) strategy is proposed to select positive PAS candidates according to the shape of lesions. (3) A statisticbased Geometry-adaptive RoI Fusion (GA-RF) module is developed for aggregating multi-scale features based on the geometry distribution of proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The central idea of our geometry-adaptive network is to refine inaccurate bounding boxes with geometry-guided predictions. To this end, we propose a Geometryadaptive Label Assignment (GA-LA) strategy and a Geometry-adaptive RoI Fusion (GA-RF) module to introduce geometry prior to detection. Then we develop a Lesion-aware Detection Head (LA-Head) to achieve the refinement of inaccurate annotations using multiple instance learning.</p><p>The overview of our method is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Given an input image, Feature Pyramid Network (FPN) <ref type="bibr" target="#b11">[12]</ref> extracts image features and then Region Proposal Network (RPN) <ref type="bibr" target="#b18">[19]</ref> generates a set of region proposals. After obtaining the candidate boxes, RoIAlign <ref type="bibr" target="#b6">[7]</ref> maps them to feature pyramid levels and extracts each proposal as a fixed-size feature map. Then the GA-RF module aggregates the multi-scale representations of each level through the statistical geometric information. Based on the fused RoI feature map, the LA-Head predicts and refines the classification and localization of lesions. During training, the GA-LA strategy assigns label to each proposal according to the shape of corresponding ground-truth box, making the model directly trainable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Geometry-Adaptive Label Assignment</head><p>The static label assignment strategy adopted by Faster R-CNN predefines the IoU threshold τ to match objects and background to each anchor, but ignores the different shapes of PAS regions. To relieve the problem, we propose a GA-LA strategy to dynamically calculate the IoU threshold τ i for each lesion bounding box g i according to its aspect ratio r i . A previous study demonstrated that the aspect ratio is larger, the detection performance is better with a low IoU threshold <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">26]</ref>. Therefore, the value of τ i should be inversely proportional to r i . We design a simple but effective weighting factor and compute τ i as below:</p><formula xml:id="formula_0">τ i = 1 αr i • τ (1)</formula><p>where α is a hyper-parameter. For each lesion g i , the proposals with an IoU greater than or equal to the threshold τ i are selected as positive samples. The labeled proposals are then used to train the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Geometry-Adaptive RoI Fusion</head><p>RoIAlign maps each proposal to a single feature pyramid level, which fails to leverage multi-scale information from other levels. Some works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref> have attempted to integrate RoI features, but they do not consider the geometric characteristics of PAS bounding boxes. Hence, we design a GA-RF module to embed geometry prior into the representations of proposals. Given a proposal, the mapped RoI features {f l i ∈ R c×h×w } L l=1 in different levels are concatenated in channel dimension, where L is the number of levels. We define the proposal whose aspect ratio is greater than R as a hard sample. The number of hard samples distributed to each level is formulated as geometry prior knowledge s i ∈ R 1×4 , which is used to generate fusion factor ω ∈ R 1×4 . The fused feature f i ∈ R cL×h×w is the weighted sum of all feature levels and expressed as follows:</p><formula xml:id="formula_1">N l = p∈P l I(r p ≥ R), ω = sigmoid(N 1 , N 2 , • • • , N L ), f i = L l=1 ω l • f l i (2)</formula><p>where {N l } L l=1 is the number of proposals with large aspect ratio in each layer. In this way, we take full advantage of the multi-scale information and the geometry distribution prior to enrich the feature representation for PAS prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lesion-Aware Detection Head</head><p>Motivated by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">25]</ref>, we present a Lesion-aware Detection Head (LA-Head) to use high-quality proposals to iteratively refine the lesion box labels.</p><p>In this work, we regard PAS detection as a multiple instance learning (MIL) problem. In the standard paradigm for object detection, a MIL method treats an image as a bag and proposals in the image as instances <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">28]</ref>. Different from previous studies, we treat an object as a bag B i and proposals corresponding to the object as instances P i = {p j i } N j=1 , where N is the number of instances. Each bag has a label y i ∈ {1, -1}, where y i = 1 denotes an inaccurate ground-truth box containing at least one lesion candidate and y i = -1 denotes a background box without lesions.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(c), our lesion-aware MIL method can be separated into three alternative parts: detector μ(θ d ), instance selector φ(θ s ) and instance classifier ψ(θ c ), where θ d , θ s and θ c are parameters to be learned. First, the detector generates lesion instances based on proposal features. Then for each instance p j i , the instance selector computes the confidence score φ(p j i ; θ s ). Considering that the classification scores are not strongly correlated with localization quality <ref type="bibr" target="#b10">[11]</ref>, we select the most positive instance p j * i as follows:</p><formula xml:id="formula_2">j * = arg max j φ(p j i ; θ s )<label>(3)</label></formula><p>where j * is the index of the instance with the highest score. To leverage the classification information of predicted bounding boxes, we fuse the most positive instance p j * i and the ground-truth instance g i to obtain a high-quality positive instance for training. The final selected instance is defined as below:</p><formula xml:id="formula_3">p * i = δ s (φ(p j * i ; θ s )) • p j * i + (1 -δ s (φ(p j * i ; θ s ))) • g i (4)</formula><p>where δ s is the weighting factor. Intuitively, the weight assigned to p * i should be higher when φ(p j * i ; θ s ) is larger, and the weight assigned to g i should have a lower bound γ to ensure that g i can provide prior knowledge. Thus δ s is calculated as:</p><formula xml:id="formula_4">δ s (x) = min(e β•x , γ)<label>(5)</label></formula><p>where β and γ are hyper-parameters. With the selected instances, instance classifier is trained to classify other instances as positive or negative. Then these proposals can serve as refined bounding box annotations for the detector. Furthermore, the detector generates instances to train the detection head and highquality proposals can improve the detection performance. Thus this enables the self-feedback relationship between the detector and the LA-Head.</p><p>During training, instance selector, instance classifier and detector are jointly optimized based on the loss function:</p><formula xml:id="formula_5">L = i (L s (B i , ϕ(θ s ))+L c (B i , p * i , φ(θ c )) + L d (B i , p * i , μ(θ d ))<label>(6)</label></formula><p>where L s is the loss of instance selector which is defined as hinge loss:</p><formula xml:id="formula_6">L s (B i , φ(θ c )) = max(0, 1 -y i max j φ(p j i ; θ c ))<label>(7)</label></formula><p>The second term L c is the loss of instance classifier and denoted as follows:</p><formula xml:id="formula_7">L c (B i , p * i , ψ(θ c )) = j log(y j i • (ψ(p j i ; θ c ) - 1 2 ) + 1 2 ) (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>where ψ(p j i ; θ c ) is the probability that p j i contains lesions, and y j i is the label of p j i and calculated as follows:</p><formula xml:id="formula_9">y j i = ⎧ ⎨ ⎩ 1, y i = 1 and IoU (p j i , p * i ) ≥ τ * i -1, y i = 1 and IoU (p j i , p * i ) &lt; τ * i -1, y i = -1 (9)</formula><p>where τ * i is the dynamic IoU threshold of p j i . The third term L d is defined as:</p><formula xml:id="formula_10">L d (B i , p * i , μ(θ d ))= j 1(y i ) • 1(y j i ) • L reg (p j i , p * i ) (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where 1(x) is the indicator function, set to 1 if x = 1; otherwise, set to 0. We adopt the smooth L 1 loss as the loss function L reg for regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset, Evaluation Metrics and Implementation Details</head><p>Dataset. Owing to the lack of open-source PAS dataset, our experiments are performed on a private dataset. We collected 110 placenta MRI scans of different patients upon the approval of Xiangya Hospital of Central South University. All T2-weighted image volumes were sliced along the sagittal plane. Two experienced radiologists with 20 and 14 years of clinical experience in medical imaging and PAS diagnosis selected the slices with PAS and manually annotated the lesion bounding boxes using LabelImg <ref type="bibr">[24]</ref>. We finally obtain 312 2D MR images with a resolution of 640 × 640 px. To verify the robustness of our network, we simulate inaccurate annotations by perturbing clean bounding box labels. With the noise rate λ, we randomly shift and scale the ground-truth box {x i , y i , w i , h i } as follows:</p><formula xml:id="formula_12">x i = x i + Δ x w i , y i = y i + Δ y h i , w i = w i + Δ w w i , h i = h i + Δ h h i (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>where Δx, Δy, Δw, and Δh follow the uniform distribution U (-λ, λ).</p><p>Evaluation Metrics. We adopt Average Precision (AP) and Sensitivity to evaluate the detection performance. In detail, AP is calculated over IoU threshold ranges from 0.25 to 0.95 at an interval of 0.05. Sensitivity denotes the proportion of correct prediction results in all ground-truths and is computed with an IoU threshold of 0.25 at 1 false positive per image.</p><p>Implementation Details. The proposed network is implemented with MMDetection 2.4.0 <ref type="bibr" target="#b3">[4]</ref> and Pytorch 1.6 on NVIDIA GeForce RTX 1080Ti. We took FPN with ResNet50 <ref type="bibr" target="#b7">[8]</ref> as backbone. The framework was trained using the SGD optimizer, where the initial learning rate, momentum, and weight decay were 5e -4, 0.9, and 1e -3, respectively. We adopt a batch size of 1 and the epochs of 120.</p><p>The hyper-parameters are set as α = 2, β = 0.2, γ = 0.8 and R = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>The results of ablative experiments are listed in Table <ref type="table" target="#tab_0">1</ref>. Faster R-CNN is set as the baseline. We first explore the impact of GA-LA and GA-RF under accurate annotations. Compared with the baseline, two components bring 3.4% and 5.0% mAP gains separately. Another 3.7% mAP improvement is achieved when applied together. This finding indicates that geometry information of lesions can behave as effective prior for PAS detection. We then add LA-Head and conduct experiments under inaccurate annotations. It outperforms the baseline by at least 1.3% mAP, which demonstrates that the classification information of predictions is beneficial to alleviate the impact of noisy bounding box labels. We subsequently combine all key designs and obtain the optimal 30.3% and 26.8% mAP under 10% and 20% noise levels, outperforming the baseline by 6.1% and 5.8% respectively. The results reveal that high-quality predictions with geometry guidance can provide precise supervision for LA-Head. In addition, our method brings more obvious improvement under high annotation noise, which further proves its robustness to inaccurate annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with State-of-the-Art Methods</head><p>We compare the geometry-adaptive network with seven object detection methods on both clean and noisy datasets. Faster R-CNN, Cascade R-CNN <ref type="bibr" target="#b2">[3]</ref> and Dynamic R-CNN <ref type="bibr" target="#b28">[30]</ref> are anchor-based methods, while FCOS is an anchor-free detector. ATSS <ref type="bibr" target="#b29">[31]</ref> and SA-S <ref type="bibr" target="#b8">[9]</ref> are dynamic label assignment strategies. Note that SA-S also uses the object shape information to select samples. AugFPN <ref type="bibr" target="#b5">[6]</ref> is a variant of FPN and contains a soft RoI selection module. The quantitative results are reported in Table <ref type="table" target="#tab_1">2</ref>. We first analyze the results of experiments under clean data. Compared with the general anchor-based and anchor-free methods, our method outperforms them by a large margin, thereby verifying that higher-quality predictions are generated under the guidance of geometry information. Compared with dynamic label assignment strategies, the improvement of mAP by 5.9%, 2.6% and sensitivity by 9.6%, 1.4% demonstrated that our GA-LA strategy is simple but effective. Although SA-S also considers the shape of objects, the proposed method still achieves superior performance, likely because our model benefits from the two-stage structure. Compared with the feature fusion method, our approach performs favorably against heuristicguided RoI selection of AugFPN. An intuitive explanation is that the optimal feature may be difficult to obtain using heuristic-guided method. Meanwhile our GA-RF module can adaptively generate representation according to the geometry prior of PAS lesions. We then analyze experimental results under 10% noise level data. Our geometry-adaptive network achieves consistent performance improvements compared with other state-of-the-art detectors. The result reveals that the high-quality predictions can serve as precise supervision signals for learning on inaccurate annotations. Figure <ref type="figure" target="#fig_1">2</ref> provides the visualization results of Faster R-CNN, Dynamic R-CNN, SA-S and AugFPN. The examples show that our method can generate PAS bounding boxes with more accurate shape and localization, which is consistent with the previous analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present a geometry-adaptive network for robust PAS detection. We point out that the geometry prior missing problem and inaccurate annotations could deteriorate the performance of detectors. To solve the problem, a Geometry-adaptive Label Assignment strategy (GA-LA) and a Geometryadaptive RoI Fusion (GA-RF) module are proposed to fully utilize the geometry prior of lesions to predict high-quality proposals. Moreover, a Lesion-aware Detection Head (LA-Head) is developed to alleviate the impact of inaccurate annotations by leveraging the classification information of predicted boxes. The experimental results under both clean and noisy labels demonstrate that our method surpasses other state-of-the-art detectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our geometry-adaptive network, with novel (a) GA-LA, (b) GA-RF and (c) LA-Head. To solve the geometric prior missing problem, GA-LA strategy dynamically selects PAS RoIs according to the shape of placenta, and then GA-RF module fuses multi-scale features based on their geometry distribution. With the high-quality predictions, LA-Head refines inaccurate annotations by multiple instance learning.</figDesc><graphic coords="3,55,98,53,93,340,18,195,76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of the detection results. Red bounding boxes represent the groundtruth while green bounding boxes represent predictions. The first row and second row are the results under clean and noisy labels respectively. (Color figure online)</figDesc><graphic coords="8,41,79,448,07,340,21,119,59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation study of the proposed method.</figDesc><table><row><cell cols="4">GA-LA GA-RF LA-Head AP25 AP50 mAP</cell><cell>Sensitivity</cell></row><row><cell cols="3">Data with clean labels</cell><cell></cell><cell></cell></row><row><cell>×</cell><cell>×</cell><cell>×</cell><cell>65.1 35.6 25.4</cell><cell>79.5</cell></row><row><cell></cell><cell>×</cell><cell>×</cell><cell cols="2">68.7 41.8 28.8 (+3.4) 90.4 (+10.9)</cell></row><row><cell>×</cell><cell></cell><cell>×</cell><cell cols="2">68.9 47.7 30.4 (+5.0) 82.2 (+2.7)</cell></row><row><cell></cell><cell></cell><cell>×</cell><cell cols="2">79.2 51.1 34.1(+8.7) 91.8(+12.3)</cell></row><row><cell cols="4">Data with noisy labels (λ = 0.1)</cell><cell></cell></row><row><cell>×</cell><cell>×</cell><cell>×</cell><cell>65.2 30.2 24.2</cell><cell>83.6</cell></row><row><cell>×</cell><cell>×</cell><cell></cell><cell cols="2">69.5 30.9 26.4 (+2.2) 84.9 (+1.3)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">74.0 40.4 30.3(+6.1) 86.3(+2.7)</cell></row><row><cell cols="4">Data with noisy labels (λ = 0.2)</cell><cell></cell></row><row><cell>×</cell><cell>×</cell><cell>×</cell><cell>60.2 25.9 21.0</cell><cell>78.1</cell></row><row><cell>×</cell><cell>×</cell><cell></cell><cell cols="2">61.4 34.3 22.3 (+1.3) 80.8 (+2.7)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">71.1 39.5 27.3(+6.3) 83.6(+5.5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with state-of-the-art methods.</figDesc><table><row><cell>Methods</cell><cell>Data with clean labels</cell><cell>Data with noisy labels</cell></row><row><cell></cell><cell cols="2">AP25 AP50 mAP Sensitivity AP25 AP50 mAP Sensitivity</cell></row><row><cell>Faster R-CNN [19]</cell><cell>65.1 35.6 25.4 79.5</cell><cell>65.2 30.2 24.2 83.6</cell></row><row><cell>Cascade R-CNN [3]</cell><cell>62.6 38.7 27.2 76.7</cell><cell>57.0 30.1 22.7 79.5</cell></row><row><cell cols="2">Dynamic R-CNN [30] 71.5 40.2 30.3 89.0</cell><cell>69.2 39.8 28.3 84.9</cell></row><row><cell>FCOS [23]</cell><cell>70.8 36.0 27.4 86.3</cell><cell>68.9 38.7 26.6 90.4</cell></row><row><cell>ATSS [31]</cell><cell>68.3 40.1 28.2 82.2</cell><cell>65.1 28.5 23.8 90.4</cell></row><row><cell>SA-S [9]</cell><cell>71.3 46.1 31.5 90.4</cell><cell>66.4 33.9 26.0 91.8</cell></row><row><cell>AugFPN [6]</cell><cell>70.7 40.1 29.0 87.8</cell><cell>59.1 36.1 24.0 79.5</cell></row><row><cell>Ours</cell><cell>79.2 51.1 34.1 91.8</cell><cell>74.0 40.4 30.3 86.3</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">61972419</rs>), and the <rs type="funder">Natural Science Foundation of Hunan Province, China</rs> (No. <rs type="grantNumber">2021JJ30865</rs> and <rs type="grantNumber">2023JJ30865</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rBFyy8s">
					<idno type="grant-number">61972419</idno>
				</org>
				<org type="funding" xml:id="_7pH7mFt">
					<idno type="grant-number">2021JJ30865</idno>
				</org>
				<org type="funding" xml:id="_gv7VQ2E">
					<idno type="grant-number">2023JJ30865</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clinical Applications -Breast</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Placenta accreta: spectrum of us and MR imaging findings</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Baughman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Corteville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiographics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1905" to="1916" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: delving into high quality object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Mmdetection: open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Placenta accreta spectrum (PAS) disorders: incidence, risk factors and outcomes of different management strategies in a tertiary referral hospital in Minia, Egypt: a prospective study</title>
		<author>
			<persName><forename type="first">S</forename><surname>El Gelany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Pregnancy Childbirth</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">AugFPN: improving multi-scale feature learning for object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12595" to="12604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shape-adaptive selection and measurement for oriented object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Figo placenta accreta diagnosis and management expert consensus panel. figo consensus guidelines on placenta accreta spectrum disorders: epidemiology</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jauniaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chantraine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langhoff-Roos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Gynaecol. Obstet</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="273" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust object detection with inaccurate bounding boxes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20080-9_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20080-94" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13670</biblScope>
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Magnetic resonance imaging in the evaluation of placental adhesive disorders: correlation with color doppler ultrasound</title>
		<author>
			<persName><forename type="first">G</forename><surname>Masselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1292" to="1299" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detection of lymph nodes in T2 MRI using neural network ensembles</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Mathai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87589-3_70</idno>
		<idno>978-3-030-87589-3 70</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MLMI 2021</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12966</biblScope>
			<biblScope unit="page" from="682" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Prenatal assessment of placenta accreta spectrum disorders from ultrasound images using deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>University of Oxford</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge-guided pretext learning for uteroplacental interface detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_57</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-857" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="582" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning and radiomics analysis for prediction of placenta invasion based on T2WI</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Biosci. Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="6198" to="6215" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Placenta accreta spectrum</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Branch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1529" to="1536" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semisupervised training of a brain MRI tumor detection model using mined annotations</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Swinburne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="89" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">FCOS: fully convolutional one-stage object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">C-mil: continuation multiple instance learning for weakly supervised object detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2199" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08529</idno>
		<title level="m">Slender object detection: diagnoses and improvements</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Global-local attention network with multi-task uncertainty loss for abnormal lymph node detection in MR images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">102345</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training robust object detectors from noisy category labels and imprecise bounding boxes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5782" to="5792" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prenatal prediction and typing of placental invasion using MRI deep and radiomic features</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Eng. Online</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">56</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic R-CNN: towards high quality object detection via dynamic training</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58555-6_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58555-616" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12360</biblScope>
			<biblScope unit="page" from="260" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
