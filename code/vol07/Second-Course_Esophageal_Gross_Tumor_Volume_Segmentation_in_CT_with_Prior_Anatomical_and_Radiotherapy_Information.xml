<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information</title>
				<funder ref="#_bVZ5N6e">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_sY5jhnC">
					<orgName type="full">Beijing Municipal Natural Science Foundation</orgName>
				</funder>
				<funder ref="#_ERa5KJr #_umrgnXV">
					<orgName type="full">Guangdong Esophageal Cancer Institute Science and Technology Program</orgName>
				</funder>
				<funder ref="#_U9faJEa">
					<orgName type="full">Institute for Intelligent Healthcare, Tsinghua University</orgName>
				</funder>
				<funder ref="#_cSaYcTR">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_Vq8R3kH #_Q8gmAyx">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_v7HZVjQ">
					<orgName type="full">Tsinghua-Foshan Innovation Special Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yihua</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hee</forename><forename type="middle">Guan</forename><surname>Khor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sijuan</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">State Key Laboratory of Oncology in South China</orgName>
								<orgName type="department" key="dep2">Collaborative Innovation Center for Cancer Medicine</orgName>
								<orgName type="laboratory">Guangdong Key Laboratory of Nasopharyngeal Carcinoma Diagnosis and Therapy</orgName>
								<orgName type="institution">Sun Yat-Sen University Cancer Center</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">MedMind Technology Co., Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaobin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">MedMind Technology Co., Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
							<email>yangxin@sysucc.org.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">State Key Laboratory of Oncology in South China</orgName>
								<orgName type="department" key="dep2">Collaborative Innovation Center for Cancer Medicine</orgName>
								<orgName type="laboratory">Guangdong Key Laboratory of Nasopharyngeal Carcinoma Diagnosis and Therapy</orgName>
								<orgName type="institution">Sun Yat-Sen University Cancer Center</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongen</forename><surname>Liao</surname></persName>
							<email>liao@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="511" to="520"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">9F3A9333399BAA3B3BF0D94FC1C638F2</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_48</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Second course radiotherapy</term>
					<term>Esophageal gross tumor volume</term>
					<term>Data efficient learning</term>
					<term>Prior anatomical information</term>
					<term>Attention</term>
				</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Esophageal cancer is a significant global health concern, and radiotherapy (RT) is a common treatment option. Accurate delineation of the gross tumor volume (GTV) is essential for optimal treatment outcomes. In clinical practice, patients may undergo a second round of RT to achieve complete tumor control when the first course of treatment fails to eradicate cancer completely. However, manual delineation is laborintensive, and automatic segmentation of esophageal GTV is difficult due to the ambiguous boundary of the tumor. Detailed tumor information naturally exists in the previous stage, however the correlation between the first and second course RT is rarely explored. In this study, we first reveal the domain gap between the first and second course RT, and aim to improve the accuracy of GTV delineation in the second course RT by incorporating prior information from the first course. We propose a novel prior Anatomy and RT information enhanced Second-course Esophageal GTV segmentation network (ARTSEG). A region-preserving attention module (RAM) is designed to understand the long-range prior knowledge of the esophageal structure, while preserving the regional patterns. Sparsely labeled medical images for various isolated tasks necessitate efficient utilization of knowledge from relevant datasets and tasks. To achieve this, we train our network in an information-querying manner. ARTSEG incorporates various prior knowledge, including: 1) Tumor volume variation between first and second RT courses, 2) Cancer cell proliferation, and 3) Reliance of GTV on esophageal anatomy. Extensive quantitative and qualitative experiments validate our designs.</p><p>H. Liao and X. Yang are the co-corresponding authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_48.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Esophageal cancer is a significant contributor to cancer-related deaths globally <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>. One effective treatment option is radiotherapy (RT), which utilizes high-energy radiation to target cancerous cells <ref type="bibr" target="#b3">[4]</ref>. To ensure optimal treatment outcomes, both the cancerous region and the adjacent organ-at-risk (OAR) must be accurately delineated, to focus the high-energy radiation solely on the cancerous area while protecting the OARs from any harm. Gross tumor volume (GTV) represents the area of the tumor that can be identified with a high degree of certainty and is of paramount importance in clinical practice.</p><p>In the clinical setting, patients may undergo a second round of RT treatment to achieve complete tumor control when initial treatment fails to completely eradicate cancer <ref type="bibr" target="#b15">[16]</ref>. However, the precise delineation of the GTV is laborintensive, and is restricted to specialized hospitals with highly skilled RT experts. The automatic identification of the esophagus presents inherent challenges due to its elongated soft structure and ambiguous boundaries between it and adjacent organs <ref type="bibr" target="#b11">[12]</ref>. Moreover, the automatic delineation of the GTV in the esophagus poses a significant difficulty, primarily attributable to the low contrast between the esophageal GTV and the neighboring tissue, as well as the limited datasets.</p><p>Recently, advances in deep learning <ref type="bibr" target="#b21">[21]</ref> have promoted research in automatic esophageal GTV segmentation from computed tomography (CT) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Since the task is challenging, Jin et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> improve the segmentation accuracy by incorporating additional information from paired positron emission tomography (PET). Nevertheless, such approaches require several imaging modalities, which can be both costly and time-consuming, while disregarding any knowledge from previous treatment or anatomical understanding. Moreover, the correlation between the first and second courses of RT is rarely investigated, where detailed prior tumor information naturally exists in the previous RT planning.</p><p>In this paper, we present a comprehensive study on accurate GTV delineation for the second course RT. We proposed a novel prior Anatomy and RT information enhanced Second-course Esophageal GTV segmentation network (ARTSEG). A region-preserving attention module (RAM) is designed to effectively capture the long-range prior knowledge in the esophageal structure, while preserving regional tumor patterns. To the best of our knowledge, we are the first to reveal the domain gap between the first and second courses for GTV segmentation, and explicitly leverage prior information from the first course to improve GTV segmentation performance in the second course.</p><p>The medical images are labeled sparsely, which are isolated by different tasks <ref type="bibr" target="#b20">[20]</ref>. Meanwhile, an ideal method for automatic esophageal GTV segmentation in the second course of RT should consider three key aspects: 1) Changes in tumor volume after the first course of RT, 2) The proliferation of cancerous cells from a tumor to neighboring healthy cells, and 3) The anatomical-dependent Our training approach leverages multi-center datasets containing relevant annotations, that challenges the network to retrieve information from E1 using the features from E2. The decoder D utilizes the prior knowledge obtained from I1 and G1 to generate the mask prediction. Our training strategy leverages three datasets that introduce prior knowledge to the network of the following three key aspects: 1) Tumor volume variation, 2) Cancer cell proliferation, and 3) Reliance of GTV on esophageal anatomy.</p><p>nature of GTV on esophageal locations. To achieve this, we efficiently exploit knowledge from multi-center datasets that are not tailored for second-course GTV segmentation. Our training strategy does not specific to any tasks but challenges the network to retrieve information from another encoder with augmented inputs, which enables the network to learn from the above three aspects. Extensive quantitative and qualitative experiments validate our designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Network Architecture</head><p>In the first course of RT, a CT image denoted as I 1 is utilized to manually delineate the esophageal GTV, G 1 . During the second course of RT, a CT image I 2 of the same patient is acquired. However, I 2 is not aligned with I 1 due to soft tissue movement and changes in tumor volume that occurred during the first course of treatment. Both images I 1/2 have the spatial shape of H × W × D.</p><p>Our objective is to predict the esophageal GTV G 2 of the second course. It would be advantageous to leverage insights from the first course, as it comprises comprehensive information pertaining to the tumor in its preceding phase. Therefore, the input to encoder E 1 consists of the concatenation of I 1 and G 1 to encode the prior information (features f d 1 ) from the first course, while encoder E 2 embeds both low-and high-level features f d 2 of the local pattern of I 2 (Fig. <ref type="figure" target="#fig_0">1</ref>),</p><formula xml:id="formula_0">f d 1 = E 1 (I 1 , G 1 ), f d 2 = E 2 (I 2 ), d = 0, 1, 2, 3, 4<label>(1)</label></formula><p>where the spatial shape of</p><formula xml:id="formula_1">f d 1/2 is H 2 d × W 2 d × D 2 d</formula><p>, with 2 d+4 channels. Region-Preserving Attention Module. To effectively learn the prior knowledge in the elongated esophagus, we design a region-preserving attention module (RAM), as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The multi-head attention (MHA) <ref type="bibr" target="#b16">[17]</ref> is employed to gather long-range informative values in f d 1 with f d 2 as queries and f d 1 as keys. The features f d 1/2 are reshaped to HW D 2 3d × C before passed to the MHA, where C is the channel dimension. The attentive features f d A can be formulated as:</p><formula xml:id="formula_2">f d A = MHA(Q, K, V ) = MHA(f d 2 , f d 1 , f d 1 ), d = 3, 4.<label>(2)</label></formula><p>Since MHA perturbs the positional information, we preserve the tumor local patterns by concatenating original features to the attentive features at the channel dimension, followed by a 1 × 1 × 1 bottleneck convolution ξ 1×1×1 to squeeze the channel features (named as RAM), as shown in the following equations,</p><formula xml:id="formula_3">f d = Concat(f d 1 , f d 2 ), d = 0, 1, 2, ξ 1×1×1 Concat(f d 1 , f d 2 , f d A ) , d = 3, 4,<label>(3)</label></formula><p>where the lower-level features from both encoders are fused by concatenation.</p><p>The decoder D generates a probabilistic prediction</p><formula xml:id="formula_4">G 2 = D(f 0 , • • • , f 4</formula><p>) with skip connections (Fig. <ref type="figure" target="#fig_0">1</ref>). We utilize the 3D Dice <ref type="bibr" target="#b13">[14]</ref> loss function,</p><formula xml:id="formula_5">L DICE (G 2 , G 2 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training Strategy</head><p>The network should learn from three aspects: 1) Tumor volume variation: the structural changes of the tumor from the first to the second course; 2) Cancer cell proliferation: The tumor in esophageal cancer tends to infiltrate into the adjacent tissue; 3) Reliance of GTV on esophageal anatomy: The anatomical dependency between esophageal GTV and the position of the esophagus. Medical images are sparsely labeled which are isolated by different tasks <ref type="bibr" target="#b20">[20]</ref>, and are often inadequate. In this study, we use a paired first-second course GTV dataset S p , an unpaired GTV dataset S v , and a public esophagus dataset S e .</p><p>In order to fully leverage both public and private datasets, the training objective should not be specific to any tasks. Here, we denote G 1 /G 2 as prior/target annotations respectively, which are not limited only to the GTV areas. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, our strategy is to challenge the network to retrieve information from augmented inputs in E 1 using the features from E 2 , which can incorporate a wide range of datasets that are not tailored for second-course GTV segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tumor Volume Variation</head><p>The differences in tumor volume between the first and second courses following an RT treatment can have a negative impact on the state-of-the-art (SOTA) learning-based techniques, which will be discussed in Sect. 4.2. To adequately monitor changes in tumor volume and integrate information from the initial course into the subsequent course, a paired first-second courses dataset S p = {i 1  p , i 2 p , g 1 p ; g 2 p } is necessary for training. In S p , i 1 p and i 2 p are the first and second course CT images, while g 1 p and g 2 p are the corresponding GTV annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cancer Cell Proliferation</head><p>The paired dataset S p for the first and second courses is limited, whereas an unpaired GTV dataset S v = {i v ; g v } can be easily obtained in a standard clinical workflow with a substantial amount. S v lacks its counterpart for the second course, in which i v /g v are the CT image and the corresponding annotation for GTV. To address this, we apply two distinct randomized augmentations, P 1 , P 2 , to mimic the unregistered issue of the first and second course CT. The transformed data is feed into the encoders E 1/2 as shown in the following equations:</p><formula xml:id="formula_6">I 1 , G 1 , I 2 , G 2 = ⎧ ⎨ ⎩ P 1 (i 1 p ), P 1 (g 1 p ), P 2 (i 2 p ), P 2 (g 2 p ), when i 1 p , i 2 p , g 1 p , g 2 p ∈ S p , P 1 (i v ), P 1 (g v ), P 2 (i v ), P 2 (g v ), when i v , g v ∈ S v , P 1 (i e )</formula><p>, P 1 (g e ), P 2 (i e ), P 2 (g e ), when i e , g e ∈ S e .</p><p>(4)</p><p>The esophageal tumor can proliferate with varying morphologies into the surrounding tissues. Although not paired, S v contains valuable information about the tumor. Challenging the network to query information within GTV will enhance the capacity to retrieve pertinent information for the tumor positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reliance of GTV on Esophageal Anatomy</head><p>To make full use of the datasets of relevant tasks, we incorporate a public esophagus segmentation dataset, denoted as S e = {i e ; g e }, where i e /g e represent the CT images and corresponding annotations of the esophagus structure. By augmenting the data as described in Eq. ( <ref type="formula">4</ref>), S e challenges the network to extract information from the entire esophagus, which enhances the network's embedding space with anatomical prior knowledge of the esophagus. Similarly, data from the paired S p is also augmented by P 1/2 to increase the network's robustness.</p><p>In summary, our training strategy is not dataset-specific or target-specific, thus allowing the integration of prior knowledge from multi-center esophageal GTV-related datasets, which effectively improves the network's ability to retrieve information for the second course from the three key aspects stated in Sect. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. The paired first-second course dataset, S p , is collected from Sun Yat-Sen University Cancer Center (Ethics Approval Number: B2023-107-01), comprising paired CT scans of 69 distinct patients from South China. We collected the GTV dataset S v from MedMind Technology Co., Ltd., which has CT scans from 179 patients. For both S p and S v , physicians annotated the esophageal cancer GTV in each CT. The GTV volume statistics (cm 3 , mean ± std.) in S v is 40.60 ± 29.75, and is 83.70 ± 55.97/71.66 ± 49.36 for the first/second course RT in S p respectively. Additionally, we collect S e from SegTHOR <ref type="bibr" target="#b11">[12]</ref>, consisting of CT scans and esophagus annotations from 40 patients who did not  Implementation Details. The CT volumes from the first and second course in S p are aligned based on the center of the lung mask <ref type="bibr" target="#b7">[8]</ref>. The CT volumes are applied with a windowing of [-100, 300] HU, and resampled to 128 3 , with a voxel size of 1.2 × 1.2 × 3 mm 3 . The augmentations P 1/2 involve a combination of random 3D resized cropping, flipping, rotation in the transverse plane, and Gaussian noise. We employ the Adam <ref type="bibr" target="#b10">[11]</ref> optimizer with (β 1 , β 2 , lr) = (0.9, 0.999, 0.001) for training for 500 epoches. The network is implemented using PyTorch <ref type="bibr" target="#b1">[2]</ref> and MONAI <ref type="bibr" target="#b0">[1]</ref>, and detailed configurations are in the supplementary material. Experiments are performed on an NVIDIA RTX 3090 GPU with 24GB memory.</p><p>Performance Metrics. Dice score (DSC), averaged surface distance (ASD) and Hausdorff distance (HSD) are used as metrics for evaluation. The Wilcoxon signed-rank test is used to compare the performance of different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Domain Gap Between the First and Second Course</head><p>As previously mentioned, the volume of the tumors changes after the first course of RT. To demonstrate the presence of a domain gap between the first and second courses, we train SOTA methods with datasets S train p and S v , by feeding the data sequentially into the network. We then evaluate the models on S test p . The results presented in Table <ref type="table" target="#tab_0">1</ref> indicate a performance gap between GTV segmentation in the first and second courses, with the latter being more challenging. Notably, the paired first-second course dataset S test p pertains to the same group of patients, thereby ensuring that any performance drop can be attributed solely to differences in courses of RT, rather than variations across different patients.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates the reduction in the GTV area after the initial course of RT, where the transverse plane is taken from the same location relative to the vertebrae (yellow lines). The blue arrows indicate that the networks failed to track these changes and produced false predictions in the second course of RT. This suggests that deep learning-based approaches may not rely solely on the identification of malignant tissue patterns, as doctors do, but rather predict highrisk areas statistically. Therefore, for accurate second-course GTV segmentation, we need to explicitly propagate prior information from the first course using dual encoders in ARTSEG, and incorporate learning about tumor changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluations of Second-Course GTV Segmentation Performance</head><p>Combination of Various Datasets. Table <ref type="table" target="#tab_1">2</ref> presents the information gain derived from multi-center datasets using quantified metrics for segmentation performance. We first utilize a standard ARTSEG (w/o RAM) as an ablation network. When prior information from the first course is explicitly introduced using S p , ARTSEG outperforms other baselines for GTV segmentation in the second course, which reaches a DSC of 66.73%. However, in Fig. <ref type="figure" target="#fig_2">3</ref>, it can be observed that the model failed to accurately track the GTV area along the esophagus (orange arrows) due to the soft and elongated nature of the esophageal tissue, which deforms easily during CT scans performed at different times.</p><p>By subsequently incorporating S e for structural esophagus prior knowledge, the DSC improved to 69.42%. Meanwhile, the esophageal tumor comprises two primary regions, the original part located in the esophagus and the extended part that has invaded the surrounding tissue. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, identifying the tumor proliferation into the surrounding tissue without comprehensive knowledge of tumor morphology can be challenging (blue arrows). To address this, incorporating S v to comprehensively learn the tumor morphology is required.</p><p>When S v is incorporated for learning tumor proliferation, the DSC improved to 72.64%. We can observe from Case 2 in Fig. <ref type="figure" target="#fig_2">3</ref> that the network has a better understanding of the tumor proliferation with S v , while it still fails to track the GTV area along the esophagus as pointed by the orange arrow. Therefore, S v and S e improve the network from two distinct aspects and are both valuable. Our proposed training strategy fully exploits the datasets S p , S v , and S e , and  further improve the DSC to 74.54% by utilizing comprehensive knowledge of both the tumor morphology and esophageal structures.</p><p>Region-Preserving Attention Module. Although introducing the esophageal structural prior knowledge using S e can improve the performance in DSC and ASD (Table <ref type="table" target="#tab_1">2</ref>), the increase in HSD (38.22 to 47.89 mm; 21.71 to 27.00 mm) indicates that there are outliers far from the ground truth boundaries. This may be attributed to the convolution that cannot effectively handle the long-range knowledge of the esophagus structure. The attention mechanism can effectively capture the long-range relationship as shown recently in <ref type="bibr" target="#b12">[13]</ref>.</p><p>However, there is no performance gain with MHA as shown in Table <ref type="table" target="#tab_1">2</ref>, and the HSD further increased to 27.33 mm. We attribute the drawback is due to the location-agnostic nature of the operations in MHA, where the local regional correlations are perturbed.</p><p>To tackle the aforementioned problem, we propose RAM which involves the concatenation of the original features with attention outputs, allowing for the preservation of convolution-generated regional tumor patterns while effectively comprehending long-range prior knowledge specific to the esophagus. Finally, our proposed ARTSEG with RAM achieves the best DSC/HSD of 75.26%/19.75 mm, and outperforms its ablations as well as other baselines, as shown in Table <ref type="table" target="#tab_1">2</ref>.</p><p>Limitations. For the method's generalizability, analysis of diverse imaging protocols and segmentation backbones are inadequate. Besides, ARTSEG requires more computational resources due to its dual-encoder and attention design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we reveal the domain gap between the first and second courses of RT for esophageal GTV segmentation. To improve the accuracy of GTV declination in the second course, we explicitly incorporated the naturally existing prior information from the first course. Besides, to efficiently leverage prior knowledge contained in various medical CT datasets, we train the network in an information-querying manner. We proposed RAM to capture long-range prior knowledge in the esophageal structure, while preserving the regional tumor patterns. Our proposed ARTSEG incorporates prior knowledge of the tumor volume variation, cancer cell proliferation, and reliance of GTV on esophageal anatomy, which enhances the GTV segmentation accuracy in the second course RT. Our future research includes accurate delineation for multiple targets in the second course and knowledge transferring through the time series of multiple courses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1.Our training approach leverages multi-center datasets containing relevant annotations, that challenges the network to retrieve information from E1 using the features from E2. The decoder D utilizes the prior knowledge obtained from I1 and G1 to generate the mask prediction. Our training strategy leverages three datasets that introduce prior knowledge to the network of the following three key aspects: 1) Tumor volume variation, 2) Cancer cell proliferation, and 3) Reliance of GTV on esophageal anatomy.</figDesc><graphic coords="3,58,98,54,47,334,48,118,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The domain gap observed between the first and second courses RT. The blue arrows indicate that the methods tend to exhibit false delineation in the second course, suggesting a lack of consideration for tumor changes after the first course. (Yellow: Locations of the transverse planes; Green: GTV ground truth contours; Red: predictions.) (Color figure online)</figDesc><graphic coords="6,73,80,195,41,284,92,94,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The impact of different prior knowledge on esophageal tumor detection. Networks with inadequate knowledge of the esophagus may fail in identifying the tumor within the esophagus (orange arrows), whereas a limited understanding of tumor morphology can deteriorate the ability to detect the tumor in the adjacent area (blue arrows). Our proposed approach, encompassing comprehensive prior knowledge, shows superior performance. (Green: GTV ground truth contours; Red: predictions.) (Color figure online)</figDesc><graphic coords="8,70,02,230,96,291,52,145,72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The results suggest a domain gap between the first and second courses, which indicates increased difficulty in GTV segmentation for the second course. Asterisks indicate p-value &lt; 0.05 for the performance gap between the first and second course.</figDesc><table><row><cell>Methods</cell><cell>First Course</cell><cell>Second Course</cell></row><row><cell></cell><cell>DSC (%) ↑</cell><cell></cell></row></table><note><p>ASD (mm) ↓ DSC (%) ↑ ASD (mm) ↓ mean ± std. med. mean ± std. med. mean ± std. med. mean ± std. med. UNETR [7] 59.77 ± 20.24 62.90 10.57 ± 14.66 7.06 53.03 ± 17.62* 55.17 11.29 ± 11.44 8.42 Swin UNETR [6] 60.84 ± 19.74 64.07 10.29 ± 17.78 6.67 57.04 ± 20.16* 60.73 9.76 ± 15.43 6.21 DenseUnet [19] 63.95 ± 18.23 68.11 8.94 ± 13.82 6.04 55.35 ± 18.59* 58.54 9.84 ± 6.91* 8.54 3D U-Net [5] 66.73 ± 17.21 69.86 8.04 ± 16.83 4.19 57.50 ± 19.49* 62.62 9.14 ± 12.03* 6.09</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison of GTV segmentation performance in the second course. Our proposed ARTSEG+RAM achieved better overall performance, where asterisks indicate ARTSEG+RAM outperforms other methods with p-value &lt; 0.05. 20.16* 60.73 9.76 ± 15.43* 6.21 58.73 ± 46.67* 51.00 59.25 3D U-Net § [5] 57.50 ± 19.49* 62.62 9.14 ± 12.03* 6.09 63.54 ± 49.59* 48.29 4.01 ARTSEG w/o RAM 66.73 ± 16.20* 71.85 4.45 ± 4.10* 3.10 38.22 ± 27.51* 31.22 9.05 69.42 ± 11.05* 71.12 3.98 ± 3.03* 2.99 47.89 ± 50.82* 33.04 72.64 ± 13.53* 74.64 2.69 ± 1.69 2.23 21.71 ± 11.95 ± 12.24 76.40 2.39 ± 1.57 1.73 19.75 ± 11.83 15.54 12.60 §Without explicit information from the first-course. (Best methods in Table 1)</figDesc><table><row><cell>Methods</cell><cell>S train p</cell><cell>Sv Se DSC (%) ↑</cell><cell>ASD(mm) ↓</cell><cell>HSD(mm) ↓</cell><cell>Inference</cell></row><row><cell></cell><cell></cell><cell>mean ± std.</cell><cell cols="2">med. mean ± std. med. mean ± std.</cell><cell>med. speed(ms)</cell></row><row><cell>Swin UNETR  § [6]</cell><cell></cell><cell cols="4">57.04 ± 19.69</cell></row><row><cell>ARTSEG w/o RAM</cell><cell></cell><cell>74.54 ± 13.33</cell><cell>76.84 2.51 ± 1.85</cell><cell>1.83 27.00 ± 41.79</cell><cell>16.35</cell></row><row><cell>ARTSEG+MHA [17]</cell><cell></cell><cell>74.34 ± 13.27</cell><cell>76.25 2.49 ± 1.62</cell><cell>1.97 27.33 ± 33.25</cell><cell>15.99 12.24</cell></row><row><cell>ARTSEG+RAM</cell><cell></cell><cell>75.26</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. Thanks to <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2022YFC2405200</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">82027807</rs>, <rs type="grantNumber">U22A2051</rs>), <rs type="funder">Beijing Municipal Natural Science Foundation</rs> (<rs type="grantNumber">7212202</rs>), <rs type="funder">Institute for Intelligent Healthcare, Tsinghua University</rs> (<rs type="grantNumber">2022ZLB001</rs>), <rs type="funder">Tsinghua-Foshan Innovation Special Fund</rs> (<rs type="grantNumber">2021THFS0104</rs>), <rs type="funder">Guangdong Esophageal Cancer Institute Science and Technology Program</rs> (<rs type="grantNumber">Q202221</rs>, <rs type="grantNumber">Q202214</rs>, <rs type="grantNumber">M-202016</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cSaYcTR">
					<idno type="grant-number">2022YFC2405200</idno>
				</org>
				<org type="funding" xml:id="_Vq8R3kH">
					<idno type="grant-number">82027807</idno>
				</org>
				<org type="funding" xml:id="_Q8gmAyx">
					<idno type="grant-number">U22A2051</idno>
				</org>
				<org type="funding" xml:id="_sY5jhnC">
					<idno type="grant-number">7212202</idno>
				</org>
				<org type="funding" xml:id="_U9faJEa">
					<idno type="grant-number">2022ZLB001</idno>
				</org>
				<org type="funding" xml:id="_v7HZVjQ">
					<idno type="grant-number">2021THFS0104</idno>
				</org>
				<org type="funding" xml:id="_ERa5KJr">
					<idno type="grant-number">Q202221</idno>
				</org>
				<org type="funding" xml:id="_umrgnXV">
					<idno type="grant-number">Q202214</idno>
				</org>
				<org type="funding" xml:id="_bVZ5N6e">
					<idno type="grant-number">M-202016</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://monai.io/" />
		<title level="m">Medical Open Network for Artificial Intelligence (MONAI)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soerjomataram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jemal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA: Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="394" to="424" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Defining the tumour and target volumes for radiotherapy</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Burnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Jefferies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Imaging</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">U-net: deep learning for cell counting, detection, and morphometry</title>
		<author>
			<persName><forename type="first">T</forename><surname>Falk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="70" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Swin UNETR: swin transformers for semantic segmentation of brain tumors in MRI images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-08999-2_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-08999-2_22" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">12962</biblScope>
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UNETR: transformers for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic lung segmentation in routine imaging is primarily a data diversity problem, not a methodology problem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hofmanninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Prayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Röhrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Prosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<idno type="DOI">10.1186/s41747-020-00173-2</idno>
		<idno>41747-020-00173-2</idno>
		<ptr target="https://doi.org/10.1186/s" />
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol. Exp</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DeepTarget: gross tumor and clinical target volume segmentation in esophageal cancer radiotherapy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">101909</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accurate esophageal gross tumor volume segmentation in PET/CT using two-stream chained 3D deep network fusion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_21" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="182" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Segthor: segmentation of thoracic organs at risk in CT images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dubray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transforming medical imaging with transformers? a comparative review of key properties, current progresses, and future perspectives</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">102762</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Oesophageal carcinoma</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pennathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Jobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Luketich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">9864</biblScope>
			<biblScope unit="page" from="400" to="412" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Carcinoma of the esophagus: results of treatment</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Van Andel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Surg</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="684" to="689" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Esophageal tumor segmentation in CT images using a dilated dense attention Unet (DDAUnet)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yousefi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="99235" to="99248" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Esophageal gross tumor volume segmentation using a 3D convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yousefi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_40</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-3_40" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A review of deep learning in medical imaging: imaging traits, technology trends, case studies with progress highlights, and future promises</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="820" to="838" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Handbook of Medical Image Computing and Computer Assisted Intervention</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
