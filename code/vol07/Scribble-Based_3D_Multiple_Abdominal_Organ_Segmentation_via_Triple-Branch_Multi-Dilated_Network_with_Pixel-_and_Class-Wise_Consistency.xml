<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency</title>
				<funder ref="#_NRvbTud">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_zFFNwYd">
					<orgName type="full">Radiation Oncology Key Laboratory of Sichuan Province Open Fund</orgName>
				</funder>
				<funder ref="#_sTrvYCb">
					<orgName type="full">Science and Technology Department of Sichuan Province, China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Meng</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical and Electrical Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangde</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical and Electrical Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenjun</forename><surname>Liao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiation Oncology</orgName>
								<orgName type="institution">Sichuan Cancer Hospital and Institute</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shichuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiation Oncology</orgName>
								<orgName type="institution">Sichuan Cancer Hospital and Institute</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical and Electrical Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guotai</forename><surname>Wang</surname></persName>
							<email>guotai.wang@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical and Electrical Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="33" to="42"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">66E841E23B7CC6D59B3C255EEE1417C1</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Weakly-supervised learning</term>
					<term>Scribble annotation</term>
					<term>Uncertainty</term>
					<term>Consistency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-organ segmentation in abdominal Computed Tomography (CT) images is of great importance for diagnosis of abdominal lesions and subsequent treatment planning. Though deep learning based methods have attained high performance, they rely heavily on largescale pixel-level annotations that are time-consuming and labor-intensive to obtain. Due to its low dependency on annotation, weakly supervised segmentation has attracted great attention. However, there is still a large performance gap between current weakly-supervised methods and fully supervised learning, leaving room for exploration. In this work, we propose a novel 3D framework with two consistency constraints for scribblesupervised multiple abdominal organ segmentation from CT. Specifically, we employ a Triple-branch multi-Dilated network (TDNet) with one encoder and three decoders using different dilation rates to capture features from different receptive fields that are complementary to each other to generate high-quality soft pseudo labels. For more stable unsupervised learning, we use voxel-wise uncertainty to rectify the soft pseudo labels and then supervise the outputs of each decoder. To further regularize the network, class relationship information is exploited by encouraging the generated class affinity matrices to be consistent across different decoders under multi-view projection. Experiments on the public WORD dataset show that our method outperforms five existing scribble-supervised methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abdominal organ segmentation from medical images is an essential work in clinical diagnosis and treatment planning of abdominal lesions <ref type="bibr" target="#b16">[17]</ref>. Recently, deep learning methods based on Convolution Neural Network (CNN) have achieved impressive performance in medical image segmentation tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24]</ref>. However, their success relies heavily on large-scale high-quality pixel-level annotations that are too expensive and time-consuming to obtain, especially for multiple organs in 3D volumes. Weakly supervised learning with a potential to reduce annotation costs has attracted great attention. Commonly-used weak annotations include dots <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>, scribbles <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>, bounding boxes <ref type="bibr" target="#b4">[5]</ref>, and imagelevel tags <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>. Compared with the other weak annotations, scribbles can provide more location information about the segmentation targets, especially for objects with irregular shapes <ref type="bibr" target="#b0">[1]</ref>. Therefore, this work focuses on exploring high-performance models for multiple abdominal organ segmentation based on scribble annotations.</p><p>Training CNNs for segmentation with scribble annotations has been increasingly studied recently. Existing methods are mainly based on pseudo label learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, regularized losses <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref> and consistency learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>. Pseudo label learning methods deal with unannotated pixels by generating fake semantic labels for learning. For example, Luo et al. <ref type="bibr" target="#b14">[15]</ref> introduced a network with two slightly different decoders that generate dynamically mixed pseudo labels for supervision. Liang et al. <ref type="bibr" target="#b10">[11]</ref> proposed to leverage minimum spanning trees to generate low-level and high-level affinity matrices based on color information and semantic features to refine the pseudo labels. Arguing that the pseudo label learning may be unreliable, Tang et al. <ref type="bibr" target="#b21">[22]</ref> introduced the Conditional Random Field (CRF) regularization loss for image segmentation directly. Obukhov et al. <ref type="bibr" target="#b17">[18]</ref> proposed to incorporate the gating function with CRF loss considering the directionality of unsupervised information propagation. Recently, consistency strategies that encourage consistent outputs of the network for the same input under different perturbations have achieved increasing attentions. Liu et al. <ref type="bibr" target="#b12">[13]</ref> introduced transformation-consistency based on an uncertaintyaware mean teacher <ref type="bibr" target="#b3">[4]</ref> model. Zhang et al. <ref type="bibr" target="#b25">[26]</ref> proposed a framework composed of mix augmentation and cycle consistency. Although these scribble-supervised methods have achieved promising results, their performance is still much lower than that of fully-supervised training, leaving room for improvement.</p><p>Differently from most existing weakly supervised methods that are designed for 2D slice segmentation with a single or few organs, we propose a highly optimized 3D triple-branch network with one encoder and three different decoders, named TDNet, to learn from scribble annotations for segmentation of multiple abdominal organs. Particularly, the decoders are assigned with different dilation rates <ref type="bibr" target="#b24">[25]</ref> to learn features from different receptive fields that are complementary to each other for segmentation, which also improves the robustness of dealing with organs at different scales as well as the feature learning ability of the shared encoder. Considering the features at different scales learned in these decoders, we fuse these multi-dilated predictions to obtain more accurate soft pseudo labels rather than hard labels <ref type="bibr" target="#b14">[15]</ref> that tend to be over-confidence predictions. For more stable unsupervised learning, we use voxel-wise uncertainty to rectify the soft pseudo labels and then impose consistency constraints on the output of each branch. In addition, we extend the consistency to the class-related information level <ref type="bibr" target="#b22">[23]</ref> to constrain inter-class affinity for better distinguishing them. Specifically, we generate the class affinity matrices in different decoders and encourage them to be consistent after projection in different views. The contributions of this paper are summarized as follows: 1) We propose a novel 3D Triple-branch multi-Dilated network called TDNet for scribblesupervised segmentation. By equipping with varying dilation rates, the network can better leverage multi-scale context for dealing with organs at different scales.</p><p>2) We propose two novel consistency loss functions, i.e., Uncertainty-weighted Soft Pseudo label Consistency (USPC) loss and Multi-view Projection-based Class-similarity Consistency (MPCC) loss, to regularize the prediction from the pixel-wise and class-wise perspectives respectively, which helps the segmentation network obtain reliable predictions on unannotated pixels. 3) Experiments results show our proposed method outperforms five existing scribble-supervised methods on the public dataset WORD <ref type="bibr" target="#b16">[17]</ref> for multiple abdominal organ segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the proposed framework for scribble-supervised medical image segmentation. We introduce a network with one encoder and three decoders with different dilation rates to learn multi-scale features. The decoders' outputs are averaged to generate a soft pseudo label that is rectified by uncertainty and then used to supervise each branch. To better deal with multi-class segmentation, a class similarity consistency loss is also used for regularization.</p><p>For the convenience of following description, we first define several mathematical symbols. Let X, S be a training image and the corresponding scribble annotation, respectively. Let C denote the number of classes for segmentation, and Ω = Ω S ∪ Ω U denote the whole set of voxels in X, where Ω S is the set of labeled pixels annotated in S, and Ω U is the unlabeled pixel set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Triple-Branch Multi-Dilated Network (TDNet)</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), the proposed TDNet consists of a shared encoder (θ e ) and three independent decoders (θ d1 , θ d2 , θ d3 ) with different dilation rates to mine unsupervised context from different receptive fields. Specifically, decoders using convolution with small dilation rates can extract detailed local features but their receptive fields are small for understanding a global context. Decoders using convolution with large dilation rates can better leverage the global information but may lose some details for accurate segmentation. In this work, our TDNet is implemented by introducing two auxiliary decoders into a 3D UNet <ref type="bibr" target="#b2">[3]</ref>. The dilation rate in the primary decoder and the two auxiliary decoders are 1, 3 and 6 respectively, with the other structure parameters (e.g., kernel size, channel number etc.) being the same in the three decoders. To further introduce perturbations for obtaining diverse outputs, the three branches are initialized with Kaiming initialization, Xavier and Normal initialization methods, respectively. In addition, the bottleneck's output features are randomly dropped out before sending into the auxiliary decoders. The probability prediction maps obtained by the three decoders are denoted as P 1 , P 2 and P 3 , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pixel-Wise and Class-Wise Consistency</head><p>Uncertainty-Weighted Soft Pseudo Label Consistency (USPC). As the three decoders capture features at different scales that are complementary to each other, an ensemble of them would be more robust than a single branch. Therefore, we take an average of P 1 , P 2 , P 3 to get a better soft pseudo label P = (P 1 + P 2 + P 3 )/3 that is used to supervise each branch during training. However, P may also contain noises and be inaccurate, and it is important to highlight reliable pseudo labels while suppressing unreliable ones. Thus, we propose a regularization term named Uncertainty-weighted Soft Pseudo label Consistency (USPC) between P n (n = 1, 2, 3) and P :</p><formula xml:id="formula_0">L USP C = 1 3 n=1,2,3 i w i KL(P n,i Pi ) i w i (1)</formula><p>where Pi refers to the prediction probability at voxel i in P , and Pn,i is the corresponding prediction probability at voxel i in Pn . KL() is the Kullback-Leibler divergence. w i is the voxel-wise weight based on uncertainty estimation:</p><formula xml:id="formula_1">w i = e c P c i log( P c i )<label>(2)</label></formula><p>where the uncertainty is estimated by entropy. c is the class index, and P c i means the probability for class c at voxel i in the pseudo label. Note that a higher uncertainty leads to a lower weight. With the uncertainty-based weighting, the model will be less affected by unreliable pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-view Projection-Based Class-Similarity Consistency (MPCC).</head><p>For multi-class segmentation tasks, it is important to learn inter-class relationship for better distinguishing them. In addition to using L USP C for pixel-wise supervision, we consider making consistency on class relationship across the outputs of the decoders as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. In order to save computing resources, we project the soft pseudo labels along each dimension and then calculate the affinity matrices, which also strengthens the class relationship information learning. We first project the soft prediction map of the n-th decoder P n ∈ R C×D×H×W in axial view to a tensor with the shape of C × 1 × H × W . It is reshaped into C ×(W H) and multiplied by its transposed version, leading to a class affinity matrix</p><formula xml:id="formula_2">Q axial n ∈ R C×C . A normalized version of Q axial n is denoted as Q axial n = Q axial n /||Q axial n ||.</formula><p>Similarly, P n is projected in the sagittal and coronal views, respectively, and the corresponding normalized class affinity matrices are denoted as Q sagittal n and Q coronal n , respectively. Here, the affinity matrices represents the relationship between any pair of classes along the dimensions. Then we constraint the consistency among the corresponding affinity matrices by Multi-view Projection-based Class-similarity Consistency (MPCC) loss:</p><formula xml:id="formula_3">L MP CC = 1 3 × 3 v n=1,2,3 KL(Q v n Qv )<label>(3)</label></formula><p>where v ∈ {axial, sagittal, coronal} is the view index, and Qv is the average class affinity matrix in a certain view obtained by the three decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Overall Loss Function</head><p>To learn from the scribbles, the partially Cross-Entropy (pCE) loss is used to train the network, where the labeled pixels are considered to calculate the gradient and the other pixels are ignored <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_4">L sup = - 1 3 |Ω S | n=1,2,3 i∈ΩS c S c i log P c n,i<label>(4)</label></formula><p>where S represents the one-hot scribble annotation, and Ω S is the set of labeled pixels in S. The total object function is summarized as:</p><formula xml:id="formula_5">L total = L sup + α t L USP C + β t L MP CC (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where α t and β t are the weights for the unsupervised losses. Following <ref type="bibr" target="#b12">[13]</ref>, we define α t based on a ramp-up function: α t = α • e (-5(1-t/tmax) 2 ) , where t denotes the current training step and t max is the maximum training step. We define β t = β • e (-5(1-t/tmax) 2 ) in a similar way. In this way, the model can learn accurate information from scribble annotations, which also avoids getting stuck in a degenerate solution due to low-quality pseudo labels at an early stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>We used the publicly available abdomen CT dataset WORD <ref type="bibr" target="#b16">[17]</ref> for experiments, which consists of 150 abdominal CT volumes from patients with rectal cancer, prostate cancer or cervical cancer before radiotherapy. Each CT volume contains 159-330 slices of 512×512 pixels, with an in-plane resolution of 0.976 × 0.976 mm and slice spacing of 2.5-3.0 mm. We aimed to segment seven organs: the liver, spleen, left kidney, right kidney, stomach, gallbladder and pancreas. Following the default settings in <ref type="bibr" target="#b16">[17]</ref>, the dataset was split into 100 for training, 20 for validation and 30 for testing, respectively, where the scribble annotations for foreground organs and background in the axial view of the training volumes had been provided and were used in model training. For pre-processing, we cut off the Hounsfield Unit (HU) values with a fixed window/level of 400/50 to focus on the abdominal organs, and normalized it to [0, 1]. We used the commonlyadopted Dice Similarity Coefficient (DSC), 95% Hausdorff Distance (HD 95 ) and the Average Surface Distance (ASD) for quantitative evaluation.</p><p>Our framework was implemented in PyTorch <ref type="bibr" target="#b18">[19]</ref> on an NVIDIA 2080Ti with 11 GB memory. We employed the 3D UNet <ref type="bibr" target="#b2">[3]</ref> as the backbone network for all experiments, and extended it with three decoders by embedding two auxiliary decoders with different dilation rates, as detailed in Sect. 2.1. To introduce perturbations, different initializations were applied to each decoder, and random perturbations (ratio = (0, 0.5)) were introduced in the bottleneck before the auxiliary decoders. The Stochastic Gradient Descent (SGD) optimizer with momentum of 0.9 and weight decay of 10 -4 was used to minimize the overall loss function formulated in Eq. 5, where α=10.0 and β=1.0 based on the best performance on the validation set. The poly learning rate strategy <ref type="bibr" target="#b15">[16]</ref> was used to decay learning rate online. The batch size, patch size and maximum iterations t max were set to 1, [80, 96, 96] and 6 × 10 4 respectively. The final segmentation results were obtained by using a sliding window strategy. For a fair comparison, we used the primary decoder's outputs as the final results during the inference stage and did not use any post-processing methods. Note that all experiments were conducted in the same experimental setting. The existing methods are implemented with the help of open source codebase from <ref type="bibr" target="#b13">[14]</ref>.</p><p>Table <ref type="table">1</ref>. Quantitative comparison between our method and existing weakly supervised methods on WORD testing set. * denotes p-value &lt; 0.05 (paired t-test) when comparing with the second place method <ref type="bibr" target="#b14">[15]</ref>. The best values are highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Organ</head><p>FullySup <ref type="bibr" target="#b2">[3]</ref> pCE TV <ref type="bibr" target="#b8">[9]</ref> USTM <ref type="bibr" target="#b12">[13]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with Other Methods</head><p>We compared our method with five weakly supervised segmentation methods with the same set of scribbles, including pCE only <ref type="bibr" target="#b11">[12]</ref>, Total Variation Loss (TV) <ref type="bibr" target="#b8">[9]</ref>, Uncertainty-aware Self-ensembling and Transformation-consistent Model (USTM) <ref type="bibr" target="#b12">[13]</ref>, Entropy Minimization (EM) <ref type="bibr" target="#b7">[8]</ref> and Dynamically Mixed Pseudo Labels Supervision (DMPLS) <ref type="bibr" target="#b14">[15]</ref>. They were also compared with the upper bound by using dense annotation to train models (FullySup) <ref type="bibr" target="#b2">[3]</ref>. The results in Table <ref type="table">1</ref> show that our method leads to the best DSC, ASD and HD 95 .</p><p>Compared with the second best method DMPLS <ref type="bibr" target="#b14">[15]</ref>, the average DSC was increased by 2.67 percent points, and the average ASD and HD 95 were decreased by 5.44 mm and 16.16 mm, respectively. It can be observed that TV <ref type="bibr" target="#b8">[9]</ref> obtained a worse performance than pCE, which is mainly because that method classifies pixels by minimizing the intra-class intensity variance, making it difficult to achieve good segmentation due to the low contrast. Figure <ref type="figure">2</ref> shows a visual comparison  between our method and the other weakly supervised methods on the WORD dataset (word 0014.nii). It can be obviously seen that the results obtained by our method are closer to the ground truth, with less mis-segmentation in both slice level and volume level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Experiment</head><p>We then performed ablation experiments to investigate the contribution of each part of our method, and the quantitative results on the validation set are shown in Table <ref type="table" target="#tab_1">2</ref>, where L USP C (-ω) means using L USP C without pixelwise uncertainty rectifying. Baseline refers to a triple-branch model with different initializations and random feature-level dropout in the bottleneck, supervised by pCE only. It can be observed that by using L USP C (-ω) with mutiple decoders, the model segmentation performance is greatly enhanced with average DSC increasing by 7.70%, ASD and HD 95 decreasing by 16.11 mm and 48.87 mm, respectively. By equipping each decoders with different dilation rates, the model's performance is further improved, especially in terms of ASD and HD 95 , which proves our hypothesis that learning features from different scales can improve the segmentation accuracy. Replacing L USP C (-ω) with L USP C further improved the DSC to 84.21%, and reduced the ASD and HD 95 by 0.52 mm and 1.01 mm through utilizing the uncertainty information. Visual comparison in Fig. <ref type="figure" target="#fig_1">3</ref> demonstrates that over-segmentation can be mitigated by using different dilation rates in the three decoders, and using the uncertainty-weighted pseudo labels can further improve the segmentation accuracy with small false positive regions removing.</p><p>Additionally, Table <ref type="table" target="#tab_1">2</ref> shows that combining L USP C and L MP CC obtained the best performance, where the average DSC, ASD and HD 95 were 84.75%, 2.64 mm and 7.91 mm, respectively, which demonstrates the effectiveness of the proposed class similarity consistency. In order to find the optimal number of decoders, we set the decoder number to 2, 3 and 4 respectively. The quantitative results in the last three rows of Table <ref type="table" target="#tab_1">2</ref> show that using three decoders outperformed using two and four decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed a scribble-supervised multiple abdominal organ segmentation method consisting of a 3D triple-branch multi-dilated network with two-level consistency constraints. By equipping each decoder with different dilation rates, the model leverages features at different scales to obtain high-quality soft pseudo labels. In addition to mine knowledge from unannotated pixels, we also proposed USPC Loss and MPCC Loss to learn unsupervised information from the uncertainty-rectified soft pseudo labels and class affinity matrix information respectively. Experiments on a public abdominal CT dataset WORD demonstrated the effectiveness of the proposed method, which outperforms five existing scribble-based methods and narrows the performance gap between weakly-supervised and fully-supervised segmentation methods. In the future, we will explore the effect of our method on sparser labels, such as a volumetric data with scribble annotations on one or few slices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed Triple-branch multi-Dilated Network (TDNet) that uses different dilation rates at three decoders. The TDNet is optimized by Uncertaintyweighted Soft Pseudo label Consistency (USPC) using the mixed soft pseudo labels and Multi-view Projection-based Class-similarity Consistency (MPCC). The class affinity calculation process is shown in (b). Best viewed in color.</figDesc><graphic coords="3,62,55,62,96,332,14,202,69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.Visualization of the improvement obtained by using different dilation rates and uncertainty rectifying. Best viewed in color.</figDesc><graphic coords="8,251,19,54,02,121,66,98,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of our proposed method on WORD validation set. N(s) and N(d) means N decoders with the same and different dilation rates, respectively. Lsup is used by default. The best values are highlighted in bold.</figDesc><table><row><cell cols="2">Decoder Loss</cell><cell>DSC(%)</cell><cell>ASD(mm) HD95(mm)</cell></row><row><cell>1(s)</cell><cell></cell><cell cols="2">74.70±8.68 25.51±10.12 79.98±30.39</cell></row><row><cell>3(s)</cell><cell>LUSPC(-ω)</cell><cell cols="2">81.92±8.04 9.40±6.79 31.11±20.92</cell></row><row><cell>3(d)</cell><cell>LUSPC(-ω)</cell><cell cols="2">82.57±7.28 3.34±2.67 9.26±7.26</cell></row><row><cell>3(d)</cell><cell>LUSPC</cell><cell cols="2">84.21±6.99 2.82±2.71 8.25±6.36</cell></row><row><cell>3(d)</cell><cell cols="3">LUSPC + LMPCC 84.75±7.01 2.64±2.46 7.91±5.93</cell></row><row><cell>2(d)</cell><cell cols="3">LUSPC + LMPCC 84.18±6.84 2.85±2.38 8.56±6.36</cell></row><row><cell>4(d)</cell><cell cols="3">LUSPC + LMPCC 83.51±7.01 2.88±2.49 8.58±6.53</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62271115</rs>), <rs type="funder">Science and Technology Department of Sichuan Province, China</rs> (<rs type="grantNumber">2022YFSY0055</rs>) and <rs type="funder">Radiation Oncology Key Laboratory of Sichuan Province Open Fund</rs> (<rs type="grantNumber">2022ROKF04</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NRvbTud">
					<idno type="grant-number">62271115</idno>
				</org>
				<org type="funding" xml:id="_sTrvYCb">
					<idno type="grant-number">2022YFSY0055</idno>
				</org>
				<org type="funding" xml:id="_zFFNwYd">
					<idno type="grant-number">2022ROKF04</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scribble2D5: weakly-supervised volumetric image segmentation via scribble annotations</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-123" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="234" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A deep learning-based auto-segmentation system for organs-atrisk on whole-body computed tomography images for radiation therapy</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiother. Oncol</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="175" to="184" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised brain lesion segmentation with an adapted mean teacher model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20351-1_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-20351-1" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2019</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bao</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11492</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Boxsup: exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Annotation by clicks: a point-supervised contrastive variance method for medical semantic segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08774</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmentation only uses sparse annotations: unified weakly and semi-supervised learning in medical images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102515</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised total variation loss for semi-supervised deep learning of semantic segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01368</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mumford-shah loss functional for image segmentation with deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1856" to="1866" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tree energy loss: towards sparsely annotated semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16907" to="16916" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Scribblesup: scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Weakly supervised segmentation of COVID19 infection with scribble annotation on CT images. Pattern Recogn</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page">108341</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<ptr target="https://github.com/Luoxd1996/WSL4MIS" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WSL4MIS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scribble-supervised medical image segmentation via dual-branch network and dynamically mixed pseudo labels supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-650" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="528" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient semi-supervised gross target volume of nasopharyngeal carcinoma segmentation via uncertainty rectified pyramid consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-330" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">WORD: a large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102642</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gated CRF loss for weakly supervised semantic image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Obukhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04651</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pytorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning affinity from attention: end-to-end weakly-supervised semantic segmentation with transformers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16846" to="16855" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Normalized cut loss for weakly-supervised CNN segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1818" to="1827" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On regularized losses for weakly-supervised CNN segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="507" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Abdominal multi-organ segmentation with organ-attention networks and statistical fusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="88" to="102" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Revisiting dilated convolution: a simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7268" to="7277" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cyclemix: a holistic strategy for medical image segmentation from scribble supervision</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11656" to="11665" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
