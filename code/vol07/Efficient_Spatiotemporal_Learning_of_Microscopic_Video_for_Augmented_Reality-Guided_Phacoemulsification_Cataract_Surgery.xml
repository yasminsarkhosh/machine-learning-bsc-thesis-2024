<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery</title>
				<funder ref="#_pZU66gG #_RCxaxuZ #_98bWfmv #_aFbzcgb">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_jFBkPVe #_CD8YWJg">
					<orgName type="full">Shanghai Jiao Tong University Foundation on Medical and Technological Joint Science Research</orgName>
				</funder>
				<funder ref="#_uU5QdWf">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_hEN4UYd #_tURAmbj #_t7g69jG">
					<orgName type="full">Foundation of Science and Technology Commission of Shanghai Municipality</orgName>
				</funder>
				<funder ref="#_sKDXH4t">
					<orgName type="full">Funding of Xiamen Science and Technology Bureau</orgName>
				</funder>
				<funder ref="#_q2dxAeK">
					<orgName type="full">Hospital Funded Clinical Research, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Puxun</forename><surname>Tu</surname></persName>
							<idno type="ORCID">0000-0003-4809-9081</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Biomedical Manufacturing and Life Quality Engineering</orgName>
								<orgName type="department" key="dep2">School of Mechanical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongfei</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology</orgName>
								<orgName type="institution">Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeff</forename><surname>Young</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Bioengineering</orgName>
								<orgName type="institution">University of Texas at Dallas</orgName>
								<address>
									<settlement>Richardson</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology</orgName>
								<orgName type="institution">Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ce</forename><surname>Zheng</surname></persName>
							<email>zhengce@xinhuamed.com.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology</orgName>
								<orgName type="institution">Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaojun</forename><surname>Chen</surname></persName>
							<email>xiaojunchen@sjtu.edu.cn</email>
							<idno type="ORCID">0000-0002-0298-4491</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Biomedical Manufacturing and Life Quality Engineering</orgName>
								<orgName type="department" key="dep2">School of Mechanical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute of Medical Robotics</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="682" to="692"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">0A620321B2D70EB7593C8A380B876EEE</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_64</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cataract surgery</term>
					<term>Augmented reality</term>
					<term>Surgical phase recognition</term>
					<term>Spatiotemporal learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Phacoemulsification cataract surgery (PCS) is typically performed under a surgical microscope and adhering to standard procedures. The success of this surgery depends heavily on the seniority and experience of the ophthalmologist performing it. In this study, we developed an augmented reality (AR) guidance system to enhance the intraoperative skills of ophthalmologists by proposing a two-stage spatiotemporal learning network for surgical microscope video recognition. In the first stage, we designed a multi-task network that recognizes surgical phases and segments the limbus region to extract limbus-focused spatial features. In the second stage, we developed a temporal pyramid-based spatiotemporal feature aggregation (TP-SFA) module that uses causal and dilated temporal convolution for smooth and online surgical phase recognition. To provide phase-specific AR guidance, we designed several intraoperative visual cues based on the parameters of the fitted limbus ellipse and the recognized surgical phase. The comparison experiments results indicate that our method outperforms several strong baselines in surgical phase recognition. Furthermore, ablation experiments show the positive effects of the multi-task feature extractor and TP-SFA module.</p><p>Our developed system has the potential for clinical application in PCS to provide real-time intraoperative AR guidance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cataract is the leading cause of blindness worldwide, and cataract surgery is one of the most common operations in health care. Among different cataract surgery techniques, phacoemulsification cataract surgery (PCS) is the standard of care <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">23]</ref>. PCS consists of the manual opening of the crystalline lens anterior capsule with forceps, removal of the opacified lens, and implantation of an intraocular lens (IOL) in the remaining capsular envelope to restore visual function.</p><p>Phacoemulsification needs microsurgical skills, which depend on numerous variables, including the amount of practice, inherent manual dexterity, and previous experience. Statistical analyses have demonstrated significant differences in completion and complication rates among ophthalmologists, with variations observed based on factors such as seniority and experience <ref type="bibr" target="#b14">[14]</ref>. During phacoemulsification, a surgical microscope with an integrated video camera is routinely used, providing rich spatiotemporal information <ref type="bibr" target="#b15">[15]</ref>. This information presents an excellent opportunity to develop surgical video recognition methods to extract valuable intraoperative information and overlay it on a 2D/3D screen or the microscopic eyepiece, thereby creating an augmented reality (AR) scene.</p><p>To bridge the experience gap among ophthalmologists, several intraoperative AR-guided systems have been developed. Zhai et al. <ref type="bibr" target="#b25">[25]</ref> used two convolutional neural networks (CNNs) to segment the limbus and track the eye's rotation, and subsequently developed an intraoperative guide system for positioning and aligning the IOL. In <ref type="bibr" target="#b16">[16]</ref>, a multi-task CNN was designed to locate the pupil and recognize the surgical phase in each frame of the surgical microscope video. Nespolo et al. <ref type="bibr" target="#b17">[17]</ref> utilized a deep CNN-based method for processing surgical videos, allowing for detecting surgical instruments and tissue boundaries to guide the ophthalmic surgery. Despite the potential of AR-guided phacoemulsification systems, limitations still hinder their clinical implementation. Firstly, the current systems process surgical videos in a frame-wise manner, enabling real-time processing but leading to lost temporal information and incoherent surgical scene recognition. Secondly, the overlaid information during surgery is not categorized by surgical phase, causing visual redundancy for ophthalmologists.</p><p>Advancements in video spatiotemporal learning, particularly in surgical phase recognition, present a promising opportunity to switch AR scenes to the current surgical phase automatically. Early attempts used a 2D CNN to extract spatial features to predict each video frame's surgical phase <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b22">22]</ref>. However, the lack of temporal information leads to unsatisfactory recognition accuracy. Other studies <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b24">24]</ref> use spatial feature maps of neighboring frames, extracted from CNNs, as an input to Gated Recurrent Unit (GRU) <ref type="bibr" target="#b1">[2]</ref> or Long Short-Term Memory (LSTM) <ref type="bibr" target="#b10">[10]</ref> to model the temporal dependencies and predict the surgical phase. However, these methods suffer from limited temporal reception field and non-parallel, slow inference. Recent studies focus on modeling long-range temporal relationships. Czempiel et al. <ref type="bibr" target="#b2">[3]</ref> introduced a multi-stage temporal convolutional network (TCN) for surgical phase recognition, leveraging causal and dilated convolutions to enable global reception field and online deployment. The current state-of-the-art methods utilize transformer-based models for aggre- gating spatial <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">27]</ref> and spatiotemporal features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">12]</ref>. However, focusing solely on global features may result in losing important local temporal dependencies and lead to inaccurate recognition of challenging frames.</p><p>In this study, we developed a novel intraoperative AR-guide system for PCS. Our contributions are two-fold: <ref type="bibr" target="#b0">(1)</ref> We propose an efficient spatiotemporal network for surgical microscope video recognition, consisting of two stages: a multitask learning stage for limbus segmentation and spatial feature extraction, and a temporal pyramid-based spatiotemporal feature aggregation (TP-SFA) module for online surgical phase recognition. (2) We use the limbus and surgical phase information to design phase-specific visual cues that offer real-time intraoperative AR guidance while avoiding distracting the ophthalmologist's attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Figure <ref type="figure" target="#fig_0">1</ref> presents an overview of our developed AR-guided system for PCS, which acquires intraoperative video streams from microscope and processes them using the proposed two-stage spatiotemporal network to obtain limbus and surgical phase information. Parameters of intraoperative visual cues are computed using the fitted limbus elliptic parameters and updated according to the recognized surgical phase, providing automatic AR scene switching for ophthalmologists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spatiotemporal Network for Microscopic Video Recognition</head><p>Limbus Segmentation and Spatial Feature Extraction. We observe that the region within the limbus displays distinguishable appearances at different phases in surgical microscope videos, whereas other regions like the sclera exhibit similar appearances. We argue that using a limbus region-focused spatial feature extraction network can improve spatiotemporal aggregation. This led us to develop a multi-task network for limbus segmentation and phase recognition in the first stage. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), we employ ResNet-50 <ref type="bibr" target="#b9">[9]</ref> as the shared backbone to extract spatial features, which are then fed into both the surgical phase recognition and limbus segmentation branches.</p><p>The surgical phase recognition branch involves a fully connected layer that is directly connected to the global average pooling layer, followed by a softmax layer. To train this branch, we use cross-entropy loss, which is defined as</p><formula xml:id="formula_0">L phase = -( Ns s=1 g s log p s )/N s , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where g s is the ground truth binary indicator of phase s, p s is the probability of the input frame belonging to phase s.</p><p>The limbus segmentation branch incorporates a decoder with upsampling and concatenation, resembling the U-net <ref type="bibr" target="#b20">[20]</ref> architecture. To train this branch, we employ a hybrid loss of cross-entropy and dice, which is defined as</p><formula xml:id="formula_2">L seg = - 1 N × C C c=1 N i=1 y c i log p c i +α(1 - 2 C c=1 N i=1 y c i p c i C c=1 N i=1 y c i + C c=1 N i=1 p c i ),<label>(2)</label></formula><p>where y c i and p c i are the pixel-level ground truth and prediction result respectively, α is a hyper-parameter to balance the loss. The final loss function for training the first stage is defined as</p><formula xml:id="formula_3">L spatial = L phase + βL seg , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where β is a hyper-parameter to balance the loss. After training the first stage, we obtain the spatial feature s t ∈ R 2048 for frame t by outputting the average pooling layer of the spatial feature extractor.</p><p>Spatiotemporal Features Aggregation. We argue that the surgical phase recognition method used for intraoperative AR should fulfill the following requirements: 1) online recognition for real-time intraoperative guidance, and 2) sufficient stability to avoid distracting ophthalmologists with incorrect phase recognition. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b), we employ our proposed TP-SFA module in the second stage, which uses multi-scale, causal, and dilated temporal convolutions to model the temporal relationships. We denote the input spatial feature as S 0 ∈ R 2048×T , where T is the sequence length of the video. The first layer of the TP-SFA module is a 1 × 1 convolutional layer that reduces the dimension of S 0 and outputs P 0 ∈ R 32×T . To obtain temporal features with different reception fields, we apply N L dilated layers with different dilation factors on P 0 . Layer k consists of a dilated convolution with a dilation factor of 2 k , followed by a ReLU activation and a 1 × 1 convolution. This can be described as where * denotes the convolutional operator, W 1 is the dilated convolution weights, W 2 is the weights of the 1 × 1 convolution, and b 1 and b 2 are bias vectors. Finally, we concatenate all P k (k = 1, • • • , N L ) together over the temporal dimension, followed by a 1 × 1 convolution, a residual connection with P 0 and another 1 × 1 convolution to adjust the output dimension. This can be described as</p><formula xml:id="formula_5">P k = W 2 * ReLU(W 1 * (P k-1 + P 0 ) + b 1 ) + b 2 , (<label>4</label></formula><formula xml:id="formula_6">)</formula><formula xml:id="formula_7">S 1 = W 4 * (W 3 * concat(P 1 , . . . , P NL ) + b 3 + P 0 ) + b 4 ,<label>(5)</label></formula><p>where W 3 and W 4 are the weights of the 1×1 convolution, and b 3 and b 4 are bias vectors. Inspired by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref>, we connect N tp TP-SFA modules together and use a weighted cross-entropy loss to train the second stage. This can be described as</p><formula xml:id="formula_8">L temp = - 1 N tp × N s Ntp n=1 Ns s=1 w s g n s log p n s , (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where w s is the weight and is inversely proportional to the surgical phase frequencies <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Phase-Specific AR Guidance in PCS</head><p>The proposed spatiotemporal learning network enables real-time limbus segmentation and surgical phase, facilitating the development of our intraoperative AR guidance system for PCS. The limbus contour can be fitted as an ellipse <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref>.</p><p>To accomplish this, we follow the steps shown in Fig. <ref type="figure" target="#fig_0">1</ref>(c), including: 1) identifying the maximum connected region to remove possible mis-segmented regions;</p><p>2) extracting the contour of the maximum connected region and sampling the contour points; 3) removing contour points near the video boundaries; 4) fitting the remaining contour points to an ellipse and outputting the length and rotation of the long and short axes of the ellipse. We segment PCS into nine phases <ref type="bibr" target="#b19">[19]</ref>: incision, rhexis, hydrodissection, phacoemulsification, epinucleus removal, viscous agent (VA) injection, implant setting-up, VA removal, and stitching up. For intraoperative AR guidance, we designed six visual cues, including: 1) fitted limbus ellipse (FLE), extracted from the segmentation results; 2) primary incision curve (PIC), defined as an arc with a length equal to the maximum diameter of the primary incision knife; 3) secondary incision curve (SIC), defined as an arc with a length equal to the maximum diameter of the secondary incision knife; 4) incision guide lines (IGL), with included angles of 95 circ for BIC and 173 circ for SIC, respectively, relative to the reference line; 5) rhexis region (RR), with a diameter equal to half the length of the long axis of the fitted ellipse; and 6) implant reference line (IRL), defined by a horizontal line. Table <ref type="table" target="#tab_0">1</ref> lists different combinations of intraoperative visual cues that are automatically updated according to the recognized surgical phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>Dataset. We evaluate our methods on CATARACTS <ref type="bibr" target="#b0">[1]</ref>, a publicly available dataset for cataract surgery. It contains 50 videos with a frame rate of 30 framesper-second (fps) and a total duration of over nine hours. All videos have been subsampled to 1 fps. Each frame has a resolution of 1920 × 1080 pixels and has been annotated in nineteen surgical steps. For the sake of intraoperative guidance convenience, we have reorganized the nineteen fine-grained surgical steps into nine standard phases <ref type="bibr" target="#b19">[19]</ref>. Additionally, the limbus region of each frame has been manually delineated by two non-M.D. experts. The dataset is split into 25 cases for training, 5 cases for validation, and 20 cases for test, following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">12]</ref>. Implementation Details. Our network was implemented in PyTorch using two NVIDIA GeForce GTX 3090 GPUs. We initialized the ResNet-50 backbone with weights trained on the ImageNet <ref type="bibr" target="#b21">[21]</ref> and implemented random horizontal flip, random crop, random rotation (±20 circ ), and color jitter for data augmentation. The first stage was trained for 100 epochs using Adam optimizer with a learning rate of 5e-5 for the backbone and 5e-4 for the fully connected layer and decoder. The second stage was trained for 50 epochs using Adam optimizer with a learning rate of 1e-4. For hyper-parameters, we set α = 0.6, β = 0.5 and N L =8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparisons with Strong Baselines</head><p>We compare our method with several strong baselines in surgical phase recognition: 1) ResNet-50 <ref type="bibr" target="#b9">[9]</ref>, a deep residual network that predicts surgical phase in a frame-wise manner; 2) SV-RCNet <ref type="bibr" target="#b11">[11]</ref>, an end-to-end architecture that utilizes LSTM to learn temporal features; 3) TeCNO <ref type="bibr" target="#b2">[3]</ref>, a multi-stage temporal convolutional network that models global temporal features; and 4) Trans-SVNet <ref type="bibr" target="#b7">[8]</ref>, a transformer-based spatiotemporal features aggregation network. Note that we only included comparison methods that support online surgical phase recognition and excluded those that do not. For fair comparison, we use the proposed multi-task learning network in the first stage as the spatial feature extraction backbone for all methods except ResNet-50 <ref type="bibr" target="#b9">[9]</ref>. The quantitative comparison results are listed in Table <ref type="table" target="#tab_1">2</ref>, which indicates that our method achieved the best performance among all compared methods. We show the quantitative results with color-coded ribbons in Fig. <ref type="figure" target="#fig_1">2(a)</ref>. The results indicate that our method can produce a smoother phase prediction compared to ResNet-50 <ref type="bibr" target="#b9">[9]</ref> and SV-RCNet <ref type="bibr" target="#b11">[11]</ref>. Additionally, our approach surpasses the performance of global feature aggregation-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref> in challenging local frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>Effect of Multi-task Feature Extractor We performed experiments to evaluate the effect of the multi-task feature extractor. ResNet-50 <ref type="bibr" target="#b9">[9]</ref> served as the backbone for all methods. We compared the accuracy of the first stage and the second stage for the single phase recognition task with that of the multi-task  approach. Moreover, we compared the segmentation Dice score of the single segmentation task with that of the multi-task approach. Table <ref type="table" target="#tab_2">3</ref> shows the results, indicating that the multi-task feature extractor enhances both segmentation and phase recognition performance compared to the single-task approach.</p><p>Effect of the TP-SFA Module We evaluate the number of the connected TP-SFA modules. The quantitative results are listed in Table <ref type="table" target="#tab_3">4</ref>, which indicates that the second stage achieves the best accuracy when two TP-SFA modules are used. Furthermore, we explore different combinations of the TP-SFA module and a typical causal TCN module <ref type="bibr" target="#b13">[13]</ref> and present the results in Table <ref type="table" target="#tab_4">5</ref>. Results show that two connected TP-SFA modules achieve the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">AR Guidance Evaluation</head><p>Our method achieves real-time intraoperative processing at a speed of 36 fps. This makes it suitable for meeting the demands of online intraoperative AR guidance, as the acquired microscope video stream has a speed of 30 fps. We show some typical failed scenes in Fig. <ref type="figure" target="#fig_1">2</ref>(b). Failed intraoperative AR guidance can result from both mis-recognition of surgical phases and mis-segmentation of the limbus. Mis-recognition of surgical phase may introduce continuous AR scene switching problem and distract the ophthalmologist's attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We proposed a two-stage spatiotemporal network for online microscope video recognition. Furthermore, we developed a phase-specific intraoperative AR guidance system for PCS. Our developed system has the potential for clinical applications to enhance ophthalmologists' intraoperative skills.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our AR-guided system for PCS. (a) Limbus region-focused spatial feature extraction. (b) TP-SFA module-based spatiotemporal aggregation. (c) Phasespecific intraoperative AR guidance.</figDesc><graphic coords="3,41,79,54,23,340,33,153,49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) The comparison results with color-coded ribbons. (b) Some typical failed AR scenes. Markers in red and green represent failed scenes caused by mis-recognition of the surgical phase and mis-segmentation of the limbus, respectively. (Color figure online)</figDesc><graphic coords="8,55,98,54,59,340,18,202,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,101,31,87,23,279,88,205,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>AR scene at different phases with different combinations of visual cues for PCS. The color of visual cues in both the text and AR scene figure is consistent.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison results with strong baselines in online surgical phase recognition. Each metric is reported as mean (%) and standard deviation (±).</figDesc><table><row><cell>Methods</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>Jaccard</cell></row><row><cell>ResNet-50 [9]</cell><cell cols="4">81.1 ± 6.4 78.4 ± 8.0 77.9 ± 7.1 63.7 ± 9.4</cell></row><row><cell cols="5">SV-RCNet [11] 84.8 ± 6.2 81.1 ± 7.3 81.8 ± 6.8 69.1 ± 8.5</cell></row><row><cell>TeCNO [3]</cell><cell cols="4">86.1 ± 5.5 81.6 ± 6.7 83.5 ± 6.3 70.8 ± 7.1</cell></row><row><cell cols="5">Trans-SVNet [8] 86.8 ± 5.9 82.7 ± 7.1 83.6 ± 6.5 71.6 ± 7.0</cell></row><row><cell>Ours</cell><cell cols="4">87.9 ± 5.4 83.7 ± 6.6 84.5 ± 5.8 73.3 ± 6.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Phase recognition and segmentation results of multi-task and single task.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell></cell><cell>Dice</cell></row><row><cell></cell><cell cols="2">First stage Second stage</cell></row><row><cell>Single phase task</cell><cell cols="2">81.1 ± 6.4 85.6 ± 5.9</cell><cell>-</cell></row><row><cell cols="2">Single segmentation task -</cell><cell>-</cell><cell>94.0 ± 2.3</cell></row><row><cell>Phase+Segmentation</cell><cell cols="3">82.6 ± 5.8 87.9 ± 5.4 94.6 ± 2.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The effect of different number of TP-SFA modules.</figDesc><table><row><cell cols="2">NTP Accuracy</cell></row><row><cell>1</cell><cell>85.7 ± 6.9</cell></row><row><cell>2</cell><cell>87.9 ± 5.4</cell></row><row><cell>3</cell><cell>87.6 ± 5.8</cell></row><row><cell>4</cell><cell>87.3 ± 5.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The effect of different combinations of TP-SFA and causal TCN.</figDesc><table><row><cell>First module Second module Accuracy</cell></row><row><cell>TP-SFA TCN TP-SFA TCN</cell></row><row><cell>87.9 ± 5.4</cell></row><row><cell>86.0 ± 5.7</cell></row><row><cell>87.2 ± 5.2</cell></row><row><cell>86.4 ± 6.3</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by grants from the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">81971709</rs>; <rs type="grantNumber">M-0019</rs>; <rs type="grantNumber">82011530141</rs>), the <rs type="funder">Foundation of Science and Technology Commission of Shanghai Municipality</rs> (<rs type="grantNumber">20490740700</rs>; <rs type="grantNumber">22Y11911700</rs>), <rs type="funder">Shanghai Jiao Tong University Foundation on Medical and Technological Joint Science Research</rs> (<rs type="grantNumber">YG2021ZD21</rs>; <rs type="grantNumber">YG2021QN72</rs>; <rs type="grantNumber">YG2022QN056</rs>; <rs type="grantNumber">YG2023ZD19</rs>; <rs type="grantNumber">YG2023ZD15</rs>), <rs type="funder">Hospital Funded Clinical Research, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine</rs> (<rs type="grantNumber">21XJMR02</rs>), and the <rs type="funder">Funding of Xiamen Science and Technology Bureau</rs> (No. <rs type="grantNumber">3502Z20221012</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uU5QdWf">
					<idno type="grant-number">81971709</idno>
				</org>
				<org type="funding" xml:id="_pZU66gG">
					<idno type="grant-number">M-0019</idno>
				</org>
				<org type="funding" xml:id="_hEN4UYd">
					<idno type="grant-number">82011530141</idno>
				</org>
				<org type="funding" xml:id="_tURAmbj">
					<idno type="grant-number">20490740700</idno>
				</org>
				<org type="funding" xml:id="_t7g69jG">
					<idno type="grant-number">22Y11911700</idno>
				</org>
				<org type="funding" xml:id="_jFBkPVe">
					<idno type="grant-number">YG2021ZD21</idno>
				</org>
				<org type="funding" xml:id="_CD8YWJg">
					<idno type="grant-number">YG2021QN72</idno>
				</org>
				<org type="funding" xml:id="_RCxaxuZ">
					<idno type="grant-number">YG2022QN056</idno>
				</org>
				<org type="funding" xml:id="_98bWfmv">
					<idno type="grant-number">YG2023ZD19</idno>
				</org>
				<org type="funding" xml:id="_q2dxAeK">
					<idno type="grant-number">YG2023ZD15</idno>
				</org>
				<org type="funding" xml:id="_sKDXH4t">
					<idno type="grant-number">21XJMR02</idno>
				</org>
				<org type="funding" xml:id="_aFbzcgb">
					<idno type="grant-number">3502Z20221012</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CATARACTS: challenge on automatic tool annotation for cataract surgery</title>
		<author>
			<persName><forename type="first">Al</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="24" to="41" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TeCNO: surgical phase recognition with multi-stage temporal convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0_33" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="343" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">OperA: attention-regularized transformers for surgical phase recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ostler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_58</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-1_58" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="604" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Laser-assisted cataract surgery versus standard ultrasound phacoemulsification cataract surgery</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bunce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cochrane Database of Systematic Reviews</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MS-TCN: multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3575" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Trans-SVNet: accurate phase recognition from surgical videos via hybrid embedding aggregation transformer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="593" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_57</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-1_57" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SV-RCNet: workflow recognition from surgical videos using recurrent convolutional network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1114" to="1126" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Trans-SVNet: hybrid embedding aggregation transformer for surgical workflow analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2193" to="2202" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: a unified approach to action segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-49409-8_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-49409-8_7" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9915</biblScope>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Surgical results of phacoemulsification performed by residents: a time-trend analysis in a teaching hospital from 2005 to 2021</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comprehensive review of surgical microscopes: technology development and medical applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Opt</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10901" to="010901" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluation of artificial intelligence-based intraoperative guidance tools for phacoemulsification cataract surgery</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Nespolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Valikodath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luciano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">I</forename><surname>Leiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="170" to="177" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature tracking and segmentation in real time via deep learning in vitreoretinal surgery-a platform for artificial intelligence-mediated surgical guidance</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Nespolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">I</forename><surname>Leiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmol. Retina</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="236" to="242" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Frame-based classification of operation phases in cataract surgery videos</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Primus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-73603-7_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-73603-7_20" />
	</analytic>
	<monogr>
		<title level="m">MMM 2018</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10704</biblScope>
			<biblScope unit="page" from="241" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time task recognition in cataract surgery videos using adaptive spatiotemporal polynomials</title>
		<author>
			<persName><forename type="first">G</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cochener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cazuguel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="877" to="887" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">EndoNet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cataract surgical rate and socioeconomics: a global study</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Ophthalmol. Vis. Sci</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="5872" to="5881" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Not end-to-end: explore multi-stage architecture for online surgical phase recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2613" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Computer-aided intraoperative toric intraocular lens positioning and alignment during cataract surgery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3921" to="3932" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ECLNet: center localization of eye structures based on adaptive gaussian ellipse heatmap</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page">106485</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ARST: auto-regressive surgical transformer for phase recognition from laparoscopic videos</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Meth. Biomech. Biomed. Eng. Imaging Visual</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1012" to="1018" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
