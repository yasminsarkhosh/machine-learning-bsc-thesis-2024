<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automated CT Lung Cancer Screening Workflow Using 3D Camera</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Brian</forename><surname>Teixeira</surname></persName>
							<email>brian.teixeira@siemens-healthineers.com</email>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology and Innovation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology and Innovation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Birgi</forename><surname>Tamersoy</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Digital Technology and Innovation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Prokein</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Computed Tomography</orgName>
								<orgName type="institution" key="instit2">Siemens Healthineers</orgName>
								<address>
									<settlement>Forchheim</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ankur</forename><surname>Kapoor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Digital Technology and Innovation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automated CT Lung Cancer Screening Workflow Using 3D Camera</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="423" to="431"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">36184FB0302F55E88AAE3FE7E50DD433</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_40</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CT</term>
					<term>Lung Screening</term>
					<term>Dose</term>
					<term>WED</term>
					<term>3D Camera</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite recent developments in CT planning that enabled automation in patient positioning, time-consuming scout scans are still needed to compute dose profile and ensure the patient is properly positioned. In this paper, we present a novel method which eliminates the need for scout scans in CT lung cancer screening by estimating patient scan range, isocenter, and Water Equivalent Diameter (WED) from 3D camera images. We achieve this task by training an implicit generative model on over 60,000 CT scans and introduce a novel approach for updating the prediction using real-time scan data. We demonstrate the effectiveness of our method on a testing set of 110 pairs of depth data and CT scan, resulting in an average error of 5 mm in estimating the isocenter, 13 mm in determining the scan range, 10 mm and 16 mm in estimating the AP and lateral WED respectively. The relative WED error of our method is 4%, which is well within the International Electrotechnical Commission (IEC) acceptance criteria of 10%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lung cancer is the leading cause of cancer death in the United States, and early detection is key to improving survival rates. CT lung cancer screening is a lowdose CT (LDCT) scan of the chest that can detect lung cancer at an early stage, when it is most treatable. However, the current workflow for performing CT lung scans still requires an experienced technician to manually perform pre-scanning steps, which greatly decreases the throughput of this high volume procedure. While recent advances in human body modeling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref> have allowed for automation of patient positioning, scout scans are still required as they are used by automatic exposure control system in the CT scanners to compute the dose to be delivered in order to maintain constant image quality <ref type="bibr" target="#b2">[3]</ref>.</p><p>Since LDCT scans are obtained in a single breath-hold and do not require any contrast medium to be injected, the scout scan consumes a significant portion of the scanning workflow time. It is further increased by the fact that tube rotation has to be adjusted between the scout and actual CT scan. Furthermore, any patient movement during the time between the two scans may cause misalignment and incorrect dose profile, which could ultimately result in a repeat of the entire process. Finally, while minimal, the radiation dose administered to the patient is further increased by a scout scan.</p><p>We introduce a novel method for estimating patient scanning parameters from non-ionizing 3D camera images to eliminate the need for scout scans during pre-scanning. For LDCT lung cancer screening, our framework automatically estimates the patient's lung position (which serves as a reference point to start the scan), the patient's isocenter (which is used to determine the table height for scanning), and an estimate of patient's Water Equivalent Diameter (WED) profiles along the craniocaudal direction which is a well established method for defining Size Specific Dose Estimate (SSDE) in CT imaging <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>. Additionally, we introduce a novel approach for updating the estimated WED in real-time, which allows for refinement of the scan parameters during acquisition, thus increasing accuracy. We present a method for automatically aborting the scan if the predicted WED deviates from real-time acquired data beyond the clinical limit. We trained our models on a large collection of CT scans acquired from over 60, 000 patients from over 15 sites across North America, Europe and Asia. The contributions of this work can be summarized as follows:</p><p>-A novel workflow for automated CT Lung Cancer Screening without the need for scout scan -A clinically relevant method meeting IEC 62985:2019 requirements on WED estimation. -A generative model of patient WED trained on over 60, 000 patients.</p><p>-A novel method for real-time refinement of WED, which can be used for dose modulation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Water Equivalent Diameter (WED) is a robust patient-size descriptor <ref type="bibr" target="#b16">[17]</ref> used for CT dose planning. It represents the diameter of a cylinder of water having the same averaged absorbed dose as the material contained in an axial plane at a given craniocaudal position z <ref type="bibr" target="#b1">[2]</ref>. The WED of a patient is thus a function taking as input a craniocaudal coordinate and outputting the WED of the patient at that given position. As WED is defined in an axial plane, the diameter needs to be known on both the Anterior-Posterior (AP) and lateral (Left-Right) axes noted respectively W ED AP (z) and W ED L (z). As our focus here is on lung cancer screening, we define 'WED profile' to be the 1D curve obtained by uniformly sampling the WED function along the craniocaudal axis within the lung region.</p><p>Our method jointly predicts the AP and lateral WED profiles. While WED can be derived from CT images, paired CT scans and camera images are rarely available, making direct regression through supervised learning challenging. We propose a semi-supervised approach to estimate WED from depth images. First, we train a WED generative model on a large collection of CT scans. We then train an encoder network to map the patient depth image to the WED manifold. Finally, we propose a novel method to refine the prediction using real-time scan data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">WED Latent Space Training</head><p>We use an AutoDecoder <ref type="bibr" target="#b9">[10]</ref> to learn the WED latent space. Our model is a fully connected network with 8 layers of 128 neurons each. We used layer normalization and ReLU activation after each layer except the last one. Our network takes as input a latent vector together with a craniocaudal coordinate z and outputs W ED AP (z) and W ED L (z), the values of the AP and lateral WED at the given coordinate. In this approach, our latent vector represents the encoding of a patient in the latent space. This way, a single AutoDecoder can learn patient-specific continuous WED functions. Since our network only takes the craniocaudal coordinate and the latent vector as input, it can be trained on partial scans of different sizes. The training consists of a joint optimization of the AutoDecoder and the latent vector: the AutoDecoder is learning a realistic representation of the WED function while the latent vector is updated to fit the data.</p><p>During training, we initialize our latent space to a unit Gaussian distribution as we want it to be compact and continuous. We then randomly sample points along the craniocaudal axis and minimize the L1 loss between the prediction and the ground truth WED. We also apply L2-regularization on the latent vector as part of the optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Depth Encoder Training</head><p>After training our generative model on a large collection of unpaired CT scans, we train our encoder network on a smaller collection of paired depth images and CT scans. We represent our encoder as a DenseNet <ref type="bibr" target="#b0">[1]</ref> taking as input the depth image and outputting a latent vector in the previously learned latent space. Our model has 3 dense blocks of 3 convolutional layers. Each convolutional layer (except the last one) is followed by a spectral normalization layer and a ReLU activation. The predicted latent vector is then used as input to the frozen AutoDecoder to generate the predicted WED profiles. We here again apply L2regularization on the latent vector during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Real-Time WED Refinement</head><p>While the depth image provides critical information on the patient anatomy, it may not always be sufficient to accurately predict the WED profiles. For example, some patients may have implants or other medical devices that cannot be guessed solely from the depth image. Additionally, since the encoder is trained on a smaller data collection, it may not be able to perfectly project the depth image to the WED manifold. To meet the strict safety criteria defined by the IEC, we propose to dynamically update the predicted WED profiles at inference time using real-time scan data. First, we use our encoder network to initialize the latent vector to a point in the manifold that is close to the current patient. Then, we use our AutoDecoder to generate initial WED profiles. As the table moves and the patient gets scanned, CT data is being acquired and ground truth WED can be computed for portion of the body that has been scanned, along with the corresponding craniocaudal coordinate. We can then use this data to optimize the latent vector by freezing the AutoDecoder and minimizing the L1 loss between the predicted and ground truth WED profiles through gradient descent. We can then feed the updated latent vector to our AutoDecoder to estimate the WED for the remaining portions of the body that have not yet been scanned and repeat the process.</p><p>In addition to improving the accuracy of the WED profiles prediction, this approach can also help detect deviation from real data. After the latent vector has been optimized to fit the previously scanned data, a large deviation between the optimized prediction and the ground truth profiles may indicate that our approach is not able to find a point in the manifold that is close to the data. In this case, we may abort the scan, which further reduces safety risks. Overall flowchart of the proposed approach is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>Our CT scan dataset consists of 62, 420 patients from 16 different sites across North America, Asia and Europe. Our 3D Camera dataset consists of 2, 742 pairs of depth image and CT scan from 2, 742 patients from 6 different sites across North America and Europe acquired using a ceiling-mounted Kinect 2 camera. Our evaluation set consists of 110 pairs of depth image and CT scan from 110 patients from a separate site in Europe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Patient Preparation</head><p>Patient positioning is the first step in lung cancer screening workflow. We first need to estimate the table position and the starting point of the scan. We propose to estimate the table position by regressing the patient isocenter and the starting point of the scan by estimating the location of the patient's lung top.</p><p>Starting Position. We define the starting position of the scan as the location of the patient's lung top. We trained a DenseUNet <ref type="bibr" target="#b6">[7]</ref> taking the camera depth image as input and outputting a Gaussian heatmap centered at the patient's lung top location. We used 4 dense blocks of 4 convolutional layers for the encoder and 4 dense blocks of 4 convolutional layers for the decoder. Each convolutional layer (except the last one) is followed by a batch normalization layer and a ReLU activation. We trained our model on 2, 742 patients using Adaloss <ref type="bibr" target="#b13">[14]</ref> and the Adam <ref type="bibr" target="#b5">[6]</ref> optimizer with a learning rate of 0.001 and a batch size of 32 for 400 epochs. Our model achieves a mean error of 12.74 mm and a 95 th percentile error of 28.32 mm. To ensure the lung is fully visible in the CT image, we added a 2 cm offset on our prediction towards the outside of the lung. We then defined the accuracy as whether the lung is fully visible in the CT image when using the offset prediction. We report an accuracy of 100% on our evaluation set of 110 patients. Third and fourth columns show the performance of our model with real-time refinement every 5 cm and 2 cm respectively. Ground truth is depicted in green and our prediction is depicted in red. While the original prediction was off towards the center of the lung, the real-time refinement was able to correct the error.</p><p>Isocenter. The patient isocenter is defined as the centerline of the patient's body. We trained a DenseNet <ref type="bibr" target="#b0">[1]</ref> taking the camera depth image as input and outputting the patient isocenter. Our model is made of 4 dense blocks of 3 convolutional layers. Each convolutional layer (except the last one) is followed by a batch normalization layer and a ReLU activation. We trained our model on 2, 742 patients using Adadelta <ref type="bibr" target="#b15">[16]</ref> with a batch size of 64 for 300 epochs. On our evaluation set, our model outperforms the technician's estimates with a mean error of 5.42 mm and a 95 th percentile error of 8.56 mm compared to 6.75 mm and 27.17 mm respectively. Results can be seen in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Water Equivalent Diameter</head><p>We trained our AutoDecoder model on our unpaired CT scan dataset of 62, 420 patients with a latent vector of size 32. The encoder was trained on our paired CT scan and depth image dataset of 2, 742 patients. We first compared our method against a simple direct regression model. We trained a DenseUNet <ref type="bibr" target="#b6">[7]</ref> taking the camera depth image as input and outputting the Water Equivalent Diameter profile. We trained this baseline model on 2, 742 patients using the Adadelta <ref type="bibr" target="#b5">[6]</ref> optimizer with a learning rate of 0.001 and a batch size of 32. We Table <ref type="table">1</ref>. WED profile errors on our testing set (in mm). 'w' corresponds to the portion size of the body that gets scanned before updating the prediction (in cm). Top of the table corresponds to lateral WED profile, bottom corresponds to AP WED profile. Updating the prediction every 20 mm produces the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method (lateral)</head><p>Mean then measured the performance of our model before and after different degrees of real-time refinement, using the same optimizer and learning rate. We report the comparative results in Table <ref type="table">1</ref>.</p><p>We observed that our method largely outperforms the direct regression baseline with a mean lateral error 40% lower and a 90 th percentile lateral error over 30% lower. Bringing in real-time refinement greatly improves the results with a mean lateral error over 40% and a 90 th percentile lateral error over 20% lower than before refinement. AP profiles show similar results with a mean AP error improvement of nearly 40% and a 90 th percentile AP error improvement close to 30%. When using our proposed method with a 20 mm window refinement, our proposed approach outperforms the direct regression baseline by over 60% for lateral profile and nearly 80% for AP.</p><p>Figures 3 highlights the benefits of using real-time refinement. Overall, our approach shows best results with an update frequency of 20 mm, with a mean lateral error of 15.93 mm and a mean AP error of 10.40 mm. Figure <ref type="figure" target="#fig_3">4</ref> presents a qualitative evaluation on patients with different body morphology.</p><p>Finally, we evaluated the clinical relevancy of our approach by computing the relative error as described in the International Electrotechnical Commission (IEC) standard IEC 62985:2019 on Methods for calculating size specific dose estimates (SSDE) for computed tomography <ref type="bibr" target="#b1">[2]</ref>. The Δ REL metric is defined as:</p><formula xml:id="formula_0">Δ REL (z) = Ŵ ED(z) -W ED(z) W ED(z)<label>(1)</label></formula><p>where:</p><p>-Ŵ ED(z) is the predicted water equivalent diameter -W ED(z) is the ground truth water equivalent diameter z is the position along the craniocaudal axis of the patient. IEC standard states the median value of the set of Δ REL (z) along the craniocaudal axis (noted Δ REL ) should be below 0.1. Our method achieved a mean lateral Δ REL error of 0.0426 and a mean AP Δ REL error of 0.0428, falling well within the acceptance criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We presented a novel 3D camera based approach for automating CT lung cancer screening workflow without the need for a scout scan. Our approach effectively estimates start of scan, isocenter and Water Equivalent Diameter from depth images and meets the IEC acceptance criteria of relative WED error. While this approach can be used for other thorax scan protocols, it may not be applicable to trauma (e.g. with large lung resections) and inpatient settings, as the deviation in predicted and actual WED would likely be much higher. In future, we plan to establish the feasibility as well as the utility of this approach for other scan protocols and body regions.<ref type="foot" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clinical Applications -Musculoskeletal</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed workflow.</figDesc><graphic coords="3,55,98,53,78,340,48,194,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Isocenter results on our evaluation set. Left column presents a qualitative result from our evaluation set. The red line corresponds to our model prediction and the green line is the ground truth computed from the CT. The right column presents a histogram of the errors in mm. (Color figure online)</figDesc><graphic coords="5,56,46,54,26,339,40,112,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. AP (top) and lateral (bottom) WED profile regression with and without realtime refinement. w corresponds to the portion size of the body that gets scanned before updating the prediction (in cm). First column shows a lateral projection view of the CT. Second column shows the performance of our model without real-time refinement. Third and fourth columns show the performance of our model with real-time refinement every 5 cm and 2 cm respectively. Ground truth is depicted in green and our prediction is depicted in red. While the original prediction was off towards the center of the lung, the real-time refinement was able to correct the error.</figDesc><graphic coords="6,42,30,53,93,339,28,169,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Qualitative analysis of the proposed method with 2 cm refinement on patient with different morphology. From left to right: Lateral CT projection, Lateral WED profile, AP WED profile, AP CT projection.</figDesc><graphic coords="8,46,29,53,84,331,36,228,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>error 90th perc error Max error</figDesc><table><row><cell>Direct Regression</cell><cell>45.07</cell><cell>76.70</cell><cell>101.50</cell></row><row><cell>Proposed (initial)</cell><cell>27.06</cell><cell>52.88</cell><cell>79.27</cell></row><row><cell cols="2">Proposed (refined, w = 5) 19.18</cell><cell>42.44</cell><cell>73.69</cell></row><row><cell cols="2">Proposed (refined, w = 2) 15.93</cell><cell>35.93</cell><cell>61.68</cell></row><row><cell>Method (AP)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Direct Regression</cell><cell>45.71</cell><cell>71.85</cell><cell>82.84</cell></row><row><cell>Proposed (initial)</cell><cell>16.52</cell><cell>31.00</cell><cell>40.89</cell></row><row><cell cols="2">Proposed (refined, w = 5) 12.19</cell><cell>25.73</cell><cell>37.36</cell></row><row><cell cols="2">Proposed (refined, w = 2) 10.40</cell><cell>22.44</cell><cell>33.85</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Disclaimer: The concepts and information presented in this paper are based on research results that are not commercially available. Future commercial availability cannot be guaranteed.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">International.Electrotechnical.Commission: IEC</title>
		<imprint>
			<biblScope unit="volume">62985</biblScope>
			<biblScope unit="page">2019</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Techniques and applications of automatic tube current modulation for CT</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">233</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="649" to="657" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards contactless patient positioning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans, Medical Imaging</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pujades</surname></persName>
		</author>
		<title level="m">Obtaining skeletal shape from outside</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">H-denseunet: hybrid densely connected UNet for liver and liver tumor segmentation from CT volumes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Use of water equivalent diameter for calculating patient size and size-specific dose estimates (SSDE</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>American Association of Physicists in Medicine</publisher>
			<pubPlace>CT</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple manual method to estimate waterequivalent diameter for calculating size-specific dose estimate in chest computed tomography</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mihailidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tsapaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tomara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British J. Radiol</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">20200473</biblScope>
			<date type="published" when="1117">1117. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deepsdf: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Size specific dose estimate (SSDE) for estimating patient dose from CT used in myocardial perfusion spect/ct</title>
		<author>
			<persName><forename type="first">V</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ponnusamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Halanaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Asia Oceania J. Nuclear Med. Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Darwin: Deformable patient avatar representation with deep image network</title>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating synthetic x-ray images of a person from the surface geometry</title>
		<author>
			<persName><forename type="first">B</forename><surname>Teixeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adaloss: Adaptive loss function for landmark localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tamersoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Towards generating personalized volumetric phantom from patient&apos;s surface geometry</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimating patient water equivalent diameter from CT localizer images -a longitudinal and multi-institutional study of the stability of calibration parameters</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Bankier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2139" to="2149" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new method for CT dose estimation by determining patient water equivalent diameter from localizer radiographs: geometric transformation and calibration methods using readily available phantoms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mihai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Barbaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Brook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3371" to="3378" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
