<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation</title>
				<funder ref="#_qUCvHaW">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_5Ju9MdX">
					<orgName type="full">Key R&amp;D Program of Zhejiang</orgName>
				</funder>
				<funder ref="#_up2Y5cn #_XuTktma">
					<orgName type="full">National Natural Sciences Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianyi</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Colorectal Surgery and Oncology (Key Laboratory of Cancer Prevention and Intervention</orgName>
								<orgName type="department" key="dep2">National Ministry of Education</orgName>
								<orgName type="department" key="dep3">Key Laboratory of Molecular Biology in Medical Sciences</orgName>
								<orgName type="department" key="dep4">The Second Affiliated Hospital</orgName>
								<orgName type="institution">Zhejiang University School of Medicine</orgName>
								<address>
									<addrLine>Zhejiang Province</addrLine>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country>China, China), China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Bioinformatics</orgName>
								<orgName type="department" key="dep2">College of Life Sciences</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengyi</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Bioinformatics</orgName>
								<orgName type="department" key="dep2">College of Life Sciences</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Hepatobiliary and Pancreatic Surgery</orgName>
								<orgName type="institution">The First Affiliated Hospital</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Medicine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Bioinformatics</orgName>
								<orgName type="department" key="dep2">College of Life Sciences</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Thoracic Surgery</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution" key="instit1">The Second Affiliated Hospital</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tian</forename><surname>Cai</surname></persName>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Department of Hepatobiliary and Pancreatic Surgery</orgName>
								<orgName type="department" key="dep2">The Second Affiliated Hospital</orgName>
								<orgName type="institution">Zhejiang University School of Medicine</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Da</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Colorectal Surgery and Oncology (Key Laboratory of Cancer Prevention and Intervention</orgName>
								<orgName type="department" key="dep2">National Ministry of Education</orgName>
								<orgName type="department" key="dep3">Key Laboratory of Molecular Biology in Medical Sciences</orgName>
								<orgName type="department" key="dep4">The Second Affiliated Hospital</orgName>
								<orgName type="institution">Zhejiang University School of Medicine</orgName>
								<address>
									<addrLine>Zhejiang Province</addrLine>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country>China, China), China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yincong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Bioinformatics</orgName>
								<orgName type="department" key="dep2">College of Life Sciences</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
							<email>mchen@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Bioinformatics</orgName>
								<orgName type="department" key="dep2">College of Life Sciences</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kefeng</forename><surname>Ding</surname></persName>
							<email>dingkefeng@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Colorectal Surgery and Oncology (Key Laboratory of Cancer Prevention and Intervention</orgName>
								<orgName type="department" key="dep2">National Ministry of Education</orgName>
								<orgName type="department" key="dep3">Key Laboratory of Molecular Biology in Medical Sciences</orgName>
								<orgName type="department" key="dep4">The Second Affiliated Hospital</orgName>
								<orgName type="institution">Zhejiang University School of Medicine</orgName>
								<address>
									<addrLine>Zhejiang Province</addrLine>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country>China, China), China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">https://github.com/Seasonsling/PETNet</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="572" to="581"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">D197376F8B9031579D6DA4AEB5500F86</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_54</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Colonoscopy</term>
					<term>Polyp Segmentation</term>
					<term>Vision Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Colorectal polyps detected during colonoscopy are strongly associated with colorectal cancer, making polyp segmentation a critical clinical decision-making tool for diagnosis and treatment planning. However, accurate polyp segmentation remains a challenging task, particularly in cases involving diminutive polyps and other intestinal substances that produce a high false-positive rate. Previous polyp segmentation networks based on supervised binary masks may have lacked global semantic perception of polyps, resulting in a loss of capture and discrimination capability for polyps in complex scenarios. To address this issue, we propose a novel Gaussian-Probabilistic guided semantic fusion method that progressively fuses the probability information of polyp positions with the decoder supervised by binary masks. Our Probabilistic Modeling Ensemble Vision Transformer Network(PETNet) effectively suppresses noise in features and significantly improves expressive capabilities at both pixel and instance levels, using just simple types of convolutional decoders. Extensive experiments on five widely adopted datasets show that PETNet outperforms existing methods in identifying polyp T. Ling and C. Wu-Equal contributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Colorectal cancer (CRC) remains a major health burden with elevated mortality worldwide <ref type="bibr" target="#b0">[1]</ref>. Most cases of CRC arise from adenomatous polyps or sessile serrated lesions in 5 to 10 years <ref type="bibr" target="#b8">[9]</ref>. Colonoscopy is considered the gold standard for the detection of colorectal polyps. Polyp segmentation is a fundamental task in the computer-aided detection (CADe) of polyps during colonoscopy, which is of great significance in the clinical prevention of CRC.</p><p>Traditional machine learning approaches in polyp segmentation primarily focus on learning low-level features, such as texture, shape, or color distribution <ref type="bibr" target="#b14">[14]</ref>. In recent years, encoder-decoder based deep learning models such as U-Net <ref type="bibr" target="#b12">[12]</ref>, UNet++ <ref type="bibr" target="#b22">[22]</ref>, ResUNet++ <ref type="bibr" target="#b5">[6]</ref>, and PraNet <ref type="bibr" target="#b3">[4]</ref> have dominated the field. Furthermore, Transformer <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref> models have also been proposed for polyp segmentation, and achieve the state-of-the-art(SOTA) performance.</p><p>Despite significant progress made by these binary mask supervised models, challenges remain in accurately locating polyps, particularly in complex clinical scenarios, due to their insensitivity to complex lesions and high false-positive rates. More specifically, most polyps have an elliptical shape with well-defined boundaries. However, supervised segmentation learning solely based on binary masks may not be effective in discriminating polyps in complex clinical scenarios. Endoscopic images often contain pseudo-polyp objects with strong boundaries, such as colon folds, blood vessels, and air bubbles, which can result in false positives. In addition, sessile and flat polyps have ambiguous and challenging boundaries to delineate. To address these limitations, Qadir et al. <ref type="bibr" target="#b11">[11]</ref> proposed using Gaussian masks for supervised model training. This approach reduces false positives significantly by assigning less attention to outer edges and prioritizing surface patterns. However, this method has limitations in accurately segmenting polyp boundaries, which are crucial for clinical decision-making.</p><p>Therefore, the primary challenge lies in enhancing polyp segmentation performance in complex scenarios by precisely preserving the polyp segmentation boundaries, while simultaneously maximizing the decoder's attention on the overall pattern of the polyps.</p><p>In this paper, we propose a novel transformer-based polyp segmentation framework, PETNet, which addresses the aforementioned challenges and achieves SOTA performance in locating polyps with high precision. Our contributions are threefold:</p><p>• We propose a novel Gaussian-Probabilistic guided semantic fusion method for polyp segmentation, which improves the decoder's global perception of polyp locations and discrimination capability for polyps in complex scenarios.</p><p>• We evaluate the performance of PETNet on five widely adopted datasets, demonstrating its superior ability to identify polyp camouflage and small polyp scenes, achieving state-of-the-art performance in locating polyps with high precision. Furthermore, we show that PETNet can achieve a speed of about 27FPS in edge computing devices (Nvidia Jetson Orin). • We design several polyp instance-level evaluation metrics, considering that conventional pixel-level calculation methods cannot explicitly and comprehensively evaluate the overall performance of polyp segmentation algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture Overview</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, PETNet is an end-to-end polyp segmentation framework consists of three core module groups. (1) The Encoder Group employs a vision transformer backbone <ref type="bibr" target="#b18">[18]</ref> cascaded with a mixed transformer attention layer to encode long-range dependent features at four scales. (2) The Gaussian-Probabilistic Modeling Group consists of a Gaussian Probabilistic Guided UNet-like decoder branch(GUDB) and Gaussian Probabilistic-Induced transition(GIT) modules. (3) The Ensemble Binary Decoders Group includes a UNet-like structure branch(UDB) <ref type="bibr" target="#b12">[12]</ref>, a fusion module(Fus), and a cascaded fusion module(CFM) <ref type="bibr" target="#b2">[3]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoder Group</head><p>To balance the trade-off between computational speed and feature representation capability, we utilize the pre-trained PVTv2-B2 model <ref type="bibr" target="#b18">[18]</ref> as the backbone.</p><p>Mixed transformer attention(MTA) layer is composed of Local-Global Gaussian-Weighted Self-Attention (LGG-SA) and External Attention (EA). We add a MTA layer to encode the last level features, enhancing the model's semantic representation and accelerating the training process <ref type="bibr" target="#b16">[16]</ref>. Moreover, the encoder output features are presented as {X E i } 4 i=1 with channels of [2C, 4C, 8C, 16C].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Gaussian-Probabilistic Modeling Group</head><p>To incorporate both polyp location probability and surface pattern information in a progressive manner, we propose the Gaussian Probabilistic-induced Transition (GIT) method. This method involves the interaction between a Gaussian auxiliary decoder and multiple binary decoders in a layer-wise fashion, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Gaussian Probabilistic Mask. Inspired by <ref type="bibr" target="#b11">[11]</ref> and <ref type="bibr" target="#b21">[21]</ref>, in addition to utilizing binary representation, polyps can also be represented as probability heatmaps with blurred borders. We present a method of converting the binary polyp mask</p><formula xml:id="formula_0">f (x, y) ∈ {0, 1} W ×H×1 into Gaussian masks Y (x, y) ∈ [0, 1]</formula><p>W ×H×1 by utilizing elliptical Gaussian kernels. Specifically, for every polyp in a binary mask, after masking other polyp pixels as background, we calculate</p><formula xml:id="formula_1">Y = exp -a(x -x o ) 2 + 2b (x -x o ) (y -y o ) + (y -y o ) 2<label>(1)</label></formula><p>where (x o , y o ) is the mass of each polyp in the binary image f (x, y). To rotate the output 2D Gaussian masks according to the orientation, we set a, b, c as followings,</p><formula xml:id="formula_2">a = cos 2 (θ) 2σ 2 x + sin 2 (θ) 2σ 2 y , (2) b = -sin (2θ) 4σ 2 x + sin (2θ) 4σ 2 y , (<label>3</label></formula><formula xml:id="formula_3">) c = cos 2 (θ) 2σ 2 x + sin 2 (θ) 2σ 2 y , (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where σ 2 x and σ 2 y are the polyp size-adaptive standard deviations <ref type="bibr" target="#b21">[21]</ref>, and θ is the orientation of each polyp <ref type="bibr" target="#b11">[11]</ref>. Finally, we determine the final Gaussian probabilistic mask P G for all polyps within an image mask by computing the element-wise maximum.</p><p>Gaussian Guided UNet-Like Decoder Branch. The Gaussian Guided UNet-like decoder branch(GUDB) module is a simple UNet-like decoding branch supervised by Gaussian Probabilistic masks. We employ four levels of encoder output features, and adjust encoder features</p><formula xml:id="formula_5">{X E i } 4 i=1 to the features {X G i } 4 i=1</formula><p>with channels of [C, 2C, 2C, 2C] in each level. At the final layer, a 1 × 1 convolution is used to convert the feature vector to one channel, producing a size of H × W × 1 Gaussian mask.</p><p>Gaussian Probabilistic-Induced Transition Module. We use the Gaussian probabilistic-induced transition module(GIT) to achieve transition between binary features and gaussian features. Given the features originally sent to the Decoder as binary features {X B i } 4 i=1 , and the transformed encoder features sent to GUDB as X G . We first splits 4 levels of X B and X G into fixed groups as:</p><formula xml:id="formula_6">{X D i,m } M m=1 ∈ R C d /M ×Hi×Wi ← X D i ∈ R C d ×Hi×Wi , (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where M is the corresponding number of groups. Then, we periodically arrange groups of X B i,m and X G i,m for each level, and generate the regrouped feature Q i ∈ R (Ci+Cg)×Hi×Wi in an Multi-layer sandwiches manner. Soft grouping convolution <ref type="bibr" target="#b6">[7]</ref> is then applied to provide parallel nonlinear projections at multiple fine-grained sub-spaces (Fig. <ref type="figure" target="#fig_1">2</ref>). We further introduce residual learning in a parallel manner at different group-aware scales. The final output</p><formula xml:id="formula_8">{Z T i } 4 i=1 ∈ R</formula><p>Ci×Hi×Wi is obtained for the UDB decoder. Considering the computation cost, The binary features X B ← X E for the Fus decoder have channel numbers of [4C, 4C, 4C, 4C]. The Fus decoder and CFM share identical transited output features, while CFM exclusively utilizes the last three levels of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Ensemble Binary Decoders Group</head><p>During colonoscopy, endoscopists often use the two-physician observation approach to improve the detection rate of polyps. Building on this manner, we propose the ensemble method that integrates multiple simple decoders to enhance the detection and discrimination of difficult polyp samples. We demonstrate the effectiveness of our approach using three commonly used convolutional decoders. After GIT process, diverse level of Gaussian probabilistic-induced binary features were sent to these decoders. The output mask P is obtained by element-wise summation of P i , where i represents the binary decoder index.</p><p>Fusion Module. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, Set X i, i∈(1, 2, 3, 4) represent multiscale mixed features. Twice convolution following with bilinear interpolation are applied to transform these feature with same 4C channels as X 1 , X 2 , X 3 , X 4 . Afterward, we get X out with the resolution of H/4 × W/4 × C1 through following formula, where F represents twice 3×3 convolution:</p><formula xml:id="formula_9">X out = F(Concat(X 1 , X 2 , X 3 , X 4 )<label>(6)</label></formula><p>UNet Decoder Branch and CFM Module. The structure of the UDB is similar to that of the GUDB, except for the absence of channel reduction prior to decoding. In our evaluation, we also examine the decoder CFM utilized in <ref type="bibr" target="#b2">[3]</ref>, which shares the same input features (excluding the first level) as the Fus.</p><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets Settings</head><p>To evaluate models fairly, we completely follow P raNet <ref type="bibr" target="#b3">[4]</ref> and use five public datasets, including 548 and 900 images from ClinicDB <ref type="bibr" target="#b1">[2]</ref> and Kvasir-SEG <ref type="bibr" target="#b4">[5]</ref> as training sets, and the remaining images as validation sets. We also test the generalization capability of all models on three unseen datasets (ETIS <ref type="bibr" target="#b13">[13]</ref> with 196 images, CVC-ColonDB <ref type="bibr" target="#b7">[8]</ref> with 380 images, and EndoScene <ref type="bibr" target="#b15">[15]</ref> with 60 images). Training settings are the same as <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss Setting</head><p>Our loss function formulates as L = N i=1 L i + λL g , and</p><formula xml:id="formula_10">L i = n i=1 (L IOU (P i , G B ) + L BCE (P i , G B )) (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where N is the total number of binary decoders, L g represents the L1 loss between the ground truth Gaussian mask G G and GUDB prediction mask P G . λ is a hyperparameter used to balance the binary and Gaussian losses. Furthermore, we employ intermediate decoder outputs to calculate auxiliary losses for convergence acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Metrics</head><p>Conventional evaluation metrics for polyp segmentation are typically limited to pixel-level calculations. However, metrics that consider the entire polyp are also crucial. Here we assess our model from both pixel-level and instance-level perspectives.</p><p>Pixel-level evaluation is based on mean intersection over union (mIoU ), mean Dice coefficient (mDic), and weighted F 1 score (wF m ). For polyp instance evaluation, a true positive (TP) is defined when the detection centroid is located within the polyp mask. False positives (FPs) occur when a wrong detection output is provided for a negative region, and false negatives (FNs) occur when a polyp is missed in a positive image. Finally, we compute sensitivity nSen = T P/(T P + F N) × 100, precision nPre = T P/(T P + F P ) × 100, and nF 1 = 2 × (Sen × Pre)/(Sen + Pre) × 100 based on the number count for instance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>Training and Learning Ability. Table <ref type="table" target="#tab_0">S1</ref> displays the results of our model's training and learning performance. Our model achieves comparable performance to the SOTA model on the Kvasir-SEG and ClinicDB datasets. Notably, our model yields superior results in false-positive instance evaluation.</p><p>Generalization Ability. The generalization results are shown in Table <ref type="table" target="#tab_0">1</ref>. We conduct three unseen datasets to test models' generalizability. Results show that P ET Net achieves excellent generalization performance compared with previous models. Most importantly, our false-positive instance counts(45 in ETIS and 55 in CVC-ColonDB) reduce significantly of other models. We also observe a performance mismatch phenomenon in pixel-level evaluation and instance-level evaluation.</p><p>Small Polyp Detection Ability. The detection capability results of small polyps are shown in Table <ref type="table" target="#tab_1">2</ref>. Diminutive polyps are hard to precisely detect, while they are the major targets of optical biopsies performed by endoscopists. We selected images from two unseen datasets with 0∼2% polyp labeled area to perform the test. As shown, P ET Net demonstrates great strength in both datasets, which indicates that one of the major advantages of our model lies in detecting small polyps with lower false-positive rates.</p><p>Ablation Analysis. Table <ref type="table" target="#tab_2">3</ref> presents the results of our ablation study, where we investigate the contribution of the two key components of our model, namely the Gaussian-Probabilistic Guided Semantic Fusion method and ensemble decoders. We observe that while the impact of each binary decoder varies, all sub binary decoders contribute to the overall performance. Furthermore, the GIT method significantly enhances instance-level evaluation without incurring performance penalty in pixel-level evaluation, especially in unseen datasets.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparative Analysis</head><p>Fig. <ref type="figure" target="#fig_0">S1</ref> shows that our proposed model, P ET Net, outperforms SOTA models in accurately identifying polyps under complex scenarios, including lighting disturbances, water reflections, and motion blur. Faluire cases are shown in Fig. <ref type="figure" target="#fig_1">S2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Running in the Real World</head><p>Furthermore, we deployed P ET Net on the edge computing device Nvidia Jetson Orin and optimized its performance using TensorRT. Our results demonstrate that P ET Net achieves real-time denoising and segmentation of polyps with high accuracy, achieving a speed of 27 frames per second on the device(Video S1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Based on intrinsic characteristics of the endoscopic polyp image, we specifically propose a novel segmentation framework named PETNet consisting of three key module groups. Experiments show that P ET Net consistently outperforms most current cutting-edge models on five challenging datasets, demonstrating its solid robustness in distinguishing other intestinal analogs. Most importantly, P ET Net shows better sensitivity to complex lesions and diminutive polyps.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Proposed PETNet Framework: (a) Comprises three critical module groups. (b) Depicts the Stage Encoder. (c) Illustrates the Mixed Transformer Attention layer (MTA).</figDesc><graphic coords="3,57,30,352,37,322,12,193,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of Gaussian Probabilistic-induced transition (GIT).</figDesc><graphic coords="4,65,25,54,56,277,12,144,16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results of the test datasets EndoScene, CVC-ColonDB and ETIS-LaribPolypDB.</figDesc><table><row><cell></cell><cell>Methods</cell><cell>mDic mIoU nSen nP re nF1</cell><cell>T P F P F N</cell></row><row><cell cols="2">EndoScene PraNet (MICCAI'20) [4]</cell><cell cols="2">0.748 0.634 1.000 0.526 0.690 60 54 0</cell></row><row><cell></cell><cell>EU-Net (CRV'21) [10]</cell><cell cols="2">0.882 0.809 1.000 0.938 0.968 60 4</cell><cell>0</cell></row><row><cell></cell><cell>LDNet (MICCAI'22) [19]</cell><cell cols="2">0.888 0.814 1.000 0.984 0.992 60 1</cell><cell>0</cell></row><row><cell></cell><cell cols="3">SSFormer-S (MICCAI'22) [17] 0.881 0.812 1.000 0.938 0.968 60 4</cell><cell>0</cell></row><row><cell></cell><cell cols="3">Polyp-PVT(CAAI AIR'23) [3] 0.894 0.829 1.000 0.968 0.984 60 2</cell><cell>0</cell></row><row><cell></cell><cell>PETNet (Ours)</cell><cell cols="2">0.899 0.834 1.000 1.000 1.000 60 0</cell><cell>0</cell></row><row><cell cols="2">ColonDB PraNet (MICCAI'20) [4]</cell><cell cols="2">0.643 0.542 0.882 0.499 0.638 351 352 47</cell></row><row><cell></cell><cell>EU-Net (CRV'21) [10]</cell><cell cols="2">0.757 0.672 0.889 0.760 0.819 354 112 44</cell></row><row><cell></cell><cell>LDNet (MICCAI'22) [19]</cell><cell cols="2">0.751 0.667 0.905 0.726 0.805 360 136 38</cell></row><row><cell></cell><cell cols="3">SSFormer-S (MICCAI'22) [17] 0.787 0.708 0.900 0.840 0.869 358 68 40</cell></row><row><cell></cell><cell cols="3">Polyp-PVT(CAAI AIR'23) [3] 0.808 0.724 0.937 0.772 0.847 373 110 25</cell></row><row><cell></cell><cell>PETNet (Ours)</cell><cell cols="2">0.817 0.740 0.935 0.871 0.902 372 55 26</cell></row><row><cell>ETIS</cell><cell>PraNet (MICCAI'20) [4]</cell><cell cols="2">0.584 0.482 0.904 0.390 0.545 188 294 20</cell></row><row><cell></cell><cell>EU-Net (CRV'21) [10]</cell><cell cols="2">0.687 0.610 0.870 0.557 0.679 181 144 27</cell></row><row><cell></cell><cell>LDNet (MICCAI'22) [19]</cell><cell cols="2">0.702 0.611 0.909 0.659 0.764 189 98 19</cell></row><row><cell></cell><cell cols="3">SSFormer-S (MICCAI'22) [17] 0.744 0.672 0.856 0.674 0.754 178 86 30</cell></row><row><cell></cell><cell cols="3">Polyp-PVT(CAAI AIR'23) [3] 0.765 0.687 0.923 0.667 0.774 192 96 16</cell></row><row><cell></cell><cell>PETNet (Ours)</cell><cell cols="2">0.782 0.703 0.904 0.807 0.853 188 45 20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results of the small polyp detection in ETIS and CVC-ColonDB dataset. Small polyps are defined as the polyp area accounts for 0∼2% of the entire image.</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="6">mDic mIoU nSen nP re nF1 T P F P F N</cell></row><row><cell>ETIS</cell><cell>PraNet [4]</cell><cell>0.43</cell><cell>0.34</cell><cell>0.87</cell><cell>0.34</cell><cell cols="2">0.49 87 167 13</cell></row><row><cell></cell><cell>Polyp-PVT [3]</cell><cell>0.68</cell><cell>0.60</cell><cell cols="2">0.93 0.61</cell><cell>0.74 93 60</cell><cell>7</cell></row><row><cell></cell><cell cols="2">PETNet (Ours) 0.69</cell><cell>0.60</cell><cell cols="4">0.88 0.73 0.80 88 33 12</cell></row><row><cell cols="2">ColonDB PraNet [4]</cell><cell>0.45</cell><cell>0.34</cell><cell>0.82</cell><cell>0.40</cell><cell cols="2">0.54 80 120 17</cell></row><row><cell></cell><cell>Polyp-PVT [3]</cell><cell>0.68</cell><cell>0.58</cell><cell cols="2">0.93 0.71</cell><cell>0.80 90 37</cell><cell>7</cell></row><row><cell></cell><cell cols="2">PETNet (Ours) 0.67</cell><cell>0.58</cell><cell cols="4">0.93 0.76 0.84 90 28 7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study for PETNet on five datasets. wFm: pixel-based weighted F1 score, nF1: instance-based weighted F1 score. w/o: without.</figDesc><table><row><cell>Dataset</cell><cell cols="7">Metric Baseline w/o Fus w/o UDB w/o CFM w/o GUDB PETNet</cell></row><row><cell>ClinicDB</cell><cell>wFm</cell><cell>0.819</cell><cell>0.929</cell><cell>0.929</cell><cell>0.941</cell><cell>0.933</cell><cell>0.932</cell></row><row><cell></cell><cell>nF1</cell><cell>0.883</cell><cell>0.978</cell><cell>0.964</cell><cell>0.971</cell><cell>0.971</cell><cell>0.964</cell></row><row><cell>Kvasir</cell><cell>wFm</cell><cell>0.804</cell><cell>0.904</cell><cell>0.914</cell><cell>0.899</cell><cell>0.911</cell><cell>0.912</cell></row><row><cell></cell><cell>nF1</cell><cell>0.853</cell><cell>0.882</cell><cell>0.879</cell><cell>0.895</cell><cell>0.881</cell><cell>0.891</cell></row><row><cell cols="2">EndoScene wFm</cell><cell>0.705</cell><cell>0.859</cell><cell>0.877</cell><cell>0.876</cell><cell>0.874</cell><cell>0.884</cell></row><row><cell></cell><cell>nF1</cell><cell>0.851</cell><cell>0.992</cell><cell>0.952</cell><cell>0.976</cell><cell>0.976</cell><cell>1.000</cell></row><row><cell>ColonDB</cell><cell>wFm</cell><cell>0.635</cell><cell>0.800</cell><cell>0.783</cell><cell>0.775</cell><cell>0.812</cell><cell>0.802</cell></row><row><cell></cell><cell>nF1</cell><cell>0.761</cell><cell>0.898</cell><cell>0.894</cell><cell>0.881</cell><cell>0.882</cell><cell>0.902</cell></row><row><cell>ETIS</cell><cell>wFm</cell><cell>0.467</cell><cell>0.738</cell><cell>0.744</cell><cell>0.725</cell><cell>0.736</cell><cell>0.747</cell></row><row><cell></cell><cell>nF1</cell><cell>0.577</cell><cell>0.840</cell><cell>0.810</cell><cell>0.805</cell><cell>0.850</cell><cell>0.853</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">National Natural Sciences Foundation of China</rs> (Nos. <rs type="grantNumber">31771477</rs>, <rs type="grantNumber">32070677</rs>), the <rs type="funder">Fundamental Research Funds for the Central Universities</rs> (No. <rs type="grantNumber">226-2022-00009</rs>), and the <rs type="funder">Key R&amp;D Program of Zhejiang</rs> (No. <rs type="grantNumber">2023C03049</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_up2Y5cn">
					<idno type="grant-number">31771477</idno>
				</org>
				<org type="funding" xml:id="_XuTktma">
					<idno type="grant-number">32070677</idno>
				</org>
				<org type="funding" xml:id="_qUCvHaW">
					<idno type="grant-number">226-2022-00009</idno>
				</org>
				<org type="funding" xml:id="_5Ju9MdX">
					<idno type="grant-number">2023C03049</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_54.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Generative adversarial networks for automatic polyp segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M A A</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06771</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fernández-Esparrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilariño</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compmedimag.2015.02.007</idno>
		<idno>02.007</idno>
		<ptr target="https://doi.org/10.1016/j.compmedimag.2015" />
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph.: Official J. Comput. Med. Imaging Soc</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06932[cs](2021</idno>
		<title level="m">Polyp-PVT: polyp segmentation with pyramid vision transformers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">PraNet: parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11392</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07069</idno>
		<title level="m">Kvasir-SEG: a segmented polyp dataset</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ResUNet++: an advanced architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07067</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep gradient learning for efficient camouflaged object detection</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liniger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12853</idno>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automated polyp detection in colon capsule endoscopy</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Mamonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H R</forename><surname>Tsai</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2014.2314959</idno>
		<ptr target="http://arxiv.org/abs/1305.1912" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1488" to="1502" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">National Health Commission of the People&apos;s Republic of China: [Chinese Protocol of Diagnosis and Treatment of Colorectal Cancer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>edition)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<idno type="DOI">10.3760/cma.j.cn112139-20200518-00390</idno>
		<idno>112139-20200518-00390</idno>
		<ptr target="https://doi.org/10.3760/cma.j.cn" />
	</analytic>
	<monogr>
		<title level="j">Zhonghua Wai Ke Za Zhi [Chinese Journal of Surgery</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="561" to="585" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhanced U-Net: a feature enhancement network for polyp segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/crv52889.2021.00032</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8341462/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Robots &amp; Vision Conference. International Robots &amp; Vision Conference 2021</title>
		<meeting>the International Robots &amp; Vision Conference. International Robots &amp; Vision Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward real-time polyp detection using fully CNNs for 2D Gaussian shapes prediction</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Solhusvik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergsland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aabakken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Balasingham</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2020.101897</idno>
		<ptr target="https://linkinghub.elsevier.com/retrieve/pii/S1361841520302619" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">101897</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04597</idno>
		<ptr target="http://arxiv.org/abs/1505.04597" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in WCE images for early diagnosis of colorectal cancer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Granado</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-013-0926-3</idno>
		<ptr target="https://doi.org/10.1007/s11548-013-0926-3" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comprehensive computer-aided polyp detection system for colonoscopy videos</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-19992-4_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-19992-4_25" />
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="327" to="338" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A benchmark for endoluminal scene segmentation of colonoscopy images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<idno type="DOI">10.1155/2017/4037190</idno>
		<ptr target="https://doi.org/10.1155/2017/4037190" />
	</analytic>
	<monogr>
		<title level="j">J. Healthc. Eng</title>
		<imprint>
			<biblScope unit="page">4037190</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mixed transformer U-Net for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.04734</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stepwise feature fusion: local guides global</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_11" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="110" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">PVTv2: improved baselines with pyramid vision transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lesion-aware dynamic kernel for polyp segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_10" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="99" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">TransFuse: fusing transformers and CNNs for medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08005[cs](2021</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1904.07850</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1904.07850" />
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">UNet++: a nested U-Net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10165</idno>
	</analytic>
	<monogr>
		<title level="s">Clinical Applications -Ophthalmology</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>cs, eess, stat</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
