<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction</title>
				<funder ref="#_ubexAKY">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Shanghai Center for Brain Science and Brain-inspired Technology</orgName>
				</funder>
				<funder ref="#_nBy3SCK">
					<orgName type="full">Shanghai Municipal of Science and Technology Project</orgName>
				</funder>
				<funder ref="#_xA9XydN">
					<orgName type="full">Natural Science Foundation of Shanghai</orgName>
				</funder>
				<funder ref="#_dGnHC6N #_YnfQcAu">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiming</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zilong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Pharmacy</orgName>
								<orgName type="institution">China Pharmaceutical University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junping</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hongming</forename><surname>Shan</surname></persName>
							<email>hmshan@fudan.edu.cn</email>
							<idno type="ORCID">0000-0002-0604-3197</idno>
							<affiliation key="aff2">
								<orgName type="department">Institute of Science and Technology for Brain-Inspired Intelligence and MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence (Fudan University)</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Shanghai Center for Brain Science and Brain-Inspired Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="403" to="412"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2DED1F6FB18FE390EC43D3C97AE6F0CC</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_38</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lung nodule classification</term>
					<term>vision-language model</term>
					<term>prompt learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lung nodule malignancy prediction has been enhanced by advanced deep-learning techniques and effective tricks. Nevertheless, current methods are mainly trained with cross-entropy loss using one-hot categorical labels, which results in difficulty in distinguishing those nodules with closer progression labels. Interestingly, we observe that clinical text information annotated by radiologists provides us with discriminative knowledge to identify challenging samples. Drawing on the capability of the contrastive language-image pre-training (CLIP) model to learn generalized visual representations from text annotations, in this paper, we propose CLIP-Lung, a textual knowledge-guided framework for lung nodule malignancy prediction. First, CLIP-Lung introduces both class and attribute annotations into the training of the lung nodule classifier without any additional overheads in inference. Second, we design a channel-wise conditional prompt (CCP) module to establish consistent relationships between learnable context prompts and specific feature maps. Third, we align image features with both class and attribute features via contrastive learning, rectifying false positives and false negatives in latent space. Experimental results on the benchmark LIDC-IDRI dataset demonstrate the superiority of CLIP-Lung, in both classification performance and interpretability of attention maps. Source code is available at https://github.com/ymLeiFDU/CLIP-Lung.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lung cancer is one of the most fatal diseases worldwide, and early diagnosis of the pulmonary nodule has been identified as an effective measure to prevent lung cancer. Deep learning-based methods for lung nodule classification have been widely studied in recent years <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. Usually, the malignancy prediction is often formulated as benign-malignant binary classification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref>, and the higher classification performance and explainable attention maps are impressive. Most previous works employ a learning paradigm that utilizes cross-entropy loss between predicted probability distributions and ground-truth one-hot labels. Furthermore, inspired by ordered labels of nodule progression, researchers have turned their attention to ordinal regression methods to evaluate the benignunsure-malignant classification task <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>, where the training set additionally includes nodules with uncertain labels. Indeed, the ordinal regressionbased methods are able to learn ordered manifolds and to further enhance the prediction accuracy.</p><p>However, the aforementioned methods still face challenges in distinguishing visually similar samples with adjacent rank labels. For example, in Fig. <ref type="figure" target="#fig_0">1</ref>(a), since we conduct unimodal contrastive learning and map the samples onto a spherical space, the false positive nodule with a malignancy score of 2.75 has a closer distance to that with a score of 4.75, and the false negative one should not be closer to that of score 2.5. To address this issue, we found that the text attributes, such as "subtlety", "sphericity", "margin", and "lobulation", annotated by radiologists, can exhibit the differences between these hard samples. Therefore, we propose leveraging text annotations to guide the learning of visual features. In practice, this also aligns with the fact that the annotated text information represents the direct justification for identifying lesion regions in the clinic. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, this text information is beneficial for distinguishing visually similar pairs, while we conduct this behavior by applying contrastive learning that pulls semantic-closer samples and pushes away semantic-farther ones.</p><p>To integrate text annotations into the image-domain learning process, an effective text encoder providing accurate textual features is required. Fortunately, recent advances in vision-language models, such as contrastive languageimage pre-training (CLIP) <ref type="bibr" target="#b15">[16]</ref>, provide us with a powerful text encoder pre-trained with text-based supervisions and have shown impressive results in downstream vision tasks. Nevertheless, it is ineffective to directly transfer CLIP to medical tasks due to the data covariate shift. Therefore, in this paper, we pro- pose CLIP-Lung, a framework to classify lung nodules using image-text pairs. Specifically, CLIP-Lung constructs learnable text descriptions for each nodule from both class and attribute perspectives. Inspired by CoCoOp <ref type="bibr" target="#b19">[20]</ref>, we propose a channel-wise conditional prompt (CCP) module to allow nodule descriptions to guide the generation of informative feature maps. Different from CoCoOp, CCP constructs specific learnable prompts conditioned on grouped feature maps and triggers more explainable attention maps such as Grad-CAM <ref type="bibr" target="#b16">[17]</ref>, whereas CoCoOp provides only the common condition for all the prompt tokens. Then, we design a textual knowledge-guided contrastive learning based on obtained image features and textual features involving classes and attributes. Experimental results on LIDC-IDRI <ref type="bibr" target="#b0">[1]</ref> dataset demonstrate the effectiveness of learning with textual knowledge for improving lung nodule malignancy prediction.</p><p>The contributions of this paper are summarized as follows.</p><p>1) We propose CLIP-Lung for lung nodule malignancy prediction, which leverages clinical textual knowledge to enhance the image encoder and classifier. 2) We design a channel-wise conditional prompt module to establish consistent relationships among the correlated text tokens and feature maps. 3) We simultaneously align the image features with class and attribute features through contrastive learning while generating more explainable attention maps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><formula xml:id="formula_0">classes. C = {c k } K k=1 is a set of text embeddings of classes. Finally, A = {a m } M</formula><p>m=1 is the set of attribute embeddings, where each element a m ∈ R d×1 is a vector representing the embedding of an attribute word such as "spiculation". Then, for a given sample {I i , y i }, our aim is to learn a mapping f θ : I i → y i , where f is a deep neural network parameterized by θ. CLIP-Lung. In Fig. <ref type="figure" target="#fig_1">2</ref>(a), the training framework contains an image encoder f θ and a text encoder g φ . First, the input image I i is fed into f θ and then generates the feature maps. According to Fig. <ref type="figure" target="#fig_1">2(b)</ref>, the feature maps are converted to channel-wise feature vectors f θ (I i ) = F t,: and then to learnable tokens l t . Second, we initialize the context tokens l t and add them with l t to construct the learnable prompts, where T is the number of context words. Next, the concatenation of the class token and l t + l t is used as input of text encoder yielding the class features g φ (c k ) = C k,: , note that C k,: is conditioned on channel-wise feature vectors F t,: . Finally, the attribute tokens a m are also fed into the text encoder to yield corresponding attribute features g φ (a m ) = A m,: . Note that the vectors F t,: , l t,: , l t,: , and C k,: are with the same dimension d = 512 in this paper. Consequently, we have image feature F ∈ R T ×d , class feature C ∈ R K×d , and attribute feature A ∈ R M ×d to conduct the textual knowledge-guided contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Instance-Specific Attribute Weighting</head><p>For the attribute annotations, all the lung nodules in the LIDC-IDRI dataset are annotated with the same eight attributes: "subtlety", "internal structure", "calcification", "sphericity", "margin", "lobulation", "spiculation", and "texture" <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, and the annotated value for each attribute ranges from 1 to 5 except for "calcification" that is ranged from 1 to 6. In this paper, we fix the parameters of a pre-trained text encoder so that the generated eight text feature vectors are the same for all the nodules. Therefore, we propose an instance-specific attribute weighting scheme to distinguish different nodules. For the i-th sample, the weight for each a m is calculated through normalizing the annotated values:</p><formula xml:id="formula_1">w m = exp(v m ) M m=1 exp(v m ) , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where v m denotes the annotated value for a m . Then the weight vectors of the i-th sample is represented as</p><formula xml:id="formula_3">w i = [w 1 , w 2 , . . . , w M ] ∈ R M ×1</formula><p>. Hence, the elementwise multiplication w i • A i is unique to I i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Channel-Wise Conditional Prompt</head><p>CoCoOp <ref type="bibr" target="#b19">[20]</ref> firstly proposed to learn language contexts for vision-language models conditioned on visual features. However, it is inferior to align context words with partial regions of the lesion. Therefore, we propose a channel-wise conditional prompt (CCP) module, in Fig. <ref type="figure" target="#fig_1">2</ref>(b), to split latent feature maps into T groups and then flatten them into vectors F t,: . Next, we denote h(•) as a context network that is composed of a multi-layer perceptron (MLP) with one hidden layer, and each learnable context token is now obtained by l t = h(F t,: ). Hence, the conditional prompt for the t-th token is l t +l t . In addition, CCP also outputs the F t,: for image-class and image-attribute contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Textual Knowledge-Guided Contrastive Learning</head><p>Recall that our aim is to enable the visual features to be similar to the textual features of the annotated classes or attributes and be dissimilar to those of irrelevant text annotations. Consequently, we accomplish this goal through contrastive learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>. In this paper, we conduct such image-text contrastive learning by utilizing pre-trained CLIP text encoder <ref type="bibr" target="#b15">[16]</ref>. In Fig. <ref type="figure" target="#fig_1">2</ref>, we align F ∈ R T ×d with C ∈ R K×d and A ∈ R M ×d , i.e., using class and attribute knowledge to regularize the feature maps.</p><p>Image-Class Alignment. First, the same to CLIP, we align the image and class information by minimizing the cross-entropy (CE) loss for the sample {I i , y i }:</p><formula xml:id="formula_4">L IC = - T t=1 K k=1 y i log exp(σ(F t,: , C k,: )/τ ) K k =1 exp(σ(F t,: , C k ,: )/τ ) , (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where</p><formula xml:id="formula_6">C k,: = g φ (c k (l 1 +l 1 , l 2 +l 2 , . . . , l T +l T )) ∈ R d×1</formula><p>and " " denotes concatenation, i.e., C k,: is conditioned on learnable prompts l t + l t . σ(•, •) calculates the cosine similarity and τ is the temperature term. Therefore, L IC implements the contrastive learning between channel-wise features and corresponding class features, i.e., the ensemble of grouped image-class alignment results. Image-Attribute Alignment. In addition to image-class alignment, we further expect the image features to correlate with specific attributes. So we conduct image-attribute alignment by minimizing the InfoNCE loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>:</p><formula xml:id="formula_7">L IA = - T t=1 M m=1 log exp(σ(F t,: , w m,: • A m,: )/τ ) M m =1 exp(σ(F t,: , w m ,: • A m ,: )/τ ) . (<label>3</label></formula><formula xml:id="formula_8">)</formula><p>Hence, L IA indicates which attribute the F t,: is closest to since each vector F t,: is mapped from the t-th group of feature maps through the context network h(•). Therefore, certain feature maps can be guided by specific annotated attributes. Class-Attribute Alignment. Although the image features have been aligned with classes and attributes, the class embeddings obtained by the pre-trained CLIP encoder may shift in the latent space, which may result in inconsistent class space and attribute space, i.e., annotated attributes do not match the corresponding classes, which is contradictory to the actual clinical diagnosis. To avoid this weakness, we further align the class and attribute features:</p><formula xml:id="formula_9">L CA = - K k=1 M m=1 log exp(σ(C k,: , w m,: • A m,: )/τ ) M m =1 exp(σ(C k,: , w m ,: • A m ,: )/τ ) , (<label>4</label></formula><formula xml:id="formula_10">)</formula><p>and this loss implies semantic consistency between classes and attributes.</p><p>Finally, the total loss function is defined as follows:</p><formula xml:id="formula_11">L = E I i∈I L CE + L IC + α • L IA + β • L CA ,<label>(5)</label></formula><p>where α and β are hyperparameters for adjusting the losses and are set as 1 and 0.5, respectively. L CE denotes the cross-entropy loss between predicted probabilities obtained by the classifier and the ground-truth labels. Note that during the inference phase, test images are only fed into the trained image encoder and classifier. As a result, CLIP-Lung does not introduce any additional computational overhead in inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>Dataset. LIDC-IDRI <ref type="bibr" target="#b0">[1]</ref> is a dataset for pulmonary nodule classification or detection based on low-dose CT, which involves 1,010 patients. According to the annotations, we extracted 2, 026 nodules, and all of them were labeled with scores from 1 to 5, indicating the malignancy progression. We cropped all the nodules with a square shape of a doubled equivalent diameter at the annotated center, then resized them to the volume of 32 × 32 × 32. Following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>, we modified the first layer of the image encoder to be with 32 channels. According to existing works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>, we regard a nodule with an average score between 2.5 and 3.5 as unsure nodules, benign and malignant categories are those with scores lower than 2.5 and larger than 3.5, respectively. In this paper, we construct three sub-datasets: LIDC-A contains three classes of nodules both in training and test sets; according to <ref type="bibr" target="#b10">[11]</ref>, we construct the LIDC-B, which contains three classes of nodules only in the training set, and the test set contains benign and malignant nodules; LIDC-C includes benign and malignant nodules both in training and test sets.</p><p>Experimental Settings. In this paper, we apply the CLIP pre-trained ViT-B/16 as the text encoder for CLIP-Lung, and the image encoder we used is ResNet-18 <ref type="bibr" target="#b5">[6]</ref> due to the relatively smaller scale of training data. The image encoder is initialized randomly. Note that for the text branch, we froze the parameters of the text encoder and updated the learnable tokens l and l during training. The learning rate is 0.001 following the cosine decay, while the optimizer is stochastic gradient descent with momentum 0.9 and weight decay 0.00005. The temperature τ is initialized as 0.07 and updated during training. All of our experiments are implemented with PyTorch <ref type="bibr" target="#b14">[15]</ref> and trained with NVIDIA A100 GPUs. The experimental results are reported with average values through five-fold cross-validation. We report the recall and F1-score values for different classes and use "±" to indicate standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results and Analysis</head><p>Performance Comparisons. In Table <ref type="table" target="#tab_1">1</ref>, we compare the classification performances on the LIDC-A dataset, where we regard the benign-unsure-malignant We argue that this is due to the indistinguishable textual annotations, such as similar attributes of different nodules. In addition, we verify the effect of textual branch of CLIP-Lung using MV-DAR <ref type="bibr" target="#b11">[12]</ref> on LIDC-A dataset. The obtained accuracy values with and without the textual branch are 58.9% and 57.3%, respectively, demonstrating the effectiveness of integrating textual knowledge. Table <ref type="table" target="#tab_2">2</ref> presents a performance comparison of CLIP-Lung on the LIDC-B and LIDC-C datasets. Notably, CLIP-Lung obtains higher evaluation values other than recalls of benign class. This disparity is likely attributed to the similarity in appearances and subtle variations in text attributes among the benign nodules. Consequently, aligning these distinct feature types becomes challenging, resulting in a bias towards the text features associated with malignant nodules.  Visual Features and Attention Maps. To illustrate the influence of incorporating class and attribute knowledge, we provide the t-SNE <ref type="bibr" target="#b13">[14]</ref> and Grad-CAM <ref type="bibr" target="#b16">[17]</ref> results obtained by CLIP, CoCoOp, and CLIP-Lung. In Fig. <ref type="figure" target="#fig_2">3</ref>, we can see that CLIP yields a non-compact latent space for two kinds of nodules.</p><p>CoCoOp and CLIP-Lung alleviate this phenomenon, which demonstrates that the learnable prompts guided by nodule classes are more effective than fixed prompt engineering. Unlike CLIP-Lung, CoCoOp does not incorporate attribute information in prompt learning, leading to increased false negatives in the latent space. From the attention maps, we can observe that CLIP cannot precisely capture spiculation and lobulation regions that are highly correlated with malignancy. Simultaneously, our CLIP-Lung performs better than CoCoOp, which demonstrates the guidance from textual descriptions such as "spiculation".</p><p>Ablation Studies. In Table <ref type="table" target="#tab_3">3</ref>, we verify the effectiveness of different loss components on the three constructed datasets. Based on L IC , L IA and L CA improve the performances on LIDC-A, indicating the effectiveness of capturing fine-grained features of ordinal ranks using class and attribute texts. However, they perform relatively worse on LIDC-B and LIDC-C, especially the L IC + L CA . That is to say, L IA is more important in latent space rectification, i.e., image-attribute consistency. In addition, we observe that L IC +L IA performs better than L IA +L CA , which is attributed to that L CA regularizes the image features indirectly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed a textual knowledge-guided framework for pulmonary nodule classification, named CLIP-Lung. We explored the utilization of clinical textual annotations based on large-scale pre-trained text encoders. CLIP-Lung aligned the different modalities of features generated from nodule classes, attributes, and images through contrastive learning. Most importantly, CLIP-Lung establishes correlations between learnable prompt tokens and feature maps using the proposed CCP module, and this guarantees explainable attention maps localizing fine-grained clinical features. Finally, CLIP-Lung outperforms compared methods, including CLIP on LIDC-IDRI benchmark. Future work will focus on extending CLIP-Lung with more diverse textual knowledge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Motivation of CLIP-Lung. (a) Unimodal contrastive learning. (b) Proposed textual knowledge-guided contrastive learning. Yellow values are the annotated malignancy scores. Dashed boxes contain pairs of textual attributes and annotated values. (Color figure online)</figDesc><graphic coords="2,42,30,54,62,339,52,107,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the proposed CLIP-Lung.</figDesc><graphic coords="3,71,46,54,50,309,52,158,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The t-SNE (Left) and Grad-CAM (Right) results.</figDesc><graphic coords="8,42,30,54,44,339,40,126,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>2.1 Overview Problem Formulation. In</head><label></label><figDesc>this paper, we arrange the lung nodule classification dataset as {I, Y, C, A}, where I = {I i } N i=1 is an image set containing N lung nodule images. Y = {y i } N</figDesc><table /><note><p>i=1 is the corresponding class label set and y i ∈ {1, 2, . . . , K}, and K is the number of</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Classification results on the test set of LIDC-A.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>Benign</cell><cell></cell><cell cols="2">Malignant</cell><cell>Unsure</cell></row><row><cell></cell><cell></cell><cell cols="2">Recall F1</cell><cell cols="2">Recall F1</cell><cell>Recall F1</cell></row><row><cell>CE Loss</cell><cell cols="2">54.2 ± 0.6 72.2</cell><cell cols="2">62.0 64.4</cell><cell>61.3 29.0</cell><cell>36.6</cell></row><row><cell>Poisson [2]</cell><cell cols="2">52.7 ± 0.7 60.5</cell><cell cols="2">56.8 58.4</cell><cell>58.7 41.0</cell><cell>44.1</cell></row><row><cell>NSB [13]</cell><cell cols="2">53.4 ± 0.7 80.7</cell><cell cols="2">63.0 67.3</cell><cell>63.8 16.0</cell><cell>24.2</cell></row><row><cell>UDM [18]</cell><cell cols="2">54.6 ± 0.4 76.7</cell><cell cols="2">64.3 49.5</cell><cell>53.5 32.5</cell><cell>39.5</cell></row><row><cell>CORF [21]</cell><cell cols="2">56.8 ± 0.4 71.3</cell><cell cols="2">63.3 61.3</cell><cell>62.3 38.5</cell><cell>44.3</cell></row><row><cell>CLIP [16]</cell><cell cols="2">56.6 ± 0.3 59.5</cell><cell cols="2">59.2 55.2</cell><cell>60.0 53.9</cell><cell>52.2</cell></row><row><cell cols="3">CoCoOp [20] 56.8 ± 0.6 59.0</cell><cell cols="2">59.2 55.2</cell><cell>60.0 55.1</cell><cell>52.8</cell></row><row><cell cols="3">CLIP-Lung 60.9 ± 0.4 67.5</cell><cell cols="2">64.4 60.9</cell><cell>66.3 53.4</cell><cell>54.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Classification results on test sets of LIDC-B and LIDC-C. Lung obtained higher recalls than CLIP and CoCoOp w.r.t. benign and malignant classes, however, the recall of unsure is lower than theirs.</figDesc><table><row><cell>Method</cell><cell>LIDC-B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LIDC-C</cell></row><row><cell></cell><cell>Accuracy</cell><cell>Benign</cell><cell></cell><cell cols="2">Malignant</cell><cell>Accuracy</cell><cell>Benign</cell><cell>Malignant</cell></row><row><cell></cell><cell></cell><cell cols="2">Recall F1</cell><cell cols="2">Recall F1</cell><cell>Recall F1</cell><cell>Recall F1</cell></row><row><cell>CE Loss</cell><cell cols="2">83.3 ± 0.6 92.4</cell><cell cols="2">88.4 63.4</cell><cell cols="2">70.3 85.5 ± 0.5 91.5</cell><cell>89.7 72.3</cell><cell>75.6</cell></row><row><cell>Poisson [2]</cell><cell cols="2">81.8 ± 0.4 94.2</cell><cell cols="2">87.7 54.5</cell><cell cols="2">65.1 84.0 ± 0.3 87.9</cell><cell>88.3 75.2</cell><cell>74.5</cell></row><row><cell>NSB [13]</cell><cell cols="2">78.1 ± 0.5 90.6</cell><cell cols="2">85.8 50.5</cell><cell cols="2">60.7 84.9 ± 0.7 91.0</cell><cell>89.2 71.3</cell><cell>74.6</cell></row><row><cell>UDM [18]</cell><cell cols="2">79.3 ± 0.4 87.0</cell><cell cols="2">86.2 62.4</cell><cell cols="2">67.7 84.6 ± 0.5 88.8</cell><cell>88.8 75.2</cell><cell>75.2</cell></row><row><cell>CORF [21]</cell><cell cols="2">81.5 ± 0.3 95.9</cell><cell cols="2">87.8 49.5</cell><cell cols="2">62.8 83.0 ± 0.2 87.9</cell><cell>87.7 72.3</cell><cell>72.6</cell></row><row><cell>CLIP [16]</cell><cell cols="2">83.6 ± 0.6 92.0</cell><cell cols="2">88.7 64.4</cell><cell cols="2">70.4 87.5 ± 0.3 92.0</cell><cell>91.0 77.0</cell><cell>78.8</cell></row><row><cell cols="3">CoCoOp [20] 86.8 ± 0.7 94.5</cell><cell cols="2">90.9 69.0</cell><cell cols="2">75.9 88.2 ± 0.6 95.0</cell><cell>91.8 72.4</cell><cell>78.8</cell></row><row><cell cols="3">CLIP-Lung 87.5 ± 0.3 94.5</cell><cell cols="2">91.7 72.3</cell><cell cols="2">79.0 89.5 ± 0.4 94.0</cell><cell>92.8 80.5</cell><cell>82.8</cell></row><row><cell cols="7">as an ordinal relationship. Compared with ordinal classification methods such</cell></row><row><cell cols="7">as Poisson, NSB, UDM, and CORF, CLIP-Lung achieves the highest accuracy</cell></row><row><cell cols="7">and F1-scores for the three classes, demonstrating the effectiveness of textual</cell></row><row><cell cols="7">knowledge-guided learning. CLIP and CoCoOp also outperform ordinal classifi-</cell></row><row><cell cols="7">cation methods and show the superiority of large-scale pre-trained text encoders.</cell></row><row><cell cols="2">Furthermore, CLIP-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on different losses. We report classification accuracies.</figDesc><table /><note><p>LIC LIA LCA LIDC-A LIDC-B LIDC-C 56.8 ± 0.6 86.8 ± 0.7 88.2 ± 0.6 59.4 ± 0.4 86.8 ± 0.6 86.7 ± 0.4 58.1 ± 0.2 85.7 ± 0.6 87.5 ± 0.5 56.9 ± 0.3 84.7 ± 0.4 84.0 ± 0.7 60.9 ± 0.4 87.5 ± 0.5 89.5 ± 0.4</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements. This work was supported in part by</head><p><rs type="funder">Natural Science Foundation of Shanghai</rs> (No. <rs type="grantNumber">21ZR1403600</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (Nos. <rs type="grantNumber">62101136</rs> and <rs type="grantNumber">62176059</rs>), <rs type="funder">China Postdoctoral Science Foundation</rs> (No. <rs type="grantNumber">2022TQ0069</rs>), <rs type="funder">Shanghai Municipal of Science and Technology Project</rs> (No. <rs type="grantNumber">20JC1419500</rs>), and <rs type="funder">Shanghai Center for Brain Science and Brain-inspired Technology</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xA9XydN">
					<idno type="grant-number">21ZR1403600</idno>
				</org>
				<org type="funding" xml:id="_dGnHC6N">
					<idno type="grant-number">62101136</idno>
				</org>
				<org type="funding" xml:id="_YnfQcAu">
					<idno type="grant-number">62176059</idno>
				</org>
				<org type="funding" xml:id="_ubexAKY">
					<idno type="grant-number">2022TQ0069</idno>
				</org>
				<org type="funding" xml:id="_nBy3SCK">
					<idno type="grant-number">20JC1419500</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Armato</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mclennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bidaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="915" to="931" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unimodal probability distributions for deep ordinal classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="411" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring simple Siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predictive capabilities of statistical learning methods for lung nodule malignancy classification using diagnostic image features: an investigation using the lung image database consortium dataset</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Magnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Diagnosis</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="558" to="569" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">O</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lung nodule malignancy classification with weakly supervised explanation generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="44502" to="044502" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shape and marginaware lung nodule classification in low-dose CT images via soft activation mapping</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">101628</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Strided self-supervised low-dose CT denoising for lung nodule classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phenomics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="257" to="268" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Meta ordinal regression forest for medical image classification with ordinal labels</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA J. Autom. Sin</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1233" to="1247" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning from ambiguous labels for lung nodule malignancy prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1874" to="1884" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ordinal regression with neuron stick-breaking for medical diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vijaya Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<title level="m">Automatic differentiation in PyTorch</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grad-CAM: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning with unsure data for medical image diagnosis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10590" to="10599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised adversarial model for benign-malignant lung nodule classification on chest CT</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="237" to="248" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional prompt learning for visionlanguage models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16816" to="16825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional ordinal regression forest for image ordinal estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4084" to="4095" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
