<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification</title>
				<funder ref="#_ScmXcmc">
					<orgName type="full">The Key R&amp;D Program of Guangdong Province, China</orgName>
				</funder>
				<funder ref="#_u2QRscZ #_pcmTBmz">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_svXaMdz">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
					<orgName type="abbreviated">STCSM</orgName>
				</funder>
				<funder>
					<orgName type="full">CAAI-Huawei MindSpore Open Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Changkai</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">State Key Laboratory of Multimodal Artificial Intelligence Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changde</forename><surname>Du</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">State Key Laboratory of Multimodal Artificial Intelligence Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Radiology</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Renji Hospital Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Institute for Medical Imaging Technology</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chong</forename><surname>Ma</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaming</forename><surname>Xie</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Radiology</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Renji Hospital Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huiguang</forename><surname>He</surname></persName>
							<email>huiguang.he@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">State Key Laboratory of Multimodal Artificial Intelligence Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<email>dgshen@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Shanghai Clinical Research and Trial Center</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="68" to="78"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">53331E93893820B71D0401C48C903FAE</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Mammogram classification</term>
					<term>Gaze</term>
					<term>Multi-view interaction</term>
					<term>Bidirectional fusion learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Breast cancer diagnosis is a challenging task. Recently, the application of deep learning techniques to breast cancer diagnosis has become a popular trend. However, the effectiveness of deep neural networks is often limited by the lack of interpretability and the need for significant amount of manual annotations. To address these issues, we present a novel approach by leveraging both gaze data and multi-view data for mammogram classification. The gaze data of the radiologist serves as a low-cost and simple form of coarse annotation, which can provide rough localizations of lesions. We also develop a pyramid loss better fitting to the gaze-supervised process. Moreover, considering many studies overlooking interactive information relevant to diagnosis, we accordingly utilize transformer-based attention in our network to mutualize multi-view pathological information, and further employ a bidirectional fusion learning (BFL) to more effectively fuse multi-view information. Experimental results demonstrate that our proposed model significantly improves both mammogram classification performance and interpretability through incorporation of gaze data and cross-view interactive information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Breast cancer is the most prevalent form of cancer among women and can have serious physical and mental health consequences if left unchecked <ref type="bibr" target="#b4">[5]</ref>. Early detection through mammography is critical for early treatment and prevention <ref type="bibr" target="#b18">[19]</ref>. Mammograms provide images of breast tissue, which are taken from two views: the cranio-caudal (CC) view, and the medio-lateral oblique (MLO) view <ref type="bibr" target="#b3">[4]</ref>. By identifying breast cancer early, patients can receive targeted treatment before the disease progresses.</p><p>Deep neural networks have been widely adopted for breast cancer diagnosis to alleviate the workload of radiologists. However, these models often require a large number of manual annotations and lack interpretability, which can prevent their broader applications in breast cancer diagnosis. Radiologists typically focus on areas with breast lesions during mammogram reading <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref>, which provides valuable guidance. We propose using real-time eye tracking information from radiologists to optimize our model. By using gaze data to guide model training, we can improve model interpretability and performance <ref type="bibr" target="#b23">[24]</ref>.</p><p>Radiologists' eye movements can be automatically and unobtrusively recorded during the process of reading mammograms, providing a valuable source of data without the need for manual labeling. Previous studies have incorporated radiologists' eye-gaze as a form of weak supervision, which directs the network's attention to the regions with possible lesions <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>. Leveraging gaze from radiologists to aid in model training not only increases efficiency and minimizes the risk of errors linked to manual annotation, but also can be seamlessly implemented without affecting radiologists' normal clinical interpretation of mammograms.</p><p>Mammography primarily detects two types of breast lesions: masses and microcalcifications <ref type="bibr" target="#b15">[16]</ref>. The determination of the benign or malignant nature of masses is largely dependent on the smoothness of their edges <ref type="bibr" target="#b12">[13]</ref>. The gaze data can guide the model's attention towards the malignant masses. Microcalcifications are small calcium deposits which exhibit irregular boundaries on mammograms <ref type="bibr" target="#b8">[9]</ref>. This feature makes them challenging to identify, often leading to missed or false detection by models. Radiologists need to magnify mammograms to differentiate between benign scattered calcifications and clustered calcifications, the latter of which are more likely to be malignant and necessitate further diagnosis. Leveraging gaze data can guide the model to locate malignant calcifications.</p><p>In this work, we propose a novel diagnostic model, namely Mammo-Net, which integrates radiologists' gaze data and interactive information between CC-view and MLO-view to enhance diagnostic performance. To the best of our knowledge, this is the first work to integrate gaze data into multi-view mammography classification. We utilize class activation map (CAM) <ref type="bibr" target="#b17">[18]</ref> to calculate the attention maps for the model. Additionally, we apply pyramid loss to maintain consistency between radiologists' gaze heat maps and the model's attention maps at multiple scales of the pyramid <ref type="bibr" target="#b0">[1]</ref>. Our model is designed for singlebreast cases. Mammo-Net extracts multi-view features and utilizes transformerbased attention to mutualize information <ref type="bibr" target="#b20">[21]</ref>. Furthermore, there are differences between multi-view mammograms of the same patient, arising from variations in breast shape and density. Capturing these multi-view shared features can be a challenge for models. To address this issue, we develop a novel method called bidirectional fusion learning (BFL) to extract shared features from multi-view mammograms.</p><p>Our contributions can be summarized as follows:</p><p>• We emphasize the significance of low-cost gaze to provide weakly-supervised positioning and visual interpretability for the model. Additionally, we develop a pyramid loss that adapts to the supervised process. • We propose a novel breast cancer diagnosis model, namely Mammo-Net. This model employs transformer-based attention to mutualize information and uses BFL to integrate task-related information to make accurate predictions. • We demonstrate the effectiveness of our approach through experiments using mammography datasets, which show the superiority of Mammo-Net.</p><p>2 Proposed Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Architecture</head><p>The pipeline of Mammo-Net is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Mammo-Net feeds two-view mammograms of the same breast into two ResNet-style <ref type="bibr" target="#b6">[7]</ref> CNN branch networks. We use several ResNet blocks pre-trained on ImageNet <ref type="bibr" target="#b2">[3]</ref> to process mammograms. Then, we use global average pooling (GAP) and fully connected layers to compute the feature vectors produced by the model. Before the final residual block, we employ cross-view attention to mutualize multi-view information. Our proposed method employs BFL to effectively fuse multi-view information to improve diagnostic accuracy. Additionally, by integrating gaze data from radiologists, our proposed model is able to generate more precise attention maps.</p><p>The fusion network combines multi-view feature representations using a stack of linear-activation layers and a fully connected layer, resulting in a classification output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gaze Supervision</head><p>In this module, we utilize CAM to calculate the attention map for the network by examining gradient-based activations in back-propagation. After that, we employ pyramid loss to make the network attention being consistent with the supervision of radiologists' gaze heat maps, guiding the network to focus on the same lesion areas as the radiologists. This module guides the network to accurately extract pathological features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Activation Map.</head><p>At the final convolutional layer of our model, the activation of the ith feature map f i (x, y) at coordinates (x, y) is associated with a weight w k i for class k. This allows us to generate the attention map H k for class k as:</p><formula xml:id="formula_0">H k = i w k i f i (x, y). (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>Pyramid Loss. To enhance the learning of important attention areas, we propose a pyramid loss constraint that requires consistency between the network and gaze attention maps. The pyramid loss is based on using a pyramid representation of the attention map:</p><formula xml:id="formula_2">L P yramid = L l ||(Z(G l (H))) + -(Z(G l (R))) + || 2 , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where H is the network attention map generated by the CAM and R is the radiologist's gaze heat map. square error (MSE) between the attention maps generated by the radiologist and the model at each level of the Gaussian pyramid. This allows the model to mimic the attention of radiologists and enhance diagnostic performance. Moreover, the pyramid representation enables the model to learn from the important pathological regions on which radiologists are focusing, without the need for precise pixel-level information. Layernorm is also employed to address the issue of imprecise gaze data. This reduces noise in the consistency process by performing consistency loss only in the regions where radiologist spent most time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Interactive Information</head><p>Transformer-Based Mutualization Model. We use transformer-based attention to mutualize information from the two views at the level of the spatial feature map. For each attention head, we compute embeddings for the source and target pixels. Our model does not utilize positional encoding, as it encodes the relative position of each pixel and is not suitable for capturing information between different views of mammograms <ref type="bibr" target="#b20">[21]</ref>. The target view feature maps are transformed into Q, the source view feature maps are transformed into K, and the original source feature maps are transformed into V . We can then obtain a weighted sum of the features from the source view for each target pixel using <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_4">Attention(Q, K, V ) = sof tmax( QK T √ d k )V.<label>(3)</label></formula><p>Subsequently, the output is transformed into attention-based feature maps X and mutualized with the feature maps Y from the other view. The mutualized feature maps are normalized and used for subsequent calculations:</p><formula xml:id="formula_5">Z = Norm(Y + Linear(X)). (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Bidirectional Fusion Learning. To enable the fusion network to retain more of the shared features between the two views and filter out noise, we propose to use BFL to learn a fusion representation that maximizes the cross-view mutual information. The optimization target is to generate a fusion representation I from multi-view representations p v , where v ∈ {cc, mlo}. We employ the Noise-Contrastive Estimation framework <ref type="bibr" target="#b5">[6]</ref> to maximize the mutual information, which is a contrastive learning framework:</p><formula xml:id="formula_7">L(I, P v ) = -E P log s(I, p i v ) p j v ∈Pv s(I, p j v ) , (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where s(I, p v ) evaluates the correlation between multi-view fused representations and single-view representations <ref type="bibr" target="#b16">[17]</ref>:</p><formula xml:id="formula_9">s(I, p v ) = exp p v N (I) T , p v = p v ||p v || 2 , N (I) = N (I) ||N (I)|| 2 , (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where N (I) is a reconstruction of p v generated by a fully connected network N from I and the Euclidean norm || • || 2 is applied to obtain unit-length vectors.</p><p>In contrastive learning, we consider the same patient mammograms as positive samples and those from different patient mammograms in the same batch P i v = P v \{p i v } as negative samples <ref type="bibr" target="#b16">[17]</ref>. Minimizing the similarity between the same patient mammograms enables the model to learn shared features. Maximizing the dissimilarity between different patient mammograms enhances the model's robustness.</p><p>In short, we require the fusion representation I to reversely reconstruct multiview representations p v so that more view-invariant information can be passed to I. By aligning the prediction N (I) to p v , we enable the model to decide how much information it should receive from each view.</p><p>The overall loss function for this module is the sum of the losses defined for each view:</p><formula xml:id="formula_11">L BF L = L cc I + L mlo I . (<label>7</label></formula><formula xml:id="formula_12">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Loss Function</head><p>We use binary cross entropy loss (BCE) between the network prediction and the ground-truth as the classification loss. In conclusion, we have proposed a total of three loss functions to guide the model training: L BCE , L BF L , and L P yramid . The overall loss function is defined as the sum of these three loss functions, with coefficients λ and μ used to adjust their relative weights:</p><formula xml:id="formula_13">L overall = L BCE + λL P yramid + μL BF L . (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Mammogram Dataset. Our experiments were conducted on CBIS-DDSM <ref type="bibr" target="#b11">[12]</ref> and INbreast <ref type="bibr" target="#b15">[16]</ref>. The CBIS-DDSM dataset contains 1249 exams that have been divided based on the presence or absence of masses, which we used to perform mass classification. The INbreast dataset contains 115 exams with both masses and micro-calcifications, on which we performed benign and malignant classification. We split the INbreast dataset into training and testing sets in a 7:3 ratio. It is worth noting that the official INbreast dataset does not provide image-level labels, so we obtained these labels following Shen et al. <ref type="bibr" target="#b19">[20]</ref>.</p><p>Eye Gaze Dataset. Eye movement data was collected by reviewing all cases in INbreast using a Tobii Pro Nano eye tracker. The scenario is shown in Appendix and can be accessed at https://github.com/JamesQFreeman/MicEye. Participated radiologist has 11 years of experience in mammography screening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We trained our model using the Adam optimizer <ref type="bibr" target="#b9">[10]</ref> with a learning rate of 10 -4 (partly implemented by MindSpore). To overcome the problem of limited data, we employed various data augmentation techniques, including translation, rotation, and flipping. To address the problem of imbalanced classes, we utilized a weighted loss function that assigns higher weights to malign cases in order to balance the number of benign and malign cases. The coefficients λ and μ of L overall were set to 0.5 and 0.2, respectively, based on 5-fold cross validation on the training set. The network was trained for 300 epochs. We used Accuracy (ACC) and the Area Under the ROC Curve (AUC) <ref type="bibr" target="#b24">[25]</ref> as our evaluation metrics, and we selected the final model based on the best validation AUC. Considering the relatively small size of our dataset, we used ResNet-18 as the backbone of our network.  <ref type="table" target="#tab_0">1</ref>, we compare our model to other methods and find that our model performs better. Lopez et al. <ref type="bibr" target="#b13">[14]</ref> proposed the use of hypercomplex networks to mimic radiologists. By leveraging the properties of hypercomplex algebra, the model is able to continually process two mammograms together. Lee et al. <ref type="bibr" target="#b25">[26]</ref> proposed a 2-channel approach that utilizes a Gaussian model to capture the spatial correlation between lesions across two views, and an LT-GAN to achieve a robust mammography classification. We also compare our model with other methods that use eye movement supervision as shown in Table <ref type="table" target="#tab_0">1</ref>. The GA-Net <ref type="bibr" target="#b22">[23]</ref> proposed a ResNet-based model with class activation mapping guided by eye gaze data. We developed a multi-view model using this approach for a fair comparison, and found that our method performed better. We believe that one possible reason for the inferior performance of GA-Net compared to Mammo-Net might be the use of a simple MSE loss by GA-Net, which neglects the coarse nature of the gaze data. Jiang et al. <ref type="bibr" target="#b7">[8]</ref> proposed a Double-model that fuses gaze maps with original images before training. However, this model did not consider the gap between research and clinical workflow. This model requires gaze input during both the training and inference stages, which limits its practical use in hospitals without eyetrackers. In contrast, our method does not rely on gaze input during inference stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Analysis</head><p>Visualization. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the visualization of our proposed model on three representative exams from the INbreast dataset that includes masses, calcifications, and a combination of both. For each exam, we present gaze heat maps generated from eye movement data. The preprocessing process is shown in Fig. <ref type="figure">5</ref> (see Appendix). To make an intuitive comparison, we exhibit attention maps generated by the model under both unsupervised and gaze-supervised cases. Each exam is composed of two views, i.e., the CC-view and the MLO-view. More exams can be found in Fig. <ref type="figure">6</ref> (see Appendix).</p><p>The results of the visualization demonstrate that the model's capability in localizing lesions becomes more precise when radiologist attention is incorporated in the training stage. The pyramid loss improves the model's robustness even when the radiologist's gaze data is not entirely focused on the breast. This intuitively demonstrates the effectiveness of training the model with eye-tracking supervision.</p><p>Ablation Study. We perform an ablation analysis to assess each component (radiologist attention, cross-view attention and BFL) in Mammo-Net. Table <ref type="table" target="#tab_0">1</ref> suggests that each part of the proposed framework contributes to the increased performance. This shows the benefits of adapting the model to mimic the radiologist's decision-making process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Discussion</head><p>In this paper, we have developed a breast cancer diagnosis model to mimic the radiologist's decision-making process. To achieve this, we integrate gaze data as a form of weak supervision for both lesion positioning and interpretability of the model. We also utilize transformer-based attention to mutualize multi-view information and further develop BFL to fully fuse multi-view information. Our experimental results on mammography datasets demonstrate the superiority of our proposed model. In future work, we intend to explore the use of scanning path analysis as a means of obtaining insights into the pathology-relevant regions of lesions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Mammo-Net consists of two components: a multi-view classification network (upper half) and an attention consistency module (lower half). The classification network interacts multi-view information, while the attention consistency module provides positional supervision.</figDesc><graphic coords="4,64,74,271,58,284,11,270,34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparative visualization of mammography diagnosis with and without gaze supervision. After integrating gaze supervision, the model's capability in localizing lesions becomes more precise.</figDesc><graphic coords="8,68,01,165,20,325,63,121,45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation study of key components of Mammo-Net, and comparison of different models in terms of AUC and ACC. "BFL" denotes "Bidirectional Fusion Learning", and "RA" denotes "Radiologist Attention".</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>AUC ACC</cell></row><row><cell cols="2">CBIS-DDSM Lopez et al. [14]</cell><cell>0.739 0.754</cell></row><row><cell></cell><cell>Tulder et al. [2]</cell><cell>0.802 0.811</cell></row><row><cell></cell><cell>Xian et al. [26]</cell><cell>0.812 0.735</cell></row><row><cell></cell><cell>MLO-view</cell><cell>0.701 0.763</cell></row><row><cell></cell><cell>CC-view</cell><cell>0.721 0.754</cell></row><row><cell></cell><cell>Cross-view</cell><cell>0.809 0.838</cell></row><row><cell></cell><cell>Cross-view+BFL</cell><cell>0.821 0.864</cell></row><row><cell>INbreast</cell><cell>Wang et al. [23]</cell><cell>0.806 0.756</cell></row><row><cell></cell><cell>Jiang et al. [8]</cell><cell>0.819 0.793</cell></row><row><cell></cell><cell>Lopez et al. [14]</cell><cell>0.793 0.830</cell></row><row><cell></cell><cell>Xian et al. [26]</cell><cell>0.859 0.791</cell></row><row><cell></cell><cell>MLO-view</cell><cell>0.663 0.716</cell></row><row><cell></cell><cell>CC-view</cell><cell>0.650 0.704</cell></row><row><cell></cell><cell>Cross-view</cell><cell>0.762 0.755</cell></row><row><cell></cell><cell>Cross-view+BFL</cell><cell>0.786 0.812</cell></row><row><cell></cell><cell>Cross-view+RA</cell><cell>0.864 0.830</cell></row><row><cell></cell><cell cols="2">Cross-view+BFL+RA (Mammo-Net) 0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.889 0.849 Performance Comparison.</head><label></label><figDesc>As shown in Table</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by <rs type="funder">The Key R&amp;D Program of Guangdong Province, China</rs> (grant number <rs type="grantNumber">2021B0101420006</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (grant numbers <rs type="grantNumber">62131015</rs>, <rs type="grantNumber">82272072</rs>), <rs type="funder">Science and Technology Commission of Shanghai Municipality (STCSM)</rs> (grant number <rs type="grantNumber">21010502600</rs>), and the <rs type="funder">CAAI-Huawei MindSpore Open Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ScmXcmc">
					<idno type="grant-number">2021B0101420006</idno>
				</org>
				<org type="funding" xml:id="_u2QRscZ">
					<idno type="grant-number">62131015</idno>
				</org>
				<org type="funding" xml:id="_pcmTBmz">
					<idno type="grant-number">82272072</idno>
				</org>
				<org type="funding" xml:id="_svXaMdz">
					<idno type="grant-number">21010502600</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 7.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pyramid methods in image processing</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ogden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RCA Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="33" to="41" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth estimation for colonoscopy images with self-supervised learning from videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-112" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluation of deep learningbased artificial intelligence techniques for breast cancer detection on mammograms: results from a retrospective study using a breastscreen victoria dataset</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Frazer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brotchie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging Radiat. Oncol</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="529" to="537" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cancer statistics for African American/black people 2022</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Giaquinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Tossas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jemal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="202" to="229" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: a new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eye tracking based deep learning analysis for the early detection of diabetic retinopathy: a pilot study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Available at SSRN</title>
		<imprint>
			<biblScope unit="page">4247845</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Breast-cancer screening-viewpoint of the iarc working group. New Engl</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Jørgensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="page">1478</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using gaze-tracking data and mixture distribution analysis to support a holistic model for the detection of cancers on mammograms</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Kundel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Nodine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Krupinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mello-Thoms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acad. Radiol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="881" to="886" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A curated mammography data set for use in computer-aided detection and diagnosis research</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hoogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Miyake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gorovoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain generalization for mammography detection via multi-style and multi-view contrastive learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-210" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12907</biblScope>
			<biblScope unit="page" from="98" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-view breast cancer classification via hypercomplex neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valleriani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05798</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Eye-gaze-guided vision transformer for rectifying shortcut learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inbreast: toward a full-field digital mammographic database</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Domingues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acad. Radiol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="248" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning hierarchical attention for weakly-supervised chest xray abnormality localization and diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2698" to="2710" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Breast Diseases: Imaging and Clinical Management</title>
		<author>
			<persName><forename type="first">R</forename><surname>Selvi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-81-322-2077-0</idno>
		<ptr target="https://doi.org/10.1007/978-81-322-2077-0" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning to improve breast cancer detection on screening mammography</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Margolies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Rothstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fluder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12495</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Investigating the association of eye gaze pattern and diagnostic error in mammography</title>
		<author>
			<persName><forename type="first">S</forename><surname>Voisin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Morin-Ducote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Tourassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Observer Performance, and Technology Assessment</title>
		<imprint>
			<biblScope unit="volume">8673</biblScope>
			<biblScope unit="page">867302</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note>Image Perception</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Follow my eye: using gaze to supervise computer-aided diagnosis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1688" to="1698" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Eye movements in medical image perception: a selective review of past, present and future</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep neural networks improve radiologists&apos; performance in breast cancer screening</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1184" to="1194" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards robust dual-view transformation via densifying sparse supervision for mammography lesion matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-334" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="355" to="365" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
