<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">You</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Gang</forename><surname>Yang</surname></persName>
							<email>yanggang@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">MOE Key Lab of DEKE</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Vistel AI Lab</orgName>
								<orgName type="institution">Visionary Intelligence Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dayong</forename><surname>Ding</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Vistel AI Lab</orgName>
								<orgName type="institution">Visionary Intelligence Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianchun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Vistel AI Lab</orgName>
								<orgName type="institution">Visionary Intelligence Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="704" to="713"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">028918A8F5F397322B13E9C35E4DFCD4</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_66</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Glaucoma recognition</term>
					<term>Multi-modal learning</term>
					<term>Multiple instance learning</term>
					<term>Contrastive learning Vanilla Encoders Vanilla Encoders Encoder k Encoder q Transform 1 Transform 2 Bilateral Contrastive Alignment (Self-supervised) ctr loss Vanilla Encoders Block Embedding Multiple Instance Learning Representation Vanilla Encoders Co-attention Encoders Hierarchical Attention Fusion Patch Embedding MLP Norm Multi-Head Attention Norm MLP</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Early glaucoma can be diagnosed with various modalities based on morphological features. However, most existing automated solutions rely on single-modality, such as Color Fundus Photography (CFP) which lacks 3D structural information, or Optical Coherence Tomography (OCT) which suffers from insufficient specificity for glaucoma. To effectively detect glaucoma with CFP and OCT, we propose a generic multi-modal Transformer-based framework for glaucoma, MM-RAF. Our framework is implemented with pure self-attention mechanisms and consists of three simple and effective modules: Bilateral Contrastive Alignment (BCA) aligns both modalities into the same semantic space to bridge the semantic gap; Multiple Instance Learning Representation (MILR) aggregates multiple OCT B-scans into a semantic structure and downsizes the scale of the OCT branch; Hierarchical Attention Fusion (HAF) enhances the cross-modality interaction capability with spatial information. By incorporating three modules, our framework can effectively handle cross-modality interaction between different modalities with huge disparity. The experimental results demonstrate that the framework outperforms the existing multi-modal methods of this task and is robust even with a clinical small dataset. Moreover, by visualizing, OCT can reveal the subtle abnormalities in CFP, indicating that the relationship between various modalities is captured. Our code is available at https://github.com/YouZhouRUC/MM-RAF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>photography (CFP) and optical coherence tomography (OCT). The gold standard of glaucoma in CFP includes Cup to Disk Ratio (CDR) enlargement and Retinal Nerve Fiber Layer Defects (RNFLD). Diagnosing with CFP has earned superb performance, but it only captures flat information. OCT scans the finegrain 3D structure of the fundus, and the quantitative analysis conducted by OCT can help diagnosis for junior clinicians. Many crucial lesions coexist in both CFP and OCT images. We list some critical lesions correlated to glaucoma in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Early research on automated glaucoma recognition focuses on CFP. Based on the ISNT rule <ref type="bibr" target="#b7">[8]</ref>, the vertical Cup-to-Disc rate (vCDR) is a feature with high specificity <ref type="bibr" target="#b14">[15]</ref>. Besides, the RNFLD is an important clinical diagnostic evidence in glaucoma <ref type="bibr" target="#b5">[6]</ref>. Previous research on OCT mainly discusses the RNFL thickness map <ref type="bibr" target="#b1">[2]</ref>, GCL thickness map <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>, and cube scan <ref type="bibr" target="#b15">[16]</ref>. Combining CFP and OCT for multi-modal diagnosis shows promise in providing accurate diagnostic performance and additional stereo information on retinal structure. However, multi-modal methods on glaucoma recognition have rarely been investigated. Simply combining CFP and the thickness map from the OCT report <ref type="bibr" target="#b0">[1]</ref> will waste most information in OCT. An image pair contains merely one CFP image and up to 256 OCT B-scan frames. Interaction between two modalities with unbalanced amounts poses challenges for the existing methods. COROLLA <ref type="bibr" target="#b2">[3]</ref> introduces supervised contrastive learning and uses a dual-stream network without modality interaction. Mehta et al. <ref type="bibr" target="#b12">[13]</ref> use two different convolutional branches to extract the features of OCT cube and CFP respectively and then ensembles to make the diagnosis. MM-MIL <ref type="bibr" target="#b10">[11]</ref> implements two ResNet50 to extract the features from different modalities and adopts Multiple Instance Learning to automatically discover crucial instances. However, the fusion stage is restricted by the Global Average Pooling operation, which limits the spatial information interaction.</p><p>To address the problems discussed above, We propose MM-RAF. By following the paradigm of multi-modal learning, i.e., representation, alignment and fusion, we construct three effective modules. To alleviate the semantic gap, BCA introduces bilateral contrastive loss to improve the intra-and inter-modal alignment capability. In the representation stage, MILR extracts the glaucoma-relevant OCT B-scans(instance) from the OCT volume (bag) to assemble a saturated and semantic structure and reduce the scale of the OCT branch. To solve the inability of cross-modality interaction and loss of spatial information, Hierarchical Attention Fusion (HAF) utilizes two strategies to extract modal-specific features and modal-agnostic features to make a final diagnosis. Two classical Transformer encoders are implemented throughout our framework.</p><p>Through the experiments on the private dataset, we illustrate that MM-RAF outperforms the existing multi-modal methods on glaucoma recognition. Also, it is demonstrated that MILR and HAF effectively enhance performance in the ablation study. Moreover, extensive experiments on GAMMA dataset <ref type="bibr" target="#b18">[19]</ref> prove that the BCA module can promote robustness. We implement relevancebased methods <ref type="bibr" target="#b3">[4]</ref> for visualization. The heatmaps illustrate that the framework manifests indistinguishable lesions like RNFLD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, MM-RAF is a two-phase framework consisting of three modules, i.e., BCA, MILR, and HAF. Inspired by Vision Transformer <ref type="bibr" target="#b6">[7]</ref> for modeling long-range dependency, we incorporate two classical transformer encoders throughout our framework to make comprehensive interaction between different modalities. In the contrastive phase, pretraining on unlabeled multi-modal data enables BCA to align the features from different modalities into the same semantic space, diminishing the semantic gap. In the following phase, MILR employs Multiple Instance Learning to refine the cumbersome OCT branch to a semantic structure. Then, with balanced streams from two modalities, HAF renders two cross-modality interaction strategies to make inter-and intra-modal diagnoses. In the following sections, we will clarify each module specifically.</p><p>Vanilla Encoder and Co-attention Encoder. Two basic encoders enable effective intra-and inter-modal interaction. As shown in Eqs. 1-2, the Vanilla encoder duplicates the input stream into query, key and value (Q m , K m and V m ) components and concentrates on intra-modal interaction, while the Co-attention receives two input streams from different modalities and focuses on inter-modal interaction, with the primary stream acting as query, Q m , and the subordinate stream replicated as key and value (K m, V m). Due to the high computational cost of the self-attention mechanism in the OCT branch, we propose block embedding, partitioning the OCT volume into n OCT blocks and each block will be embedded as</p><formula xml:id="formula_0">T k O,blk ∈ R N ×dim (N is 196, dim is 768, k ∈ {1, . . . , n}). V anilla Encoder(Q m , K m , V m ) = sof tmax Q m K T m √ d k V m (1) Co-attention Encoder(Q m , K m, V m) = sof tmax Q m K T m √ d k V m<label>(2)</label></formula><p>where m and m denote different modalities, CFP or OCT in this task. d k denotes dimension of self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bilateral Contrastive Alignment</head><p>The BCA module aims to align the extracted features with the self-supervised strategy before interaction. The semantic gap between CFP and OCT is huge, and direct interaction between different modalities without alignment will lead to a mismatch. ALBEF <ref type="bibr" target="#b9">[10]</ref> adopts a similar strategy with the momentum encoders and negative queues, but the weakly-correlated phenomenon and stronglycorrelated negative sample in the medical area are so common that ALBEF can hardly reach convergence. To simplify the proxy task, BCA employs the theory of MoCov3 <ref type="bibr" target="#b4">[5]</ref> and redesigns the "ctr loss" to adapt to multi-modal tasks.</p><p>Considering preserving the stereo information of OCT, each block-level token, T k O blk , is averaged in the token dimension before being projected. To equally align both branches, 4 projectors are followed symmetrically in both branches to map the tokens to contrastive space. Different augmentation will cause huge discrepancies, especially for OCT images. Therefore, as shown in Fig. <ref type="figure" target="#fig_1">2</ref> and Eq. 3 (Bilateral loss), to mitigate the alignment difficulty for multi-modal tasks, we align both modalities by concentrating on the same modality m with different augmentation ū and different modalities m with the same augmentation u.</p><formula xml:id="formula_1">L = m∈{CF P,OCT } 2 u=1 (L inter,m,u + L intra,m,u ); (3) L intra,m,u = I u =ū ctr(preditor m,m,u (q m,u ), k m,ū ); (4) L inter,m,u = I m = mctr(preditor m, m,u (q m,u ), q m,u );<label>(5)</label></formula><p>where q m,u ∈ R dim (dim is 256 by default) denotes modality m with augmentation u in contrastive space with the query encoder. k m,ū denote the vectors from the key encoder. preditor m,m,u denotes the predictor to map modality m to another modality m. ctr() denotes "ctr loss" <ref type="bibr" target="#b4">[5]</ref>. Intuitively, each token behaves like a "query", and BCA aligns the corresponding "values" by projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multiple Instance Learning Representation</head><p>Direct interaction between different modalities with unbalanced amounts is computational-consuming, and the cross-modality relationship is difficult to build. Therefore, we conjecture that the OCT block-level tokens(features) can be formulated as an embedding-level MIL problem in two aspects: <ref type="bibr" target="#b0">(1)</ref> In an OCT volume, only certain salient slices are related to glaucoma. ( <ref type="formula" target="#formula_0">2</ref>) High-level embedding features after the BCA module are more distinguishable. In MILR, by defining the i th OCT block, namely T i O,blk , as an embedding instance, the integral OCT volume is taken as the bag B = T i O,blk |1 ≤ i ≤ n . Then we concatenate the OCT bag, B, with an aggregated tokens T O,agg ∈ R N ×dim . As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, several Vanilla encoders render the interaction among B and T O,agg , and eventually, T i O,agg get semantic information from OCT block instances to form a bag prediction. To ensure that MILR aggregates glaucoma-related instances, a supervised signal is incorporated. For efficiency and effectiveness, we only pass semantic T depth O,agg to the subsequent fusion module. MILR can be formulated as:</p><formula xml:id="formula_2">T i+1 O,agg |B i+1 = V anilla i T i O,agg ⊗B i , i ∈ {1, . . . , depth}<label>(6)</label></formula><p>where depth is the depth of the Vanilla Encoder in MILR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hierarchical Attention Fusion</head><p>Before HAF, each modality interacts within its internal modality. To extract modal-specific features and modal-agnostic features respectively, HAF implements a mid-fusion strategy consisting of two fusion stages, Merged-attention and Cross-attention. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, in the Merged-attention blocks, CFP tokens and T depth O,agg will be concatenated, T all ∈ R 2 * N ×dim , to pass through several Vanilla encoders. Except for interaction within their own modality, the salient area will also engage mildly with the other modality, e.g., CFP will leverage intra-modal (CFP) information and inter-modal (OCT) refined knowledge to reinforce the modal-specific features in CFP. Co-attention encoders in the Crossattention stage render the CFP tokens to interact solely with OCT and thus extract the modal-agnostic features related in both modalities. Eventually, the modal-specific and modal-agnostic features from CFP and OCT will be fed into a projector for joint diagnosis. We will mention the HAF module again in the interpretability experiments in Sect. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>For multi-modal glaucoma recognition, the existing public dataset, GAMMA <ref type="bibr" target="#b18">[19]</ref> on glaucoma grading, includes macular OCT and CFP. GAMMA dataset consists of 100 accessible labeled cases and another 100 unlabeled cases as the benchmark. As the dataset is limited in size for Transformer-based models, we construct a new dataset. 872 multi-modal cases are collected using Topcon Maestro-1 at the outpatient clinic in the Department of Ophthalmology of a state hospital from July 2020 to January 2021. The scan mode is 3D Optic Disc with one CFP image and 128 horizontal OCT B-scan images obtained simultaneously. Due to the expensive human annotations, we acquire pseudo labels of CFP by our advanced ensemble model for training. To build a trust-worthy test set for evaluation, we first split the dataset in train/val/test in 6:2:2. A clinician relabels the test set (172 cases) as GON/normal by considering CFP and OCT thickness map. The performance is evaluated on both private test set and GAMMA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment Details</head><p>Due to the high computational cost, the fixed sampling interval technique is employed to extract 32 OCT images. To avoid over-fitting, we reduce the depths of the encoders to 3 layers. For a fair comparison between all models in this study, we use the following standard setup: initializing with the pre-trained weight of ViT-Base-16 on ImageNet. In the first experiment, the existing multi-modal methods and classical baselines are compared with MM-RAF on the private test set. The baseline includes ResNet, ViT, DeiT <ref type="bibr" target="#b16">[17]</ref>, and Swin-Transformer <ref type="bibr" target="#b11">[12]</ref> (pre-trained weight from timm <ref type="bibr" target="#b17">[18]</ref>) with single-modal or early-fusion multimodal experiments. Multi-modal methods include COROLLA <ref type="bibr" target="#b2">[3]</ref>, MBT <ref type="bibr" target="#b13">[14]</ref>and MM-MIL <ref type="bibr" target="#b10">[11]</ref>. The robustness is evaluated on the GAMMA dataset by comparing it with CNNs. The metrics are averaged over three runs. All experiments are implemented in Python 3.7 and Pytorch 1.7 with four NVIDIA TITAN X GPUs and the training configuration is included in Supplementary Material. <ref type="table" target="#tab_0">1</ref>, ResNet50 for CFP attains the best AP score in single modality, proving that CFP is more sensitive than OCT for glaucoma diagnosis. Besides, the transformer-based method is inferior to ResNet in CFP but surpasses CNN in OCT modality, indicating that the transformer-based methods need sufficient data to learn inductive bias. MM-ViT outperforms CFP-ViT and OCT-ViT, exemplifying that Transformer can benefit from multi-modal learning. Our framework, MM-RAF, outperformed MM-ViT with a 6% improvement in AP score and achieved SOTA with F1, AP, and AUC metrics in this study. Robustness. Due to the limited size of GAMMA(100 cases), our transformerbased method is likely to get overfitting if training from scratch. To this end, pre-training on our private mid-scale dataset which captures optic disc OCT can gain enhancements even when transferring to the cross-domain dataset (GAMMA scans macular area). Cohen's kappa coefficient is implemented as metrics. As shown in Table <ref type="table" target="#tab_1">2</ref>, our method has better cross-domain generalization after applying BCA and transfer learning strategy. Furthermore, MM-RAF achieves results comparable to CNNs, highlighting our framework's robustness to learn inductive biases from images even with a limited dataset. When providing more domain-related data, our approach has the potential to perform marginally better than CNNs for compensating the lack of inductive bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared with Single-Modal and Multi-modal Solutions. As shown in Table</head><p>Ablation Study. The ablation study on the private dataset examines the contribution of three modules, the order of HAF, and the depth of each module. From Table .3, MILR and HAF modules bring 0.03 and 0.02 AP increases, respec-  tively. Reversing the order of the HAF module brings a decrease, which indicates that the modal-agnostic features should be extracted after the Merged-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Visualization</head><p>For visualization, we employ a class-dependent relevance-based method <ref type="bibr" target="#b3">[4]</ref> that captures inter-and intra-modal relevance. Since MILR has aggregated the OCT into high-level T depth O,agg features which are complex to visualize, we choose CFP images to interpret the mechanism of how different modalities interact. For each case presented in Fig. <ref type="figure" target="#fig_2">3</ref>, intra-modal and inter-modal heatmaps are calculated by CFP tokens and OCT tokens, respectively. In intra-modal maps, the salient area is centralized on the optic disc, while the inter-modal maps are sensitive to the temporal region where OCT can provide fine-grain stereo information of the optic nerve, indicating that our framework incorporates multi-modal information. The sparsely distributed situation, e.g., case(a) GON's inter-modal view, may be attributed to the lack of significant lesions in the image, causing a random selection of tokens. Also, we visualize how the framework considers the correct prediction with the network going deeper. In case (b), deeper layers concentrate intra-and inter-modal features on lesion areas. The intra-modal Mergedattention focuses on the optic disc(modal-specific feature), and the inter-modal Cross-attention maps are more sensitive to the temporal region (modal-agnostic feature), demonstrating the effectiveness of HAF in extracting and combining modal-agnostic and modal-specific features to make the multi-modal decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The challenges in multi-modal glaucoma recognition include the huge discrepancies, the unbalanced amounts, and the lack of spatial information interaction between different modalities. To this end, we propose MM-RAF, a pure selfattention multi-modal framework consisting of three modules dedicated to the problems: BCA fills the semantic gap between CFP and OCT and promotes robustness. MILR and HAF complete semantic aggregation and comprehensive relationship probing with better performance. While MM-RAF outperforms other solutions in multi-modal glaucoma recognition, the performance can be further improved with sufficient data. Our next direction is to utilize a lightweight transformer to leverage more information from both modalities. Besides, addressing the issue of uncertainty measurement and preventing the bias of any specific modality from influencing the overall decision in the multi-modal recognition scenario is crucial, especially when diagnosing glaucoma using OCT for its limited specificity. Cross-modal uncertainty measurement is also our further research direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Some Lesions of glaucoma in clinical examination. Lesions in the blue box appear in CFP. Yellow for OCT. (Color figure online)</figDesc><graphic coords="2,78,96,54,08,294,10,114,61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our proposed MM-RAF framework.</figDesc><graphic coords="3,41,79,53,99,340,33,146,71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization. Case (a): Normal; Case (b): GON (Glaucomatous optic neuropathy) with RNFLD. Each column denotes a different class decision or a different stage in our framework. "Merged" denotes Merged-attention, "Cross" denotes Cross-attention. It is recommended to zoom in to view this figure.</figDesc><graphic coords="8,55,47,197,66,341,41,110,05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results of baseline and existing works comparison in the private test set. In each column, bold text denotes the best results.</figDesc><table><row><cell>lMethod</cell><cell>Metrics</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Sen</cell><cell>Spe</cell><cell>F1</cell><cell>AP</cell><cell>AUC</cell></row><row><cell>Single-modal CFP-ResNet50</cell><cell cols="5">0.9333 0.8078 0.8515 0.8925 0.9584</cell></row><row><cell>CFP-ViT</cell><cell cols="5">0.6286 0.9756 0.7565 0.8483 0.9585</cell></row><row><cell>CFP-DeiT</cell><cell cols="5">0.7348 0.9512 0.8291 0.8531 0.9512</cell></row><row><cell cols="3">CFP-Swin-Transformer 0.6335 0.892</cell><cell cols="3">0.7729 0.8212 0.9241</cell></row><row><cell>OCT-ResNet50</cell><cell cols="5">0.4571 0.9854 0.5994 0.8605 0.9363</cell></row><row><cell>OCT-ViT</cell><cell cols="5">0.7333 0.9854 0.8397 0.8892 0.944</cell></row><row><cell>OCT-DeiT</cell><cell cols="5">0.6286 0.9756 0.7565 0.8897 0.9585</cell></row><row><cell cols="6">OCT-Swin-Transformer 0.6111 0.9281 0.7309 0.7925 0.911</cell></row><row><cell>Multi-modal MM-ViT</cell><cell cols="5">0.8334 0.8873 0.8579 0.8983 0.934</cell></row><row><cell>MM-DeiT</cell><cell cols="5">0.7904 0.9513 0.8629 0.8982 0.9514</cell></row><row><cell cols="6">MM-Swin-Transformer 0.6111 0.7961 0.6853 0.5771 0.7639</cell></row><row><cell>MM-MIL [11]</cell><cell>0.781</cell><cell cols="4">0.9416 0.8453 0.8837 0.9608</cell></row><row><cell>MBT [14]</cell><cell cols="5">0.6667 0.9640 0.7859 0.8287 0.9384</cell></row><row><cell>COROLLA [3]</cell><cell cols="5">0.6667 0.9376 0.7735 0.8318 0.942</cell></row><row><cell>MM-RAF</cell><cell cols="5">0.9238 0.9027 0.9081 0.9584 0.9855</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Robustness experiments on GAMMA dataset. The glaucoma grading task (normal/early-glaucoma/progressive-glaucoma) is evaluated by kappa.</figDesc><table><row><cell>Method</cell><cell>kappa</cell></row><row><cell>Dual-ResNet50</cell><cell>0.7352</cell></row><row><cell>MM-MIL w/o transfer learning</cell><cell>0.8502</cell></row><row><cell>MM-MIL w/ transfer learning</cell><cell>0.8562</cell></row><row><cell>Ours w/o transfer learning, BCA</cell><cell>0.5289</cell></row><row><cell cols="2">Ours w/ transfer learning; w/o BCA 0.6277</cell></row><row><cell cols="2">Ours w/ BCA; w/o transfer learning 0.8072</cell></row><row><cell>Ours w/</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>transfer learning, BCA 0.8467</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on private dataset. HAF-R denotes to reverse the order of two stages in HAF modules. Depth controls the encoder depth in all modules.</figDesc><table><row><cell cols="2">BCA MILR HAF-Merged HAF-Cross HAF-R depth Sen</cell><cell cols="2">Spe F1</cell><cell>AP</cell><cell>AUC</cell></row><row><cell>3</cell><cell cols="4">0.7714 0.8759 0.8195 0.8348 0.9311</cell></row><row><cell>3</cell><cell cols="4">0.8476 0.9537 0.8938 0.9275 0.9718</cell></row><row><cell>3</cell><cell cols="4">0.8473 0.9803 0.9074 0.9554 0.9862</cell></row><row><cell>3</cell><cell cols="2">0.6574 0.904</cell><cell cols="2">0.7147 0.8261 0.9401</cell></row><row><cell>1</cell><cell cols="4">0.8381 0.9538 0.8906 0.9187 0.9678</cell></row><row><cell>6</cell><cell cols="4">0.8095 0.9732 0.8803 0.938</cell><cell>0.9793</cell></row><row><cell>3</cell><cell cols="4">0.9238 0.9027 0.9081 0.9584 0.9855</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 66.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Glaucoma diagnosis with machine learning based on optical coherence tomography and color fundus images</title>
		<author>
			<persName><forename type="first">G</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Healthcare Eng</title>
		<imprint>
			<biblScope unit="page">2019</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using deep learning and transfer learning to accurately diagnose early-onset glaucoma from macular optical coherence tomography images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Asaoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="page" from="136" to="145" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Corolla: an efficient multi-modality fusion framework with supervised contrastive learning for glaucoma grading</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9640" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Retinal nerve fiber layer defect detection with position guidance</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_72</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59722-172" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="745" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The isnt rule and differentiation of normal from glaucomatous eyes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Harizman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1579" to="1583" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diagnosing glaucoma with spectraldomain optical coherence tomography using deep learning classifier</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Jeoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Glaucoma</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="287" to="294" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Align before fuse: vision and language representation learning with momentum distillation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9694" to="9705" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-modal multi-instance learning for retinal disease recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2474" to="2482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated detection of glaucoma with interpretable machine learning using clinical data and multimodal retinal images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="page" from="154" to="169" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention bottlenecks for multimodal fusion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="14200" to="14213" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Novel expert system for glaucoma identification using non-parametric spatial envelope energy spectrum with fundus images</title>
		<author>
			<persName><forename type="first">U</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Bhandary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gudigar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biocybernetics Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="170" to="180" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detection of glaucomatous optic neuropathy with spectraldomain optical coherence tomography: a retrospective training and validation deeplearning analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Ran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="172" to="e182" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers; distillation through attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021-07">July 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4414861" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Gamma challenge: Glaucoma grAding from Multi-Modality imAges</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
