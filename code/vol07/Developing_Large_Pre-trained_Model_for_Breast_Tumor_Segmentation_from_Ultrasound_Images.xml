<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images</title>
				<funder ref="#_XzA3AsP">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
					<orgName type="abbreviated">STCSM</orgName>
				</funder>
				<funder ref="#_9NWg4gH">
					<orgName type="full">The Key R&amp;D Program of Guangdong Province, China</orgName>
				</funder>
				<funder ref="#_EkWKnY9">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Meiyu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaicong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuning</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiqun</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Yunnan Cancer Hospital</orgName>
								<address>
									<settlement>Kunming</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<email>dgshen@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="89" to="96"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">AF71A73723F5C658E54B03B41BA877CD</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Large-scale clinical dataset</term>
					<term>Deep-supervision</term>
					<term>Multi-scale segmentation</term>
					<term>Breast ultrasound images Registration Number: 4319</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Early detection and diagnosis of breast cancer using ultrasound images are crucial for timely diagnostic decision and treatment in clinical application. However, the similarity between tumors and background and also severe shadow noises in ultrasound images make accurate segmentation of breast tumor challenging. In this paper, we propose a large pre-trained model for breast tumor segmentation, with robust performance when applied to new datasets. Specifically, our model is built upon UNet backbone with deep supervision for each stage of the decoder. Besides using Dice score, we also design discriminator-based loss on each stage of the decoder to penalize the distribution dissimilarity from multiscales. Our proposed model is validated on a large clinical dataset with more than 10000 cases, and shows significant improvement than other representative models. Besides, we apply our large pretrained model to two public datasets without fine tuning, and obtain extremely good results. This indicates great generalizability of our large pre-trained model, as well as robustness to multi-site data. The code is publicly available at https://github.com/limy-ulab/US-SEG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Breast cancer is a serious health problem with high incidence and wide prevalence for women throughout the world <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Regular screening and early detection are crucial for effective diagnosis and treatment, and hence for improved prognosis and survival rate <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Clinical researches have shown that ultrasound imaging is an effective tool for screening breast cancer, due to its critical characteristics of non-invasiveness, non-radiation and inexpensiveness <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. In clinical diagnosis, delineating tumor regions from background is a crucial step for quantitative analysis <ref type="bibr" target="#b7">[8]</ref>. Manual delineation always depends on the experience of radiologists, which tends to be subjective and time-consuming <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Therefore, there is a high demand for automatic and robust methods to achieve accurate breast tumor segmentation. However, due to speckle noise and shadows in ultrasound images, breast tumor boundaries tend to be blurry and are difficult to be distinguished from background. Furthermore, the boundary and size of breast tumors are always variable and irregular <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. These issues pose challenges and difficulties for accurate breast tumor segmentation in ultrasound images.</p><p>Various approaches based on deep learning have been developed for tumor segmentation with promising results <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. Su et al. <ref type="bibr" target="#b12">[13]</ref> designed a multi-scale U-Net to extract more semantic and diverse features for medical image segmentation, using multiple convolution sequences and convolution kernels with different receptive fields. Zhou et al. <ref type="bibr" target="#b13">[14]</ref> raised a deeply-supervised encoder-decoder network, which is connected through a series of nested and dense skip pathways to reduce semantic gap between feature maps. In <ref type="bibr" target="#b14">[15]</ref>, a multi-scale selection and multi-channel fusion segmentation model was built, which gathers global information from multiple receptive fields and integrates multi-level features from different network positions for accurate pancreas segmentation. Oktay et al. <ref type="bibr" target="#b15">[16]</ref> proposed an attention gate model, which is capable of suppressing irrelevant regions while highlighting useful features for a specific task. Huang et al. <ref type="bibr" target="#b16">[17]</ref> introduced a UNet 3+ for medical image segmentation, which incorporates low-level and high-level feature maps in different scales and learns full-scale aggregated feature representations. Liu et al. <ref type="bibr" target="#b17">[18]</ref> established a convolution neural network optimized by super-pixel and support vector machine, segmenting multiple organs from CT scans to assist physicians diagnosis. Pei et al. <ref type="bibr" target="#b18">[19]</ref> introduced channel and position attention module into deep learning neural network to obtain contextual information for colorectal tumors segmentation in CT scans. However, although these proposed models have achieved satisfactory results in different medical segmentation tasks, their performances are limited for breast tumor segmentation in ultrasound images due to the low image contrast and blurry tissue boundary.</p><p>To address these challenges, we present, to the best of our knowledge, the first work to adopt multi-scale features collected from large set of clinical ultrasound images for breast tumor segmentation. The main contributions of our work are as follows: <ref type="bibr" target="#b0">(1)</ref> we propose a well-pruned simple but effective network for breast tumor segmentation, which shows remarkable and solid performance on large clinical dataset; (2) our large pretrained model is evaluated on two additional public datasets without fine-tuning and shows extremely stabilized improvement, indicating that our model has outstanding generalizability and good robustness against multi-site data data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We demonstrate the architecture of our proposed network in Fig. <ref type="figure" target="#fig_0">1</ref>. It is based on the UNet <ref type="bibr" target="#b19">[20]</ref> backbone. In order to collect multi-scale rich information for tumor tissues, we propose to use GAN <ref type="bibr" target="#b20">[21]</ref>-based multi-scale deep supervision. In particular, we apply similarity constraint for each stage of the UNet decoder to obtain consistent and stable segmentation maps. Instead of using Dice score in the final layer of UNet, we also use Dice loss on each of the decoder stages. Besides, we integrate an adversarial loss as additional constraint to penalize the distribution dissimilarity between the predicted segmentation map and the ground truth. In the framework of GAN, we take our segmentation network as the generator and a convolutional neural network as the discriminator. The discriminator consists of five convolution layers with the kernel sizes of 7 × 7, 5 × 5, 4 × 4, 4 × 4 and 4 × 4. Therefore, we formulate the overall loss for the generator, namely the segmentation network, as</p><formula xml:id="formula_0">L overall = 4 n=1 α n • (1 -2 P (n) ∩ G • P (n) + |G| -1 ) + β n • E P (n) ∼P SN (proba) log(1 -CN θ n CN (SN θ SN (P (n) ))) (1)</formula><p>where SN represents the segmentation network and CN represents the involved convolutional network. θ SN and θ CN refer to the parameters in the segmentation and convolutional network, respectively. P (n) represents the segmentation maps obtained from the n-th stage in the segmentation network, and G refers to the corresponding ground truth. p SN (proba) denotes the distribution of probability maps. CN θCN (SN θSN (P (2) )) represents the probability for the input of CN coming from the predicted maps rather than the real ones. The parameters α 1 is empirically set as 1, α 2 , α 3 , α 4 are set as 0.1, and β 1 , β 2 , β 3 and β 4 are set as 0.05. It should be noted that, in UNet, there are 4 stages and hence we employ 4 CNNs for each of them without sharing their weights. Meanwhile, the adversarial loss for each of the CNN is defined as:</p><formula xml:id="formula_1">L CN G, P (2) = -E G∼p CN (truth) log(CN θ CN (G)) -E P (2) ∼p SN (proba) log(1 -CN θ CN (SN θ SN (P (2) )))<label>(2)</label></formula><p>where p CN (truth) denotes the distribution of original samples. CN θCN (G) represents the probability for the input of CN coming from the original dataset.</p><p>In the implementation, we update the segmentation network and all the discriminators alternatingly in each iteration until both the generator and discriminators are converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>We collected 10927 cases for this research from Yunnan Cancer Hospital. Each scan is with resolution of 1 × 1 mm 2 and size of 512 × 480. The breast tumors of each case are delineated by three experienced experts. Five-fold cross validation is performed on the dataset in all experiments to verify our proposed network. For external validation, we further test our model on two independent publicly-available datasets collected by STU-Hospital (Dataset 1) <ref type="bibr" target="#b21">[22]</ref> and SYU-University (Dataset 2) <ref type="bibr" target="#b22">[23]</ref>. In order to comprehensively evaluate segmentation efficiency of our model, Dice Similarity Coefficient (DSC), Precision, Recall, Jaccard, and Root Mean Squared Error (RMSE) are used as evaluation metrics in this work. Our proposed algorithm is conducted on PyTorch, and all experiments are performed on NVIDIA Tesla A100 GPU. We use Adam optimizer to train the framework with an initial learning rate of 10 -4 . The epochs to train all models are 100 and the batch size in training process is set as 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with State-of-the-Art Methods</head><p>To verify the advantages of our proposed model for breast tumor segmentation in ultrasound images, we compare our deep-supervised convolutional network with the state-ofthe-art tumor segmentation methods, including DeepRes <ref type="bibr" target="#b23">[24]</ref>, MSUNet <ref type="bibr" target="#b12">[13]</ref>, UNet++ <ref type="bibr" target="#b13">[14]</ref>, SegNet <ref type="bibr" target="#b24">[25]</ref>, AttU-Net <ref type="bibr" target="#b15">[16]</ref>, U 2 -Net <ref type="bibr" target="#b25">[26]</ref> and UNet 3+ <ref type="bibr" target="#b16">[17]</ref>. The comparison experiments are carried on a large-scale clinical breast ultrasound dataset, and the quantitative results are reported in Table <ref type="table" target="#tab_0">1</ref>. It is obvious that our proposed model achieves the optimal performance compared with other segmentation models. For example, our method obtains a DSC score of 80.97%, which is 5.81%, 9.13%, 7.77%, 4.13%, 7.51%, 2.19%, and 3.83% higher than other seven models. These results indicate the effectiveness of the proposed model in delineating breast tumors in ultrasound images.</p><p>Representative segmentation results using different methods are provided in Fig. <ref type="figure" target="#fig_1">2</ref>. The probability maps predicted by our model are more consistent with the ground truth, especially in the tiny structures which are difficult to capture. This verifies the superior ability of the proposed model in maintaining detailed edge information compared with state-of-the-art methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Studies</head><p>We demonstrate the efficacy of the deep supervision strategy using ablation studies. Four groups of frameworks (Stage I, Stage II, Stage III and Stage IV) are designed, with the numerals denoting the level of deep supervision counting from the last deconvolutional layer.</p><p>We test these four frameworks on the in-house breast ultrasound dataset, and verify their segmentation performance using the same five evaluation criteria. The evaluation metrics from all cases are presented by the scatter plots in Fig. <ref type="figure" target="#fig_2">3</ref>. The obtained quantitative results are shown in Table <ref type="table" target="#tab_1">2</ref>, where Stage IV model achieves the optimal DSC, Precision, Recall, and Jaccard. All these results draw a unanimous conclusion on the relationship between these four frameworks. That is, the segmentation ability of the proposed Stage IV is ameliorated from every possible perspective. Moreover, Stage IV obtains minimal RMSE compared with other three models (0.68 mm vs 0.84 mm, 0.82 mm, 0.75 mm), which means better matching of the predicted maps from Stage IV with the corresponding ground truth. All these comparison results verify the superiority of deep supervision for breast tumor segmentation in ultrasound images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we have developed a large pre-trained model for breast tumor segmentation from ultrasound images. In particular, two constraints are proposed to exploit both image similarity and space correlation information for refining the prediction maps. Moreover, our proposed deep supervision strategy is used for quality control at each decoding stage, optimizing prediction maps layer-by-layer for overall performance improvement. Using a large clinical dataset, our proposed model demonstrates not only state-of-the-art segmentation performance, but also the outstanding generalizability to new ultrasound data from different sites. Besides, our large pre-trained model is general and robust in handling various tumor types and shadow noises in our acquired clinical ultrasound images. This also shows the potential of directly applying our model in real clinical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Overview of the proposed architecture. The first constraint is used to enhance prediction similarity to the standard ones, for preliminary segmentation. The second constraint is used to capture data distribution to maintain consistency in high-dimensional space for map refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Segmentation results of five subjects obtained from different models. Red boxes are used to highlight the boundaries which are difficult to segment.</figDesc><graphic coords="5,55,98,217,82,340,75,140,14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Five evaluation criteria for total cases from different frameworks: Stage I, Stage II, Stage III, and Stage IV (From left to right in each index).</figDesc><graphic coords="5,57,48,416,39,337,33,72,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison with state-of-the-art segmentation methods on large-scale clinical breast ultrasound dataset.</figDesc><table><row><cell>Method</cell><cell>DSC (%)↑</cell><cell cols="2">Precision (%)↑ Recall (%)↑</cell><cell>Jaccard (%)↑ RMSE (mm)↓</cell></row><row><cell>DeepRes</cell><cell cols="2">75.16 ± 1.13 80.04 ± 1.33</cell><cell cols="2">73.52 ± 2.18 61.24 ± 1.53 0.81 ± 0.08</cell></row><row><cell>[24]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MSUNet</cell><cell cols="2">71.84 ± 1.19 67.32 ± 2.04</cell><cell cols="2">81.67 ± 1.85 57.10 ± 1.54 0.82 ± 0.10</cell></row><row><cell>[13]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UNet++</cell><cell cols="2">73.20 ± 0.96 75.98 ± 1.42</cell><cell cols="2">73.58 ± 2.00 58.59 ± 1.27 0.84 ± 0.09</cell></row><row><cell>[14]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SegNet</cell><cell cols="2">76.84 ± 0.87 79.08 ± 1.33</cell><cell cols="2">77.29 ± 1.73 63.23 ± 1.27 0.77 ± 0.08</cell></row><row><cell>[25]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AttU-Net</cell><cell cols="2">73.46 ± 0.98 80.81 ± 1.33</cell><cell cols="2">69.93 ± 1.95 58.94 ± 1.31 0.86 ± 0.09</cell></row><row><cell>[16]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>U 2 -Net</cell><cell cols="2">78.78 ± 0.91 75.90 ± 1.52</cell><cell cols="2">84.58 ± 1.43 65.91 ± 1.39 0.72 ± 0.08</cell></row><row><cell>[26]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UNet 3+</cell><cell cols="2">77.14 ± 0.86 79.45 ± 1.20</cell><cell cols="2">77.32 ± 1.72 63.61 ± 1.25 0.77 ± 0.08</cell></row><row><cell>[17]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell cols="2">80.97 ± 0.88 81.64 ± 1.30</cell><cell cols="2">82.19 ± 1.61 68.83 ± 1.38 0.68 ± 0.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results among four groups of segmentation frameworks Stage I, Stage II, Stage III, and Stage IV.</figDesc><table><row><cell>Method</cell><cell>DSC (%)↑</cell><cell cols="2">Precision (%)↑ Recall (%)↑</cell><cell>Jaccard (%)↑ RMSE (mm)↓</cell></row><row><cell>Stage I</cell><cell cols="2">70.52 ± 1.25 70.01 ± 1.77</cell><cell cols="2">76.16 ± 2.04 55.72 ± 1.53 0.84 ± 0.09</cell></row><row><cell>Stage II</cell><cell cols="2">73.43 ± 1.07 76.10 ± 1.30</cell><cell cols="2">74.19 ± 2.30 58.99 ± 1.37 0.82 ± 0.08</cell></row><row><cell cols="3">Stage III 77.67 ± 1.06 79.63 ± 1.43</cell><cell cols="2">78.17 ± 1.98 64.42 ± 1.47 0.75 ± 0.07</cell></row><row><cell cols="2">Stage IV 80.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>97 ± 0.88 81.64 ± 1.30 82.19 ± 1.61 68.83 ± 1.38 0.68 ± 0.06 3.4 Performance on Two External Public Datasets In</head><label></label><figDesc>order to evaluate the generalizability of the proposed model, we introduce external Dataset 1 and Dataset 2 for external validation experiments. Specifically, Dataset 1 and Dataset 2 are used as testing data to evaluate the generalization performance of the models trained on our own dataset without fine tuning, and the corresponding results are shown in Table3. Promising performance demonstrates outstanding generalization ability of our large pre-trained model, with a DSC score of 81.35% and a Recall of 80.96% on Dataset 1, and a DSC score of 77.16% and a Recall of 93.22% on Dataset 2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>External validation performance on two independent publicly-available Dataset 1 and Dataset 2. Dataset 1 81.35 ± 2.04 85.75 ± 2.97 80.96 ± 2.09 70.65 ± 3.07 0.19 ± 0.01 Dataset 2 77.16 ± 2.51 68.34 ± 4.03 93.22 ± 0.65 65.26 ± 3.67 0.42 ± 0.01</figDesc><table><row><cell>Dataset</cell><cell>DSC (%)</cell><cell>Precision (%) Recall (%)</cell><cell>Jaccard (%)</cell><cell>RMSE (mm)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported in part by <rs type="funder">The Key R&amp;D Program of Guangdong Province, China</rs> (grant number <rs type="grantNumber">2021B0101420006</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (grant number <rs type="grantNumber">62131015</rs>), and <rs type="funder">Science and Technology Commission of Shanghai Municipality (STCSM)</rs> (grant number <rs type="grantNumber">21010502600</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9NWg4gH">
					<idno type="grant-number">2021B0101420006</idno>
				</org>
				<org type="funding" xml:id="_EkWKnY9">
					<idno type="grant-number">62131015</idno>
				</org>
				<org type="funding" xml:id="_XzA3AsP">
					<idno type="grant-number">21010502600</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jemal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cancer statistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="7" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Current and future burden of breast cancer: global statistics for 2020 and 2040</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rumgay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mafra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laversanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Breast</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="15" to="23" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detection of breast cancer with addition of annual screening ultrasound or a single screening MRI to mammography in women with elevated breast cancer risk</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lehrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Pisano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Barr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Med. Assoc</title>
		<imprint>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1394" to="1404" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>JAMA)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved breast cancer survival following introduction of an organized mammography screening program among both screened and unscreened women: a population-based cohort study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Haldorsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bretthauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Thoresen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Adami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Breast Cancer Res. BCR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated breast cancer detection and classification using ultrasound images: a survey</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="299" to="317" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ultrasound and photoacoustic imaging of breast cancer: clinical systems, challenges, and future outlook</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kratkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pattyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alijabbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehrmohammadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Med</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1165</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ensemble deep-learning-enabled clinical decision support system for breast cancer diagnosis and classification on ultrasound images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ragab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albukhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alyami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biology (Basel)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">439</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic breast ultrasound image segmentation: a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="340" to="355" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Intraobserver interpretation of breast ultrasonography following the BI-RADS classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J G</forename><surname>Calas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M V R</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gutfilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C A</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Radiol</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="525" to="528" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Processed images in human perception: a case study in ultrasound breast imaging</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Edirisinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Bez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Radiol</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="682" to="687" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Computer-aided detection/diagnosis of breast cancer in mammography and ultrasound: a review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jalalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Mashohor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I B</forename><surname>Saripan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R B</forename><surname>Ramli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karasfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="420" to="426" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-model attention-guided tumor segmentation for 3D automated breast ultrasound (ABUS) images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="301" to="311" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Msu-net: Multi-scale u-net for 2D medical image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Genet</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">639930</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unet++: a nested U-net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-5_1" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-scale selection and multi-channel fusion model for pancreas segmentation using adversarial deep convolutional nets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="47" to="55" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno>arXiv:180403999</idno>
		<title level="m">Attention U-net: learning where to look for the pancreas</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unet 3+: a full-scale connected unet for medical image segmentation</title>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic organ segmentation for CT scans based on super-pixel and convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="748" to="760" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Colorectal tumor segmentation of CT scans based on a convolutional neural network with an attention mechanism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64131" to="64138" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Montreal, Canada; Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">RDA-UNET-WGAN: an accurate breast ultrasound lesion segmentation using Wasserstein generative adversarial networks. Arab</title>
		<author>
			<persName><forename type="first">A</forename><surname>Negi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N J</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nersisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murugappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="6399" to="6410" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Segmentation of breast ultrasound image with semantic classification of superpixels</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101657</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual u-net</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Segnet: a deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">U2-Net: going deeper with nested U-structure for salient object detection. Pattern Recogn</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaiane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Clinical Applications -Cardiac</publisher>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">107404</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
