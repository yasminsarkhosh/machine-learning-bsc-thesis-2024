<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning</title>
				<funder ref="#_96Zehaz">
					<orgName type="full">National Institute of Biomedical Imaging and Bioengineering of the National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_ehfz7eU">
					<orgName type="full">Advanced Cyberinfrastructure Coordination Ecosystem</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yimu</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tongan</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manas</forename><surname>Mehta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alison</forename><forename type="middle">D</forename><surname>Gernand</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeffery</forename><forename type="middle">A</forename><surname>Goldstein</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leena</forename><surname>Mithal</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Lurie Children&apos;s Hospital</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Delia</forename><surname>Mwinyelle</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The University of Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kelly</forename><surname>Gallagher</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">College of Information Sciences and Tech-nology of The</orgName>
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="116" to="126"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">09B8DA4F51FBFE00F0FFFC0DD33B05EF</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_12</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Placenta Analysis</term>
					<term>Representation</term>
					<term>Vision-Language</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The placenta is a valuable organ that can aid in understanding adverse events during pregnancy and predicting issues postbirth. Manual pathological examination and report generation, however, are laborious and resource-intensive. Limitations in diagnostic accuracy and model efficiency have impeded previous attempts to automate placenta analysis. This study presents a novel framework for the automatic analysis of placenta images that aims to improve accuracy and efficiency. Building on previous vision-language contrastive learning (VLC) methods, we propose two enhancements, namely Pathology Report Feature Recomposition and Distributional Feature Recomposition, which increase representation robustness and mitigate feature suppression. In addition, we employ efficient neural networks as image encoders to achieve model compression and inference acceleration. Experiments validate that the proposed approach outperforms prior work in both performance and efficiency by significant margins. The benefits of our method,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>including enhanced efficacy and deployability, may have significant implications for reproductive healthcare, particularly in rural areas or low-and middle-income countries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>World Bank data from 2020 suggests that while the infant mortality rate in high-income countries is as low as 0.4%, the number is over ten times higher in low-income countries (approximately 4.7%). This stark contrast underlines the necessity for accessible healthcare. The placenta, as a vital organ connecting the fetus to the mother, has discernable features such as meconium staining, infections, and inflammation. These can serve as indicators of adverse pregnancy outcomes, including preterm delivery, growth restriction, respiratory or neurodevelopmental conditions, and even neonatal deaths <ref type="bibr" target="#b9">[9]</ref>.</p><p>In a clinical context, these adverse outcomes are often signaled by morphological changes in the placenta, identifiable through pathological analysis <ref type="bibr" target="#b19">[19]</ref>. Timely conducted placental pathology can reduce the risks of serious consequences of pregnancy-related infections and distress, ultimately improving the well-being of newborns and their families. Unfortunately, traditional placenta pathology examination is resource-intensive, requiring specialized equipment and expertise. It is also a time-consuming task, where a full exam can easily take several days, limiting its widespread applications even in developed countries. To overcome these challenges, researchers have been exploring the use of automatic placenta analysis tools that rely on photographic images. By enabling broader and more timely placental analysis, these tools could help reduce infant fatalities and improve the quality of life for families with newborns. <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b23">23]</ref> and classifying <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b26">26]</ref> placenta images using histopathological, ultrasound, or MRI data. However, these methods are dependent on expensive and bulky equipment, restricting the accessibility of reproductive healthcare. Only limited research has been conducted on the gross analysis of post-birth placenta photographs, which have a lower equipment barrier. AI-PLAX <ref type="bibr" target="#b4">[4]</ref> combines handcrafted features and deep learning, and a more recent study <ref type="bibr" target="#b29">[29]</ref> relies on deep learning and domain adaptation. Unfortunately, both are constrained by issues such as data scarcity and single modality, which hinder their robustness and generalizability. To address these, Pan et al. <ref type="bibr" target="#b16">[16]</ref> incorporated vision-andlanguage contrastive learning (VLC) using pathology reports. However, their method struggles with variable-length reports and is computationally demanding, making it impractical for low-resource communities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work. Considerable progress has been made in segmenting</head><p>With growing research in vision-and-language and contrastive learning <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b28">28]</ref>, recent research has focused on improving the performance and efficiency of VLC approaches. They propose new model architectures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">24]</ref>, better visual representation <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b27">27]</ref>, loss function design <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b16">16]</ref>, or sampling strategies <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b12">12]</ref>.</p><p>However, these methods are still not suitable for variable-length reports and are inefficient in low-resource settings.</p><p>Our Contributions. We propose a novel framework for more accurate and efficient computer-aided placenta analysis. Our framework introduces two key enhancements: Pathology Report Feature Recomposition, a first in the medical VLC domain that captures features from pathology reports of variable lengths, and Distributional Feature Recomposition, which provides a more robust, distribution-aware representation. We demonstrate that our approach improves representational power and surpasses previous methods by a significant performance margin, without additional data. Furthermore, we boost training and testing efficiency by eliminating the large language model (LLM) from the training process and incorporating more efficient encoders. To the best of our knowledge, this is the first study to improve both the efficiency and performance of VLC training techniques for placenta analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>We use the exact dataset from Pan et al. <ref type="bibr" target="#b16">[16]</ref> collected using a professional photography instrument in the pathology department of the Northwestern Memorial Hospital (Chicago) from 2014 to 2018 and an iPad in 2021. There are three parts of the dataset: 1) the pre-training dataset, containing 10,193 image-and-text pairs; 2) the primary fine-tuning dataset, comprising 2,811 images labeled for five tasks: meconium, fetal inflammatory response (FIR), maternal inflammatory response (MIR), and histologic chorioamnionitis, and neonatal sepsis; and 3) the iPad evaluation dataset, consisting of 52 images from an iPad labeled for MIR and clinical chorioamnionitis. As with the original study, we assess the effectiveness of our method on the primary dataset, while utilizing iPad images to evaluate the robustness against distribution shifts. All images contain the fetal side of a placenta, the cord, and a ruler for scale. The pre-training data is also accompanied by a corresponding text sequence for the image containing a part of the corresponding pathology report as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. A detailed breakdown of the images is provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>This section aims to provide an introduction to the background, intuition, and specifics of the proposed methods. An overview is given in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Our tasks are to train an encoder to produce placenta features and a classifier to classify them. Formally, we aim to learn a function f v using a learned function f u , such that for any pair of input (x i , t i ) and a similarity function sim, we have where sim(u, v) represents the cosine similarity between the two feature vectors u = f u (x), v = f v (t). The objective function for achieving inequality (1) is:</p><formula xml:id="formula_0">sim(ui, vi) &gt; sim(ui, vj), i = j ,<label>(1)</label></formula><formula xml:id="formula_1">(v→u) i = -log exp(sim(ui, vi)/τ ) N k=1 exp(sim(ui, v k )/τ ) , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where τ is the temperature hyper-parameter and N is the mini-batch size.</p><p>To train a classifier, we aim to learn a function f c t using the learned function</p><formula xml:id="formula_3">f v for each task t ∈ [1 : T ], such that for a pair of input (x i , l t i ), f c t (f v (x i )) = l t i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pathology Report Feature Recomposition</head><p>Traditional VLC approaches for medical image and text analysis, such as Con-VIRT <ref type="bibr" target="#b28">[28]</ref>, encode the entire natural language medical report or electronic health record (EHR) associated with each patient into a single vector representation using a language model. However, solely relying on a pre-trained language model presents two significant challenges. First, the encoding process can result in suppression of important features in the report as the encoder is allowed to ignore certain placental features to minimize loss, leading to a single dominant feature influencing the objective (1), rather than the consideration of all relevant features in the report. Second, the length of the pathology report may exceed the capacity of the text encoder, causing truncation (e.g., a BERT <ref type="bibr" target="#b6">[6]</ref> usually allows 512 sub-word tokens during training). Moreover, recent LLMs may handle text length but not feature suppression. Our method seeks to address both challenges simultaneously.</p><p>Our approach addresses the limitations of traditional VLC methods in the medical domain by first decomposing the placenta pathology report into set T of arbitrary size, where each t i ∈ T represents a distinct placental feature; the individual items depicted in the pathology report in Fig. <ref type="figure" target="#fig_0">1</ref> correspond to distinct placental features. Since the order of items in a pathology report does not impact its integrity, we obtain the set of vector representations of the features V using an expert language model f v , where v i = f v (t i ) for v i ∈ V. These resulting vectors are weighted equally to recompose the global representation (see Fig. <ref type="figure" target="#fig_0">1</ref>), v = v∈V v, which is subsequently used to calculate the cosine similarity sim(u, v) with the image representation u. The recomposition of feature vectors from full medical text enables the use of pathology reports or EHRs of any length and ensures that all placental features are captured and equally weighted, thereby improving feature representation. Additionally, our approach reduces computational resources by precomputing text features, eliminating the need for an LLM in training. Moreover, it is adaptable to any language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distributional Feature Recomposition</head><p>Since our pathology reports are decomposed and encoded as a set of feature vectors, to ensure an accurate representation, it is necessary to consider potential limitations associated with vector operations. In the context of vector summation, we anticipate similar representations when two sets differ only slightly. However, even minor changes in individual features within the set can significantly alter the overall representation. This is evident in the substantial difference between v1 and v2 in Fig. <ref type="figure" target="#fig_1">2</ref>, despite V 1 and V 2 differing by only one vector magnitude. On the other hand, two distinct sets may result in the same representation, as shown by v1 and v3 in Fig. <ref type="figure" target="#fig_1">2</ref>, even when the individual feature vectors have drastically different meanings. Consequently, it is crucial to develop a method that ensures sim( To address these limitations, we extend the feature recomposition in Sect. 3.2 to Distributional Feature Recomposition that estimates a stable high-dimensional vector space defined by each set of features. We suggest utilizing the distribution N (μ(V), σ(V)) of the feature vectors V, instead of point estimates (single vector sum) as a more comprehensive representation, where μ(V) and σ(V) denote the mean and standard deviation, respectively. As shown by the shaded area in Fig. <ref type="figure" target="#fig_1">2</ref>, the proposed distributional feature recomposition is more stable and representative than the point estimate sum of vector:</p><formula xml:id="formula_4">V 1 , V 2 ) &gt; sim(V 1 , V 3 ).</formula><formula xml:id="formula_5">N (μ(V 1 ), σ(V 1 )) is similar to N (μ(V 2 ), σ(V 2 )), but significantly different from N (μ(V 3 ), σ(V 3 )).</formula><p>Implementation-wise, we employ bootstrapping to estimate the distribution of the mean vector. We assume that the vectors adhere to a normal distribution with zero covariance between dimensions. During each training iteration, we randomly generate a new bootstrapped sample set Ṽ from the estimated normal distribution N (μ(V), σ(V)). Note that a slightly different sample set is generated in each training epoch to cover the variations in the feature distribution. We can therefore represent this distribution by the vector ṽ = v∈ Ṽ v, the sum of the sampled vectors, which captures the mean feature distribution in its values and carries the feature variation through epochs. By leveraging a sufficient amount of training data and running multiple epochs, we anticipate achieving a reliable estimation. The distributional feature recomposition not only inherits the scalability and efficiency of the traditional sum of vector approach but also provides a more robust estimate of the distribution of the mean vector, resulting in improved representational power and better generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Efficient Neural Networks</head><p>Efficient models, which are smaller and faster neural networks, facilitate easy deployment across a variety of devices, making them beneficial for low-resource communities. EfficientNet <ref type="bibr" target="#b22">[22]</ref> and MobileNetV3 <ref type="bibr" target="#b11">[11]</ref> are two notable examples of such networks. These models achieve comparable or better performance than state-of-the-art ResNet on ImageNet. However, efficient models generally have shallower network layers and can underperform when the features are more difficult to learn, particularly in medical applications <ref type="bibr" target="#b25">[25]</ref>. To further demonstrate the representation power of our proposed method and expedite the diagnosis process, we experimentally substitute our image backbone with two efficient models, EfficientNet-B0 and MobileNetV3-Large-1.0, both of which exhibit highly competitive performance on ImageNet when compared to the original ResNet50. This evaluation serves two purposes: First, to test the applicability of our proposed method across different models, and second, to provide a more efficient and accessible placenta analysis model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation</head><p>We implemented the proposed methods and baselines using the Python/PyTorch framework and deployed the system on a computing server. For input images, we used PlacentaNet <ref type="bibr" target="#b2">[3]</ref> for segmentation and applied random augmentations such as random rotation and color jittering. We used a pre-trained BERT<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b6">[6]</ref> as our text encoder. EfficientNet-B0 and MobileNetV3-Large-1.0 followed official PyTorch implementations. All models and baselines were trained for 400 epochs.</p><p>The encoder in the last epoch was saved and evaluated on their task-specific performance on the test set, measured by the AUC-ROC scores (area under the ROC curve). To ensure the reliability of the results, each evaluation experiment was repeated five times using different fine-tuning dataset random splits. The same testing procedure was adopted for all our methods. We masked all iPad images using the provided manual segmentation masks. For more information, please refer to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We compare our proposed methods (Ours) with three strong baselines: a ResNet-50 classification network, the ConVIRT <ref type="bibr" target="#b28">[28]</ref> Medical VLC framework, and Pan et al. The mean results and confidence intervals (CIs) reported for each of the experiments on the two datasets are shown in Table <ref type="table" target="#tab_0">1</ref>. Some qualitative examples are in the supplementary material. Our performance-optimized method with the ResNet backbone consistently outperforms all other methods in all placental analysis tasks. These results confirm the effectiveness of our approach in reducing feature suppression and enhancing representational power. Moreover, compared to Pan et al., our method generally has lower variation across different random splits, indicating that our training method can improve the stability of learned representations. Furthermore, the qualitative examples provided in the supplementary material show that incorrect predictions are often associated with incorrect salient locations.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the speed improvements of our method. Since the efficiency of Pan et al. and ConVIRT is the same, we only present one of them for brevity. By removing the LLM during training, our method reduces the training time by a factor of 2.0. Moreover, the efficient version (e.g., MobileNet encoder) of our method has 2.4 to 4.1 times the throughput of the original model while still outperforming the traditional baseline approaches in most of the tasks, as shown </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation</head><p>To better understand the improvements, we conduct a component-wise ablation study. We use the ConVIRT method (instead of Pan et al.) as the starting point to keep the loss function the same. We report the mean AUC-ROC across all tasks to minimize the effects of randomness. As shown in Table <ref type="table" target="#tab_3">3</ref>, the text feature recomposition resulted in a significant improvement in performance since it treats all placental features equally to reduce the feature suppression problem. Moreover, applying distributional feature recomposition further improved performance, indicating that using a distribution to represent a set produces a more robust representation than a simple sum. Additionally, even the efficient version of our approach outperformed the performance version that was trained using the traditional VLC method. These improvements demonstrate the effectiveness of the proposed methods across different model architectures. However, we observed that the additional improvement from the distributional method was relatively small compared to that from the recomposition method. This may be due to the fact that the feature suppression problem is more prevalent than the misleading representation problem, or that the improvements may not be linearly proportional to the effectiveness-it may be more challenging to improve a better-performing model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We presented a novel automatic placenta analysis framework that achieves improved performance and efficiency. Additionally, our framework can accommodate architectures of different sizes, resulting in better-performing models that are faster and smaller, thereby enabling a wider range of applications. The framework demonstrated clear performance advantages over previous work without requiring additional data, while significantly reducing the model size and computational cost. These improvements have the potential to promote the clinical deployment of automated placenta analysis, which is particularly beneficial for resource-constrained communities.</p><p>Nonetheless, we acknowledge the large variance and performance drop when evaluating the iPad images. Hence, further research is required to enhance the model's robustness, and a larger external validation dataset is essential. Moreover, the performance of the image encoder is heavily reliant on the pre-trained language model, and our framework does not support online training of the language model. We aim to address these limitations in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A diagram illustrating the difference between the proposed approach (left) and the traditional VLC approach (right). x and t are images and text inputs, respectively. One sample input image and text are shown on the left. The loss function is defined as L = 1 N</figDesc><graphic coords="4,55,98,54,56,340,30,110,14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A diagram illustrating the idea of the proposed distributional feature recomposition. vi denotes the point estimate sum of the placenta pathological text vectors set Vi. N (μ(Vi), σ(Vi)) represents the distribution of the mean placental feature estimated from each Vi. The dark vectors represent the changing vectors from V1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>AUC-ROC scores (in %) for placenta analysis tasks. The mean and 95% CI of five random splits. The highest means are in bold and the second-highest means are underlined. Primary stands for the main placenta dataset, and iPad stands for the iPad dataset. (Mecon.: meconium; H.Chorio.: histologic chorioamnionitis; C.Chorio.: clinical chorioamnionitis)</figDesc><table><row><cell>Method</cell><cell cols="2">Primary Task</cell><cell></cell><cell></cell><cell>iPad Task</cell></row><row><cell></cell><cell>Mecon.</cell><cell>FIR</cell><cell>MIR</cell><cell>H.Chorio. Sepsis</cell><cell>MIR</cell><cell>C.Chorio</cell></row><row><cell cols="7">Supervised (ResNet-50) 77.0±2.9 74.2±3.3 68.5±3.4 67.4±2.7 88.4±2.0 50.8±21.6 47.0±16.7</cell></row><row><cell>ConVIRT (ResNet-50)</cell><cell cols="6">77.5±2.7 76.5±2.6 69.2±2.8 68.0±2.5 89.2±3.6 52.5±25.7 50.7±6.6</cell></row><row><cell>Pan et al. (ResNet-50)</cell><cell cols="6">79.4±1.3 77.4±3.4 70.3±4.0 68.9±5.0 89.8±2.8 61.9±14.4 53.6±4.2</cell></row><row><cell>Ours (ResNet-50)</cell><cell cols="6">81.3±2.3 81.3±3.0 75.0±1.6 72.3±2.6 92.0±0.9 74.9±5.0 59.9±4.5</cell></row><row><cell>Ours (EfficientNet)</cell><cell cols="6">79.7±1.5 78.5±3.9 71.5±2.6 67.8±2.8 87.7±4.1 58.7±13.3 61.2±4.6</cell></row><row><cell>Ours (MobileNet)</cell><cell cols="6">81.4±1.6 80.5±4.0 73.3±1.1 70.9±3.6 88.4±3.6 58.3±10.1 52.3±11.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Training and inference efficiency metrics. All these measurements are performed on a Tesla V100 GPU with a batch size of 32 at full precision (fp32). ResNet-50 s have the same inference efficiency and the number of parameters. (#params: number of parameters; Time: total training time in hours; throughput: examples/second; TFLOPS : Tera FLoating-point Operations/second). Improvements are in green.</figDesc><table><row><cell>Method</cell><cell cols="2">#params↓ Training</cell><cell>Inference</cell></row><row><cell></cell><cell></cell><cell>Time↓</cell><cell cols="2">Throughput↑ TFLOPS↓</cell></row><row><cell cols="2">Pan et al. (ResNet-50) 27.7M</cell><cell>38 hrs</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (ResNet-50)</cell><cell>27.7M</cell><cell cols="2">20 hrs ÷1.9 334</cell><cell>4.12</cell></row><row><cell>Ours (EfficientNet)</cell><cell>6.9M÷4.01</cell><cell cols="2">19 hrs÷2.0 822×2.46</cell><cell>0.40÷10.3</cell></row><row><cell>Ours (MobileNet)</cell><cell>7.1M÷3.90</cell><cell cols="2">18 hrs÷2.1 1368×4.10</cell><cell>0.22÷18.7</cell></row><row><cell>in</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>These results further support the superiority of the proposed representation and training method in terms of both training and testing efficiency.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Mean AUC-ROC scores over placenta analysis tasks on the primary dataset. The mean and 95% CI of five random splits. +Recomposition means the use of Pathology Report Feature Recomposition over the baseline, ∼+Distributional stands for the further adoption of the Distributional Feature Recomposition. Improvements are in green. The abbreviations follow Table1.</figDesc><table><row><cell></cell><cell>Mecon. FIR</cell><cell>MIR</cell><cell>H. Chorio. Sepsis</cell><cell>Mean</cell></row><row><cell cols="5">Baseline (ConVIRT) 77.5±2.7 76.5±2.6 69.2±2.8 68.0±2.5 89.2±3.6 76.1</cell></row><row><cell>+ Recomposition</cell><cell cols="4">80.8±1.9 80.2±3.1 74.6±1.8 71.8±3.2 92.0±1.4 79.9+3.8</cell></row><row><cell cols="5">∼ + Distributional 81.3±2.3 81.3±3.0 75.0±1.6 72.3±2.6 92.0±0.9 80.4+4.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://tfhub.dev/google/experts/bert/pubmed/2.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>Research reported in this publication was supported by the <rs type="funder">National Institute of Biomedical Imaging and Bioengineering of the National Institutes of Health (NIH)</rs> under award number <rs type="grantNumber">R01EB030130</rs> and the <rs type="institution">Pittsburgh Supercomputer Center</rs> through allocation IRI180002 from the <rs type="funder">Advanced Cyberinfrastructure Coordination Ecosystem</rs>: <rs type="programName">Services &amp; Support (ACCESS) program</rs>, which is supported by <rs type="funder">National Science Foundation</rs> grants Nos.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_96Zehaz">
					<idno type="grant-number">R01EB030130</idno>
				</org>
				<org type="funding" xml:id="_ehfz7eU">
					<orgName type="program" subtype="full">Services &amp; Support (ACCESS) program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 12.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated placental abruption identification using semantic segmentation, quantitative features, SVM, ensemble and multi-path CNN</title>
		<author>
			<persName><forename type="first">V</forename><surname>Asadpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Puttock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Getahun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Fassett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heliyon</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VLCDoC: visionlanguage contrastive pre-training model for cross-modal document classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakkali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coustaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rusiñol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Terrades</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2023">109419. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pla-centaNet: automatic morphological characterization of placenta photos with deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Gernand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_54</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-754" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">AI-PLAX: AI-based placental assessment and examination using photos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020">101744. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contrastive vision-language pre-training with limited resources</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20059-5_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20059-514" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13696</biblScope>
			<biblScope unit="page" from="236" to="253" />
		</imprint>
	</monogr>
	<note>ECCV 2022</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">MaskCLIP: masked self-distillation advances contrastive languageimage pretraining</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.12262</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CascadeNet for hysterectomy prediction in pregnant women due to placenta accreta spectrum</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Dormer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE-the International Society for Optical Engineering</title>
		<meeting>SPIE-the International Society for Optical Engineering</meeting>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">12032</biblScope>
			<biblScope unit="page" from="156" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maternalfetal inflammation in the placenta and the developmental origins of health and disease</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Gernand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Immunol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2020">531543. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ultrasound placental image texture analysis using artificial intelligence to predict hypertension in pregnancy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Balyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lamba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Matern.-Fetal Neonatal. Med</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="5587" to="5594" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Searching for MobileNetV3</title>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic placental distal villous hypoplasia scoring using a deep convolutional neural network regression model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khodaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grynspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bainbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ukwatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Instrumentation and Measurement Technology Conference (I2MTC)</title>
		<meeting>the IEEE International Instrumentation and Measurement Technology Conference (I2MTC)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Addressing feature suppression in unsupervised visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1411" to="1420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GestAltNet: aggregation and attention to improve deep learning of gestational age from placental whole-slide images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mobadersany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lab. Invest</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="942" to="951" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vision-language contrastive learning approach to robust automatic placenta analysis using photographic images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Gernand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mithal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mwinyelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_68</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-868" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="707" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">APPLAUSE: automatic prediction of PLAcental health via Unet segmentation and statistical evaluation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pietsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">102145. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Placental pathology, a survival guide</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Pathol. Labor. Med</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="641" to="651" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A bootstrap self-training method for sequence transfer: state-of-the-art placenta segmentation in fetal MRI</title>
		<author>
			<persName><forename type="first">B</forename><surname>Specktor-Fadida</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87735-4_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87735-418" />
	</analytic>
	<monogr>
		<title level="m">UNSURE/PIPPI -2021</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12959</biblScope>
			<biblScope unit="page" from="189" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal fusion model for classifying placenta ultrasound imaging in pregnancies with hypertension disorders</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pregnancy Hypertension</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">EfficientNet: rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">RU-net: an improved U-Net placenta segmentation network based on ResNet</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Program. Biomed</title>
		<imprint>
			<biblScope unit="volume">227</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2022">107206. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">COOKIE: contrastive crossmodal knowledge sharing pre-training for vision-language representation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A comparative analysis of eleven neural networks architectures for small datasets of lung images of COVID-19 patients toward improved clinical decisions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2021">104887. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prediction of placenta accreta spectrum by combining deep learning and radiomics using T2WI: A multicenter study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Abdom. Radiol</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4205" to="4218" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vinvl: revisiting visual representations in vision-language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Contrastive learning of medical visual representations from paired images and text</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Machine Learning for Healthcare Conference</title>
		<meeting>the Machine Learning for Healthcare Conference</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-region saliency-aware learning for cross-domain placenta image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Davaasuren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Gernand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="165" to="171" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
