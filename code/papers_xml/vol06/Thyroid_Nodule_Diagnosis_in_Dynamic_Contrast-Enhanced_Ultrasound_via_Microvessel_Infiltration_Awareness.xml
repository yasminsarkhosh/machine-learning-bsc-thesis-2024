<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haojie</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Brain-Machine Intelligence Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>211106</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongen</forename><surname>Liao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>10084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daoqiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Brain-Machine Intelligence Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>211106</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wentao</forename><surname>Kong</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Ultrasound</orgName>
								<orgName type="department" key="dep2">Affiliated Drum Tower Hospital</orgName>
								<orgName type="institution">Nanjing University Medical School</orgName>
								<address>
									<postCode>21008</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Fang</forename><surname>Chen</surname></persName>
							<email>chenfang@nuaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Brain-Machine Intelligence Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>211106</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DEC32C95204758F1163603C913AA980D</idno>
					<idno type="DOI">10.1007/978-3-031-43987-217.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Contrast-enhanced ultrasound</term>
					<term>Thyroid nodule</term>
					<term>Dynamic perfusion</term>
					<term>Infiltration Awareness</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dynamic contrast-enhanced ultrasound (CEUS) video with microbubble contrast agents reflects the microvessel distribution and dynamic microvessel perfusion, and may provide more discriminative information than conventional gray ultrasound (US). Thus, CEUS video has vital clinical value in differentiating between malignant and benign thyroid nodules. In particular, the CEUS video can show numerous neovascularisations around the nodule, which constantly infiltrate the surrounding tissues. Although the infiltrative of microvessel is ambiguous on CEUS video, it causes the tumor size and margin to be larger on CEUS video than on conventional gray US and may promote the diagnosis of thyroid nodules. In this paper, we propose a novel framework to diagnose thyroid nodules based on dynamic CEUS video by considering microvessel infiltration and via segmented confidence mapping assists diagnosis. Specifically, the Temporal Projection Attention (TPA) is proposed to complement and interact with the semantic information of microvessel perfusion from the time dimension of dynamic CEUS. In addition, we employ a group of confidence maps with a series of flexible Sigmoid Alpha Functions (SAF) to aware and describe the infiltrative area of microvessel for enhancing diagnosis. The experimental results on clinical CEUS video data indicate that our approach can attain an diagnostic accuracy of 88.79% for thyroid nodule and perform better than conventional methods. In addition, we also achieve an optimal dice of 85.54% compared to other classical segmentation methods. Therefore, consideration of dynamic microvessel perfusion and infiltrative expansion is helpful for CEUS-based diagnosis and segmentation of thyroid nodules. The datasets and codes will be available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Contrast-enhanced ultrasound (CEUS) as a modality of functional imaging has the ability to assess the intensity of vascular perfusion and haemodynamics in the thyroid nodule, thus considered a valuable new approach in the determination of benign vs. malignant nodules <ref type="bibr" target="#b0">[1]</ref>. In practice, CEUS video allows the dynamic observation of microvascular perfusion through intravenous injection of contrast agents. According to clinical experience, for thyroid nodules diagnosis, there are two characteristic that are important when analyzing CEUS video. 1) Dynamic microvessel perfusion. As shown in Fig. <ref type="figure" target="#fig_0">1(A)</ref>, clinically acquired CEUS records the dynamic relative intensity changes (microvessel perfusion pattern) throughout the whole examination <ref type="bibr" target="#b1">[2]</ref>. 2) Infiltrative expansion of microvessel. Many microvessels around nodules are constantly infiltrating and growing into the surrounding tissue. As shown in Fig. <ref type="figure" target="#fig_0">1(B)</ref>, based on the difference in lesion size displayed by the two modalities, clinical practice shows that gray US underestimates the size of lesions, and CEUS video overestimates the size of some lesions <ref type="bibr" target="#b2">[3]</ref>. Although the radiologist's cognition of microvascular invasive expansion is fuzzy, they think it may promote diagnosing thyroid nodules <ref type="bibr" target="#b0">[1]</ref>.  used the spatial feature enhancement for disease diagnosis based on dynamic CEUS. Furthermore, by combing the US modality, Chen et al. <ref type="bibr" target="#b4">[5]</ref> proposed a domain-knowledge-guided temporal attention module for breast cancer diagnosis. However, due to artifacts in CEUS, SOTA classification methods often fail to learn regions where thyroid nodules are prominent (As in Appendix Fig. <ref type="figure" target="#fig_0">A1</ref>) <ref type="bibr" target="#b5">[6]</ref>. Even the SOTA segmentation methods cannot accurately identify the lesion area for blurred lesion boundaries, thus, the existing automatic diagnosis network using CEUS still requires manual labeling of pixel-level labels which will lose key information around the tissues <ref type="bibr" target="#b6">[7]</ref>. In particular, few studies have developed the CEUS video based diagnostic model inspired by the dynamic microvessel perfusion, or these existing methods generally ignore the influence of microvessel infiltrative expansion. Whether the awareness of infiltrative area information can be helpful in the improvement of diagnostic accuracy is still unexplored.</p><p>Here, we propose an explanatory framework for the diagnosis of thyroid nodules based on dynamic CEUS video, which considers the dynamic perfusion characteristics and the amplification of the lesion region caused by microvessel infiltration. Our contributions are twofolds. First, the Temporal Projection Attention (TPA) is proposed to complement and interact with the semantic information of microvessel perfusion from the time dimension. Second, we adopt a group of confidence maps instead of binary masks to perceive the infiltrative expansion area from gray US to CEUS of microvessels for improving diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The architecture of the proposed framework is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The tasks of lesion area recognition and differential diagnosis are pixel-level and image-level classifications, and some low-level features of these two tasks can be shared interactively <ref type="bibr" target="#b7">[8]</ref>. We first fed the CEUS video I ∈ R C×T ×H×W into the cross-task feature extraction (CFA) module to jointly generate the features F iden and F cls for lesion area recognition and differential diagnosis, respectively. After that, in the temporal-based lesions area recognition (TLAR) module, an enhanced V-Net with the TPA is implemented to identify the relatively clear lesion area which are visible on both gray US and CEUS video. Because microvessel invasion expansion causes the tumor size and margin depicted by CEUS video to be larger than that of gray US, we further adopt a group of confidence maps based on Sigmoid Alpha Functions (SAF) to aware the infiltrative area of microvessels for improving diagnosis. Finally, the confidence maps are fused with F cls and fed into a diagnosis subnetwork based on lightweight C3D <ref type="bibr" target="#b8">[9]</ref> to predict the probability of benign and malignant. In the CFA, we first use the 3D inception block to extract multi-scale features F muti . The 3D inception block has 4 branches with cascaded 3D convolutions. Multiple receptive fields are obtained through different branches, and then group normalization and ReLU activation are performed to obtain multi-scale features F muti . Then, we use the cross-task feature adaptive unit to generate the features F iden and F cls required for lesions area recognition and thyroid nodules diagnosis via the following formula <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_0">[F iden , F cls ] = [ω iden , ω cls ] * F muti + [F muti , F muti ]<label>(1)</label></formula><p>where ω iden , ω cls are the learnable weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Temporal-Based Lesions Area Recognition (TLAR)</head><p>The great challenge of automatic recognition of lesion area from CEUS video is that the semantic information of the lesion area is different in the CEUS video of the different microvessel perfusion periods. Especially in the perfusion period and the regression period, the semantic information of lesions cannot be fully depicted in an isolated CEUS frame. Thus, the interactive fusion of semantic information of the whole microvessel perfusion period will promote the identification of the lesion area, and we design the Temporal Projection Attention (TPA) to realize this idea. We use V-Net as the backbone, which consists of four encoder/decoder blocks for TLAR, and the TPA is used in the bottleneck of the V-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Projection Attention (TPA). Given a feature</head><formula xml:id="formula_1">F 4th ∈ R C×T × H 16 × W</formula><p>16 after four down-sampling operations in encoder, its original 3D feature map is projected <ref type="bibr" target="#b10">[11]</ref> to 2D plane to get keys and queries:</p><formula xml:id="formula_2">K, Q ∈ R C× H 16 × W</formula><p>16 , and we use global average pooling (GAP) and global maximum pooling (GMP) as temporal projection operations. Here,</p><formula xml:id="formula_3">V ∈ R C×T × H 16 × W</formula><p>16 is obtained by a single convolution. This operation can also filter out the irrelevant background and display the key information of the lesions. After the temporal projection, a group convolution with a group size of 4 is employed on K to extract the local temporal attention L ∈ R C× H 16 × W 16 . Then, we concatenate L with Q to further obtain the global attention G ∈ R C×1× H 16 × W 16 by two consecutive 1 × 1 2D convolutions and dimension expend. Those operations are described as follows:</p><formula xml:id="formula_4">K = Q = GAP (F 4th ) + GM P (F 4th ) (2) G = Expend(Conv(σ(Conv(σ(Gonv(K)) ⊕ Q))))<label>(3)</label></formula><p>where Gonv(•) is the group convolution, σ denotes the normalization, "⊕" is the concatenation operation. The global attention G encodes not only the contextual information within isolated query-key pairs but also the attention inside the keys <ref type="bibr" target="#b12">[12]</ref>. After that, based on the 2D global attention G, we multiply V and G to calculate the global temporal fusion attention map</p><formula xml:id="formula_5">M ∈ R C×T × H 16 × W</formula><p>16 to enhance the feature representation.</p><p>Meanwhile, to make better use of the channel information, we use 3 16 . Then, we use parallel average pooling and full connection operation to reweight the channel information of F 4th to obtain the reweighted feature</p><formula xml:id="formula_6">× 3 × 3 Conv to get enhanced channel feature F 4th ∈ R C1×T × H 16 × W</formula><formula xml:id="formula_7">F 4th ∈ R C×T × H 16 × W 16 .</formula><p>The obtained global temporal fusion attention maps M are fused with the reweighted feature F 4th to get the output features F fin . Finally, we input F fin into the decoder of the TLAR to acquire the feature map of lesion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Microvessel Infiltration Awareness (MIA)</head><p>We design a MIA module to learn the infiltrative areas of microvessel. The tumors and margin depicted by CEUS may be larger than those depicted by gray US because of continuous infiltrative expansion. Inspired by the continuous infiltrative expansion, a series of flexible Sigmoid Alpha Functions (SAF) simulate the infiltrative expansion of microvessels by establishing the distance maps from the pixel to lesion boundary. Here, the distance maps <ref type="bibr" target="#b13">[13]</ref> are denoted as the initial probability distribution P D . Then, we utilize Iterative Probabilistic Optimization (IPO) unit to produce a set of optimized probability maps P = {p 1 , p2 . . . pn } to aware the microvessel infiltration for thyroid nodules diagnosis. Based on SAF and IPO, CEUS-based diagnosis of thyroid nodules can make full use of the ambiguous information caused by microvessel infiltration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sigmoid Alpha Function (SAF).</head><p>It is generally believed that the differentiation between benign and malignant thyroid nodules is related to the pixels around the boundaries of the lesion <ref type="bibr" target="#b14">[14]</ref>, especially in the infiltrative areas of microvessel <ref type="bibr" target="#b2">[3]</ref>. Therefore, we firstly build the initial probability distribution P D based on the distance between the pixels and the annotation boundaries by using SAF in order to aware the infiltrative areas. Here, SAF is defined as follows:</p><formula xml:id="formula_8">SAF (i,j) = C * 2 1 + e -αD(i,j) max(D(i,j )) -1<label>(4)</label></formula><formula xml:id="formula_9">C = 1 + e -α / 1 -e -α ; α ∈ (0, +∞)<label>(5)</label></formula><p>where α is the conversion factor for generating initial probability distribution P D (when α → ∞, the generated P D is binary mask); C is used to control the function value within the range of [0, 1]; (i, j) is the coordinate point in feature map; D (i, j) indicates the shortest distance from (i, j) to lesion's boundaries.</p><p>Iterative Probabilistic Optimization (IPO) Unit. Based on the fact that IncepText <ref type="bibr" target="#b15">[15]</ref> has experimentally demonstrated that asymmetric convolution can effectively solve the problem of highly variable size and aspect ratio, we use asymmetric convolution in the IPO unit. Asymmetric convolution-based IPO unit can optimize the initial distribution P D to generate optimized probability maps P that can reflect the confidence of benign and malignant diagnosis. Specifically, with the IPO, our network can make full use of the prediction information in low-level iteration layer, which may improve the prediction accuracy of high-level iteration layer. In addition, the parameters in the high-level iteration layer can be optimized through the back-propagation gradient from the high-level iteration layer. IPO unit can be shown as the following formula:</p><formula xml:id="formula_10">p1 = ConvBlock(SAF (f 0 , α 1 )) (6) pi = ConvBlock(SAF ((f 0 ⊕ pi-1 ), α i )) i ∈ (1, n]<label>(7)</label></formula><p>where "⊕" represents the concatenation operation;ConvBlock consists of a group of asymmetric convolutions (e.g., Conv1 × 5, Conv5 × 1 and Conv1 × 1); n denotes the number of the layers of IPO unit. With the lesion's feature map f 0 from the TLAR module, the initial distribution P D obtained by SAF is fed into the first optimize layer of IPO unit to produce the first optimized probability map p1 . Then, p1 is contacted with f 0 , and used to generate optimized probability map p2 through the continuous operation based on SAF and the second optimize layer of IPO unit. The optimized probability map pi-1 provides prior information for producing the next probability map pi . In this way, we can get a group of probability map P to aware the microvascular infiltration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Loss Function</head><p>With continuous probability map P obtained from MIA, P are multiplied with the feature F cls . Then, these maps are fed into a lightweight C3D to predict the probability of benign and malignant, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. We use the mean square error L MSE to constrain the generation of P . Assuming that the generated P is ready to supervise the classification network, we want to ensure that the probability maps can accurately reflect the classification confidence. Thus, we design a task focus loss L ta to generate confidence maps P , as follows:</p><formula xml:id="formula_11">L MSE = n i=1 1 Ω p∈Ω g i (pi), pi (pi) 2 (8) L ta = 1 2σ 2 n i=1 p i -p i 2 2 + log σ (9)</formula><p>where g i is the label of pi , which is generated by the operation of SAF(D (i,j) , α i ); pi denotes pixel in the image domain Ω, σ is a learnable parameter to eliminate the hidden uncertainty information.</p><p>For differentiating malignant and benign, we employ a hybrid loss L total that consists of the cross-entropy loss L cls , the loss of L MSE computing optimized probability maps P , and task focus loss L ta . The L total is denoted as follows:</p><formula xml:id="formula_12">L total = λ 1 • L cls + λ 2 • L MSE + λ 3 • L ta (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>where λ 1 , λ 2 , λ 3 are the hyper-parameters to balance the corresponding loss. As the weight parameter, we set λ 1 , λ 2 , λ 3 are 0.5,0.2,0.3 in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset. Our dataset contained 282 consecutive patients who underwent thyroid nodule examination at Nanjing Drum Tower Hospital. All patients performed dynamic CEUS examination by an experienced sonographer using an iU22 scanner (Philips Healthcare, Bothell, WA) equipped with a linear transducer L9-3 probe. These 282 cases included 147 malignant nodules and 135 benign nodules. On the one hand, the percutaneous biopsy based pathological examination was implemented to determine the ground-truth of malignant and benign. On the other hand, a sonographer with more than 10 years of experience manually annotated the nodule lesion mask to obtain the pixel-level groundtruth of thyroid nodules segmentation. All data were approved by the Institutional Review Board of Nanjing Drum Tower Hospital, and all patients signed the informed consent before enrollment into the study.</p><p>Implementation Details. Our network was implemented using Pytorch framework with the single 12 GB GPU of NVIDIA RTX 3060. During training, we first pre-trained the TALR backbone via dice loss for 30 epochs and used Adam optimizer with learning rate of 0.0001. Then, we loaded the pre-trained weights to train the whole model for 100 epochs and used Adam optimizer with learning rate of 0.0001. Here, we set batch-size to 4 during the entire training process The CEUS consisted the full wash-in and wash-out phases, and the resolution of each frame was (600 × 800). In addition, we carried out data augmentation, including random rotation and cropping, and we resize the resolution of input frames to (224 × 224). We adopted 5-fold cross-validation to achieve quantitative evaluation. Three indexes including Dice, Recall, and IOU, were used to evaluate the lesion recognition task, while five indexes, namely average accuracy (ACC), sensitivity (Se), specificity (Sp), F1-score (F1), and AUC, were used to evaluate the diagnosis task.</p><p>Experimental Results. As in Table <ref type="table" target="#tab_0">1</ref>, we compared our method with SOTA method including V-Net, Unet3D, TransUnet. For the task of identifying lesions, the index of Recall is important, because information in irrelevant regions can be discarded, but it will be disastrous to lose any lesion information. V-Net achieved the highest Recall scores compared to others; thus, it was chosen as the backbone of TLAR. Table <ref type="table" target="#tab_0">1</ref> revealed that the modules (TPA, SAF, and IPO) used in the network greatly improved the segmentation performance compared to baseline, increasing Dice and Recall scores by 7.60% and 7.23%, respectively. For the lesion area recognition task, our method achieved the highest Dice of 85.54% and Recall of 90.40%, and the visualized results were shown in Fig. <ref type="figure" target="#fig_2">3</ref>. To evaluate the effectiveness of the baseline of lightweight C3D, we compared the results with SOTA video classification methods including C3D, R3D, R2plus1D and ConvLSTM. For fair comparison, all methods used the manually annotated lesion mask to assist the diagnosis. Experimental results in Table <ref type="table" target="#tab_1">2</ref> revealed that our baseline network could be useful for the diagnosis. With the effective baseline, the introduced modules including TLAR, SAF and IPO further improved the diagnosis accuracy, increasing the accuracy by 9.5%. The awareness of microvascular infiltration using SAF and IPO unit was helpful for CEUS-based diagnosis, as it could improve the diagnosis accuracy by 7.69% (As in Table <ref type="table" target="#tab_1">2</ref>). As in Appendix Fig. <ref type="figure" target="#fig_0">A1</ref>, although SOTA method fails to focus on lesion areas, our method can pinpoint discriminating lesion areas. Influence of α Values. The value of α in SAF is associated with simulating microvessel infiltration. Figure <ref type="figure" target="#fig_2">3</ref> (C) showed that the diagnosis accuracy increased along with the increment of α and then tended to become stable when α was close to 9. Therefore, for balancing the efficiency and performance, the number of IPO was set as n = 3 and α was set as α = {1, 5, 9} to generate a group of confidence maps that can simulate the process of microvessel infiltration. (More details about the setting of n is in Appendix Fig. <ref type="figure">A4</ref> of the supplementary material.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The microvessel infiltration leads to the observation that the lesions detected on CEUS tend to be larger than those on gray US. Considering the microvessel infiltration, we propose an method for thyroid nodule diagnosis based on CEUS videos. Our model utilizes a set of confidence maps to recreate the lesion expansion process; it effectively captures the ambiguous information caused by microvessel infiltration, thereby improving the accuracy of diagnosis. This method is an attempt to eliminate the inaccuracy of diagnostic task due to the fact that gray US underestimates lesion size and CEUS generally overestimates lesion size. To the best of our knowledge, this is the first attempt to develop an automated diagnostic tool for thyroid nodules that takes into account the effects of microvessel infiltration. The way in which we fully exploit the information in time dimension through TPA also makes the model more clinically explanatory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (A) The dynamic cropped frames in CEUS video. Radiologists identify the area of lesions by comparing various frames, but each individual frame does not effectively describe the lesions area with precision. From the start to the end of the timeline, the three colours represent the change in intensity of the CEUS video over three different time periods. The red border represents the area of the lesion. (B) The size of thyroid nodules described on CEUS is significantly larger than detected through gray US. The yellow line indicates the lesion area labeled by radiologists on gray US, while the red arrow corresponds to the infiltration area or continuous lesion expansion on CEUS. (Color figure online)</figDesc><graphic coords="2,61,80,335,57,300,10,98,50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overall architecture which contains four parts: cross-task shared feature extraction, temporal-based lesions area recognition, microvessel infiltration awareness and thyroid nodules diagnosis. Here, P = {p1, p2 . . . pn} represents the infiltration process of microvessels from gray US to CEUS, and pn is the final segmentation result.</figDesc><graphic coords="3,55,98,54,02,340,15,182,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (A)Comparison of the visualised results with the SOTA method, green and red contours is the automatically recognized area and ground-truth. By enlarging the local details, we show that our model can obtain the optimal result (More visuals provided in Appendix Fig. A2 and Fig. A3.). (B) Microvascular infiltration was simulated from gray US to CEUS via a set of confidence maps. (C) Influence of α values. (Color figure online)</figDesc><graphic coords="9,80,97,54,26,290,23,86,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative lesion recognition results are compared with SOTA methods and ablation experiments.</figDesc><table><row><cell cols="2">DICE(%)↑ 73.63 ± 5.54 77.94 ± 4.77 72.84 ± 6.88</cell><cell>81.22 ± 4.18 82.96 ± 4.11</cell><cell>83.32 ± 4.03</cell><cell>85.54 ± 4.93</cell></row><row><cell cols="2">Recall(%)↑ 79.96 ± 4.39 83.17 ± 5.05 77.69 ± 5.18</cell><cell>83.96 ± 3.66 88.71 ± 3.92</cell><cell>89.45 ± 3.77</cell><cell>90.40 ± 5.93</cell></row><row><cell>IOU(%)↑</cell><cell>60.56 ± 5.83 65.20 ± 5.09 59.83 ± 6.19</cell><cell>69.96 ± 3.58 72.63 ± 3.15</cell><cell>73.26 ± 4.03</cell><cell>74.99 ± 4.72</cell></row></table><note><p><p><p><p><p><p><p><p>Network</p>UNet3D</p><ref type="bibr" target="#b18">[18]</ref> </p>V-Net</p><ref type="bibr" target="#b16">[16]</ref> </p>TransUNet</p><ref type="bibr" target="#b17">[17]</ref> </p>V-Net+ TPA V-Net+ TPA+SAF V-Net+ TPA+IPO Ours</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative diagnostic results are compared with SOTA methods and ablation experiments.</figDesc><table><row><cell>Network</cell><cell>ACC (%) ↑</cell><cell>Se (%) ↑</cell><cell>Sp (%) ↑</cell><cell>F1 (%) ↑</cell><cell>AUC (%) ↑</cell></row><row><cell>C3D+Mask [19]</cell><cell cols="5">76.61 ± 4.13 88.05 ± 3.73 76.92 ± 3.17 77.52 ± 3.32 88.05 ± 3.01</cell></row><row><cell>R3D+Mask [20]</cell><cell cols="5">77.01 ± 3.25 87.17 ± 4.08 79.49 ± 3.88 87.05 ± 3.93 83.10 ± 2.35</cell></row><row><cell>R2plus1D+Mask [21]</cell><cell cols="5">78.88 ± 2.81 85.02 ± 2.49 77.45 ± 3.43 87.75 ± 2.79 81.10 ± 3.82</cell></row><row><cell cols="6">ConvLSTM+Mask [22] 78.59 ± 4.44 84.37 ± 3.66 76.26 ± 4.26 80.22 ± 3.92 85.95 ± 3.73</cell></row><row><cell>Baseline+Mask</cell><cell cols="5">79.29 ± 2.58 89.83 ± 1.80 80.84 ± 3.04 86.01 ± 2.00 88.25 ± 2.84</cell></row><row><cell>Baseline+TLAR</cell><cell cols="5">81.10 ± 2.24 84.97 ± 1.28 81.58 ± 2.74 82.49 ± 1.81 88.67 ± 1.96</cell></row><row><cell cols="6">Baseline+TLAR+SAF 84.15 ± 1.78 89.90 ± 0.94 79.08 ± 1.85 83.95 ± 1.79 89.90 ± 1.97</cell></row><row><cell cols="6">Baseline+TLAR+IPO 86.56 ± 2.45 92.58 ± 2.38 79.93 ± 2.53 86.41 ± 1.36 93.33 ± 2.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Ours 88.79 ± 1.40 94.26 ± 1.68 88.37 ± 1.80 90.41 ± 1.85 94.54 ± 1.54</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Performance of contrast-enhanced ultrasound in thyroid nodules: review of current state and future perspectives</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radzina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ratniece</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Putrins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancers</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">5469</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Superb microvascular imaging compared with contrast-enhanced ultrasound to assess microvessels in thyroid nodules</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yongfeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wengang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10396-020-01011-z</idno>
		<ptr target="https://doi.org/10.1007/s10396-020-01011-z" />
	</analytic>
	<monogr>
		<title level="j">J. Med. Ultrasonics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="297" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Breast tumor size assessment: comparison of conventional ultrasound and contrast-enhanced ultrasound</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Med. Biol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1873" to="1881" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical temporal attention network for thyroid nodule recognition using dynamic CEUS imaging</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1646" to="1660" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain knowledge powered deep learning for breast cancer diagnosis based on contrast-enhanced ultrasound videos</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2439" to="2451" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-attribute attention network for interpretable diagnosis of thyroid nodules in ultrasound images</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Manh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ultrason. Ferroelect. Frequency Control</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2611" to="2620" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computer-aided prediction of axillary lymph node status in breast cancer using tumor surrounding tissue features in ultrasound images</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Meth. Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="143" to="150" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simultaneous detection and classification of partially and weakly supervised cells</title>
		<author>
			<persName><forename type="first">A</forename><surname>Golts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Livneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zohar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-816" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13803</biblScope>
			<biblScope unit="page" from="313" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Key-frame guided network for thyroid nodule recognition using ultrasound videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-823" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint learning of 3D lesion segmentation and classification for explainable COVID-19 diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2463" to="2476" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Axial-DeepLab: stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12349</biblScope>
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58548-8_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58548-87" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">APAUNet: axis projection attention UNet for small target in 3D medical segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Arbitrary shape text detection via segmentation with probability maps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2022.3176122</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2022.3176122" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Assessment of the invariance and discriminant power of morphological features under geometric transformations for breast tumor classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gómez-Flores</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page">105173</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inceptext: a new inception-text module with deformable psroipooling for multi-oriented scene text detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1071" to="1077" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">V-Net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">TransuNet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A CNN-LSTM approach to human activity recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mutegeki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="362" to="366" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
