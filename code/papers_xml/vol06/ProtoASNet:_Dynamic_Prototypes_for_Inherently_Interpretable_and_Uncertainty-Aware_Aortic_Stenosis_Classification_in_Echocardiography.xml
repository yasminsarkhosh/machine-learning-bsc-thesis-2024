<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography</title>
				<funder>
					<orgName type="full">Canadian Institutes of Health Research</orgName>
					<orgName type="abbreviated">CIHR</orgName>
				</funder>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hooman</forename><surname>Vaseli</surname></persName>
							<email>hoomanv@ece.ubc.ca</email>
							<idno type="ORCID">0000-0002-8259-9488</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ang</forename><forename type="middle">Nan</forename><surname>Gu</surname></persName>
							<email>purang@ece.ubc.ca</email>
							<idno type="ORCID">0000-0001-8926-2397</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">Neda</forename><surname>Ahmadi Amiri</surname></persName>
							<idno type="ORCID">0000-0002-4091-6916</idno>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">Y</forename><surname>Tsang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Vancouver General Hospital</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Fung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nima</forename><surname>Kondori</surname></persName>
							<idno type="ORCID">0009-0000-5153-9039</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Armin</forename><surname>Saadat</surname></persName>
							<idno type="ORCID">0009-0005-8322-1216</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Purang</forename><surname>Abolmaesumi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Teresa</forename><forename type="middle">S M</forename><surname>Tsang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Vancouver General Hospital</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="368" to="378"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">7B781C440DAD2D39F154D8FB91E08A78</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Aleatoric Uncertainty • Aortic Stenosis</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aortic stenosis (AS) is a common heart valve disease that requires accurate and timely diagnosis for appropriate treatment. Most current automatic AS severity detection methods rely on black-box models with a low level of trustworthiness, which hinders clinical adoption. To address this issue, we propose ProtoASNet, a prototypical network that directly detects AS from B-mode echocardiography videos, while making interpretable predictions based on the similarity between the input and learned spatio-temporal prototypes. This approach provides supporting evidence that is clinically relevant, as the prototypes typically highlight markers such as calcification and restricted movement of aortic valve leaflets. Moreover, ProtoASNet utilizes abstention loss to estimate aleatoric uncertainty by defining a set of prototypes that capture ambiguity and insufficient information in the observed data. This provides a reliable system that can detect and explain when it may fail. We evaluate ProtoASNet on a private dataset and the publicly available TMED-2 dataset, where it outperforms existing state-of-the-art methods with an accuracy of 80.0% and 79.7%, respectively. Furthermore, Pro-toASNet provides interpretability and an uncertainty measure for each prediction, which can improve transparency and facilitate the interactive usage of deep networks to aid clinical decision-making. Our source code is available at: https://github.com/hooman007/ProtoASNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aortic stenosis (AS) is a common heart valve disease characterized by the calcification of the aortic valve (AV) and the restriction of its movement. It affects 5% of individuals aged 65 or older <ref type="bibr" target="#b1">[2]</ref> and can progress rapidly from mild or moderate to severe, reducing life expectancy to 2 to 3 years <ref type="bibr" target="#b19">[20]</ref>. Echocardiography (echo) is the primary diagnostic modality for AS. This technique measures Doppler-derived clinical markers <ref type="bibr" target="#b15">[16]</ref> and captures valve motion from the parasternal long (PLAX) and short axis (PSAX) cross-section views. However, obtaining and interpreting Doppler measurements requires specialized training and is subject to significant inter-observer variability <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>To alleviate this issue, deep neural network (DNN) models have been proposed for automatic assessment of AS directly from two-dimensional B-mode echo, a modality more commonly used in point-of-care settings. Huang et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> proposed a multitask model to classify the severity of AS using echo images. Ginsberg et al. <ref type="bibr" target="#b5">[6]</ref> proposed an ordinal regression-based method that predicts the severity of AS and provides an estimate of aleatoric uncertainty due to uncertainty in training labels. However, these works utilized black-box DNNs, which could not provide an explanation of their prediction process.</p><p>Explainable AI (XAI) methods can provide explanations of a DNN's decision making process and can generally be categorized into two classes. Post-hoc XAI methods explain the decisions of trained black-box DNNs. For example, gradientbased saliency maps <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> show where a model pays attention to, but these methods do not necessarily explain why one class is chosen over another <ref type="bibr" target="#b16">[17]</ref>, and at times result in misleading explanations <ref type="bibr" target="#b0">[1]</ref>. Ante-hoc XAI methods are explicitly designed to be explainable. For instance, prototype-based models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, which the contributions of our paper fall under, analyze a given input based on its similarity to learned discriminative features (or "prototypes") for each class. Both the learned prototypes and salient image patches of the input can be visualized for users to validate the model's decision making.</p><p>There are two limitations to applying current prototype-based methods to the task of classifying AS severity from echo cine series. First, prototypes should be spatio-temporal instead of only spatial, since AS assessment requires attention to small anatomical regions in echo (such as the AV) at a particular phase of the heart rhythm (mid-systole). Second, user variability in cardiac view acquisition and poor image quality can complicate AV visualization in standard PLAX and PSAX views. The insufficient information in such cases can lead to more plausible diagnoses than one. Therefore, a robust solution should avoid direct prediction and notify the user. These issues have been largely unaddressed in previous work.</p><p>We propose ProtoASNet (Fig. <ref type="figure" target="#fig_0">1</ref>), a prototype-based model for classifying AS severity from echo cine series. ProtoASNet discovers dynamic prototypes that describe shape-and movement-based phenomena relevant to AS severity, outperforming existing models that only utilize image-based prototypes. Additionally, our model can detect ambiguous decision-making scenarios based on similarity with less informative samples in the training set. This similarity is expressed as a measure of aleatoric uncertainty. To the best of our knowledge, the only prior work for dynamic prototypes published to-date is <ref type="bibr" target="#b6">[7]</ref>. ProtoASNet is the first work to use dynamic prototypes in medical imaging and the first to incorporate aleatoric uncertainty estimation with prototype-based networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background: Prototype-Based Models</head><p>Prototype-based models explicitly make their decisions using similarities to cases in the training set. These models generally consist of three key components structured as h(g(f (x))). Firstly, f (.) is a feature encoder such as a ConvNet that maps images x ∈ R Ho×Wo×3 to f (x) ∈ R H×W ×D , where H, W , and D correspond to the height, width, and feature depth of the ConvNet's intermediate layer, respectively. Secondly, g(.) ∈ R H×W ×D → R P is a prototype pooling function that computes the similarity of encoded features f (x) to P prototype vectors. There are K learnable prototypes defined for each of C classes, denoted as p c k . Finally, h(.) ∈ R P → R C is a fully-connected layer that learns to weigh the input-prototype similarities against each other to produce a prediction score for each class. To ensure that the prototypes p </p><formula xml:id="formula_0">p c k ← arg min z∈Zc z -p c k 2 , where Z c = {z : z ∈ f p c k (x i ) s.t. y i ∈ c} (1)</formula><p>Such models are inherently interpretable since they are enforced to first search for similar cases in the training set and then to compute how these similarities contribute to the classification. As a result, they offer a powerful approach for identifying and classifying similar patterns in data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ProtoASNet</head><p>Feature Extraction. The overall structure of ProtoASNet is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>The feature extraction layer consists of a convolutional backbone, in our case the first three blocks of a pre-trained R(2+1)D-18 <ref type="bibr" target="#b20">[21]</ref> model, followed by two branches of feature and region of interest (ROI) modules made up of two and three convolutional layers respectively. In both modules, the convolutional layers have ReLU activation function, except the last layers which have linear activations. Given an input video x ∈ R Ho×Wo×To×3 with T o frames, the first branch learns a feature F (x) ∈ R H×W ×T ×D , where each D-dimensional vector in F (x) corresponds to a specific spatio-temporal region in the video. The second branch generates P regions of interest, M p c k (x) ∈ R H×W ×T , that specify which regions of F (x) are relevant for comparing with each prototype p c k . The features from different spatio-temporal regions must be pooled before being compared to prototypes. As in <ref type="bibr" target="#b11">[12]</ref>, we perform a weighted average pooling with the learned regions of interest as follows:</p><formula xml:id="formula_1">f p c k (x) = 1 HW T H,W,T |M p c k (x)| • F (x),<label>(2)</label></formula><p>where |.| is the absolute value and • is the Hadamard product.</p><p>Prototype Pooling. The similarity score of a feature vector f p c k and prototype p c k is calculated using cosine similarity, which is then shifted to [0, 1]:</p><formula xml:id="formula_2">g(x, p c k ) = 1 2 (1 + &lt; f p c k (x), p c k &gt; f p c k (x) 2 p c k 2</formula><p>).</p><p>(</p><formula xml:id="formula_3">)<label>3</label></formula><p>Prototypes for Aleatoric Uncertainty Estimation. In Fig. <ref type="figure" target="#fig_0">1</ref>, trainable uncertainty prototypes (denoted p u k ) are added to capture regions in the data distribution that are inherently ambiguous (Fig. <ref type="figure" target="#fig_0">1</ref>.B). We use similarity between f p u k (x) and p u k to quantify aleatoric uncertainty, denoted α ∈ [0, 1]. We use an "abstention loss" (Eq. ( <ref type="formula" target="#formula_4">6</ref>)) method inspired by <ref type="bibr" target="#b4">[5]</ref> to learn α and thereby p u k . In this loss, α is used to interpolate between the ground truth and prediction, pushing the model to "abstain" from its own answer at a penalty.</p><formula xml:id="formula_4">ŷ = σ(h(g(x, p c k ))), α = σ(h(g(x, p u k ))); (4) ŷ = (1 -α)ŷ + αy; (5) L abs = CrsEnt(ŷ , y) -λ abs log(1 -α), (<label>6</label></formula><formula xml:id="formula_5">)</formula><p>where σ denotes Softmax normalization in the output of h(.), y and ŷ are the ground truth and the predicted probabilities, respectively, and λ abs is a regularization constant.</p><p>When projecting p u k to the nearest extracted feature from training examples, we relax the requirement in Eq. ( <ref type="formula">1</ref>) allowing the uncertainty prototypes to be pushed to data with the ground truth of any AS severity class.</p><p>Class-Wise Similarity Score. The fully connected (FC) layer h(.) is a dense mapping from prototype similarity scores to prediction logits. Its weights, w h , are initialized to be 1 between class c and the corresponding prototypes and 0 otherwise to enforce the process to resemble positive reasoning. h(.) produces a score for membership in each class and for α.</p><p>Loss Function. As in previous prototype-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>, the following losses are introduced to improve performance: 1) Clustering and separation losses (Eq. ( <ref type="formula" target="#formula_6">7</ref>)), which encourage clustering based on class, where P y denotes the set of prototypes belonging to class y. Due to lack of ground truth uncertainties, these losses are only measured on p c k , not p u k ; 2) Orthogonality loss (Eq. ( <ref type="formula" target="#formula_7">8</ref>)), which encourages prototypes to be more diverse; 3) Transformation loss L trns (described in <ref type="bibr" target="#b11">[12]</ref>), which regularizes the consistency of the predicted occurrence regions under random affine transformations; 4) Finally, L norm (described in <ref type="bibr" target="#b3">[4]</ref>) regularizes w h to be close to its initialization and penalizes relying on similarity to one class to influence the logits of other classes. Equation ( <ref type="formula" target="#formula_8">9</ref>) describes the overall loss function where λ represent regularization coefficients for each loss term. The network is trained end-to-end. We conduct a "push" stage (see Eq. ( <ref type="formula">1</ref>)) every 5 epochs to ensure that the learned prototypes are consistent with the embeddings from real examples.</p><formula xml:id="formula_6">L clst = -max p c k ∈Py g(x, p c k ), L sep = max p c k / ∈Py g(x, p c k );<label>(7)</label></formula><formula xml:id="formula_7">L orth = i&gt;j &lt; p i , p j &gt; p i 2 p j 2 ;<label>(8)</label></formula><formula xml:id="formula_8">L = L abs + λ clst L clst + λ sep L sep + λ orth L orth + λ trns L trns + λ norm L norm . (<label>9</label></formula><formula xml:id="formula_9">)</formula><p>3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We conducted experiments on a private AS dataset and the public TMED-2 dataset <ref type="bibr" target="#b9">[10]</ref>. The private dataset was extracted from an echo study database of a tertiary care hospital with institutional review ethics board approval. Videos were acquired with Philips iE33, Vivid i, and Vivid E9 ultrasound machines. For each study, the AS severity was classified using clinically standard Doppler echo guidelines <ref type="bibr" target="#b2">[3]</ref> by a level III echocardiographer, keeping only cases with concordant Doppler measurements. PLAX and PSAX view cines were extracted from each study using a view-detection algorithm <ref type="bibr" target="#b12">[13]</ref>, and subsequently screened by a level III echocardiographer to remove misclassified cines. For each cine, the echo beam area was isolated and image annotations were removed. The dataset consists of 5055 PLAX and 4062 PSAX view cines, with a total of 2572 studies. These studies were divided into training, validation, and test sets, ensuring patient exclusivity and following an 80-10-10 ratio. We performed randomized augmentations including resized cropping and rotation. The TMED-2 dataset <ref type="bibr" target="#b9">[10]</ref> consists of 599 fully labeled echo studies containing 17270 images in total. Each study consists of 2D echo images with clinicianannotated view labels (PLAX/PSAX/Other) and Doppler-derived study-level AS severity labels (no AS/early AS/significant AS). Though the dataset includes an unlabeled portion, we trained on the labeled set only. We performed data augmentation similar to the private dataset without time-domain operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>To better compare the results with TMED-2 dataset, we adopted their labeling scheme of no AS (normal), early AS (mild), and significant AS (moderate and severe) in our private dataset. We split longer cines into 32-frame clips which are approximately one heart cycle long. In both layers of the feature module, we used D convolutional filters, while the three layers in the ROI module had D, D  2 , and P convolutional filters, preventing an abrupt reduction of channels to the relatively low value of P . In both modules, we used kernel size of 1×1×1. We set D = 256 and K = 10 for AS class and aleatoric uncertainty prototypes. Derived from the hyperparameter selection of ProtoPNet <ref type="bibr" target="#b3">[4]</ref>, we assigned the values of 0.8, 0.08, and 10 -4 to λ clst , λ sep , and λ norm respectively. Through a search across five values of 0.1, 0.3, 0.5, 0.9, and 1.0, we found the optimal λ abs to be 0.3 based on the mean F1 score of the validation set. Additionally, we found λ orth and λ trns to be empirically better as 10 -2 and 10 -3 respectively. We implemented our framework in PyTorch and trained the model end-to-end on one 16 GB NVIDIA Tesla V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluations on Private Dataset</head><p>Quantitative Assessment. In Table <ref type="table">1</ref>, we report the performance of ProtoAS-Net in AS severity classification against the black-box baselines for image (Huang et al. <ref type="bibr" target="#b8">[9]</ref>), video (Ginsberg et al. <ref type="bibr" target="#b5">[6]</ref>), as well as other prototypical methods, i.e. ProtoPNet <ref type="bibr" target="#b3">[4]</ref> and XProtoNet <ref type="bibr" target="#b11">[12]</ref>. In particular, for ProtoASNet, ProtoPNet <ref type="bibr" target="#b3">[4]</ref>, and XProtoNet <ref type="bibr" target="#b11">[12]</ref>, we conduct both image-based and video-based experiments with ResNet-18 and R(2+1)D-18 backbones respectively. We apply softmax to normalize the ProtoASNet output scores, including α, to obtain class probabilities that account for the presence of aleatoric uncertainty. We aggregate model predictions by averaging their probabilities from the image-(or clip-) level to obtain cine-and study-level predictions. We believe the uncertainty probabilities reduce the effect of less informative datapoints on final aggregated results. Additionally, the video-based models perform better than the image-based ones because the learnt prototypes can also capture AV motion which is an indicator of AS severity. These two factors may explain why our proposed method, ProtoASNet, outperforms all other methods for study-level classification.</p><p>Table <ref type="table">1</ref>. Quantitative results on the test set of our private dataset in terms of balanced accuracy (bACC), mean F1 score, and balanced mean absolute error (bMAE). bMAE is the average of the MAE of each class, assuming labels of 0, 1, 2 for no AS, early AS and significant AS respectively. Study-level results were calculated by averaging the prediction probabilities over all cines of each study. Results are shown as"mean(std)" calculated across five repetitions for each experiment. Best results are in bold. Qualitative Assessment. The interpretable reasoning process of ProtoASNet for a video example is shown in Fig. <ref type="figure" target="#fig_2">2</ref>. We observe that ProtoASNet places significant importance on prototypes corresponding to thickened AV leaflets due to calcification, which is a characteristic of both early and significant AS. Additionally, prototypes mostly capture the part of the heart cycle that aligns with the opening of the AV, providing a clinical indication of how well the valve opens up to be able to pump blood to the rest of the body. This makes ProtoASNet's reasoning process interpretable for the user. Note how the uncertainty prototypes focusing on AV regions where the valve leaflets are not visible, are contributing to the uncertainty measure, resulting in the case being flagged as uncertain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ablation Study. We assessed the effect of removing distinct components of our design: uncertainty prototypes (L abs , p u k ), clustering and separation (L clst , L sep ), and push mechanism. As shown in Table <ref type="table" target="#tab_1">2</ref>, keeping all the aforementioned components results in superior performance in terms of bACC and bMAE. We evaluated whether the model is capable of detecting its own misclassification using the value of α (or entropy of the class predictions in the case without L abs , p u k ). This is measured by the AUROC of detecting (y = ŷ). Learning p u k may benefit accuracy by mitigating the overfitting of p c k to poor-quality videos. Furthermore, α seems to be a stronger indicator for misclassification than entropy. Moreover, we measured prototype quality using diversity and sparsity <ref type="bibr" target="#b7">[8]</ref>, normalized by the total number of prototypes. Ideally, each prediction can be explained by a low number of prototypes (low s spars ) but different predictions are explained with different prototypes (high Diversity). When L clst and L sep are removed, the protoypes are less constrained, which contributes to stronger misclassification detection and more diversity, but reduce accuracy and cause explanations to be less sparse. Finally, the push mechanism improves performance, countering the intuition of an interpretability-performance trade-off. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation on TMED-2, a Public Dataset</head><p>We also applied our method to TMED-2, a public image-based dataset for AS diagnosis. Consistent with <ref type="bibr" target="#b9">[10]</ref>, images were fed to a WideResNet-based prototype model with two output branches. The view classifier branch used averagepooling of patches followed by a fully connected layer. However, the AS diagnosis branch used the prototype setup outlined in Methods. A diagram of the overall architecture is available in the supplementary material. We trained the model end-to-end with images from all views. During inference, images with high entropy in the predicted view and high aleatoric uncertainty for AS classification were discarded. Then, probabilities for PLAX and PSAX were used for weighted averaging to determine the study-level prediction. Addition of the prototypical layer and thresholding on predicted uncertainty achieves 79.7% accuracy for AS severity, outperforming existing black-box method <ref type="bibr" target="#b9">[10]</ref> at 74.6%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We introduce ProtoASNet, an interpretable method for classifying AS severity using B-mode echo that outperforms existing black-box methods. ProtoASNet identifies clinically relevant spatio-temporal prototypes that can be visualized to improve algorithmic transparency. In addition, we introduce prototypes for estimating aleatoric uncertainty, which help flag difficult-to-diagnose scenarios, such as videos with poor visual quality. Future work will investigate methods to optimize the number of prototypes, or explore out-of-distribution detection using prototype-based methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (A) An overview of our proposed ProtoASNet architecture. ProtoASNet extracts spatio-temporal feature vectors f p c k (x) from the video, which are compared with learned prototypes. Similarity values between features and prototypes are aggregated to produce a score for class membership and aleatoric uncertainty. (B) Prototypes representing aleatoric uncertainty (blue) can capture regions of the data distribution with inherent ambiguity (intersection between green and yellow regions). In practice, this region consists of videos with poor visual quality. (Color figure online)</figDesc><graphic coords="3,43,29,54,62,337,45,149,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>c k reflect those of true examples in the training distribution, they are projected ("pushed") towards the embeddings of the closest training examples of class c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of the ProtoASNet decision-making process for a test cine video showing significant AS but poor valve leaflet visualization. We visualize most similar video parts by overlaying the upsampled model-generated ROI, M p c k (xtest), on the test cine video. Likewise, we visualize prototypes by finding the training clip each prototype is drawn from, xp, and overlaying M p c k (xp). ProtoASNet explains which spatio-temporal parts of the test echo are most similar to the prototypes and how accumulation of these supporting evidence results in the prediction probabilities. More visualizations of our model's performance are included in the supplementary material in video format.</figDesc><graphic coords="8,56,46,54,32,339,13,141,37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the validation set of our private dataset. Method Clip-level (N = 1280) bACC ↑ bMAE ↓ AUROC y =ŷ ↑ sspars ↓ Diversity ↑ w/o L abs , p u</figDesc><table><row><cell>k</cell><cell>76.1</cell><cell>0.25</cell><cell>0.73</cell><cell>0.37</cell><cell>0.50</cell></row><row><cell cols="2">w/o L clst , Lsep 74.8</cell><cell>0.26</cell><cell>0.79</cell><cell>0.49</cell><cell>0.50</cell></row><row><cell>w/o push</cell><cell>77.9</cell><cell>0.23</cell><cell>0.75</cell><cell>0.35</cell><cell>0.43</cell></row><row><cell cols="2">All parts (ours) 78.4</cell><cell>0.23</cell><cell>0.75</cell><cell>0.33</cell><cell>0.45</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by the <rs type="funder">Canadian Institutes of Health Research (CIHR)</rs> and in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 36.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sanity checks for saliency maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Epidemiology of aortic valve stenosis (AS) and of aortic valve incompetence (AI): is the prevalence of AS/AI similar in different parts of the world</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Soc. Cardiol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ACC/AHA 2006 guidelines for the management of patients with valvular heart disease: a report of the American college of cardiology/american heart association task force on practice guidelines (writing committee to revise the 1998 guidelines for the management of patients with valvular heart disease) developed in collaboration with the society of cardiovascular anesthesiologists endorsed by the society for cardiovascular angiography and interventions and the society of thoracic surgeons</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Bonow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Coll. Cardiol</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="e148" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">This looks like that: deep learning for interpretable image recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning confidence for out-of-distribution detection in neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04865</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep video networks for automatic assessment of aortic stenosis in echocardiography</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ginsberg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87583-1_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87583-120" />
	</analytic>
	<monogr>
		<title level="m">ASMUS 2021</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Aylward</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Grimwood</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Min</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S.-L</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12967</biblScope>
			<biblScope unit="page" from="202" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Hierarchical explanations for video action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gulshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Van Noord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">2301</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">InsightR-Net: interpretable neural network for regression using similarity-based comparisons to prototypical examples</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Namburete</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new semi-supervised learning benchmark for classifying view and diagnosing aortic stenosis from echocardiograms</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Machine Learning for Healthcare Conference</title>
		<meeting>the 6th Machine Learning for Healthcare Conference</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TMED 2: a dataset for semisupervised classification of echocardiograms</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DataPerf: Benchmarking Data for Data-Centric AI Workshop</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interpretable and accurate fine-grained recognition via region grouping</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8662" to="8672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">XProtoNet: diagnosis in chest radiography with global and local explanations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15714" to="15723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On modelling label uncertainty in deep neural networks: automatic estimation of intra-observer variability in 2D echocardiography quality assessment</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1868" to="1883" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inconsistencies of echocardiographic criteria for the grading of aortic valve stenosis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Minners</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allgeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gohlke-Baerwolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Heart J</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1043" to="1048" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inconsistent grading of aortic valve stenosis by current guidelines: haemodynamic studies in patients with apparently normal left ventricular function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Minners</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allgeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gohlke-Baerwolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heart</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="1463" to="1468" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ACC/AHA guideline for the management of patients with valvular heart disease: a report of the American college of cardiology/American heart association joint committee on clinical practice guidelines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Coll. Cardiol. Found. Wash. DC</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="25" to="e197" />
			<date type="published" when="2020">2020. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Grad-CAM: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: visualising image classification models and saliency maps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<imprint>
			<publisher>Workshop Track Proceedings</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Patient screening for early detection of aortic stenosis (AS)review of current practice and future perspectives</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thoenes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Thorac. Dis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">5584</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interpretable and trustworthy deepfake detection via dynamic prototypes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rambhatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1973" to="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interpretable image recognition by constructing transparent embedding space</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="895" to="904" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
