<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maosong</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manman</forename><surname>Fei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiangdong</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luyan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lichi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
							<email>qianwang@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="243" to="252"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">D2F0266356BC21FF502AB86D9801EBEE</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_24</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Detection-free</term>
					<term>Contrastive Learning</term>
					<term>Pathology Image Classification</term>
					<term>Cervical Cancer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cervical cancer is a significant health burden worldwide, and computer-aided diagnosis (CAD) pipelines have the potential to improve diagnosis efficiency and treatment outcomes. However, traditional CAD pipelines have limitations due to the requirement of a detection model trained on a large annotated dataset, which can be expensive and timeconsuming. They also have a clear performance limit and low data utilization efficiency. To address these issues, we introduce a two-stage detection-free pipeline, incorporating pooling transformer and MoCo pretraining strategies, that optimizes data utilization for whole slide images (WSIs) while relying solely on sample-level diagnosis labels for training. The experimental results demonstrate the effectiveness of our approach, with performance scaling up as the amount of data increases. Overall, our novel pipeline has the potential to fully utilize massive data in WSI classification and can significantly improve cancer diagnosis and treatment. By reducing the reliance on expensive data labeling and detection models, our approach could enable more widespread and cost-effective implementation of CAD pipelines in clinical settings. Our code and model is available at https://github.com/thebestannie/Detection-free-MICCAI2023.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cervical cancer is a common and severe disease that affects millions of women globally, particularly in developing countries <ref type="bibr" target="#b8">[9]</ref>. Early diagnosis is vital for successful treatment, which can significantly increase the cure rate <ref type="bibr" target="#b16">[17]</ref>. In recent years, computer-aided diagnosis (CAD) methods have become an important tool in the fight against cervical cancer, as they aim to improve the accuracy and efficiency of diagnosis.</p><p>Several computer-aided cervical cancer screening methods have been proposed for whole slide images (WSIs) in the literature. Most of them are detectionbased methods, which typically contain a detection model as well as some postprocessing modules in their frameworks. For instance, Zhou et al. <ref type="bibr" target="#b29">[29]</ref> proposed a three-step framework for cervical thin-prep cytologic test (TCT) <ref type="bibr" target="#b11">[12]</ref>. The first step involves training a RetinaNet <ref type="bibr" target="#b12">[13]</ref> as a cell detection network to localize suspiciously abnormal cervical cells from WSIs. In the second step, the patches centered on these detected cells are processed through a classification model, to refine the judgment of whether they are positive or negative. Finally, the positive patches refined by the patch-level classification are further combined to produce an overall positive/negative diagnosis for the WSI at the sample level.</p><p>Some methods improve the final classification performance by improving the detection model to identify positive cells more reliably. Cao et al. <ref type="bibr" target="#b0">[1]</ref> improved the detection performance by incorporating clinical knowledge and attention mechanism into their cell detection model of AttFPN. Wei et al. <ref type="bibr" target="#b24">[24]</ref> adopted the Yolo <ref type="bibr" target="#b20">[20]</ref> architecture with a variety of convolution kernels of different sizes to accommodate diverse cell clusters. Other methods improve the classification performance by changing the post-processing modules behind the detection model. Cheng et al. <ref type="bibr" target="#b4">[5]</ref> proposed a progressive identification method that leveraged multi-scale visual cues to identify abnormal cells and then an RNN <ref type="bibr" target="#b27">[27]</ref> for sample-level classification. Zhang et al. <ref type="bibr" target="#b28">[28]</ref> used GAT <ref type="bibr" target="#b23">[23]</ref> to model the relation of the suspicious positive cells provided by detection, thus obtaining a global description of the WSI and performing sample-level classification.</p><p>These methods have achieved good results through continuous improvement on the detection-based pipeline, but there are some common drawbacks. First, they are not able to get rid of their reliance on detection models, which means they have a high need for expensive detection data labeling to train the detection model. Cervical cancer cell detection datasets involve labeling individual and small bounding boxes in a large number of cells. It often requires multiple experienced pathologists to annotate <ref type="bibr" target="#b14">[15]</ref>, which is very time-consuming and labor-intensive. Second, the widely used detection-based pipeline has not fully utilized the massive information in WSIs. A WSI is typically large (sized of about 20000 × 20000 pixels). A lot of data would be wasted if only a small part of annotated images (e.g., corresponding to positive cells and bounding boxes) was used as training data. Finally, many existing methods focus on detecting and classifying individual cells. The tendency to neglect effective integration of the overall information across the entire WSI results in poor performance in sample-level classification.</p><p>To address the aforementioned issues, we propose a detection-free pipeline in this paper, which does not rely on any detection model. Instead, our pipeline requires only sample-level diagnosis labels, which are naturally available in clinical scenarios and thus get rid of additional image labeling. To attain this goal, we have designed a two-stage pipeline as in Fig. <ref type="figure">1</ref>. In the coarse-grained stage, we crop and downsample a WSI into multiple images, and conduct sample-level classification roughly based on all resized images. The coarse-grained classification yields attention scores, from which we perform attention guided selection to localize these key patches from the original WSI. Then, in the fine-grained stage, we use these key patches for fine prediction of the sample. The two stages in our pipeline adopt the same network design (i.e., encoder + pooling trans-former), which makes our solution friendly to develop and to use. We also adopt contrastive learning to effectively utilize the massive information in WSIs when training the encoder for classification. As a summary, our pipeline surpasses previous detection-based methods and achieves state-of-the-art performance with large-scale training. Our experiments show that our method becomes more effective when increasing the data size for training. Moreover, while many pathological images are also based on WSIs, our pipeline has a high potential to extend to other pathological tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Fig. <ref type="figure">1</ref>. The overview of our proposed method. For feasibility of computation, we crop a WSI into mutiple images. The cropped images are passed through the coarse-grained and fine-grained stages, where only sample-level diagnosis labels of WSIs, instead of any additional manual labeling, are required for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-Stage Pipeline with Attention Guided Selection.</head><p>The overview of our two-stage pipeline is shown in Fig. <ref type="figure">1</ref>. The input WSI is typically too big to be directly processed by a common deep learning pipeline <ref type="bibr" target="#b17">[18]</ref>, so we crop each WSI into local images sized 1024 × 1024 pixels. The images are then processed through the coarse-grained and fine-grained stages in order to obtain the WSIlevel classification results, respectively. In general, the purpose of the coarsegrained stage is to replace the detection model and identify local images that may contain abnormal positive cells. The fine-grained stage then integrates these key regions, producing refined classification for the sample.</p><p>To complete sample-level classification, both stages share basically the same network architecture. The input images are first processed by a CNN encoder to extract features. Then, we propose the pooling transformer, which is modified from the basic transformer module in Sect. 2, to integrate these features for WSI classification. Additionally, the input images for both stages are 256 × 256. In the coarse-grained stage, in order to allow the model to examine as many local images as possible, we resize the cropped local images from 1024 × 1024 to 256 × 256. In the fine-grained stage, we enlarge suspicious local abnormality and thus crop input images to 256 × 256 from 1024 × 1024.</p><p>For the coarse-grained stage, after passing the resized local images through encoder and pooling transformer, we obtain a rough prediction result at the sample level. We then use the Cross-Entropy (CE) loss to minimize the difference between the predicted WSI label and the ground truth. In addition, we calculate the attention score to identify the local image inputs that are most likely to yield positive reading. We describe the attention score as</p><formula xml:id="formula_0">AS(x 0 , f) = Sof tmax( x 0 • f T d x0 )f,<label>(1)</label></formula><p>where x 0 represents classification token (which is a commonly used setting in transformer <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">22]</ref>), and d x0 is 512 in our implementation, f represents the feature vector of a certain input local image. After calculating attention scores, we preserve top-8 (resized) local images with the highest scores from the entire WSI for subsequent fine-grained classification.</p><p>Next, in the fine-grained stage, each local image that has passed attention guided selection is cropped into 16 patches of the size 256 × 256. We expect that those patches contain positive cells and are thus critical to diagnosis at the sample level. The network of the fine-grained stage is the same as that of the coarse-grained stage, but the weights of the encoder is pre-trained in an unsupervised manner (Sect. 2). The same CE loss supervised by sample-level ground truth is used for the fine-grained stage here. For inference, the output of the fine-grained stage will be treated as the final result of the test WSI. Pooling Transformer. We use a transformer network to aggregate features of multiple inputs and to derive the sample-level outcome in both coarse-grained and fine-grained stages. We have observed that different local images of the same sample often have patterns of grouped similarity (such as the first two images in the upper-right of Fig. <ref type="figure">1</ref>). For negative samples, most of the local images are similar with each other. For positive samples, the images of abnormal cells are inclined to be grouped into several clusters.</p><p>Therefore, inspired by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>, we propose pooling transformer that is effective to reduce the redundancy and distortion from the input images. The pooling transformer in Fig. <ref type="figure" target="#fig_0">2</ref> is designed to integrate all inputs toward the samplelevel diagnosis. To remove redundant features, between two transformer layers, we use the affinity propagation algorithm <ref type="bibr" target="#b7">[8]</ref> to cluster the inputs into several classes. Within each clustered class, we average the features and aggregate a single token. Finally, the classification (CLS) token is concatenated with all tokens after clustering-based pooling, and passed through the rest of the network to obtain the classification result. In this way, we find that the similar yet redundant input features can be fused, making the network more concise and efficient to calculate the attention between pooled features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Pre-training of Encoder.</head><p>To make full use of WSI data and provide a better feature encoder, inspired by MoCo <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref> and other contrastive learning methods <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref> pre-training on ImageNet <ref type="bibr" target="#b5">[6]</ref>, we also perform pretraining for fine-grained encoder on a large scale of pathology images. Generally, large-scale pre-training usually requires a massive dataset and a suitable loss function. For data, WSI naturally has the advantage of having a large amount of training data. A WSI (20000 × 20000) can be cropped into about 5000-6000 patches (256 × 256). Therefore, we only need 2,000-3,000 WSI samples to obtain a dataset that can even be compared to ImageNet in quantity. For the loss function, there are typically two ways: one is like MAE <ref type="bibr" target="#b9">[10]</ref> to model the loss function using masks, and the other is to use contrastive learning as in MoCo and CLIP <ref type="bibr" target="#b19">[19]</ref>. In our task, since the structural features of cells are relatively weak compared to natural images, it is not suitable to model the loss function using masks. Therefore, we adopt a contrastive learning approach.</p><p>Specifically, in the same training batch, a patch (256 × 256, the same to the input size of the fine-grained stage) and its augmented patch are treated as a positive pair (note that here "positive/negative" is defined in the context of contrastive learning), and their features are required to be as similar as possible. Meanwhile, their features are required to be as dissimilar as possible from those of other patches. So the loss function can be described as</p><formula xml:id="formula_1">L = - n i=0 ( f i • f ia |f i ||f ia | - n j=1 f i • f j |f i ||f j | ) (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>f i and f ia represent the positive pair, and f j represents another patch negatively paired with f i . Using this method, we can pre-train a feature encoder in an unsupervised manner and initialize it into our encoder for the fine-grained stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment and Results</head><p>Dataset and Experimental Setup. In this study, we have collected 5384 cervical cytopathological WSI by 20x lens, each with 20000 × 20000 pixels, from our collaborating hospitals. Among them, there 2853 negative samples, and 2531 positive samples (962 ASCUS, and 1569 high-level positive samples). All WSIs only have diagnosis labels at the sample level, without annotation boxes at the cell level. And all sample labels are strictly diagnosed according to the TBS <ref type="bibr" target="#b15">[16]</ref> criterion by a pathologist with 35 years of clinical experience. We conduct the experiment with initial learning rate of 1.0 × 10 -4 , batch size of 4, and SGD optimizer <ref type="bibr" target="#b21">[21]</ref> for 30 epochs each stage. For contrastive pre-training, we follow the settings of MoCov2 <ref type="bibr" target="#b2">[3]</ref> and trained for 300 epochs.</p><p>Comparison to SOTA Methods. In this section, we experiment to compare our method with popular state-of-the-art (SOTA) methods, which are all fully supervised and detection-based. To the best of our knowledge, there are few good methods to train cervical cancer classification models in weakly supervised or unsupervised learning ways. No methods can achieve the detection-free goal either.</p><p>All the detection-based methods are evaluated in the following way. First, we label a dataset with cell-level bounding boxes to train a detection model. The detection dataset has 3761 images and 7623 cell-level annotations. After obtaining the suspicious cell patches provided by the detection model, we use the subsequent classification models used in these SOTA works to classify them and obtain the final classification results. As shown in Table <ref type="table" target="#tab_0">1</ref> for fair five-fold cross-validation, our method outperforms all compared detection-based methods. While our method has a large margin with most methods in the table, the improvement against <ref type="bibr" target="#b28">[28]</ref> (top-ranked in current detection-based methods) is relatively limited. On one hand, in <ref type="bibr" target="#b28">[28]</ref>, GAT aggregates local patches that are detected by Retinanet. And the attention mechanism of GAT is similar with the transformer used in our pipeline to certain extent. On the other hand, the result implies that our coarse-grained task has replaced the role of cell detection in early works. Thus, we conclude that a detection model trained with an expensive annotated dataset is not necessary to build a CAD pipeline for cervical abnormality.</p><p>Ablation Study. In this section, we experiment to demonstrate the effectiveness of all the proposed parts in our pipeline. We divide all 5384 samples into five independent parts for five-fold cross-validation, and the results are shown in Table <ref type="table" target="#tab_1">2</ref>. Here, CG means the classification passes only the coarse-grained stage. As can be seen, its performance is low, in that the resized images sacrifices the resolution and thus perform poorly for image-based classification. FG refers to classifying in the fine-grained stage. It is worth noting that without the attention scores provided by the coarse-grained stage, we have no way of knowing which local images might contain suspicious positive cells. Thus, we use random selection to experiment for FG only, as exhaustively checking all local images is computationally forbidden. As can be seen, the classification result is the lowest because it lacks enough access to the key image content in WSIs. By combining the two stages for attention guided selection, it is effective to improve the classification performance compared to the two previous experiments. Here, for the cases of CG, FG and CG+FG, an original transformer network without clustering-based pooling is used. In addition, as shown in the last two rows of the table, both pooling transformer (PT) and unsupervised pretraining (CL) contribute to our pipeline. Ultimately, we combine them together to achieve the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample Numbers and Inference Time.</head><p>In order to further demonstrate the huge potential of our method, we also perform an ablation study on the number of samples used for training and compare the time consuming of the different methods. For the experiment of sample numbers, We compare the best fully supervised detection-based method (Retinanet+GAT <ref type="bibr" target="#b28">[28]</ref>) with ours under the sample numbers of 500, 1000, 2000, and 5384. As shown by Table <ref type="table" target="#tab_2">3</ref> and  left of Fig. <ref type="figure" target="#fig_1">3</ref>, the traditional detection-based method has quickly encountered a saturation bottleneck as the amount of data increases. Although our method initially has poorer performance, it has shown an impressive growth trend. And at our current maximum data number (5384), the proposed pipeline has already exceeded the performance of the detection-based method. The above results also demonstrate that our new pipeline method has greater potential, even though it requires no cell-level image annotation. For inference time consuming, as shown in right of Fig. <ref type="figure" target="#fig_1">3</ref>, our method has shorter inference time and a good balance between accuracy and inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Discussion</head><p>In this paper, we propose a novel two-stage detection-free pipeline for WSI classification of cervical abnormality. Our method does not rely on detection models and eliminates the need for expensive cell-level data annotation. By leveraging just sample-level diagnosis labels, we achieve results that are competitive with fully supervised detection-based methods. Through the use of the proposed pooling transformer and unsupervised pre-training, our method makes full use of information within WSIs, resulting in improved efficiency in the use of pathological images. Importantly, our method offers even greater advantages with increasing amounts of data. And also, by utilizing attention weights, we can calculate attention scores to visually represent the importance of each image in the sample, making it easier for doctors to make judgments. Relevant visualization results can be found on our project homepage. Admittedly, our method has some limitations, such as slow training. Accelerating the training of massive data can be our next optimization direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Details of pooling transformer. It contains a token pooling layer in the middle of two transformer layers to cluster and aggregate redundant tokens.</figDesc><graphic coords="4,44,79,433,82,334,69,96,10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of accuracy and inference time consuming for different methods.</figDesc><graphic coords="8,44,79,212,21,334,51,113,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,57,96,215,69,336,49,160,39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with SOTA methods (%).</figDesc><table><row><cell>Method</cell><cell>Accuracy↑</cell><cell>Precision↑</cell><cell>Recall↑</cell><cell>F1-Score↑</cell></row><row><cell cols="2">AttFPN+Average [1] 78.33 ± 1.51</cell><cell>71.23 ± 2.10</cell><cell>79.39 ± 1.56</cell><cell>74.10 ± 2.33</cell></row><row><cell cols="2">RetinaNet+MLP [29] 74.31 ± 2.31</cell><cell>65.34 ± 3.57</cell><cell>78.12 ± 1.59</cell><cell>73.44 ± 2.78</cell></row><row><cell cols="2">RetinaNet+SVM [29] 72.37 ± 1.40</cell><cell>73.96 ± 0.79</cell><cell>77.38 ± 0.88</cell><cell>75.86 ± 0.95</cell></row><row><cell>LRModel+RNN [5]</cell><cell>80.55 ± 1.53</cell><cell>74.59 ± 1.66</cell><cell>82.31 ± 1.74</cell><cell>78.51 ± 1.32</cell></row><row><cell cols="2">RetinaNet+GAT [28] 83.74 ± 1.35</cell><cell cols="2">80.38 ± 1.43 84.58 ± 1.48</cell><cell>81.93 ± 1.02</cell></row><row><cell cols="3">Ours (Detection-Free) 83.84 ± 1.56 78.36 ± 1.23</cell><cell cols="2">85.22 ± 0.98 82.12 ± 0.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of our proposed methods. CG indicates coarse-grained classification, FG indicates fine-grained classification, PT indicates pooling transformer, and CL indicates contrastive learning.</figDesc><table><row><cell cols="3">Configuration</cell><cell></cell><cell>Metric (%)</cell></row><row><cell cols="5">CG FG PT CL ACC</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 Score</cell></row><row><cell cols="2">✔ -</cell><cell>-</cell><cell>-</cell><cell>74.96 ± 1.23 71.39 ± 1.21 82.49 ± 1.39 75.34 ± 1.66</cell></row><row><cell>-</cell><cell cols="2">✔ -</cell><cell>-</cell><cell>73.11 ± 2.10 70.48 ± 1.45 81.59 ± 1.45 74.09 ± 1.49</cell></row><row><cell cols="3">✔ ✔ -</cell><cell>-</cell><cell>79.72 ± 1.30 74.64 ± 1.34 84.45 ± 1.95 78.48 ± 1.23</cell></row><row><cell cols="4">✔ ✔ ✔ -</cell><cell>81.34 ± 1.56 77.36 ± 1.23 84.79 ± 0.98 80.72 ± 0.93</cell></row><row><cell cols="5">✔ ✔ ✔ ✔ 83.84 ± 1.56 78.36 ± 1.23 85.22 ± 0.98 82.12 ± 0.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Alation study on sample number between Retinanet+GAT<ref type="bibr" target="#b28">[28]</ref> and ours (%). RetinaNet+GAT 77.59 ± 1.33 71.32 ± 1.39 75.23 ± 1.03 73.85 ± 1.45 Ours 70.34 ± 2.58 64.49 ± 1.98 71.49 ± 2.49 68.88 ± 2.36 1000 RetinaNet+GAT 79.85 ± 1.70 75.60 ± 1.26 80.06 ± 0.96 77.96 ± 1.50 Ours 74.47 ± 1.29 70.23 ± 1.21 76.45 ± 0.73 73.96 ± 0.78 2000 RetinaNet+GAT 81.59 ± 1.09 78.12 ± 1.73 82.37 ± 1.55 80.79 ± 1.38 Ours 78.10 ± 1.32 73.88 ± 1.38 79.93 ± 1.06 77.39 ± 1.08 5384 RetinaNet+GAT 83.74 ± 1.35 80.38 ± 1.43 84.58 ± 1.48 81.93 ± 1.02 Ours 83.84 ± 1.56 78.36 ± 1.23 85.22 ± 0.98 82.12 ± 0.93</figDesc><table><row><cell>Number Method</cell><cell>Accuracy↑ Precision↑ Recall↑</cell><cell>F1-Score↑</cell></row><row><cell>500</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A novel attention-guided convolutional network for the detection of abnormal cervical cells in cervical cancer screening</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102197</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">PSViT: better vision transformer via token pooling and attention sharing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03428</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9640" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust whole slide image analysis for cervical cancer screening using deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Clustering by passing messages between data points</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="issue">5814</biblScope>
			<biblScope unit="page" from="972" to="976" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">World health organization call for action to eliminate cervical cancer globally</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gultekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Broutet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hutubessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Gynecol. Cancer</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="426" to="427" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The papanicolaou test for cervical cancer detection: a triumph and a tragedy</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Koss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="737" to="743" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03860</idno>
		<title level="m">Token pooling in vision transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A cervical histopathology dataset for computer aided diagnosis of precancerous lesions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1531" to="1541" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Wilbur</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-11074-5</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-11074-5" />
		<title level="m">The Bethesda System for Reporting Cervical Cytology: Definitions, Criteria, and Explanatory Notes</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cervical pap smear study and its utility in cancer screening, to specify the strategy for cervical cancer control</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Modi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National J. Commun. Med</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="49" to="51" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DGMIL: distribution guided multiple instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-73" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: an incremental improvement</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="page" from="400" to="407" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An efficient cervical whole slide image analysis framework based on multi-scale semantic and spatial deep features</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15113</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Whole slide cervical cancer screening using graph attention network and supervised contrastive learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-720" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical pathology screening for cervical abnormality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiaping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">101892</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
