<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound</title>
				<funder ref="#_v6rCCBV">
					<orgName type="full">Key Research and Development Plan of Jiangsu Province</orgName>
				</funder>
				<funder ref="#_WBFjSyM #_U8XhXsa #_TMBqatT">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_9QcDTqt">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_wkrsy9K">
					<orgName type="full">National Nature Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhi</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Brain-Machine Intelligence Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>210016</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weijing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Ultrasound</orgName>
								<orgName type="department" key="dep2">The Affiliated Hospital</orgName>
								<orgName type="institution" key="instit1">Nanjing Drum Tower Hosptial</orgName>
								<orgName type="institution" key="instit2">Nanjing University Medical School</orgName>
								<address>
									<postCode>210008</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Keke</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Ultrasound</orgName>
								<orgName type="department" key="dep2">The Affiliated Hospital</orgName>
								<orgName type="institution" key="instit1">Nanjing Drum Tower Hosptial</orgName>
								<orgName type="institution" key="instit2">Nanjing University Medical School</orgName>
								<address>
									<postCode>210008</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Di</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Ultrasound</orgName>
								<orgName type="department" key="dep2">The Affiliated Hospital</orgName>
								<orgName type="institution" key="instit1">Nanjing Drum Tower Hosptial</orgName>
								<orgName type="institution" key="instit2">Nanjing University Medical School</orgName>
								<address>
									<postCode>210008</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daoqiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Brain-Machine Intelligence Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>210016</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongen</forename><surname>Liao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Fang</forename><surname>Chen</surname></persName>
							<email>chenfang@nuaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Brain-Machine Intelligence Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>210016</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="159" to="168"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F313918DEA210E5F6CC66D485EE7E87A</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_16</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Musculoskeletal ultrasound</term>
					<term>Gout diagnosis</term>
					<term>Gaze tracking</term>
					<term>Reasonability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore the potential of deep convolutional neural network (CNN) models for differential diagnosis of gout from musculoskeletal ultrasound (MSKUS), as no prior study on this topic is known. Our exhaustive study of state-of-the-art (SOTA) CNN image classification models for this problem reveals that they often fail to learn the gouty MSKUS features, including the double contour sign, tophus, and snowstorm, which are essential for sonographers' decisions. To address this issue, we establish a framework to adjust CNNs to "think like sonographers" for gout diagnosis, which consists of three novel components:</p><p>(1) Where to adjust: Modeling sonographers' gaze map to emphasize the region that needs adjust; (2) What to adjust: Classifying instances to systematically detect predictions made based on unreasonable/biased reasoning and adjust; (3) How to adjust: Developing a training mechanism to balance gout prediction accuracy and attention reasonability for improved CNNs. The experimental results on clinical MSKUS datasets demonstrate the superiority of our method over several SOTA CNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gout is the most common inflammatory arthritis and musculoskeletal ultrasound (MSKUS) scanning is recommended to diagnose gout due to the non-ionizing radiation, fast imaging speed, and non-invasive characteristics of MSKUS <ref type="bibr" target="#b6">[7]</ref>. However, misdiagnosis of gout can occur frequently when a patient's clinical characteristics are atypical. Traditional MSKUS diagnosis relies on the experience of the radiologist which is time-consuming and labor-intensive. Although convolutional neural networks (CNNs) based ultrasound classification models have been successfully used for diseases such as thyroid nodules and breast cancer, conspicuously absent from these successful applications is the use of CNNs for gout diagnosis from MSKUS images. There are significant challenges in CNN based gout diagnosis. Firstly, the gout-characteristics contain various types including double contour sign, synovial hypertrophy, synovial effusion, synovial dislocation and bone erosion, and these gout-characteristics are small and difficult to localize in MSKUS. Secondly, the surrounding fascial tissues such as the muscle, sarcolemma and articular capsule have similar visual traits with gout-characteristics, and we found the existing CNN models can't accurately pay attention to the gout-characteristics that radiologist doctors pay attention to during the diagnosis process (as shown in Fig. <ref type="figure" target="#fig_0">1</ref>). Due to these issues, SOTA CNN models often fail to learn the gouty MSKUS features which are key factors for sonographers' decision.</p><p>In medical image analysis, recent works have attempted to inject the recorded gaze information of clinicians into deep CNN models for helping the models to predict correctly based on lesion area. Mall et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> modeled the visual search behavior of radiologists for breast cancer using CNN and injected human visual attention into CNN to detect missing cancer in mammography. Wang et al. <ref type="bibr" target="#b14">[15]</ref> demonstrated that the eye movement of radiologists can be a new supervision form to train the CNN model. Cai et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> developed the SonoNet <ref type="bibr" target="#b0">[1]</ref> model, which integrates eye-gaze data of sonographers and used Generative Adversarial Networks to address the lack of eye-gaze data. Patra et al. <ref type="bibr" target="#b10">[11]</ref> proposed the use of a teacher-student knowledge transfer framework for US image analysis, which combines doctor's eye-gaze data with US images as input to a large teacher model, whose outputs and intermediate feature maps are used to condition a student model. Although these methods have led to promising results, they can be difficult to implement due to the need to collect doctors' eye movement data for each image, along with certain restrictions on the network structure.</p><p>Different from the existing studies, we propose a novel framework to adjust the general CNNs to "think like sonographers" from three different levels. <ref type="bibr" target="#b0">(1)</ref> Where to adjust: Modeling sonographers' gaze map to emphasize the region that needs adjust; (2) What to adjust: Classify the instances to systemically detect predictions made based on unreasonable/biased reasoning and adjust; (3) How to adjust: Developing a training mechanism to strike the balance between gout prediction accuracy and attention reasonability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Fig. <ref type="figure">2</ref>. The overall framework of the proposed method.</p><p>Figure <ref type="figure">2</ref> presents the overall framework, which controls CNNs to "think like sonographers" for gout diagnosis from three levels. 1) Where to adjust: we model the sonographers' gaze map to emphasize the region that needs control. This part learns the eye gaze information of the sonographers which is collected by the Eye-Tracker. 2) What to adjust: we divide instances into four categories to reflect whether the model prediction given to the instance is reasonable and precise. 3) How to adjust: a training mechanism is developed to strike the balance between gout diagnosis and attention accuracy for improving CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Where to Adjust</head><p>It is essential to obtain the gaze map corresponding to each MSKUS to emphasize the region where gouty features are obvious. Inspired by studies of saliency model <ref type="bibr" target="#b7">[8]</ref>, we integrate transformer into CNNs to capture multi-scale and longrange contextual visual information for modeling sonographers' gaze map. This gaze map learns the eye gaze information, collected by the Eye-Tracker, of the sonographers when they perform diagnosis. As shown in Fig. <ref type="figure">2</ref>, this part consists of a CNN encoder for extracting multi-scale feature, a transformer-encoder for capturing long-range dependency, and a CNN decoder for predicting gaze map.</p><p>The MSKUS image I 0 ∈ R H×W ×3 is first input into CNN encoder that contains five convolution blocks. The output feature maps from the deeper last three convolution blocks are denoted as F 0 , F 1 , F 2 and are respectively fed into transformer encoders to enhance the long-range and contextual information. During the transformer-encoder, we first flatten the feature maps produced by the CNN encoder into a 1D sequence. Considering that flatten operation leads to losing the spatial information, the absolute position encoding <ref type="bibr" target="#b13">[14]</ref> is combined with the flatten feature map via element-wise addition to form the input of the transformer layer. The transformer layer contains the standard Multi-head Self-Attention (MSA) and Multi-layer Perceptron (MLP) blocks. Layer Normalization (LN) and residual connection are applied before and after each block respectively.</p><p>In the CNN decoder part, a pure CNN architecture progressively up-samples the feature maps into the original image resolution and implements pixel-wise prediction for modeling sonographers' gaze map. The CNN decoder part includes five convolution blocks. In each block, 3 × 3 convolution operation, Batch normalization (BN), RELU activation function, and 2-scale upsampling that adopts nearest-neighbor interpolation is performed. In addition, the transformer's output is fused with the feature map from the decoding process by an element-wise product operation to further enhance the long-range and multi-scale visual information. After five CNN blocks, a 3 × 3 convolution operation and Sigmoid activation is performed to output the predicted sonographers' gaze map. We use the eye gaze information of the sonographers which is collected by the Eye-Tracker to restrain the predicted sonographers' gaze map. The loss function is the sum of the Normalized Scanpath Saliency (NSS), the Linear Correlation Coefficient (CC), Kullback-Leibler divergence (KLD) and Similarity (SIM) <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">What to Adjust</head><p>Common CNN classification models for gout diagnosis often fail to learn the gouty MSKUS features including the double contour sign, tophus, and snowstorm which are key factors for sonographers' decision. A CAM for a particular category indicates the discriminative regions used by the CNN to identify that category. Inspired by CAM technique, it is needed to decide whether the attention region given to an CNN model is reasonable for diagnosis of gout. We firstly use the Grad-CAM technique <ref type="bibr" target="#b11">[12]</ref> to acquire the salient attention region S CAM that CNN model perceives for differential diagnosis of gout. To ensure the scale of the attention region S CAM is the same as the sonographers' gaze map S sono which is modeled by saliency model, we normalize S CAM to the values between 0 and 1, get S CAM . Then we make bit-wise intersection over union(IoU) operations with the S sono and S CAM to measure how well the two maps overlap. Note that we only calculate the part of S CAM that is greater than 0.5. For instances whose IoU is less than 50%, we consider that the model's prediction for that instance is unreasonable. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, when CNN do prediction, we can divide the instances into four categories: RP: Reasonable Precise: The attention region focusses on the gouty features which are important for sonographers' decision, and the diagnosis is precise. RIP: Reasonable Imprecise: Although attention region focusses on the gouty features, while the diagnosis result is imprecise. UP: Unreasonable Precise: Although the gout diagnosis is precise, amount of attention is given to irrelevant feature of MSKUS image. UIP: Unreasonable Imprecise: The attention region focusses on irrelevant features, and the diagnosis is imprecise.</p><p>Our target of adjustment is to reduce imprecise and unreasonable predictions. In this way, CNNs not only finish correct gout diagnosis, but also acquire the attention region that agreements with the sonographers' gaze map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">How to Adjust</head><p>We proposed a training mechanism (Algorithm 1) which can strike the balance between the gout diagnosis error and the reasonability error of attention region to promote the CNNs to "think like sonographers". In addition to reducing the diagnosis error, we also want to minimize the difference between sonographers' gaze map S sono and normalized salient attention region S CAM , which directly leads to our target: </p><formula xml:id="formula_0">L reasonability = L 1 ( S CAM , S sono )</formula><p>The total loss function can be expressed as the weighted sum the gout diagnosis error and the reasonability error, as follows:</p><formula xml:id="formula_1">L total = αL diagnosis + (1 -α)L reasonability</formula><p>The gout diagnosis error L diagnosis is calculated by the Cross-entropy loss, and the reasonability is calculated by the L1-loss. This training mechanism uses the quadrant of instances to identify whether samples' attention needs to adjusted. For MSKUS sample in the quadrant of UP, α can be set 0.2 to control the CNN pay more attention to reasonability. Correspondingly, for sample in RIP, α can be set 0.8 to make CNN pay more attention to precise. For sample in RP and UIP, α can be set 0.5 to strike the balance between accuracy and reasonability. Gaze Data Collection. We collected the eye movement data with the Tobii 4C eye-tracker operating at 90 Hz. The MSKUS images were displayed on a 1920 × 1080 27-inch LCD screen. The eye tracker was attached beneath the screen with a magnetic mounting bracket. Sonographers were seated in front of the screen and free to adjust the chair's height and the display's inclination. Binary maps of the same size as the corresponding MSKUS images were generated using the gaze data, with the pixel corresponding to the point of gaze marked with a'1' and the other pixels marked with a'0'. A sonographer gaze map S was generated for each binary map by convolving it with a truncated Gaussian Kernel G(σ x,y ), where G has 299 pixels along x dimension, and 119 pixels along y dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSKUS</head><p>Evaluation Metrics. Five metrics were used to evaluate model performance: Accuracy (ACC), Area Under Curve (AUC), Correlation Coefficient (CC), Similarity (SIM) and Kullback-Leibler divergence (KLD) <ref type="bibr" target="#b1">[2]</ref>. ACC and AUC were implemented to assess the gout classification performance of each model, while CC, SIM, and KLD were used to evaluate the similarity of the areas that the model and sonographers focus on during diagnoses.</p><p>Evaluation of "Thinking like Sonographers" Mechanism. To evaluate the effectiveness of our proposed mechanism of "Thinking like Sonographers" (TLS) that combines "where to adjust", "what to adjust" and "how to adjust", we compared the gout diagnosis results of several classic CNN classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13</ref>] models without/with our TLS mechanism. The results, shown in Table <ref type="table" target="#tab_0">1</ref>, revealed that using our TLS mechanism led to a significant improvement in all metrics. Specifically, for ACC and AUC, the model with our TLS mechanism achieved better results than the model without it. Resnet34 with TLS acquired the highest improvement in ACC with a 4.41% increase, and Resnet18 with TLS had a 0.027 boost in AUC. Our TLS mechanism consistently performed well in improving the gout classification performance of the CNN models. More comparison results were shown in Appendix Fig. <ref type="figure" target="#fig_2">A1</ref> and Fig. <ref type="figure">A2</ref>. The CC, SIM, and KLD metrics were utilized to assess the similarity between the CAMs of classification models and the collected gaze maps, providing an indication of whether the model was able to "think" like a sonographer. Table <ref type="table" target="#tab_0">1</ref> showed that the models with our TLS mechanism achieved significantly better results in terms of CC and SIM (i.e., higher is better), as well as a decline of more than 1.50 in KLD (lower is better), when compared to the original models. This indicated that the models with TLS focused on the areas shown to be similar to the actual sonographers. Furthermore, Fig. <ref type="figure" target="#fig_4">4</ref> illustrated the qualitative results of CAMs of models with and without TLS mechanism. The original models without TLS paid more attention to noise, textures, and artifacts, resulting in unreasonable gout diagnosis. With TLS, however, models could focus on the crucial areas in lesions, allowing them to think like sonographers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stability Under Different Gaze Maps via t-Test.</head><p>To evaluate the prediction's stability under the predicted gaze map from the generation model in "Where to adjust", we conducted three t-test studies. Specifically, we trained two classification models (M C and M P ), using the actual collected gaze maps, and the predicted maps from the generation model, respectively. During the testing, we used the collected maps as input for M C and M P to get classification results R CC and R P C . Similarly, we used the predicted maps as input for M C andM P As shown in Table <ref type="table" target="#tab_1">2</ref>, the p-values of t-test (1)( <ref type="formula">2</ref>) and ( <ref type="formula">3</ref>) are all greater than 0.005, suggesting that no significant difference was observed between the classification results obtained from different generative strategies. This implied that our training mechanism was model-insensitive. Consequently, it was possible to use predicted gaze maps for both the training and testing phases of the classification models without any notable performance decrease. This removed the need to collect eye movement maps during the training and testing phases, significantly lightening the workload of data collection. Therefore, our TLS mechanism, which involved predicting the gaze maps, could potentially be used in clinical environments. This would allow us to bypass the need to collect the real gaze maps of the doctors while classifying newly acquired US images, and thus improved the clinical implications of our mechanism, "Thinking like Sonographers". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we propose a framework to adjust CNNs to "think like sonographers", and diagnose gout from MSKUS images. The mechanism of "thinking like sonographers" contains three levels: where to adjust, what to adjust, and how to adjust. The proposed design not only steers CNN models as we intended, but also helps the CNN classifier focus on the crucial gout features. Extensive experiments show that our framework, combined with the mechanism of "thinking like sonographers" improves performance over the baseline deep classification architectures. Additionally, we can bypass the need to collect the real gaze maps of the doctors during the classification of newly acquired MSKUS images, thus our method has good clinical application values.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) MSKUS images. Yellow boxes denote the gaze areas of the sonographers and red arrows denote the surrounding fascial tissues; (b) Grad-Cam visual of ResNet18; (c) Grad-Cam visual of Densenet121; (d) Grad-Cam visual of our method. (Color figure online)</figDesc><graphic coords="2,92,31,121,52,239,83,106,63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Four categories: (a) RP (b) RIP (c) UP (d) UIP. Yellow boxes denote the gaze areas of the sonographers.</figDesc><graphic coords="5,261,06,128,63,138,40,131,53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Proposed training mechanism Input:D train , O target , M attn generated from saliency model, W base parameters of base model needed to be adjusted Output:Optimized model parameters W * O pred , M cam = model(D train |W base ); split D train into 4 categories based on (O pred , O target ) and (M cam , M attn ); set the α base on the ratio of 4 categories; W * = W base ; for epoch = {1 ... , N } do W * = train(W * , D train , M attn , α); end min(L diagnosis + L reasonability )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Dataset Collection. The MSKUS data were collected for patients suspected of metatarsal gout in Nanjing Drum Tower Hosptial. Informed written consent was obtained at the time of recruitment. Dataset totally contains 1127 US images from different patients including 509 gout images and 618 healthy images. The resolution of the MSKUS images were resized to 224 × 224. During experiments, we randomly divided 10% of the dataset into testing sets, then the remaining data was divided equally into two parts for the different phases of the training. We used 5-fold cross validation to divide the training sets and validation sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Grad-CAM for ResNet18, ResNet34, ResNet50, vgg16 and Densenet121. Yellow boxes denote sonographers' gaze areas and red arrows denote the fascial tissues. (Color figure online)</figDesc><graphic coords="8,84,81,53,69,255,01,165,01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,56,46,202,64,340,00,189,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The performances of models training wi/wo our mechanism in MSKUS.</figDesc><table><row><cell>Method</cell><cell>ACC↑ (%)</cell><cell>AUC↑</cell><cell>CC↑</cell><cell>SIM↑</cell><cell>KLD↓</cell></row><row><cell>Resnet18 wo TLS</cell><cell cols="5">86.46 ± 3.90 0.941 ± 0.023 0.161 ± 0.024 0.145 ± 0.011 3.856 ± 0.235</cell></row><row><cell>Resnet18 wi TLS</cell><cell cols="5">89.13 ± 3.32 0.968 ± 0.005 0.404 ± 0.004 0.281 ± 0.003 1.787 ± 0.017</cell></row><row><cell>Resnet34 wo TLS</cell><cell cols="5">83.15 ± 3.78 0.922 ± 0.024 0.190 ± 0.054 0.151 ± 0.020 3.500 ± 0.530</cell></row><row><cell>Resnet34 wi TLS</cell><cell cols="5">87.56 ± 1.89 0.947 ± 0.018 0.376 ± 0.006 0.252 ± 0.004 1.951 ± 0.043</cell></row><row><cell>Resnet50 wo TLS</cell><cell cols="5">88.82 ± 1.16 0.956 ± 0.008 0.189 ± 0.024 0.157 ± 0.013 4.019 ± 0.522</cell></row><row><cell>Resnet50 wi TLS</cell><cell cols="5">89.61 ± 3.05 0.967 ± 0.011 0.402 ± 0.028 0.298 ± 0.020 2.133 ± 0.232</cell></row><row><cell>Vgg16 wo TLS</cell><cell cols="5">89.13 ± 3.20 0.958 ± 0.021 0.221 ± 0.089 0.182 ± 0.044 3.461 ± 0.776</cell></row><row><cell>Vgg16 wi TLS</cell><cell cols="5">91.50 ± 2.88 0.966 ± 0.020 0.416 ± 0.020 0.305 ± 0.013 1.932 ± 0.084</cell></row><row><cell cols="6">DenseNet121 wo TLS 88.82 ± 2.08 0.956 ± 0.015 0.175 ± 0.030 0.152 ± 0.011 3.822 ± 0.599</cell></row><row><cell cols="6">DenseNet121 wi TLS 89.45 ± 1.62 0.965 ± 0.010 0.368 ± 0.011 0.239 ± 0.007 1.991 ± 0.062</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Statistical test results</figDesc><table><row><cell cols="5">Method/p-value ResNet18 ResNet34 ResNet50 Vgg16 DenseNet121</cell></row><row><cell>t-test(1)</cell><cell>0.4219</cell><cell>0.8719</cell><cell>0.8701</cell><cell>0.6281 0.4428</cell></row><row><cell>t-test(2)</cell><cell>0.4223</cell><cell>0.8700</cell><cell>0.8725</cell><cell>0.6272 0.4434</cell></row><row><cell>t-test(3)</cell><cell>0.4192</cell><cell>0.8714</cell><cell>0.8727</cell><cell>0.6191 0.4446</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported in part by <rs type="funder">National Nature Science Foundation of China</rs> grants (<rs type="grantNumber">62271246</rs>, <rs type="grantNumber">U20A20389</rs>, <rs type="grantNumber">82027807</rs>, <rs type="grantNumber">U22A2051</rs>), <rs type="funder">Key Research and Development Plan of Jiangsu Province</rs> (No. <rs type="grantNumber">BE2022842</rs>), <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2022YFC2405200</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wkrsy9K">
					<idno type="grant-number">62271246</idno>
				</org>
				<org type="funding" xml:id="_WBFjSyM">
					<idno type="grant-number">U20A20389</idno>
				</org>
				<org type="funding" xml:id="_U8XhXsa">
					<idno type="grant-number">82027807</idno>
				</org>
				<org type="funding" xml:id="_v6rCCBV">
					<idno type="grant-number">U22A2051</idno>
				</org>
				<org type="funding" xml:id="_9QcDTqt">
					<idno type="grant-number">BE2022842</idno>
				</org>
				<org type="funding" xml:id="_TMBqatT">
					<idno type="grant-number">2022YFC2405200</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 16.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sononet: real-time detection and localisation of fetal standard scan planes in freehand ultrasound</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Baumgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2204" to="2215" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What do different evaluation metrics tell us about saliency models?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="740" to="757" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-task SonoEyeNet: detection of fetal standardized planes assisted by generated sonographer attention maps</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00928-1_98</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00928-198" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11070</biblScope>
			<biblScope unit="page" from="871" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sonoeyenet: standardized fetal ultrasound plane detection informed by eye tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1475" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning in medical ultrasound analysis: a review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="275" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transalnet: towards perceptually relevant visual saliency prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">494</biblScope>
			<biblScope unit="page" from="455" to="467" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling visual search behavior of breast radiologists using a deep convolution neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mello-Thoms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="35502" to="035502" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Missed cancer and visual search of mammograms: what feature-based machine-learning can tell us that deep-convolution learning cannot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Krupinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mello-Thoms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Observer Performance, and Technology Assessment</title>
		<imprint>
			<biblScope unit="volume">10952</biblScope>
			<biblScope unit="page" from="281" to="287" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Image Perception</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient ultrasound image analysis models with sonographer gaze assisted distillation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Patra</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-943" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="394" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradcam: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Follow my eye: using gaze to supervise computer-aided diagnosis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1688" to="1698" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
