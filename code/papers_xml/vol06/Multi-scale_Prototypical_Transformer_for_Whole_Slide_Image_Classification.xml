<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-scale Prototypical Transformer for Whole Slide Image Classification</title>
				<funder ref="#_NTPjzHm">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_TpwshKQ">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Saisai</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information Engineering</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information Engineering</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juncheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information Engineering</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jun</forename><surname>Shi</surname></persName>
							<email>junshi@shu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information Engineering</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-scale Prototypical Transformer for Whole Slide Image Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="602" to="611"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">EACDE11428E065847AC011B4D1ACFE04</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_58</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Whole slide image</term>
					<term>Multiple instance learning</term>
					<term>Multi-scale feature</term>
					<term>Prototypical Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Whole slide image (WSI) classification is an essential task in computational pathology. Despite the recent advances in multiple instance learning (MIL) for WSI classification, accurate classification of WSIs remains challenging due to the extreme imbalance between the positive and negative instances in bags, and the complicated pre-processing to fuse multi-scale information of WSI. To this end, we propose a novel multi-scale prototypical Transformer (MSPT) for WSI classification, which includes a prototypical Transformer (PT) module and a multi-scale feature fusion module (MFFM). The PT is developed to reduce redundant instances in bags by integrating prototypical learning into the Transformer architecture. It substitutes all instances with cluster prototypes, which are then re-calibrated through the self-attention mechanism of Transformer. Thereafter, an MFFM is proposed to fuse the clustered prototypes of different scales, which employs MLP-Mixer to enhance the information communication between prototypes. The experimental results on two public WSI datasets demonstrate that the pro-posed MSPT outperforms all the compared algorithms, suggesting its potential applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Histopathological images are regarded as the 'gold standard' in the diagnosis of cancers. With the advent of the whole slide image (WSI) scanner, deep learning has gained its reputation in the field of computational pathology <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. However, WSIs are extremely large in the size and lack of pixel-level annotations, making it difficult to adopt the traditional supervised learning methods for WSI classification <ref type="bibr" target="#b3">[4]</ref>.</p><p>To address this issue, multiple instance learning (MIL) has been successfully applied to the WSI classification task as a weakly supervised learning problem <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. In this context, a WSI is considered as a bag, and the cropped patches within the slide are the instances in this bag. However, the lesion regions usually only account for a small portion of the WSI, resulting in a large number of negative patches. When the positive and negative instances in the bag are highly imbalanced, the MIL models are prone to incorrectly discriminate these positive instances when using simple aggregation operations. To this end, several attention-based MIL models, such as ABMIL <ref type="bibr" target="#b7">[8]</ref> and DSMIL <ref type="bibr" target="#b8">[9]</ref>, apply variants of the attention mechanism to re-weight instance features. Thereafter, the recent works develop the Transformer-based architectures to better model long-range instance correlations via self-attention <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b13">[13]</ref>. However, since the average bag size of a WSI is more than 8000 at 20 Ã— magnification, it is computationally infeasible to use the conventional Transformer and other stacked self-attention network architectures in MIL-related tasks.</p><p>Recently, prototypical learning is applied in WSI analysis to identify representative instances in the bag <ref type="bibr" target="#b14">[14]</ref>. Some works adopt the K-means clustering on all instances in a bag to obtain K cluster centers i.e., instance prototypes, and then use these prototypes to represent the bags <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. These clustering-based MIL algorithms can significantly reduce the redundant instances, and thereby improving the training efficiency for WSI classification. However, it is different for K-means to specify the cluster number as well as the initial cluster centers, and different initial values may lead to different cluster results, thus affecting the performance of MIL. Besides, affected by the feature extractor, the clustering-based MIL algorithms may ignore the most important instances that contain critical diagnostic information. Therefore, it is necessary to develop a method that can fully exploit the potential complementary information between critical instances and prototypes to improve representation learning of prototypes.</p><p>On the other hand, when pathologists analysis the WSIs, they always observe the tissues at various resolutions <ref type="bibr" target="#b17">[17]</ref>. Inspired by this diagnostic manner, some works use multi-scale information of WSIs to improve diagnostic accuracy. For example, Li et al. <ref type="bibr" target="#b8">[9]</ref> adopted a pyramidal concatenation mechanism to fuse the multi-scale features of WSIs, in which the feature vectors of low-resolution patches are replicated and concatenated with the those of their corresponding high-resolution patches; Hou et al. <ref type="bibr" target="#b18">[18]</ref> propose a heterogeneous graph neural network to learn the hierarchical representation of WSIs from a heterogeneous graph, which is constructed by the feature and spatial-scaling relationship of multi-resolution patches. However, since the number of patches at each resolution is quite different, it requires complex pre-processing to spatially align feature vectors of patches in different resolutions. Therefore, it is significant to develop an efficient and effective patch aggregation strategy to learn multi-scale information from WSIs.</p><p>In this work, we propose a Multi-Scale Prototypical Transformer (MSPT) for WSI classification. The MSPT includes two key components: a prototypical Transformer (PT) and a multi-scale feature fusion module (MFFM). The specifically developed PT uses a clustering algorithm to extract instance prototypes from the bags, and then re-calibrates these prototypes at each scale with the self-attention mechanism in Transformer <ref type="bibr" target="#b19">[19]</ref>. MFFM is designed to effectively fuse multi-scale information of WSIs, which utilizes the MLP-Mixer <ref type="bibr" target="#b20">[20]</ref> to learn effective representations by aggregating the multi-scale prototypes generated by the PT. The MLP-Mixer adopts two types of MLP layers to allow information communication in different dimensions of data.</p><p>The contributions of this work are summarized as follows:</p><p>1) A novel prototypical Transformer (PT) is proposed to learn superior prototype representation for WSI classification by integrating prototypical learning into the Transformer architecture. It can effectively re-calibrate the cluster prototypes as well as reduce the computational complexity of the Transformer. 2) A new multi-scale feature fusion module (MFFM) is developed based on the MLP-Mixer to enhance the information communication among phenotypes. It can effectively capture multi-scale information in WSI to improve the performance of WSI classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MIL Problem Formulation</head><p>MIL is a typical weakly supervised learning method, where the training data consists of a set of bags, and each bag contains multiple instances. The goal of MIL is to learn a classifier that can predict the label of a bag based on the instances in it. In binary classification, a bag can be marked as negative if all in-stances in the bag are negative, otherwise, the bag is labeled as positive with at least one positive instance. In the MIL setting, a WSI is considered as a bag and the numerous cropped patches in WSI are regarded as instances in the bag. A WSI dataset T can be defined as:</p><formula xml:id="formula_0">T = {x i , y i } i=N i=1 , x i = I j i j=n j=1 ,<label>(1)</label></formula><p>where x i denotes a patient, y i the label of x i , I j i is the j-th instance of x i , N is the number of patients and n is the number of instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-scale Prototypical Transformer (MSPT)</head><p>The overall architecture of MSPT is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. A WSI is first divided into nonoverlapping patches at different resolutions, and a pre-trained ResNet18 <ref type="bibr" target="#b21">[21]</ref> is used to extract features from each patch. The learned multi-scale features are then fed into the proposed MSPT, which consists of a PT and an MFFM, to re-calibrate cluster prototypes at each scale and fuse multi-scale information of WSI. Finally, a WSI-level classifier is trained to predict the bag label.</p><p>Pre-training. It is a time consuming and tedious task for pathologists to annotate the patch-level labels in gigapixel WSIs, thus, a common practice is to use a pre-trained encoder network to extract instance-level features, such as an ImageNet pre-trained encoder or a self-supervised pre-trained encoder. In this work, we follow <ref type="bibr" target="#b8">[9]</ref> to adopt SimCLR <ref type="bibr" target="#b22">[22]</ref> to pre-training the patch encoder at different resolutions. SimCLR is a self-supervised learning algorithm to pre-trainng a network by maximizing the similarity between positive pairs and minimizing the similarity between negative pairs <ref type="bibr" target="#b22">[22]</ref>. After pre-training, the extracted instances of different scales are fed into MSPT for prototype learning and multi-scale learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prototypical Transformer (PT).</head><p>Most tissues in WSIs are redundancy, and therefore, we introduce the instance prototypes to reduce redundant instances. Specifically, for each instance bag X bag âˆˆ R nÃ—d k , the K-means clustering algorithm is applied on all instances to get K centers (prototypes). These cluster prototypes can be used as instances to represent a new bag P bag âˆˆ R kÃ—d k . However, the K-means clustering algorithm is sensitive to the initial selection of cluster centers, i.e. different initializations can lead to different results, and the final result may not be the global optimal solution. It is essential to try different initializations and choose the one with the lowest error. However, the WSI dataset generally has a long sequence of instances, which makes the clustering algorithms computationally expensive and slow down as the size of the bag increases.</p><p>To solve the issue above, we propose to apply the self-attention (SA) mechanism in Transformer to re-calibrate these cluster prototypes. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the optimization process can be divided into two steps: 1) the initial cluster prototype bag P bag is obtained in the pre-processing stage by using the K-means clustering on X bag ; ; 2) PT uses X bag to optimize P bag via the self-attention mechanism in Transformer. The detailed process is as follows:</p><formula xml:id="formula_1">SA P bag , X bag = softmax QK T âˆš d k â€¢ V = softmax W q P bag (X bag W k ) T âˆš d k W v X bag â†’ A map W v X bag â†’ P<label>(2)</label></formula><p>where W q , W k , W v âˆˆ R d k Ã—d k are trainable matrices of query P bag and the key-value pair (X bag , X bag ), respectively, and A map âˆˆ R kÃ—n is the attention matrix to compute the weight of X bag . Thus, the computational complexity of SA is O(nm) instead of O n 2 , and the k is much less than n. Specifically, for a single clustering prototype p k âˆˆ P, the SA layer scores the pairwise similarity between p k and x n for all x n âˆˆ X, which can be written as a row vector [a k1 , a k2 , a k3 , . . . , a kn ] in A map . These attention scores are then weighted to X bag to update the p k âˆˆ R 1Ã—d k for completing the calibration of the clustering prototypes P âˆˆ R kÃ—d k .</p><p>As mentioned above, existing clustering-based MIL methods use the K-means clustering to identify instances prototypes in the bag, where the most important instances that contain the key semantic information may be ignored. On the contrary, our PT can efficiently use all the instances to update the cluster prototypes multiple times. Therefore, the combination of bag instances is no longer static and fixed, but diverse and dynamic. It means that different new bags can be fed into the MFFM each time. In addition, by applying PT to each scale, the number of cluster prototypes obtained at different scales is consistent, so there is no need for additional operations to align multi-scale features.</p><p>Multi-scale Feature Fusion Module (MFFM). To fuse the output clustered prototypes at different scales in MSPT, we proposed an MFFM, which consists of an MLP-Mixer and a Gated Attention Pooling (GAP). The MLP-Mixer is used to enhance the information communication of the prototype representation, and the GAP is used to get the WSI-level representation for WSI classification.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, The Mixer layer of MLP-Mixer contains one token-mixing MLP and one channel-mixing MLP, each consisting of two fully-connected layers and a GELU activation function <ref type="bibr" target="#b23">[23]</ref>. Token-mixing MLP is a cross-location operation to mix all prototypes, while channel-mixing MLP is a pre-location operation to mix features of each prototype. Thus, MLP-Mixer allows the information communication between different prototypes and prototype features to learn superior representation through information aggregation. Specifically, the procedure of MFFM is described as follows:</p><p>We first perform the feature concatenation operation on the multi-scale output clustering prototypes P20Ã— , P10Ã— , P5Ã— to construct a feature pyramid P:</p><formula xml:id="formula_2">concat P20Ã— , P10Ã— , P5Ã— â†’ P âˆˆ R kÃ—3d k (3)</formula><p>where d k is the feature vector dimension of the prototypes.</p><p>Then, the P is fed to the MLP-Mixer to obtain the corresponding hidden feature representation H âˆˆ R kÃ—3d k as follows:</p><formula xml:id="formula_3">H 1 = P T + W 2 Ïƒ W 1 LN P T H = H T 1 + W 4 Ïƒ W 3 LN H T 1 (4)</formula><p>where LN denotes the layer normalization, Ïƒ denotes the activation function implemented by GELU,</p><formula xml:id="formula_4">W 1 âˆˆ R kÃ—c , W 2 âˆˆ R cÃ—k , W 3 âˆˆ R 3d k Ã—d s and W 4 âˆˆ R d s Ã—3d</formula><p>k are the weight matrices of MLP layers.c and d s are tunable hidden widths in the token-mixing and channel-mixing MLP, respectively. Finally, the H is fed to the gated attention pooling (GAP) <ref type="bibr" target="#b7">[8]</ref> to get the WSI-level representation Z âˆˆ R 1Ã—3d k for WSI classification:</p><formula xml:id="formula_5">Z = GAP(H ) Å¶ = softmax(MLP(Z)) (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where Y âˆˆ R 1Ã—d out is the class label probability of the bag, and d out is the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>To evaluate the effectiveness of MSPT, we conducted experiments on two public dataset, namely Camelyon16 <ref type="bibr" target="#b24">[24]</ref> and TCGA-NSCLC. Camelyon16 is a WSI dataset for the automated detection of metastases in lymph node tissue slides. It includes 270 training samples and 129 testing samples. After pre-processing, a total of 2.4 million patches at Ã—20 magnification, 0.56 million patches at Ã—10 magnification, and 0.16 million patches at Ã—5 magnification, with an average of about 5900, 1400, and 400 patches per bag. The TCGA-NSCLC dataset includes two sub-types of lung cancer, i.e., Lung Squamous Cell Carcinoma (TGCA-LUSC) and Lung Adenocarcinoma (TCGA-LUAD). We collected a total of 854 diagnostic slides from the National Cancer Institute Data Portal (https:// portal.gdc.cancer.gov). The dataset yields 4.3 million patches at 20Ã— magnification, 1.1 million patches at 10Ã— magnification, and 0.30 million patches at 5Ã— magnification with an average of about 5000, 1200, and 350 patches per bag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment Setup and Evaluation Metrics</head><p>In WSI pre-processing, each slide is cropped into non-overlapping 256 Ã— 256 patches at different magnifications, and a threshold is set to filter out background ones. After patching, we use a pre-trained ResNet18 model to convert each 256 Ã— 256 patch into a 512-dimensional feature vector. We selected accuracy (ACC) and area under curve (AUC) as evaluation metrics. For Camelyon16 dataset, we reported the results of the official testing set. For TCGA-NSCLC, we conducted five cross-validation on the 854 slides, and the results are reported in the format of mean Â±SD (standard deviation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>For the feature extractor, we employed the SimCLR encoder trained by Lee et al. <ref type="bibr" target="#b8">[9]</ref> for the Camelyon16 and TCGA datasets. But <ref type="bibr" target="#b8">[9]</ref> only trained SimCLR encoders at 20Ã— and 5Ã— magnification, to align with that setting, we used the same settings to train the SimCLR encoder at 10Ã— magnification on both datasets. For the proposed MSPT, the Adam optimizer was used to update the model weights, the initial learning rate of 1e-4 with a weight decay of 1e-5. The mini-batch size was set as 1. The MSPT models were trained for 150 epochs and they would early stop if the loss would not decrease in the past 30 epochs. All models were implemented by Python 3.8 with PyTorch toolkit 1.11.0 on a platform equipped with an NVIDIA GeForce RTX 3090 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparisons Experiment</head><p>Comparison Algorithms. The proposed MSPT was compared to state-of-the-art MILbased algorithms: 1) The traditional pooling operators, such as mean-pooling and maxpooling; 2) the attention-based algorithms, including ABMIL <ref type="bibr" target="#b7">[8]</ref> and DSMIL <ref type="bibr" target="#b8">[9]</ref>; 3) the Transformer-based algorithm TransMIL <ref type="bibr" target="#b10">[11]</ref>; 4) The clustering-based algorithm ReMix <ref type="bibr" target="#b16">[16]</ref>.  <ref type="table" target="#tab_0">1</ref> shows the comparison results on the Camelyon16 and TCGA-NSCLC datasets. In CAMELYON16, it can be found that the proposed MSPT outperforms all the compared algorithms with the best accuracy of 0.9536, and AUC of 0.9869. Compared to other algorithms, MSPT improves at least 0.78%, and 1.07% on classification ACC and AUC, indicating the effectiveness of MFFM to learn the multi-scale information of WSIs. In addition, PT achieves the best classification results in the single-resolution methods and outperforms ReMix on all indices, which proves PT can effectively re-calibrate the clustering prototypes.</p><p>In TCGA-NSCLC, the proposed MSPT algorithm again outperforms all the compared algorithms on all indices. It achieves the best classification performance of 0.9289 Â± 0.011 and 0.9622 Â± 0.015 on the ACC and AUC. Moreover, MSPT improves at least 0.78% and 1.03%, respectively, on the corresponding indices compared with all other algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>To evaluate the contribution of PT and MFFM in the proposed MSPT, we further conducted a series of ablation studies.</p><p>Investigation of the Number of Prototypes in PT. To evaluate the effectiveness of the PT, we first changed the number of prototypes K in the range of {1, 2, 4, 8, 16, 32} to get the optimal K for each dataset. Then, the following two variants were compared with PT: (1) Full-bag: the first variant was only trained on all the instances; (2) Prototype-bag: the second variant was only trained on the cluster prototypes.</p><p>As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the horizontal axes denote the number of prototypes, and the vertical axes denote the classification accuracy. In the Camelyon16 dataset, the performance of both PT and Prototype-bag increases with the increase of K value, and achieves the best results with K = 16. In the TCGA-NSCLC dataset, PT always outperforms the Fullbag and Prototype-bag. These experimental results demonstrate that PT can effectively re-calibrate the clustering prototypes to achieve superior results. Investigation of Multi-scale Fusion. We further compared our MFFM with several other fusion strategies, including (1) Concatenation: this variant concatenated the cluster prototypes of each magnification before the classifier. (2) MS-Max: this variant used max-pooling on the cluster prototypes for each magnification, and then added them. (3) MS-Attention: this variant used attention-pooling <ref type="bibr" target="#b7">[8]</ref> on the cluster prototypes for each magnification, and then added them.</p><p>Table <ref type="table" target="#tab_1">2</ref> gives the results on the Camelyon16 and TCGA-NSCLC datasets. Compared with other multi-scale variants, the proposed MSPT improves ACC by at least 0.78% and 0.85% on Camelyon16 and TCGA-NSCLC, respectively, which proves that the MLP-Mixer in MFFM can effectively enhance the information communication among phenotypes and their features, thus improving the performance of feature aggregation. More Studies. We provide more empirical studies, i.e., the effect of the multiresolution scheme, the visualization results, and the training budgets, in Supplementary Materials to better understand MSPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In summary, we propose an MSPT for WSI classification that combine the prototypebased learning and multi-scale learning to generate powerful WSI-level representation. The MSPT reduces redundant instances in WSI bags by replacing instances with updatable instance prototypes, and avoids complicated procedures to align patch features at different scales. Extensive experiments validate the effectiveness of the proposed MSPT. In the future, we will develop an attention mechanism based on the magnification level to re-weight the features from different scales before fusion in MSPT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed MSPT.</figDesc><graphic coords="4,82,98,56,42,286,45,161,98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The structure of MFFM.</figDesc><graphic coords="5,75,81,316,07,272,71,93,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Ablation study on the number of prototypes.</figDesc><graphic coords="8,71,97,305,33,308,80,132,31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison results on the Camelyon16 and TCGA datasets.</figDesc><table><row><cell>Method</cell><cell>Camelyon16</cell><cell></cell><cell>TCGA-NSCLC</cell><cell></cell></row><row><cell></cell><cell>Accuracy</cell><cell>AUC</cell><cell>Accuracy</cell><cell>AUC</cell></row><row><cell>Mean-Pooling</cell><cell>0.8837</cell><cell>0.8916</cell><cell>0.8911 Â± 0.011</cell><cell>0.9230 Â± 0.010</cell></row><row><cell>Max-Pooling</cell><cell>0.9147</cell><cell>0.9666</cell><cell>0.9136 Â± 0.014</cell><cell>0.9441 Â± 0.016</cell></row><row><cell>ABMIL [8]</cell><cell>0.9302</cell><cell>0.9752</cell><cell>0.9123 Â± 0.015</cell><cell>0.9457 Â± 0.017</cell></row><row><cell>DSMIL [9]</cell><cell>0.9380</cell><cell>0.9762</cell><cell>0.9049 Â± 0.010</cell><cell>0.9359 Â± 0.011</cell></row><row><cell>TransMIL [11]</cell><cell>0.9225</cell><cell>0.9734</cell><cell>0.9095 Â± 0.014</cell><cell>0.9432 Â± 0.016</cell></row><row><cell>ReMix [16]</cell><cell>0.9458</cell><cell>0.9740</cell><cell>0.9167 Â± 0.013</cell><cell>0.9509 Â± 0.016</cell></row><row><cell>PT (Ours)</cell><cell>0.9458</cell><cell>0.9809</cell><cell>0.9257 Â± 0.011</cell><cell>0.9567 Â± 0.013</cell></row><row><cell>MSPT (Ours)</cell><cell>0.9536</cell><cell>0.9869</cell><cell>0.9289 Â± 0.011</cell><cell>0.9622 Â± 0.015</cell></row><row><cell cols="2">Experimental Results. Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Classification results for evaluating different fusion strategies. All variants used the WSIs with three resolutions (5Ã— + 10Ã— + 20Ã—).</figDesc><table><row><cell>Method</cell><cell>Camelyon16</cell><cell></cell><cell>TCGA-NSCLC</cell><cell></cell></row><row><cell></cell><cell>ACC</cell><cell>AUC</cell><cell>ACC</cell><cell>AUC</cell></row><row><cell>Concatenation</cell><cell>0.9147</cell><cell>0.9598</cell><cell>0.9147 Â± 0.018</cell><cell>0.9438 Â± 0.016</cell></row><row><cell>MS-Max</cell><cell>0.9302</cell><cell>0.9729</cell><cell>0.9203 Â± 0.014</cell><cell>0.9527 Â± 0.019</cell></row><row><cell>MS-Attention</cell><cell>0.9458</cell><cell>0.9786</cell><cell>0.9204 Â± 0.016</cell><cell>0.9571 Â± 0.012</cell></row><row><cell>MFFM</cell><cell>0.9536</cell><cell>0.9869</cell><cell>0.9289 Â± 0.011</cell><cell>0.9722 Â± 0.015</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work is supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">81871428</rs>) and 111 Project (<rs type="grantNumber">D20031</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NTPjzHm">
					<idno type="grant-number">81871428</idno>
				</org>
				<org type="funding" xml:id="_TpwshKQ">
					<idno type="grant-number">D20031</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clinical-grade computational pathology using weakly supervised deep learning on whole slide images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Campanella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1301" to="1309" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Artificial intelligence in digital pathology-new tools for diagnosis and precision oncology</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Schalper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Rimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Velcheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Clin. Oncol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="703" to="715" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A practical guide to whole slide imaging: a white paper from the digital pathology association</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zarella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Pathol. Lab. Med</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="234" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural network models for computational histopathology: a survey</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Srinidhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ciga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101813</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cellular community detection for tissue phenotyping in colorectal cancer histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101696</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A graph-transformer for whole slide image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3003" to="3015" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data-efficient and weakly supervised computational pathology on whole-slide images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="555" to="570" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Eliceiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14318" to="14328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling vision transformers to gigapixel images via hierarchical self-supervised learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16144" to="16155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TransMIL: transformer based correlated multiple instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DT-MIL: deformable transformer for multi-instance learning on histopathological image</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="206" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-3_20" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Integration of patch features through self-supervised learning and transformer for survival analysis on whole slide images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_54</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-3_54" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="561" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lymph node metastasis prediction from whole slide images with transformer-guided multiinstance learning and knowledge transfer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2777" to="2787" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jonnagaddala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101789</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ReMix: a general and efficient framework for multiple instance learning based whole slide image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-7_4" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-scale domain-adversarial multiple-instance CNN for cancer subtype classification with unannotated histopathological images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3852" to="3861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">H^2-MIL: exploring hierarchical representation with heterogeneous multiple instance learning for whole slide image analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MLP-Mixer: an all-MLP architecture for vision</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24261" to="24272" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2199" to="2210" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
