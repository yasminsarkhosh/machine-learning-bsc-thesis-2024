<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lulin</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Chemical and Biological Engineering</orgName>
								<orgName type="laboratory">Translational and Advanced Bioimaging Laboratory</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Chemical and Biological Engineering</orgName>
								<orgName type="laboratory">Translational and Advanced Bioimaging Laboratory</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ivy</forename><forename type="middle">H M</forename><surname>Wong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Chemical and Biological Engineering</orgName>
								<orgName type="laboratory">Translational and Advanced Bioimaging Laboratory</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Claudia</forename><forename type="middle">T K</forename><surname>Lo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Chemical and Biological Engineering</orgName>
								<orgName type="laboratory">Translational and Advanced Bioimaging Laboratory</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Terence</forename><forename type="middle">T W</forename><surname>Wong</surname></persName>
							<email>ttwwong@ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Chemical and Biological Engineering</orgName>
								<orgName type="laboratory">Translational and Advanced Bioimaging Laboratory</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="735" to="744"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">976500A1BE061DA01EEBD3F027DDB2F9</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_71</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Virtual staining</term>
					<term>Multi-domain image translation</term>
					<term>Thick tissues</term>
					<term>Light-sheet microscopy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The conventional histopathology paradigm can provide the gold standard for clinical diagnosis, which, however, suffers from lengthy processing time and requires costly laboratory equipment. Recent advancements made in deep learning for computational histopathology have sparked lots of efforts in achieving a rapid chemical-free staining technique. Yet, existing approaches are limited to well-prepared thin sections, and invalid in handling more than one stain. In this paper, we present a multiple histological staining model for thick tissues (Mul-HiST), without any laborious sample preparation, sectioning, and staining process. We use the grey-scale light-sheet microscopy image of thick tissues as model input and transfer it into different histologically stained versions, including hematoxylin and eosin (H&amp;E), Masson's trichrome (MT), and periodic acid-Schiff (PAS). This is the first work that enables the automatic and simultaneous generation of multiple histological staining for thick biological samples. Moreover, we empirically demonstrate that the AdaIN-based generator offers an advantage over other configurations to achieve higher-quality multi-style image generation. Extensive experiments also indicated that multi-domain data fusion is conducive to the model capturing shared pathological features. We believe that the proposed MulHiST can potentially be applied in clinical rapid pathology and will significantly improve the current histological workflow.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Histological staining is regarded as the standard protocol in clinical pathological examination, which is used to label biological structures and morphological changes in tissues <ref type="bibr" target="#b0">[1]</ref>. The most frequently used histological staining is H&amp;E stain for the inspection of cell nuclei and the extracellular matrix, in addition to some special stains to complement specific biomarkers and particular structures, such as MT stain used for connective tissues and PAS stain used for mucopolysaccharides <ref type="bibr" target="#b1">[2]</ref>. However, multiple tissue sections are required if special stains are desired since the same section cannot be stained several times in conventional pathology workflow. In general, pathologists need to check the H&amp;E-stained images firstly for a basic examination, and then decide whether to prepare additional sections and perform special stains, which will increase the time for diagnosis. More importantly, the abovementioned traditional histochemical staining techniques can only be performed on thin sections of 2-10 Âµm. Therefore, sample preparation steps, including paraffin embedding, tissue slicing, and chemical dewaxing, will result in long turnaround times and high laboratory infrastructure demands.</p><p>The rapidly emerging field of digital virtual staining has shown great promise to revolutionize the decade-old staining workflow. Zhang et al. <ref type="bibr" target="#b2">[3]</ref> have done pioneering works on multi-stain translation from unstained thin sections, and Yang et al. <ref type="bibr" target="#b3">[4]</ref> also tried to achieve multiple stains generation from label-free tissue images. Both used an image registration to prepare pixel-level matched source-unstained and target-stained image pairs for supervised model training. However, obtaining such pixel-wise aligned data is not accessible for thick tissues as the traditional histological staining can only be performed on thin sections. Even though we can collect the surface cut from the thick specimen and then stain it with chemical reagents, there is still a huge morphological difference due to multiple-layer information captured by a slide-free microscope. Therefore, the virtual staining of thick tissues has to rely on unsupervised methods. There were some primary investigations on virtual staining of thick tissues that use slide-free imaging systems, such as MUSE <ref type="bibr" target="#b4">[5]</ref>, CHAMP <ref type="bibr" target="#b5">[6]</ref>, and UV-PAM <ref type="bibr" target="#b6">[7]</ref>. However, those methods can only produce virtual H&amp;E-stained images instead of multi-stained images. The emergency of starGAN opens new possibilities for multi-domain image translation <ref type="bibr" target="#b7">[8]</ref>, and they achieve flexible facial attribute transfer with the proposed domain label. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> employ the idea of the domain label to represent different staining for multiple histological staining generations. However, those models use H&amp;E staining as input and transfer H&amp;E staining into other stains, which still require laborious tissue embedding and slide sectioning process. <ref type="bibr" target="#b10">[11]</ref> focus on the unsupervised multiple virtual staining from autofluorescence images, yet the input should be images obtained from thin slides, which is not ideal for thick tissues.</p><p>In this paper, we propose MulHiST, a novel Multiple Histological Staining model for Thick biological tissues, which is not feasible in the traditional histochemical staining workflow. To our knowledge, this is the first attempt to achieve multiple histological staining generations for thick tissues. <ref type="foot" target="#foot_0">1</ref> . Our key contributions are: <ref type="bibr" target="#b0">(1)</ref> we propose MulHiST: a generative adversarial network (GAN)-based multi-domain image translation model capable of mapping a given light-sheet (LS) image of thick tissue into its histologically stained (HS) versions. We utilize unsupervised learning and do not require paired images, tailored for the virtual staining from slide-free imaging techniques; (2) we verify that the multi-domain translation can capture more reliable histopathological features for generating high-quality images, eliminating the ambiguity brought by multiple layers of information in thick tissue images; (3) both qualitative and quantitative results on H&amp;E/PAS/MT staining generations show the superiority and efficiency of the proposed MulHiST over other baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>To represent different staining types, we follow the idea of the domain-specific attribute vector proposed in <ref type="bibr" target="#b7">[8]</ref>, aiming to use a one-hot vector c to indicate unstained and various stained domains. During the training, the generator aims to transfer the image style of input image x to the desired domain c while keeping the content of x : G (x, c) â y, which means y has the same pathological context as x but with a different image style. When the training is finished, we use the well-trained generator to achieve multiple histological stain generations.</p><p>In general, the image of thick tissues contains several-layer information, and information from different layers interferes with each other, which will lead to ambiguity in the determination of pathological features, e.g., cell boundary and tissue content. We believe that all the image domains, i.e., LS images and H&amp;E/PAS/MT-stained images, share some domain-invariant features that can facilitate the model training. Moreover, our generator learns the mapping between every two domains. Then, a reconstruction loss can be employed in the single-generator model (orange dashed arrows in Fig. <ref type="figure" target="#fig_0">1</ref>). We only need to input the LS image for the inference to get its corresponding histological stained versions (right part in Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>Unlike starGAN, we add the style code into the model with the Adaptive Instance Normalization (AdaIN) Layer <ref type="bibr" target="#b11">[12]</ref> instead of concatenating the style code with the input image along the channel dimension. We only add the AdaIN to the decoder module, which will not affect the image feature extraction and encoding of the model input. The parameters of AdaIN are dynamically generated by a multilayer perceptron (MLP) based on the style code. The discriminator used in this work follows the one in <ref type="bibr" target="#b12">[13]</ref>, providing adversarial feedback to guide the learning of the generator. However, our discriminator will further classify images into different domains, not limited to real or fake signals.</p><p>During the training, we incorporate multiple domain images and shuffle the ensembled dataset. A style code c can be generated randomly to indicate the target domain for the generator. The generator will transfer the input image into an image with the target style indicated by c, and the semantic content of the original input will not change. The model forward can be expressed by:</p><formula xml:id="formula_0">G(x, c) = Dec(R(Enc(x)), MLP (c)) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where c is the style code that is randomly generated during the training. R is residual block. Enc and Dec are the encoder and decoder in the generator, respectively. The AdaIN layer in the decoder can be computed as:</p><formula xml:id="formula_2">AdaIN (x, s) = Ï(s) x -Î¼(x) Ï(x) + Î¼(s) (2)</formula><p>where x here is the feature map results of the previous layer, and s is the output of MLP. AdaIN aims to align the mean and variance of the input to match those of the desired style. The overall loss formulation of the generator is:</p><formula xml:id="formula_3">L G = Î» 1 L G adv + Î» 2 L rec (<label>3</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">L rec = x -G(G(x, c trg ), c org )<label>(4)</label></formula><p>where the L G adv is the adversarial loss and L rec is the image reconstruction loss. The c trg is the generated target style code and c org is the style code of the original input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>In this work, we prepared six thick tissue slabs and obtained scanned images of â¼15,000 Ã 15,000 pixels using an open-top LS microscope with an excitation wavelength of 266 nm. After imaging, the specimens were sectioned and histologically processed with the standard protocol to obtain HS images for the model training. We chose one set of scanned images as training data, and the others were used for the testing. For the training, we extracted small image patches with the size of 128 Ã 128 randomly. During testing, we divided the tested whole-slide image into 256 Ã 256 patches with 16 pixels overlap to avoid artifacts. There are 11,643 extracted image patches in the testing set.</p><p>Our model was implemented in PyTorch on a single NVIDIA GeForce RTX 3090 GPU. We trained our model with the Adam optimizer (with Î² 1 = 0.5 and Î² 2 = 0.999) <ref type="bibr" target="#b13">[14]</ref>. The initial learning rate was set to 1Ã10 -4 for both generator and discriminator with a linear decay scheduled after 50,000 iterations. The batch size was set to 16. The Î» 1 in (3) was set to 5 and Î» 2 was set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>We quantitatively evaluate our model and results with Kernel Inception Distance (KID) <ref type="bibr" target="#b14">[15]</ref> and FrÃ©chet Inception Distance (FID) <ref type="bibr" target="#b15">[16]</ref>, which are prevalent for evaluating the quality of digitally synthesized images. Unlike natural image generation, biomedical image generation not only require these indicators that reflect the image quality but also require some more convincing metrics with clinical values. As shown in Fig. <ref type="figure" target="#fig_2">3</ref> (5 th column, cycleGAN), the image style of generated results is similar to that of the real staining. However, the MT staining generated by cycleGAN is incorrect since the background and tissue content is reversed, which means that sometimes the model can produce target images with high fidelity, but it is difficult to keep correct semantic content or targeted biomarkers. It is not easy to identify pathological features from the LS image, therefore, we need a ground truth for a more reliable comparison. However, it is infeasible to obtain the well-matched ground truth of thick tissues.</p><p>In this work, we observed that LS images of thick tissues share a similar style as fluorescence images of thin sections i.e., the cell nuclei are highlighted with positive contrast for both thin and thick tissues with the help of fluorescent labels. In this case, we used the model trained on LS images of thick tissues to test the fluorescence images of thin sections. Then, we could prepare the ground truth of thin sections for comprehensive comparison, as well as quantitative indicators, such as mean square error (MSE), structural similarity (SSIM), and peak signalto-noise ratio (PSNR). It is worth mentioning that when tested on the thinsection images, the model trained with thick-tissue images would underperform the model trained with thin-section images. Therefore, for virtual staining on thin slices, it is better to train on thin-section images as there are still some differences in details between data from thick tissues and thin sections. Here, we use the thin-section data only for model validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Quantitative and Qualitative Results</head><p>In this paper, we select cycleGAN <ref type="bibr" target="#b17">[17]</ref>, MUNIT <ref type="bibr" target="#b18">[18]</ref>, and starGAN <ref type="bibr" target="#b7">[8]</ref> as baseline models. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our model surpasses all comparison methods on visual results. From the virtually stained PAS (top zoomed-in regions, yellow square), we can distinguish two different convoluted tubules according to the PAS-positive/negative patterns (yellow/green arrows) in our results. In general, the glomerular and tubular basement membrane, as well as the brush border of the proximal tubules, can be visualized by the PAS staining with pink color, whereas the interior of the distal tubule will not be stained <ref type="bibr" target="#b19">[19]</ref>. There is something pink inside the tubules pointed by yellow arrows in our results and no pink area inside the tubules indicated by green arrows. However, from other model results, it is hard to recognize corresponding histopathological features. Here we train cycleGAN multiple times for every pair of source/target domains so that those three cycleGAN models are independent. For such single-domain image translation, it is hard to satisfy all domains with a correct transformation. Specifically, we can see that the H&amp;E-stained images of cycleGAN are correct, whereas the model reverses the background and cell nuclei in the MT domain. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, neither MUNIT nor starGAN can perform well in this task.</p><p>Moreover, we quantitatively evaluate the model performance with FID and KID scores, as shown in Table <ref type="table" target="#tab_0">1</ref> (top part). We can observe that our model outperforms the other baseline models significantly in three different image domains. Although the H&amp;E staining of cycleGAN also achieves good FID and KID scores, the corresponding PAS and MT staining results are much worse than ours, which also agrees with the qualitative analysis shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>As no ground truth can be provided for the virtual staining of thick tissue. We collected 2 sets of thin mouse kidney sections for the model validation. We used the model trained with LS images of thick tissues to test the scanned images of prepared thin sections. In this situation, we can perform traditional histological staining to obtain ground truth for further comparison. Figure <ref type="figure" target="#fig_2">3</ref> confirms that our MulHiST can generate the correct pathological features, i.e., the PAS-positive proximal convoluted tubules indicated by yellow arrows and PAS-negative distal tubules pointed by green arrows, which are consistent with the ground truth. The same pathological representation can also be observed in the PAS result of cycleGAN, whereas the MT staining generated by cycleGAN presents an obvious error between the background and tissue content. Meanwhile, the quantitative evaluation in Table <ref type="table" target="#tab_0">1</ref> (bottom part) also shows that our proposed model outperforms other baseline models in various evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Analysis</head><p>In this paper, we have two main hypotheses, one is that the model can benefit from data fusion from multiple domains due to the domain-invariant features, and the other is that the AdaIN-based style transfer is better in source image feature extraction compared with channel-wise style code concatenation.  We first verify the importance of domain-invariant features shared by multiple domains. The starGAN <ref type="bibr" target="#b7">[8]</ref> also reached a similar conclusion that different domains will share the same facial context, which is beneficial to facial expression synthesis. Similarly, there are also some shared features in our multi-domain dataset, such as cell nuclei and cytoplasm membranes. We claim that the model training can benefit from incorporating multiple data domains. From Fig. <ref type="figure" target="#fig_3">4</ref>, the synthetic image quality improves with the increase of input data domains. For single-domain translation, the model is sensitive to different domains, where only H&amp;E staining is correct and the other two cannot be determined. This also agrees with the performance of cycleGAN in Fig. <ref type="figure" target="#fig_1">2,</ref><ref type="figure" target="#fig_2">3</ref>. The main reason is that the different models are built independently for single-domain image translation, resulting in the inability to share effective information among domains. When adding another domain, the model can correctly translate the background and tissue content. This can be attributed to the feature-sharing between different data domains. In addition, the triple-domain model is superior to the dualdomain ones, where all H&amp;E, PAS, and MT show clear and natural structures.  Next, we compare the AdaIN-based method and channel-wise concatenation (Fig. <ref type="figure" target="#fig_4">5</ref>). We can observe that both ways can achieve correct style transfer, but from the PAS staining, the AdaIN-based method can produce plausible pathological patterns that another model fails (yellow arrows). The PAS should be stained inside the tubules, and the results from the channel-wise concatenation method will cause ambiguity in the judgment by pathologists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposes a multiple histological image generation model for thick biological samples. This is undesirable in the traditional histopathology workflow as the chemical histological staining should be performed on the thin sections, and the same section cannot be stained with various stains simultaneously. We use slide-free microscopy to capture the thick tissues and translate the scanned images into multiple stained-versions. The model is optimized in an unsupervised manner, fitting the issue of large morphological mismatches between the scanned thick tissue and histologically stained images. Experiment results demonstrated the superiority and the great promise of the proposed method in developing a slide-free, cost-effective, and chemical-free histopathology staining pipeline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed MulHiST network.</figDesc><graphic coords="3,58,98,54,14,334,54,118,57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Virtual generation of PAS-, H&amp;E-, and MT-stained images of thick tissues (from the top to the bottom). Yellow arrows indicate proximal convoluted tubules (positive for PAS), and green ones mean distal convoluted tubules (negative for PAS). (Color figure online)</figDesc><graphic coords="6,43,29,54,47,337,45,139,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Virtual staining output of thin section using different models. The 3 rd column shows the ground truth (GT) obtained via the traditional histology staining protocol.</figDesc><graphic coords="7,58,98,237,59,334,54,113,74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Efficiency evaluation of multi-domain data fusion. Single-domain means 1-to-1 translation, dual-domain is 2-to-2 translation, and triple-domain refers to 3-to-3 one.</figDesc><graphic coords="8,44,79,53,96,334,57,169,75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Comparison of AdaIN-based and channel-wised concatenation-based generator.</figDesc><graphic coords="8,44,79,269,87,334,57,65,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation results on testing data. The top four rows are tested results of thick tissues, and the following ones are that of thin sections.</figDesc><table><row><cell></cell><cell>H&amp;E</cell><cell>PAS</cell><cell>MT</cell><cell>MSE</cell><cell>SSIM</cell><cell>PSNR</cell></row><row><cell></cell><cell>FID</cell><cell>KID FID</cell><cell>KID FID</cell><cell>KID</cell><cell></cell></row><row><cell cols="5">CycleGAN 68.06 0.042 111.29 0.062 150.77 0.096 -</cell><cell>-</cell><cell>-</cell></row><row><cell>MUNIT</cell><cell cols="4">150.20 0.114 131.93 0.086 128.80 0.098 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">StarGAN 165.20 0.114 207.67 0.158 208.89 0.180 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">MulHiST 65.96 0.049 63.20 0.038 67.83 0.036 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">CycleGAN 74.16 0.044 137.18 0.080 159.24 0.098 2257.73 0.5793 15.90</cell></row><row><cell>MUNIT</cell><cell cols="6">141.71 0.078 136.23 0.074 118.51 0.067 7749.29 0.4675 9.35</cell></row><row><cell cols="7">StarGAN 160.91 0.108 185.07 0.127 155.02 0.105 1266.57 0.6811 17.40</cell></row><row><cell cols="2">MulHiST 69.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>90 0.039 82.49 0.048 84.44 0.055 1217.36 0.6475 17.50</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>An implementation of MulHiST is available at https://github.com/TABLAB-HKUST/MulHiST.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Histopathological image analysis: a review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Gurcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="147" to="171" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Histological stains: a literature review and case study</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Alturkistani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Tashkandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Mohammedsaleh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Glob. J. Health Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Digital synthesis of histological stains using micro-structured and multiplexed virtual staining of label-free tissue</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rivenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Delis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Light: Sci. Appl</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Virtual stain transfer in histology via cascaded deep neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Photonics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3134" to="3143" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep-learning-assisted microscopy with ultraviolet surface excitation for rapid slide-free histological imaging</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Exp</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5920" to="5938" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High-throughput, label-free and slide-free histological imaging by computational microscopy and unsupervised learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2102358</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Label-free intraoperative histology of bone tissue via deep-learningassisted ultraviolet photoacoustic microscopy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stargan: unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mvfstain: multiple virtual functional stain histopathology images generation based on specific domain mapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102520</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unpaired multi-domain stain transfer for kidney histopathological images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1630" to="1637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised multiple virtual histological staining from label-free autofluorescence images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Wk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Biomedical Imaging</title>
		<meeting>the IEEE International Symposium on Biomedical Imaging</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>BiÅkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01401</idno>
		<title level="m">Demystifying mmd gans</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised imageto-image translation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Renal fibrosis, immune cell infiltration and changes of trpc channel expression after unilateral ureteral obstruction in trpc6-/-mice</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Haschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>NÃ¼rnberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>KrÃ¤mer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gollasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>MarkÃ³</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell. Physiol. Biochem</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1484" to="1502" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
